<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="machine learning,Python,statistics,"><link rel="alternate" href="/atom.xml" title="Abracdabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="利用朴素贝叶斯进行文本分类准备数据从文本中构建词向量下面写一个词表到向量的转换函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def LoadDataSet():    &quot;&quot;&quot; Load a vector-like dat"><meta property="og:type" content="article"><meta property="og:title" content="Naive Bayes"><meta property="og:url" content="http://yoursite.com/2017/02/25/Naive-Bayes/index.html"><meta property="og:site_name" content="Abracdabra"><meta property="og:description" content="利用朴素贝叶斯进行文本分类准备数据从文本中构建词向量下面写一个词表到向量的转换函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def LoadDataSet():    &quot;&quot;&quot; Load a vector-like dat"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/nb/nb.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/nb/gnb_algo.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/nb/gnb_mle.png"><meta property="og:updated_time" content="2017-02-25T09:58:29.981Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Naive Bayes"><meta name="twitter:description" content="利用朴素贝叶斯进行文本分类准备数据从文本中构建词向量下面写一个词表到向量的转换函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def LoadDataSet():    &quot;&quot;&quot; Load a vector-like dat"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/nb/nb.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/02/25/Naive-Bayes/"><title>Naive Bayes | Abracdabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracdabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/25/Naive-Bayes/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracdabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracdabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Naive Bayes</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-25T13:40:23+08:00">2017-02-25 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/02/25/Naive-Bayes/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/02/25/Naive-Bayes/" itemprop="commentsCount"></span> </a></span><span id="/2017/02/25/Naive-Bayes/" class="leancloud_visitors" data-flag-title="Naive Bayes"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="利用朴素贝叶斯进行文本分类"><a href="#利用朴素贝叶斯进行文本分类" class="headerlink" title="利用朴素贝叶斯进行文本分类"></a>利用朴素贝叶斯进行文本分类</h1><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>从文本中构建词向量</p><p>下面写一个词表到向量的转换函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">LoadDataSet</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">""" Load a vector-like data set tranfered by</span></div><div class="line">    a data set list that generated by artifical.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        return_vec: the vector-like data set.</div><div class="line">        class_vec: the class label corresponds to the data items.</div><div class="line">    """</div><div class="line">    posting_list = [[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>,</div><div class="line">                     <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>],</div><div class="line">                    [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>,</div><div class="line">                     <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],</div><div class="line">                    [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>,</div><div class="line">                     <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],</div><div class="line">                    [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],</div><div class="line">                    [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>,</div><div class="line">                     <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],</div><div class="line">                    [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]]</div><div class="line">    class_vec = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1 is abusive, 0 not</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> posting_list, class_vec</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">CreateVocabList</span><span class="params">(data_set)</span>:</span></div><div class="line">    <span class="string">""" Create vocabulary list from vector-like data set.</span></div><div class="line"></div><div class="line">    Arguments:</div><div class="line">        data_set: the data source.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        vocab_list: the vocabulary list.</div><div class="line">    """</div><div class="line"></div><div class="line">    vocab_set = set([])</div><div class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> data_set:</div><div class="line">        vocab_set = vocab_set | set(document)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> list(vocab_set)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">SetOfWords2Vec</span><span class="params">(vocab_list, input_set)</span>:</span></div><div class="line">    <span class="string">""" Transfer the words list to vector for each posting.</span></div><div class="line"></div><div class="line">    Arguments:</div><div class="line">        vocab_list: The vocabulary list.</div><div class="line">        input_set: The posting that ready to transfer to vector.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        return_vec: the result vector.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># Initialize</span></div><div class="line">    return_vec = [<span class="number">0</span>] * len(vocab_list)</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> input_set:</div><div class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocab_list:</div><div class="line">            return_vec[vocab_list.index(word)] = <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">"the word: %s is not in my Vocabulary"</span> % word)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> return_vec</div></pre></td></tr></table></figure><p>下面对函数的功能进行测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">13</span>]: <span class="keyword">import</span> bayes</div><div class="line"></div><div class="line">In [<span class="number">14</span>]: list_of_posts, list_classes = bayes.LoadDataSet()</div><div class="line"></div><div class="line">In [<span class="number">15</span>]: my_vocab_list = bayes.CreateVocabList(list_of_posts)</div><div class="line"></div><div class="line">In [<span class="number">16</span>]: my_vocab_list</div><div class="line">Out[<span class="number">16</span>]:</div><div class="line">[<span class="string">'help'</span>,</div><div class="line"> <span class="string">'worthless'</span>,</div><div class="line"> <span class="string">'I'</span>,</div><div class="line"> <span class="string">'take'</span>,</div><div class="line"> <span class="string">'love'</span>,</div><div class="line"> <span class="string">'maybe'</span>,</div><div class="line"> <span class="string">'stupid'</span>,</div><div class="line"> <span class="string">'to'</span>,</div><div class="line"> <span class="string">'not'</span>,</div><div class="line"> <span class="string">'please'</span>,</div><div class="line"> <span class="string">'quit'</span>,</div><div class="line"> <span class="string">'park'</span>,</div><div class="line"> <span class="string">'posting'</span>,</div><div class="line"> <span class="string">'dog'</span>,</div><div class="line"> <span class="string">'dalmation'</span>,</div><div class="line"> <span class="string">'steak'</span>,</div><div class="line"> <span class="string">'my'</span>,</div><div class="line"> <span class="string">'how'</span>,</div><div class="line"> <span class="string">'food'</span>,</div><div class="line"> <span class="string">'so'</span>,</div><div class="line"> <span class="string">'stop'</span>,</div><div class="line"> <span class="string">'is'</span>,</div><div class="line"> <span class="string">'garbage'</span>,</div><div class="line"> <span class="string">'flea'</span>,</div><div class="line"> <span class="string">'problems'</span>,</div><div class="line"> <span class="string">'has'</span>,</div><div class="line"> <span class="string">'buying'</span>,</div><div class="line"> <span class="string">'ate'</span>,</div><div class="line"> <span class="string">'him'</span>,</div><div class="line"> <span class="string">'licks'</span>,</div><div class="line"> <span class="string">'mr'</span>,</div><div class="line"> <span class="string">'cute'</span>]</div><div class="line"></div><div class="line">In [<span class="number">17</span>]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[<span class="number">0</span>])</div><div class="line">Out[<span class="number">17</span>]:</div><div class="line">[<span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>]</div><div class="line"></div><div class="line">In [<span class="number">18</span>]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[<span class="number">3</span>])</div><div class="line">Out[<span class="number">18</span>]:</div><div class="line">[<span class="number">0</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">1</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>,</div><div class="line"> <span class="number">0</span>]</div></pre></td></tr></table></figure><p>看上去一切都work，可以进入下一步了。</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>从词向量计算概率</p><p>我们来看看贝叶斯公式：</p><script type="math/tex;mode=display">p(c_i | w) = \frac{p(w | c_i) p(c_i)}{p(w)}</script><p>这里$w$代表词向量， 可以看出$c_i$的计算十分简单，值得注意的是，根据朴素贝叶斯的假设，有：</p><script type="math/tex;mode=display">p(w | c_i) = p(w_0, w_1, w_2, \cdots, w_N | c_i) = p(w_0 | c_i)p(w_1 | c_i)p(w_2 | c_i) \cdots p(w_N | c_i)</script><p>当需要预测新样本的类别时：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/nb/nb.png" alt="nb"></p><p>这样一切就很清楚了，下面给出伪代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">计算每个类别的文档数目</div><div class="line"><span class="keyword">for</span> 每一篇文档：</div><div class="line">	<span class="keyword">for</span> 每一个类别：</div><div class="line">		<span class="keyword">if</span> 词条出现在文档中：</div><div class="line">			增加该词条的计数值</div><div class="line">		增加总词条数的计数值</div><div class="line">	<span class="keyword">for</span> 每一个类别：</div><div class="line">		<span class="keyword">for</span> 每一个词条：</div><div class="line">			将该词条的数目除以总词条数目得到条件概率</div><div class="line">	返回每个类别的条件概率</div></pre></td></tr></table></figure><p>注意，这里的$p(w_j | c_i)$是要根据整个训练集来算，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">TrainNaiveBayes0</span><span class="params">(train_matrix, train_category)</span>:</span></div><div class="line">    <span class="string">""" the training method.</span></div><div class="line"></div><div class="line">    Arguments:</div><div class="line">        train_matrix: The train data.</div><div class="line">        train_category: The train label.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        p0_vect: The conditional probability of w by c0</div><div class="line">        p1_vect: The conditional probability of w by c1</div><div class="line">        p_abusive: The conditional probability of c1</div><div class="line">    """</div><div class="line"></div><div class="line">    num_train_docs = len(train_matrix)</div><div class="line">    num_words = len(train_matrix[<span class="number">0</span>])</div><div class="line">    p_abusive = sum(train_category) / num_train_docs</div><div class="line">    p0_num = zeros(num_words)</div><div class="line">    p1_num = zeros(num_words)</div><div class="line">    p0_denom = <span class="number">0.0</span></div><div class="line">    p1_denom = <span class="number">0.0</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train_docs):</div><div class="line">        <span class="keyword">if</span> train_category[i] == <span class="number">1</span>:</div><div class="line">            p1_num += train_matrix[i]</div><div class="line">            p1_denom += sum(train_matrix[i])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            p0_num += train_matrix[i]</div><div class="line">            p0_denom += sum(train_matrix[i])</div><div class="line"></div><div class="line">    p1_vect = p1_num / p1_denom</div><div class="line">    p0_vect = p0_num / p0_denom</div><div class="line"></div><div class="line">    <span class="keyword">return</span> p0_vect, p1_vect, p_abusive</div></pre></td></tr></table></figure><p>同样进行一下测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">25</span>]: train_mat = []</div><div class="line"></div><div class="line">In [<span class="number">26</span>]: <span class="keyword">for</span> post_in_doc <span class="keyword">in</span> list_of_posts:</div><div class="line">    ...:     train_mat.append(bayes.SetOfWords2Vec(my_vocab_list, post_in_doc))</div><div class="line">    ...:</div><div class="line"></div><div class="line">In [<span class="number">27</span>]: p0_v, p1_v, p_ab = bayes.TrainNaiveBayes0(train_mat, list_classes)</div><div class="line"></div><div class="line">In [<span class="number">28</span>]: p_ab</div><div class="line">Out[<span class="number">28</span>]: <span class="number">0.5</span></div><div class="line"></div><div class="line">In [<span class="number">29</span>]: p0_v</div><div class="line">Out[<span class="number">29</span>]:</div><div class="line">array([ <span class="number">0.04166667</span>,  <span class="number">0.</span>        ,  <span class="number">0.04166667</span>,  <span class="number">0.</span>        ,  <span class="number">0.04166667</span>,</div><div class="line">        <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.04166667</span>,  <span class="number">0.</span>        ,  <span class="number">0.04166667</span>,</div><div class="line">        <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.04166667</span>,  <span class="number">0.04166667</span>,</div><div class="line">        <span class="number">0.04166667</span>,  <span class="number">0.125</span>     ,  <span class="number">0.04166667</span>,  <span class="number">0.</span>        ,  <span class="number">0.04166667</span>,</div><div class="line">        <span class="number">0.04166667</span>,  <span class="number">0.04166667</span>,  <span class="number">0.</span>        ,  <span class="number">0.04166667</span>,  <span class="number">0.04166667</span>,</div><div class="line">        <span class="number">0.04166667</span>,  <span class="number">0.</span>        ,  <span class="number">0.04166667</span>,  <span class="number">0.08333333</span>,  <span class="number">0.04166667</span>,</div><div class="line">        <span class="number">0.04166667</span>,  <span class="number">0.04166667</span>])</div><div class="line"></div><div class="line">In [<span class="number">30</span>]: p1_v</div><div class="line">Out[<span class="number">30</span>]:</div><div class="line">array([ <span class="number">0.</span>        ,  <span class="number">0.10526316</span>,  <span class="number">0.</span>        ,  <span class="number">0.05263158</span>,  <span class="number">0.</span>        ,</div><div class="line">        <span class="number">0.05263158</span>,  <span class="number">0.15789474</span>,  <span class="number">0.05263158</span>,  <span class="number">0.05263158</span>,  <span class="number">0.</span>        ,</div><div class="line">        <span class="number">0.05263158</span>,  <span class="number">0.05263158</span>,  <span class="number">0.05263158</span>,  <span class="number">0.10526316</span>,  <span class="number">0.</span>        ,</div><div class="line">        <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,  <span class="number">0.05263158</span>,  <span class="number">0.</span>        ,</div><div class="line">        <span class="number">0.05263158</span>,  <span class="number">0.</span>        ,  <span class="number">0.05263158</span>,  <span class="number">0.</span>        ,  <span class="number">0.</span>        ,</div><div class="line">        <span class="number">0.</span>        ,  <span class="number">0.05263158</span>,  <span class="number">0.</span>        ,  <span class="number">0.05263158</span>,  <span class="number">0.</span>        ,</div><div class="line">        <span class="number">0.</span>        ,  <span class="number">0.</span>        ])</div></pre></td></tr></table></figure><p>但是上述代码存在一些缺陷，首先，计算$p(w_j | c_i)$可能会出现结果为0的情况，那么最后的结果就会为0，那么需要进行一些修改 (为什么是2？)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">p0_num = ones(num_words)</div><div class="line">p1_num = ones(num_words)</div><div class="line">p0_denom = 2.0</div><div class="line">p1_denom = 2.0</div></pre></td></tr></table></figure><p>另外一个就是下溢的问题，所以要改用log函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">p1_vect = log(p1_num / p1_denom)</div><div class="line">p0_vect = log(p0_num / p0_denom)</div></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>最后写一个分类和测试函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ClassifyNaiveBayes</span><span class="params">(vec_to_classify, p0_vect, p1_vect, p_abusive)</span>:</span></div><div class="line">    <span class="string">""" Classify.</span></div><div class="line"></div><div class="line">    Arguments:</div><div class="line">        vec_to_classify: The vector to classify.</div><div class="line">        p0_vect: The conditional probability of w by c0.</div><div class="line">        p1_vect: The conditional probability of w by c1.</div><div class="line">        p_abusive: The conditional probability of c1.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        0: The predict class is 0</div><div class="line">        1: The predict class is 1</div><div class="line"></div><div class="line">    """</div><div class="line"></div><div class="line">    p1 = sum(vec_to_classify * p1_vect) + log(p_abusive)</div><div class="line">    p0 = sum(vec_to_classify * p0_vect) + log(<span class="number">1</span> - p_abusive)</div><div class="line">    <span class="keyword">if</span> p1 &gt; p0:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">TestNaiveBayes</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">""" A test method.</span></div><div class="line">    """</div><div class="line"></div><div class="line">    list_of_posts, list_of_classes = LoadDataSet()</div><div class="line">    my_vocab_list = CreateVocabList(list_of_posts)</div><div class="line">    train_mat = []</div><div class="line">    <span class="keyword">for</span> post_in_doc <span class="keyword">in</span> list_of_posts:</div><div class="line">        train_mat.append(SetOfWords2Vec(my_vocab_list, post_in_doc))</div><div class="line">    p0_v, p1_v, p_ab = TrainNaiveBayes0(</div><div class="line">        array(train_mat), array(list_of_classes))</div><div class="line">    test_entry = [<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</div><div class="line">    this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry))</div><div class="line">    print(test_entry, <span class="string">'classified as: '</span>,</div><div class="line">          ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab))</div><div class="line">    test_entry = [<span class="string">'stupid'</span>, <span class="string">'garbage'</span>]</div><div class="line">    this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry))</div><div class="line">    print(test_entry, <span class="string">'classified as: '</span>,</div><div class="line">          ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab))</div></pre></td></tr></table></figure><p>Test</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">32</span>]: bayes.TestNaiveBayes()</div><div class="line">[<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>] classified <span class="keyword">as</span>:  <span class="number">0</span></div><div class="line">[<span class="string">'stupid'</span>, <span class="string">'garbage'</span>] classified <span class="keyword">as</span>:  <span class="number">1</span></div></pre></td></tr></table></figure><p>Ok, bravo!</p><h2 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h2><p>现在有一个问题， 到目前为止，我们将每个词是否出现作为特征，这被称为词集模型。但是如果有一个词在文档中不止出现一次，那么就需要词袋模型进行建模。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">BagOfWords2Vec</span><span class="params">(vocab_list, input_set)</span>:</span></div><div class="line">    <span class="string">""" Transfer the words list to vector for each posting.</span></div><div class="line"></div><div class="line">    Arguments:</div><div class="line">        vocab_list: The vocabulary list.</div><div class="line">        input_set: The posting that ready to transfer to vector.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        return_vec: the result vector.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># Initialize</span></div><div class="line">    return_vec = [<span class="number">0</span>] * len(vocab_list)</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> input_set:</div><div class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocab_list:</div><div class="line">            return_vec[vocab_list.index(word)] += <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">"the word: %s is not in my Vocabulary"</span> % word)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> return_vec</div></pre></td></tr></table></figure><h1 id="高斯朴素贝叶斯"><a href="#高斯朴素贝叶斯" class="headerlink" title="高斯朴素贝叶斯"></a>高斯朴素贝叶斯</h1><p>一般的朴素贝叶斯算法的输入特征为离散值，那么当输入变量为连续值时就不能处理了，一般这时候假设输入变量服从一个正态分布，这样$p(w_j | c_i)$就可以计算了，所以整个的流程如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/nb/gnb_algo.png" alt="gnb_algo"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/nb/gnb_mle.png" alt="gnb_mle"></p><p>采用sk-learn进行下实验</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">33</span>]: <span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line">    ...: iris = datasets.load_iris()</div><div class="line">    ...: <span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line">    ...: gnb = GaussianNB()</div><div class="line">    ...: y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)</div><div class="line">    ...: print(<span class="string">"Number of mislabeled points out of a total %d points : %d"</span></div><div class="line">    ...:       % (iris.data.shape[<span class="number">0</span>],(iris.target != y_pred).sum()))</div><div class="line">    ...:</div><div class="line">Number of mislabeled points out of a total <span class="number">150</span> points : <span class="number">6</span></div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/Python/" rel="tag"># Python</a> <a href="/tags/statistics/" rel="tag"># statistics</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/02/24/CS231n-Lecture2-note/" rel="next" title="CS231n Lecture2 note"><i class="fa fa-chevron-left"></i> CS231n Lecture2 note</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/02/26/Python-data-analysis-Learning-note-Ch04/" rel="prev" title="Python data analysis-Learning note-Ch04">Python data analysis-Learning note-Ch04 <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">53</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">35</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#利用朴素贝叶斯进行文本分类"><span class="nav-number">1.</span> <span class="nav-text">利用朴素贝叶斯进行文本分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#准备数据"><span class="nav-number">1.1.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练"><span class="nav-number">1.2.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#测试"><span class="nav-number">1.3.</span> <span class="nav-text">测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#词袋模型"><span class="nav-number">1.4.</span> <span class="nav-text">词袋模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#高斯朴素贝叶斯"><span class="nav-number">2.</span> <span class="nav-text">高斯朴素贝叶斯</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/02/25/Naive-Bayes/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(t,a.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>