<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="machine learning,deep learning,"><link rel="alternate" href="/atom.xml" title="Abracdabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Loss functionSource is here.Multiclass Support Vector Machine lossThe SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by"><meta property="og:type" content="article"><meta property="og:title" content="CS231n Lecture3 note"><meta property="og:url" content="http://yoursite.com/2017/03/02/CS231n-Lecture3-note/index.html"><meta property="og:site_name" content="Abracdabra"><meta property="og:description" content="Loss functionSource is here.Multiclass Support Vector Machine lossThe SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/cs231n/03/margin.jpg"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/cs231n/03/stepsize.jpg"><meta property="og:updated_time" content="2017-03-03T11:06:37.472Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="CS231n Lecture3 note"><meta name="twitter:description" content="Loss functionSource is here.Multiclass Support Vector Machine lossThe SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/cs231n/03/margin.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/03/02/CS231n-Lecture3-note/"><title>CS231n Lecture3 note | Abracdabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracdabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/03/02/CS231n-Lecture3-note/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracdabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracdabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">CS231n Lecture3 note</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-03-02T14:33:34+08:00">2017-03-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/03/02/CS231n-Lecture3-note/#comments" itemprop="discussionUrl"><span class="post-comments-count ds-thread-count" data-thread-key="2017/03/02/CS231n-Lecture3-note/" itemprop="commentCount"></span> </a></span><span id="/2017/03/02/CS231n-Lecture3-note/" class="leancloud_visitors" data-flag-title="CS231n Lecture3 note"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h1><p>Source is <a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="external">here</a>.</p><h2 id="Multiclass-Support-Vector-Machine-loss"><a href="#Multiclass-Support-Vector-Machine-loss" class="headerlink" title="Multiclass Support Vector Machine loss"></a>Multiclass Support Vector Machine loss</h2><p>The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$</p><p>The Multiclass SVM loss for the i-th example is formalized as follows:</p><p><strong>Example</strong></p><p>$s = [13, -7, 11]$ and $\Delta = 10$, then,</p><script type="math/tex;mode=display">L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)</script><p>In summary, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by at least by $\Delta$ (delta). If this is not the case, we will accumulate loss.</p><p>Note that $f(x_i; W) = W x_i$, so we can also rewrite the loss function in this equivalent form:</p><script type="math/tex;mode=display">L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)</script><p>A last piece of terminology we’ll mention before we finish with this section is that the threshold at zero $\max(0,−)$ function is often called the <strong>hinge loss</strong>. You’ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form $\max(0,−)^2$ that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation.</p><p>The follow image shows the motivation of the SVM loss function”</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs231n/03/margin.jpg" alt="margin"></p><h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>There is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters <strong>W</strong> that correctly classify every example (i.e. all scores are so that all the margins are met, and $L_i=0$ for all i). The issue is that this set of <strong>W</strong> is not necessarily unique: there might be many similar <strong>W</strong> that correctly classify the examples. One easy way to see this is that if some parameters <strong>W</strong> correctly classify all examples (so loss is zero for each example), then any multiple of these parameters $\lambda W$ where $\lambda &gt; 1$ will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of <strong>W</strong> by 2 would make the new difference 30.</p><p>We can avoid this by extending the loss function with a <strong>regularization penalty</strong> $R(W)$. The most common regularization penalty is the <strong>L2</strong> norm:</p><script type="math/tex;mode=display">R(W) = \sum_k\sum_l W_{k,l}^2</script><p>That is, the full Multiclass SVM loss becomes:</p><script type="math/tex;mode=display">L =  \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\</script><p>Or expanding this out in its full form:</p><script type="math/tex;mode=display">L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2</script><p>Including the L2 penalty leads to the appealing <strong>max margin</strong> property in SVMs (See <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="external">CS229</a> lecture notes for full details if you are interested).</p><p>The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. For example, suppose that we have some input vector $x=[1,1,1,1]$ and two weight vectors $w_1=[1,0,0,0]$, $w_2=[0.25,0.25,0.25,0.25]$. Then $w^T_1x=w^T_2x=1$ so both weight vectors lead to the same dot product, but the <strong>L2</strong> penalty of $w_1$ is 1.0 while the <strong>L2</strong> penalty of $w_2$ is only 0.25. Therefore, according to the <strong>L2</strong> penalty the weight vector $w_2$ would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in $w_2$ are smaller and more diffuse. Since the <strong>L2</strong> penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less <em>overfitting</em>.</p><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_i</span><span class="params">(x, y, W)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  unvectorized version. Compute the multiclass svm loss for a single example (x,y)</div><div class="line">  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)</div><div class="line">    with an appended bias dimension in the 3073-rd position (i.e. bias trick)</div><div class="line">  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)</div><div class="line">  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)</div><div class="line">  """</div><div class="line">  delta = <span class="number">1.0</span> <span class="comment"># see notes about delta later in this section</span></div><div class="line">  scores = W.dot(x) <span class="comment"># scores becomes of size 10 x 1, the scores for each class</span></div><div class="line">  correct_class_score = scores[y]</div><div class="line">  D = W.shape[<span class="number">0</span>] <span class="comment"># number of classes, e.g. 10</span></div><div class="line">  loss_i = <span class="number">0.0</span></div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> xrange(D): <span class="comment"># iterate over all wrong classes</span></div><div class="line">    <span class="keyword">if</span> j == y:</div><div class="line">      <span class="comment"># skip for the true class to only loop over incorrect classes</span></div><div class="line">      <span class="keyword">continue</span></div><div class="line">    <span class="comment"># accumulate loss for the i-th example</span></div><div class="line">    loss_i += max(<span class="number">0</span>, scores[j] - correct_class_score + delta)</div><div class="line">  <span class="keyword">return</span> loss_i</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_i_vectorized</span><span class="params">(x, y, W)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  A faster half-vectorized implementation. half-vectorized</div><div class="line">  refers to the fact that for a single example the implementation contains</div><div class="line">  no for loops, but there is still one loop over the examples (outside this function)</div><div class="line">  """</div><div class="line">  delta = <span class="number">1.0</span></div><div class="line">  scores = W.dot(x)</div><div class="line">  <span class="comment"># compute the margins for all classes in one vector operation</span></div><div class="line">  margins = np.maximum(<span class="number">0</span>, scores - scores[y] + delta)</div><div class="line">  <span class="comment"># on y-th position scores[y] - scores[y] canceled and gave delta. We want</span></div><div class="line">  <span class="comment"># to ignore the y-th position and only consider margin on max wrong class</span></div><div class="line">  margins[y] = <span class="number">0</span></div><div class="line">  loss_i = np.sum(margins)</div><div class="line">  <span class="keyword">return</span> loss_i</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">L</span><span class="params">(X, y, W)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  fully-vectorized implementation :</div><div class="line">  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)</div><div class="line">  - y is array of integers specifying correct class (e.g. 50,000-D array)</div><div class="line">  - W are weights (e.g. 10 x 3073)</div><div class="line">  """</div><div class="line">  <span class="comment"># evaluate loss over all examples in X without using any for loops</span></div><div class="line">  <span class="comment"># left as exercise to reader in the assignment</span></div></pre></td></tr></table></figure><h3 id="Practice-Considerations"><a href="#Practice-Considerations" class="headerlink" title="Practice Considerations"></a>Practice Considerations</h3><h4 id="Setting-Delta"><a href="#Setting-Delta" class="headerlink" title="Setting Delta"></a>Setting Delta</h4><p>Note that we brushed over the hyperparameter $\Delta$ and its setting. What value should it be set to, and do we have to cross-validate it? It turns out that this hyperparameter can safely be set to $\Delta = 1.0$ in all cases. The hyperparameters $\Delta$ and $\lambda$ seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights <strong>W</strong> has direct effect on the scores (and hence also their differences): As we shrink all values inside <strong>W</strong> the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. $\Delta = 10$, or$\Delta = 100$) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength $\lambda$).</p><h2 id="Softmax-classifier"><a href="#Softmax-classifier" class="headerlink" title="Softmax classifier"></a>Softmax classifier</h2><p>In the Softmax classifier, the function mapping $f(x_i; W) = W x_i$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the <em>hinge loss</em> with a <strong>cross-entropy loss</strong> that has the form:</p><script type="math/tex;mode=display">L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}</script><p>The function $f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}$ is called the <strong>softmax function</strong>.</p><h3 id="Information-theory-view"><a href="#Information-theory-view" class="headerlink" title="Information theory view"></a>Information theory view</h3><p>The <em>cross-entropy</em> between a “true” distribution $p$ and an estimated distribution $q$ is defined as:</p><script type="math/tex;mode=display">H(p,q) = - \sum_x p(x) \log q(x)</script><p>The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ( $q = e^{f_{y_i}} / \sum_j e^{f_j}$ as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. $p = [0, \ldots 1, \ldots, 0]$ contains a single 1 at the $y_i$ -th position.).</p><p>Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q) = H(p) + D_{KL}(p||q)$, and the entropy of the delta function $p$ is zero, this is also equivalent to minimizing the <strong>KL</strong> divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective <em>wants</em> the predicted distribution to have all of its mass on the correct answer.</p><h3 id="Probabilistic-interpretation"><a href="#Probabilistic-interpretation" class="headerlink" title="Probabilistic interpretation"></a>Probabilistic interpretation</h3><script type="math/tex;mode=display">P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }</script><p>can be interpreted as the (normalized) probability assigned to the correct label $y_i$given the image $x_i$ and parameterized by <strong>W</strong>. To see this, remember that the Softmax classifier interprets the scores inside the output vector $f$ as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing <em>Maximum Likelihood Estimation</em> (MLE). A nice feature of this view is that we can now also interpret the regularization term $R(W)$ in the full loss function as coming from a Gaussian prior over the weight matrix <strong>W</strong>, where instead of MLE we are performing the <em>Maximum a posteriori</em> (MAP) estimation.</p><h3 id="Numeric-stability"><a href="#Numeric-stability" class="headerlink" title="Numeric stability"></a>Numeric stability</h3><p>When you’re writing code for computing the Softmax function in practice, the intermediate term $e^{f_{y_i}}$ and $\sum_j e^{f_j}$ may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant <strong>C</strong> and push it into the sum, we get the following (mathematically equivalent) expression:</p><script type="math/tex;mode=display">\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}</script><p>We are free to choose the value of <strong>C</strong>. This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for <strong>C</strong> is to set $\log C = -\max_j f_j$. This simply states that we should shift the values inside the vector ff so that the highest value is zero. In code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># example with 3 classes and each having large scores</span></div><div class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># Bad: Numeric problem, potential blowup</span></div><div class="line"></div><div class="line"><span class="comment"># instead: first shift the values of f so that the highest number is 0:</span></div><div class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></div><div class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># safe to do, gives the correct answer</span></div></pre></td></tr></table></figure><h3 id="Some-tricks"><a href="#Some-tricks" class="headerlink" title="Some tricks"></a>Some tricks</h3><p>How peaky or diffuse these probabilities are depends directly on the regularization strength $\lambda$ - which you are in charge of as input to the system. For example, suppose that the unnormalized log-probabilities for some three classes come out to be [1, -2, 0]. The softmax function would then compute:</p><script type="math/tex;mode=display">[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]</script><p>Where the steps taken are to exponentiate and normalize to sum to one. Now, if the regularization strength $\lambda$ was higher, the weights <strong>W</strong> would be penalized more and this would lead to smaller weights. For example, suppose that the weights became one half smaller ([0.5, -1, 0]). The softmax would now compute:</p><script type="math/tex;mode=display">[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]</script><p>where the probabilites are now more diffuse.</p><h3 id="Futher-Reading"><a href="#Futher-Reading" class="headerlink" title="Futher Reading"></a>Futher Reading</h3><ul><li><a href="http://arxiv.org/abs/1306.0239" target="_blank" rel="external">Deep Learning using Linear Support Vector Machines</a> from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.</li></ul><h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><h2 id="Strategy-1-A-first-very-bad-idea-solution-Random-search"><a href="#Strategy-1-A-first-very-bad-idea-solution-Random-search" class="headerlink" title="Strategy #1: A first very bad idea solution: Random search"></a>Strategy #1: A first very bad idea solution: Random search</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># assume X_train is the data where each column is an example (e.g. 3073 x 50,000)</span></div><div class="line"><span class="comment"># assume Y_train are the labels (e.g. 1D array of 50,000)</span></div><div class="line"><span class="comment"># assume the function L evaluates the loss function</span></div><div class="line"></div><div class="line">bestloss = float(<span class="string">"inf"</span>) <span class="comment"># Python assigns the highest possible float value</span></div><div class="line"><span class="keyword">for</span> num <span class="keyword">in</span> xrange(<span class="number">1000</span>):</div><div class="line">  W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.0001</span> <span class="comment"># generate random parameters</span></div><div class="line">  loss = L(X_train, Y_train, W) <span class="comment"># get the loss over the entire training set</span></div><div class="line">  <span class="keyword">if</span> loss &lt; bestloss: <span class="comment"># keep track of the best solution</span></div><div class="line">    bestloss = loss</div><div class="line">    bestW = W</div><div class="line">  <span class="keyword">print</span> <span class="string">'in attempt %d the loss was %f, best %f'</span> % (num, loss, bestloss)</div><div class="line"></div><div class="line"><span class="comment"># prints:</span></div><div class="line"><span class="comment"># in attempt 0 the loss was 9.401632, best 9.401632</span></div><div class="line"><span class="comment"># in attempt 1 the loss was 8.959668, best 8.959668</span></div><div class="line"><span class="comment"># in attempt 2 the loss was 9.044034, best 8.959668</span></div><div class="line"><span class="comment"># in attempt 3 the loss was 9.278948, best 8.959668</span></div><div class="line"><span class="comment"># in attempt 4 the loss was 8.857370, best 8.857370</span></div><div class="line"><span class="comment"># in attempt 5 the loss was 8.943151, best 8.857370</span></div><div class="line"><span class="comment"># in attempt 6 the loss was 8.605604, best 8.605604</span></div><div class="line"><span class="comment"># ... (trunctated: continues for 1000 lines)</span></div></pre></td></tr></table></figure><p>We can take the best weights <strong>W</strong> found by this search and try it out on the test set:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Assume X_test is [3073 x 10000], Y_test [10000 x 1]</span></div><div class="line">scores = Wbest.dot(Xte_cols) <span class="comment"># 10 x 10000, the class scores for all test examples</span></div><div class="line"><span class="comment"># find the index with max score in each column (the predicted class)</span></div><div class="line">Yte_predict = np.argmax(scores, axis = <span class="number">0</span>)</div><div class="line"><span class="comment"># and calculate accuracy (fraction of predictions that are correct)</span></div><div class="line">np.mean(Yte_predict == Yte)</div><div class="line"><span class="comment"># returns 0.1555</span></div></pre></td></tr></table></figure><p>With the best <strong>W</strong> this gives an accuracy of about <strong>15.5%</strong>.</p><p><strong>Core idea: iterative refinement</strong>. Of course, it turns out that we can do much better. The core idea is that finding the best set of weights <strong>W</strong> is a very difficult or even impossible problem (especially once <strong>W</strong> contains weights for entire complex neural networks), but the problem of refining a specific set of weights <strong>W</strong> to be slightly better is significantly less difficult. In other words, our approach will be to start with a random <strong>W</strong> and then iteratively refine it, making it slightly better each time.</p><blockquote><p>Our strategy will be to start with random weights and iteratively refine them over time to get lower loss</p></blockquote><h2 id="Strategy-2-Random-Local-Search"><a href="#Strategy-2-Random-Local-Search" class="headerlink" title="Strategy #2: Random Local Search"></a>Strategy #2: Random Local Search</h2><p>Concretely, we will start out with a random WW, generate random perturbations $\delta W$ to it and if the loss at the perturbed $W + \delta W$ is lower, we will perform an update. The code for this procedure is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># generate random starting W</span></div><div class="line">bestloss = float(<span class="string">"inf"</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1000</span>):</div><div class="line">  step_size = <span class="number">0.0001</span></div><div class="line">  Wtry = W + np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * step_size</div><div class="line">  loss = L(Xtr_cols, Ytr, Wtry)</div><div class="line">  <span class="keyword">if</span> loss &lt; bestloss:</div><div class="line">    W = Wtry</div><div class="line">    bestloss = loss</div><div class="line">  <span class="keyword">print</span> <span class="string">'iter %d loss is %f'</span> % (i, bestloss)</div></pre></td></tr></table></figure><p>This approach achieves test set classification accuracy of <strong>21.4%</strong>.</p><h2 id="Strategy-3-Following-the-Gradient"><a href="#Strategy-3-Following-the-Gradient" class="headerlink" title="Strategy #3: Following the Gradient"></a>Strategy #3: Following the Gradient</h2><p>The mathematical expression for the derivative of a 1-D function with respect its input is:</p><script type="math/tex;mode=display">\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}</script><p>When the functions of interest take a vector of numbers instead of a single number, we call the derivatives <strong>partial derivatives</strong>, and the gradient is simply the vector of partial derivatives in each dimension.</p><h3 id="Computing-the-gradient"><a href="#Computing-the-gradient" class="headerlink" title="Computing the gradient"></a>Computing the gradient</h3><p>There are two ways to compute the gradient: A slow, approximate but easy way (<strong>numerical gradient</strong>), and a fast, exact but more error-prone way that requires calculus (<strong>analytic gradient</strong>). We will now present both.</p><h4 id="Computing-the-gradient-numerically-with-finite-differences"><a href="#Computing-the-gradient-numerically-with-finite-differences" class="headerlink" title="Computing the gradient numerically with finite differences"></a>Computing the gradient numerically with finite differences</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_numerical_gradient</span><span class="params">(f, x)</span>:</span></div><div class="line">  <span class="string">""" </span></div><div class="line">  a naive implementation of numerical gradient of f at x </div><div class="line">  - f should be a function that takes a single argument</div><div class="line">  - x is the point (numpy array) to evaluate the gradient at</div><div class="line">  """ </div><div class="line"></div><div class="line">  fx = f(x) <span class="comment"># evaluate function value at original point</span></div><div class="line">  grad = np.zeros(x.shape)</div><div class="line">  h = <span class="number">0.00001</span></div><div class="line"></div><div class="line">  <span class="comment"># iterate over all indexes in x</span></div><div class="line">  it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</div><div class="line">  <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</div><div class="line"></div><div class="line">    <span class="comment"># evaluate function at x+h</span></div><div class="line">    ix = it.multi_index</div><div class="line">    old_value = x[ix]</div><div class="line">    x[ix] = old_value + h <span class="comment"># increment by h</span></div><div class="line">    fxh = f(x) <span class="comment"># evalute f(x + h)</span></div><div class="line">    x[ix] = old_value <span class="comment"># restore to previous value (very important!)</span></div><div class="line"></div><div class="line">    <span class="comment"># compute the partial derivative</span></div><div class="line">    grad[ix] = (fxh - fx) / h <span class="comment"># the slope</span></div><div class="line">    it.iternext() <span class="comment"># step to next dimension</span></div><div class="line"></div><div class="line">  <span class="keyword">return</span> grad</div></pre></td></tr></table></figure><p><strong>Practical considerations</strong>.</p><p>Note that in the mathematical formulation the gradient is defined in the limit as <strong>h</strong> goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the <strong>centered difference formula</strong>:</p><script type="math/tex;mode=display">[f(x+h) - f(x-h)] / 2 h</script><p>See <a href="http://en.wikipedia.org/wiki/Numerical_differentiation" target="_blank" rel="external">wiki</a> for details.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># to use the generic code above we want a function that takes a single argument</span></div><div class="line"><span class="comment"># (the weights in our case) so we close over X_train and Y_train</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">CIFAR10_loss_fun</span><span class="params">(W)</span>:</span></div><div class="line">  <span class="keyword">return</span> L(X_train, Y_train, W)</div><div class="line"></div><div class="line">W = np.random.rand(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># random weight vector</span></div><div class="line">df = eval_numerical_gradient(CIFAR10_loss_fun, W) <span class="comment"># get the gradient</span></div></pre></td></tr></table></figure><p>Then we can use to make an update:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">loss_original = CIFAR10_loss_fun(W) <span class="comment"># the original loss</span></div><div class="line"><span class="keyword">print</span> <span class="string">'original loss: %f'</span> % (loss_original, )</div><div class="line"></div><div class="line"><span class="comment"># lets see the effect of multiple step sizes</span></div><div class="line"><span class="keyword">for</span> step_size_log <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">-9</span>, <span class="number">-8</span>, <span class="number">-7</span>, <span class="number">-6</span>, <span class="number">-5</span>,<span class="number">-4</span>,<span class="number">-3</span>,<span class="number">-2</span>,<span class="number">-1</span>]:</div><div class="line">  step_size = <span class="number">10</span> ** step_size_log</div><div class="line">  W_new = W - step_size * df <span class="comment"># new position in the weight space</span></div><div class="line">  loss_new = CIFAR10_loss_fun(W_new)</div><div class="line">  <span class="keyword">print</span> <span class="string">'for step size %f new loss: %f'</span> % (step_size, loss_new)</div><div class="line"></div><div class="line"><span class="comment"># prints:</span></div><div class="line"><span class="comment"># original loss: 2.200718</span></div><div class="line"><span class="comment"># for step size 1.000000e-10 new loss: 2.200652</span></div><div class="line"><span class="comment"># for step size 1.000000e-09 new loss: 2.200057</span></div><div class="line"><span class="comment"># for step size 1.000000e-08 new loss: 2.194116</span></div><div class="line"><span class="comment"># for step size 1.000000e-07 new loss: 2.135493</span></div><div class="line"><span class="comment"># for step size 1.000000e-06 new loss: 1.647802</span></div><div class="line"><span class="comment"># for step size 1.000000e-05 new loss: 2.844355</span></div><div class="line"><span class="comment"># for step size 1.000000e-04 new loss: 25.558142</span></div><div class="line"><span class="comment"># for step size 1.000000e-03 new loss: 254.086573</span></div><div class="line"><span class="comment"># for step size 1.000000e-02 new loss: 2539.370888</span></div><div class="line"><span class="comment"># for step size 1.000000e-01 new loss: 25392.214036</span></div></pre></td></tr></table></figure><p><strong>Effect of step size</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs231n/03/stepsize.jpg" alt="margin"></p><h4 id="Computing-the-gradient-analytically-with-Calculus"><a href="#Computing-the-gradient-analytically-with-Calculus" class="headerlink" title="Computing the gradient analytically with Calculus"></a>Computing the gradient analytically with Calculus</h4><p>However, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a <strong>gradient check</strong>.</p><p>Lets use the example of the SVM loss function for a single datapoint:</p><script type="math/tex;mode=display">L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]</script><script type="math/tex;mode=display">\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) \right) x_i</script><p>when you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector $x_i$ scaled by this number is the gradient.</p><script type="math/tex;mode=display">\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) x_i</script></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/deep-learning/" rel="tag"># deep learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/02/28/Python-data-analysis-Learning-note-Ch07/" rel="next" title="Python data analysis Learning note Ch07"><i class="fa fa-chevron-left"></i> Python data analysis Learning note Ch07</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/03/02/cs231n-Assignment-1-kNN/" rel="prev" title="cs231n Assignment#1 kNN">cs231n Assignment#1 kNN <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="ds-share flat" data-thread-key="2017/03/02/CS231n-Lecture3-note/" data-title="CS231n Lecture3 note" data-content="" data-url="http://yoursite.com/2017/03/02/CS231n-Lecture3-note/"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li><li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li><li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li><li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li><li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div></div></div></div><p>热评文章</p><div class="ds-top-threads" data-range="weekly" data-num-items="4"></div><div class="comments" id="comments"><div class="ds-thread" data-thread-key="2017/03/02/CS231n-Lecture3-note/" data-title="CS231n Lecture3 note" data-url="http://yoursite.com/2017/03/02/CS231n-Lecture3-note/"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">26</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">16</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Loss-function"><span class="nav-number">1.</span> <span class="nav-text">Loss function</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Multiclass-Support-Vector-Machine-loss"><span class="nav-number">1.1.</span> <span class="nav-text">Multiclass Support Vector Machine loss</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization"><span class="nav-number">1.1.1.</span> <span class="nav-text">Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Code"><span class="nav-number">1.1.2.</span> <span class="nav-text">Code</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Practice-Considerations"><span class="nav-number">1.1.3.</span> <span class="nav-text">Practice Considerations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Setting-Delta"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Setting Delta</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Softmax-classifier"><span class="nav-number">1.2.</span> <span class="nav-text">Softmax classifier</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Information-theory-view"><span class="nav-number">1.2.1.</span> <span class="nav-text">Information theory view</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Probabilistic-interpretation"><span class="nav-number">1.2.2.</span> <span class="nav-text">Probabilistic interpretation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Numeric-stability"><span class="nav-number">1.2.3.</span> <span class="nav-text">Numeric stability</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Some-tricks"><span class="nav-number">1.2.4.</span> <span class="nav-text">Some tricks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Futher-Reading"><span class="nav-number">1.2.5.</span> <span class="nav-text">Futher Reading</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Optimization"><span class="nav-number">2.</span> <span class="nav-text">Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Strategy-1-A-first-very-bad-idea-solution-Random-search"><span class="nav-number">2.1.</span> <span class="nav-text">Strategy #1: A first very bad idea solution: Random search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Strategy-2-Random-Local-Search"><span class="nav-number">2.2.</span> <span class="nav-text">Strategy #2: Random Local Search</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Strategy-3-Following-the-Gradient"><span class="nav-number">2.3.</span> <span class="nav-text">Strategy #3: Following the Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Computing-the-gradient"><span class="nav-number">2.3.1.</span> <span class="nav-text">Computing the gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-the-gradient-numerically-with-finite-differences"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">Computing the gradient numerically with finite differences</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Computing-the-gradient-analytically-with-Calculus"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">Computing the gradient analytically with Calculus</span></a></li></ol></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">var duoshuoQuery={short_name:"ewanlee"};!function(){var t=document.createElement("script");t.type="text/javascript",t.async=!0,t.id="duoshuo-script",t.src=("https:"==document.location.protocol?"https:":"http:")+"//static.duoshuo.com/embed.js",t.charset="UTF-8",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(t)}()</script><script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script><script src="/js/src/hook-duoshuo.js"></script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>