<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="machine learning,spark,MLlib,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Example: Spam ClassificationThis program uses two MLlib algorithms: HashingTF, which builds term frequency feature vectors from text data, and LogisticRegressionWithSGD, which implements the logistic"><meta property="og:type" content="article"><meta property="og:title" content="Machine Learning with MLlib of Spark"><meta property="og:url" content="http://yoursite.com/2017/04/25/Machine-Learning-with-MLlib-of-Spark/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Example: Spam ClassificationThis program uses two MLlib algorithms: HashingTF, which builds term frequency feature vectors from text data, and LogisticRegressionWithSGD, which implements the logistic"><meta property="og:updated_time" content="2017-04-25T13:21:00.738Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Machine Learning with MLlib of Spark"><meta name="twitter:description" content="Example: Spam ClassificationThis program uses two MLlib algorithms: HashingTF, which builds term frequency feature vectors from text data, and LogisticRegressionWithSGD, which implements the logistic"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/04/25/Machine-Learning-with-MLlib-of-Spark/"><title>Machine Learning with MLlib of Spark | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/25/Machine-Learning-with-MLlib-of-Spark/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Machine Learning with MLlib of Spark</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-25T19:13:07+08:00">2017-04-25 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/25/Machine-Learning-with-MLlib-of-Spark/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/25/Machine-Learning-with-MLlib-of-Spark/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/25/Machine-Learning-with-MLlib-of-Spark/" class="leancloud_visitors" data-flag-title="Machine Learning with MLlib of Spark"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Example-Spam-Classification"><a href="#Example-Spam-Classification" class="headerlink" title="Example: Spam Classification"></a>Example: Spam Classification</h2><p>This program uses two MLlib algorithms: <code>HashingTF</code>, which builds term frequency feature vectors from text data, and LogisticRegressionWithSGD, which implements the logistic regression procedure using stochastic gradient descent (<code>SGD</code>). We assume that we start with two files, spam.txt an normal.txt, each of which contains examples of spam and non-spam emails, one per line. We then turn the text in each file into a feature vector with <code>TF</code>, and train a logistic regression model to separate the two types of messages.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></div><div class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></div><div class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></div><div class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></div><div class="line"><span class="comment"># (the "License"); you may not use this file except in compliance with</span></div><div class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></div><div class="line"><span class="comment"># distributed under the License is distributed on an "AS IS" BASIS,</span></div><div class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></div><div class="line"><span class="comment"># See the License for the specific language governing permissions and</span></div><div class="line"><span class="comment"># limitations under the License.</span></div><div class="line"><span class="comment">#</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> LogisticRegressionWithSGD</div><div class="line"><span class="keyword">from</span> pyspark.mllib.feature <span class="keyword">import</span> HashingTF</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    sc = SparkContext(appName=<span class="string">"PythonBookExample"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Load 2 types of emails from text files: spam and ham (non-spam).</span></div><div class="line">    <span class="comment"># Each line has text from one email.</span></div><div class="line">    spam = sc.textFile(<span class="string">"file:///home/hduser/learning-spark/files/spam.txt"</span>)</div><div class="line">    ham = sc.textFile(<span class="string">"file:///home/hduser/learning-spark/files/ham.txt"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Create a HashingTF instance to map email text to vectors of 100 features.</span></div><div class="line">    tf = HashingTF(numFeatures = <span class="number">100</span>)</div><div class="line">    <span class="comment"># Each email is split into words, and each word is mapped to one feature.</span></div><div class="line">    spamFeatures = spam.map(<span class="keyword">lambda</span> email: tf.transform(email.split(<span class="string">" "</span>)))</div><div class="line">    hamFeatures = ham.map(<span class="keyword">lambda</span> email: tf.transform(email.split(<span class="string">" "</span>)))</div><div class="line"></div><div class="line">    <span class="comment"># Create LabeledPoint datasets for positive (spam) and negative (ham) examples.</span></div><div class="line">    positiveExamples = spamFeatures.map(<span class="keyword">lambda</span> features: LabeledPoint(<span class="number">1</span>, features))</div><div class="line">    negativeExamples = hamFeatures.map(<span class="keyword">lambda</span> features: LabeledPoint(<span class="number">0</span>, features))</div><div class="line">    training_data = positiveExamples.union(negativeExamples)</div><div class="line">    training_data.cache() <span class="comment"># Cache data since Logistic Regression is an iterative algorithm.</span></div><div class="line"></div><div class="line">    <span class="comment"># Run Logistic Regression using the SGD optimizer.</span></div><div class="line">    <span class="comment"># regParam is model regularization, which can make models more robust.</span></div><div class="line">    model = LogisticRegressionWithSGD.train(training_data)</div><div class="line"></div><div class="line">    <span class="comment"># Test on a positive example (spam) and a negative one (ham).</span></div><div class="line">    <span class="comment"># First apply the same HashingTF feature transformation used on the training data.</span></div><div class="line">    posTestExample = tf.transform(<span class="string">"O M G GET cheap stuff by sending money to ..."</span>.split(<span class="string">" "</span>))</div><div class="line">    negTestExample = tf.transform(<span class="string">"Hi Dad, I started studying Spark the other ..."</span>.split(<span class="string">" "</span>))</div><div class="line"></div><div class="line">    <span class="comment"># Now use the learned model to predict spam/ham for new emails.</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"Prediction for positive test example: %g"</span> % model.predict(posTestExample)</div><div class="line">    <span class="keyword">print</span> <span class="string">"Prediction for negative test example: %g"</span> % model.predict(negTestExample)</div><div class="line"></div><div class="line">    sc.stop()</div></pre></td></tr></table></figure><h2 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h2><p><strong><em>Here only has some usual APIs.</em></strong></p><h3 id="Feature-Extraction"><a href="#Feature-Extraction" class="headerlink" title="Feature Extraction"></a>Feature Extraction</h3><h4 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h4><p>Most machine learning algorithms consider the magnitude of each element in the feature vector, and thus work best when the features are scaled so they weigh equally (e.g., all features have a mean of 0 and standard deviation of 1). Once you have built feature vectors, you can use the StandardScaler class in MLlib to do this scaling, both for the mean and the standard deviation. You create a StandardScaler, call fit() on a dataset to obtain a StandardScalerModel (i.e., compute the mean and variance of each column), and then call transform() on the model to scale a dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.feature <span class="keyword">import</span> StandardScaler</div><div class="line"></div><div class="line">vectors = [Vectors.dense([<span class="number">-2.0</span>, <span class="number">5.0</span>, <span class="number">1.0</span>]), Vectors.dense([<span class="number">2.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>])]</div><div class="line">dataset = sc.parallelize(vectors)</div><div class="line">scaler = StandardScaler(withMean=<span class="keyword">True</span>, withStd=<span class="keyword">True</span>)</div><div class="line">model = scaler.fit(dataset)</div><div class="line">result = model.transform(dataset)</div><div class="line"></div><div class="line"><span class="comment"># Result: &#123;[-0.7071, 0.7071, 0.0], [0.7071, -0.7071, 0.0]&#125;</span></div></pre></td></tr></table></figure><h4 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h4><p>Simply use <code>Normalizer().transform(rdd)</code>. By default Normalizer uses the L 2 norm (i.e, Euclidean length), but you can also pass a power <code>p</code>to Normalizer to use the L p norm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.feature <span class="keyword">import</span> Normalizer</div><div class="line"><span class="keyword">from</span> pyspark.mllib.util <span class="keyword">import</span> MLUtils</div><div class="line"></div><div class="line">data = MLUtils.loadLibSVMFile(sc, <span class="string">"data/mllib/sample_libsvm_data.txt"</span>)</div><div class="line">labels = data.map(<span class="keyword">lambda</span> x: x.label)</div><div class="line">features = data.map(<span class="keyword">lambda</span> x: x.features)</div><div class="line"></div><div class="line">normalizer1 = Normalizer()</div><div class="line">normalizer2 = Normalizer(p=float(<span class="string">"inf"</span>))</div><div class="line"></div><div class="line"><span class="comment"># Each sample in data1 will be normalized using $L^2$ norm.</span></div><div class="line">data1 = labels.zip(normalizer1.transform(features))</div><div class="line"></div><div class="line"><span class="comment"># Each sample in data2 will be normalized using $L^\infty$ norm.</span></div><div class="line">data2 = labels.zip(normalizer2.transform(features))</div></pre></td></tr></table></figure><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>Once you have trained the model (withWord2Vec.fit(rdd)), you will receive a Word2VecModel that can be used to transform() each word into a vector. Note that the size of the models in Word2Vec will be equal to the number of words in your vocabulary times the size of a vector (by default, 100). You may wish to filter out words that are not in a standard dictionary to limit the size. In general, a good size for the vocabulary is 100,000 words.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.feature <span class="keyword">import</span> Word2Vec</div><div class="line"></div><div class="line">inp = sc.textFile(<span class="string">"data/mllib/sample_lda_data.txt"</span>).map(<span class="keyword">lambda</span> row: row.split(<span class="string">" "</span>))</div><div class="line"></div><div class="line">word2vec = Word2Vec()</div><div class="line">model = word2vec.fit(inp)</div><div class="line"></div><div class="line">synonyms = model.findSynonyms(<span class="string">'1'</span>, <span class="number">5</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> word, cosine_distance <span class="keyword">in</span> synonyms:</div><div class="line">    print(<span class="string">"&#123;&#125;: &#123;&#125;"</span>.format(word, cosine_distance))</div></pre></td></tr></table></figure><h3 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h3><p><strong><em>Statistics.colStats(rdd)</em></strong><br>Computes a statistical summary of an RDD of vectors, which stores the min, max, mean, and variance for each column in the set of vectors. This can be used to obtain a wide variety of statistics in one pass.</p><p><strong><em>Statistics.corr(rdd, method)</em></strong><br>Computes the correlation matrix between columns in an RDD of vectors, using either the Pearson or Spearman correlation (method must be one of pearson and spearman).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.stat <span class="keyword">import</span> Statistics</div><div class="line"></div><div class="line">seriesX = sc.parallelize([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">3.0</span>, <span class="number">5.0</span>])  <span class="comment"># a series</span></div><div class="line"><span class="comment"># seriesY must have the same number of partitions and cardinality as seriesX</span></div><div class="line">seriesY = sc.parallelize([<span class="number">11.0</span>, <span class="number">22.0</span>, <span class="number">33.0</span>, <span class="number">33.0</span>, <span class="number">555.0</span>])</div><div class="line"></div><div class="line"><span class="comment"># Compute the correlation using Pearson's method. Enter "spearman" for Spearman's method.</span></div><div class="line"><span class="comment"># If a method is not specified, Pearson's method will be used by default.</span></div><div class="line">print(<span class="string">"Correlation is: "</span> + str(Statistics.corr(seriesX, seriesY, method=<span class="string">"pearson"</span>)))</div><div class="line"></div><div class="line">data = sc.parallelize(</div><div class="line">    [np.array([<span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>]), np.array([<span class="number">2.0</span>, <span class="number">20.0</span>, <span class="number">200.0</span>]), np.array([<span class="number">5.0</span>, <span class="number">33.0</span>, <span class="number">366.0</span>])]</div><div class="line">)  <span class="comment"># an RDD of Vectors</span></div><div class="line"></div><div class="line"><span class="comment"># calculate the correlation matrix using Pearson's method. Use "spearman" for Spearman's method.</span></div><div class="line"><span class="comment"># If a method is not specified, Pearson's method will be used by default.</span></div><div class="line">print(Statistics.corr(data, method=<span class="string">"pearson"</span>))</div></pre></td></tr></table></figure><p><strong><em>Statistics.corr(rdd1, rdd2, method)</em></strong><br>Computes the correlation between two RDDs of floating-point values, using either the Pearson or Spearman correlation (method must be one of pearson and spearman).</p><p><strong><em>Statistics.chiSqTest(rdd)</em></strong><br>Computes Pearson’s independence test for every feature with the label on an RDD of LabeledPoint objects. Returns an array of ChiSqTestResult objects that capture the p-value, test statistic, and degrees of freedom for each feature. Label and feature values must be categorical (i.e., discrete values).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.linalg <span class="keyword">import</span> Matrices, Vectors</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"><span class="keyword">from</span> pyspark.mllib.stat <span class="keyword">import</span> Statistics</div><div class="line"></div><div class="line">vec = Vectors.dense(<span class="number">0.1</span>, <span class="number">0.15</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.25</span>)  <span class="comment"># a vector composed of the frequencies of events</span></div><div class="line"></div><div class="line"><span class="comment"># compute the goodness of fit. If a second vector to test against</span></div><div class="line"><span class="comment"># is not supplied as a parameter, the test runs against a uniform distribution.</span></div><div class="line">goodnessOfFitTestResult = Statistics.chiSqTest(vec)</div><div class="line"></div><div class="line"><span class="comment"># summary of the test including the p-value, degrees of freedom,</span></div><div class="line"><span class="comment"># test statistic, the method used, and the null hypothesis.</span></div><div class="line">print(<span class="string">"%s\n"</span> % goodnessOfFitTestResult)</div><div class="line"></div><div class="line">mat = Matrices.dense(<span class="number">3</span>, <span class="number">2</span>, [<span class="number">1.0</span>, <span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>])  <span class="comment"># a contingency matrix</span></div><div class="line"></div><div class="line"><span class="comment"># conduct Pearson's independence test on the input contingency matrix</span></div><div class="line">independenceTestResult = Statistics.chiSqTest(mat)</div><div class="line"></div><div class="line"><span class="comment"># summary of the test including the p-value, degrees of freedom,</span></div><div class="line"><span class="comment"># test statistic, the method used, and the null hypothesis.</span></div><div class="line">print(<span class="string">"%s\n"</span> % independenceTestResult)</div><div class="line"></div><div class="line">obs = sc.parallelize(</div><div class="line">    [LabeledPoint(<span class="number">1.0</span>, [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>]),</div><div class="line">     LabeledPoint(<span class="number">1.0</span>, [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">0.0</span>]),</div><div class="line">     LabeledPoint(<span class="number">1.0</span>, [<span class="number">-1.0</span>, <span class="number">0.0</span>, <span class="number">-0.5</span>])]</div><div class="line">)  <span class="comment"># LabeledPoint(feature, label)</span></div><div class="line"></div><div class="line"><span class="comment"># The contingency table is constructed from an RDD of LabeledPoint and used to conduct</span></div><div class="line"><span class="comment"># the independence test. Returns an array containing the ChiSquaredTestResult for every feature</span></div><div class="line"><span class="comment"># against the label.</span></div><div class="line">featureTestResults = Statistics.chiSqTest(obs)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i, result <span class="keyword">in</span> enumerate(featureTestResults):</div><div class="line">    print(<span class="string">"Column %d:\n%s"</span> % (i + <span class="number">1</span>, result))</div></pre></td></tr></table></figure><h3 id="Classification-and-Regression"><a href="#Classification-and-Regression" class="headerlink" title="Classification and Regression"></a>Classification and Regression</h3><p>MLlib includes a variety of methods for classification and regression, including simple linear methods and decision trees and forests.</p><h4 id="Linear-regression"><a href="#Linear-regression" class="headerlink" title="Linear regression"></a>Linear regression</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LinearRegressionWithSGD</div><div class="line"></div><div class="line">points = <span class="comment"># (create RDD of LabeledPoint)</span></div><div class="line">model = LinearRegressionWithSGD.train(points, iterations=<span class="number">200</span>, intercept=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">"weights: %s, intercept: %s"</span> % (model.weights, model.intercept)</div></pre></td></tr></table></figure><h4 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h4><p>The logistic regression algorithm has a very similar API to linear regression, covered in the previous section. One difference is that there are two algorithms available for solving it: <code>SGD</code> and <code>LBFGS</code>. <code>LBFGS</code> is generally the best choice, but is not available in some earlier versions of <code>MLlib</code> (before <code>Spark 1.2</code>). These algorithms are available in the <code>mllib.classification.LogisticRegressionWithLBFGS</code> and <code>WithSGD</code> classes, which have interfaces similar to <code>LinearRegressionWithSGD</code>. They take all the same parameters as linear regression.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> LogisticRegressionWithLBFGS, LogisticRegressionModel</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsePoint</span><span class="params">(line)</span>:</span></div><div class="line">    values = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">' '</span>)]</div><div class="line">    <span class="keyword">return</span> LabeledPoint(values[<span class="number">0</span>], values[<span class="number">1</span>:])</div><div class="line"></div><div class="line">data = sc.textFile(<span class="string">"data/mllib/sample_svm_data.txt"</span>)</div><div class="line">parsedData = data.map(parsePoint)</div><div class="line"></div><div class="line"><span class="comment"># Build the model</span></div><div class="line">model = LogisticRegressionWithLBFGS.train(parsedData)</div><div class="line"></div><div class="line"><span class="comment"># Evaluating the model on training data</span></div><div class="line">labelsAndPreds = parsedData.map(<span class="keyword">lambda</span> p: (p.label, model.predict(p.features)))</div><div class="line">trainErr = labelsAndPreds.filter(<span class="keyword">lambda</span> (v, p): v != p).count() / float(parsedData.count())</div><div class="line">print(<span class="string">"Training Error = "</span> + str(trainErr))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">model.save(sc, <span class="string">"target/tmp/pythonLogisticRegressionWithLBFGSModel"</span>)</div><div class="line">sameModel = LogisticRegressionModel.load(sc,</div><div class="line">                                         <span class="string">"target/tmp/pythonLogisticRegressionWithLBFGSModel"</span>)</div></pre></td></tr></table></figure><h4 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h4><p>They are available through the <code>SVMWithSGD</code> class, with similar parameters to linear and logisitic regression. The returned <code>SVMModel</code> uses a threshold for prediction like <code>LogisticRegressionModel</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> SVMWithSGD, SVMModel</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsePoint</span><span class="params">(line)</span>:</span></div><div class="line">    values = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">' '</span>)]</div><div class="line">    <span class="keyword">return</span> LabeledPoint(values[<span class="number">0</span>], values[<span class="number">1</span>:])</div><div class="line"></div><div class="line">data = sc.textFile(<span class="string">"data/mllib/sample_svm_data.txt"</span>)</div><div class="line">parsedData = data.map(parsePoint)</div><div class="line"></div><div class="line"><span class="comment"># Build the model</span></div><div class="line">model = SVMWithSGD.train(parsedData, iterations=<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># Evaluating the model on training data</span></div><div class="line">labelsAndPreds = parsedData.map(<span class="keyword">lambda</span> p: (p.label, model.predict(p.features)))</div><div class="line">trainErr = labelsAndPreds.filter(<span class="keyword">lambda</span> (v, p): v != p).count() / float(parsedData.count())</div><div class="line">print(<span class="string">"Training Error = "</span> + str(trainErr))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">model.save(sc, <span class="string">"target/tmp/pythonSVMWithSGDModel"</span>)</div><div class="line">sameModel = SVMModel.load(sc, <span class="string">"target/tmp/pythonSVMWithSGDModel"</span>)</div></pre></td></tr></table></figure><h4 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h4><p>In <code>MLlib</code>, you can use <code>Naive Bayes</code> through the<code>mllib.classification.NaiveBayes</code> class. It supports one parameter, <code>lambda</code> (or <code>lambda_</code> in Python), used for smoothing. You can call it on an <code>RDD</code> of <code>LabeledPoints</code>, where the labels are between 0 and C–1 for C classes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> NaiveBayes, NaiveBayesModel</div><div class="line"><span class="keyword">from</span> pyspark.mllib.util <span class="keyword">import</span> MLUtils</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data file.</span></div><div class="line">data = MLUtils.loadLibSVMFile(sc, <span class="string">"data/mllib/sample_libsvm_data.txt"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Split data approximately into training (60%) and test (40%)</span></div><div class="line">training, test = data.randomSplit([<span class="number">0.6</span>, <span class="number">0.4</span>])</div><div class="line"></div><div class="line"><span class="comment"># Train a naive Bayes model.</span></div><div class="line">model = NaiveBayes.train(training, <span class="number">1.0</span>)</div><div class="line"></div><div class="line"><span class="comment"># Make prediction and test accuracy.</span></div><div class="line">predictionAndLabel = test.map(<span class="keyword">lambda</span> p: (model.predict(p.features), p.label))</div><div class="line">accuracy = <span class="number">1.0</span> * predictionAndLabel.filter(<span class="keyword">lambda</span> (x, v): x == v).count() / test.count()</div><div class="line">print(<span class="string">'model accuracy &#123;&#125;'</span>.format(accuracy))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">output_dir = <span class="string">'target/tmp/myNaiveBayesModel'</span></div><div class="line">shutil.rmtree(output_dir, ignore_errors=<span class="keyword">True</span>)</div><div class="line">model.save(sc, output_dir)</div><div class="line">sameModel = NaiveBayesModel.load(sc, output_dir)</div><div class="line">predictionAndLabel = test.map(<span class="keyword">lambda</span> p: (sameModel.predict(p.features), p.label))</div><div class="line">accuracy = <span class="number">1.0</span> * predictionAndLabel.filter(<span class="keyword">lambda</span> (x, v): x == v).count() / test.count()</div><div class="line">print(<span class="string">'sameModel accuracy &#123;&#125;'</span>.format(accuracy))</div></pre></td></tr></table></figure><h4 id="Decision-trees-and-random-forests"><a href="#Decision-trees-and-random-forests" class="headerlink" title="Decision trees and random forests"></a>Decision trees and random forests</h4><p>In <code>MLlib</code>, you can train trees using the <code>mllib.tree.DecisionTree</code> class, through the static methods <code>trainClassifier()</code> and <code>trainRegressor()</code>. Unlike in some of the other algorithms, the Java and Scala APIs also use static methods instead of a <code>DecisionTree</code> object with setters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.tree <span class="keyword">import</span> DecisionTree, DecisionTreeModel</div><div class="line"><span class="keyword">from</span> pyspark.mllib.util <span class="keyword">import</span> MLUtils</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data file into an RDD of LabeledPoint.</span></div><div class="line">data = MLUtils.loadLibSVMFile(sc, <span class="string">'data/mllib/sample_libsvm_data.txt'</span>)</div><div class="line"><span class="comment"># Split the data into training and test sets (30% held out for testing)</span></div><div class="line">(trainingData, testData) = data.randomSplit([<span class="number">0.7</span>, <span class="number">0.3</span>])</div><div class="line"></div><div class="line"><span class="comment"># Train a DecisionTree model.</span></div><div class="line"><span class="comment">#  Empty categoricalFeaturesInfo indicates all features are continuous.</span></div><div class="line">model = DecisionTree.trainClassifier(trainingData, numClasses=<span class="number">2</span>, categoricalFeaturesInfo=&#123;&#125;,</div><div class="line">                                     impurity=<span class="string">'gini'</span>, maxDepth=<span class="number">5</span>, maxBins=<span class="number">32</span>)</div><div class="line"></div><div class="line"><span class="comment"># Evaluate model on test instances and compute test error</span></div><div class="line">predictions = model.predict(testData.map(<span class="keyword">lambda</span> x: x.features))</div><div class="line">labelsAndPredictions = testData.map(<span class="keyword">lambda</span> lp: lp.label).zip(predictions)</div><div class="line">testErr = labelsAndPredictions.filter(<span class="keyword">lambda</span> (v, p): v != p).count() / float(testData.count())</div><div class="line">print(<span class="string">'Test Error = '</span> + str(testErr))</div><div class="line">print(<span class="string">'Learned classification tree model:'</span>)</div><div class="line">print(model.toDebugString())</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">model.save(sc, <span class="string">"target/tmp/myDecisionTreeClassificationModel"</span>)</div><div class="line">sameModel = DecisionTreeModel.load(sc, <span class="string">"target/tmp/myDecisionTreeClassificationModel"</span>)</div></pre></td></tr></table></figure><h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><h4 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> array</div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</div><div class="line"></div><div class="line"><span class="keyword">from</span> pyspark.mllib.clustering <span class="keyword">import</span> KMeans, KMeansModel</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data</span></div><div class="line">data = sc.textFile(<span class="string">"data/mllib/kmeans_data.txt"</span>)</div><div class="line">parsedData = data.map(<span class="keyword">lambda</span> line: array([float(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">' '</span>)]))</div><div class="line"></div><div class="line"><span class="comment"># Build the model (cluster the data)</span></div><div class="line">clusters = KMeans.train(parsedData, <span class="number">2</span>, maxIterations=<span class="number">10</span>, initializationMode=<span class="string">"random"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Evaluate clustering by computing Within Set Sum of Squared Errors</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(point)</span>:</span></div><div class="line">    center = clusters.centers[clusters.predict(point)]</div><div class="line">    <span class="keyword">return</span> sqrt(sum([x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> (point - center)]))</div><div class="line"></div><div class="line">WSSSE = parsedData.map(<span class="keyword">lambda</span> point: error(point)).reduce(<span class="keyword">lambda</span> x, y: x + y)</div><div class="line">print(<span class="string">"Within Set Sum of Squared Error = "</span> + str(WSSSE))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">clusters.save(sc, <span class="string">"target/org/apache/spark/PythonKMeansExample/KMeansModel"</span>)</div><div class="line">sameModel = KMeansModel.load(sc, <span class="string">"target/org/apache/spark/PythonKMeansExample/KMeansModel"</span>)</div></pre></td></tr></table></figure><h3 id="Collaborative-Filtering-and-Recommendation"><a href="#Collaborative-Filtering-and-Recommendation" class="headerlink" title="Collaborative Filtering and Recommendation"></a>Collaborative Filtering and Recommendation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS, MatrixFactorizationModel, Rating</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data</span></div><div class="line">data = sc.textFile(<span class="string">"data/mllib/als/test.data"</span>)</div><div class="line">ratings = data.map(<span class="keyword">lambda</span> l: l.split(<span class="string">','</span>))\</div><div class="line">    .map(<span class="keyword">lambda</span> l: Rating(int(l[<span class="number">0</span>]), int(l[<span class="number">1</span>]), float(l[<span class="number">2</span>])))</div><div class="line"></div><div class="line"><span class="comment"># Build the recommendation model using Alternating Least Squares</span></div><div class="line">rank = <span class="number">10</span></div><div class="line">numIterations = <span class="number">10</span></div><div class="line">model = ALS.train(ratings, rank, numIterations)</div><div class="line"></div><div class="line"><span class="comment"># Evaluate the model on training data</span></div><div class="line">testdata = ratings.map(<span class="keyword">lambda</span> p: (p[<span class="number">0</span>], p[<span class="number">1</span>]))</div><div class="line">predictions = model.predictAll(testdata).map(<span class="keyword">lambda</span> r: ((r[<span class="number">0</span>], r[<span class="number">1</span>]), r[<span class="number">2</span>]))</div><div class="line">ratesAndPreds = ratings.map(<span class="keyword">lambda</span> r: ((r[<span class="number">0</span>], r[<span class="number">1</span>]), r[<span class="number">2</span>])).join(predictions)</div><div class="line">MSE = ratesAndPreds.map(<span class="keyword">lambda</span> r: (r[<span class="number">1</span>][<span class="number">0</span>] - r[<span class="number">1</span>][<span class="number">1</span>])**<span class="number">2</span>).mean()</div><div class="line">print(<span class="string">"Mean Squared Error = "</span> + str(MSE))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">model.save(sc, <span class="string">"target/tmp/myCollaborativeFilter"</span>)</div><div class="line">sameModel = MatrixFactorizationModel.load(sc, <span class="string">"target/tmp/myCollaborativeFilter"</span>)</div></pre></td></tr></table></figure><p>The <a href="https://databricks-training.s3.amazonaws.com/index.html" target="_blank" rel="external">training exercises</a> from the Spark Summit 2014 include a hands-on tutorial for <a href="https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html" target="_blank" rel="external">personalized movie recommendation with <code>spark.mllib</code></a>.</p><h3 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h3><h4 id="Principal-component-analysis"><a href="#Principal-component-analysis" class="headerlink" title="Principal component analysis"></a>Principal component analysis</h4><p>PCA in Scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Matrix</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.distributed.<span class="type">RowMatrix</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> points: <span class="type">RDD</span>[<span class="type">Vector</span>] = <span class="comment">// ...</span></div><div class="line"><span class="keyword">val</span> mat: <span class="type">RowMatrix</span> = <span class="keyword">new</span> <span class="type">RowMatrix</span>(points)</div><div class="line"><span class="keyword">val</span> pc: <span class="type">Matrix</span> = mat.computePrincipalComponents(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment">// Project points to low-dimensional space</span></div><div class="line"><span class="keyword">val</span> projected = mat.multiply(pc).rows</div><div class="line"></div><div class="line"><span class="comment">// Train a k-means model on the projected 2-dimensional data</span></div><div class="line"><span class="keyword">val</span> model = <span class="type">KMeans</span>.train(projected, <span class="number">10</span>)</div></pre></td></tr></table></figure><h4 id="Singular-value-decomposition"><a href="#Singular-value-decomposition" class="headerlink" title="Singular value decomposition"></a>Singular value decomposition</h4><p>SVD in Scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Compute the top 20 singular values of a RowMatrix mat and their singular vectors.</span></div><div class="line"><span class="keyword">val</span> svd: <span class="type">SingularValueDecomposition</span>[<span class="type">RowMatrix</span>, <span class="type">Matrix</span>] =</div><div class="line"> mat.computeSVD(<span class="number">20</span>, computeU=<span class="literal">true</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> <span class="type">U</span>: <span class="type">RowMatrix</span> = svd.<span class="type">U</span> <span class="comment">// U is a distributed RowMatrix.</span></div><div class="line"><span class="keyword">val</span> s: <span class="type">Vector</span> = svd.s <span class="comment">// Singular values are a local dense vector.</span></div><div class="line"><span class="keyword">val</span> <span class="type">V</span>: <span class="type">Matrix</span> = svd.<span class="type">V</span> <span class="comment">// V is a local dense matrix.</span></div></pre></td></tr></table></figure><h3 id="Pipeline-API"><a href="#Pipeline-API" class="headerlink" title="Pipeline API"></a>Pipeline API</h3><p>Pipeline API version of spam classification in Scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.<span class="type">Pipeline</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.<span class="type">LogisticRegression</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.&#123;<span class="type">HashingTF</span>, <span class="type">Tokenizer</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.tuning.&#123;<span class="type">CrossValidator</span>, <span class="type">ParamGridBuilder</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">BinaryClassificationEvaluator</span></div><div class="line"></div><div class="line"><span class="comment">// A class to represent documents -- will be turned into a SchemaRDD</span></div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">LabeledDocument</span>(<span class="params">id: <span class="type">Long</span>, text: <span class="type">String</span>, label: <span class="type">Double</span></span>)</span></div><div class="line"><span class="keyword">val</span> documents = <span class="comment">// (load RDD of LabeledDocument)</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</div><div class="line"><span class="keyword">import</span> sqlContext._</div><div class="line"></div><div class="line"><span class="comment">// Configure an ML pipeline with three stages: tokenizer, tf, and lr; each stage</span></div><div class="line"><span class="comment">// outputs a column in a SchemaRDD and feeds it to the next stage's input column</span></div><div class="line"><span class="keyword">val</span> tokenizer = <span class="keyword">new</span> <span class="type">Tokenizer</span>() <span class="comment">// Splits each email into words</span></div><div class="line"> .setInputCol(<span class="string">"text"</span>)</div><div class="line"> .setOutputCol(<span class="string">"words"</span>)</div><div class="line"><span class="keyword">val</span> tf = <span class="keyword">new</span> <span class="type">HashingTF</span>() <span class="comment">// Maps email words to vectors of 10000 features</span></div><div class="line"> .setNumFeatures(<span class="number">10000</span>)</div><div class="line"> .setInputCol(tokenizer.getOutputCol)</div><div class="line"> .setOutputCol(<span class="string">"features"</span>)</div><div class="line"><span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>() <span class="comment">// Uses "features" as inputCol by default</span></div><div class="line"><span class="keyword">val</span> pipeline = <span class="keyword">new</span> <span class="type">Pipeline</span>().setStages(<span class="type">Array</span>(tokenizer, tf, lr))</div><div class="line"></div><div class="line"><span class="comment">// Fit the pipeline to the training documents</span></div><div class="line"><span class="keyword">val</span> model = pipeline.fit(documents)</div><div class="line"></div><div class="line"><span class="comment">// Alternatively, instead of fitting once with the parameters above, we can do a</span></div><div class="line"><span class="comment">// grid search over some parameters and pick the best model via cross-validation</span></div><div class="line"><span class="keyword">val</span> paramMaps = <span class="keyword">new</span> <span class="type">ParamGridBuilder</span>()</div><div class="line"> .addGrid(tf.numFeatures, <span class="type">Array</span>(<span class="number">10000</span>, <span class="number">20000</span>))</div><div class="line"> .addGrid(lr.maxIter, <span class="type">Array</span>(<span class="number">100</span>, <span class="number">200</span>))</div><div class="line"> .build() <span class="comment">// Builds all combinations of parameters</span></div><div class="line"><span class="keyword">val</span> eval = <span class="keyword">new</span> <span class="type">BinaryClassificationEvaluator</span>()</div><div class="line"><span class="keyword">val</span> cv = <span class="keyword">new</span> <span class="type">CrossValidator</span>()</div><div class="line"> .setEstimator(lr)</div><div class="line"> .setEstimatorParamMaps(paramMaps)</div><div class="line"> .setEvaluator(eval)</div><div class="line"><span class="keyword">val</span> bestModel = cv.fit(documents)</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/spark/" rel="tag"># spark</a> <a href="/tags/MLlib/" rel="tag"># MLlib</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/04/23/Solution-for-Bracket-in-markdown-link-address/" rel="next" title="Solution for Bracket in markdown link address"><i class="fa fa-chevron-left"></i> Solution for Bracket in markdown link address</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/04/26/Movie-Recommendation-with-MLlib/" rel="prev" title="Movie Recommendation with MLlib">Movie Recommendation with MLlib <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">73</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">44</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Example-Spam-Classification"><span class="nav-number">1.</span> <span class="nav-text">Example: Spam Classification</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Algorithms"><span class="nav-number">2.</span> <span class="nav-text">Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-Extraction"><span class="nav-number">2.1.</span> <span class="nav-text">Feature Extraction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Scaling"><span class="nav-number">2.1.1.</span> <span class="nav-text">Scaling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Normalization"><span class="nav-number">2.1.2.</span> <span class="nav-text">Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Word2Vec"><span class="nav-number">2.1.3.</span> <span class="nav-text">Word2Vec</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Statistics"><span class="nav-number">2.2.</span> <span class="nav-text">Statistics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classification-and-Regression"><span class="nav-number">2.3.</span> <span class="nav-text">Classification and Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-regression"><span class="nav-number">2.3.1.</span> <span class="nav-text">Linear regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic-regression"><span class="nav-number">2.3.2.</span> <span class="nav-text">Logistic regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Support-Vector-Machines"><span class="nav-number">2.3.3.</span> <span class="nav-text">Support Vector Machines</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Naive-Bayes"><span class="nav-number">2.3.4.</span> <span class="nav-text">Naive Bayes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Decision-trees-and-random-forests"><span class="nav-number">2.3.5.</span> <span class="nav-text">Decision trees and random forests</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Clustering"><span class="nav-number">2.4.</span> <span class="nav-text">Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#K-means"><span class="nav-number">2.4.1.</span> <span class="nav-text">K-means</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Collaborative-Filtering-and-Recommendation"><span class="nav-number">2.5.</span> <span class="nav-text">Collaborative Filtering and Recommendation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dimensionality-Reduction"><span class="nav-number">2.6.</span> <span class="nav-text">Dimensionality Reduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Principal-component-analysis"><span class="nav-number">2.6.1.</span> <span class="nav-text">Principal component analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Singular-value-decomposition"><span class="nav-number">2.6.2.</span> <span class="nav-text">Singular value decomposition</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pipeline-API"><span class="nav-number">2.7.</span> <span class="nav-text">Pipeline API</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/04/25/Machine-Learning-with-MLlib-of-Spark/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(t,a.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>