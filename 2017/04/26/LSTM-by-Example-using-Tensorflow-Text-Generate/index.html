<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="machine learning,deep learning,LSTM,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="In Deep Learning, Recurrent Neural Networks (RNN) are a family of neural networks that excels in learning from sequential data. A class of RNN that has found practical applications is Long Short-Term"><meta property="og:type" content="article"><meta property="og:title" content="LSTM by Example using Tensorflow (Text Generate)"><meta property="og:url" content="http://yoursite.com/2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="In Deep Learning, Recurrent Neural Networks (RNN) are a family of neural networks that excels in learning from sequential data. A class of RNN that has found practical applications is Long Short-Term"><meta property="og:image" content="https://cdn-images-1.medium.com/max/800/1*epcf2SBjRHBynBNFf-CpQA.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/800/1*XAJdt_EbedqDlrTT9eqWvQ.png"><meta property="og:updated_time" content="2017-04-26T13:10:20.095Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="LSTM by Example using Tensorflow (Text Generate)"><meta name="twitter:description" content="In Deep Learning, Recurrent Neural Networks (RNN) are a family of neural networks that excels in learning from sequential data. A class of RNN that has found practical applications is Long Short-Term"><meta name="twitter:image" content="https://cdn-images-1.medium.com/max/800/1*epcf2SBjRHBynBNFf-CpQA.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/"><title>LSTM by Example using Tensorflow (Text Generate) | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">LSTM by Example using Tensorflow (Text Generate)</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-26T21:05:01+08:00">2017-04-26 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/" class="leancloud_visitors" data-flag-title="LSTM by Example using Tensorflow (Text Generate)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In Deep Learning, Recurrent Neural Networks (RNN) are a family of neural networks that excels in learning from sequential data. A class of RNN that has found practical applications is Long Short-Term Memory (LSTM) because it is robust against the problems of long-term dependency.</p><p>What seems to be lacking is a good documentation and example on how to build an easy to understand Tensorflow application based on LSTM. This is the motivation behind this article.</p><p>Suppose we want to train a LSTM to predict the next word using a sample short story, <a href="http://www.taleswithmorals.com/" target="_blank" rel="external">Aesop’s Fables</a>:</p><blockquote><p>long ago , the mice had a general council to consider what measures they could take to outwit their common enemy , the cat . some said this , and some said that but at last a young mouse got up and said he had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in the sly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easily escape from her . i venture , therefore , to propose that a small bell be procured , and attached by a ribbon round the neck of the cat . by this means we should always know when she was about , and could easily retire while she was in the neighbourhood . this proposal met with general applause , until an old mouse got up and said that is all very well , but who is to bell the cat ? the mice looked at one another and nobody spoke . then the old mouse said it is easy to propose impossible remedies .</p></blockquote><p>If we feed a LSTM with correct sequences from the text of 3 symbols as inputs and 1 labeled symbol, eventually the neural network will learn to predict the next symbol correctly.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*epcf2SBjRHBynBNFf-CpQA.png" alt="lstm"></p><p>Technically, LSTM inputs can only understand real numbers. A way to convert symbol to number is to assign a unique integer to each symbol based on frequency of occurrence. For example, there are 112 unique symbols in the text above. The function in Listing 2 builds a dictionary with the following entries [ “,” : 0 ][ “the” : 1 ], …, [ “council” : 37 ],…,[ “spoke” : 111 ]. The reverse dictionary is also generated since it will be used in decoding the output of LSTM.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words)</span>:</span></div><div class="line">    count = collections.Counter(words).most_common()</div><div class="line">    dictionary = dict()</div><div class="line">    <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</div><div class="line">        dictionary[word] = len(dictionary)</div><div class="line">    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</div><div class="line">    <span class="keyword">return</span> dictionary, reverse_dictionary</div></pre></td></tr></table></figure><p>Similarly, the prediction is a unique integer identifying the index in the reverse dictionary of the predicted symbol. For example, if the prediction is 37, the predicted symbol is actually “council”.</p><p>The generation of output may sound simple but actually LSTM produces a 112-element vector of probabilities of prediction for the next symbol normalized by the softmax() function. The index of the element with the highest probability is the predicted index of the symbol in the reverse dictionary (ie a one-hot vector).</p><p><img src="https://cdn-images-1.medium.com/max/800/1*XAJdt_EbedqDlrTT9eqWvQ.png" alt="word-gen"></p><p>There is the source code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''</span></div><div class="line">A Recurrent Neural Network (LSTM) implementation example using TensorFlow..</div><div class="line">Next word prediction after n_input words learned from text file.</div><div class="line">A story is automatically generated if the predicted word is fed back as input.</div><div class="line">Author: Rowel Atienza</div><div class="line">Project: https://github.com/roatienza/Deep-Learning-Experiments</div><div class="line">'''</div><div class="line"></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> collections</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">start_time = time.time()</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">elapsed</span><span class="params">(sec)</span>:</span></div><div class="line">    <span class="keyword">if</span> sec&lt;<span class="number">60</span>:</div><div class="line">        <span class="keyword">return</span> str(sec) + <span class="string">" sec"</span></div><div class="line">    <span class="keyword">elif</span> sec&lt;(<span class="number">60</span>*<span class="number">60</span>):</div><div class="line">        <span class="keyword">return</span> str(sec/<span class="number">60</span>) + <span class="string">" min"</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> str(sec/(<span class="number">60</span>*<span class="number">60</span>)) + <span class="string">" hr"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Target log path</span></div><div class="line">logs_path = <span class="string">'/tmp/tensorflow/rnn_words'</span></div><div class="line">writer = tf.summary.FileWriter(logs_path)</div><div class="line"></div><div class="line"><span class="comment"># Text file containing words for training</span></div><div class="line">training_file = <span class="string">'belling_the_cat.txt'</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(fname)</span>:</span></div><div class="line">    <span class="keyword">with</span> open(fname) <span class="keyword">as</span> f:</div><div class="line">        content = f.readlines()</div><div class="line">    content = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> content]</div><div class="line">    content = [content[i].split() <span class="keyword">for</span> i <span class="keyword">in</span> range(len(content))]</div><div class="line">    content = np.array(content)</div><div class="line">    content = np.reshape(content, [<span class="number">-1</span>, ])</div><div class="line">    <span class="keyword">return</span> content</div><div class="line"></div><div class="line">training_data = read_data(training_file)</div><div class="line">print(<span class="string">"Loaded training data..."</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words)</span>:</span></div><div class="line">    count = collections.Counter(words).most_common()</div><div class="line">    dictionary = dict()</div><div class="line">    <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</div><div class="line">        dictionary[word] = len(dictionary)</div><div class="line">    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</div><div class="line">    <span class="keyword">return</span> dictionary, reverse_dictionary</div><div class="line"></div><div class="line">dictionary, reverse_dictionary = build_dataset(training_data)</div><div class="line">vocab_size = len(dictionary)</div><div class="line"></div><div class="line"><span class="comment"># Parameters</span></div><div class="line">learning_rate = <span class="number">0.001</span></div><div class="line">training_iters = <span class="number">50000</span></div><div class="line">display_step = <span class="number">1000</span></div><div class="line">n_input = <span class="number">3</span></div><div class="line"></div><div class="line"><span class="comment"># number of units in RNN cell</span></div><div class="line">n_hidden = <span class="number">512</span></div><div class="line"></div><div class="line"><span class="comment"># tf Graph input</span></div><div class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_input, <span class="number">1</span>])</div><div class="line">y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, vocab_size])</div><div class="line"></div><div class="line"><span class="comment"># RNN output node weights and biases</span></div><div class="line">weights = &#123;</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden, vocab_size]))</div><div class="line">&#125;</div><div class="line">biases = &#123;</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([vocab_size]))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">RNN</span><span class="params">(x, weights, biases)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># reshape to [1, n_input]</span></div><div class="line">    x = tf.reshape(x, [<span class="number">-1</span>, n_input])</div><div class="line"></div><div class="line">    <span class="comment"># Generate a n_input-element sequence of inputs</span></div><div class="line">    <span class="comment"># (eg. [had] [a] [general] -&gt; [20] [6] [33])</span></div><div class="line">    x = tf.split(x,n_input,<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 2-layer LSTM, each layer has n_hidden units.</span></div><div class="line">    <span class="comment"># Average Accuracy= 95.20% at 50k iter</span></div><div class="line">    </div><div class="line">    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])</div><div class="line"></div><div class="line">    <span class="comment"># 1-layer LSTM with n_hidden units but with lower accuracy.</span></div><div class="line">    <span class="comment"># Average Accuracy= 90.60% 50k iter</span></div><div class="line">    <span class="comment"># Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above</span></div><div class="line">    <span class="comment"># rnn_cell = rnn.BasicLSTMCell(n_hidden)</span></div><div class="line"></div><div class="line">    <span class="comment"># generate prediction</span></div><div class="line">    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)</div><div class="line"></div><div class="line">    <span class="comment"># there are n_input outputs but</span></div><div class="line">    <span class="comment"># we only want the last output</span></div><div class="line">    <span class="keyword">return</span> tf.matmul(outputs[<span class="number">-1</span>], weights[<span class="string">'out'</span>]) + biases[<span class="string">'out'</span>]</div><div class="line"></div><div class="line">pred = RNN(x, weights, biases)</div><div class="line"></div><div class="line"><span class="comment"># Loss and optimizer</span></div><div class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))</div><div class="line">optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)</div><div class="line"></div><div class="line"><span class="comment"># Model evaluation</span></div><div class="line">correct_pred = tf.equal(tf.argmax(pred,<span class="number">1</span>), tf.argmax(y,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</div><div class="line"></div><div class="line"><span class="comment"># Initializing the variables</span></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Launch the graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</div><div class="line">    session.run(init)</div><div class="line">    step = <span class="number">0</span></div><div class="line">    offset = random.randint(<span class="number">0</span>,n_input+<span class="number">1</span>)</div><div class="line">    end_offset = n_input + <span class="number">1</span></div><div class="line">    acc_total = <span class="number">0</span></div><div class="line">    loss_total = <span class="number">0</span></div><div class="line"></div><div class="line">    writer.add_graph(session.graph)</div><div class="line"></div><div class="line">    <span class="keyword">while</span> step &lt; training_iters:</div><div class="line">        <span class="comment"># Generate a minibatch. Add some randomness on selection process.</span></div><div class="line">        <span class="keyword">if</span> offset &gt; (len(training_data)-end_offset):</div><div class="line">            offset = random.randint(<span class="number">0</span>, n_input+<span class="number">1</span>)</div><div class="line"></div><div class="line">        symbols_in_keys = [ [dictionary[ str(training_data[i])]] <span class="keyword">for</span> i <span class="keyword">in</span> range(offset, offset+n_input) ]</div><div class="line">        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [<span class="number">-1</span>, n_input, <span class="number">1</span>])</div><div class="line"></div><div class="line">        symbols_out_onehot = np.zeros([vocab_size], dtype=float)</div><div class="line">        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = <span class="number">1.0</span></div><div class="line">        symbols_out_onehot = np.reshape(symbols_out_onehot,[<span class="number">1</span>,<span class="number">-1</span>])</div><div class="line"></div><div class="line">        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \</div><div class="line">                                                feed_dict=&#123;x: symbols_in_keys, y: symbols_out_onehot&#125;)</div><div class="line">        loss_total += loss</div><div class="line">        acc_total += acc</div><div class="line">        <span class="keyword">if</span> (step+<span class="number">1</span>) % display_step == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"Iter= "</span> + str(step+<span class="number">1</span>) + <span class="string">", Average Loss= "</span> + \</div><div class="line">                  <span class="string">"&#123;:.6f&#125;"</span>.format(loss_total/display_step) + <span class="string">", Average Accuracy= "</span> + \</div><div class="line">                  <span class="string">"&#123;:.2f&#125;%"</span>.format(<span class="number">100</span>*acc_total/display_step))</div><div class="line">            acc_total = <span class="number">0</span></div><div class="line">            loss_total = <span class="number">0</span></div><div class="line">            symbols_in = [training_data[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(offset, offset + n_input)]</div><div class="line">            symbols_out = training_data[offset + n_input]</div><div class="line">            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, <span class="number">1</span>).eval())]</div><div class="line">            print(<span class="string">"%s - [%s] vs [%s]"</span> % (symbols_in,symbols_out,symbols_out_pred))</div><div class="line">        step += <span class="number">1</span></div><div class="line">        offset += (n_input+<span class="number">1</span>)</div><div class="line">    print(<span class="string">"Optimization Finished!"</span>)</div><div class="line">    print(<span class="string">"Elapsed time: "</span>, elapsed(time.time() - start_time))</div><div class="line">    print(<span class="string">"Run on command line."</span>)</div><div class="line">    print(<span class="string">"\ttensorboard --logdir=%s"</span> % (logs_path))</div><div class="line">    print(<span class="string">"Point your web browser to: http://localhost:6006/"</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        prompt = <span class="string">"%s words: "</span> % n_input</div><div class="line">        sentence = input(prompt)</div><div class="line">        sentence = sentence.strip()</div><div class="line">        words = sentence.split(<span class="string">' '</span>)</div><div class="line">        <span class="keyword">if</span> len(words) != n_input:</div><div class="line">            <span class="keyword">continue</span></div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            symbols_in_keys = [dictionary[str(words[i])] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words))]</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">                keys = np.reshape(np.array(symbols_in_keys), [<span class="number">-1</span>, n_input, <span class="number">1</span>])</div><div class="line">                onehot_pred = session.run(pred, feed_dict=&#123;x: keys&#125;)</div><div class="line">                onehot_pred_index = int(tf.argmax(onehot_pred, <span class="number">1</span>).eval())</div><div class="line">                sentence = <span class="string">"%s %s"</span> % (sentence,reverse_dictionary[onehot_pred_index])</div><div class="line">                symbols_in_keys = symbols_in_keys[<span class="number">1</span>:]</div><div class="line">                symbols_in_keys.append(onehot_pred_index)</div><div class="line">            print(sentence)</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            print(<span class="string">"Word not in dictionary"</span>)</div></pre></td></tr></table></figure><p><strong>source blog:</strong> <a href="https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537" target="_blank" rel="external">https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537</a></p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/deep-learning/" rel="tag"># deep learning</a> <a href="/tags/LSTM/" rel="tag"># LSTM</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/04/26/Xiaomi-mini-wifi-cannot-build-the-connection/" rel="next" title="Xiaomi mini wifi cannot build the connection"><i class="fa fa-chevron-left"></i> Xiaomi mini wifi cannot build the connection</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/04/28/Generative-Adversarial-Networks-GANs-in-50-lines-of-code-PyTorch/" rel="prev" title="Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)">Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch) <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">77</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">45</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>