<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="machine learning,reinforcement learning,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary proba"><meta property="og:type" content="article"><meta property="og:title" content="k-Armed Bandit Problem"><meta property="og:url" content="http://yoursite.com/2017/05/27/k-Armed-Bandit-Problem/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary proba"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/action_value_distributions.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_optimal_action.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_average_reward.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values_average_reward.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/ucb_average_reward.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/gradient_bandit_optimal_action.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/bandit_algorithms_parameter_study.png"><meta property="og:updated_time" content="2017-06-13T06:00:32.852Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="k-Armed Bandit Problem"><meta name="twitter:description" content="Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary proba"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/action_value_distributions.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/05/27/k-Armed-Bandit-Problem/"><title>k-Armed Bandit Problem | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/27/k-Armed-Bandit-Problem/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">k-Armed Bandit Problem</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-27T12:27:49+08:00">2017-05-27 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/27/k-Armed-Bandit-Problem/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/27/k-Armed-Bandit-Problem/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/27/k-Armed-Bandit-Problem/" class="leancloud_visitors" data-flag-title="k-Armed Bandit Problem"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or <em>time steps</em>.</p><p>This is the original form of the <em>k-armed bandit problem</em>. Each of the <em>k</em> actions has an excepted or mean reward given that action is selected; let us call this <em>value</em> of that action. We denote the action selected on time step <em>t</em> as $A_{t}$, and the corresponding reward as $R_{t}$. The value then of an arbitrary action <em>a</em>, denoted $q_{\star}(a)$, is the excepted reward given that <em>a</em> is selected:<br>$$<br>q_{\star}(a) = \mathbb{E}[R_t|A_t=a]<br>$$<br>If you knew the value of each action, then it would be trivial to solve the <em>k</em>-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action <em>a</em> at time <em>t</em> as $Q_{t}(a) \approx q_{\star}(a)$.</p><p>We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when action is selected. One natural way to estimate this is by averaging the rewards actually received:</p><p>$$Q_t(a) \doteq \frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbf{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbf{1}_{A_i=a}}$$</p><p>where $\mathbf{1}_{\text{predicate}}$ denotes the random variable that is 1 if <em>predicate</em> is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as $Q_1(a) = 0$. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_{\star} (a)$. We call this the <strong><em>sample-average</em></strong> method for estimating action values because each estimate is an average of the sample of relevant rewards.</p><p>The simplest action selection rule is to select the action (or one of the actions) with highest estimated action value, that is, to select at step <em>t</em> one of the greedy actions, $A_t^\star$ for which $Q_t(A_t^\star) = \max_a Q_t(a)$. This <em>greedy</em> action selection method can be written as<br>$$<br>A_t \doteq argmax_a Q_t(a)<br>$$<br>Naturally, we could use the <em>$\epsilon$-greedy</em> method rather the <em>greedy</em> method. We’ll show their difference on the performance. Now, let’s jump into the implementation details. In order to be able to see the results quickly, we set to <em>k</em> to be <em>10</em>. The first, we generate 10 stationary probability distributions that we’ll sample from to generate action values. The generate method is below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data=np.random.randn(<span class="number">200</span>,<span class="number">10</span>) + np.random.randn(<span class="number">10</span>)</div></pre></td></tr></table></figure><p>We first generate randomly 10 true excepted values by <code>np.random.randn(10)</code>, then I’m going to change the variance to 1. So we get 10 stationary probability distributions. The visualizations are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/action_value_distributions.png" alt="action_value_distributions"></p><p>We’re going to compare how different $\epsilon$ values affect the end result.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">epsilonGreedy</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    epsilons = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.01</span>]</div><div class="line">    bandits = []</div><div class="line">    <span class="keyword">for</span> epsInd, eps <span class="keyword">in</span> enumerate(epsilons):</div><div class="line">        bandits.append([Bandit(epsilon=eps, sampleAverages=<span class="keyword">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)])</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, counts <span class="keyword">in</span> zip(epsilons, bestActionCounts):</div><div class="line">        plt.plot(counts, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, rewards <span class="keyword">in</span> zip(epsilons, averageRewards):</div><div class="line">        plt.plot(rewards, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Before we go into the details, we introduce the <strong>Bandit</strong> object first.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bandit</span>:</span></div><div class="line">    <span class="comment"># @kArm: # of arms</span></div><div class="line">    <span class="comment"># @epsilon: probability for exploration in epsilon-greedy algorithm</span></div><div class="line">    <span class="comment"># @initial: initial estimation for each action</span></div><div class="line">    <span class="comment"># @stepSize: constant step size for updating estimations</span></div><div class="line">    <span class="comment"># @sampleAverages: if True, use sample averages to update estimations instead of constant step size</span></div><div class="line">    <span class="comment"># @UCB: if not None, use UCB algorithm to select action</span></div><div class="line">    <span class="comment"># @gradient: if True, use gradient based bandit algorithm</span></div><div class="line">    <span class="comment"># @gradientBaseline: if True, use average reward as baseline for gradient based bandit algorithm</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kArm=<span class="number">10</span>, epsilon=<span class="number">0.</span>, initial=<span class="number">0.</span>, stepSize=<span class="number">0.1</span>, sampleAverages=False, UCBParam=None, gradient=False, gradientBaseline=False, trueReward=<span class="number">0.</span>)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div></pre></td></tr></table></figure><p>For now we just introduce <em>sample-average</em> method, so skip other methods parameters. Let us see the initialization method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kArm=<span class="number">10</span>, epsilon=<span class="number">0.</span>, initial=<span class="number">0.</span>, stepSize=<span class="number">0.1</span>, sampleAverages=False, UCBParam=None,</span></span></div><div class="line">             gradient=False, gradientBaseline=False, trueReward=<span class="number">0.</span>):</div><div class="line">    self.k = kArm</div><div class="line">    self.stepSize = stepSize</div><div class="line">    self.sampleAverages = sampleAverages</div><div class="line">    self.indices = np.arange(self.k)</div><div class="line">    self.time = <span class="number">0</span></div><div class="line">    self.UCBParam = UCBParam</div><div class="line">    self.gradient = gradient</div><div class="line">    self.gradientBaseline = gradientBaseline</div><div class="line">    self.averageReward = <span class="number">0</span></div><div class="line">    self.trueReward = trueReward</div><div class="line"></div><div class="line">    <span class="comment"># real reward for each action</span></div><div class="line">    self.qTrue = []</div><div class="line"></div><div class="line">    <span class="comment"># estimation for each action</span></div><div class="line">    self.qEst = np.zeros(self.k)</div><div class="line"></div><div class="line">    <span class="comment"># # of chosen times for each action</span></div><div class="line">    self.actionCount = []</div><div class="line"></div><div class="line">    self.epsilon = epsilon</div><div class="line"></div><div class="line">    <span class="comment"># initialize real rewards with N(0,1) distribution and estimations with desired initial value</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.k):</div><div class="line">        self.qTrue.append(np.random.randn() + trueReward)</div><div class="line">        self.qEst[i] = initial</div><div class="line">        self.actionCount.append(<span class="number">0</span>)</div><div class="line"></div><div class="line">    self.bestAction = np.argmax(self.qTrue)</div></pre></td></tr></table></figure><p>There are some important attributes. <strong>time</strong> is a number that represents the time steps now. <strong>actionCount</strong> is the times that correspond actions have been taken prior to current time steps. <strong>qTrue</strong> is a list. And each item is the true excepted value corresponding to each action. <strong>qEst</strong> is the estimate value of each action. It’s initialized to zero. <strong>epsilon</strong> is the $\epsilon$. Next, in the for loop, we initialize real rewards with N(0, 1) distribution and estimations with desired initial value. At last, the <strong>bestAction</strong> store the current best action will be take.</p><p>The next method tell us how to get the next action should be take:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># explore</span></div><div class="line">    <span class="keyword">if</span> self.epsilon &gt; <span class="number">0</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, self.epsilon) == <span class="number">1</span>:</div><div class="line">            np.random.shuffle(self.indices)</div><div class="line">            <span class="keyword">return</span> self.indices[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># exploit</span></div><div class="line">    <span class="keyword">if</span> self.UCBParam <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        UCBEst = self.qEst + \</div><div class="line">                 self.UCBParam * np.sqrt(np.log(self.time + <span class="number">1</span>) / (np.asarray(self.actionCount) + <span class="number">1</span>))</div><div class="line">        <span class="keyword">return</span> np.argmax(UCBEst)</div><div class="line">    <span class="keyword">if</span> self.gradient:</div><div class="line">        expEst = np.exp(self.qEst)</div><div class="line">        self.actionProb = expEst / np.sum(expEst)</div><div class="line">        <span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div><div class="line">    <span class="keyword">return</span> np.argmax(self.qEst)</div></pre></td></tr></table></figure><p>We can skip the second and the third if statements (we’ll introduce this two methods later). If we use <em>greedy</em> method, we just return the action that has highest value. Otherwise, we’re choosing randomly at $\epsilon$ probability.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div><div class="line">    <span class="comment"># generate the reward under N(real reward, 1)</span></div><div class="line">    reward = np.random.randn() + self.qTrue[action]</div><div class="line">    self.time += <span class="number">1</span></div><div class="line">    self.averageReward = (self.time - <span class="number">1.0</span>) / self.time * self.averageReward + reward / self.time</div><div class="line">    self.actionCount[action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> self.sampleAverages:</div><div class="line">        <span class="comment"># update estimation using sample averages</span></div><div class="line">        self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">elif</span> self.gradient:</div><div class="line">        oneHot = np.zeros(self.k)</div><div class="line">        oneHot[action] = <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> self.gradientBaseline:</div><div class="line">            baseline = self.averageReward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            baseline = <span class="number">0</span></div><div class="line">        self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># update estimation with constant step size</span></div><div class="line">        self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">return</span> reward</div></pre></td></tr></table></figure><p>Similarly, we just skip other if statements and focus on this row:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>This formula seems different with the earlier one. The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. We now turn to the question of how these averages can be computed in a computationally efficient manner, in particular, with constant memory and per-time-step computation.</p><p>To simplify notation we concentrate on a single action. Let $R_i$ now denote the reward received after <em>i</em>th selection of <em>this action</em>, and let $Q_n$ denote the estimate of its action value after it has been select $n-1$ times, which we can now write simply as<br>$$<br>Q_n \doteq \frac{R_1 + R_2 + \cdots + R_{n-1}}{n-1}<br>$$<br>The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, in this case the memory and computational requirements would grow over time as more rewards are seen.</p><p>It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given $Q_n$ and the <em>n</em>th reward, $R_n$, the new average of all <em>n</em> rewards can be computed by<br>$$<br>\begin{align}<br>Q_{n+1} &amp;\doteq \frac{1}{n}\sum_{i=1}^{n} R_i \\<br>&amp;= \frac{1}{n}(R_n + \sum_{i=1}^{n-1} R_i) \\<br>&amp;= \frac{1}{n}(R_n + (n-1) \frac{1}{n-1}\sum_{i=1}^{n-1} R_i) \\<br>&amp;= \frac{1}{n}(R_n + (n-1) Q_n) \\<br>&amp;= \frac{1}{n}(R_n + n Q_n - Q_n) \\<br>&amp;= Q_n + \frac{1}{n}[R_n - Q_n]<br>\end{align}<br>$$<br>So this is why the code is look like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>Back to <strong>epsilonGreedy()</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">epsilonGreedy</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    epsilons = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.01</span>]</div><div class="line">    bandits = []</div><div class="line">    <span class="keyword">for</span> epsInd, eps <span class="keyword">in</span> enumerate(epsilons):</div><div class="line">        bandits.append([Bandit(epsilon=eps, sampleAverages=<span class="keyword">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)])</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, counts <span class="keyword">in</span> zip(epsilons, bestActionCounts):</div><div class="line">        plt.plot(counts, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, rewards <span class="keyword">in</span> zip(epsilons, averageRewards):</div><div class="line">        plt.plot(rewards, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Now, we get <strong>nBandits</strong> bandits and each bandit has 10 arm. Then we use a bandit simulation to simulate this process.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">banditSimulation</span><span class="params">(nBandits, time, bandits)</span>:</span></div><div class="line">    bestActionCounts = [np.zeros(time, dtype=<span class="string">'float'</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, len(bandits))]</div><div class="line">    averageRewards = [np.zeros(time, dtype=<span class="string">'float'</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, len(bandits))]</div><div class="line">    <span class="keyword">for</span> banditInd, bandit <span class="keyword">in</span> enumerate(bandits):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nBandits):</div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, time):</div><div class="line">                action = bandit[i].getAction()</div><div class="line">                reward = bandit[i].takeAction(action)</div><div class="line">                averageRewards[banditInd][t] += reward</div><div class="line">                <span class="keyword">if</span> action == bandit[i].bestAction:</div><div class="line">                    bestActionCounts[banditInd][t] += <span class="number">1</span></div><div class="line">        bestActionCounts[banditInd] /= nBandits</div><div class="line">        averageRewards[banditInd] /= nBandits</div><div class="line">    <span class="keyword">return</span> bestActionCounts, averageRewards</div></pre></td></tr></table></figure><p>The <strong>bandits</strong> is a list that has three item. Each item is a list that contains <strong>nBandits</strong> bandits that has a corresponding epsilon value. We calculate the average total reward of 2000 bandits is to eliminate the effect of noise. Ok, let us see the final result.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_optimal_action.png" alt="epsilon_greedy_optimal_action"><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_average_reward.png" alt="epsilon_greedy_average_reward"></p><p>We can see the algorithm reaches the best performance when epsilon is set to 0.1.</p><p>The averaging methods discussed so far are appropriate in a stationary environment, but not if the bandit is changing over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to weight recent rewards more heavily than long-past ones. One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule for updating an average $Q_n$ of the $n-1$ past rewards is modified to be<br>$$<br>Q_{n+1} \doteq Q_n + \alpha [R_n - Q_n]<br>$$<br>where the step-size parameter $\alpha \in (0, 1]$ is constant. This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_1$:<br>$$<br>\begin{align}<br>Q_{n+1} &amp;\doteq Q_n + \alpha [R_n - Q_n] \\<br>&amp;= \alpha R_n + (1 - \alpha) Q_n \\<br>&amp;= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] \\<br>&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 Q_{n-1} \\<br>&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 R_{n-1} + \cdots + (1 - \alpha) ^ {n-1} \alpha R_1 + (1 - \alpha) ^ n Q_1 \\<br>&amp;= (1 - \alpha) ^ n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha) ^ {n-i} R_i<br>\end{align}<br>$$<br>We call this a <em>weighted average</em> because the sum of the weights is 1. In fact, the weight decays exponentially according to the exponent on $1-\alpha$. According, this is sometimes called an <em>exponential, recency-weighted average</em>.</p><p>Sometimes it is convenient to vary the step-size parameter from step to step. Let $\alpha_n(a)$ denote the step-size parameter used to process the reward received after <em>n</em>th selection of action <em>a</em>. As we noted, the choice $\alpha_n(a)=\frac{1}{n}$ results in the sample-average method, which is guaranteed to converge to the true action values by the law of the large numbers. A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:<br>$$<br>\sum_{n=1}^{\infty}\alpha_{n}(a) = \infty \; \text{and} \; \sum_{n=1}^{\infty}\alpha_{n}^{2}(a) &lt; \infty<br>$$<br>All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, $Q_1(a)$. <strong>In the language of statistics, these methods are <em>biased</em> by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant $\alpha$ (weighted avetrage), the bias is permanent, though decreasing over time. In practice, this kind of bias is usually not a problem and can sometimes be very helpful.</strong> The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be excepted. So, let us do a experiment. The first we set them all to zero and the we set them all to a constant, 5.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimisticInitialValues</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    bandits = [[], []]</div><div class="line">    bandits[<span class="number">0</span>] = [Bandit(epsilon=<span class="number">0</span>, initial=<span class="number">5</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bandits[<span class="number">1</span>] = [Bandit(epsilon=<span class="number">0.1</span>, initial=<span class="number">0</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(bestActionCounts[<span class="number">0</span>], label=<span class="string">'epsilon = 0, q = 5'</span>)</div><div class="line">    plt.plot(bestActionCounts[<span class="number">1</span>], label=<span class="string">'epsilon = 0.1, q = 0'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(averageRewards[<span class="number">0</span>], label=<span class="string">'epsilon=0, initial=5, stepSize=0.1'</span>)</div><div class="line">    plt.plot(averageRewards[<span class="number">1</span>], label=<span class="string">'epsilon=0.1, initial=0, stepSize=0.1'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The <strong>Bandit</strong> object’s <strong>takeAction()</strong> has a little difference:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values.png" alt="optimistic_initial_value_optimal_action"><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values_average_reward.png" alt="optimistic_initial_value_optimal_action"></p><p>We can see it reaches the better performance than the $\epsilon$-greedy method, when they are all used the weighted average method. Notice that the $\epsilon$-greedy with weighted-average method is worsen than the $\epsilon$-greedy with sample-average method.</p><p>We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial state in any special way is unlikely to help with the general nonstationary case.</p><p>Exploration is needed because the estimates of the action values are uncertain. The greedy actions are those that look best at present, but some of the other actions may actually be better. $\epsilon$-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions as<br>$$<br>A_t \doteq argmax_a \left [Q_t(a) + c\sqrt{\frac{\ln{t}}{N_t(a)}}\ \right]<br>$$<br>where $N_t(a)$ denotes the number of times that action <em>a</em> has been selected prior to time <em>t</em>, and the number $c&gt;0$ controls the degree of exploration. If $N_t(a)=0$, then <em>a</em> is considered to be a maximizing action. The idea of this is called <em>upper confidence bound</em> (UCB). Let us implement it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ucb</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    bandits = [[], []]</div><div class="line">    bandits[<span class="number">0</span>] = [Bandit(epsilon=<span class="number">0</span>, stepSize=<span class="number">0.1</span>, UCBParam=<span class="number">2</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bandits[<span class="number">1</span>] = [Bandit(epsilon=<span class="number">0.1</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    _, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(averageRewards[<span class="number">0</span>], label=<span class="string">'UCB c = 2'</span>)</div><div class="line">    plt.plot(averageRewards[<span class="number">1</span>], label=<span class="string">'epsilon greedy epsilon = 0.1'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>We note that the <strong>UCBParam=2</strong>. The Bandit object explains this. The <strong>getAction()</strong> method and <strong>takeAction()</strong> method are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># explore</span></div><div class="line">    <span class="keyword">if</span> self.epsilon &gt; <span class="number">0</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, self.epsilon) == <span class="number">1</span>:</div><div class="line">            np.random.shuffle(self.indices)</div><div class="line">            <span class="keyword">return</span> self.indices[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># exploit</span></div><div class="line">    <span class="keyword">if</span> self.UCBParam <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        UCBEst = self.qEst + \</div><div class="line">                 self.UCBParam * np.sqrt(np.log(self.time + <span class="number">1</span>) / (np.asarray(self.actionCount) + <span class="number">1</span>))</div><div class="line">        <span class="keyword">return</span> np.argmax(UCBEst)</div><div class="line">    <span class="keyword">if</span> self.gradient:</div><div class="line">        expEst = np.exp(self.qEst)</div><div class="line">        self.actionProb = expEst / np.sum(expEst)</div><div class="line">        <span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div><div class="line">    <span class="keyword">return</span> np.argmax(self.qEst)</div><div class="line"></div><div class="line"><span class="comment"># take an action, update estimation for this action</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div><div class="line">    <span class="comment"># generate the reward under N(real reward, 1)</span></div><div class="line">    reward = np.random.randn() + self.qTrue[action]</div><div class="line">    self.time += <span class="number">1</span></div><div class="line">    self.averageReward = (self.time - <span class="number">1.0</span>) / self.time * self.averageReward + reward / self.time</div><div class="line">    self.actionCount[action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> self.sampleAverages:</div><div class="line">        <span class="comment"># update estimation using sample averages</span></div><div class="line">        self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">elif</span> self.gradient:</div><div class="line">        oneHot = np.zeros(self.k)</div><div class="line">        oneHot[action] = <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> self.gradientBaseline:</div><div class="line">            baseline = self.averageReward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            baseline = <span class="number">0</span></div><div class="line">        self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># update estimation with constant step size</span></div><div class="line">        self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">return</span> reward</div></pre></td></tr></table></figure><p>We can see the policy get next action has changed but the update policy has not changed. The result is here:<img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/ucb_average_reward.png" alt="ucb_average_reward"></p><p>We omit the figure about the optimal action because the trend of this two figures are the same. UCB will often perform well, but is more difficult than $\epsilon$-greedy to extend beyond bandits to the more general reinforcement learning setting considered in the more advanced problems. In these more advanced settings there is currently no known practical way of utilizing the idea of UCB action selection.</p><p>So far we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. Now, we consider learning a numerical <em>preference</em> $H_t(a)$ for each action <em>a</em>. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the preferences there is no effect on the action probabilities, which are determined according to a softmax distribution as follows:<br>$$<br>Pr\{A_t=a\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} \doteq \pi_t(a)<br>$$<br>where here we have also introduced a useful new notation $\pi_t(a)$ for the probability of taking action <em>a</em> at time <em>t</em>. Initially all preferences are the same (e.g., $H_1(a)=0, \forall a$) so that all actions have an equal probability of being selected.</p><p>There is a natural learning algorithm for this setting based on the idea of stochastic gradient ascent. On each step, after selection the action $A_t$ and receiving the reward $R_t$, the preferences are updated by:<br>$$<br>\begin{align}<br>H_{t+1}(A_t) \doteq H_t(A_t) + \alpha (R_t - \overline{R_t}) (1 - \pi_t(A_t)), \; \text{and} \\<br>H_{t+1}(a) \doteq H_t(a) - \alpha (R_t - \overline{R_t}) \pi_t(a), \;\;\;\;\;\; \forall{a \neq A_t}<br>\end{align}<br>$$<br>where $\alpha&gt;0$ is a ste-size parameter, and $\overline{R_t} \in \mathbb{R}$ is the average of all the rewards up through and including time <em>t</em>, which can be computed incrementally. The $\overline{R_t}$ term serves as a <strong><em>baseline</em></strong> with which the reward is compared. Let us see the implement details (takeAction() method and getAction() method).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> self.gradient:</div><div class="line">	expEst = np.exp(self.qEst)</div><div class="line">	self.actionProb = expEst / np.sum(expEst)</div><div class="line">	<span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">elif</span> self.gradient:</div><div class="line">	oneHot = np.zeros(self.k)</div><div class="line">	oneHot[action] = <span class="number">1</span></div><div class="line">	<span class="keyword">if</span> self.gradientBaseline:</div><div class="line">		baseline = self.averageReward</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		baseline = <span class="number">0</span></div><div class="line">	self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot self.actionProb)</div></pre></td></tr></table></figure><p>The below figure shows results with the gradient bandit algorithm on a variant of the 10-armed testbed in which the true excepted rewards were selected according to a normal distribution with a mean of +4 instead of zero (and with unit variance as before).</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/gradient_bandit_optimal_action.png" alt="gradient_bandit_optimal_action"></p><p>Despite their simplicity, in our opinion the methods presented in here can fairly be considered the state of the art. Finally, we do a parameter study of the various bandit algorithms presented in here.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">figure2_6</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    labels = [<span class="string">'epsilon-greedy'</span>, <span class="string">'gradient bandit'</span>,</div><div class="line">              <span class="string">'UCB'</span>, <span class="string">'optimistic initialization'</span>]</div><div class="line">    generators = [<span class="keyword">lambda</span> epsilon: Bandit(epsilon=epsilon, sampleAverages=<span class="keyword">True</span>),</div><div class="line">                  <span class="keyword">lambda</span> alpha: Bandit(gradient=<span class="keyword">True</span>, stepSize=alpha, gradientBaseline=<span class="keyword">True</span>),</div><div class="line">                  <span class="keyword">lambda</span> coef: Bandit(epsilon=<span class="number">0</span>, stepSize=<span class="number">0.1</span>, UCBParam=coef),</div><div class="line">                  <span class="keyword">lambda</span> initial: Bandit(epsilon=<span class="number">0</span>, initial=initial, stepSize=<span class="number">0.1</span>)]</div><div class="line">    parameters = [np.arange(<span class="number">-7</span>, <span class="number">-1</span>),</div><div class="line">                  np.arange(<span class="number">-5</span>, <span class="number">2</span>),</div><div class="line">                  np.arange(<span class="number">-4</span>, <span class="number">3</span>),</div><div class="line">                  np.arange(<span class="number">-2</span>, <span class="number">3</span>)]</div><div class="line"></div><div class="line">    bandits = [[generator(math.pow(<span class="number">2</span>, param)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)] <span class="keyword">for</span> generator, parameter <span class="keyword">in</span> zip(generators, parameters) <span class="keyword">for</span> param <span class="keyword">in</span> parameter]</div><div class="line">    _, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    rewards = np.sum(averageRewards, axis=<span class="number">1</span>)/time</div><div class="line"></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    i = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> label, parameter <span class="keyword">in</span> zip(labels, parameters):</div><div class="line">        l = len(parameter)</div><div class="line">        plt.plot(parameter, rewards[i:i+l], label=label)</div><div class="line">        i += l</div><div class="line">    plt.xlabel(<span class="string">'Parameter(2^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The results as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/bandit_algorithms_parameter_study.png" alt="parameters_study"></p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/05/26/Tic-tac-toe/" rel="next" title="Tic-Tac-Toe Game"><i class="fa fa-chevron-left"></i> Tic-Tac-Toe Game</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/05/29/The-GridWorld-problem/" rel="prev" title="The GridWorld problem">The GridWorld problem <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">88</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">48</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/05/27/k-Armed-Bandit-Problem/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(t,a.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>