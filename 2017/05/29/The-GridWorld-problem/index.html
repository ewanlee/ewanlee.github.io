<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="reinforcement learning,machine learning,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="A reinforcement learning task that satisfied the Markov property is called Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision proces"><meta property="og:type" content="article"><meta property="og:title" content="The GridWorld problem"><meta property="og:url" content="http://yoursite.com/2017/05/29/The-GridWorld-problem/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="A reinforcement learning task that satisfied the Markov property is called Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision proces"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/grid_world/grid_world.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/grid_world/optimal_value.png"><meta property="og:updated_time" content="2017-05-29T06:14:07.620Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="The GridWorld problem"><meta name="twitter:description" content="A reinforcement learning task that satisfied the Markov property is called Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision proces"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/grid_world/grid_world.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/05/29/The-GridWorld-problem/"><title>The GridWorld problem | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/29/The-GridWorld-problem/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">The GridWorld problem</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-29T14:09:26+08:00">2017-05-29 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/29/The-GridWorld-problem/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/29/The-GridWorld-problem/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/29/The-GridWorld-problem/" class="leancloud_visitors" data-flag-title="The GridWorld problem"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>A reinforcement learning task that satisfied the Markov property is called <em>Markov decision process</em>, or <em>MDP</em>. If the state and action spaces are finite, then it is called a <em>finite Markov decision process</em> (<em>finite MDP</em>).</p><p>A particular finite MDP is defined by its state and action sets and by the one-step dynamics of the environment. Given any state and action <em>s</em> and <em>a</em>, the probability of each possible pair of next state and reward, <em>s’</em>, <em>r</em>, is denoted<br>$$<br>p(s^{\prime}, r | s, a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime}, R_{t+1}=r \ | \ S_{t}=s, A_{t}=a \}<br>$$<br>Given that, one can compute anything else one might want to know about the environment, such as the excepted rewards of state-action pairs,<br>$$<br>r(s,a) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a] = \sum_{r \in \mathcal{R}}r\sum_{s^{\prime} \in \mathcal{S}}p(s^{\prime},r|s, a)<br>$$<br>the <em>state-transition probabilities</em>,<br>$$<br>p(s^{\prime}|s,a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime} \ | \ S_{t}=s, A_{t}=a\} = \sum_{r \in \mathcal{R}} p(s^{\prime},r|s, a)<br>$$<br>and the excepted rewards for state-action-next-state triples,<br>$$<br>r(s, a, s^{\prime}) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a, S_{t+1}=s^{\prime}] = \frac{\sum_{r \in \mathcal{R}}rp(s^{\prime},r|s,a)}{p(s^{\prime}|s,a)}<br>$$<br>Almost all reinforcement learning algorithms involve estimating <em>value functions</em>–functions of states (or of state-action pairs) that estimate <em>how good</em> it is for the agent to be in a given state (or how good it is to perform a given action in a given state).</p><p>Recall that a policy, $\pi$, is a mapping from a each state, $s \in \mathcal{S}$, and action, $a \in \mathcal{A}(s)$, to the probability $\pi(a|s)$ of taking action <em>a</em> when in state <em>s</em>. Informally, the <em>value</em> of a state <em>s</em> under a policy $\pi$, denoted $v_{\pi}(s)$, is the excepted return when starting in <em>s</em> and following $\pi$ thereafter. For MDPs, we can define $v_{\pi}(s)$ formally as<br>$$<br>v_{\pi}(s) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right]<br>$$<br>Note that the value of the terminal state, if any, is always zero. We call the function $v_{\pi}$ the <em>state-value function for policy $\pi$.</em></p><p>Similarly, we define the value of taking action <em>a</em> in state <em>s</em> under a policy $\pi$, denoted $q_{\pi}(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:<br>$$<br>q_{\pi}(s,a) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s, A_{t}=a] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s, A_{t}=a\right]<br>$$<br>We call $q_{\pi}$ the <em>action-value function for policy $\pi$</em>.</p><p>A fundamental property of the value functions used in reinforcement learning and dynamic programming is that they satisfy particular recursive relationships. For any policy $\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E_{\pi}[G_{t} \ | \ S_{t}=s]} \\<br>&amp;= \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right] \\<br>&amp;= \mathbb{E_{\pi}}\left[R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} \ | \ S_{t}=s \right] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}}\sum_{r}p(s^{\prime},r|s,a) \left[ r + \gamma \mathbb{E_{\pi}} \left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} | S_{t+1}=s^{\prime} \right] \right] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},<br>\end{align}<br>$$<br>Equation (11) is the <em>Bellman equation for $v_{\pi}$</em>.</p><p>Figure 1 (left) shows a rectangular grid world representation of a simple finite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: <strong>north</strong>, <strong>south</strong>, <strong>east</strong>, and <strong>west</strong>, which deterministically cause the agent to move one cell in the respective direction on the grid. Action would take the agent off the grid leave its location unchanged, but also result in a reward of -1. Other actions result in a reward of 0, excepted those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to $\mathrm{A^{\prime}}$. From state B, all actions yield a reward +5 and take the agent to $\mathrm{B^{\prime}}$.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/grid_world/grid_world.png" alt="grid_world"></p><p><em>Figure 1</em></p><p>Suppose the agent selects all four actions with equal probability in all states. Figure 1 (right) shows the value function, $v_{\pi}$, for this policy, for the discounted reward case with $\gamma = 0.9$. This value function was computed by solving the system of linear equations (11).</p><p>OK, now let us solve this problem. The first, we need to define the grid world by code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">WORLD_SIZE = <span class="number">5</span></div><div class="line">A_POS = [<span class="number">0</span>, <span class="number">1</span>]</div><div class="line">A_PRIME_POS = [<span class="number">4</span>, <span class="number">1</span>]</div><div class="line">B_POS = [<span class="number">0</span>, <span class="number">3</span>]</div><div class="line">B_PRIME_POS = [<span class="number">2</span>, <span class="number">3</span>]</div><div class="line">discount = <span class="number">0.9</span></div><div class="line"></div><div class="line">world = np.zeros((WORLD_SIZE, WORLD_SIZE))</div></pre></td></tr></table></figure><p>This world has 5 by 5 cells, and there are four special cells: A, A’, B, B’. Discount represents the $\gamma $ in equation (11). We know that the agent in the world selects all four actions with equal probability in all states (cells). So we have:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># left, up, right, down</span></div><div class="line">actions = [<span class="string">'L'</span>, <span class="string">'U'</span>, <span class="string">'R'</span>, <span class="string">'D'</span>]</div><div class="line"></div><div class="line">actionProb = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">    actionProb.append([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        actionProb[i].append(dict(&#123;<span class="string">'L'</span>:<span class="number">0.25</span>, <span class="string">'U'</span>:<span class="number">0.25</span>, <span class="string">'R'</span>:<span class="number">0.25</span>, <span class="string">'D'</span>:<span class="number">0.25</span>&#125;))</div></pre></td></tr></table></figure><p>The <strong>actionProb</strong> is a list that has five items. Each item represents a row in the grid and it also is a list that has five items that represents a column in corresponding row, that is, each item in a row represents a cell in the grid. In all cells (states), there are four direction could be selected with equal probability 0.25. Then, we’ll define a undirected graph with weights. The node represented the cell in grid. If between two node has a edge then the agent could move between this two nodes (cells). The weight on the edges represents the reward do this move.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">nextState = []</div><div class="line">actionReward = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">    nextState.append([])</div><div class="line">    actionReward.append([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        next = dict()</div><div class="line">        reward = dict()</div><div class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</div><div class="line">            next[<span class="string">'U'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'U'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'U'</span>] = [i - <span class="number">1</span>, j]</div><div class="line">            reward[<span class="string">'U'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> i == WORLD_SIZE - <span class="number">1</span>:</div><div class="line">            next[<span class="string">'D'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'D'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'D'</span>] = [i + <span class="number">1</span>, j]</div><div class="line">            reward[<span class="string">'D'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</div><div class="line">            next[<span class="string">'L'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'L'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'L'</span>] = [i, j - <span class="number">1</span>]</div><div class="line">            reward[<span class="string">'L'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> j == WORLD_SIZE - <span class="number">1</span>:</div><div class="line">            next[<span class="string">'R'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'R'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'R'</span>] = [i, j + <span class="number">1</span>]</div><div class="line">            reward[<span class="string">'R'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> [i, j] == A_POS:</div><div class="line">            next[<span class="string">'L'</span>] = next[<span class="string">'R'</span>] = next[<span class="string">'D'</span>] = next[<span class="string">'U'</span>] = A_PRIME_POS</div><div class="line">            reward[<span class="string">'L'</span>] = reward[<span class="string">'R'</span>] = reward[<span class="string">'D'</span>] = reward[<span class="string">'U'</span>] = <span class="number">10.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> [i, j] == B_POS:</div><div class="line">            next[<span class="string">'L'</span>] = next[<span class="string">'R'</span>] = next[<span class="string">'D'</span>] = next[<span class="string">'U'</span>] = B_PRIME_POS</div><div class="line">            reward[<span class="string">'L'</span>] = reward[<span class="string">'R'</span>] = reward[<span class="string">'D'</span>] = reward[<span class="string">'U'</span>] = <span class="number">5.0</span></div><div class="line"></div><div class="line">        nextState[i].append(next)</div><div class="line">        actionReward[i].append(reward)</div></pre></td></tr></table></figure><p>The <strong>nextState</strong> and <strong>actionReward</strong> are the same as <strong>actionProb</strong> that we explained earlier.</p><p>Now, we could solve this problem by use the equation (11):<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},<br>\end{align}<br>$$<br>Let us jump into the implementation detail.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep iteration until convergence</span></div><div class="line">    newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                newPosition = nextState[i][j][action]</div><div class="line">                <span class="comment"># bellman equation</span></div><div class="line">                newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + discount * world[newPosition[<span class="number">0</span>], newPosition[<span class="number">1</span>]])</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(world - newWorld)) &lt; <span class="number">1e-4</span>:</div><div class="line">        print(<span class="string">'Random Policy'</span>)</div><div class="line">        print(newWorld)</div><div class="line">        <span class="keyword">break</span></div><div class="line">    world = newWorld</div></pre></td></tr></table></figure><p>The core code is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + 		 discount * world[newPosition[<span class="number">0</span>], newPosition[<span class="number">1</span>]])</div></pre></td></tr></table></figure><p>The <code>+=</code> represents the first sum notation in the equation (11). If we ensure the current state (cell) and action will take in this world, then the next state and reward also will be ensured. So $\sum_{s^{\prime},r} p(s^{\prime}, r | s, a)$ is equal to 1.</p><p>The result as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Random Policy</div><div class="line">[[ <span class="number">3.30902999</span>  <span class="number">8.78932551</span>  <span class="number">4.42765281</span>  <span class="number">5.3224012</span>   <span class="number">1.49221235</span>]</div><div class="line"> [ <span class="number">1.52162172</span>  <span class="number">2.9923515</span>   <span class="number">2.25017358</span>  <span class="number">1.90760531</span>  <span class="number">0.5474363</span> ]</div><div class="line"> [ <span class="number">0.05085614</span>  <span class="number">0.73820423</span>  <span class="number">0.67314689</span>  <span class="number">0.35821982</span> <span class="number">-0.40310755</span>]</div><div class="line"> [<span class="number">-0.97355865</span> <span class="number">-0.43546179</span> <span class="number">-0.35484864</span> <span class="number">-0.58557148</span> <span class="number">-1.18304148</span>]</div><div class="line"> [<span class="number">-1.8576669</span>  <span class="number">-1.34519762</span> <span class="number">-1.22923364</span> <span class="number">-1.42288454</span> <span class="number">-1.97514545</span>]]</div></pre></td></tr></table></figure><p>We can see the value of all states is the same as the Figure 1.</p><p>Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi^{\prime}$ if its excepted return is greater than or equal to that of $\pi^{\prime}$ for all states. In other words, $\pi \ge \pi^{\prime}$ if and only if $v_{\pi}(s) \ge v_{\pi^{\prime}}(s)$ for all $s \in \mathcal{S}$. There is always at least one policy that is better than or equal to all other policies. This is an <em>optimal policy</em>. Although there may be more than one, we denote all the optimal policies by $\pi_{\star}$. They share the same state-value function, called the <em>optimal state-value function</em>, denote $v_{\star}$, and defined as<br>$$<br>v_{\star}(s) \doteq \max_{\pi} v_{\pi}(s),<br>$$<br>for all $s \in \mathcal{S}$.</p><p>Optimal policies also share the same <em>optimal action-value function</em>, denoted $q_{\star}$, and defined as<br>$$<br>q_{\star}(s, a) \doteq \max_{\pi} q_{\pi}(s, a)<br>$$<br>for all $s \in \mathcal{S}$ and $a \in \mathcal{A(s)}$. For the state-action pair <em>(s, a)</em>, this function gives the excepted return for taking action <em>a</em> in state <em>s</em> and thereafter following an optimal policy. Thus, we can write $q_{\star}$ in terms of $v_{\star}$ as follows:<br>$$<br>q_{\star}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\star} \ | \ S_{t}=s, A_{t}=a]<br>$$<br>Suppose we solve the Bellman equation for $v_{\star}$ for the simple grid task introduced in earlier and shown again in Figure 2 (left). Recall that state A is followed by a reward of +10 and transition to state A’. while state B is followed by a reward of +5 and transition to state B’. Figure 2 (middle) shows the optimal value function, and Figure 2 (right) shows the corresponding optimal policies. Where there are multiple arrows in a cell, any of the corresponding actions are optimal.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/grid_world/optimal_value.png" alt="optimal_value"></p><p><em>Figure 2</em></p><p>Now, let us solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">world = np.zeros((WORLD_SIZE, WORLD_SIZE))</div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep iteration until convergence</span></div><div class="line">    newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">            values = []</div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                newPosition = nextState[i][j][action]</div><div class="line">                <span class="comment"># value iteration</span></div><div class="line">                values.append(actionReward[i][j][action] + discount * world[newPosition[<span class="number">0</span>], newPosition[<span class="number">1</span>]])</div><div class="line">            newWorld[i][j] = np.max(values)</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(world - newWorld)) &lt; <span class="number">1e-4</span>:</div><div class="line">        print(<span class="string">'Optimal Policy'</span>)</div><div class="line">        print(newWorld)</div><div class="line">        <span class="keyword">break</span></div><div class="line">    world = newWorld</div></pre></td></tr></table></figure><p>We can see the core code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">newWorld[i][j] = np.max(values)</div></pre></td></tr></table></figure><p>The only difference between this code and the earlier code is the prior only uses the maximum value and the latter uses the weighted average.</p><p>The result is below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Optimal Policy</div><div class="line">[[ <span class="number">21.97744338</span>  <span class="number">24.41938153</span>  <span class="number">21.97744338</span>  <span class="number">19.41938153</span>  <span class="number">17.47744338</span>]</div><div class="line"> [ <span class="number">19.77969904</span>  <span class="number">21.97744338</span>  <span class="number">19.77969904</span>  <span class="number">17.80172914</span>  <span class="number">16.02153504</span>]</div><div class="line"> [ <span class="number">17.80172914</span>  <span class="number">19.77969904</span>  <span class="number">17.80172914</span>  <span class="number">16.02153504</span>  <span class="number">14.41938153</span>]</div><div class="line"> [ <span class="number">16.02153504</span>  <span class="number">17.80172914</span>  <span class="number">16.02153504</span>  <span class="number">14.41938153</span>  <span class="number">12.97744338</span>]</div><div class="line"> [ <span class="number">14.41938153</span>  <span class="number">16.02153504</span>  <span class="number">14.41938153</span>  <span class="number">12.97744338</span>  <span class="number">11.67969904</span>]]</div></pre></td></tr></table></figure><p>It is not doubt that the result is the same as the Figure 2 (middle).</p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a> <a href="/tags/machine-learning/" rel="tag"># machine learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/05/27/k-Armed-Bandit-Problem/" rel="next" title="k-Armed Bandit Problem"><i class="fa fa-chevron-left"></i> k-Armed Bandit Problem</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/05/31/Dynamic-Programming/" rel="prev" title="Dynamic Programming">Dynamic Programming <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">85</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">48</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/05/29/The-GridWorld-problem/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(t,a.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>