<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="machine learning,reinforcement learning,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of environment as a Markov decision process (MDP). Classical D"><meta property="og:type" content="article"><meta property="og:title" content="Dynamic Programming"><meta property="og:url" content="http://yoursite.com/2017/05/31/Dynamic-Programming/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of environment as a Markov decision process (MDP). Classical D"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_policy.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_return.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_policy.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_value.png"><meta property="og:updated_time" content="2017-05-31T03:31:21.833Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Dynamic Programming"><meta name="twitter:description" content="The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of environment as a Markov decision process (MDP). Classical D"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/05/31/Dynamic-Programming/"><title>Dynamic Programming | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/31/Dynamic-Programming/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Dynamic Programming</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-31T10:19:52+08:00">2017-05-31 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/31/Dynamic-Programming/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/31/Dynamic-Programming/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/31/Dynamic-Programming/" class="leancloud_visitors" data-flag-title="Dynamic Programming"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are provides an essential foundation for the understanding of the methods presented later. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.</p><p>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. In here we show how DP can be used to compute the value functions defined in earlier. As discussed there, we can easily obtain optimal policies once we have found the optimal value functions $v_{\star}$ or $q_{\star}$ which satisfy the Bellman optimality equations:<br>$$<br>\begin{align}<br>v_{\star}(s) &amp;= \max_{a} \mathbb{E} [R_{t+1} + \gamma v_{\star}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \max_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) \left[r + \gamma v_{\star}(s^{\prime})\right]<br>\end{align}<br>$$<br>or<br>$$<br>\begin{align}<br>q_{\star}(s, a) &amp;= \mathbb{E} [R_{t+1} + \gamma \max_{a^{\prime}} q_{\star}(S_{t+1}, a^{\prime}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma \max_{a^{\prime}} q_{\star}(s^{\prime}, a^{\prime})]<br>\end{align}<br>$$<br>for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}, \; \text{and} \; s^{\prime} \in \mathcal{S^{+}}$. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.</p><p>First we consider how to compute the state-value function $v_{\pi}$ for an arbitrary policy $\pi$. This is called <em>policy evaluation</em> in the DP literature. We also refer to it as the <em>prediction problem</em>. Recall that for all $s \in \mathcal{S}$,<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots \ | \ S_{t}=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_{t}=s] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{\pi}(s^{\prime})]<br>\end{align}<br>$$<br>If the environment’s dynamics are complete known, then (7) is a system of $|\mathcal{S}|$ simultaneous linear equations in $|\mathcal{S}|$ unknowns (the $v_{\pi}(s), s \in \mathcal{S}$). In principle, its solution is a straightforward, if tedious, computation. For our purpose, iterative solution methods are most suitable. The initial approximation, $v_0$, is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation for $v_{\pi}$ as an update rule:<br>$$<br>\begin{align}<br>v_{k+1}(s) &amp;\doteq \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r|s, a) [r + \gamma v_{k} (s^{\prime})]<br>\end{align}<br>$$<br>This algorithm is called <em>iterative policy evaluation</em>.</p><blockquote><p><strong>Iterative policy evaluation</strong></p><p>Input $\pi$, the policy to be evaluated</p><p>Initialize an array $V(s) = 0$, for all $s \in \mathcal{S^{+}}$</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ for each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p><p>Output $V \approx v_{\pi}$</p></blockquote><p>We can see the algorithm used in the <a href="https://ewanlee.github.io/2017/05/29/The-GridWorld-problem/" target="_blank" rel="external">grid world problem</a> just is the <em>iterative policy evaluation</em>.</p><p>Our reason for computing the value function for a policy is to help find better policies. Once a policy $\pi$ has been improved using $v_{\pi}$ to yield a better policy $\pi^{\prime}$, we can then compute $v_{\pi^{\prime}}$and improve it again to yield an even better $\pi^{\prime\prime}$. We can thus obtain a sequence of monotonically improving policies and value functions:<br>$$<br>\pi_{0} \stackrel{E}\longrightarrow v_{\pi_{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow v_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow v_{\star},<br>$$<br>where $\stackrel{E}\longrightarrow$ denotes a policy <em>evaluation</em> and $\stackrel{I}\longrightarrow$ denotes a policy <em>improvement</em>. This way of finding an optimal policy is called <em>policy iteration</em>.</p><blockquote><p><strong>Policy iteration (using iterative policy evaluation)</strong></p><ol><li><p>Initialization</p><p>$V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A(s)}$ arbitrarily for all $s \in \mathcal{S}$</p></li><li><p>Policy Evaluation</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ For each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \sum_{s^{\prime}, r} p(s^{\prime}, r | s, \pi(s)) [r + \gamma v(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p></li><li><p>Policy Improvement</p><p><em>policy-stable</em> $\leftarrow$ <em>true</em></p><p>For each $s \in \mathcal{S}$:</p><p>​ <em>old-action</em> $\leftarrow$ $\pi_(s)$</p><p>​ $\pi (s) \leftarrow argmax_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$</p><p>​ If <em>old-action</em> $\neq \pi(s)$, then <em>policy-stable</em> $\leftarrow$ <em>false</em></p><p>If <em>policy-stable</em>, then stop and return $V \approx v_{\star} \; \text{and} \; \pi \approx \pi_{\star}$; else go to 2.</p></li></ol></blockquote><p>Let us solve a problem used by <strong>policy iteration</strong>. The problem defined as follows:</p><blockquote><p>Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and it credited \$10 by the national company. If he out of cats at that location, then the business is lost. Cars become available for renting the day after they are returned. To ensure that cars are available where they are needed, Jack ca move them between the two locations overnight, at a cost of \$2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is $n$ is $\frac{\lambda^{n}}{n!}e^{-\lambda}$, where $\lambda$ is the excepted number. Suppose $\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\lambda=0.9$ and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight.</p></blockquote><p>The excepted result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental.png" alt="car_rental"></p><p><em>Figure 1</em></p><p>The first, we define some facts of this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># maximum # of cars in each location</span></div><div class="line">MAX_CARS = <span class="number">20</span></div><div class="line"><span class="comment"># maximum # of cars to move during night</span></div><div class="line">MAX_MOVE_OF_CARS = <span class="number">5</span></div><div class="line"><span class="comment"># expectation for rental requests in first location</span></div><div class="line">RENTAL_REQUEST_FIRST_LOC = <span class="number">3</span></div><div class="line"><span class="comment"># expectation for rental requests in second location</span></div><div class="line">RENTAL_REQUEST_SECOND_LOC = <span class="number">4</span></div><div class="line"><span class="comment"># expectation for # of cars returned in first location</span></div><div class="line">RETURNS_FIRST_LOC = <span class="number">3</span></div><div class="line"><span class="comment"># expectation for # of cars returned in second location</span></div><div class="line">RETURNS_SECOND_LOC = <span class="number">2</span></div><div class="line">DISCOUNT = <span class="number">0.9</span></div><div class="line"><span class="comment"># credit earned by a car</span></div><div class="line">RENTAL_CREDIT = <span class="number">10</span></div><div class="line"><span class="comment"># cost of moving a car</span></div><div class="line">MOVE_CAR_COST = <span class="number">2</span></div></pre></td></tr></table></figure><p>From the problem definition, we know that in this MDP the states is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight. Each action is a integer that positive number represents the number of cars moving from the first location to second location and vice verse.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># current policy</span></div><div class="line">policy = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line"><span class="comment"># current state value</span></div><div class="line">stateValue = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line"><span class="comment"># all possible states</span></div><div class="line">states = []</div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">actions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + <span class="number">1</span>)</div></pre></td></tr></table></figure><p>For visualization (Figure 1) convenient, we define a method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># axes for printing use</span></div><div class="line">AxisXPrint = []</div><div class="line">AxisYPrint = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, MAX_CARS + <span class="number">1</span>):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, MAX_CARS + <span class="number">1</span>):</div><div class="line">        AxisXPrint.append(i)</div><div class="line">        AxisYPrint.append(j)</div><div class="line">        states.append([i, j])</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># plot a policy/state value matrix</span></div><div class="line">figureIndex = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span><span class="params">(data, labels)</span>:</span></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    fig = plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</div><div class="line">    AxisZ = []</div><div class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">        AxisZ.append(data[i, j])</div><div class="line">    ax.scatter(AxisXPrint, AxisYPrint, AxisZ)</div><div class="line">    ax.set_xlabel(labels[<span class="number">0</span>])</div><div class="line">    ax.set_ylabel(labels[<span class="number">1</span>])</div><div class="line">    ax.set_zlabel(labels[<span class="number">2</span>])</div></pre></td></tr></table></figure><p>Next, we define a Poisson function that return the probability:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># An up bound for poisson distribution</span></div><div class="line"><span class="comment"># If n is greater than this value, then the probability of getting n is truncated to 0</span></div><div class="line">POISSON_UP_BOUND = <span class="number">11</span></div><div class="line"></div><div class="line"><span class="comment"># Probability for poisson distribution</span></div><div class="line"><span class="comment"># @lam: lambda should be less than 10 for this function</span></div><div class="line">poissonBackup = dict()</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">poisson</span><span class="params">(n, lam)</span>:</span></div><div class="line">    <span class="keyword">global</span> poissonBackup</div><div class="line">    key = n * <span class="number">10</span> + lam</div><div class="line">    <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> poissonBackup.keys():</div><div class="line">        poissonBackup[key] = exp(-lam) * pow(lam, n) / factorial(n)</div><div class="line">    <span class="keyword">return</span> poissonBackup[key]</div></pre></td></tr></table></figure><p>Now, the preparation is done. We’ll implement the policy iteration algorithm as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">newStateValue = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line">improvePolicy = <span class="keyword">False</span></div><div class="line">policyImprovementInd = <span class="number">0</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">if</span> improvePolicy == <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># start policy improvement</span></div><div class="line">        print(<span class="string">'Policy improvement'</span>, policyImprovementInd)</div><div class="line">        policyImprovementInd += <span class="number">1</span></div><div class="line">        newPolicy = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line">        <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">            actionReturns = []</div><div class="line">            <span class="comment"># go through all actions and select the best one</span></div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                <span class="keyword">if</span> (action &gt;= <span class="number">0</span> <span class="keyword">and</span> i &gt;= action) <span class="keyword">or</span> (action &lt; <span class="number">0</span> <span class="keyword">and</span> j &gt;= abs(action)):</div><div class="line">                    actionReturns.append(expectedReturn([i, j], action, stateValue))</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    actionReturns.append(-float(<span class="string">'inf'</span>))</div><div class="line">            bestAction = argmax(actionReturns)</div><div class="line">            newPolicy[i, j] = actions[bestAction]</div><div class="line"></div><div class="line">        <span class="comment"># if policy is stable</span></div><div class="line">        policyChanges = np.sum(newPolicy != policy)</div><div class="line">        print(<span class="string">'Policy for'</span>, policyChanges, <span class="string">'states changed'</span>)</div><div class="line">        <span class="keyword">if</span> policyChanges == <span class="number">0</span>:</div><div class="line">            policy = newPolicy</div><div class="line">            <span class="keyword">break</span></div><div class="line">        policy = newPolicy</div><div class="line">        improvePolicy = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="comment"># start policy evaluation</span></div><div class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">        newStateValue[i, j] = expectedReturn([i, j], policy[i, j], stateValue)</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(newStateValue - stateValue)) &lt; <span class="number">1e-4</span>:</div><div class="line">        stateValue[:] = newStateValue</div><div class="line">        improvePolicy = <span class="keyword">True</span></div><div class="line">        <span class="keyword">continue</span></div><div class="line">    stateValue[:] = newStateValue</div></pre></td></tr></table></figure><p>We can see the logistic is the same as the pseudocode of the policy iteration algorithm. There is a core method in the code, that is, <strong>exceptedReturn()</strong> is used to calculate the reward of cars rental.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># @state: [# of cars in first location, # of cars in second location]</span></div><div class="line"><span class="comment"># @action: positive if moving cars from first location to second location,</span></div><div class="line"><span class="comment">#          negative if moving cars from second location to first location</span></div><div class="line"><span class="comment"># @stateValue: state value matrix</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">expectedReturn</span><span class="params">(state, action, stateValue)</span>:</span></div><div class="line">    <span class="comment"># initailize total return</span></div><div class="line">    returns = <span class="number">0.0</span></div><div class="line"></div><div class="line">    <span class="comment"># cost for moving cars</span></div><div class="line">    returns -= MOVE_CAR_COST * abs(action)</div><div class="line"></div><div class="line">    <span class="comment"># go through all possible rental requests</span></div><div class="line">    <span class="keyword">for</span> rentalRequestFirstLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">        <span class="keyword">for</span> rentalRequestSecondLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">            <span class="comment"># moving cars</span></div><div class="line">            numOfCarsFirstLoc = int(min(state[<span class="number">0</span>] - action, MAX_CARS))</div><div class="line">            numOfCarsSecondLoc = int(min(state[<span class="number">1</span>] + action, MAX_CARS))</div><div class="line"></div><div class="line">            <span class="comment"># valid rental requests should be less than actual # of cars</span></div><div class="line">            realRentalFirstLoc = min(numOfCarsFirstLoc, rentalRequestFirstLoc)</div><div class="line">            realRentalSecondLoc = min(numOfCarsSecondLoc, rentalRequestSecondLoc)</div><div class="line"></div><div class="line">            <span class="comment"># get credits for renting</span></div><div class="line">            reward = (realRentalFirstLoc + realRentalSecondLoc) * RENTAL_CREDIT</div><div class="line">            numOfCarsFirstLoc -= realRentalFirstLoc</div><div class="line">            numOfCarsSecondLoc -= realRentalSecondLoc</div><div class="line"></div><div class="line">            <span class="comment"># probability for current combination of rental requests</span></div><div class="line">            prob = poisson(rentalRequestFirstLoc, RENTAL_REQUEST_FIRST_LOC) * \</div><div class="line">                         poisson(rentalRequestSecondLoc, RENTAL_REQUEST_SECOND_LOC)</div><div class="line"></div><div class="line">            <span class="comment"># if set True, model is simplified such that the # of cars returned in daytime becomes constant</span></div><div class="line">            <span class="comment"># rather than a random value from poisson distribution, which will reduce calculation time</span></div><div class="line">            <span class="comment"># and leave the optimal policy/value state matrix almost the same</span></div><div class="line">            constantReturnedCars = <span class="keyword">True</span></div><div class="line">            <span class="keyword">if</span> constantReturnedCars:</div><div class="line">                <span class="comment"># get returned cars, those cars can be used for renting tomorrow</span></div><div class="line">                returnedCarsFirstLoc = RETURNS_FIRST_LOC</div><div class="line">                returnedCarsSecondLoc = RETURNS_SECOND_LOC</div><div class="line">                numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS)</div><div class="line">                numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS)</div><div class="line">                returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc])</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                numOfCarsFirstLoc_ = numOfCarsFirstLoc</div><div class="line">                numOfCarsSecondLoc_ = numOfCarsSecondLoc</div><div class="line">                prob_ = prob</div><div class="line">                <span class="keyword">for</span> returnedCarsFirstLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">                    <span class="keyword">for</span> returnedCarsSecondLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">                        numOfCarsFirstLoc = numOfCarsFirstLoc_</div><div class="line">                        numOfCarsSecondLoc = numOfCarsSecondLoc_</div><div class="line">                        prob = prob_</div><div class="line">                        numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS)</div><div class="line">                        numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS)</div><div class="line">                        prob = poisson(returnedCarsFirstLoc, RETURNS_FIRST_LOC) * \</div><div class="line">                               poisson(returnedCarsSecondLoc, RETURNS_SECOND_LOC) * prob</div><div class="line">                        returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc])</div><div class="line">    <span class="keyword">return</span> returns</div></pre></td></tr></table></figure><p>The comments are very clear, and we’re going to do a lot of this. Finally, let us print the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">prettyPrint(policy, [<span class="string">'# of cars in first location'</span>, <span class="string">'# of cars in second location'</span>, <span class="string">'# of cars to move during night'</span>])</div><div class="line">prettyPrint(stateValue, [<span class="string">'# of cars in first location'</span>, <span class="string">'# of cars in second location'</span>, <span class="string">'expected returns'</span>])</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>The results are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Policy improvement <span class="number">0</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">332</span> states changed</div><div class="line">Policy improvement <span class="number">1</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">286</span> states changed</div><div class="line">Policy improvement <span class="number">2</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">83</span> states changed</div><div class="line">Policy improvement <span class="number">3</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">19</span> states changed</div><div class="line">Policy improvement <span class="number">4</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">0</span> states changed</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_policy.png" alt="car_rental_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_return.png" alt="car_rental_return"></p><p>One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to $v_{\pi}$ occurs only in the limit. In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This algorithm is called <em>value iteration</em>. It can be written as a particular simple backup operation that combines the policy improvement and truncated policy evaluation steps:<br>$$<br>\begin{align}<br>v_{k+1} &amp;\doteq \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{k}(s^{\prime})],<br>\end{align}<br>$$<br>for all $s \in \mathcal{S}$.</p><blockquote><p><strong>Value iteration</strong></p><p>Initialize array $V$ arbitrarily (e.g. $V(s) = 0$ for all $s \in \mathcal{S^{+}}$)</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ For each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p><p>Output a deterministic policy, $\pi \approx \pi_{\star}$, such that</p><p>​ $\pi(s) = \arg\max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$</p></blockquote><p>Let us use the value iteration algorithm to solve a Gambler’s Problem. The problem defined as follows:</p><blockquote><p>A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he staked on the flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of \$100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer number of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gambler’s capital, $s \in \{1, 2, \cdots, 99\}$ and the actions are stakes, $a \in \{0, 1, \cdots, \min(s, 100-s)\}$. The reward is zero on all transitions excepted those on which the gambler reaches his goal, when it is +1. The state-value function then gives the probability of winning from each state. A policy is mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal. Let $p_h$ denote the probability of the coin coming up heads. If $p_h$ is known, then the entire problem is known and it can be solved, for instance, by value iteration.</p></blockquote><p>OK, now let us to solve this problem by use the value iteration algorithm.</p><p>The first we defined some facts and some auxiliary data structure:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># goal</span></div><div class="line">GOAL = <span class="number">100</span></div><div class="line"><span class="comment"># all states, including state 0 and state 100</span></div><div class="line">states = np.arange(GOAL + <span class="number">1</span>)</div><div class="line"><span class="comment"># probability of head</span></div><div class="line">headProb = <span class="number">0.4</span></div><div class="line"><span class="comment"># optimal policy</span></div><div class="line">policy = np.zeros(GOAL + <span class="number">1</span>)</div><div class="line"><span class="comment"># state value</span></div><div class="line">stateValue = np.zeros(GOAL + <span class="number">1</span>)</div><div class="line">stateValue[GOAL] = <span class="number">1.0</span></div></pre></td></tr></table></figure><p>The step of value iteration:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># value iteration</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    delta = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states[<span class="number">1</span>:GOAL]:</div><div class="line">        <span class="comment"># get possilbe actions for current state</span></div><div class="line">        actions = np.arange(min(state, GOAL - state) + <span class="number">1</span>)</div><div class="line">        actionReturns = []</div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">            actionReturns.append(headProb * stateValue[state + action] + (<span class="number">1</span> - headProb) * stateValue[state - action])</div><div class="line">        newValue = np.max(actionReturns)</div><div class="line">        delta += np.abs(stateValue[state] - newValue)</div><div class="line">        <span class="comment"># update state value</span></div><div class="line">        stateValue[state] = newValue</div><div class="line">    <span class="keyword">if</span> delta &lt; <span class="number">1e-9</span>:</div><div class="line">        <span class="keyword">break</span></div></pre></td></tr></table></figure><p>Calculate the optimal policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># calculate the optimal policy</span></div><div class="line"><span class="keyword">for</span> state <span class="keyword">in</span> states[<span class="number">1</span>:GOAL]:</div><div class="line">    actions = np.arange(min(state, GOAL - state) + <span class="number">1</span>)</div><div class="line">    actionReturns = []</div><div class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">        actionReturns.append(headProb * stateValue[state + action] + (<span class="number">1</span> - headProb) * stateValue[state - action])</div><div class="line">    <span class="comment"># due to tie and precision, can't reproduce the optimal policy in book</span></div><div class="line">    policy[state] = actions[argmax(actionReturns)]</div></pre></td></tr></table></figure><p>Print the results:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">plt.figure(<span class="number">1</span>)</div><div class="line">plt.xlabel(<span class="string">'Capital'</span>)</div><div class="line">plt.ylabel(<span class="string">'Value estimates'</span>)</div><div class="line">plt.plot(stateValue)</div><div class="line">plt.figure(<span class="number">2</span>)</div><div class="line">plt.scatter(states, policy)</div><div class="line">plt.xlabel(<span class="string">'Capital'</span>)</div><div class="line">plt.ylabel(<span class="string">'Final policy (stake)'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_policy.png" alt="gambler_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_value.png" alt="gambler_value"></p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/05/29/The-GridWorld-problem/" rel="next" title="The GridWorld problem"><i class="fa fa-chevron-left"></i> The GridWorld problem</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" rel="prev" title="Monte Carlo Methods (Reinforcement Learning)">Monte Carlo Methods (Reinforcement Learning) <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">130</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">64</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/tomaxent" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/05/31/Dynamic-Programming/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(e,a.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>