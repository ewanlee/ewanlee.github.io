<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="reinforcement learning,machine learning,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Here we consider our first learning methods for estimating value functions and discovering optimal policy. Unlike the previous algorithms, we do not assume complete knowledge of the environment. Monte"><meta property="og:type" content="article"><meta property="og:title" content="Monte Carlo Methods (Reinforcement Learning)"><meta property="og:url" content="http://yoursite.com/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Here we consider our first learning methods for estimating value functions and discovering optimal policy. Unlike the previous algorithms, we do not assume complete knowledge of the environment. Monte"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/blackjack_c.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_10000.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_10000.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_500000.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_500000.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_policy.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_policy.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_state_value.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_state_value.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/off_policy.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance_example.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance.png"><meta property="og:updated_time" content="2017-06-02T06:11:31.491Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Monte Carlo Methods (Reinforcement Learning)"><meta name="twitter:description" content="Here we consider our first learning methods for estimating value functions and discovering optimal policy. Unlike the previous algorithms, we do not assume complete knowledge of the environment. Monte"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/blackjack_c.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/"><title>Monte Carlo Methods (Reinforcement Learning) | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Monte Carlo Methods (Reinforcement Learning)</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-02T14:10:57+08:00">2017-06-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" class="leancloud_visitors" data-flag-title="Monte Carlo Methods (Reinforcement Learning)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Here we consider our first learning methods for <strong>estimating</strong> value functions and discovering optimal policy. Unlike the previous algorithms, we do not assume complete knowledge of the environment. Monte Carlo methods require only <em>experience</em> – sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Although a model is required, the model need only generate sample transition, not the complete probability distributions of all possible transitions that is required for <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">dynamic programming (DP)</a>.</p><p>Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample return. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. Only on the completion of an episode are value estimates and policies changed.</p><p>To handle the nonstationarity, we adapt the idea of general policy iteration (GPI) developed in earlier for <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">DP</a>.</p><p>Suppose we wish to estimate $v_{\pi}(s)$, the value of a state $s$ under policy $\pi$, given a set of episodes obtained by following $\pi$ and passing though $s$. Each occurrence of state $s$ in an episode is called a <em>visit</em> to $s$. Let us call the first time it is visited in an episode the <em>first visit</em> to $s$. The <em>first-visit MC method</em> estimates $v_{\pi}(s)$ as the average of the returns following first visits to $s$, whereas the <em>every-visit MC method</em> averages the returns following all visits to $s$.</p><blockquote><p><strong>First-visit MC policy evaluation (returns $V \approx v_{\pi}$)</strong></p><p>Initialize:</p><p>​ $\pi \leftarrow$ policy to be evaluated</p><p>​ $V \leftarrow $ an arbitrary state-value function</p><p>​ $Return(s) \leftarrow$ an empty list, for all $s \in \mathcal{S}$</p><p>Repeat forever:</p><p>​ Generate an episode using $\pi$</p><p>​ For each state $s$ appearing in the episode:</p><p>​ $G \leftarrow$ return following the first occurrence of $s$</p><p>​ Append $G$ to $Return(s)$</p><p>​ $V(s) \leftarrow$ $\text{average}(Return(s))$</p></blockquote><p>Next, we’ll use this algorithm to solve a naive problem that defined as follows:</p><blockquote><p>The object of the popular casino card game of <em>blackjack</em> is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a <em>natural</em>. He then wins unless the dealer also has a natural, in which case the game is draw. If the player does not have a natural, then he can request additional cards, one by one (<em>hits</em>), until he either stop (<em>sticks</em>) or excepted 21 (<em>goes bust</em>). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome–win, lose, draw–is determined by whose final sum is closer to 21.</p></blockquote><p>Playing blackjack is naturally formulated as an episode finite MDP. Each game of blackjack is an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount ($\gamma=1$); therefore these terminal rewards are also returns. The player’s actions are to hit or to stick. We assume that cards are dealt from an infinite deck (i.e., with replacement). If the player holds an ace that he could count as 11 without going bust, then the ace is said to be <em>usable</em>. Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits. Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.</p><p>Note that, although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events–in particular, they require the quantities $p(s^{\prime}, r|s, a)$–and it is not easy to determine these for blackjack. For example, suppose the play’s sum is 14 and he chooses to sticks. What is his excepted reward as a function of the dealer’s showing card? All of these rewards and transition probabilities must be computed <em>before</em> DP can be applied, and such computations are often complex and error-prone.</p><p>The conceptual diagram of the experimental results is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/blackjack_c.png" alt="blackjack_c"></p><p><em>Figure 1</em></p><p>The first we define some auxiliary variables and methods:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># actions: hit or stand (stick)</span></div><div class="line">ACTION_HIT = <span class="number">0</span></div><div class="line">ACTION_STAND = <span class="number">1</span></div><div class="line">actions = [ACTION_HIT, ACTION_STAND]</div><div class="line"></div><div class="line"><span class="comment"># policy for player</span></div><div class="line">policyPlayer = np.zeros(<span class="number">22</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">20</span>):</div><div class="line">    policyPlayer[i] = ACTION_HIT</div><div class="line">policyPlayer[<span class="number">20</span>] = ACTION_STAND</div><div class="line">policyPlayer[<span class="number">21</span>] = ACTION_STAND</div><div class="line"></div><div class="line"><span class="comment"># function form of target policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">return</span> policyPlayer[playerSum]</div><div class="line"></div><div class="line"><span class="comment"># function form of behavior policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> ACTION_STAND</div><div class="line">    <span class="keyword">return</span> ACTION_HIT</div><div class="line"></div><div class="line"><span class="comment"># policy for dealer</span></div><div class="line">policyDealer = np.zeros(<span class="number">22</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">17</span>):</div><div class="line">    policyDealer[i] = ACTION_HIT</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">17</span>, <span class="number">22</span>):</div><div class="line">    policyDealer[i] = ACTION_STAND</div><div class="line"></div><div class="line"><span class="comment"># get a new card</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCard</span><span class="params">()</span>:</span></div><div class="line">    card = np.random.randint(<span class="number">1</span>, <span class="number">14</span>)</div><div class="line">    card = min(card, <span class="number">10</span>)</div><div class="line">    <span class="keyword">return</span> card</div></pre></td></tr></table></figure><p>Furthermore, we also have a print method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># print the state value</span></div><div class="line">figureIndex = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span><span class="params">(data, tile, zlabel=<span class="string">'reward'</span>)</span>:</span></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    fig = plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    fig.suptitle(tile)</div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</div><div class="line">    axisX = []</div><div class="line">    axisY = []</div><div class="line">    axisZ = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">22</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</div><div class="line">            axisX.append(i)</div><div class="line">            axisY.append(j)</div><div class="line">            axisZ.append(data[i - <span class="number">12</span>, j - <span class="number">1</span>])</div><div class="line">    ax.scatter(axisX, axisY, axisZ)</div><div class="line">    ax.set_xlabel(<span class="string">'player sum'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'dealer showing'</span>)</div><div class="line">    ax.set_zlabel(zlabel)</div></pre></td></tr></table></figure><p>In order to get the figure above, we wrote the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onPolicy</span><span class="params">()</span>:</span></div><div class="line">    statesUsableAce1, statesNoUsableAce1 = monteCarloOnPolicy(<span class="number">10000</span>)</div><div class="line">    statesUsableAce2, statesNoUsableAce2 = monteCarloOnPolicy(<span class="number">500000</span>)</div><div class="line">    prettyPrint(statesUsableAce1, <span class="string">'Usable Ace, 10000 Episodes'</span>)</div><div class="line">    prettyPrint(statesNoUsableAce1, <span class="string">'No Usable Ace, 10000 Episodes'</span>)</div><div class="line">    prettyPrint(statesUsableAce2, <span class="string">'Usable Ace, 500000 Episodes'</span>)</div><div class="line">    prettyPrint(statesNoUsableAce2, <span class="string">'No Usable Ace, 500000 Episodes'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>There is a term named <em>on policy</em>, we’ll explain this term later. Now let us jump into the <strong>monteCarloOnPolicy</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo Sample with On-Policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOnPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    statesUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    statesNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesNoUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        state, reward, _ = play(targetPolicyPlayer)</div><div class="line">        state[<span class="number">1</span>] -= <span class="number">12</span></div><div class="line">        state[<span class="number">2</span>] -= <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> state[<span class="number">0</span>]:</div><div class="line">            statesUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            statesNoUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesNoUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">    <span class="keyword">return</span> statesUsableAce / statesUsableAceCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>We ignore he first four variables now and explain them later. <strong>nEpisodes</strong> represents the number of the episodes and the <strong>play</strong> method simulates the process of the blackjack cards game. So what is it like? This method is too long (more than 100 rows) so we partially explain it. In the first place, this method define some auxiliary variables:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># play a game</span></div><div class="line"><span class="comment"># @policyPlayerFn: specify policy for player</span></div><div class="line"><span class="comment"># @initialState: [whether player has a usable Ace, sum of player's cards, one card of dealer]</span></div><div class="line"><span class="comment"># @initialAction: the initial action</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">(policyPlayerFn, initialState=None, initialAction=None)</span>:</span></div><div class="line">    <span class="comment"># player status</span></div><div class="line">	<span class="comment"># sum of player</span></div><div class="line">    playerSum = <span class="number">0</span></div><div class="line">    <span class="comment"># trajectory of player</span></div><div class="line">    playerTrajectory = []</div><div class="line">    <span class="comment"># whether player uses Ace as 11</span></div><div class="line">    usableAcePlayer = <span class="keyword">False</span></div><div class="line">    <span class="comment"># dealer status</span></div><div class="line">    dealerCard1 = <span class="number">0</span></div><div class="line">    dealerCard2 = <span class="number">0</span></div><div class="line">    usableAceDealer = <span class="keyword">False</span></div></pre></td></tr></table></figure><p>Then, we generate a random initial state if the initial state is null. If not, we load the initial state from initialState variable:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> initialState <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">    <span class="comment"># generate a random initial state</span></div><div class="line">    numOfAce = <span class="number">0</span></div><div class="line">    <span class="comment"># initialize cards of player</span></div><div class="line">    <span class="keyword">while</span> playerSum &lt; <span class="number">12</span>:</div><div class="line">        <span class="comment"># if sum of player is less than 12, always hit</span></div><div class="line">        card = getCard()</div><div class="line">        <span class="comment"># if get an Ace, use it as 11</span></div><div class="line">        <span class="keyword">if</span> card == <span class="number">1</span>:</div><div class="line">            numOfAce += <span class="number">1</span></div><div class="line">            card = <span class="number">11</span></div><div class="line">            usableAcePlayer = <span class="keyword">True</span></div><div class="line">        playerSum += card</div><div class="line">    <span class="comment"># if player's sum is larger than 21, he must hold at least one Ace, two Aces are possible</span></div><div class="line">    <span class="keyword">if</span> playerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="comment"># use the Ace as 1 rather than 11</span></div><div class="line">        playerSum -= <span class="number">10</span></div><div class="line">        <span class="comment"># if the player only has one Ace, then he doesn't have usable Ace any more</span></div><div class="line">        <span class="keyword">if</span> numOfAce == <span class="number">1</span>:</div><div class="line">            usableAcePlayer = <span class="keyword">False</span></div><div class="line">    <span class="comment"># initialize cards of dealer, suppose dealer will show the first card he gets</span></div><div class="line">    dealerCard1 = getCard()</div><div class="line">    dealerCard2 = getCard()</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">    	<span class="comment"># use specified initial state</span></div><div class="line">    	usableAcePlayer = initialState[<span class="number">0</span>]</div><div class="line">    	playerSum = initialState[<span class="number">1</span>]</div><div class="line">    	dealerCard1 = initialState[<span class="number">2</span>]</div><div class="line">    	dealerCard2 = getCard()</div><div class="line"></div><div class="line">	<span class="comment"># initial state of the game</span></div><div class="line">	state = [usableAcePlayer, playerSum, dealerCard1]</div><div class="line"></div><div class="line">	<span class="comment"># initialize dealer's sum</span></div><div class="line">    dealerSum = <span class="number">0</span></div><div class="line">    <span class="keyword">if</span> dealerCard1 == <span class="number">1</span> <span class="keyword">and</span> dealerCard2 != <span class="number">1</span>:</div><div class="line">        dealerSum += <span class="number">11</span> + dealerCard2</div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> dealerCard1 != <span class="number">1</span> <span class="keyword">and</span> dealerCard2 == <span class="number">1</span>:</div><div class="line">        dealerSum += dealerCard1 + <span class="number">11</span></div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> dealerCard1 == <span class="number">1</span> <span class="keyword">and</span> dealerCard2 == <span class="number">1</span>:</div><div class="line">        dealerSum += <span class="number">1</span> + <span class="number">11</span></div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        dealerSum += dealerCard1 + dealerCard2</div></pre></td></tr></table></figure><p>Game start! Above all is player’s turn:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># player's turn</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">if</span> initialAction <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        action = initialAction</div><div class="line">        initialAction = <span class="keyword">None</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># get action based on current sum</span></div><div class="line">        action = policyPlayerFn(usableAcePlayer, playerSum, dealerCard1)</div><div class="line"></div><div class="line">    <span class="comment"># track player's trajectory for importance sampling</span></div><div class="line">    playerTrajectory.append([action, (usableAcePlayer, playerSum, dealerCard1)])</div><div class="line"></div><div class="line">    <span class="keyword">if</span> action == ACTION_STAND:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># if hit, get new card</span></div><div class="line">    playerSum += getCard()</div><div class="line"></div><div class="line">    <span class="comment"># player busts</span></div><div class="line">    <span class="keyword">if</span> playerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="comment"># if player has a usable Ace, use it as 1 to avoid busting and continue</span></div><div class="line">        <span class="keyword">if</span> usableAcePlayer == <span class="keyword">True</span>:</div><div class="line">            playerSum -= <span class="number">10</span></div><div class="line">            usableAcePlayer = <span class="keyword">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># otherwise player loses</span></div><div class="line">            <span class="keyword">return</span> state, <span class="number">-1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>Then is the dealer’s turn if the player’s turn is end:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># get action based on current sum</span></div><div class="line">    action = policyDealer[dealerSum]</div><div class="line">    <span class="keyword">if</span> action == ACTION_STAND:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># if hit, get a new card</span></div><div class="line">    dealerSum += getCard()</div><div class="line">    <span class="comment"># dealer busts</span></div><div class="line">    <span class="keyword">if</span> dealerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="keyword">if</span> usableAceDealer == <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># if dealer has a usable Ace, use it as 1 to avoid busting and continue</span></div><div class="line">            dealerSum -= <span class="number">10</span></div><div class="line">            usableAceDealer = <span class="keyword">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># otherwise dealer loses</span></div><div class="line">            <span class="keyword">return</span> state, <span class="number">1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>If the both sides have finished the game:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compare the sum between player and dealer</span></div><div class="line"><span class="keyword">if</span> playerSum &gt; dealerSum:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">1</span>, playerTrajectory</div><div class="line"><span class="keyword">elif</span> playerSum == dealerSum:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">0</span>, playerTrajectory</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">-1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>Now, let us come back the <strong>mentoCarloOnPolicy</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOnPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    statesUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    statesNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesNoUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        state, reward, _ = play(targetPolicyPlayer)</div><div class="line">        state[<span class="number">1</span>] -= <span class="number">12</span></div><div class="line">        state[<span class="number">2</span>] -= <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> state[<span class="number">0</span>]:</div><div class="line">            statesUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            statesNoUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesNoUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">    <span class="keyword">return</span> statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>In this method we ignore the player’s trajectory (represent by the <strong>playerTrajectory</strong> variable). If you remember a sentence in the game definition (as follows) it will easy to understand.</p><blockquote><p>Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.</p></blockquote><p>This row (as follows) is to calculate the average returns of each state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">return</span> statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>Recall the beginning of the code and let’s see what results are like:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_10000.png" alt="usable_ace_10000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_10000.png" alt="no_usable_ace_10000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_500000.png" alt="usable_ace_500000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_500000.png" alt="no_usable_ace_500000"></p><p><em>Figure 2 (from up to down). (1) Usable Ace, 10000 Episodes. (2) No Usable Ace, 10000 Episodes. (3) Usable Ace, 500000 Episodes. (4) No Usable Ace, 500000 Episodes.</em></p><p>If a model is not available, then it is particularly useful to estimate <em>action values</em> (the value of state-value pairs) rather than <em>state values</em>. With a model, state values alone are sufficient to determine a policy; one simply look ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the earlier on <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">DP</a>. Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of out primary goals for Monte Carlo methods is to estimate $q_{\star}$. To achieve this, we first consider the policy evaluation problem for action values.</p><p>The policy evaluation problem for action values is estimate $q_{\pi}(s, a)$. The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a state-action pair rather than to a state. The only complication is that many state-value pairs may never be visited. This is the general problem of <em>maintaining exploration</em>, as discussed in the context of the k-armed bandit problem in <a href="https://ewanlee.github.io/2017/05/27/k-Armed-Bandit-Problem/" target="_blank" rel="external">here</a>. One way to do this is by specifying that the episodes <em>start in a state-action pair</em>, and that every pair has a nonzero probability of being selected as the start. We call this the assumption of <em>exploring starts</em>.</p><p>We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal policies. To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with a arbitrary policy $\pi_{0}$ and ending with the optimal policy and optimal action-value function:<br>$$<br>\pi_{0} \stackrel{E}\longrightarrow q_{\pi{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow q_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow q_{\star},<br>$$<br>We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes. We postpone consideration of the first assumption until later.</p><p>The primary approach to avoiding the infinite number of episodes nominally required for policy evaluation is to forgo to complete policy evaluation before returning to policy improvement. For Monte Carlo policy evaluation it it natural to alternate between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation,and then the policy is improved at all the states visited in the episode.</p><blockquote><p><strong>Monte Carlo ES (Exploring Starts)</strong></p><p>Initialize, for all $s \in \mathcal{S},\; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow \text{arbitrary}$</p><p>​ $\pi(s) \leftarrow \text{arbitrary}$</p><p>​ $Returns(s,a) \leftarrow \text{empty list}$</p><p>Repeat forever:</p><p>​ Choose $S_{0} \in \mathcal{S}$ and $A_{0} \in \mathcal{A(S_{0})}$ s.t. all pairs have probability &gt; 0</p><p>​ Generate an episode starting from $S_0, A_0$, following $\pi$</p><p>​ For each pair $s, a$ appearing in the episode:</p><p>​ $G \leftarrow \text{return following the first occurrence of} \; s, a$</p><p>​ Append $G$ to $Returns(s,a)$</p><p>​ $Q(s,a) \leftarrow \text{average}(Returns(s,a))$</p><p>​ For each $s$ in the episode:</p><p>​ $\pi(s) \leftarrow \arg\max_{a} Q(s,a)$</p></blockquote><p>Recall the blackjack card game. It is straightforward to apply Monte Carlo ES to blackjack.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">figure5_3</span><span class="params">()</span>:</span></div><div class="line">    stateActionValues = monteCarloES(<span class="number">500000</span>)</div><div class="line">    stateValueUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    stateValueNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># get the optimal policy</span></div><div class="line">    actionUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="string">'int'</span>)</div><div class="line">    actionNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="string">'int'</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">            stateValueNoUsableAce[i, j] = np.max(stateActionValues[i, j, <span class="number">0</span>, :])</div><div class="line">            stateValueUsableAce[i, j] = np.max(stateActionValues[i, j, <span class="number">1</span>, :])</div><div class="line">            actionNoUsableAce[i, j] = argmax(stateActionValues[i, j, <span class="number">0</span>, :])</div><div class="line">            actionUsableAce[i, j] = argmax(stateActionValues[i, j, <span class="number">1</span>, :])</div><div class="line">    prettyPrint(stateValueUsableAce, <span class="string">'Optimal state value with usable Ace'</span>)</div><div class="line">    prettyPrint(stateValueNoUsableAce, <span class="string">'Optimal state value with no usable Ace'</span>)</div><div class="line">    prettyPrint(actionUsableAce, <span class="string">'Optimal policy with usable Ace'</span>, <span class="string">'Action (0 Hit, 1 Stick)'</span>)</div><div class="line">    prettyPrint(actionNoUsableAce, <span class="string">'Optimal policy with no usable Ace'</span>, <span class="string">'Action (0 Hit, 1 Stick)'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Run the code we’ll get the conceptual diagram like follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces.png" alt="mces"></p><p>Let us to see the implementation (<strong>monteCarloES</strong> method) of this algorithm. Note that, some auxiliary variables are defined earlier.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo with Exploring Starts</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloES</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    <span class="comment"># (playerSum, dealerCard, usableAce, action)</span></div><div class="line">    stateActionValues = np.zeros((<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">2</span>))</div><div class="line">    <span class="comment"># set default to 1 to avoid being divided by 0</span></div><div class="line">    stateActionPairCount = np.ones((<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">2</span>))</div><div class="line">    <span class="comment"># behavior policy is greedy</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicy</span><span class="params">(usableAce, playerSum, dealerCard)</span>:</span></div><div class="line">        usableAce = int(usableAce)</div><div class="line">        playerSum -= <span class="number">12</span></div><div class="line">        dealerCard -= <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> argmax(stateActionValues[playerSum, dealerCard, usableAce, :])</div><div class="line"></div><div class="line">    <span class="comment"># play for several episodes</span></div><div class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(nEpisodes):</div><div class="line">        print(<span class="string">'episode:'</span>, episode)</div><div class="line">        <span class="comment"># for each episode, use a randomly initialized state and action</span></div><div class="line">        initialState = [bool(np.random.choice([<span class="number">0</span>, <span class="number">1</span>])),</div><div class="line">                       np.random.choice(range(<span class="number">12</span>, <span class="number">22</span>)),</div><div class="line">                       np.random.choice(range(<span class="number">1</span>, <span class="number">11</span>))]</div><div class="line">        initialAction = np.random.choice(actions)</div><div class="line">        _, reward, trajectory = play(behaviorPolicy, initialState, initialAction)</div><div class="line">        <span class="keyword">for</span> action, (usableAce, playerSum, dealerCard) <span class="keyword">in</span> trajectory:</div><div class="line">            usableAce = int(usableAce)</div><div class="line">            playerSum -= <span class="number">12</span></div><div class="line">            dealerCard -= <span class="number">1</span></div><div class="line">            <span class="comment"># update values of state-action pairs</span></div><div class="line">            stateActionValues[playerSum, dealerCard, usableAce, action] += reward</div><div class="line">            stateActionPairCount[playerSum, dealerCard, usableAce, action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> stateActionValues / stateActionPairCount</div></pre></td></tr></table></figure><p>You can see we use the <strong>trajectory</strong> variable now. The difference between Monte Carlo On Policy and Monte Carlo ES is prior calculates the average return of each state and later calculates the average return of each state-action pair.</p><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_policy.png" alt="mcse_usbale_ace_optimal_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_policy.png" alt="mcse_usbale_ace_optimal_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_state_value.png" alt="mcse_usable_ace_optimal_state_value"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_state_value.png" alt="mcse_no_usable_ace_optimal_state_value"></p><p>How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call <em>on-policy</em> (Do you remember this term?) methods and <em>off-policy</em> methods. On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a policy different from that used to generate the data.. The First-visit Monte Carlo method and the Monte Carlo ES method are an example of an on-policy method. Here we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. Off-policy methods are considered later.</p><p>The on-policy method we presented in here uses $\epsilon \text{-} greedy$ policies. The complete algorithm is given below.</p><blockquote><p><strong>On-policy first-visit MC control (for $\epsilon \text{-soft}$ policies)</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a ) \leftarrow \text{arbitrary}$</p><p>​ $Returns(s,a) \leftarrow \text{empty list}$</p><p>​ $\pi(a|s) \leftarrow \text{an arbitrary} \; \epsilon \text{-soft policy}$</p><p>Repeat forever:</p><p>​ (a) Generate an episode using $\pi$</p><p>​ (b) For each pair $s, a$ appearing in the episode:</p><p>​ $G \leftarrow $ return following the first occurrence of $s, a$</p><p>​ Append $G$ to $Returns(s,a)$</p><p>​ $Q(s, a) \leftarrow \text{average}(Returns(s,a))$</p><p>​ (c) For each s in the episode:</p><p>​ $A^{\star} \leftarrow \arg\max_{a} Q(s,a)$</p><p>​ For all $a \in \mathcal{A(s)}$:</p><p>​ $\pi(a|s) \leftarrow \left\{ \begin{array}{c} 1 - \epsilon + \epsilon/|\mathcal{A(s)}|\;\;\;\text{if} \;a=A^{\star} \\ \epsilon/|\mathcal{A(s)}|\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{if} \;a\neq A^{\star} \end{array} \right.$</p></blockquote><p>All learning control methods face a dilemma: They seek to learn action values conditional on subsequent <em>optimal</em> behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromise–it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to user two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the <em>target policy</em>, and the policy used to generate behavior is called the <em>behavior policy</em>. In this case we say that learning is from data “off” the target policy, and the overall process is termed <em>off-policy learning</em>.</p><p>We begin the study of off-policy methods by considering the <em>prediction</em> problem, in which both target and behavior policies are fixed. That is, suppose we wish to estimate $v_{\pi}$ or $q_{\pi}$, but all we have are episodes following another policy $\mu$, where $\mu \neq \pi$. In this case, $\pi$ is the target policy, $\mu$ is the behavior policy, and both policies are considered fixed and given.</p><p>In order to use episodes from $\mu$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $\mu$. That is, we require that $\pi(a|s) &gt; 0$ implies $\mu(a|s) &gt; 0$. This is called the assumption of <em>converge</em>. It follows from converge that $\mu$ must be stochastic in states where it is not identical to $\pi$. The target policy $\pi$, on the other hand, may be deterministic. In here, we consider the prediction problem, in which $\pi$ is unchanging and given.</p><p>Almost all off-policy methods utilize <em>importance sampling</em>ddd, a general technique for estimating excepted values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectory occurring under the target and behavior policies, called the <em>importance-sampling ratio</em>. Given a starting state $S_{t}$, the probability of the subsequent state-action trajectory , $A_{t}, S_{t+1}, A_{t+1, \cdots, S_{T}}$, occurring under any policy $\pi$ is<br>$$<br>\prod_{k=t}^{T-1} \pi(A_k | S_k)p(S_{k+1} | S_k, A_k),<br>$$<br>where $p$ here is the state-transition probability function. Thus, the relative probability of the trajectory under the target and behavior policies (the important-sampling ratio) is<br>$$<br>\rho_{t}^{T} \doteq \frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1} \mu(A_k|S_k)p(S_{k+1}|S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi (A_k | S_k)}{\mu (A_k | S_k)}<br>$$<br>Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy $\mu$ to estimate $v_{\pi}(s)$. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time $t=101$. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state $s$ is visited, denote $\tau(s)$. This is for an every-visit method; for a first-visit method, $\tau(s)$ would only include time steps that were first visits to $s$ within their episodes. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up though $T(t)$. Then $\{G_t\}_{t \in \tau(s)}$ are returns that pertain to state $s$, and $\{\rho_{t}^{T(t)}\}_{t \in \tau(s)}$ are the corresponding importance-sampling ratios. To estimate $v_{\pi}(s)$, we simply scale the returns by the ratios and average the results:<br>$$<br>V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{|\tau(s)|}.<br>$$<br>When importance sampling is done as a simple average in this way it is called <em>ordinary importance sampling</em>.</p><p>An important alternative is <em>weighted importance sampling</em>, which uses a weighted average, defined as<br>$$<br>V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{\sum_{t \in \tau(s)} \rho_{t}^{T(t)}},<br>$$<br>or zero if the denominator is zero.</p><p>We applied both ordinary and weighted importance-sampling methods to estimate the value of as single blackjack state from off-policy data. Recall that one of the advantages of Monte Carlo methods is that they can be used to evaluate a single state without forming estimated for any other states. In this example, we evaluated the state in which the dealer is showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three aces). The data was generate by starting in this state then choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21. The value of this state under the target policy is approximately -0.27726 (this was determined by separately generating one-hundred million episodes using the target policy and averaging their returns). Both off-policy methods closely approximated this value after 1000 off-policy episodes using the random policy. To make sure they did this reliably, we performed 100 independent runs, each starting from estimates of zero and learning for 10,000 episodes. The implementation details are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo Sample with Off-Policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOffPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    initialState = [<span class="keyword">True</span>, <span class="number">13</span>, <span class="number">2</span>]</div><div class="line">    sumOfImportanceRatio = [<span class="number">0</span>]</div><div class="line">    sumOfRewards = [<span class="number">0</span>]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        _, reward, playerTrajectory = play(behaviorPolicyPlayer, initialState=initialState)</div><div class="line"></div><div class="line">        <span class="comment"># get the importance ratio</span></div><div class="line">        importanceRatioAbove = <span class="number">1.0</span></div><div class="line">        importanceRatioBelow = <span class="number">1.0</span></div><div class="line">        <span class="keyword">for</span> action, (usableAce, playerSum, dealerCard) <span class="keyword">in</span> playerTrajectory:</div><div class="line">            <span class="keyword">if</span> action == targetPolicyPlayer(usableAce, playerSum, dealerCard):</div><div class="line">                importanceRatioBelow *= <span class="number">0.5</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                importanceRatioAbove = <span class="number">0.0</span></div><div class="line">                <span class="keyword">break</span></div><div class="line">        importanceRatio = importanceRatioAbove / importanceRatioBelow</div><div class="line">        sumOfImportanceRatio.append(sumOfImportanceRatio[<span class="number">-1</span>] + importanceRatio)</div><div class="line">        sumOfRewards.append(sumOfRewards[<span class="number">-1</span>] + reward * importanceRatio)</div><div class="line">    <span class="keyword">del</span> sumOfImportanceRatio[<span class="number">0</span>]</div><div class="line">    <span class="keyword">del</span> sumOfRewards[<span class="number">0</span>]</div><div class="line"></div><div class="line">    sumOfRewards= np.asarray(sumOfRewards)</div><div class="line">    sumOfImportanceRatio= np.asarray(sumOfImportanceRatio)</div><div class="line">    ordinarySampling = sumOfRewards / np.arange(<span class="number">1</span>, nEpisodes + <span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> np.errstate(divide=<span class="string">'ignore'</span>,invalid=<span class="string">'ignore'</span>):</div><div class="line">        weightedSampling = np.where(sumOfImportanceRatio != <span class="number">0</span>, sumOfRewards / sumOfImportanceRatio, <span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> ordinarySampling, weightedSampling</div></pre></td></tr></table></figure><p>Note that the <strong>behaviorPolicyPlayer</strong> that is a function that define the behavior policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># function form of behavior policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> ACTION_STAND</div><div class="line">    <span class="keyword">return</span> ACTION_HIT</div></pre></td></tr></table></figure><p>And the calculation of the numerator and denominator of the importance-sampling are the summation operation. As the number of iterations increases, we need to accumulate the result from each episode. So we need to store the previous results. The <strong>sumOfRewards</strong> and <strong>sumOfImportanceRatio</strong> are used for this purpose.</p><p>Then we need to show the result (mean square error):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Figure 5.4</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">offPolicy</span><span class="params">()</span>:</span></div><div class="line">    trueValue = <span class="number">-0.27726</span></div><div class="line">    nEpisodes = <span class="number">10000</span></div><div class="line">    nRuns = <span class="number">100</span></div><div class="line">    ordinarySampling = np.zeros(nEpisodes)</div><div class="line">    weightedSampling = np.zeros(nEpisodes)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nRuns):</div><div class="line">        ordinarySampling_, weightedSampling_ = monteCarloOffPolicy(nEpisodes)</div><div class="line">        <span class="comment"># get the squared error</span></div><div class="line">        ordinarySampling += np.power(ordinarySampling_ - trueValue, <span class="number">2</span>)</div><div class="line">        weightedSampling += np.power(weightedSampling_ - trueValue, <span class="number">2</span>)</div><div class="line">    ordinarySampling /= nRuns</div><div class="line">    weightedSampling /= nRuns</div><div class="line">    axisX = np.log10(np.arange(<span class="number">1</span>, nEpisodes + <span class="number">1</span>))</div><div class="line">    plt.plot(axisX, ordinarySampling, label=<span class="string">'Ordinary Importance Sampling'</span>)</div><div class="line">    plt.plot(axisX, weightedSampling, label=<span class="string">'Weighted Importance Sampling'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Episodes (10^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Mean square error'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/off_policy.png" alt="off_policy"></p><p>Formally, the difference between the two kinds of importance sampling is expressed in their biases and variances. The ordinary importance-sampling estimator is unbiased whereas the weighted importance-sampling estimator is biased (the bias converges asymptotically to zero). On the other hand, the variance of the ordinary importance-sampling estimator is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximate that we explore later.</p><p>The estimates of ordinary importance sampling will typical have infinite variance, and this can easily happen in off-policy learning when trajectories contains loops. A simple example is shown below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance_example.png" alt="infinite_variance_example"></p><p>There is only one nonterminal state $s$ and two action, <strong>end</strong> and <strong>back</strong>. The <strong>end</strong> action causes a deterministic transition to termination, whereas the <strong>back</strong> action transitions, with probability 0.9, back to $s$ or, with probability 0.1, on to termination. The rewards are +1 on latter transition and otherwise zero. Consider the target policy that always selects back. All episodes under this policy consist of some number (possibly zero) of transitions back to $s$ followed by termination with a reward and return of +1. Thus the value of $s$ under the target policy is 1. Suppose we are estimating this value from off-policy data using the behavior policy that selects <strong>end</strong> and <strong>back</strong> with equal probability.</p><p>The implementation details are as follows. We first define the two policies:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">ACTION_BACK = <span class="number">0</span></div><div class="line">ACTION_END = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># behavior policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicy</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># target policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetPolicy</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> ACTION_BACK</div></pre></td></tr></table></figure><p>Then we define how an episode runs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># one turn</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># track the action for importance ratio</span></div><div class="line">    trajectory = []</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        action = behaviorPolicy()</div><div class="line">        trajectory.append(action)</div><div class="line">        <span class="keyword">if</span> action == ACTION_END:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span>, trajectory</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.9</span>) == <span class="number">0</span>:</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span>, trajectory</div></pre></td></tr></table></figure><p>Now we start our off-policy (first-visit MC) learning process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Figure 5.5</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloSample</span><span class="params">()</span>:</span></div><div class="line">    runs = <span class="number">10</span></div><div class="line">    episodes = <span class="number">100000</span></div><div class="line">    axisX = np.log10(np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>))</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        sumOfRewards = [<span class="number">0</span>]</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            reward, trajectory = play()</div><div class="line">            <span class="keyword">if</span> trajectory[<span class="number">-1</span>] == ACTION_END:</div><div class="line">                importanceRatio = <span class="number">0</span> <span class="comment"># Because it is impossible on the target policy</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                importanceRatio = <span class="number">1.0</span> / pow(<span class="number">0.5</span>, len(trajectory))</div><div class="line">            sumOfRewards.append(sumOfRewards[<span class="number">-1</span>] + importanceRatio * reward)</div><div class="line">        <span class="keyword">del</span> sumOfRewards[<span class="number">0</span>]</div><div class="line">        estimations = np.asarray(sumOfRewards) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">        plt.plot(axisX, estimations)</div><div class="line">    plt.xlabel(<span class="string">'Episodes (10^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Ordinary Importance Sampling'</span>)</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span></div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance.png" alt="inifinite_variance"></p><p>The correct estimate here is 1, and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite, and the estimates do not convergence to this value.</p><p>At last, we proposed two fancy algorithms, that is, the <strong>Incremental off-policy every-visit MC policy evaluation</strong> and the <strong>Off-policy every-visit MC control</strong>.</p><blockquote><p><strong>Incremental off-policy every-visit MC policy evaluation</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow$ arbitrary</p><p>​ $C(s,a) \leftarrow$ 0</p><p>​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy</p><p>​ $\pi(a|s) \leftarrow$ an arbitrary target policy</p><p>Repeat forever:</p><p>​ Generate an episode using $\mu$:</p><p>​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$</p><p>​ $G \leftarrow 0$</p><p>​ $W \leftarrow 1$</p><p>​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:</p><p>​ $G \leftarrow \gamma G + R_{t+1}$</p><p>​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$</p><p>​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$</p><p>​ $W \leftarrow W\frac{\pi(A_t | S_t)}{\mu(A_t | S_t)}$</p><p>​ If $W = 0$ then ExitForLoop</p><p><strong>Off-policy every-visit MC control (returns $\pi \approx \pi_{\star}$)</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow$ arbitrary</p><p>​ $C(s,a) \leftarrow$ 0</p><p>​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy</p><p>​ $\pi(s) \leftarrow$ an deterministic policy that is greedy with respect $Q$</p><p>Repeat forever:</p><p>​ Generate an episode using $\mu$:</p><p>​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$</p><p>​ $G \leftarrow 0$</p><p>​ $W \leftarrow 1$</p><p>​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:</p><p>​ $G \leftarrow \gamma G + R_{t+1}$</p><p>​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$</p><p>​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$</p><p>​ $\pi(S_t) \leftarrow \arg \max_{a}Q(S_t, a)$ (with ties broken consistently)</p><p>​ If $A_t \neq \pi(S_t)$ then ExitForLoop</p><p>​ $W \leftarrow W\frac{1}{\mu(A_t | S_t)}$</p></blockquote></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a> <a href="/tags/machine-learning/" rel="tag"># machine learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/05/31/Dynamic-Programming/" rel="next" title="Dynamic Programming"><i class="fa fa-chevron-left"></i> Dynamic Programming</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/06/14/Learning-to-act-by-predicting-the-future/" rel="prev" title="Learning to act by predicting the future">Learning to act by predicting the future <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">89</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">49</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>