<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="reinforcement learning,machine learning,TD,"><link rel="alternate" href="/atom.xml" title="Abracdabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic"><meta property="og:type" content="article"><meta property="og:title" content="Temporal-Difference Learning"><meta property="og:url" content="http://yoursite.com/2017/07/02/Temporal-Difference-Learning/index.html"><meta property="og:site_name" content="Abracdabra"><meta property="og:description" content="If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td_0.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td0_bg.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_td0.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_rmse.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/batch_update.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/example_6_4.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/sarsabg.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_gridworld.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_render.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_bg.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_world.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_show.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning_result.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png"><meta property="og:updated_time" content="2017-07-03T06:40:45.657Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Temporal-Difference Learning"><meta name="twitter:description" content="If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td_0.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/07/02/Temporal-Difference-Learning/"><title>Temporal-Difference Learning | Abracdabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracdabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/02/Temporal-Difference-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracdabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracdabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Temporal-Difference Learning</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-02T12:44:00+08:00">2017-07-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/02/Temporal-Difference-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/02/Temporal-Difference-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/02/Temporal-Difference-Learning/" class="leancloud_visitors" data-flag-title="Temporal-Difference Learning"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be <em>temporal-difference</em> (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome.</p><h3 id="TD-0"><a href="#TD-0" class="headerlink" title="TD(0)"></a><strong>TD(0)</strong></h3><p>Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environment is<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)],<br>$$<br>where $G_t$ is the <strong>actual return</strong> following time $t$. Let us call this method $constant\text{-}\alpha \ MC$. Notice that, if we are in a stationary environment (like <a href="https://ewanlee.github.io/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" target="_blank" rel="external">earlier</a>. For some reason, don’t use incremental implementation), the $\alpha$ is equals to $\frac{1}{N(S_t)}$. whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t+1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]<br>$$<br>immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_t$, whereas the target for the TD update is $R_{t+1} + \gamma V(S_{t+1})$. This TD method is called $TD(0)$, or <strong>one-step</strong> TD. The box below specifies TD(0) completely in procedural form.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td_0.png" alt="td_0"></p><p>TD(0)’s backup diagram is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td0_bg.png" alt="td0bg"></p><p>Because the TD(0) bases its update in part on an existing estimate, we say that it is a <em>bootstrapping</em> method, like DP. We know that<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [G_t \ | \ S_t=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} \ | \ S_t=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_t=s].<br>\end{align}<br>$$<br>Roughly speaking, Monte Carlo methods use an estimate of (3) as a target, whereas DP methods use an estimate of (5) as a target, The Monte Carlo target is an estimate because the expected value in (3) is not known; a sample return is used in place of the real expected return. The DP target is an estimate not because of the excepted value, which are assumed to be completely provided by a model of the environment (the environment is known for the DP methods), but because $v_{\pi}(S_{t+1})$ is not known and the current estimate, $V(S_{t+1})$, is used instead. The TD target is an estimate for both reasons.</p><p>Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \gamma V(S_{t+1})$. This quantity, called the <strong>TD error</strong>, arises in various forms throughout reinforcement learning:<br>$$<br>\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t).<br>$$<br>Notice that the TD error at each time is the error in the estimate <strong>made at that time</strong>. Because the TD error depends on the next state and the next reward, it is not actually available until one time step later. Also note that if the array $V$ does not change during the episode (as it does not in Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:<br>$$<br>\begin{align}<br>G_t - V(S_t) &amp;= R_{t+1} + \gamma G(S_{t+1}) - V(S_t) + \gamma V(S_{t+1} ) - \gamma V(S_{t+1}) \\<br>&amp;= \delta_t + \gamma (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(G_t-V(S_T)) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(0 -0) \\<br>&amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k.<br>\end{align}<br>$$<br>This identity is not exact if $V$ is updated during the episode (as it is in TD(0)), but if the step size is small then it may still hold approximately. Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning.</p><h4 id="Example-Random-walk"><a href="#Example-Random-walk" class="headerlink" title="Example: Random walk"></a><strong>Example: Random walk</strong></h4><p>In this example we empirically compare the prediction abilities of TD(0) and constant-$\alpha$ MC applied to the small Markov reward process shown in the upper part of the figure below. All episodes start in the center state, <strong>C</strong>, and the proceed either left or right by one state on each step, with equal probability. This behavior can be thought of as due to the combined effect of a fixed policy and an environment’s state-transition probabilities, but we do not care which; we are concerned only with predicting returns however they are generated. Episodes terminates on the right, a reward of +1 occurs; all other reward are zero. For example, a typical episode might consist of the following state-and-reward sequence: <strong>C, 0, B, 0, C, 0, D, 0, E, 1.</strong> Because this task is undiscounted, the true value of each state is the probability of terminating on the right if starting from that state. Thus, the true value of the center state is $v_{\pi}(\text{C}) = 0.5$. The true values of all the states, <strong>A</strong> through <strong>E</strong>, are $\frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}$, and $\frac{5}{6}$. In all cases the approximate value function was initialized to the intermediate value $V(s)=0.5$, for all $s$.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk.png" alt="random_walk"></p><p>Now, let us develop the codes to solve problem.</p><p>The first, we initialize some truth.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 0 is the left terminal state</span></div><div class="line"><span class="comment"># 6 is the right terminal state</span></div><div class="line"><span class="comment"># 1 ... 5 represents A ... E</span></div><div class="line">states = np.zeros(<span class="number">7</span>)</div><div class="line">states[<span class="number">1</span>:<span class="number">6</span>] = <span class="number">0.5</span></div><div class="line"><span class="comment"># For convenience, we assume all rewards are 0</span></div><div class="line"><span class="comment"># and the left terminal state has value 0, the right terminal state has value 1</span></div><div class="line"><span class="comment"># This trick has been used in Gambler's Problem</span></div><div class="line">states[<span class="number">6</span>] = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># set up true state values</span></div><div class="line">trueValue = np.zeros(<span class="number">7</span>)</div><div class="line">trueValue[<span class="number">1</span>:<span class="number">6</span>] = np.arange(<span class="number">1</span>, <span class="number">6</span>) / <span class="number">6.0</span></div><div class="line">trueValue[<span class="number">6</span>] = <span class="number">1</span></div><div class="line"></div><div class="line">ACTION_LEFT = <span class="number">0</span></div><div class="line">ACTION_RIGHT = <span class="number">1</span></div></pre></td></tr></table></figure><p>The below box is the TD(0) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">temporalDifference</span><span class="params">(states, alpha=<span class="number">0.1</span>, batch=False)</span>:</span></div><div class="line">    state = <span class="number">3</span></div><div class="line">    trajectory = [state]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        oldState = state</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == ACTION_LEFT:</div><div class="line">            state -= <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state += <span class="number">1</span></div><div class="line">        <span class="comment"># Assume all rewards are 0</span></div><div class="line">        reward = <span class="number">0</span></div><div class="line">        trajectory.append(state)</div><div class="line">        <span class="comment"># TD update</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> batch:</div><div class="line">            states[oldState] += alpha * (reward + states[state] - states[oldState])</div><div class="line">        <span class="keyword">if</span> state == <span class="number">6</span> <span class="keyword">or</span> state == <span class="number">0</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        rewards.append(reward)</div><div class="line">    <span class="keyword">return</span> trajectory, rewards</div></pre></td></tr></table></figure><p>And below box is the constant-$\alpha$ Monte Carlo algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarlo</span><span class="params">(states, alpha=<span class="number">0.1</span>, batch=False)</span>:</span></div><div class="line">    state = <span class="number">3</span></div><div class="line">    trajectory = [<span class="number">3</span>]</div><div class="line">    <span class="comment"># if end up with left terminal state, all returns are 0</span></div><div class="line">    <span class="comment"># if end up with right terminal state, all returns are 1</span></div><div class="line">    returns = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == ACTION_LEFT:</div><div class="line">            state -= <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state += <span class="number">1</span></div><div class="line">        trajectory.append(state)</div><div class="line">        <span class="keyword">if</span> state == <span class="number">6</span>:</div><div class="line">            returns = <span class="number">1.0</span></div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">elif</span> state == <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="keyword">break</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> batch:</div><div class="line">        <span class="keyword">for</span> state_ <span class="keyword">in</span> trajectory[:<span class="number">-1</span>]:</div><div class="line">            <span class="comment"># MC update</span></div><div class="line">            states[state_] += alpha * (returns - states[state_])</div><div class="line">    <span class="keyword">return</span> trajectory, [returns] * (len(trajectory) - <span class="number">1</span>)</div></pre></td></tr></table></figure><p>First of all, let us test the performance of the TD(0) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stateValue</span><span class="params">()</span>:</span></div><div class="line">    episodes = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]</div><div class="line">    currentStates = np.copy(states)</div><div class="line">    plt.figure(<span class="number">1</span>)</div><div class="line">    axisX = np.arange(<span class="number">0</span>, <span class="number">7</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, episodes[<span class="number">-1</span>] + <span class="number">1</span>):</div><div class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> episodes:</div><div class="line">            plt.plot(axisX, currentStates, label=str(i) + <span class="string">' episodes'</span>)</div><div class="line">        temporalDifference(currentStates)</div><div class="line">    plt.plot(axisX, trueValue, label=<span class="string">'true values'</span>)</div><div class="line">    plt.xlabel(<span class="string">'state'</span>)</div><div class="line">    plt.legend()</div><div class="line">    </div><div class="line">stateValue()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_td0.png" alt="random_walk_td0"></p><p>And then let us show the RMS error of the TD(0) algorithm and constant-$\alpha$ Monte Carlo algorithm, for various $\alpha$ values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMSError</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># I'm lazy here, so do not let same alpha value appear in both arrays</span></div><div class="line">    <span class="comment"># For example, if in TD you want to use alpha = 0.2, then in MC you can use alpha = 0.201</span></div><div class="line">    TDAlpha = [<span class="number">0.15</span>, <span class="number">0.1</span>, <span class="number">0.05</span>]</div><div class="line">    MCAlpha = [<span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.03</span>, <span class="number">0.04</span>]</div><div class="line">    episodes = <span class="number">100</span> + <span class="number">1</span></div><div class="line">    runs = <span class="number">100</span></div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    axisX = np.arange(<span class="number">0</span>, episodes)</div><div class="line">    <span class="keyword">for</span> alpha <span class="keyword">in</span> TDAlpha + MCAlpha:</div><div class="line">        totalErrors = np.zeros(episodes)</div><div class="line">        <span class="keyword">if</span> alpha <span class="keyword">in</span> TDAlpha:</div><div class="line">            method = <span class="string">'TD'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            method = <span class="string">'MC'</span></div><div class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">            errors = []</div><div class="line">            currentStates = np.copy(states)</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                errors.append(np.sqrt(np.sum(np.power(trueValue - currentStates, <span class="number">2</span>)) / <span class="number">5.0</span>))</div><div class="line">                <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                    temporalDifference(currentStates, alpha=alpha)</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    monteCarlo(currentStates, alpha=alpha)</div><div class="line">            totalErrors += np.asarray(errors)</div><div class="line">        totalErrors /= runs</div><div class="line">        plt.plot(axisX, totalErrors, label=method + <span class="string">', alpha='</span> + str(alpha))</div><div class="line">    plt.xlabel(<span class="string">'episodes'</span>)</div><div class="line">    plt.legend()</div><div class="line">    </div><div class="line">RMSError()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_rmse.png" alt="random_walk_error"></p><p>We can see, the TD method was consistently better than the MC method on this task.</p><p>Now, suppose that there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning method is to present the experience repeatedly until the method converges upon an answer. We call this <em>batch updating</em>.</p><h4 id="Example-Random-walk-under-batch-updating"><a href="#Example-Random-walk-under-batch-updating" class="headerlink" title="Example: Random walk under batch updating"></a><strong>Example: Random walk under batch updating</strong></h4><p>After each new episodes, all episodes seen so far were treated as a batch. They were repeatedly presented to the algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchUpdating</span><span class="params">(method, episodes, alpha=<span class="number">0.001</span>)</span>:</span></div><div class="line">    <span class="comment"># perform 100 independent runs</span></div><div class="line">    runs = <span class="number">100</span></div><div class="line">    totalErrors = np.zeros(episodes - <span class="number">1</span>)</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        currentStates = np.copy(states)</div><div class="line">        errors = []</div><div class="line">        <span class="comment"># track shown trajectories and reward/return sequences</span></div><div class="line">        trajectories = []</div><div class="line">        rewards = []</div><div class="line">        <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">1</span>, episodes):</div><div class="line">            print(<span class="string">'Run:'</span>, run, <span class="string">'Episode:'</span>, ep)</div><div class="line">            <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                trajectory_, rewards_ = temporalDifference(currentStates, batch=<span class="keyword">True</span>)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                trajectory_, rewards_ = monteCarlo(currentStates, batch=<span class="keyword">True</span>)</div><div class="line">            trajectories.append(trajectory_)</div><div class="line">            rewards.append(rewards_)</div><div class="line">            <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">                <span class="comment"># keep feeding our algorithm with trajectories seen so far until state value function converges</span></div><div class="line">                updates = np.zeros(<span class="number">7</span>)</div><div class="line">                <span class="keyword">for</span> trajectory_, rewards_ <span class="keyword">in</span> zip(trajectories, rewards):</div><div class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(trajectory_) - <span class="number">1</span>):</div><div class="line">                        <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                            updates[trajectory_[i]] += rewards_[i] + currentStates[trajectory_[i + <span class="number">1</span>]] - currentStates[trajectory_[i]]</div><div class="line">                        <span class="keyword">else</span>:</div><div class="line">                            updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]]</div><div class="line">                updates *= alpha</div><div class="line">                <span class="keyword">if</span> np.sum(np.abs(updates)) &lt; <span class="number">1e-3</span>:</div><div class="line">                    <span class="keyword">break</span></div><div class="line">                <span class="comment"># perform batch updating</span></div><div class="line">                currentStates += updates</div><div class="line">            <span class="comment"># calculate rms error</span></div><div class="line">            errors.append(np.sqrt(np.sum(np.power(currentStates - trueValue, <span class="number">2</span>)) / <span class="number">5.0</span>))</div><div class="line">        totalErrors += np.asarray(errors)</div><div class="line">    totalErrors /= runs</div><div class="line">    <span class="keyword">return</span> totalErrors</div></pre></td></tr></table></figure><p>Notice that the core codes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep feeding our algorithm with trajectories seen so far until state</span></div><div class="line">    <span class="comment"># value function converges</span></div><div class="line">    updates = np.zeros(<span class="number">7</span>)</div><div class="line">    <span class="keyword">for</span> trajectory_, rewards_ <span class="keyword">in</span> zip(trajectories, rewards):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(trajectory_) - <span class="number">1</span>):</div><div class="line">            <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                updates[trajectory_[i]] += rewards_[i] + \</div><div class="line">                    currentStates[trajectory_[i + <span class="number">1</span>]] - currentStates[trajectory_[i]]</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]]</div><div class="line">    updates *= alpha</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(updates)) &lt; <span class="number">1e-3</span>:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># perform batch updating</span></div><div class="line">    currentStates += updates</div></pre></td></tr></table></figure><p>Either TD methods or MC methods, the target is to minimize the TD error (or MC error, I say).</p><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/batch_update.png" alt="batch_update"></p><p>Under batch training, constant-$\alpha$ MC converges to value, $V(s)$, that are sample averages of the actual returns experienced after visiting each state $s$. These are optimal estimate in the sense that they minimize the mean-squared error from the actual returns in the training set. In this sense it is surprising that the batch TD method was able to perform better according to the root mean-squared error measure shown in the top figure. How is it that batch TD was able to perform better than this optimal methods? Consider the example in below box:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/example_6_4.png" alt="example6_4"></p><p>Example illustrates a general difference between the estimates founds by batch TD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the <strong>certainty-equivalence estimate</strong>.</p><h3 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a><strong>Sarsa</strong></h3><p>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\right]<br>$$</p><p>This update is done after every transition from a nonterminal state $S_t$. If $S_{t+1}$ is terminal, then $Q(S_{t+1}, A_{t+1})$ is defined as zero. This rule uses every element of the quintuple of events, $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that make up a transition from one state-action pair to the next. This quintuple gives rise to the name <em>Sarsa</em> for the algorithm. The backup diagram for Sarsa is as shown to the bottom.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/sarsabg.png" alt="sarsa_bg"></p><p>The general form of the Sarsa control algorithm is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png" alt="sarsa"></p><h4 id="Example-Windy-Gridworld"><a href="#Example-Windy-Gridworld" class="headerlink" title="Example: Windy Gridworld"></a><strong>Example: Windy Gridworld</strong></h4><p>The figure below is a standard grid-world, with start and goal states, but with one diﬀerence: there is a crosswind upward through the middle of the grid. The actions are the standard four—up, down,right, and left—but in the middle region the resultant next states are shifted upward by a “wind,” the strength of which varies from column to column. The strength of the wind is given below each column, in number of cells shifted upward. For example, if you are one cell to the right of the goal, then the action left takes you to the cell just above the goal. Let us treat this as an undiscounted episodic task, with constant rewards of −1 until the goal state is reached.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_gridworld.png" alt="windy_gridworld"></p><p>To demonstrate the problem clearly, we use the <a href="https://gym.openai.com/" target="_blank" rel="external">OpenAI gym</a> toolkit to develop the algorithm.</p><p>First of all, we need to define a environment (the windy grid world):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># represents every action as a integer</span></div><div class="line">UP = <span class="number">0</span></div><div class="line">RIGHT = <span class="number">1</span></div><div class="line">DOWN = <span class="number">2</span></div><div class="line">LEFT = <span class="number">3</span></div></pre></td></tr></table></figure><p>The environment is a class that inherit the gym default class <strong>discrete.DiscreteEnv</strong> (shows that the states are discrete):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">WindyGridworldEnv</span><span class="params">(discrete.DiscreteEnv)</span></span></div></pre></td></tr></table></figure><p>First we need to construct our world:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">    self.shape = (<span class="number">7</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="comment"># the number of all states</span></div><div class="line">    nS = np.prod(self.shape)</div><div class="line">    <span class="comment"># the number of all actions</span></div><div class="line">    nA = <span class="number">4</span></div><div class="line"></div><div class="line">    <span class="comment"># Wind strength</span></div><div class="line">    winds = np.zeros(self.shape)</div><div class="line">    winds[:,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>]] = <span class="number">1</span></div><div class="line">    winds[:,[<span class="number">6</span>,<span class="number">7</span>]] = <span class="number">2</span></div><div class="line"></div><div class="line">    <span class="comment"># Calculate transition probabilities</span></div><div class="line">    <span class="comment"># P is the transition matrix</span></div><div class="line">    P = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(nS):</div><div class="line">        position = np.unravel_index(s, self.shape)</div><div class="line">        P[s] = &#123; a : [] <span class="keyword">for</span> a <span class="keyword">in</span> range(nA) &#125;</div><div class="line">        P[s][UP] = self._calculate_transition_prob(position, [<span class="number">-1</span>, <span class="number">0</span>], winds)</div><div class="line">        P[s][RIGHT] = self._calculate_transition_prob(position, [<span class="number">0</span>, <span class="number">1</span>], winds)</div><div class="line">        P[s][DOWN] = self._calculate_transition_prob(position, [<span class="number">1</span>, <span class="number">0</span>], winds)</div><div class="line">        P[s][LEFT] = self._calculate_transition_prob(position, [<span class="number">0</span>, <span class="number">-1</span>], winds)</div><div class="line"></div><div class="line">    <span class="comment"># We always start in state (3, 0)</span></div><div class="line">    isd = np.zeros(nS)</div><div class="line">    isd[np.ravel_multi_index((<span class="number">3</span>,<span class="number">0</span>), self.shape)] = <span class="number">1.0</span></div><div class="line"></div><div class="line">    super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)</div></pre></td></tr></table></figure><p>This is natural, uh? Notice that there is a method called <strong>_calculate_transition_prob</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_calculate_transition_prob</span><span class="params">(self, current, delta, winds)</span>:</span></div><div class="line">        new_position = np.array(current) + np.array(delta) + np.array([<span class="number">-1</span>, <span class="number">0</span>]) * winds[tuple(current)]</div><div class="line">        new_position = self._limit_coordinates(new_position).astype(int)</div><div class="line">        new_state = np.ravel_multi_index(tuple(new_position), self.shape)</div><div class="line">        is_done = tuple(new_position) == (<span class="number">3</span>, <span class="number">7</span>)</div><div class="line">        <span class="keyword">return</span> [(<span class="number">1.0</span>, new_state, <span class="number">-1.0</span>, is_done)]</div></pre></td></tr></table></figure><p>and <strong>_limit_corrdinates</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_limit_coordinates</span><span class="params">(self, coord)</span>:</span></div><div class="line">    coord[<span class="number">0</span>] = min(coord[<span class="number">0</span>], self.shape[<span class="number">0</span>] - <span class="number">1</span>)</div><div class="line">    coord[<span class="number">0</span>] = max(coord[<span class="number">0</span>], <span class="number">0</span>)</div><div class="line">    coord[<span class="number">1</span>] = min(coord[<span class="number">1</span>], self.shape[<span class="number">1</span>] - <span class="number">1</span>)</div><div class="line">    coord[<span class="number">1</span>] = max(coord[<span class="number">1</span>], <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> coord</div></pre></td></tr></table></figure><p>It is worth to mention that the default gym environment class has some useful parameters: <strong>nS</strong>, <strong>nA</strong>, <strong>P</strong> and <strong>is_done</strong>. nS is the total number of states and nA is the total number of actions (here assume all states only could take the same fixed actions). P is the state transition matrix, the default environment class has a <strong>step</strong> method (accept a parameter <strong>action</strong>) that could generates episode automatically according the P and is_done that represents whether a state is terminal state or not.</p><p>Finally, we define a output method for pretty show the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_render</span><span class="params">(self, mode=<span class="string">'human'</span>, close=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> close:</div><div class="line">        <span class="keyword">return</span></div><div class="line"></div><div class="line">    outfile = StringIO() <span class="keyword">if</span> mode == <span class="string">'ansi'</span> <span class="keyword">else</span> sys.stdout</div><div class="line"></div><div class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(self.nS):</div><div class="line">        position = np.unravel_index(s, self.shape)</div><div class="line">        <span class="comment"># print(self.s)</span></div><div class="line">        <span class="keyword">if</span> self.s == s:</div><div class="line">            output = <span class="string">" x "</span></div><div class="line">        <span class="keyword">elif</span> position == (<span class="number">3</span>,<span class="number">7</span>):</div><div class="line">            output = <span class="string">" T "</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            output = <span class="string">" o "</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> position[<span class="number">1</span>] == <span class="number">0</span>:</div><div class="line">            output = output.lstrip()</div><div class="line">        <span class="keyword">if</span> position[<span class="number">1</span>] == self.shape[<span class="number">1</span>] - <span class="number">1</span>:</div><div class="line">            output = output.rstrip()</div><div class="line">            output += <span class="string">"\n"</span></div><div class="line"></div><div class="line">        outfile.write(output)</div><div class="line">    outfile.write(<span class="string">"\n"</span>)</div></pre></td></tr></table></figure><p>Then, let us test our model：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">env = WindyGridworldEnv()</div><div class="line"></div><div class="line">print(env.reset())</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">2</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_render.png" alt="windy_show"></p><p>Each state transition, the step method return a tuple <strong>(next_state, reward, is_done, some_extra_info)</strong>.</p><p>Next, we define the episodes generation policy:</p><p>def make_epsilon_greedy_policy(Q, epsilon, nA):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">Creates an epsilon-greedy policy based on a given Q-function and epsilon.</div><div class="line"></div><div class="line">Args:</div><div class="line">    Q: A dictionary that maps from state -&gt; action-values.</div><div class="line">        Each value is a numpy array of length nA (see below)</div><div class="line">    epsilon: The probability to select a random action . float between 0 and 1.</div><div class="line">    nA: Number of actions in the environment.</div><div class="line"></div><div class="line">Returns:</div><div class="line">    A function that takes the observation as an argument and returns</div><div class="line">    the probabilities for each action in the form of a numpy array of length nA.</div><div class="line"></div><div class="line">"""</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></div><div class="line">    A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">    best_action = np.argmax(Q[observation])</div><div class="line">    A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">    <span class="keyword">return</span> A</div><div class="line"><span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Now, let us implement the sarsa algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sarsa</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, alpha=<span class="number">0.5</span>, epsilon=<span class="number">0.1</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        alpha: TD learning rate.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A tuple (Q, stats).</div><div class="line">        Q is the optimal action-value function, a dictionary mapping state -&gt; action values.</div><div class="line">        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># The final action-value function.</span></div><div class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></div><div class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</div><div class="line">    </div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))</div><div class="line"></div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</div><div class="line">    </div><div class="line"></div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode + <span class="number">1</span>, num_episodes), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Implement this!</span></div><div class="line">        state = env.reset()</div><div class="line">        action_probs = policy(state)</div><div class="line">        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">        </div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">            next_state, reward, is_done, _ = env.step(action)</div><div class="line">            next_action_probs = policy(next_state)</div><div class="line">            </div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)</div><div class="line">            Q[state][action] += alpha * (reward + discount_factor * Q[next_state][next_action] - Q[state][action])</div><div class="line">            </div><div class="line">            <span class="keyword">if</span> is_done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">            </div><div class="line">            state = next_state</div><div class="line">            action = next_action</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Q, stats</div></pre></td></tr></table></figure><p>For understand easily, we put the pesudo-code here again:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png" alt="sarsa"></p><p>The results (with $\varepsilon=0.1,\ \alpha=0.5$) are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png" alt="sarsa_result"></p><p>The increasing slope (bottom figure) of the graph shows that the goal is reached more and more quickly over time. Note that Monte Carlo methods cannot easily be used on this task because termination is not guaranteed for all policies. If a policy was ever found that caused the agent to stay in the same state, then the next episode would never end. Step-by-step learning methods such as Sarsa do not have this problem because they quickly learn <strong>during the episode</strong> that such<br>policies are poor, and switch to something else.</p><h3 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h3><p>One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning, defined by<br>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)\right]<br>$$<br>The algorithm is shown in procedural form in the box below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning.png" alt="q_learning"></p><p>And below is the backup diagram:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_bg.png" alt="q_bg"></p><h4 id="Example-Cliff-Walking"><a href="#Example-Cliff-Walking" class="headerlink" title="Example: Cliff Walking"></a>Example: Cliff Walking</h4><p>This grid world example compares Sarsa and Q-learning, highlighting the difference between on-policy (Sarsa) and off-policy (Q-learning) methods. Consider the grid world shown in the figure below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_world.png" alt="cliff_world"></p><p>The same as earlier, we define the environment first. But the new environment just changes a little, so we just paste the code <a href="https://github.com/ewanlee/reinforcement-learning/blob/master/lib/envs/cliff_walking.py" target="_blank" rel="external">here</a>.</p><p>Let us test the environment first:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">env = CliffWalkingEnv()</div><div class="line"></div><div class="line">print(env.reset())</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">0</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">2</span>))</div><div class="line">env.render()</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_show.png" alt="cliff_walk_show"></p><p>Not bad.</p><p>Then, let us develop the Q-learning algorithm (the episodes generation policy is not change):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_learning</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, alpha=<span class="number">0.5</span>, epsilon=<span class="number">0.1</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy</div><div class="line">    while following an epsilon-greedy policy</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        alpha: TD learning rate.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A tuple (Q, episode_lengths).</div><div class="line">        Q is the optimal action-value function, a dictionary mapping state -&gt; action values.</div><div class="line">        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># The final action-value function.</span></div><div class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></div><div class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))    </div><div class="line">    </div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode + <span class="number">1</span>, num_episodes), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Reset the environment and pick the first action</span></div><div class="line">        state = env.reset()</div><div class="line">        </div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="comment"># total_reward = 0.0</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">            </div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            action_probs = policy(state)</div><div class="line">            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            next_state, reward, done, _ = env.step(action)</div><div class="line"></div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            <span class="comment"># TD Update</span></div><div class="line">            best_next_action = np.argmax(Q[next_state])    </div><div class="line">            td_target = reward + discount_factor * Q[next_state][best_next_action]</div><div class="line">            td_delta = td_target - Q[state][action]</div><div class="line">            Q[state][action] += alpha * td_delta</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">                </div><div class="line">            state = next_state</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Q, stats</div></pre></td></tr></table></figure><p>Results ($\varepsilon=0.1$) are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning_result.png" alt="q_learning_result"></p><p>For compare convenience, we put the result of Sarsa here again:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png" alt="sarsa_result"></p><p>We can see, for average, After an initial transient, Q-learning learns values for the optimal policy, that which travels right along the edge of the cliﬀ. Unfortunately, this results in its occasionally falling oﬀ the cliﬀ because of the ε-greedy action selection. Sarsa, on the other hand, takes the action selection into account and learns the longer but safer path through the upper part of the<br>grid. Although Q-learning actually learns the values of the optimal policy, its online performance is worse than that of Sarsa, which learns the roundabout policy. Of course, if ε were gradually reduced, then both methods would asymptotically converge to the optimal policy.</p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a> <a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/TD/" rel="tag"># TD</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/06/30/Reinforcement-Learning-Resources/" rel="next" title="Reinforcement Learning Resources"><i class="fa fa-chevron-left"></i> Reinforcement Learning Resources</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">73</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">44</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#TD-0"><span class="nav-number">1.</span> <span class="nav-text">TD(0)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Example-Random-walk"><span class="nav-number">1.1.</span> <span class="nav-text">Example: Random walk</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Example-Random-walk-under-batch-updating"><span class="nav-number">1.2.</span> <span class="nav-text">Example: Random walk under batch updating</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sarsa"><span class="nav-number">2.</span> <span class="nav-text">Sarsa</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Example-Windy-Gridworld"><span class="nav-number">2.1.</span> <span class="nav-text">Example: Windy Gridworld</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Q-learning"><span class="nav-number">3.</span> <span class="nav-text">Q-learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Example-Cliff-Walking"><span class="nav-number">3.1.</span> <span class="nav-text">Example: Cliff Walking</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/07/02/Temporal-Difference-Learning/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>