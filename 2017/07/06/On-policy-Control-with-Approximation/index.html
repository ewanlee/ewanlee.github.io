<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="reinforcement learning,machine learning,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a fini"><meta property="og:type" content="article"><meta property="og:title" content="On-policy Control with Approximation"><meta property="og:url" content="http://yoursite.com/2017/07/06/On-policy-Control-with-Approximation/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a fini"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/episode-semi-grad-sarsa.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mountain-car.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/n-step-sg-sarsa.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-n.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha-n.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-gym-test.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-ql-gym.png"><meta property="og:updated_time" content="2017-07-06T17:06:06.685Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="On-policy Control with Approximation"><meta name="twitter:description" content="In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a fini"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/episode-semi-grad-sarsa.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/07/06/On-policy-Control-with-Approximation/"><title>On-policy Control with Approximation | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/06/On-policy-Control-with-Approximation/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">On-policy Control with Approximation</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-06T17:41:34+08:00">2017-07-06 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/06/On-policy-Control-with-Approximation/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/06/On-policy-Control-with-Approximation/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/06/On-policy-Control-with-Approximation/" class="leancloud_visitors" data-flag-title="On-policy Control with Approximation"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a finite-dimensional weight vector.</p><h3 id="Episodic-Semi-gradient-Control"><a href="#Episodic-Semi-gradient-Control" class="headerlink" title="Episodic Semi-gradient Control"></a>Episodic Semi-gradient Control</h3><p>The general gradient-descent update for action-value prediction is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>For example, the update for the one-step Sarsa method is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t)- \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>We call this method <strong>episode semi-gradient one-step sarsa</strong>.</p><p>To form control methods, we need to couple such action-value prediction methods with techniques for policy improvement and action selection. Suitable techniques applicable to continuous actions, or to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution. On the other hand, if the action set is discrete and not too large, then we can use the techniques already developed in previous chapters.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/episode-semi-grad-sarsa.png" alt="episode-semi-grad-sarsa"></p><h4 id="Example-Mountain-Car-Task"><a href="#Example-Mountain-Car-Task" class="headerlink" title="Example: Mountain-Car Task"></a>Example: Mountain-Car Task</h4><p>Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram in the below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mountain-car.png" alt="mountain-car"></p><p>The diﬃculty is that gravity is stronger than the car’s engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to ﬁrst move away from the goal and up the opposite slope on the left. Then, by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way. This is a simple example of a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. Many control methodologies have great diﬃculties with tasks of this kind unless explicitly aided by a human designer.</p><p>The reward in this problem is −1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (−1), and zero throttle (0). The car moves according to a simpliﬁed physics. Its position, $x_t$, and velocity, $\dot{x}_t$, are updated by<br>$$<br>\begin{align}<br>x_{t+1} &amp;\doteq bound \big[x_t + \dot{x}_{t+1} \big] \\<br>\dot{x}_{t+1} &amp;\doteq bound \big[\dot{x}_t + 0.001 A_t - 0.0025 \cos(3x_t) \big],<br>\end{align}<br>$$<br>where the bound operation enforces $-1.2 \leq x_{t+1} \leq 0.5 \;\; \text{and} \; -0.07 \leq \dot{x}_{t+1} \leq 0.07$. In addition, when $x_{t+1}$ reached the left bound, $\dot{x}_{t+1}$ was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position $x_t \in [−0.6, −0.4)$ and zero velocity. To convert the two continuous state variables to binary features, we used grid-tilings.</p><p>First of all, we define the environment of this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all possible actions</span></div><div class="line">ACTION_REVERSE = <span class="number">-1</span></div><div class="line">ACTION_ZERO = <span class="number">0</span></div><div class="line">ACTION_FORWARD = <span class="number">1</span></div><div class="line"><span class="comment"># order is important</span></div><div class="line">ACTIONS = [ACTION_REVERSE, ACTION_ZERO, ACTION_FORWARD]</div><div class="line"></div><div class="line"><span class="comment"># bound for position and velocity</span></div><div class="line">POSITION_MIN = <span class="number">-1.2</span></div><div class="line">POSITION_MAX = <span class="number">0.5</span></div><div class="line">VELOCITY_MIN = <span class="number">-0.07</span></div><div class="line">VELOCITY_MAX = <span class="number">0.07</span></div><div class="line"></div><div class="line"><span class="comment"># use optimistic initial value, so it's ok to set epsilon to 0</span></div><div class="line">EPSILON = <span class="number">0</span></div></pre></td></tr></table></figure><p>After take an action, we transition to a new state and get a reward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take an @action at @position and @velocity</span></div><div class="line"><span class="comment"># @return: new position, new velocity, reward (always -1)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(position, velocity, action)</span>:</span></div><div class="line">    newVelocity = velocity + <span class="number">0.001</span> * action - <span class="number">0.0025</span> * np.cos(<span class="number">3</span> * position)</div><div class="line">    newVelocity = min(max(VELOCITY_MIN, newVelocity), VELOCITY_MAX)</div><div class="line">    newPosition = position + newVelocity</div><div class="line">    newPosition = min(max(POSITION_MIN, newPosition), POSITION_MAX)</div><div class="line">    reward = <span class="number">-1.0</span></div><div class="line">    <span class="keyword">if</span> newPosition == POSITION_MIN:</div><div class="line">        newVelocity = <span class="number">0.0</span></div><div class="line">    <span class="keyword">return</span> newPosition, newVelocity, reward</div></pre></td></tr></table></figure><p>The $\varepsilon$-greedy policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get action at @position and @velocity based on epsilon greedy policy and @valueFunction</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(position, velocity, valueFunction)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(ACTIONS)</div><div class="line">    values = []</div><div class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">        values.append(valueFunction.value(position, velocity, action))</div><div class="line">    <span class="keyword">return</span> np.argmax(values) - <span class="number">1</span></div></pre></td></tr></table></figure><p>We need map out continuous state to discrete state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># wrapper class for state action value function</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValueFunction</span>:</span></div><div class="line">    <span class="comment"># In this example I use the tiling software instead of implementing standard tiling by myself</span></div><div class="line">    <span class="comment"># One important thing is that tiling is only a map from (state, action) to a series of indices</span></div><div class="line">    <span class="comment"># It doesn't matter whether the indices have meaning, only if this map satisfy some property</span></div><div class="line">    <span class="comment"># View the following webpage for more information</span></div><div class="line">    <span class="comment"># http://incompleteideas.net/sutton/tiles/tiles3.html</span></div><div class="line">    <span class="comment"># @maxSize: the maximum # of indices</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, stepSize, numOfTilings=<span class="number">8</span>, maxSize=<span class="number">2048</span>)</span>:</span></div><div class="line">        self.maxSize = maxSize</div><div class="line">        self.numOfTilings = numOfTilings</div><div class="line"></div><div class="line">        <span class="comment"># divide step size equally to each tiling</span></div><div class="line">        self.stepSize = stepSize / numOfTilings</div><div class="line"></div><div class="line">        self.hashTable = IHT(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># weight for each tile</span></div><div class="line">        self.weights = np.zeros(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># position and velocity needs scaling to satisfy the tile software</span></div><div class="line">        self.positionScale = self.numOfTilings / (POSITION_MAX - POSITION_MIN)</div><div class="line">        self.velocityScale = self.numOfTilings / (VELOCITY_MAX - VELOCITY_MIN)</div><div class="line"></div><div class="line">    <span class="comment"># get indices of active tiles for given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getActiveTiles</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="comment"># I think positionScale * (position - position_min) would be a good normalization.</span></div><div class="line">        <span class="comment"># However positionScale * position_min is a constant, so it's ok to ignore it.</span></div><div class="line">        activeTiles = tiles(self.hashTable, self.numOfTilings,</div><div class="line">                            [self.positionScale * position, self.velocityScale * velocity],</div><div class="line">                            [action])</div><div class="line">        <span class="keyword">return</span> activeTiles</div><div class="line"></div><div class="line">    <span class="comment"># estimate the value of given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="keyword">if</span> position == POSITION_MAX:</div><div class="line">            <span class="keyword">return</span> <span class="number">0.0</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        <span class="keyword">return</span> np.sum(self.weights[activeTiles])</div><div class="line"></div><div class="line">    <span class="comment"># learn with given state, action and target</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, position, velocity, action, target)</span>:</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        estimation = np.sum(self.weights[activeTiles])</div><div class="line">        delta = self.stepSize * (target - estimation)</div><div class="line">        <span class="keyword">for</span> activeTile <span class="keyword">in</span> activeTiles:</div><div class="line">            self.weights[activeTile] += delta</div><div class="line"></div><div class="line">    <span class="comment"># get # of steps to reach the goal under current state value function</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">costToGo</span><span class="params">(self, position, velocity)</span>:</span></div><div class="line">        costs = []</div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">            costs.append(self.value(position, velocity, action))</div><div class="line">        <span class="keyword">return</span> -np.max(costs)</div></pre></td></tr></table></figure><p>Because the one-step semi-gradient sarsa is a special case of n-step, so we develop the n-step algorithm<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \; n \geq 1,0 \leq t \leq T-n.<br>$$<br>The n-step equation is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ G_{t:t+n}- \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}), \;\;\; 0 \leq t \leq T.<br>$$<br>Complete pseudocode is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/n-step-sg-sarsa.png" alt="n-step-sg-sarsa"></p><p>So the code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># semi-gradient n-step Sarsa</span></div><div class="line"><span class="comment"># @valueFunction: state value function to learn</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">semiGradientNStepSarsa</span><span class="params">(valueFunction, n=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="comment"># start at a random position around the bottom of the valley</span></div><div class="line">    currentPosition = np.random.uniform(<span class="number">-0.6</span>, <span class="number">-0.4</span>)</div><div class="line">    <span class="comment"># initial velocity is 0</span></div><div class="line">    currentVelocity = <span class="number">0.0</span></div><div class="line">    <span class="comment"># get initial action</span></div><div class="line">    currentAction = getAction(currentPosition, currentVelocity, valueFunction)</div><div class="line"></div><div class="line">    <span class="comment"># track previous position, velocity, action and reward</span></div><div class="line">    positions = [currentPosition]</div><div class="line">    velocities = [currentVelocity]</div><div class="line">    actions = [currentAction]</div><div class="line">    rewards = [<span class="number">0.0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># take current action and go to the new state</span></div><div class="line">            newPostion, newVelocity, reward = takeAction(currentPosition, currentVelocity, currentAction)</div><div class="line">            <span class="comment"># choose new action</span></div><div class="line">            newAction = getAction(newPostion, newVelocity, valueFunction)</div><div class="line"></div><div class="line">            <span class="comment"># track new state and action</span></div><div class="line">            positions.append(newPostion)</div><div class="line">            velocities.append(newVelocity)</div><div class="line">            actions.append(newAction)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newPostion == POSITION_MAX:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += rewards[t]</div><div class="line">            <span class="comment"># add estimated state action value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += valueFunction.value(positions[updateTime + n],</div><div class="line">                                               velocities[updateTime + n],</div><div class="line">                                               actions[updateTime + n])</div><div class="line">            <span class="comment"># update the state value function</span></div><div class="line">            <span class="keyword">if</span> positions[updateTime] != POSITION_MAX:</div><div class="line">                valueFunction.learn(positions[updateTime], velocities[updateTime], actions[updateTime], returns)</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentPosition = newPostion</div><div class="line">        currentVelocity = newVelocity</div><div class="line">        currentAction = newAction</div><div class="line"></div><div class="line">    <span class="keyword">return</span> time</div></pre></td></tr></table></figure><p>Next, we use the method mentioned earlier to solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">episodes = <span class="number">9000</span></div><div class="line">targetEpisodes = [<span class="number">1</span><span class="number">-1</span>, <span class="number">12</span><span class="number">-1</span>, <span class="number">104</span><span class="number">-1</span>, <span class="number">1000</span><span class="number">-1</span>, episodes - <span class="number">1</span>]</div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alpha = <span class="number">0.3</span></div><div class="line">valueFunction = ValueFunction(alpha, numOfTilings)</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">    print(<span class="string">'episode:'</span>, episode)</div><div class="line">    semiGradientNStepSarsa(valueFunction)</div><div class="line">    <span class="keyword">if</span> episode <span class="keyword">in</span> targetEpisodes:</div><div class="line">        prettyPrint(valueFunction, <span class="string">'Episode: '</span> + str(episode + <span class="number">1</span>))</div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa.png" alt="mcar-sg-sarsa"></p><p>The result shows what typically happens while learning to solve this task with this form of function approximation. Shown is the negative of the value function (the cost-to-go function) learned on a single run. The initial action values were all zero, which was optimistic (all true values are negative in this task), causing extensive exploration to occur even though the exploration parameter, $\varepsilon$, was 0. All the states visited frequently are valued worse than unexplored states, because the actual rewards have been worse than what was (unrealistically) expected. This continually drives the agent away from wherever it has been, to explore new states, until a solution is found.</p><p>Next, let us test the performance of various step size (learning rate).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'alpha:'</span>, alphas[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'alpha = '</span>+str(alphas[i])+<span class="string">'/'</span>+str(numOfTilings))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha.png" alt="mcar-sg-sarsa-var-alpha"></p><p>And then, let us test the performance of one-step sarsa and multi-step sarsa on the Mountain Car task:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.5</span>, <span class="number">0.3</span>]</div><div class="line">nSteps = [<span class="number">1</span>, <span class="number">8</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nSteps[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index], nSteps[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-n.png" alt="mcar-sg-sarsa-var-n"></p><p>Finally, let me shows the results of a more detailed study of the eﬀect of the parameters $\alpha$ and $n$ on the rate of learning on this task.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">alphas = np.arange(<span class="number">0.25</span>, <span class="number">1.75</span>, <span class="number">0.25</span>)</div><div class="line">nSteps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">5</span>))</div><div class="line">episodes = <span class="number">50</span></div><div class="line">runs = <span class="number">5</span></div><div class="line"></div><div class="line">truncateStep = <span class="number">300</span></div><div class="line">steps = np.zeros((len(nSteps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> nStepIndex, nStep <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(nSteps)), nSteps):</div><div class="line">        <span class="keyword">for</span> alphaIndex, alpha <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(alphas)), alphas):</div><div class="line">            <span class="keyword">if</span> (nStep == <span class="number">8</span> <span class="keyword">and</span> alpha &gt; <span class="number">1</span>) <span class="keyword">or</span> \</div><div class="line">                    (nStep == <span class="number">16</span> <span class="keyword">and</span> alpha &gt; <span class="number">0.75</span>):</div><div class="line">                <span class="comment"># In these cases it won't converge, so ignore them</span></div><div class="line">                steps[nStepIndex, alphaIndex] += truncateStep * episodes</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            valueFunction = ValueFunction(alpha)</div><div class="line">            <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nStep, <span class="string">'alpha:'</span>, alpha, <span class="string">'episode:'</span>, episode)</div><div class="line">                step = semiGradientNStepSarsa(valueFunction, nStep)</div><div class="line">                steps[nStepIndex, alphaIndex] += step</div><div class="line"><span class="comment"># average over independent runs and episodes</span></div><div class="line">steps /= runs * episodes</div><div class="line"><span class="comment"># truncate high values for better display</span></div><div class="line">steps[steps &gt; truncateStep] = truncateStep</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(nSteps)):</div><div class="line">    plt.plot(alphas, steps[i, :], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha * number of tilings(8)'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha-n.png" alt="mcar-sg-sarsa-var-alpha-n"></p><h3 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h3><h4 id="Use-OpenAI-gym"><a href="#Use-OpenAI-gym" class="headerlink" title="Use OpenAI gym"></a>Use OpenAI gym</h4><p>Now, let us use the OpenAI gym toolkit to simply our Mountain Car task. As we know, we need to define the environment of task before develop the detail algorithm. It’s easy to make mistakes with some detail. So it is fantastic if we do not need to care about that. The gym toolkit help us to define the environment. Such as the Mountain Car task, there is a build-in model in the gym toolkit. We just need to write one row code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env = gym.envs.make(<span class="string">"MountainCar-v0"</span>)</div></pre></td></tr></table></figure><p>That is amazing!</p><p>We also can test the environment very convenience and get a pretty good user graphic:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">env.reset()</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line"><span class="comment"># for x in range(10000):</span></div><div class="line"><span class="comment">#     env.step(0)</span></div><div class="line"><span class="comment">#     plt.figure()</span></div><div class="line"><span class="comment">#     plt.imshow(env.render(mode='rgb_array'))  </span></div><div class="line">[env.step(<span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)]</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">env.render(close=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>These codes will return the result as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-gym-test.png" alt="mcar-gym-test"></p><p>Bravo~</p><p>Now, let us solve the task by to use the Q-Learning algorithm. Meanwhile, we will use the RBF kernel to construct the features and use the SGDRegressor gradient-descent method of scikit-learn package to update $\mathbf{w}$.</p><p>First of all, we need to normalized our features to accelerate the convergence speed and use the RBF kernel to reconstruct our feature space. The both methods (Normalization and Reconstruction) need to use some training set to train a model. Then we use this model to transform our raw data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Feature Preprocessing: Normalize to zero mean and unit variance</span></div><div class="line"><span class="comment"># We use a few samples from the observation space to do this</span></div><div class="line">observation_examples = np.array([env.observation_space.sample() <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)])</div><div class="line">scaler = sklearn.preprocessing.StandardScaler()</div><div class="line">scaler.fit(observation_examples)</div><div class="line"></div><div class="line"><span class="comment"># Used to converte a state to a featurizes represenation.</span></div><div class="line"><span class="comment"># We use RBF kernels with different variances to cover different parts of the space</span></div><div class="line">featurizer = sklearn.pipeline.FeatureUnion([</div><div class="line">        (<span class="string">"rbf1"</span>, RBFSampler(gamma=<span class="number">5.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf2"</span>, RBFSampler(gamma=<span class="number">2.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf3"</span>, RBFSampler(gamma=<span class="number">1.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf4"</span>, RBFSampler(gamma=<span class="number">0.5</span>, n_components=<span class="number">100</span>))</div><div class="line">        ])</div><div class="line">featurizer.fit(scaler.transform(observation_examples))</div></pre></td></tr></table></figure><p>Next, we define a class named Estimator to simply the gradient descent process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Estimator</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Value Function approximator. </div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># We create a separate model for each action in the environment's</span></div><div class="line">        <span class="comment"># action space. Alternatively we could somehow encode the action</span></div><div class="line">        <span class="comment"># into the features, but this way it's easier to code up.</span></div><div class="line">        self.models = []</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(env.action_space.n):</div><div class="line">            model = SGDRegressor(learning_rate=<span class="string">"constant"</span>)</div><div class="line">            <span class="comment"># We need to call partial_fit once to initialize the model</span></div><div class="line">            <span class="comment"># or we get a NotFittedError when trying to make a prediction</span></div><div class="line">            <span class="comment"># This is quite hacky.</span></div><div class="line">            model.partial_fit([self.featurize_state(env.reset())], [<span class="number">0</span>])</div><div class="line">            self.models.append(model)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">featurize_state</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Returns the featurized representation for a state.</div><div class="line">        """</div><div class="line">        scaled = scaler.transform([state])</div><div class="line">        featurized = featurizer.transform(scaled)</div><div class="line">        <span class="keyword">return</span> featurized[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, s, a=None)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Makes value function predictions.</div><div class="line">        </div><div class="line">        Args:</div><div class="line">            s: state to make a prediction for</div><div class="line">            a: (Optional) action to make a prediction for</div><div class="line">            </div><div class="line">        Returns</div><div class="line">            If an action a is given this returns a single number as the prediction.</div><div class="line">            If no action is given this returns a vector or predictions for all actions</div><div class="line">            in the environment where pred[i] is the prediction for action i.</div><div class="line">            </div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> a:</div><div class="line">            <span class="keyword">return</span> np.array([m.predict([features])[<span class="number">0</span>] <span class="keyword">for</span> m <span class="keyword">in</span> self.models])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self.models[a].predict([features])[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, s, a, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Updates the estimator parameters for a given state and action towards</div><div class="line">        the target y.</div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        self.models[a].partial_fit([features], [y])</div></pre></td></tr></table></figure><p>We also need a $\varepsilon$-greedy policy to select action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(estimator, epsilon, nA)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        estimator: An estimator that returns q values for a given state</div><div class="line">        epsilon: The probability to select a random action . float between 0 and 1.</div><div class="line">        nA: Number of actions in the environment.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A function that takes the observation as an argument and returns</div><div class="line">        the probabilities for each action in the form of a numpy array of length nA.</div><div class="line">    </div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></div><div class="line">        A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">        q_values = estimator.predict(observation)</div><div class="line">        best_action = np.argmax(q_values)</div><div class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">        <span class="keyword">return</span> A</div><div class="line">    <span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Then we develop the Q-Learning method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_learning</span><span class="params">(env, estimator, num_episodes, discount_factor=<span class="number">1.0</span>, epsilon=<span class="number">0.1</span>, epsilon_decay=<span class="number">1.0</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm for fff-policy TD control using Function Approximation.</div><div class="line">    Finds the optimal greedy policy while following an epsilon-greedy policy.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        estimator: Action-Value function estimator</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">        epsilon_decay: Each episode, epsilon is decayed by this factor</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))    </div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        </div><div class="line">        <span class="comment"># The policy we're following</span></div><div class="line">        policy = make_epsilon_greedy_policy(</div><div class="line">            estimator, epsilon * epsilon_decay**i_episode, env.action_space.n)</div><div class="line">        </div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="comment"># Also print reward for last episode</span></div><div class="line">        last_reward = stats.episode_rewards[i_episode - <span class="number">1</span>]</div><div class="line">        sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Reset the environment and pick the first action</span></div><div class="line">        state = env.reset()</div><div class="line">        </div><div class="line">        <span class="comment"># Only used for SARSA, not Q-Learning</span></div><div class="line">        next_action = <span class="keyword">None</span></div><div class="line">        </div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">                        </div><div class="line">            <span class="comment"># Choose an action to take</span></div><div class="line">            <span class="comment"># If we're using SARSA we already decided in the previous step</span></div><div class="line">            <span class="keyword">if</span> next_action <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                action_probs = policy(state)</div><div class="line">                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                action = next_action</div><div class="line">            </div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            next_state, reward, done, _ = env.step(action)</div><div class="line">    </div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            <span class="comment"># TD Update</span></div><div class="line">            q_values_next = estimator.predict(next_state)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for Q-Learning</span></div><div class="line">            <span class="comment"># Q-Value TD Target</span></div><div class="line">            td_target = reward + discount_factor * np.max(q_values_next)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for SARSA TD Target for on policy-training:</span></div><div class="line">            <span class="comment"># next_action_probs = policy(next_state)</span></div><div class="line">            <span class="comment"># next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)             </span></div><div class="line">            <span class="comment"># td_target = reward + discount_factor * q_values_next[next_action]</span></div><div class="line">            </div><div class="line">            <span class="comment"># Update the function approximator using our target</span></div><div class="line">            estimator.update(state, action, td_target)</div><div class="line">            </div><div class="line">            print(<span class="string">"\rStep &#123;&#125; @ Episode &#123;&#125;/&#123;&#125; (&#123;&#125;)"</span>.format(t, i_episode + <span class="number">1</span>, num_episodes, last_reward), end=<span class="string">""</span>)</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">                </div><div class="line">            state = next_state</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> stats</div></pre></td></tr></table></figure><p>Run this method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">estimator = Estimator()</div><div class="line"><span class="comment"># Note: For the Mountain Car we don't actually need an epsilon &gt; 0.0</span></div><div class="line"><span class="comment"># because our initial estimate for all states is too "optimistic" which leads</span></div><div class="line"><span class="comment"># to the exploration of all states.</span></div><div class="line">stats = q_learning(env, estimator, <span class="number">100</span>, epsilon=<span class="number">0.0</span>)</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-ql-gym.png" alt="mcar-ql-gym"></p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a> <a href="/tags/machine-learning/" rel="tag"># machine learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/07/05/On-policy-Prediction-with-Approximation/" rel="next" title="On-policy Prediction with Approximation"><i class="fa fa-chevron-left"></i> On-policy Prediction with Approximation</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/07/07/Demystifying-Deep-Reinforcement-Learning/" rel="prev" title="Demystifying Deep Reinforcement Learning (Repost)">Demystifying Deep Reinforcement Learning (Repost) <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">78</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">46</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Episodic-Semi-gradient-Control"><span class="nav-number">1.</span> <span class="nav-text">Episodic Semi-gradient Control</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Example-Mountain-Car-Task"><span class="nav-number">1.1.</span> <span class="nav-text">Example: Mountain-Car Task</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Update"><span class="nav-number">2.</span> <span class="nav-text">Update</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Use-OpenAI-gym"><span class="nav-number">2.1.</span> <span class="nav-text">Use OpenAI gym</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/07/06/On-policy-Control-with-Approximation/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>