<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="deep learning,machine learning,reinforcement learning,deep reinforcement learning,DQN,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="200 lines of python code to demonstrate DQN with KerasOverviewThis project demonstrates how to use the Deep-Q Learning algorithm with Keras together to play FlappyBird.This article is intended to targ"><meta property="og:type" content="article"><meta property="og:title" content="Using Keras and Deep Q-Network to Play FlappyBird (Repost)"><meta property="og:url" content="http://yoursite.com/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="200 lines of python code to demonstrate DQN with KerasOverviewThis project demonstrates how to use the Deep-Q Learning algorithm with Keras together to play FlappyBird.This article is intended to targ"><meta property="og:image" content="https://yanpanlau.github.io/img/animation1.gif"><meta property="og:image" content="https://yanpanlau.github.io/img/bird.jpg"><meta property="og:image" content="https://yanpanlau.github.io/img/Convolution_schematic.gif"><meta property="og:image" content="https://yanpanlau.github.io/img/generic-taj-convmatrix-edge-detect.jpg"><meta property="og:updated_time" content="2017-07-06T20:34:48.861Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Using Keras and Deep Q-Network to Play FlappyBird (Repost)"><meta name="twitter:description" content="200 lines of python code to demonstrate DQN with KerasOverviewThis project demonstrates how to use the Deep-Q Learning algorithm with Keras together to play FlappyBird.This article is intended to targ"><meta name="twitter:image" content="https://yanpanlau.github.io/img/animation1.gif"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/"><title>Using Keras and Deep Q-Network to Play FlappyBird (Repost) | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Using Keras and Deep Q-Network to Play FlappyBird (Repost)</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-07T04:19:18+08:00">2017-07-07 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" class="leancloud_visitors" data-flag-title="Using Keras and Deep Q-Network to Play FlappyBird (Repost)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>200 lines of python code to demonstrate DQN with Keras</p><p><img src="https://yanpanlau.github.io/img/animation1.gif" alt="img"></p><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>This project demonstrates how to use the Deep-Q Learning algorithm with Keras together to play FlappyBird.</p><p>This article is intended to target newcomers who are interested in Reinforcement Learning.</p><h1 id="Installation-Dependencies"><a href="#Installation-Dependencies" class="headerlink" title="Installation Dependencies:"></a>Installation Dependencies:</h1><p>(Update : 13 March 2017, code and weight file has been updated to support latest version of tensorflow and keras)</p><ul><li>Python 2.7</li><li>Keras 1.0</li><li>pygame</li><li>scikit-image</li></ul><h1 id="How-to-Run"><a href="#How-to-Run" class="headerlink" title="How to Run?"></a>How to Run?</h1><p><strong>CPU only/TensorFlow</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><p><strong>GPU version (Theano)</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">THEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=0.2 python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><p><strong>lib.cnmem=0.2</strong> means you assign only 20% of the GPU’s memory to the program.</p><p><strong>If you want to train the network from beginning, delete “model.h5” and run qlearn.py -m “Train”</strong></p><h1 id="What-is-Deep-Q-Network"><a href="#What-is-Deep-Q-Network" class="headerlink" title="What is Deep Q-Network?"></a>What is Deep Q-Network?</h1><p>Deep Q-Network is a learning algorithm developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the algorithm is generic enough to play various games.</p><p>The following post is a must-read for those who are interested in deep reinforcement learning.</p><p><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">Demystifying Deep Reinforcement Learning</a></p><h1 id="Code-Explanation-in-details"><a href="#Code-Explanation-in-details" class="headerlink" title="Code Explanation (in details)"></a>Code Explanation (in details)</h1><p>Let’s go though the example in <strong>qlearn.py</strong>, line by line. If you familiar with Keras and DQN, you can skip this session</p><p>The code simply does the following:</p><ol><li>The code receives the Game Screen Input in the form of a pixel array</li><li>The code does some image pre-processing</li><li>The processed image will be fed into a neural network (Convolution Neural Network), and the network will then decide the best action to execute (Flap or Not Flap)</li><li>The network will be trained millions of times, via an algorithm called Q-learning, to maximize the future expected reward.</li></ol><h3 id="Game-Screen-Input"><a href="#Game-Screen-Input" class="headerlink" title="Game Screen Input"></a>Game Screen Input</h3><p>First of all, the FlappyBird is already written in Python via pygame, so here is the code snippet to access the FlappyBird API</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import wrapped_flappy_bird as game</div><div class="line">x_t1_colored, r_t, terminal = game_state.frame_step(a_t)</div></pre></td></tr></table></figure><p>The idea is quite simple, the input is <strong>a_t</strong> (0 represent don’t flap, 1 represent flap), the API will give you the next frame <strong>x_t1_colored</strong>, the <strong>reward</strong> (0.1 if alive, +1 if pass the pipe, -1 if die) and <strong>terminal</strong> is a boolean flag indicates whether the game is FINISHED or NOT. We also followed DeepMind suggestion to clip the reward between [-1,+1] to improve the stability. I have not yet get a chance to test out different reward functions but it would be an interesting exercise to see how the performance is changed with different reward functions.</p><p>Interesting readers can modify the reward function in <strong>game/wrapped_flappy_bird.py”, under the function **def frame_step(self, input_actions)</strong></p><h3 id="Image-pre-processing"><a href="#Image-pre-processing" class="headerlink" title="Image pre-processing"></a>Image pre-processing</h3><p><img src="https://yanpanlau.github.io/img/bird.jpg" alt="img"></p><p>In order to make the code train faster, it is vital to do some image processing. Here are the key elements:</p><ol><li>I first convert the color image into grayscale</li><li>I crop down the image size into 80x80 pixel</li><li>I stack 4 frames together before I feed into neural network.</li></ol><p>Why do I need to stack 4 frames together? This is one way for the model to be able to infer the velocity information of the bird.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x_t1 = skimage.color.rgb2gray(x_t1_colored)</div><div class="line">x_t1 = skimage.transform.resize(x_t1,(<span class="number">80</span>,<span class="number">80</span>))</div><div class="line">x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(<span class="number">0</span>, <span class="number">255</span>))</div><div class="line"></div><div class="line">x_t1 = x_t1.reshape(<span class="number">1</span>, <span class="number">1</span>, x_t1.shape[<span class="number">0</span>], x_t1.shape[<span class="number">1</span>])</div><div class="line">s_t1 = np.append(x_t1, s_t[:, :<span class="number">3</span>, :, :], axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p><strong>x_t1</strong> is a single frame with shape (1x1x80x80) and <strong>s_t1</strong> is the stacked frame with shape (1x4x80x80). You might ask, why the input dimension is (1x4x80x80) but not (4x80x80)? Well, it is a requirement in Keras so let’s stick with it.</p><p>Note: Some readers may ask what is <strong>axis=1</strong>? It means that when I stack the frames, I want to stack on the “2nd” dimension. i.e. I am stacking under (1x<strong>4</strong>x80x80), the 2nd index.</p><h3 id="Convolution-Neural-Network"><a href="#Convolution-Neural-Network" class="headerlink" title="Convolution Neural Network"></a>Convolution Neural Network</h3><p>Now, we can input the pre-processed screen into the neural network, which is a convolution neural network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildmodel</span><span class="params">()</span>:</span></div><div class="line">    print(<span class="string">"Now we build the model"</span>)</div><div class="line">    model = Sequential()</div><div class="line">    model.add(Convolution2D(<span class="number">32</span>, <span class="number">8</span>, <span class="number">8</span>, subsample=(<span class="number">4</span>,<span class="number">4</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>,input_shape=(img_channels,img_rows,img_cols)))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">4</span>, <span class="number">4</span>, subsample=(<span class="number">2</span>,<span class="number">2</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, subsample=(<span class="number">1</span>,<span class="number">1</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Flatten())</div><div class="line">    model.add(Dense(<span class="number">512</span>, init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Dense(<span class="number">2</span>,init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)))</div><div class="line">   </div><div class="line">    adam = Adam(lr=<span class="number">1e-6</span>)</div><div class="line">    model.compile(loss=<span class="string">'mse'</span>,optimizer=adam)</div><div class="line">    print(<span class="string">"We finish building the model"</span>)</div><div class="line">    <span class="keyword">return</span> model</div></pre></td></tr></table></figure><p>The exact architecture is following : The input to the neural network consists of an 4x80x80 images. The first hidden layer convolves 32 filters of 8 x 8 with stride 4 and applies ReLU activation function. The 2nd layer convolves a 64 filters of 4 x 4 with stride 2 and applies ReLU activation function. The 3rd layer convolves a 64 filters of 3 x 3 with stride 1 and applies ReLU activation function. The final hidden layer is fully-connected consisted of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action.</p><p>So wait, what is convolution? The easiest way to understand a convolution is by thinking of it as a sliding window function apply to a matrix. The following gif file should help to understand.</p><p><img src="https://yanpanlau.github.io/img/Convolution_schematic.gif" alt="Convolution with 3×3 Filter. Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution"></p><p>You might ask what’s the purpose the convolution? It actually help computer to learn higher features like edges and shapes. The example below shows how the edges are stand out after a convolution filter is applied</p><p><img src="https://yanpanlau.github.io/img/generic-taj-convmatrix-edge-detect.jpg" alt="Using Convolution to detect Edges"></p><p>For more details about Convolution in Neural Network, please read <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_blank" rel="external">Understanding Convolution Neural Networks for NLP</a></p><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>Keras makes it very easy to build convolution neural network. However, there are few things I would like to highlight</p><p>A) It is important to choose a right initialization method. I choose normal distribution with $\sigma=0.01$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)</div></pre></td></tr></table></figure><p>B) The ordering of the dimension is important, the default setting is 4x80x80 (Theano setting), so if your input is 80x80x4 (Tensorflow setting) then you are in trouble because the dimension is wrong. <strong>Alert</strong>: If your input dimension is 80x80x4 (Tensorflow setting) you need to set <strong>dim_ordering = tf</strong> (tf means tensorflow, th means theano)</p><p>C) In Keras, <strong>subsample=(2,2)</strong> means you down sample the image size from (80x80) to (40x40). In ML literature it is often called “stride”</p><p>D) We have used an adaptive learning algorithm called ADAM to do the optimization. The learning rate is <strong>1-e6</strong>.</p><p>Interested readers who want to learn more various learning algoithms please read below</p><p><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">An overview of gradient descent optimization algorithms</a></p><h3 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h3><p>Finally, we can using the Q-learning algorithm to train the neural network.</p><p>So, what is Q-learning? In Q-learning the most important thing is the Q-function : Q(s, a) representing the maximum discounted future reward when we perform action <strong>a</strong> in state <strong>s</strong>. <strong>Q(s, a)</strong> gives you an estimation of how good to choose an action <strong>a</strong> in state <strong>s</strong>.</p><p>REPEAT : <strong>Q(s, a)</strong> representing the maximum discounted future reward when we perform action <strong>a</strong> in state <strong>s</strong></p><p>You might ask 1) Why Q-function is useful? 2) How can I get the Q-function?</p><p>Let me give you an analogy of the Q-function: Suppose you are playing a difficult RPG game and you don’t know how to play it well. If you have bought a strategy guide, which is an instruction book that contain hints or complete solutions to a specific video game, then playing that video game is easy. You just follow the guidiance from the strategy book. Here, Q-function is similar to a strategy guide. Suppose you are in state <strong>s</strong> and you need to decide whether you take action <strong>a</strong> or <strong>b</strong>. If you have this magical Q-function, the answers become really simple – pick the action with highest Q-value!</p><p>$$<br>{\pi(s) = {argmax}_{a} Q(s,a)}<br>$$<br>Here, $\pi$ represents the policy, which you will often see in the ML literature.</p><p>How do we get the Q-function? That’s where Q-learning is coming from. Let me quickly derive here:</p><p>Define total future reward from time <strong>t</strong> onward<br>$$<br>R_t = r_t + r_{t+1} + r_{t+2} … + r_n<br>$$<br>But because our environment is stochastic, we can never be sure, the more future we go, the more uncertain. Therefore, it is common to use <strong>discount future reward</strong> instead<br>$$<br>R_t = r_t + \gamma r_{t+1} + \gamma^{2} r_{t+2} … + \gamma^{n-t} r_n<br>$$<br>which, can be written as<br>$$<br>R_t = r_t + \gamma \ast R_{t+1}<br>$$<br>Recall the definition of Q-function (maximum discounted future reward if we choose action <strong>a</strong> in state <strong>s</strong>)<br>$$<br>Q(s_t, a_t) = max R_{t+1}<br>$$<br>therefore, we can rewrite the Q-function as below<br>$$<br>Q(s, a) = r + \gamma \ast max_{a^{‘}} Q(s^{\prime}, a^{\prime})<br>$$<br>In plain English, it means maximum future reward for this state and action (s,a) is the immediate reward r <strong>plus</strong> maximum future reward for the next state $s^{\prime}$ , action $a^{\prime}$</p><p>We could now use an iterative method to solve for the Q-function. Given a transition $(s, a, r, s^{\prime})$ , we are going to convert this episode into training set for the network. i.e. We want $r + \gamma max_a Q (s,a)$ to be equal to $Q(s,a)$. You can think of finding a Q-value is a regession task now, I have a estimator $r + \gamma max_a Q (s,a)$ and a predictor $Q(s,a)$, I can define the mean squared error (MSE), or the loss function, as below:</p><p>Define a loss function<br>$$<br>L = [r + \gamma max_{a^{\prime}} Q (s^{\prime},a^{\prime}) - Q(s,a)]^{2}<br>$$<br>Given a transition $(s, a, r, s^{\prime})$, how can I optimize my Q-function such that it returns the smallest mean squared error loss? If L getting smaller, we know the Q-function is getting converged into the optimal value, which is our “strategy book”.</p><p>Now, you might ask, where is the role of the neural network? This is where the <strong>DEEP Q-Learning</strong> comes in. You recall that $Q(s,a)$, is a stategy book, which contains millions or trillions of states and actions if you display as a table. The idea of the DQN is that I use the neural network to <strong>COMPRESS</strong> this Q-table, using some parameters \thetaθ <strong>(We called it weight in Neural Network)</strong>. So instead of handling a large table, I just need to worry the weights of the neural network. By smartly tuning the weight parameters, I can find the optimal Q-function via the various Neural Network training algorithm.<br>$$<br>Q(s,a) = f_{\theta}(s)<br>$$<br>where $f$ is our neural network with input $s$ and weight parameters $\theta$</p><p>Here is the code below to demonstrate how it works</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> t &gt; OBSERVE:</div><div class="line">    <span class="comment">#sample a minibatch to train on</span></div><div class="line">    minibatch = random.sample(D, BATCH)</div><div class="line"></div><div class="line">    inputs = np.zeros((BATCH, s_t.shape[<span class="number">1</span>], s_t.shape[<span class="number">2</span>], s_t.shape[<span class="number">3</span>]))   <span class="comment">#32, 80, 80, 4</span></div><div class="line">    targets = np.zeros((inputs.shape[<span class="number">0</span>], ACTIONS))                         <span class="comment">#32, 2</span></div><div class="line"></div><div class="line">    <span class="comment">#Now we do the experience replay</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(minibatch)):</div><div class="line">        state_t = minibatch[i][<span class="number">0</span>]</div><div class="line">        action_t = minibatch[i][<span class="number">1</span>]   <span class="comment">#This is action index</span></div><div class="line">        reward_t = minibatch[i][<span class="number">2</span>]</div><div class="line">        state_t1 = minibatch[i][<span class="number">3</span>]</div><div class="line">        terminal = minibatch[i][<span class="number">4</span>]</div><div class="line">        <span class="comment"># if terminated, only equals reward</span></div><div class="line"></div><div class="line">        inputs[i:i + <span class="number">1</span>] = state_t    <span class="comment">#I saved down s_t</span></div><div class="line"></div><div class="line">        targets[i] = model.predict(state_t)  <span class="comment"># Hitting each buttom probability</span></div><div class="line">        Q_sa = model.predict(state_t1)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> terminal:</div><div class="line">            targets[i, action_t] = reward_t</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)</div><div class="line"></div><div class="line">        loss += model.train_on_batch(inputs, targets)</div><div class="line"></div><div class="line">    s_t = s_t1</div><div class="line">    t = t + <span class="number">1</span></div></pre></td></tr></table></figure><h3 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h3><p>If you examine the code above, there is a comment called “Experience Replay”. Let me explain what it does: It was found that approximation of Q-value using non-linear functions like neural network is not very stable. The most important trick to solve this problem is called <strong>experience replay</strong>. During the gameplay all the episode $(s, a, r, s^{‘})$ are stored in replay memory <strong>D</strong>. (I use Python function deque() to store it). When training the network, random mini-batches from the replay memory are used instead of most the recent transition, which will greatly improve the stability.</p><h3 id="Exploration-vs-Exploitation"><a href="#Exploration-vs-Exploitation" class="headerlink" title="Exploration vs. Exploitation"></a>Exploration vs. Exploitation</h3><p>There is another issue in the reinforcement learning algorithm which called Exploration vs. Exploitation. How much of an agent’s time should be spent exploiting its existing known-good policy, and how much time should be focused on exploring new, possibility better, actions? We often encounter similar situations in real life too. For example, we face on which restaurant to go to on Saturday night. We all have a set of restaurants that we prefer, based on our policy/strategy book $Q(s,a)$. If we stick to our normal preference, there is a strong probability that we’ll pick a good restaurant. However, sometimes, we occasionally like to try new restaurants to see if they are better. The RL agents face the same problem. In order to maximize future reward, they need to balance the amount of time that they follow their current policy (this is called being “greedy”), and the time they spend exploring new possibilities that might be better. A popular approach is called $\epsilon$ greedy approach. Under this approach, the policy tells the agent to try a random action some percentage of the time, as defined by the variable $\epsilon$ (epsilon), which is a number between 0 and 1. The strategy will help the RL agent to occasionally try something new and see if we can achieve ultimate strategy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> random.random() &lt;= epsilon:</div><div class="line">    print(<span class="string">"----------Random Action----------"</span>)</div><div class="line">    action_index = random.randrange(ACTIONS)</div><div class="line">    a_t[action_index] = <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">    q = model.predict(s_t)       <span class="comment">#input a stack of 4 images, get the prediction</span></div><div class="line">    max_Q = np.argmax(q)</div><div class="line">    action_index = max_Q</div><div class="line">    a_t[max_Q] = <span class="number">1</span></div></pre></td></tr></table></figure><p>I think that’s it. I hope this blog will help you to understand how DQN works.</p><h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h3 id="My-training-is-very-slow"><a href="#My-training-is-very-slow" class="headerlink" title="My training is very slow"></a>My training is very slow</h3><p>You might need a GPU to accelerate the calculation. I used a TITAN X and train for at least 1 million frames to make it work</p><h1 id="Future-works-and-thoughts"><a href="#Future-works-and-thoughts" class="headerlink" title="Future works and thoughts"></a>Future works and thoughts</h1><ol><li>Current DQN depends on large experience replay. Is it possible to replace it or even remove it?</li><li>How can one decide on the optimal Convolution Neural Network?</li><li>Training is very slow, how to speed it up/to make the model converge faster?</li><li>What does the Neural Network actually learn? Is the knowledge transferable?</li></ol><p>I believe the questions are still not resolved and it’s an active research area in Machine Learning.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. <strong>Human-level Control through Deep Reinforcement Learning</strong>. Nature, 529-33, 2015.</p><h1 id="Disclaimer"><a href="#Disclaimer" class="headerlink" title="Disclaimer"></a>Disclaimer</h1><p>This work is highly based on the following repos:</p><p><a href="https://github.com/yenchenlin/DeepLearningFlappyBird" target="_blank" rel="external">https://github.com/yenchenlin/DeepLearningFlappyBird</a></p><p><a href="http://edersantana.github.io/articles/keras_rl/" target="_blank" rel="external">http://edersantana.github.io/articles/keras_rl/</a></p><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>I must thank to <a href="https://twitter.com/hardmaru" target="_blank" rel="external">@hardmaru</a> to encourage me to write this blog. I also thank to <a href="https://twitter.com/fchollet" target="_blank" rel="external">@fchollet</a> to help me on the weight initialization in Keras and <a href="https://twitter.com/edersantana" target="_blank" rel="external">@edersantana</a> his post on Keras and reinforcement learning which really help me to understand it.</p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/deep-learning/" rel="tag"># deep learning</a> <a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a> <a href="/tags/deep-reinforcement-learning/" rel="tag"># deep reinforcement learning</a> <a href="/tags/DQN/" rel="tag"># DQN</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/07/07/Demystifying-Deep-Reinforcement-Learning/" rel="next" title="Demystifying Deep Reinforcement Learning (Repost)"><i class="fa fa-chevron-left"></i> Demystifying Deep Reinforcement Learning (Repost)</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/07/08/Summary-of-the-papers/" rel="prev" title="Summary of Papers">Summary of Papers <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">115</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">58</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview"><span class="nav-number">1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Installation-Dependencies"><span class="nav-number">2.</span> <span class="nav-text">Installation Dependencies:</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#How-to-Run"><span class="nav-number">3.</span> <span class="nav-text">How to Run?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-Deep-Q-Network"><span class="nav-number">4.</span> <span class="nav-text">What is Deep Q-Network?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Code-Explanation-in-details"><span class="nav-number">5.</span> <span class="nav-text">Code Explanation (in details)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Game-Screen-Input"><span class="nav-number">5.0.1.</span> <span class="nav-text">Game Screen Input</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-pre-processing"><span class="nav-number">5.0.2.</span> <span class="nav-text">Image pre-processing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convolution-Neural-Network"><span class="nav-number">5.0.3.</span> <span class="nav-text">Convolution Neural Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Note"><span class="nav-number">5.0.4.</span> <span class="nav-text">Note</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DQN"><span class="nav-number">5.0.5.</span> <span class="nav-text">DQN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experience-Replay"><span class="nav-number">5.0.6.</span> <span class="nav-text">Experience Replay</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exploration-vs-Exploitation"><span class="nav-number">5.0.7.</span> <span class="nav-text">Exploration vs. Exploitation</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#FAQ"><span class="nav-number">6.</span> <span class="nav-text">FAQ</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#My-training-is-very-slow"><span class="nav-number">6.0.1.</span> <span class="nav-text">My training is very slow</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Future-works-and-thoughts"><span class="nav-number">7.</span> <span class="nav-text">Future works and thoughts</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">8.</span> <span class="nav-text">Reference</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Disclaimer"><span class="nav-number">9.</span> <span class="nav-text">Disclaimer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Acknowledgement"><span class="nav-number">10.</span> <span class="nav-text">Acknowledgement</span></a></li></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(t,a.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>