<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="deep learning,machine learning,reinforcement learning,deep reinforcement learning,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="In previous blog, we use the Keras to play the FlappyBird. Similarity, we will use another deep learning toolkit Tensorflow to develop the DQN and Double DQN and to play the another game Breakout (Ata"><meta property="og:type" content="article"><meta property="og:title" content="Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout"><meta property="og:url" content="http://yoursite.com/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="In previous blog, we use the Keras to play the FlappyBird. Similarity, we will use another deep learning toolkit Tensorflow to develop the DQN and Double DQN and to play the another game Breakout (Ata"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/games/breakout-env.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/games/croped-breakout-image.png"><meta property="og:updated_time" content="2017-07-09T04:38:52.104Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout"><meta name="twitter:description" content="In previous blog, we use the Keras to play the FlappyBird. Similarity, we will use another deep learning toolkit Tensorflow to develop the DQN and Double DQN and to play the another game Breakout (Ata"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/rl/games/breakout-env.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/"><title>Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-09T11:34:18+08:00">2017-07-09 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/" class="leancloud_visitors" data-flag-title="Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In previous <a href="https://ewanlee.github.io/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" target="_blank" rel="external">blog</a>, we use the Keras to play the FlappyBird. Similarity, we will use another deep learning toolkit Tensorflow to develop the DQN and Double DQN and to play the another game Breakout (Atari 3600).</p><p>Here, we will use the OpenAI gym toolkit to construct out environment. Detail implementation is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env = gym.envs.make(<span class="string">"Breakout-v0"</span>)</div></pre></td></tr></table></figure><p>And then we look some demos:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"Action space size: &#123;&#125;"</span>.format(env.action_space.n))</div><div class="line"><span class="comment"># print(env.get_action_meanings())</span></div><div class="line"></div><div class="line">observation = env.reset()</div><div class="line">print(<span class="string">"Observation space shape: &#123;&#125;"</span>.format(observation.shape))</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">[env.step(<span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>)]</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">env.render(close=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/games/breakout-env.png" alt="breakout-env"></p><p>For deep learning purpose, we need to crop the image to a square image:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Check out what a cropped image looks like</span></div><div class="line">plt.imshow(observation[<span class="number">34</span>:<span class="number">-16</span>,:,:])</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/games/croped-breakout-image.png" alt="croped-breakout-image"></p><p>Not bad !</p><p>Ok, now let us to use the Tensorflow to develop the DQN algorithm first.</p><p>First of all, we need to reference some packages and initialize the environment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"></div><div class="line"><span class="keyword">import</span> gym</div><div class="line"><span class="keyword">from</span> gym.wrappers <span class="keyword">import</span> Monitor</div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">if</span> <span class="string">"../"</span> <span class="keyword">not</span> <span class="keyword">in</span> sys.path:</div><div class="line">  sys.path.append(<span class="string">"../"</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> lib <span class="keyword">import</span> plotting</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque, namedtuple</div><div class="line"></div><div class="line">env = gym.envs.make(<span class="string">"Breakout-v0"</span>)</div><div class="line"><span class="comment"># Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions</span></div><div class="line">VALID_ACTIONS = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div></pre></td></tr></table></figure><p>As mentioned above, we need to crop the image and preprocess the input before feed the raw image into the algorithm. So we define a <strong>StateProcessor</strong> class to do this.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StateProcessor</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Processes a raw Atari images. Resizes it and converts it to grayscale.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># Build the Tensorflow graph</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"state_processor"</span>):</div><div class="line">            self.input_state = tf.placeholder(shape=[<span class="number">210</span>, <span class="number">160</span>, <span class="number">3</span>], dtype=tf.uint8)</div><div class="line">            self.output = tf.image.rgb_to_grayscale(self.input_state)</div><div class="line">            self.output = tf.image.crop_to_bounding_box(self.output, <span class="number">34</span>, <span class="number">0</span>, <span class="number">160</span>, <span class="number">160</span>)</div><div class="line">            self.output = tf.image.resize_images(</div><div class="line">                self.output, [<span class="number">84</span>, <span class="number">84</span>], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</div><div class="line">            self.output = tf.squeeze(self.output)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, sess, state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Args:</div><div class="line">            sess: A Tensorflow session object</div><div class="line">            state: A [210, 160, 3] Atari RGB State</div><div class="line"></div><div class="line">        Returns:</div><div class="line">            A processed [84, 84, 1] state representing grayscale values.</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> sess.run(self.output, &#123; self.input_state: state &#125;)</div></pre></td></tr></table></figure><p>We first convert the image to gray image and then resize it to 84 by 84 (the size DQN paper used). Then, we construct the neural network to estimate the value function. The structure of the network as the same as the DQN paper.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Estimator</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""Q-Value Estimator neural network.</span></div><div class="line"></div><div class="line">    This network is used for both the Q-Network and the Target Network.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, scope=<span class="string">"estimator"</span>, summaries_dir=None)</span>:</span></div><div class="line">        self.scope = scope</div><div class="line">        <span class="comment"># Writes Tensorboard summaries to disk</span></div><div class="line">        self.summary_writer = <span class="keyword">None</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</div><div class="line">            <span class="comment"># Build the graph</span></div><div class="line">            self._build_model()</div><div class="line">            <span class="keyword">if</span> summaries_dir:</div><div class="line">                summary_dir = os.path.join(summaries_dir, <span class="string">"summaries_&#123;&#125;"</span>.format(scope))</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(summary_dir):</div><div class="line">                    os.makedirs(summary_dir)</div><div class="line">                self.summary_writer = tf.summary.FileWriter(summary_dir)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_model</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Builds the Tensorflow graph.</div><div class="line">        """</div><div class="line"></div><div class="line">        <span class="comment"># Placeholders for our input</span></div><div class="line">        <span class="comment"># Our input are 4 RGB frames of shape 160, 160 each</span></div><div class="line">        self.X_pl = tf.placeholder(shape=[<span class="keyword">None</span>, <span class="number">84</span>, <span class="number">84</span>, <span class="number">4</span>], dtype=tf.uint8, name=<span class="string">"X"</span>)</div><div class="line">        <span class="comment"># The TD target value</span></div><div class="line">        self.y_pl = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.float32, name=<span class="string">"y"</span>)</div><div class="line">        <span class="comment"># Integer id of which action was selected</span></div><div class="line">        self.actions_pl = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.int32, name=<span class="string">"actions"</span>)</div><div class="line"></div><div class="line">        X = tf.to_float(self.X_pl) / <span class="number">255.0</span></div><div class="line">        batch_size = tf.shape(self.X_pl)[<span class="number">0</span>]</div><div class="line"></div><div class="line">        <span class="comment"># Three convolutional layers</span></div><div class="line">        conv1 = tf.contrib.layers.conv2d(</div><div class="line">            X, <span class="number">32</span>, <span class="number">8</span>, <span class="number">4</span>, activation_fn=tf.nn.relu)</div><div class="line">        conv2 = tf.contrib.layers.conv2d(</div><div class="line">            conv1, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>, activation_fn=tf.nn.relu)</div><div class="line">        conv3 = tf.contrib.layers.conv2d(</div><div class="line">            conv2, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, activation_fn=tf.nn.relu)</div><div class="line"></div><div class="line">        <span class="comment"># Fully connected layers</span></div><div class="line">        flattened = tf.contrib.layers.flatten(conv3)</div><div class="line">        fc1 = tf.contrib.layers.fully_connected(flattened, <span class="number">512</span>)</div><div class="line">        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))</div><div class="line"></div><div class="line">        <span class="comment"># Get the predictions for the chosen actions only</span></div><div class="line">        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[<span class="number">1</span>] + self.actions_pl</div><div class="line">        self.action_predictions = tf.gather(tf.reshape(self.predictions, [<span class="number">-1</span>]), gather_indices)</div><div class="line"></div><div class="line">        <span class="comment"># Calcualte the loss</span></div><div class="line">        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)</div><div class="line">        self.loss = tf.reduce_mean(self.losses)</div><div class="line"></div><div class="line">        <span class="comment"># Optimizer Parameters from original paper</span></div><div class="line">        self.optimizer = tf.train.RMSPropOptimizer(<span class="number">0.00025</span>, <span class="number">0.99</span>, <span class="number">0.0</span>, <span class="number">1e-6</span>)</div><div class="line">        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())</div><div class="line"></div><div class="line">        <span class="comment"># Summaries for Tensorboard</span></div><div class="line">        self.summaries = tf.summary.merge([</div><div class="line">            tf.summary.scalar(<span class="string">"loss"</span>, self.loss),</div><div class="line">            tf.summary.histogram(<span class="string">"loss_hist"</span>, self.losses),</div><div class="line">            tf.summary.histogram(<span class="string">"q_values_hist"</span>, self.predictions),</div><div class="line">            tf.summary.scalar(<span class="string">"max_q_value"</span>, tf.reduce_max(self.predictions))</div><div class="line">        ])</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sess, s)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Predicts action values.</div><div class="line"></div><div class="line">        Args:</div><div class="line">          sess: Tensorflow session</div><div class="line">          s: State input of shape [batch_size, 4, 160, 160, 3]</div><div class="line"></div><div class="line">        Returns:</div><div class="line">          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated </div><div class="line">          action values.</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> sess.run(self.predictions, &#123; self.X_pl: s &#125;)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, sess, s, a, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Updates the estimator towards the given targets.</div><div class="line"></div><div class="line">        Args:</div><div class="line">          sess: Tensorflow session object</div><div class="line">          s: State input of shape [batch_size, 4, 160, 160, 3]</div><div class="line">          a: Chosen actions of shape [batch_size]</div><div class="line">          y: Targets of shape [batch_size]</div><div class="line"></div><div class="line">        Returns:</div><div class="line">          The calculated loss on the batch.</div><div class="line">        """</div><div class="line">        feed_dict = &#123; self.X_pl: s, self.y_pl: y, self.actions_pl: a &#125;</div><div class="line">        summaries, global_step, _, loss = sess.run(</div><div class="line">            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],</div><div class="line">            feed_dict)</div><div class="line">        <span class="keyword">if</span> self.summary_writer:</div><div class="line">            self.summary_writer.add_summary(summaries, global_step)</div><div class="line">        <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>As mentioned in DQN paper, there are two network that share the same parameters in DQN algorithm. We need to copy the parameters to the target network on each $t$ steps.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">copy_model_parameters</span><span class="params">(sess, estimator1, estimator2)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Copies the model parameters of one estimator to another.</div><div class="line"></div><div class="line">    Args:</div><div class="line">      sess: Tensorflow session instance</div><div class="line">      estimator1: Estimator to copy the paramters from</div><div class="line">      estimator2: Estimator to copy the parameters to</div><div class="line">    """</div><div class="line">    e1_params = [t <span class="keyword">for</span> t <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> t.name.startswith(estimator1.scope)]</div><div class="line">    e1_params = sorted(e1_params, key=<span class="keyword">lambda</span> v: v.name)</div><div class="line">    e2_params = [t <span class="keyword">for</span> t <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> t.name.startswith(estimator2.scope)]</div><div class="line">    e2_params = sorted(e2_params, key=<span class="keyword">lambda</span> v: v.name)</div><div class="line"></div><div class="line">    update_ops = []</div><div class="line">    <span class="keyword">for</span> e1_v, e2_v <span class="keyword">in</span> zip(e1_params, e2_params):</div><div class="line">        op = e2_v.assign(e1_v)</div><div class="line">        update_ops.append(op)</div><div class="line"></div><div class="line">    sess.run(update_ops)</div></pre></td></tr></table></figure><p>We also need a policy to take an action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(estimator, nA)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.</div><div class="line"></div><div class="line">    Args:</div><div class="line">        estimator: An estimator that returns q values for a given state</div><div class="line">        nA: Number of actions in the environment.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        A function that takes the (sess, observation, epsilon) as an argument and returns</div><div class="line">        the probabilities for each action in the form of a numpy array of length nA.</div><div class="line"></div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(sess, observation, epsilon)</span>:</span></div><div class="line">        A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">        q_values = estimator.predict(sess, np.expand_dims(observation, <span class="number">0</span>))[<span class="number">0</span>]</div><div class="line">        best_action = np.argmax(q_values)</div><div class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">        <span class="keyword">return</span> A</div><div class="line">    <span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Now let us to develop the DQN algorithm (we skip the details here because we explained it earlier).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">deep_q_learning</span><span class="params">(sess,</span></span></div><div class="line">                    env,</div><div class="line">                    q_estimator,</div><div class="line">                    target_estimator,</div><div class="line">                    state_processor,</div><div class="line">                    num_episodes,</div><div class="line">                    experiment_dir,</div><div class="line">                    replay_memory_size=<span class="number">500000</span>,</div><div class="line">                    replay_memory_init_size=<span class="number">50000</span>,</div><div class="line">                    update_target_estimator_every=<span class="number">10000</span>,</div><div class="line">                    discount_factor=<span class="number">0.99</span>,</div><div class="line">                    epsilon_start=<span class="number">1.0</span>,</div><div class="line">                    epsilon_end=<span class="number">0.1</span>,</div><div class="line">                    epsilon_decay_steps=<span class="number">500000</span>,</div><div class="line">                    batch_size=<span class="number">32</span>,</div><div class="line">                    record_video_every=<span class="number">50</span>):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm for fff-policy TD control using Function Approximation.</div><div class="line">    Finds the optimal greedy policy while following an epsilon-greedy policy.</div><div class="line"></div><div class="line">    Args:</div><div class="line">        sess: Tensorflow Session object</div><div class="line">        env: OpenAI environment</div><div class="line">        q_estimator: Estimator object used for the q values</div><div class="line">        target_estimator: Estimator object used for the targets</div><div class="line">        state_processor: A StateProcessor object</div><div class="line">        num_episodes: Number of episodes to run for</div><div class="line">        experiment_dir: Directory to save Tensorflow summaries in</div><div class="line">        replay_memory_size: Size of the replay memory</div><div class="line">        replay_memory_init_size: Number of random experiences to sampel when initializing </div><div class="line">          the reply memory.</div><div class="line">        update_target_estimator_every: Copy parameters from the Q estimator to the </div><div class="line">          target estimator every N steps</div><div class="line">        discount_factor: Lambda time discount factor</div><div class="line">        epsilon_start: Chance to sample a random action when taking an action.</div><div class="line">          Epsilon is decayed over time and this is the start value</div><div class="line">        epsilon_end: The final minimum value of epsilon after decaying is done</div><div class="line">        epsilon_decay_steps: Number of steps to decay epsilon over</div><div class="line">        batch_size: Size of batches to sample from the replay memory</div><div class="line">        record_video_every: Record a video every N episodes</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line"></div><div class="line">    Transition = namedtuple(<span class="string">"Transition"</span>, [<span class="string">"state"</span>, <span class="string">"action"</span>, <span class="string">"reward"</span>, <span class="string">"next_state"</span>, <span class="string">"done"</span>])</div><div class="line"></div><div class="line">    <span class="comment"># The replay memory</span></div><div class="line">    replay_memory = []</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))</div><div class="line"></div><div class="line">    <span class="comment"># Create directories for checkpoints and summaries</span></div><div class="line">    checkpoint_dir = os.path.join(experiment_dir, <span class="string">"checkpoints"</span>)</div><div class="line">    checkpoint_path = os.path.join(checkpoint_dir, <span class="string">"model"</span>)</div><div class="line">    monitor_path = os.path.join(experiment_dir, <span class="string">"monitor"</span>)</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):</div><div class="line">        os.makedirs(checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(monitor_path):</div><div class="line">        os.makedirs(monitor_path)</div><div class="line"></div><div class="line">    saver = tf.train.Saver()</div><div class="line">    <span class="comment"># Load a previous checkpoint if we find one</span></div><div class="line">    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> latest_checkpoint:</div><div class="line">        print(<span class="string">"Loading model checkpoint &#123;&#125;...\n"</span>.format(latest_checkpoint))</div><div class="line">        saver.restore(sess, latest_checkpoint)</div><div class="line">    </div><div class="line">    <span class="comment"># Get the current time step</span></div><div class="line">    total_t = sess.run(tf.contrib.framework.get_global_step())</div><div class="line"></div><div class="line">    <span class="comment"># The epsilon decay schedule</span></div><div class="line">    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)</div><div class="line"></div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(</div><div class="line">        q_estimator,</div><div class="line">        len(VALID_ACTIONS))</div><div class="line"></div><div class="line">    <span class="comment"># Populate the replay memory with initial experience</span></div><div class="line">    print(<span class="string">"Populating replay memory..."</span>)</div><div class="line">    state = env.reset()</div><div class="line">    state = state_processor.process(sess, state)</div><div class="line">    state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(replay_memory_init_size):</div><div class="line">        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps<span class="number">-1</span>)])</div><div class="line">        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])</div><div class="line">        next_state = state_processor.process(sess, next_state)</div><div class="line">        next_state = np.append(state[:,:,<span class="number">1</span>:], np.expand_dims(next_state, <span class="number">2</span>), axis=<span class="number">2</span>)</div><div class="line">        replay_memory.append(Transition(state, action, reward, next_state, done))</div><div class="line">        <span class="keyword">if</span> done:</div><div class="line">            state = env.reset()</div><div class="line">            state = state_processor.process(sess, state)</div><div class="line">            state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state = next_state</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment"># Record videos</span></div><div class="line">    <span class="comment"># Add env Monitor wrapper</span></div><div class="line">    env = Monitor(env, directory=monitor_path, video_callable=<span class="keyword">lambda</span> count: count % record_video_every == <span class="number">0</span>, resume=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line"></div><div class="line">        <span class="comment"># Save the current checkpoint</span></div><div class="line">        saver.save(tf.get_default_session(), checkpoint_path)</div><div class="line"></div><div class="line">        <span class="comment"># Reset the environment</span></div><div class="line">        state = env.reset()</div><div class="line">        state = state_processor.process(sess, state)</div><div class="line">        state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">        loss = <span class="keyword">None</span></div><div class="line"></div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line"></div><div class="line">            <span class="comment"># Epsilon for this time step</span></div><div class="line">            epsilon = epsilons[min(total_t, epsilon_decay_steps<span class="number">-1</span>)]</div><div class="line"></div><div class="line">            <span class="comment"># Add epsilon to Tensorboard</span></div><div class="line">            episode_summary = tf.Summary()</div><div class="line">            episode_summary.value.add(simple_value=epsilon, tag=<span class="string">"epsilon"</span>)</div><div class="line">            q_estimator.summary_writer.add_summary(episode_summary, total_t)</div><div class="line"></div><div class="line">            <span class="comment"># Maybe update the target estimator</span></div><div class="line">            <span class="keyword">if</span> total_t % update_target_estimator_every == <span class="number">0</span>:</div><div class="line">                copy_model_parameters(sess, q_estimator, target_estimator)</div><div class="line">                print(<span class="string">"\nCopied model parameters to target network."</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Print out which step we're on, useful for debugging.</span></div><div class="line">            print(<span class="string">"\rStep &#123;&#125; (&#123;&#125;) @ Episode &#123;&#125;/&#123;&#125;, loss: &#123;&#125;"</span>.format(</div><div class="line">                    t, total_t, i_episode + <span class="number">1</span>, num_episodes, loss), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line"></div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            action_probs = policy(sess, state, epsilon)</div><div class="line">            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])</div><div class="line">            next_state = state_processor.process(sess, next_state)</div><div class="line">            next_state = np.append(state[:,:,<span class="number">1</span>:], np.expand_dims(next_state, <span class="number">2</span>), axis=<span class="number">2</span>)</div><div class="line"></div><div class="line">            <span class="comment"># If our replay memory is full, pop the first element</span></div><div class="line">            <span class="keyword">if</span> len(replay_memory) == replay_memory_size:</div><div class="line">                replay_memory.pop(<span class="number">0</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Save transition to replay memory</span></div><div class="line">            replay_memory.append(Transition(state, action, reward, next_state, done))   </div><div class="line"></div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line"></div><div class="line">            <span class="comment"># Sample a minibatch from the replay memory</span></div><div class="line">            samples = random.sample(replay_memory, batch_size)</div><div class="line">            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))</div><div class="line"></div><div class="line">            <span class="comment"># Calculate q values and targets</span></div><div class="line">            q_values_next = target_estimator.predict(sess, next_states_batch)</div><div class="line">            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Perform gradient descent update</span></div><div class="line">            states_batch = np.array(states_batch)</div><div class="line">            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line"></div><div class="line">            state = next_state</div><div class="line">            total_t += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="comment"># Add summaries to tensorboard</span></div><div class="line">        episode_summary = tf.Summary()</div><div class="line">        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=<span class="string">"episode_reward"</span>, tag=<span class="string">"episode_reward"</span>)</div><div class="line">        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=<span class="string">"episode_length"</span>, tag=<span class="string">"episode_length"</span>)</div><div class="line">        q_estimator.summary_writer.add_summary(episode_summary, total_t)</div><div class="line">        q_estimator.summary_writer.flush()</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> total_t, plotting.EpisodeStats(</div><div class="line">            episode_lengths=stats.episode_lengths[:i_episode+<span class="number">1</span>],</div><div class="line">            episode_rewards=stats.episode_rewards[:i_episode+<span class="number">1</span>])</div><div class="line"></div><div class="line">    <span class="keyword">return</span> stats</div></pre></td></tr></table></figure><p>Finally, run it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">tf.reset_default_graph()</div><div class="line"></div><div class="line"><span class="comment"># Where we save our checkpoints and graphs</span></div><div class="line">experiment_dir = os.path.abspath(<span class="string">"./experiments/&#123;&#125;"</span>.format(env.spec.id))</div><div class="line"></div><div class="line"><span class="comment"># Create a glboal step variable</span></div><div class="line">global_step = tf.Variable(<span class="number">0</span>, name=<span class="string">'global_step'</span>, trainable=<span class="keyword">False</span>)</div><div class="line">    </div><div class="line"><span class="comment"># Create estimators</span></div><div class="line">q_estimator = Estimator(scope=<span class="string">"q"</span>, summaries_dir=experiment_dir)</div><div class="line">target_estimator = Estimator(scope=<span class="string">"target_q"</span>)</div><div class="line"></div><div class="line"><span class="comment"># State processor</span></div><div class="line">state_processor = StateProcessor()</div><div class="line"></div><div class="line"><span class="comment"># Run it!</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    <span class="keyword">for</span> t, stats <span class="keyword">in</span> deep_q_learning(sess,</div><div class="line">                                    env,</div><div class="line">                                    q_estimator=q_estimator,</div><div class="line">                                    target_estimator=target_estimator,</div><div class="line">                                    state_processor=state_processor,</div><div class="line">                                    experiment_dir=experiment_dir,</div><div class="line">                                    num_episodes=<span class="number">10000</span>,</div><div class="line">                                    replay_memory_size=<span class="number">500000</span>,</div><div class="line">                                    replay_memory_init_size=<span class="number">50000</span>,</div><div class="line">                                    update_target_estimator_every=<span class="number">10000</span>,</div><div class="line">                                    epsilon_start=<span class="number">1.0</span>,</div><div class="line">                                    epsilon_end=<span class="number">0.1</span>,</div><div class="line">                                    epsilon_decay_steps=<span class="number">500000</span>,</div><div class="line">                                    discount_factor=<span class="number">0.99</span>,</div><div class="line">                                    batch_size=<span class="number">32</span>):</div><div class="line"></div><div class="line">        print(<span class="string">"\nEpisode Reward: &#123;&#125;"</span>.format(stats.episode_rewards[<span class="number">-1</span>]))</div></pre></td></tr></table></figure><hr><p>Next, we will develop the Double-DQN algorithm. This algorithm only has a little changes.</p><p>In DQN <strong>q_learning</strong> method,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate q values and targets</span></div><div class="line">q_values_next = target_estimator.predict(sess, next_states_batch)</div><div class="line">targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p>we just change these codes to,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate q values and targets</span></div><div class="line"><span class="comment"># This is where Double Q-Learning comes in!</span></div><div class="line">q_values_next = q_estimator.predict(sess, next_states_batch)</div><div class="line">best_actions = np.argmax(q_values_next, axis=<span class="number">1</span>)</div><div class="line">q_values_next_target = target_estimator.predict(sess, next_states_batch)</div><div class="line">targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * q_values_next_target[np.arange(batch_size), best_actions]</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/deep-learning/" rel="tag"># deep learning</a> <a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a> <a href="/tags/deep-reinforcement-learning/" rel="tag"># deep reinforcement learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/07/08/Summary-of-the-papers/" rel="next" title="Summary of the Reinforcement Learning Papers"><i class="fa fa-chevron-left"></i> Summary of the Reinforcement Learning Papers</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/07/10/Policy-Gradient-Methods/" rel="prev" title="Policy Gradient Methods">Policy Gradient Methods <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">90</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">50</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var o=document.getElementsByTagName("script")[0];o.parentNode.insertBefore(t,o.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>