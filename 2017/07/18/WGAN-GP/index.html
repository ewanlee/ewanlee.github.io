<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="deep learning,machine learning,GAN,WGAN,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="WGAN存在着训练困难、收敛速度慢等问题。WGAN的作者Martin Arjovsky不久后就在reddit上表示他也意识到了这个问题，认为关键在于原设计中Lipschitz限制的施加方式不对：I am now pretty convinced that the problems that happen sometimes in WGANs is due to the specific way o"><meta property="og:type" content="article"><meta property="og:title" content="WGAN-GP [Repost]"><meta property="og:url" content="http://yoursite.com/2017/07/18/WGAN-GP/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="WGAN存在着训练困难、收敛速度慢等问题。WGAN的作者Martin Arjovsky不久后就在reddit上表示他也意识到了这个问题，认为关键在于原设计中Lipschitz限制的施加方式不对：I am now pretty convinced that the problems that happen sometimes in WGANs is due to the specific way o"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-clip.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-gp.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-gradient-norm.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-exper.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-image-gen.png"><meta property="og:image" content="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-text-gen.png"><meta property="og:updated_time" content="2017-07-18T13:49:35.700Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="WGAN-GP [Repost]"><meta name="twitter:description" content="WGAN存在着训练困难、收敛速度慢等问题。WGAN的作者Martin Arjovsky不久后就在reddit上表示他也意识到了这个问题，认为关键在于原设计中Lipschitz限制的施加方式不对：I am now pretty convinced that the problems that happen sometimes in WGANs is due to the specific way o"><meta name="twitter:image" content="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-clip.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/07/18/WGAN-GP/"><title>WGAN-GP [Repost] | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/18/WGAN-GP/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">WGAN-GP [Repost]</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-18T11:16:24+08:00">2017-07-18 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/18/WGAN-GP/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/18/WGAN-GP/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/18/WGAN-GP/" class="leancloud_visitors" data-flag-title="WGAN-GP [Repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><a href="https://ewanlee.github.io/2017/04/29/The-awesome-Wasserstein-GAN/" target="_blank" rel="external">WGAN</a>存在着训练困难、收敛速度慢等问题。<a href="https://link.zhihu.com/?target=https%3A//www.reddit.com/r/MachineLearning/comments/5zd4c0/d_survey_whats_the_most_stable_regiment_for/dexfhxu/%3Futm_content%3Dpermalink%26utm_medium%3Dfront%26utm_source%3Dreddit%26utm_name%3DMachineLearning" target="_blank" rel="external">WGAN的作者Martin Arjovsky不久后就在reddit上表示他也意识到了这个问题</a>，认为关键在于原设计中Lipschitz限制的施加方式不对：</p><blockquote><p>I am now pretty convinced that the problems that happen sometimes in WGANs is due to the specific way of how weight clipping works. It’s just a terrible way of enforcing a Lipschitz constraint, and better ways are out there. I feel like apologizing for being too lazy and sticking to what could be done in one line of torch code.</p><p>A simple alternative (less than 5 lines of code) has been found by Montréal students. It works on quite a few settings (inc 100 layer resnets) with default hyperparameters. Arxiv coming this or next week, stay tuned.</p></blockquote><p>并在新论文中提出了相应的改进方案：</p><ul><li>论文：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.00028" target="_blank" rel="external">[1704.00028] Improved Training of Wasserstein GANs</a></li><li>Tensorflow实现：<ul><li><a href="https://github.com/brianherman/improved_wgan_training" target="_blank" rel="external">brianherman/improved_wgan_training</a> (Python 3)</li><li><a href="https://github.com/igul222/improved_wgan_training" target="_blank" rel="external">igul222/improved_wgan_training</a> (Python 2)</li></ul></li></ul><p><strong>首先回顾一下WGAN的关键部分——Lipschitz限制是什么。</strong>WGAN中，判别器D和生成器G的loss函数分别是<br>$$<br>\begin{align}<br>L(D) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] \\<br>L(G) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)]<br>\end{align}<br>$$<br>公式1表示判别器希望尽可能拉高真样本的分数，拉低假样本的分数，公式2表示生成器希望尽可能拉高假样本的分数。</p><p>Lipschitz限制则体现为，在整个样本空间$\mathcal{X}$上，要求判别器函数$D(x)$梯度的$L_p$ norm大于一个有限的常数K：<br>$$<br>| \nabla_x D(x) |_p \leq K, \forall x \in \mathcal{X}<br>$$<br>直观上解释，就是当输入的样本稍微变化后，判别器给出的分数不能发生太过剧烈的变化。在原来的论文中，这个限制具体是通过weight clipping的方式实现的：每当更新完一次判别器的参数之后，就检查判别器的所有参数的绝对值有没有超过一个阈值，比如0.01，有的话就把这些参数clip回 [-0.01, 0.01] 范围内。通过在训练过程中保证判别器的所有参数有界，就保证了判别器不能对两个略微不同的样本给出天差地别的分数值，从而间接实现了Lipschitz限制。</p><p><strong>然而weight clipping的实现方式存在两个严重问题：</strong></p><p>第一，如公式1所言，判别器loss希望尽可能拉大真假样本的分数差，然而weight clipping独立地限制每一个网络参数的取值范围，在这种情况下我们可以想象，最优的策略就是尽可能让所有参数走极端，要么取最大值（如0.01）要么取最小值（如-0.01）！为了验证这一点，作者统计了经过充分训练的判别器中所有网络参数的数值分布，发现真的集中在最大和最小两个极端上：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-clip.png" alt="weight-clipping"></p><p>这样带来的结果就是，判别器会非常倾向于学习一个简单的映射函数（想想看，几乎所有参数都是正负0.01，都已经可以直接视为一个<a href="https://link.zhihu.com/?target=http%3A//synchuman.baijia.baidu.com/article/385441" target="_blank" rel="external">二值神经网络**</a>了，太简单了）。而作为一个深层神经网络来说，这实在是对自身强大拟合能力的巨大浪费！判别器没能充分利用自身的模型能力，经过它回传给生成器的梯度也会跟着变差。</p><p>在正式介绍gradient penalty之前，我们可以先看看在它的指导下，同样充分训练判别器之后，参数的数值分布就合理得多了，判别器也能够充分利用自身模型的拟合能力：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-gp.png" alt="weight-gp"></p><p>第二个问题，weight clipping会导致很容易一不小心就梯度消失或者梯度爆炸。原因是判别器是一个多层网络，如果我们把clipping threshold设得稍微小了一点，每经过一层网络，梯度就变小一点点，多层之后就会指数衰减；反之，如果设得稍微大了一点，每经过一层网络，梯度变大一点点，多层之后就会指数爆炸。只有设得不大不小，才能让生成器获得恰到好处的回传梯度，然而在实际应用中这个平衡区域可能很狭窄，就会给调参工作带来麻烦。相比之下，gradient penalty就可以让梯度在后向传播的过程中保持平稳。论文通过下图体现了这一点，其中横轴代表判别器从低到高第几层，纵轴代表梯度回传到这一层之后的尺度大小（注意纵轴是对数刻度），c是clipping threshold：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-gradient-norm.png" alt="gradient-norm"></p><p><strong>说了这么多，gradient penalty到底是什么？</strong></p><p>前面提到，Lipschitz限制是要求判别器的梯度不超过K，那我们何不直接设置一个额外的loss项来体现这一点呢？比如说：<br>$$<br>ReLU[| \nabla_x D(x) |_p - K]<br>$$<br>不过，既然判别器希望尽可能拉大真假样本的分数差距，那自然是希望梯度越大越好，变化幅度越大越好，所以判别器在充分训练之后，其梯度norm其实就会是在K附近。知道了这一点，我们可以把上面的loss改成要求梯度norm离K越近越好，效果是类似的：<br>$$<br>[| \nabla_x D(x) |_p - K]^2<br>$$<br>究竟是公式4好还是公式5好，我看不出来，可能需要实验验证，反正论文作者选的是公式5。接着我们简单地把K定为1，再跟WGAN原来的判别器loss加权合并，就得到新的判别器loss：<br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \mathcal{X}} [| \nabla_x D(x) |_p - 1]^2<br>$$<br>这就是所谓的gradient penalty了吗？还没完。公式6有两个问题，首先是loss函数中存在梯度项，那么优化这个loss岂不是要算梯度的梯度？一些读者可能对此存在疑惑，不过这属于实现上的问题，放到后面说。</p><p>其次，3个loss项都是期望的形式，落到实现上肯定得变成采样的形式。前面两个期望的采样我们都熟悉，第一个期望是从真样本集里面采，第二个期望是从生成器的噪声输入分布采样后，再由生成器映射到样本空间。可是第三个分布要求我们在整个样本空间$\mathcal{X}$上采样，这完全不科学！由于所谓的维度灾难问题，如果要通过采样的方式在图片或自然语言这样的高维样本空间中估计期望值，所需样本量是指数级的，实际上没法做到。</p><p>所以，论文作者就非常机智地提出，我们其实没必要在整个样本空间上施加Lipschitz限制，只要重点抓住生成样本集中区域、真实样本集中区域以及夹在它们中间的区域就行了。具体来说，我们先随机采一对真假样本，还有一个0-1的随机数：<br>$$<br>x_r \sim P_r, x_g \sim P_g, \epsilon \sim Uniform[0, 1]<br>$$<br>然后在$x_r$和$x_g$的连线上随机插值采样：<br>$$<br>\hat{x} = \epsilon x_r + (1 - \epsilon) x_g<br>$$<br>把按照上述流程采样得到的$\hat{x}$所满足的分布记为$P_{\hat{x}}$, 就得到最终版本的判别器loss：<br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \hat{x}} [| \nabla_x D(x) |_p - 1]^2<br>$$<br><strong>这就是新论文所采用的gradient penalty方法，相应的新WGAN模型简称为WGAN-GP。</strong>我们可以做一个对比：</p><ul><li>weight clipping是对样本空间全局生效，但因为是间接限制判别器的梯度norm，会导致一不小心就梯度消失或者梯度爆炸；</li><li>gradient penalty只对真假样本集中区域、及其中间的过渡地带生效，但因为是直接把判别器的梯度norm限制在1附近，所以梯度可控性非常强，容易调整到合适的尺度大小。</li></ul><p>论文还讲了一些使用gradient penalty时需要注意的配套事项，这里只提一点：由于我们是对每个样本独立地施加梯度惩罚，所以判别器的模型架构中不能使用Batch Normalization，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择其他normalization方法，如Layer Normalization、Weight Normalization和Instance Normalization，这些方法就不会引入样本之间的依赖。论文推荐的是Layer Normalization。</p><p>实验表明，gradient penalty能够显著提高训练速度，解决了原始WGAN收敛缓慢的问题：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-exper.png" alt="exper"></p><p>虽然还是比不过DCGAN，但是因为WGAN不存在平衡判别器与生成器的问题，所以会比DCGAN更稳定，还是很有优势的。不过，作者凭什么能这么说？因为下面的实验体现出，在各种不同的网络架构下，其他GAN变种能不能训练好，可以说是一件相当看人品的事情，但是WGAN-GP全都能够训练好，尤其是最下面一行所对应的101层残差神经网络：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-image-gen.png" alt="image-gen"></p><p><strong>剩下的实验结果中，比较厉害的是第一次成功做到了“纯粹的”的文本GAN训练！</strong>我们知道在图像上训练GAN是不需要额外的有监督信息的，但是之前就没有人能够像训练图像GAN一样训练好一个文本GAN，要么依赖于预训练一个语言模型，要么就是利用已有的有监督ground truth提供指导信息。而现在WGAN-GP终于在无需任何有监督信息的情况下，生成出下图所示的英文字符序列：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-text-gen.png" alt="text-gen"></p><p>它是怎么做到的呢？我认为关键之处是对样本形式的更改。<strong>以前我们一般会把文本这样的离散序列样本表示为sequence of index，但是它把文本表示成sequence of probability vector。</strong>对于生成样本来说，我们可以取网络softmax层输出的词典概率分布向量，作为序列中每一个位置的内容；而对于真实样本来说，每个probability vector实际上就蜕化为我们熟悉的onehot vector。</p><p>但是如果按照传统GAN的思路来分析，这不是作死吗？一边是hard onehot vector，另一边是soft probability vector，判别器一下子就能够区分它们，生成器还怎么学习？没关系，对于WGAN来说，真假样本好不好区分并不是问题，WGAN只是拉近两个分布之间的Wasserstein距离，就算是一边是hard onehot另一边是soft probability也可以拉近，在训练过程中，概率向量中的有些项可能会慢慢变成0.8、0.9到接近1，整个向量也会接近onehot，最后我们要真正输出sequence of index形式的样本时，只需要对这些概率向量取argmax得到最大概率的index就行了。</p><p>新的样本表示形式+WGAN的分布拉近能力是一个“黄金组合”，但除此之外，还有其他因素帮助论文作者跑出上图的效果，包括：</p><ul><li>文本粒度为英文字符，而非英文单词，所以字典大小才二三十，大大减小了搜索空间</li><li>文本长度也才32</li><li>生成器用的不是常见的LSTM架构，而是多层反卷积网络，输入一个高斯噪声向量，直接一次性转换出所有32个字符</li></ul><p><strong>最后说回gradient penalty的实现问题。</strong>loss中本身包含梯度，优化loss就需要求梯度的梯度，这个功能并不是现在所有深度学习框架的标配功能，不过好在Tensorflow就有提供这个接口—<code>tf.gradients</code>。开头链接的GitHub源码中就是这么写的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># interpolates就是随机插值采样得到的图像，gradients就是loss中的梯度惩罚项</div><div class="line">gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]</div></pre></td></tr></table></figure><p>完整的loss是这样实现的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">gen_cost = -tf.reduce_mean(disc_fake)</div><div class="line">disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)</div><div class="line"></div><div class="line">alpha = tf.random_uniform(</div><div class="line">	shape=[BATCH_SIZE,<span class="number">1</span>], </div><div class="line">	minval=<span class="number">0.</span>,</div><div class="line">	maxval=<span class="number">1.</span></div><div class="line">)</div><div class="line">differences = fake_data - real_data</div><div class="line">interpolates = real_data + (alpha*differences)</div><div class="line">gradients = tf.gradients(Discriminator(interpolates), [interpolates])[<span class="number">0</span>]</div><div class="line">slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[<span class="number">1</span>]))</div><div class="line">gradient_penalty = tf.reduce_mean((slopes<span class="number">-1.</span>)**<span class="number">2</span>)</div><div class="line">disc_cost += LAMBDA*gradient_penalty</div><div class="line"></div><div class="line">gen_train_op = tf.train.AdamOptimizer(</div><div class="line">	learning_rate=<span class="number">1e-4</span>, </div><div class="line">	beta1=<span class="number">0.5</span>,</div><div class="line">	beta2=<span class="number">0.9</span></div><div class="line">).minimize(gen_cost, var_list=gen_params)</div><div class="line">disc_train_op = tf.train.AdamOptimizer(</div><div class="line">	learning_rate=<span class="number">1e-4</span>, </div><div class="line">	beta1=<span class="number">0.5</span>, </div><div class="line">	beta2=<span class="number">0.9</span></div><div class="line">).minimize(disc_cost, var_list=disc_params)</div></pre></td></tr></table></figure><p>对于我这样的PyTorch党就非常不幸了，高阶梯度的功能还在开发，感兴趣的PyTorch党可以订阅这个GitHub的pull request：<a href="https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/pull/1016" target="_blank" rel="external">Autograd refactor</a>，如果它被merged了话就可以在最新版中使用高阶梯度的功能实现gradient penalty了。</p><p>但是除了等待我们就没有别的办法了吗？<strong>其实可能是有的，我想到了一种近似方法来实现gradient penalty，只需要把微分换成差分：</strong><br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x_1 \sim \hat{x}, x_2 \sim \hat{x}} [ \frac{|D(x_1) - D(x_2)|}{| x_1 - x_2 |_p} - 1]^2<br>$$<br>也就是说，我们仍然是在分布 $P_{\hat{x}}$ 上随机采样，但是一次采两个，然后要求它们的连线斜率要接近1，这样理论上也可以起到跟公式9一样的效果，我自己在MNIST+MLP上简单验证过有作用，PyTorch党甚至Tensorflow党都可以尝试用一下。</p><hr><p><strong>作者：郑华滨链接：<a href="https://www.zhihu.com/question/52602529/answer/158727900" target="_blank" rel="external">https://www.zhihu.com/question/52602529/answer/158727900</a></strong></p><p><strong>来源：知乎</strong></p><p><strong>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</strong></p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/deep-learning/" rel="tag"># deep learning</a> <a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/GAN/" rel="tag"># GAN</a> <a href="/tags/WGAN/" rel="tag"># WGAN</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/07/10/Policy-Gradient-Methods/" rel="next" title="Policy Gradient Methods"><i class="fa fa-chevron-left"></i> Policy Gradient Methods</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/07/20/sklearn-based-feature-engineering/" rel="prev" title="sklearn-based feature engineering">sklearn-based feature engineering <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">88</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">48</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/07/18/WGAN-GP/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(e,a.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>