<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="machine learning,deep learning,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="The blog source:https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504.I have added the PyTorch implementation fromhttps://github.com/gpleiss/efficient_den"><meta property="og:type" content="article"><meta property="og:title" content="Note of the DenseNet (contains TensorFlow and PyTorch Implementation)"><meta property="og:url" content="http://yoursite.com/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="The blog source:https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504.I have added the PyTorch implementation fromhttps://github.com/gpleiss/efficient_den"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*7WdURialIGTojNI9ltrplA.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*N_o10WxGx_e6vBSNnV5k1A.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*1F8wom5X1DeCj5BWeGfJbw.jpeg"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*-C2QoqhfCjG8xZRu-UwO_Q.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*6JoB4BeIZyWDNGH5U63y_Q.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*-CQyW4huxxxMYpffb72emg.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*SSn5H14SKhhaZZ5XYWN3Cg.jpeg"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*FvW30tjOQ2hDQ7LdEkCRLQ.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*N-nU5WXQcrNfhnQMwxquFA.png"><meta property="og:updated_time" content="2017-11-20T05:23:15.767Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Note of the DenseNet (contains TensorFlow and PyTorch Implementation)"><meta name="twitter:description" content="The blog source:https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504.I have added the PyTorch implementation fromhttps://github.com/gpleiss/efficient_den"><meta name="twitter:image" content="https://cdn-images-1.medium.com/max/1600/1*7WdURialIGTojNI9ltrplA.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/"><title>Note of the DenseNet (contains TensorFlow and PyTorch Implementation) | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Note of the DenseNet (contains TensorFlow and PyTorch Implementation)</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-11-20T12:15:43+08:00">2017-11-20 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/" itemprop="commentsCount"></span> </a></span><span id="/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/" class="leancloud_visitors" data-flag-title="Note of the DenseNet (contains TensorFlow and PyTorch Implementation)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p><strong>The blog source:</strong></p><p><strong><a href="https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504" target="_blank" rel="external">https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504</a>.</strong></p><p>I have added the PyTorch implementation from</p><p><strong><a href="https://github.com/gpleiss/efficient_densenet_pytorch" target="_blank" rel="external">https://github.com/gpleiss/efficient_densenet_pytorch</a>.</strong></p></blockquote><p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">DenseNet</a>(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. It’s quite similar to <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet</a> but has some fundamental differences.</p><p>With all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*7WdURialIGTojNI9ltrplA.png" alt="img"></p><p><em>Error rates on various datasets(from source paper)</em></p><p>And for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*N_o10WxGx_e6vBSNnV5k1A.png" alt="img"></p><p><em>Сomparison of the DenseNet and ResNet Top-1 error rates on the ImageNet classification dataset as a function of learned parameters (left) and flops during test-time (right)(from source paper).</em></p><p>This post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:</p><ul><li>Why dense net differs from another convolution networks.</li><li>What difficulties I’ve met during the implementation of DenseNet in tensorflow.</li></ul><p>If you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the <a href="https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#aabd" target="_blank" rel="external">second chapter</a> or check the <a href="https://github.com/ikhlestov/vision_networks" target="_blank" rel="external">source code on GitHub</a>. If you not familiar with any topics but want to get some knowledge — I highly advise you <a href="http://cs231n.github.io/" target="_blank" rel="external">CS231n Stanford classes</a>.</p><h4 id="Compare-DenseNet-with-other-Convolution-Networks"><a href="#Compare-DenseNet-with-other-Convolution-Networks" class="headerlink" title="Compare DenseNet with other Convolution Networks"></a>Compare DenseNet with other Convolution Networks</h4><p>Usually, ConvNets work such way:<br>We have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.<br>So the output from the Lᵢ layer is input to the Lᵢ₊₁ layer. It seems like this:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*1F8wom5X1DeCj5BWeGfJbw.jpeg" alt="img"></p><p><em>source: &lt;<a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">http://cs231n.github.io/convolutional-networks/</a></em>&gt;</p><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet</a> architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the Lᵢ layer was obtained by summation of outputs from previous layers.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-C2QoqhfCjG8xZRu-UwO_Q.png" alt="img"></p><p>In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.<br>So, let’s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.<br>The output from Lᵢ layer will be (28, 28, 12).<br>But input to the Lᵢ₊₁ will be (28, 28, 24+12), for Lᵢ₊₂ (28, 28, 24 + 12 + 12) and so on.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*6JoB4BeIZyWDNGH5U63y_Q.png" alt="img"></p><p><em>Block of convolution layers with results concatenated</em></p><p>After a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).<br>All these N layers are named Block in the paper. There’s also batch normalization, nonlinearity and dropout inside the block.<br>To reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-CQyW4huxxxMYpffb72emg.png" alt="img"></p><p><em>Transition layer</em></p><p>Now we can again pass the image through the block with N convolutions.<br>With this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.<br>Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*SSn5H14SKhhaZZ5XYWN3Cg.jpeg" alt="img"></p><p><em>Full DenseNet example with 3 blocks from source paper</em></p><h4 id="Notes-about-implementation"><a href="#Notes-about-implementation" class="headerlink" title="Notes about implementation"></a>Notes about implementation</h4><p>In the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.</p><p>First of all, it was not clear how many blocks should be used depends on depth. After I’ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.</p><p>Second I’ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .</p><p>Despite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> block <span class="keyword">in</span> range(required_blocks):</div><div class="line">    output = build_block(output)</div><div class="line">    <span class="keyword">if</span> block != (required_blocks — <span class="number">1</span>):</div><div class="line">        output = transition_layer(output)</div></pre></td></tr></table></figure><p>For weights initialization authors proposed use MRSA initialization(as per<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="external">this paper</a>). In tensorflow this initialization can be easy implemented with<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer" target="_blank" rel="external">variance scaling initializer</a>.</p><p>In the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">batch norm -&gt; relu -&gt; conv 1x1 -&gt; dropout -&gt; batch norm -&gt; relu -&gt; conv 3x3 -&gt; dropout -&gt; output.</div></pre></td></tr></table></figure><p>Despite two conv filters, only last output will be concatenated to the main pool of features.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*FvW30tjOQ2hDQ7LdEkCRLQ.png" alt="img"></p><p>Also at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*N-nU5WXQcrNfhnQMwxquFA.png" alt="img"></p><p>Where theta — some reduction values, in the range (0, 1).</p><p>In case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.</p><p>Last, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used — just divide every image pixel by 255, so we have pixels values in the range [0, 1].</p><p>At first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next I’ve implemented per channel normalization… And networks began works even worse. It was not clear for me why. So I’ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.</p><p>And some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># without this line next slice assignment will silently fail!</span></div><div class="line"><span class="comment"># at least in numpy 1.12.0</span></div><div class="line">images = images.astype(‘float64’)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(channels):</div><div class="line">    images[:, :, :, i] = (</div><div class="line">        (images[:, :, :, i] — self.images_means[i]) /</div><div class="line">         self.images_stds[i])</div></pre></td></tr></table></figure><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>DenseNets are powerful neural nets that achieve state of the art performance on many datasets. And it’s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.</p><a id="more"></a><h4 id="Appendix-PyTorch-Implementation-naive-version-100-lines"><a href="#Appendix-PyTorch-Implementation-naive-version-100-lines" class="headerlink" title="Appendix: PyTorch Implementation (naive version ~100 lines)"></a>Appendix: PyTorch Implementation (naive version ~100 lines)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># This implementation is based on the DenseNet-BC implementation in torchvision</span></div><div class="line"><span class="comment"># https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DenseLayer</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_features, growth_rate, bn_size, drop_rate)</span>:</span></div><div class="line">        super(_DenseLayer, self).__init__()</div><div class="line">        self.add_module(<span class="string">'norm.1'</span>, nn.BatchNorm2d(num_input_features)),</div><div class="line">        self.add_module(<span class="string">'relu.1'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">        self.add_module(<span class="string">'conv.1'</span>, nn.Conv2d(num_input_features, bn_size *</div><div class="line">                        growth_rate, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        self.add_module(<span class="string">'norm.2'</span>, nn.BatchNorm2d(bn_size * growth_rate)),</div><div class="line">        self.add_module(<span class="string">'relu.2'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">        self.add_module(<span class="string">'conv.2'</span>, nn.Conv2d(bn_size * growth_rate, growth_rate,</div><div class="line">                        kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        self.drop_rate = drop_rate</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        new_features = super(_DenseLayer, self).forward(x)</div><div class="line">        <span class="keyword">if</span> self.drop_rate &gt; <span class="number">0</span>:</div><div class="line">            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)</div><div class="line">        <span class="keyword">return</span> torch.cat([x, new_features], <span class="number">1</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_Transition</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_features, num_output_features)</span>:</span></div><div class="line">        super(_Transition, self).__init__()</div><div class="line">        self.add_module(<span class="string">'norm'</span>, nn.BatchNorm2d(num_input_features))</div><div class="line">        self.add_module(<span class="string">'relu'</span>, nn.ReLU(inplace=<span class="keyword">True</span>))</div><div class="line">        self.add_module(<span class="string">'conv'</span>, nn.Conv2d(num_input_features, num_output_features,</div><div class="line">                                          kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>))</div><div class="line">        self.add_module(<span class="string">'pool'</span>, nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DenseBlock</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate)</span>:</span></div><div class="line">        super(_DenseBlock, self).__init__()</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</div><div class="line">            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)</div><div class="line">            self.add_module(<span class="string">'denselayer%d'</span> % (i + <span class="number">1</span>), layer)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseNet</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">r"""Densenet-BC model class, based on</span></div><div class="line">    `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`</div><div class="line">    Args:</div><div class="line">        growth_rate (int) - how many filters to add each layer (`k` in paper)</div><div class="line">        block_config (list of 3 or 4 ints) - how many layers in each pooling block</div><div class="line">        num_init_features (int) - the number of filters to learn in the first convolution layer</div><div class="line">        bn_size (int) - multiplicative factor for number of bottle neck layers</div><div class="line">            (i.e. bn_size * k features in the bottleneck layer)</div><div class="line">        drop_rate (float) - dropout rate after each dense layer</div><div class="line">        num_classes (int) - number of classification classes</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, growth_rate=<span class="number">12</span>, block_config=<span class="params">(<span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>)</span>, compression=<span class="number">0.5</span>,</span></span></div><div class="line">                 num_init_features=<span class="number">24</span>, bn_size=<span class="number">4</span>, drop_rate=<span class="number">0</span>, avgpool_size=<span class="number">8</span>,</div><div class="line">                 num_classes=<span class="number">10</span>):</div><div class="line"></div><div class="line">        super(DenseNet, self).__init__()</div><div class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt; compression &lt;= <span class="number">1</span>, <span class="string">'compression of densenet should be between 0 and 1'</span></div><div class="line">        self.avgpool_size = avgpool_size</div><div class="line"></div><div class="line">        <span class="comment"># First convolution</span></div><div class="line">        self.features = nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'conv0'</span>, nn.Conv2d(<span class="number">3</span>, num_init_features, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">        <span class="comment"># Each denseblock</span></div><div class="line">        num_features = num_init_features</div><div class="line">        <span class="keyword">for</span> i, num_layers <span class="keyword">in</span> enumerate(block_config):</div><div class="line">            block = _DenseBlock(num_layers=num_layers,</div><div class="line">                                num_input_features=num_features,</div><div class="line">                                bn_size=bn_size, growth_rate=growth_rate,</div><div class="line">                                drop_rate=drop_rate)</div><div class="line">            self.features.add_module(<span class="string">'denseblock%d'</span> % (i + <span class="number">1</span>), block)</div><div class="line">            num_features = num_features + num_layers * growth_rate</div><div class="line">            <span class="keyword">if</span> i != len(block_config) - <span class="number">1</span>:</div><div class="line">                trans = _Transition(num_input_features=num_features,</div><div class="line">                                    num_output_features=int(num_features</div><div class="line">                                                            * compression))</div><div class="line">                self.features.add_module(<span class="string">'transition%d'</span> % (i + <span class="number">1</span>), trans)</div><div class="line">                num_features = int(num_features * compression)</div><div class="line"></div><div class="line">        <span class="comment"># Final batch norm</span></div><div class="line">        self.features.add_module(<span class="string">'norm_final'</span>, nn.BatchNorm2d(num_features))</div><div class="line"></div><div class="line">        <span class="comment"># Linear layer</span></div><div class="line">        self.classifier = nn.Linear(num_features, num_classes)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        features = self.features(x)</div><div class="line">        out = F.relu(features, inplace=<span class="keyword">True</span>)</div><div class="line">        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(</div><div class="line">                           features.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">        out = self.classifier(out)</div><div class="line">        <span class="keyword">return</span> out</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/deep-learning/" rel="tag"># deep learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/11/14/Word2Vec-The-Skip-Gram-Model/" rel="next" title="Word2Vec: The Skip-Gram Model"><i class="fa fa-chevron-left"></i> Word2Vec: The Skip-Gram Model</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2017/11/21/The-Right-Way-to-Oversample-in-Predictive-Modeling/" rel="prev" title="The Right Way to Oversample in Predictive Modeling">The Right Way to Oversample in Predictive Modeling <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">92</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">50</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Compare-DenseNet-with-other-Convolution-Networks"><span class="nav-number">1.</span> <span class="nav-text">Compare DenseNet with other Convolution Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Notes-about-implementation"><span class="nav-number">2.</span> <span class="nav-text">Notes about implementation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Conclusion"><span class="nav-number">3.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Appendix-PyTorch-Implementation-naive-version-100-lines"><span class="nav-number">4.</span> <span class="nav-text">Appendix: PyTorch Implementation (naive version ~100 lines)</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2017</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>