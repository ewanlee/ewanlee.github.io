<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="deep learning,machine learning,CNN,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Lessons from a Dropped BallImagine we drop a ball from some height onto the ground, where it only has one dimension of motion. How likely is it that a ball will go a distance $c$ if you drop it and th"><meta property="og:type" content="article"><meta property="og:title" content="Understanding Convolutions"><meta property="og:url" content="http://yoursite.com/2018/01/18/Understanding-Convolutions/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Lessons from a Dropped BallImagine we drop a ball from some height onto the ground, where it only has one dimension of motion. How likely is it that a ball will go a distance $c$ if you drop it and th"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-fagb.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-split-21.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-splits-12-03.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-OnePath.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-SumPaths.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Reverse.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-BackProb.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Align.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Sep.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Wiki-BoxConvAnim.gif"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-TwoDim.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/RiverTrain-ImageConvDiagram.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Blur.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Edge.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY-W.png"><meta property="og:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv2-5x5-Conv2-XY.png"><meta property="og:updated_time" content="2018-01-18T10:26:33.245Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Understanding Convolutions"><meta name="twitter:description" content="Lessons from a Dropped BallImagine we drop a ball from some height onto the ground, where it only has one dimension of motion. How likely is it that a ball will go a distance $c$ if you drop it and th"><meta name="twitter:image" content="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-fagb.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2018/01/18/Understanding-Convolutions/"><title>Understanding Convolutions | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/18/Understanding-Convolutions/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Understanding Convolutions</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-18T17:01:29+08:00">2018-01-18 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/18/Understanding-Convolutions/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/18/Understanding-Convolutions/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/18/Understanding-Convolutions/" class="leancloud_visitors" data-flag-title="Understanding Convolutions"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Lessons-from-a-Dropped-Ball"><a href="#Lessons-from-a-Dropped-Ball" class="headerlink" title="Lessons from a Dropped Ball"></a>Lessons from a Dropped Ball</h2><p>Imagine we drop a ball from some height onto the ground, where it only has one dimension of motion. <em>How likely is it that a ball will go a distance $c$ if you drop it and then drop it again from above the point at which it landed?</em></p><p>Let’s break this down. After the first drop, it will land aa units away from the starting point with probability $f(a)$, where $f$ is the probability distribution.</p><p>Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed. The probability of the ball rolling $b$ units away from the new starting point is $g(b)$, where $g$ may be a different probability distribution if it’s dropped from a different height.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-fagb.png" alt="img"></p><p>If we fix the result of the first drop so we know the ball went distance aa, for the ball to go a total distance cc, the distance traveled in the second drop is also fixed at bb, where $a+b=c$. So the probability of this happening is simply $f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn1" target="_blank" rel="external">1</a></p><p>Let’s think about this with a specific discrete example. We want the total distance $c$ to be 3. If the first time it rolls, $a=2$, the second time it must roll $b=1$ in order to reach our total distance $a+b=3$. The probability of this is $f(2)⋅g(1)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-split-21.png" alt="img"></p><p>However, this isn’t the only way we could get to a total distance of 3. The ball could roll 1 units the first time, and 2 the second. Or 0 units the first time and all 3 the second. It could go any $a$ and $b$, as long as they add to 3.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-splits-12-03.png" alt="img"></p><p>The probabilities are $f(1)⋅g(2)$ and $f(0)⋅g(3)$, respectively.</p><p>In order to find the <em>total likelihood</em> of the ball reaching a total distance of $c$, we can’t consider only one possible way of reaching $c$. Instead, we consider <em>all the possible ways</em> of partitioning $c$ into two drops $a$ and $b$ and sum over the <em>probability of each way</em>.<br>$$<br>…\;\; f(0)\cdot g(3) ~+~ f(1)\cdot g(2) ~+~ f(2)\cdot g(1)\;\;…<br>$$<br>We already know that the probability for each case of $a+b=c$ is simply $f(a)⋅g(b)$. So, summing over every solution to $a+b=c$, we can denote the total likelihood as:<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Turns out, we’re doing a convolution! In particular, the convolution of $f$ and $g$, evluated at $c$ is defined:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)~~~~<br>$$<br>If we substitute $b=c−a$, we get:<br>$$<br>(f\ast g)(c) = \sum_a f(a) \cdot g(c-a)<br>$$<br>This is the standard definition<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn2" target="_blank" rel="external">2</a> of convolution.</p><p>To make this a bit more concrete, we can think about this in terms of positions the ball might land. After the first drop, it will land at an intermediate position aa with probability $f(a)$. If it lands at $a$, it has probability $g(c−a)$ of landing at a position $c$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-OnePath.png" alt="img"></p><p>To get the convolution, we consider all intermediate positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-SumPaths.png" alt="img"></p><h2 id="Visualizing-Convolutions"><a href="#Visualizing-Convolutions" class="headerlink" title="Visualizing Convolutions"></a>Visualizing Convolutions</h2><p>There’s a very nice trick that helps one think about convolutions more easily.</p><p>First, an observation. Suppose the probability that a ball lands a certain distance $x$ from where it started is $f(x)$. Then, afterwards, the probability that it started a distance $x$ from where it landed is $f(−x)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Reverse.png" alt="img"></p><p>If we know the ball lands at a position $c$ after the second drop, what is the probability that the previous position was $a$?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-BackProb.png" alt="img"></p><p>So the probability that the previous position was $a$ is $g(−(a−c))=g(c−a)$.</p><p>Now, consider the probability each intermediate position contributes to the ball finally landing at $c$. We know the probability of the first drop putting the ball into the intermediate position a is $f(a)$. We also know that the probability of it having been in $a$, if it lands at $c$ is $g(c−a)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate.png" alt="img"></p><p>Summing over the $a$s, we get the convolution.</p><p>The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value $c$ in a single picture. By shifting the bottom half around, we can evaluate the convolution at other values of $c$. This allows us to understand the convolution as a whole.</p><p>For example, we can see that it peaks when the distributions align.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Align.png" alt="img"></p><p>And shrinks as the intersection between the distributions gets smaller.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Sep.png" alt="img"></p><p>By using this trick in an animation, it really becomes possible to visually understand convolutions.</p><p>Below, we’re able to visualize the convolution of two box functions:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Wiki-BoxConvAnim.gif" alt="Wiki-BoxConvAnim"></p><p><em>From Wikipedia</em></p><p>Armed with this perspective, a lot of things become more intuitive.</p><p>Let’s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy.</p><h2 id="Higher-Dimensional-Convolutions"><a href="#Higher-Dimensional-Convolutions" class="headerlink" title="Higher Dimensional Convolutions"></a>Higher Dimensional Convolutions</h2><p>Convolutions are an extremely general idea. We can also use them in a higher number of dimensions.</p><p>Let’s consider our example of a falling ball again. Now, as it falls, it’s position shifts not only in one dimension, but in two.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-TwoDim.png" alt="img"></p><p>Convolution is the same as before:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Except, now $a$, $b$ and $c$ are vectors. To be more explicit,<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{\begin{array}{c}a_1+b_1=c_1\\a_2+b_2=c_2\end{array}} f(a_1,a_2) \cdot g(b_1,b_2)<br>$$<br>Or in the standard definition:<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{a_1, a_2} f(a_1, a_2) \cdot g(c_1-a_1,~ c_2-a_2)<br>$$<br>Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding.</p><p>One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a “kernel.”</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/RiverTrain-ImageConvDiagram.png" alt="RiverTrain-ImageConvDiagram"></p><p><em>From the <a href="http://intellabs.github.io/RiverTrail/tutorial/" target="_blank" rel="external">River Trail documentation</a></em></p><p>The kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over.</p><p>For example, by averaging a 3x3 box of pixels, we can blur an image. To do this, our kernel takes the value $\dfrac{1}{9}$ on each pixel in the box,</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Blur.png" alt="Gimp-Blur"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>We can also detect edges by taking the values −1−1 and 11 on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Edge.png" alt="Gimp-Edge"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>The gimp documentation has <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">many other examples</a>.</p><h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><p>So, how does convolution relate to convolutional neural networks?</p><p>Consider a 1-dimensional convolutional layer with inputs $\{x_n\}$ and outputs $\{y_n\}$, like we discussed in the <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" target="_blank" rel="external">previous post</a>:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY.png" alt="img"></p><p>As we observed, we can describe the outputs in terms of the inputs:<br>$$<br>y_n = A(x_{n}, x_{n+1}, …)<br>$$<br>Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.</p><p>Recall that a typical neuron in a neural network is described by:<br>$$<br>\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~…~ + b)<br>$$<br>Where $x_0$, $x_1$… are the inputs. The weights, $w_0$, $w_1$, … describe how the neuron connects to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn3" target="_blank" rel="external">3</a> Saying that multiple neurons are identical is the same thing as saying that the weights are the same.</p><p>It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.</p><p>Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:<br>$$<br>y = \sigma(Wx + b)<br>$$<br>For example, we get:<br>$$<br>y_0 = \sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 …)<br>$$</p><p>$$<br>y_1 = \sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 …)<br>$$</p><p>Each row of the matrix describes the weights connecting a neuron to its inputs.</p><p>Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY-W.png" alt="img"></p><p>Which corresponds to the equations:<br>$$<br>y_0 = \sigma(W_0x_0 + W_1x_1 -b)<br>$$</p><p>$$<br>y_1 = \sigma(W_0x_1 + W_1x_2 -b)<br>$$</p><p>So while, normally, a weight matrix connects every input to every neuron with different weights:<br>$$<br>W = \left[\begin{array}{ccccc}<br>W_{0,0} &amp; W_{0,1} &amp; W_{0,2} &amp; W_{0,3} &amp; …\\<br>W_{1,0} &amp; W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; …\\<br>W_{2,0} &amp; W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; …\\<br>W_{3,0} &amp; W_{3,1} &amp; W_{3,2} &amp; W_{3,3} &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don’t connect to many possible inputs, there’s lots of zeros.<br>$$<br>W = \left[\begin{array}{ccccc}<br>w_0 &amp; w_1 &amp; 0 &amp; 0 &amp; …\\<br>0 &amp; w_0 &amp; w_1 &amp; 0 &amp; …\\<br>0 &amp; 0 &amp; w_0 &amp; w_1 &amp; …\\<br>0 &amp; 0 &amp; 0 &amp; w_0 &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>Multiplying by the above matrix is the same thing as convolving with $[…0, w_1, w_0, 0…]$. The function sliding to different positions corresponds to having neurons at those positions.</p><p>What about two-dimensional convolutional layers?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv2-5x5-Conv2-XY.png" alt="img"></p><p>The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.</p><p>Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image.</p><ol><li><p>We want the probability of the ball rolling aa units the first time and also rolling bb units the second time. The distributions $P(A)=f(a)$ and $P(b)=g(b)$ are independent, with both distributions centered at 0. So $P(a,b)=P(a)∗P(b)=f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref1" target="_blank" rel="external">↩</a></p></li><li><p>The non-standard definition, which I haven’t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious.</p><p>For example, convolution is a commutative operation. That is, $f∗g=g∗f$. Why?</p><p>​<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b) ~~=~ \sum_{b+a=c} g(b) \cdot f(a)<br>$$<br>Convolution is also associative. That is, $(f∗g)∗h=f∗(g∗h)$. Why?</p><p>​<br>$$<br>\sum_{(a+b)+c=d} (f(a) \cdot g(b)) \cdot h(c) ~~=~ \sum_{a+(b+c)=d} f(a) \cdot (g(b) \cdot h(c))<br>$$<br>↩</p><p>​</p></li><li><p>There’s also the bias, which is the “threshold” for whether the neuron fires, but it’s much simpler and I don’t want to clutter this section talking about it.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref3" target="_blank" rel="external">↩</a></p></li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/deep-learning/" rel="tag"># deep learning</a> <a href="/tags/machine-learning/" rel="tag"># machine learning</a> <a href="/tags/CNN/" rel="tag"># CNN</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/01/17/PCA-With-Tensorflow/" rel="next" title="PCA With Tensorflow"><i class="fa fa-chevron-left"></i> PCA With Tensorflow</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" rel="prev" title="Seq2Seq with Attention and Beam Search [Repost]">Seq2Seq with Attention and Beam Search [Repost] <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">131</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">64</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/tomaxent" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Lessons-from-a-Dropped-Ball"><span class="nav-number">1.</span> <span class="nav-text">Lessons from a Dropped Ball</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Visualizing-Convolutions"><span class="nav-number">2.</span> <span class="nav-text">Visualizing Convolutions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Higher-Dimensional-Convolutions"><span class="nav-number">3.</span> <span class="nav-text">Higher Dimensional Convolutions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Convolutional-Neural-Networks"><span class="nav-number">4.</span> <span class="nav-text">Convolutional Neural Networks</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2018/01/18/Understanding-Convolutions/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>