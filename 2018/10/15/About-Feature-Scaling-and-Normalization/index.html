<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="machine learning,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="About standardizationThe result of standardization (or Z-score normalization) is that the features will be rescaled so that they’ll have the properties of a standard normal distribution with $\mu=0$ a"><meta property="og:type" content="article"><meta property="og:title" content="About Feature Scaling and Normalization"><meta property="og:url" content="http://yoursite.com/2018/10/15/About-Feature-Scaling-and-Normalization/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="About standardizationThe result of standardization (or Z-score normalization) is that the features will be rescaled so that they’ll have the properties of a standard normal distribution with $\mu=0$ a"><meta property="og:image" content="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_44_0.png"><meta property="og:image" content="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_48_0.png"><meta property="og:image" content="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_64_0.png"><meta property="og:image" content="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_89_0.png"><meta property="og:updated_time" content="2018-10-15T14:05:31.310Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="About Feature Scaling and Normalization"><meta name="twitter:description" content="About standardizationThe result of standardization (or Z-score normalization) is that the features will be rescaled so that they’ll have the properties of a standard normal distribution with $\mu=0$ a"><meta name="twitter:image" content="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_44_0.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2018/10/15/About-Feature-Scaling-and-Normalization/"><title>About Feature Scaling and Normalization | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/15/About-Feature-Scaling-and-Normalization/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">About Feature Scaling and Normalization</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-15T21:48:20+08:00">2018-10-15 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/10/15/About-Feature-Scaling-and-Normalization/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/10/15/About-Feature-Scaling-and-Normalization/" itemprop="commentsCount"></span> </a></span><span id="/2018/10/15/About-Feature-Scaling-and-Normalization/" class="leancloud_visitors" data-flag-title="About Feature Scaling and Normalization"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="About-standardization"><a href="#About-standardization" class="headerlink" title="About standardization"></a>About standardization</h2><p>The result of <strong>standardization</strong> (or <strong>Z-score normalization</strong>) is that the features will be rescaled so that they’ll have the properties of a standard normal distribution with $\mu=0$ and $\sigma=1$</p><p>where $\mu$ is the mean (average) and $\sigma$ is the standard deviation from the mean; standard scores (also called <strong>z</strong> scores) of the samples are calculated as follows:<br>$$<br>z = \frac{x - \mu}{\sigma}<br>$$<br>Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. Intuitively, we can think of gradient descent as a prominent example (an optimization algorithm often used in logistic regression, SVMs, perceptrons, neural networks etc.); with features being on different scales, certain weights may update faster than others since the feature values $x_j$ play a role in the weight updates<br>$$<br>\Delta w_j = - \eta \frac{\partial J}{\partial w_j} = \eta \sum_i (t^{(i)} - o^{(i)}) x_j^{(i)},<br>$$<br>so that</p><p>$w_j := w_j + \Delta w_j$, where $\eta$ is the learning rate, $t$ the target class label, and $o$ the actual output. Other intuitive examples include K-Nearest Neighbor algorithms and clustering algorithms that use, for example, Euclidean distance measures – in fact, tree-based classifier are probably the only classifiers where feature scaling doesn’t make a difference.</p><p>In fact, the only family of algorithms that I could think of being scale-invariant are tree-based methods. Let’s take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as “is feature x_i &gt;= some_val?” Intuitively, we can see that it really doesn’t matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale – it really doesn’t matter).</p><p>Some examples of algorithms where feature scaling matters are:</p><ul><li>k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally</li><li>k-means (see k-nearest neighbors)</li><li>logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others</li><li>linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not.</li></ul><p>In addition, we’d also want to think about whether we want to “standardize” or “normalize” (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights “equally.” As a rule of thumb I’d say: When in doubt, just standardize the data, it shouldn’t hurt.</p><h2 id="About-Min-Max-scaling"><a href="#About-Min-Max-scaling" class="headerlink" title="About Min-Max scaling"></a>About Min-Max scaling</h2><p>An alternative approach to Z-score normalization (or standardization) is the so-called <strong>Min-Max scaling</strong>(often also simply called “normalization” - a common cause for ambiguities).<br>In this approach, the data is scaled to a fixed range - usually 0 to 1.<br>The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.</p><p>A Min-Max scaling is typically done via the following equation:<br>$$<br>X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}<br>$$</p><h2 id="Z-score-standardization-or-Min-Max-scaling"><a href="#Z-score-standardization-or-Min-Max-scaling" class="headerlink" title="Z-score standardization or Min-Max scaling?"></a>Z-score standardization or Min-Max scaling?</h2><p><em>“Standardization or Min-Max scaling?”</em> - There is no obvious answer to this question: it really depends on the application.</p><p>For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; <a href="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html" target="_blank" rel="external">but more about PCA in my previous article</a>).</p><p>However, this doesn’t mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale.</p><h2 id="Standardizing-and-normalizing-how-it-can-be-done-using-scikit-learn"><a href="#Standardizing-and-normalizing-how-it-can-be-done-using-scikit-learn" class="headerlink" title="Standardizing and normalizing - how it can be done using scikit-learn"></a>Standardizing and normalizing - how it can be done using scikit-learn</h2><p>Of course, we could make use of NumPy’s vectorization capabilities to calculate the z-scores for standardization and to normalize the data using the equations that were mentioned in the previous sections. However, there is an even more convenient approach using the preprocessing module from one of Python’s open-source machine learning library <a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn</a>.</p><p>For the following examples and discussion, we will have a look at the free “Wine” Dataset that is deposited on the UCI machine learning repository<br>(<a href="http://archive.ics.uci.edu/ml/datasets/Wine" target="_blank" rel="external">http://archive.ics.uci.edu/ml/datasets/Wine</a>).</p><blockquote><p>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</p><p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml" target="_blank" rel="external">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p></blockquote><p>The Wine dataset consists of 3 different classes where each row correspond to a particular wine sample.</p><p>The class labels (1, 2, 3) are listed in the first column, and the columns 2-14 correspond to 13 different attributes (features):</p><p>1) Alcohol<br>2) Malic acid<br>…</p><h4 id="Loading-the-wine-dataset"><a href="#Loading-the-wine-dataset" class="headerlink" title="Loading the wine dataset"></a>Loading the wine dataset</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.io.parsers.read_csv(</div><div class="line">    <span class="string">'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv'</span>,</div><div class="line">     header=<span class="keyword">None</span>,</div><div class="line">     usecols=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</div><div class="line">    )</div><div class="line"></div><div class="line">df.columns=[<span class="string">'Class label'</span>, <span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]</div><div class="line"></div><div class="line">df.head()</div></pre></td></tr></table></figure><table><thead><tr><th></th><th>Class label</th><th>Alcohol</th><th>Malic acid</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>14.23</td><td>1.71</td></tr><tr><td>1</td><td>1</td><td>13.20</td><td>1.78</td></tr><tr><td>2</td><td>1</td><td>13.16</td><td>2.36</td></tr><tr><td>3</td><td>1</td><td>14.37</td><td>1.95</td></tr><tr><td>4</td><td>1</td><td>13.24</td><td>2.59</td></tr></tbody></table><p>As we can see in the table above, the features <strong>Alcohol</strong> (percent/volumne) and <strong>Malic acid</strong> (g/l) are measured on different scales, so that <strong>Feature Scaling</strong> is necessary important prior to any comparison or combination of these data.</p><h4 id="Standardization-and-Min-Max-scaling"><a href="#Standardization-and-Min-Max-scaling" class="headerlink" title="Standardization and Min-Max scaling"></a>Standardization and Min-Max scaling</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line">std_scale = preprocessing.StandardScaler().fit(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line">df_std = std_scale.transform(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line"></div><div class="line">minmax_scale = preprocessing.MinMaxScaler().fit(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line">df_minmax = minmax_scale.transform(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'Mean after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_std[:,<span class="number">0</span>].mean(), df_std[:,<span class="number">1</span>].mean()))</div><div class="line">print(<span class="string">'\nStandard deviation after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_std[:,<span class="number">0</span>].std(), df_std[:,<span class="number">1</span>].std()))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Mean after standardization:</div><div class="line">Alcohol=<span class="number">0.00</span>, Malic acid=<span class="number">0.00</span></div><div class="line"></div><div class="line">Standard deviation after standardization:</div><div class="line">Alcohol=<span class="number">1.00</span>, Malic acid=<span class="number">1.00</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'Min-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_minmax[:,<span class="number">0</span>].min(), df_minmax[:,<span class="number">1</span>].min()))</div><div class="line">print(<span class="string">'\nMax-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_minmax[:,<span class="number">0</span>].max(), df_minmax[:,<span class="number">1</span>].max()))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Min-value after min-max scaling:</div><div class="line">Alcohol=<span class="number">0.00</span>, Malic acid=<span class="number">0.00</span></div><div class="line"></div><div class="line">Max-value after min-max scaling:</div><div class="line">Alcohol=<span class="number">1.00</span>, Malic acid=<span class="number">1.00</span></div></pre></td></tr></table></figure><h4 id="Plotting"><a href="#Plotting" class="headerlink" title="Plotting"></a>Plotting</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">()</span>:</span></div><div class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</div><div class="line"></div><div class="line">    plt.scatter(df[<span class="string">'Alcohol'</span>], df[<span class="string">'Malic acid'</span>],</div><div class="line">            color=<span class="string">'green'</span>, label=<span class="string">'input scale'</span>, alpha=<span class="number">0.5</span>)</div><div class="line"></div><div class="line">    plt.scatter(df_std[:,<span class="number">0</span>], df_std[:,<span class="number">1</span>], color=<span class="string">'red'</span>,</div><div class="line">            label=<span class="string">'Standardized [$$N  (\mu=0, \; \sigma=1)$$]'</span>, alpha=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">    plt.scatter(df_minmax[:,<span class="number">0</span>], df_minmax[:,<span class="number">1</span>],</div><div class="line">            color=<span class="string">'blue'</span>, label=<span class="string">'min-max scaled [min=0, max=1]'</span>, alpha=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">    plt.title(<span class="string">'Alcohol and Malic Acid content of the wine dataset'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Alcohol'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Malic Acid'</span>)</div><div class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</div><div class="line">    plt.grid()</div><div class="line"></div><div class="line">    plt.tight_layout()</div><div class="line"></div><div class="line">plot()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_44_0.png" alt="png"></p><p>The plot above includes the wine datapoints on all three different scales: the input scale where the alcohol content was measured in volume-percent (green), the standardized features (red), and the normalized features (blue). In the following plot, we will zoom in into the three different axis-scales.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">fig, ax = plt.subplots(<span class="number">3</span>, figsize=(<span class="number">6</span>,<span class="number">14</span>))</div><div class="line"></div><div class="line"><span class="keyword">for</span> a,d,l <span class="keyword">in</span> zip(range(len(ax)),</div><div class="line">               (df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]].values, df_std, df_minmax),</div><div class="line">               (<span class="string">'Input scale'</span>,</div><div class="line">                <span class="string">'Standardized [$$N  (\mu=0, \; \sigma=1)$$]'</span>,</div><div class="line">                <span class="string">'min-max scaled [min=0, max=1]'</span>)</div><div class="line">                ):</div><div class="line">    <span class="keyword">for</span> i,c <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>)):</div><div class="line">        ax[a].scatter(d[df[<span class="string">'Class label'</span>].values == i, <span class="number">0</span>],</div><div class="line">                  d[df[<span class="string">'Class label'</span>].values == i, <span class="number">1</span>],</div><div class="line">                  alpha=<span class="number">0.5</span>,</div><div class="line">                  color=c,</div><div class="line">                  label=<span class="string">'Class %s'</span> %i</div><div class="line">                  )</div><div class="line">    ax[a].set_title(l)</div><div class="line">    ax[a].set_xlabel(<span class="string">'Alcohol'</span>)</div><div class="line">    ax[a].set_ylabel(<span class="string">'Malic Acid'</span>)</div><div class="line">    ax[a].legend(loc=<span class="string">'upper left'</span>)</div><div class="line">    ax[a].grid()</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_48_0.png" alt="png"></p><h2 id="Bottom-up-approaches"><a href="#Bottom-up-approaches" class="headerlink" title="Bottom-up approaches"></a>Bottom-up approaches</h2><p>Of course, we can also code the equations for standardization and 0-1 Min-Max scaling “manually”. However, the scikit-learn methods are still useful if you are working with test and training data sets and want to scale them equally.</p><p>E.g.,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">std_scale = preprocessing.StandardScaler().fit(X_train)</div><div class="line">X_train = std_scale.transform(X_train)</div><div class="line">X_test = std_scale.transform(X_test)</div></pre></td></tr></table></figure><p>Below, we will perform the calculations using “pure” Python code, and an more convenient NumPy solution, which is especially useful if we attempt to transform a whole matrix.</p><h3 id="Vanilla-Python"><a href="#Vanilla-Python" class="headerlink" title="Vanilla Python"></a>Vanilla Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Standardization</span></div><div class="line"></div><div class="line">x = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">mean = sum(x)/len(x)</div><div class="line">std_dev = (<span class="number">1</span>/len(x) * sum([ (x_i - mean)**<span class="number">2</span> <span class="keyword">for</span> x_i <span class="keyword">in</span> x]))**<span class="number">0.5</span></div><div class="line"></div><div class="line">z_scores = [(x_i - mean)/std_dev <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</div><div class="line"></div><div class="line"><span class="comment"># Min-Max scaling</span></div><div class="line"></div><div class="line">minmax = [(x_i - min(x)) / (max(x) - min(x)) <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</div></pre></td></tr></table></figure><h3 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Standardization</span></div><div class="line"></div><div class="line">x_np = np.asarray(x)</div><div class="line">z_scores_np = (x_np - x_np.mean()) / x_np.std()</div><div class="line"></div><div class="line"><span class="comment"># Min-Max scaling</span></div><div class="line"></div><div class="line">np_minmax = (x_np - x_np.min()) / (x_np.max() - x_np.min())</div></pre></td></tr></table></figure><h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><p>Just to make sure that our code works correctly, let us plot the results via matplotlib.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=<span class="number">2</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line"></div><div class="line">y_pos = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x))]</div><div class="line"></div><div class="line">ax1.scatter(z_scores, y_pos, color=<span class="string">'g'</span>)</div><div class="line">ax1.set_title(<span class="string">'Python standardization'</span>, color=<span class="string">'g'</span>)</div><div class="line"></div><div class="line">ax2.scatter(minmax, y_pos, color=<span class="string">'g'</span>)</div><div class="line">ax2.set_title(<span class="string">'Python Min-Max scaling'</span>, color=<span class="string">'g'</span>)</div><div class="line"></div><div class="line">ax3.scatter(z_scores_np, y_pos, color=<span class="string">'b'</span>)</div><div class="line">ax3.set_title(<span class="string">'Python NumPy standardization'</span>, color=<span class="string">'b'</span>)</div><div class="line"></div><div class="line">ax4.scatter(np_minmax, y_pos, color=<span class="string">'b'</span>)</div><div class="line">ax4.set_title(<span class="string">'Python NumPy Min-Max scaling'</span>, color=<span class="string">'b'</span>)</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> (ax1, ax2, ax3, ax4):</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.grid()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_64_0.png" alt="png"></p><h2 id="The-effect-of-standardization-on-PCA-in-a-pattern-classification-task"><a href="#The-effect-of-standardization-on-PCA-in-a-pattern-classification-task" class="headerlink" title="The effect of standardization on PCA in a pattern classification task"></a>The effect of standardization on PCA in a pattern classification task</h2><p>Earlier, I mentioned the Principal Component Analysis (PCA) as an example where standardization is crucial, since it is “analyzing” the variances of the different features.<br>Now, let us see how the standardization affects PCA and a following supervised classification on the whole wine dataset.</p><p>In the following section, we will go through the following steps:</p><ul><li>Reading in the dataset</li><li>Dividing the dataset into a separate training and test dataset</li><li>Standardization of the features</li><li>Principal Component Analysis (PCA) to reduce the dimensionality</li><li>Training a naive Bayes classifier</li><li>Evaluating the classification accuracy with and without standardization</li></ul><h3 id="Reading-in-the-dataset"><a href="#Reading-in-the-dataset" class="headerlink" title="Reading in the dataset"></a>Reading in the dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">df = pd.io.parsers.read_csv(</div><div class="line">    <span class="string">'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv'</span>,</div><div class="line">    header=<span class="keyword">None</span>,</div><div class="line">    )</div></pre></td></tr></table></figure><h3 id="Dividing-the-dataset-into-a-separate-training-and-test-dataset"><a href="#Dividing-the-dataset-into-a-separate-training-and-test-dataset" class="headerlink" title="Dividing the dataset into a separate training and test dataset"></a>Dividing the dataset into a separate training and test dataset</h3><p>In this step, we will randomly divide the wine dataset into a training dataset and a test dataset where the training dataset will contain 70% of the samples and the test dataset will contain 30%, respectively.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line">X_wine = df.values[:,<span class="number">1</span>:]</div><div class="line">y_wine = df.values[:,<span class="number">0</span>]</div><div class="line"></div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine,</div><div class="line">    test_size=<span class="number">0.30</span>, random_state=<span class="number">12345</span>)</div></pre></td></tr></table></figure><h3 id="Feature-Scaling-Standardization"><a href="#Feature-Scaling-Standardization" class="headerlink" title="Feature Scaling - Standardization"></a>Feature Scaling - Standardization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line">std_scale = preprocessing.StandardScaler().fit(X_train)</div><div class="line">X_train_std = std_scale.transform(X_train)</div><div class="line">X_test_std = std_scale.transform(X_test)</div></pre></td></tr></table></figure><h3 id="Dimensionality-reduction-via-Principal-Component-Analysis-PCA"><a href="#Dimensionality-reduction-via-Principal-Component-Analysis-PCA" class="headerlink" title="Dimensionality reduction via Principal Component Analysis (PCA)"></a>Dimensionality reduction via Principal Component Analysis (PCA)</h3><p>Now, we perform a PCA on the standardized and the non-standardized datasets to transform the dataset onto a 2-dimensional feature subspace.<br>In a real application, a procedure like cross-validation would be done in order to find out what choice of features would yield a optimal balance between “preserving information” and “overfitting” for different classifiers. However, we will omit this step since we don’t want to train a perfect classifier here, but merely compare the effects of standardization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"></div><div class="line"><span class="comment"># on non-standardized data</span></div><div class="line">pca = PCA(n_components=<span class="number">2</span>).fit(X_train)</div><div class="line">X_train = pca.transform(X_train)</div><div class="line">X_test = pca.transform(X_test)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># om standardized data</span></div><div class="line">pca_std = PCA(n_components=<span class="number">2</span>).fit(X_train_std)</div><div class="line">X_train_std = pca_std.transform(X_train_std)</div><div class="line">X_test_std = pca_std.transform(X_test_std)</div></pre></td></tr></table></figure><p>Let us quickly visualize how our new feature subspace looks like (note that class labels are not considered in a PCA - in contrast to a Linear Discriminant Analysis - but I will add them in the plot for clarity).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">fig, (ax1, ax2) = plt.subplots(ncols=<span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">4</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">for</span> l,c,m <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'blue'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>), (<span class="string">'^'</span>, <span class="string">'s'</span>, <span class="string">'o'</span>)):</div><div class="line">    ax1.scatter(X_train[y_train==l, <span class="number">0</span>], X_train[y_train==l, <span class="number">1</span>],</div><div class="line">        color=c,</div><div class="line">        label=<span class="string">'class %s'</span> %l,</div><div class="line">        alpha=<span class="number">0.5</span>,</div><div class="line">        marker=m</div><div class="line">        )</div><div class="line"></div><div class="line"><span class="keyword">for</span> l,c,m <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'blue'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>), (<span class="string">'^'</span>, <span class="string">'s'</span>, <span class="string">'o'</span>)):</div><div class="line">    ax2.scatter(X_train_std[y_train==l, <span class="number">0</span>], X_train_std[y_train==l, <span class="number">1</span>],</div><div class="line">        color=c,</div><div class="line">        label=<span class="string">'class %s'</span> %l,</div><div class="line">        alpha=<span class="number">0.5</span>,</div><div class="line">        marker=m</div><div class="line">        )</div><div class="line"></div><div class="line">ax1.set_title(<span class="string">'Transformed NON-standardized training dataset after PCA'</span>)    </div><div class="line">ax2.set_title(<span class="string">'Transformed standardized training dataset after PCA'</span>)    </div><div class="line"></div><div class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> (ax1, ax2):</div><div class="line"></div><div class="line">    ax.set_xlabel(<span class="string">'1st principal component'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'2nd principal component'</span>)</div><div class="line">    ax.legend(loc=<span class="string">'upper right'</span>)</div><div class="line">    ax.grid()</div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_89_0.png" alt="png"></p><h3 id="Training-a-naive-Bayes-classifier"><a href="#Training-a-naive-Bayes-classifier" class="headerlink" title="Training a naive Bayes classifier"></a>Training a naive Bayes classifier</h3><p>We will use a naive Bayes classifier for the classification task. If you are not familiar with it, the term “naive” comes from the assumption that all features are “independent”.<br>All in all, it is a simple but robust classifier based on Bayes’ rule</p><p>I don’t want to get into more detail about Bayes’ rule in this article, but if you are interested in a more detailed collection of examples, please have a look at the <a href="https://github.com/rasbt/pattern_classification#statistical-pattern-recognition-examples" target="_blank" rel="external">Statistical Patter Classification</a> in my pattern classification repository.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"></div><div class="line"><span class="comment"># on non-standardized data</span></div><div class="line">gnb = GaussianNB()</div><div class="line">fit = gnb.fit(X_train, y_train)</div><div class="line"></div><div class="line"><span class="comment"># on standardized data</span></div><div class="line">gnb_std = GaussianNB()</div><div class="line">fit_std = gnb_std.fit(X_train_std, y_train)</div></pre></td></tr></table></figure><h3 id="Evaluating-the-classification-accuracy-with-and-without-standardization"><a href="#Evaluating-the-classification-accuracy-with-and-without-standardization" class="headerlink" title="Evaluating the classification accuracy with and without standardization"></a>Evaluating the classification accuracy with and without standardization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"></div><div class="line">pred_train = gnb.predict(X_train)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the training dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;'</span>.format(metrics.accuracy_score(y_train, pred_train)))</div><div class="line"></div><div class="line">pred_test = gnb.predict(X_test)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the test dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;\n'</span>.format(metrics.accuracy_score(y_test, pred_test)))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Prediction accuracy <span class="keyword">for</span> the training dataset</div><div class="line"><span class="number">81.45</span>%</div><div class="line"></div><div class="line">Prediction accuracy <span class="keyword">for</span> the test dataset</div><div class="line"><span class="number">64.81</span>%</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">pred_train_std = gnb_std.predict(X_train_std)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the training dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;'</span>.format(metrics.accuracy_score(y_train, pred_train_std)))</div><div class="line"></div><div class="line">pred_test_std = gnb_std.predict(X_test_std)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the test dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;\n'</span>.format(metrics.accuracy_score(y_test, pred_test_std)))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Prediction accuracy <span class="keyword">for</span> the training dataset</div><div class="line"><span class="number">96.77</span>%</div><div class="line"></div><div class="line">Prediction accuracy <span class="keyword">for</span> the test dataset</div><div class="line"><span class="number">98.15</span>%</div></pre></td></tr></table></figure><p>As we can see, the standardization prior to the PCA definitely led to an decrease in the empirical error rate on classifying samples from test dataset.</p><h2 id="Appendix-A-The-effect-of-scaling-and-mean-centering-of-variables-prior-to-PCA"><a href="#Appendix-A-The-effect-of-scaling-and-mean-centering-of-variables-prior-to-PCA" class="headerlink" title="Appendix A: The effect of scaling and mean centering of variables prior to PCA"></a>Appendix A: The effect of scaling and mean centering of variables prior to PCA</h2><p>Let us think about whether it matters or not if the variables are centered for applications such as Principal Component Analysis (PCA) if the PCA is calculated from the covariance matrix (i.e., the kkprincipal components are the eigenvectors of the covariance matrix that correspond to the kk largest eigenvalues.</p><h3 id="1-Mean-centering-does-not-affect-the-covariance-matrix"><a href="#1-Mean-centering-does-not-affect-the-covariance-matrix" class="headerlink" title="1. Mean centering does not affect the covariance matrix"></a>1. Mean centering does not affect the covariance matrix</h3><p>Here, the rational is: If the covariance is the same whether the variables are centered or not, the result of the PCA will be the same.</p><p>Let’s assume we have the 2 variables $x$ and $y$ Then the covariance between the attributes is calculated as<br>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>Let us write the centered variables as<br>$$<br>x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}<br>$$<br>The centered covariance would then be calculated as follows:<br>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i’ - \bar{y}’)<br>$$<br>But since after centering, $\bar{x}’ = 0$ and $\bar{y}’ = 0$ we have</p><p>$\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} x_i’ y_i’$ which is our original covariance matrix if we resubstitute back the terms $x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}$.</p><p>Even centering only one variable, e.g., xx wouldn’t affect the covariance:</p><p>$$<br>\sigma_{\text{xy}} = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i - \bar{y})<br>$$</p><h3 id="2-Scaling-of-variables-does-affect-the-covariance-matrix"><a href="#2-Scaling-of-variables-does-affect-the-covariance-matrix" class="headerlink" title="2. Scaling of variables does affect the covariance matrix"></a>2. Scaling of variables does affect the covariance matrix</h3><p>If one variable is scaled, e.g, from pounds into kilogram (1 pound = 0.453592 kg), it does affect the covariance and therefore influences the results of a PCA.</p><p>Let cc be the scaling factor for $x$</p><p>Given that the “original” covariance is calculated as</p><p>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>the covariance after scaling would be calculated as:</p><p>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (c \cdot x_i - c \cdot \bar{x})(y_i - \bar{y}) = \frac{c}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y}) \Rightarrow \sigma_{xy}’ = c \cdot \sigma_{xy}<br>$$<br>Therefore, the covariance after scaling one attribute by the constant $c$ will result in a rescaled covariance $c \sigma_{xy}$ So if we’d scaled $x$ from pounds to kilograms, the covariance between $x$ and $y$ will be 0.453592 times smaller.</p><h3 id="3-Standardizing-affects-the-covariance"><a href="#3-Standardizing-affects-the-covariance" class="headerlink" title="3. Standardizing affects the covariance"></a>3. Standardizing affects the covariance</h3><p>Standardization of features will have an effect on the outcome of a PCA (assuming that the variables are originally not standardized). This is because we are scaling the covariance between every pair of variables by the product of the standard deviations of each pair of variables.</p><p>The equation for standardization of a variable is written as</p><p>$$<br>z = \frac{x_i - \bar{x}}{\sigma}<br>$$<br>The “original” covariance matrix:</p><p>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>And after standardizing both variables:</p><p>$$<br>x’ = \frac{x - \bar{x}}{\sigma_x} \text{ and } y’ =\frac{y - \bar{y}}{\sigma_y}<br>$$</p><p>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - 0)(y_i’ - 0) = \frac{1}{n-1} \sum_{i}^{n} \bigg(\frac{x - \bar{x}}{\sigma_x}\bigg)\bigg(\frac{y - \bar{y}}{\sigma_y}\bigg) = \frac{1}{(n-1) \cdot \sigma_x \sigma_y} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$</p><p>$$<br>\Rightarrow \sigma_{xy}’ = \frac{\sigma_{xy}}{\sigma_x \sigma_y}<br>$$</p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/machine-learning/" rel="tag"># machine learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/06/07/Install-SerpentAI-on-Windows-10/" rel="next" title="Install SerpentAI on Windows 10"><i class="fa fa-chevron-left"></i> Install SerpentAI on Windows 10</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018/10/15/数据预处理相关技术/" rel="prev" title="数据预处理相关技术">数据预处理相关技术 <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">124</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">59</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#About-standardization"><span class="nav-number">1.</span> <span class="nav-text">About standardization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#About-Min-Max-scaling"><span class="nav-number">2.</span> <span class="nav-text">About Min-Max scaling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Z-score-standardization-or-Min-Max-scaling"><span class="nav-number">3.</span> <span class="nav-text">Z-score standardization or Min-Max scaling?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Standardizing-and-normalizing-how-it-can-be-done-using-scikit-learn"><span class="nav-number">4.</span> <span class="nav-text">Standardizing and normalizing - how it can be done using scikit-learn</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Loading-the-wine-dataset"><span class="nav-number">4.0.1.</span> <span class="nav-text">Loading the wine dataset</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Standardization-and-Min-Max-scaling"><span class="nav-number">4.0.2.</span> <span class="nav-text">Standardization and Min-Max scaling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Plotting"><span class="nav-number">4.0.3.</span> <span class="nav-text">Plotting</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#Bottom-up-approaches"><span class="nav-number">5.</span> <span class="nav-text">Bottom-up approaches</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanilla-Python"><span class="nav-number">5.1.</span> <span class="nav-text">Vanilla Python</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NumPy"><span class="nav-number">5.2.</span> <span class="nav-text">NumPy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Visualization"><span class="nav-number">5.3.</span> <span class="nav-text">Visualization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-effect-of-standardization-on-PCA-in-a-pattern-classification-task"><span class="nav-number">6.</span> <span class="nav-text">The effect of standardization on PCA in a pattern classification task</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reading-in-the-dataset"><span class="nav-number">6.1.</span> <span class="nav-text">Reading in the dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dividing-the-dataset-into-a-separate-training-and-test-dataset"><span class="nav-number">6.2.</span> <span class="nav-text">Dividing the dataset into a separate training and test dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-Scaling-Standardization"><span class="nav-number">6.3.</span> <span class="nav-text">Feature Scaling - Standardization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dimensionality-reduction-via-Principal-Component-Analysis-PCA"><span class="nav-number">6.4.</span> <span class="nav-text">Dimensionality reduction via Principal Component Analysis (PCA)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-a-naive-Bayes-classifier"><span class="nav-number">6.5.</span> <span class="nav-text">Training a naive Bayes classifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating-the-classification-accuracy-with-and-without-standardization"><span class="nav-number">6.6.</span> <span class="nav-text">Evaluating the classification accuracy with and without standardization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Appendix-A-The-effect-of-scaling-and-mean-centering-of-variables-prior-to-PCA"><span class="nav-number">7.</span> <span class="nav-text">Appendix A: The effect of scaling and mean centering of variables prior to PCA</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Mean-centering-does-not-affect-the-covariance-matrix"><span class="nav-number">7.1.</span> <span class="nav-text">1. Mean centering does not affect the covariance matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Scaling-of-variables-does-affect-the-covariance-matrix"><span class="nav-number">7.2.</span> <span class="nav-text">2. Scaling of variables does affect the covariance matrix</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Standardizing-affects-the-covariance"><span class="nav-number">7.3.</span> <span class="nav-text">3. Standardizing affects the covariance</span></a></li></ol></li></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2018/10/15/About-Feature-Scaling-and-Normalization/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(e,a.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>