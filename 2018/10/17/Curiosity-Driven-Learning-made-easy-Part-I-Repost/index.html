<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="deep learning,reinforcement learning,"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Curiosity-Driven Learning made easy Part IThis article is part of Deep Reinforcement Learning Course with Tensorflow üïπÔ∏è. Check the syllabus here.OpenAI Five contestIn the recent years, we‚Äôve seen a l"><meta property="og:type" content="article"><meta property="og:title" content="Curiosity-Driven Learning made easy Part I (Repost)"><meta property="og:url" content="http://yoursite.com/2018/10/17/Curiosity-Driven-Learning-made-easy-Part-I-Repost/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Curiosity-Driven Learning made easy Part IThis article is part of Deep Reinforcement Learning Course with Tensorflow üïπÔ∏è. Check the syllabus here.OpenAI Five contestIn the recent years, we‚Äôve seen a l"><meta property="og:image" content="https://cdn-images-1.medium.com/max/2000/0*qvMOs9XAWBAGfCgU.jpg"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*SI8itmr1PZPkXCBtgIh2Sw.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*jhrhZm8G1rQfpxEF8l5XoQ.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*c6DjPzsDc9Qd-Iv-_Y8fxg.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*r5i0ZxqEWNE5nvY5thMdYg.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*JHhacgi6jzpzKtReLgNE2w.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*pLDg3MIz5Q6TRsGesVmRwA.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*xJehwVNbkI6SdrShiSyCbQ.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*k8gMwh8_ZVgE2bVCKZo_gA.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*G7O492AyEu-jlOHHQvTRug.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*hw9WW9_DqI2DLiK5GjOSig.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*EsMzj_wLYR_kx1UZ2fC1dQ.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*cmKEatcnl83GRZ8kJriBiQ.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1600/1*-hRqX-e4OEcJlc6jp8rgRw.png"><meta property="og:image" content="https://cdn-images-1.medium.com/max/2000/1*4BiRJ-_jGRF8N1HRFInBtQ.png"><meta property="og:updated_time" content="2018-10-17T09:15:07.256Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Curiosity-Driven Learning made easy Part I (Repost)"><meta name="twitter:description" content="Curiosity-Driven Learning made easy Part IThis article is part of Deep Reinforcement Learning Course with Tensorflow üïπÔ∏è. Check the syllabus here.OpenAI Five contestIn the recent years, we‚Äôve seen a l"><meta name="twitter:image" content="https://cdn-images-1.medium.com/max/2000/0*qvMOs9XAWBAGfCgU.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2018/10/17/Curiosity-Driven-Learning-made-easy-Part-I-Repost/"><title>Curiosity-Driven Learning made easy Part I (Repost) | Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>ÂÖ¨Áõä404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/17/Curiosity-Driven-Learning-made-easy-Part-I-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Curiosity-Driven Learning made easy Part I (Repost)</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-17T17:13:19+08:00">2018-10-17 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/10/17/Curiosity-Driven-Learning-made-easy-Part-I-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/10/17/Curiosity-Driven-Learning-made-easy-Part-I-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2018/10/17/Curiosity-Driven-Learning-made-easy-Part-I-Repost/" class="leancloud_visitors" data-flag-title="Curiosity-Driven Learning made easy Part I (Repost)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="Curiosity-Driven-Learning-made-easy-Part-I"><a href="#Curiosity-Driven-Learning-made-easy-Part-I" class="headerlink" title="Curiosity-Driven Learning made easy Part I"></a>Curiosity-Driven Learning made easy Part I</h1><blockquote><p>This article is part of Deep Reinforcement Learning Course with Tensorflow üïπÔ∏è. Check the syllabus <a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" target="_blank" rel="external">here</a>.</p></blockquote><p><img src="https://cdn-images-1.medium.com/max/2000/0*qvMOs9XAWBAGfCgU.jpg" alt="img"></p><p>OpenAI Five contest</p><p>In the recent years, we‚Äôve seen a lot of innovations in Deep Reinforcement Learning. From <a href="https://deepmind.com/research/dqn/" target="_blank" rel="external">DeepMind and the Deep Q learning architecture</a> in 2014 to <a href="https://blog.openai.com/openai-five/" target="_blank" rel="external">OpenAI playing Dota2 with OpenAI five in 2018</a>, we live in an exciting and promising moment.</p><p>And today we‚Äôll learn about Curiosity-Driven Learning, <strong>one of the most exciting and promising strategy in Deep Reinforcement Learning.</strong></p><p>Reinforcement Learning is based on the <a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419" target="_blank" rel="external">reward hypothesis</a>, which is the idea that each goal can be described as the maximization of the rewards. However, the current problem of extrinsic rewards (aka rewards given by the environment) is that <strong>this function is hard coded by a human, which is not scalable.</strong></p><p>The idea of Curiosity-Driven learning, is to build a reward function that is <strong>intrinsic to the agent</strong> (generated by the agent itself). It means that the agent will be a self-learner since he will be the student but also the feedback master.</p><a id="more"></a><p><img src="https://cdn-images-1.medium.com/max/1600/1*SI8itmr1PZPkXCBtgIh2Sw.png" alt="img"></p><p>Sounds crazy? Yes but that‚Äôs a genius idea that was introduced in the 2017 paper <a href="https://pathak22.github.io/noreward-rl/" target="_blank" rel="external">Curiosity-driven Exploration by Self-supervised Prediction</a>. The results were then improved with the second paper <a href="https://pathak22.github.io/large-scale-curiosity/" target="_blank" rel="external">Large-Scale Study of Curiosity-Driven Learning.</a></p><p>They discovered that curiosity driven learning agents perform as good as if they had extrinsic rewards, <strong>and were able to generalize better with unexplored environments.</strong></p><p>In this first article we‚Äôll talk about the theory and explain how works Curiosity Driven Learning in theory.</p><p>Then, in a second article, we‚Äôll implement a Curiosity driven PPO agent playing Super Mario Bros.</p><p>Sounds fun? Let‚Äôs dive on in !</p><h3 id="Two-main-problems-in-Reinforcement-Learning"><a href="#Two-main-problems-in-Reinforcement-Learning" class="headerlink" title="Two main problems in Reinforcement Learning"></a>Two main problems in Reinforcement Learning</h3><p>First, the problem of <em>sparse rewards</em>, which is the time difference between an action and its feedback (its reward). An agent learns fast if each of its action has a reward, so that he gets a rapid feedback.</p><p>For instance, if you play Space Invaders, you shoot and kill an enemy, you get a reward. Consequently you‚Äôll understand that this action at that state was good.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*jhrhZm8G1rQfpxEF8l5XoQ.png" alt="img"></p><p>Thanks the reward our agent knows that this action at that state was good</p><p>However, in complex games such as real time strategy games, you will not have a direct reward for each of your actions. Therefore, a bad decision will not have a feedback until hours later.</p><p>For example, if we take Age Of Empires II, we can see on the first image that agent decided to build one barrack and focus on collecting resources. Thus in the second picture (some hours after) the enemies destroyed our barrack consequently we have ton of resources but we can‚Äôt create an army so we‚Äôre dead.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*c6DjPzsDc9Qd-Iv-_Y8fxg.png" alt="img"></p><p>Enemies destroyed our barrack</p><p>The second big problem is that <em>extrinsic rewards are not scalable</em>. Since in each environment, a human implemented a reward function. But how we can scale that in big and complex environments?</p><p>The solution is to develop a reward function that is intrinsic to the agent (generated by the agent itself). <strong>This reward function will be called curiosity.</strong></p><h3 id="A-new-reward-function-curiosity"><a href="#A-new-reward-function-curiosity" class="headerlink" title="A new reward function: curiosity"></a>A new reward function: curiosity</h3><p>Curiosity is an intrinsic reward that is equal to the <strong>error of our agent to predict the consequence of its own actions given its current state (aka to predict the next state given current state and action taken).</strong></p><p>Why? Because the idea of curiosity is <strong>to encourage our agent to perform actions that reduce the uncertainty in the agent‚Äôs ability to predict the consequence of its own action</strong> (uncertainty will be higher <strong>in areas where the agent has spent less time,</strong> or in areas with complex dynamics).</p><p>Consequently measuring error requires <strong>building a model of environmental dynamics that predicts the next state given the current state and the action a.</strong></p><blockquote><p>The question that we can ask here is how we can calculate this error?</p></blockquote><p>To calculate curiosity, we will use a module introduced in the first paper called Intrinsic Curiosity module.</p><h3 id="Introducing-the-Intrinsic-Curiosity-Module"><a href="#Introducing-the-Intrinsic-Curiosity-Module" class="headerlink" title="Introducing the Intrinsic Curiosity Module"></a>Introducing the Intrinsic Curiosity Module</h3><h4 id="The-need-of-a-good-feature-space"><a href="#The-need-of-a-good-feature-space" class="headerlink" title="The need of a good feature space"></a>The need of a good feature space</h4><p>Before diving into the description of the module, we must ask ourselves <strong>how our agent can predict the next state given our current state and our action?</strong></p><p>We know that we can define the curiosity as the error between the predicted new state (st+1) given our state st and action at and the real new state.</p><p>But, remember that most of the time, our state is a stack of 4 frames (pixels). It means that we need to find a way to predict the next stack of frames which is really hard for two reasons:</p><p>First of all, it‚Äôs hard to predict the pixels directly, imagine you‚Äôre in Doom you move left, you need to predict 248*248 = 61504 pixels!</p><p>Second, the researchers think that‚Äôs not the right thing to do and take a good example to prove it.</p><p>Imagine you need to study the movement of the tree leaves in a breeze. First of all, it‚Äôs already hard to model breeze, consequently it is much harder to predict the pixel location of each leaves at each time step.</p><p>The problem, is that because you‚Äôll always have a big pixel prediction error, the agent will always be curious even if the movement of the leaves is not the consequence of the agent actions <strong>therefore its continued curiosity is undesirable.</strong></p><p>Trying to predict the movement of each pixel at each timeframe is really hard</p><p>So instead of making prediction in the raw sensory space (pixels), we <strong>need to transform the raw sensory input (array of pixels) into a feature space with only relevant information.</strong></p><p>We need to define what rules must respect a good feature space, there are 3:</p><ul><li>Needs to model things that can be <strong>controlled by the agent.</strong></li><li>Needs also to model things that can‚Äôt be controlled by the agent but that <strong>can affect an agent.</strong></li><li>Needs to not model (and consequently be unaffected) by things that are not in agent‚Äôs control and have no effect on him.</li></ul><p>Let‚Äôs take this example, your agent is a car, if we want to create a good feature representation we need to model:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*r5i0ZxqEWNE5nvY5thMdYg.png" alt="img"></p><p>The yellow boxes are the important elements</p><p>Our car (controlled by our agent), the other cars (we can‚Äôt control it but that can affect the agent) but we don‚Äôt need to model the leaves (not affect the agent and we can‚Äôt control it). This way we will have a feature representation with less noise.</p><p>The desired embedding space should:</p><ul><li>Be compact in terms of dimensional (remove irrelevant parts of the observation space).</li><li>Preserve sufficient information about the observation.</li><li>Stable: <strong>because non-stationary rewards make it difficult for reinforcement agents to learn.</strong></li></ul><h4 id="Intrinsic-Curiosity-Module-ICM"><a href="#Intrinsic-Curiosity-Module-ICM" class="headerlink" title="Intrinsic Curiosity Module (ICM)"></a>Intrinsic Curiosity Module (ICM)</h4><p><img src="https://cdn-images-1.medium.com/max/1600/1*JHhacgi6jzpzKtReLgNE2w.png" alt="img"></p><p>ICM Taken from the<a href="https://pathak22.github.io/noreward-rl/resources/icml17.pdf" target="_blank" rel="external"> Paper</a></p><p>The Intrinsic Curiosity Module is the system <strong>that helps us to generate curiosity.</strong> It is composed of two neural networks.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*pLDg3MIz5Q6TRsGesVmRwA.png" alt="img"></p><p>Remember, we want to only predict changes in the environment <strong>that could possibly be due to the actions of our agent or affect the agent and ignore the rest.</strong> It means, we need instead of making predictions from a raw sensory space (pixels), transform the sensory input <strong>into a feature vector where only the information relevant to the action performed by the agent is represented.</strong></p><p>To learn this feature space: we <strong>use self-supervision</strong>, training a neural network on a proxy inverse dynamics task of predicting the agent action (√¢t) given its current and next states (st and st+1).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*xJehwVNbkI6SdrShiSyCbQ.png" alt="img"></p><p>Inverse Model Part</p><p>Since the neural network is only required to predict the action, <strong>it has no incentive to represent within its feature embedding space, the factors of variation in the environment that does not affect the agent itself.</strong></p><p><img src="https://cdn-images-1.medium.com/max/1600/1*k8gMwh8_ZVgE2bVCKZo_gA.png" alt="img"></p><p>Forward Model Part</p><p>Then we use this feature space to train a forward dynamics model that predicts the future representation of the next state phi(st+1), <strong>given the feature representation of the current state phi(st) and the action at.</strong></p><p>And we provide the prediction error of the forward dynamics model to the agent <strong>as an intrinsic reward to encourage its curiosity.</strong></p><p>Curiosity = predicted_phi(st+1)‚Ää‚Äî‚Ääphi(st+1)</p><p>So, we have two models in ICM:</p><ul><li><em>Inverse Model</em> (Blue): Encode the states st and st+1 into the feature vectors phi(st) and phi(st+1) that are trained to predict action √¢t.</li></ul><p><img src="https://cdn-images-1.medium.com/max/1600/1*G7O492AyEu-jlOHHQvTRug.png" alt="img"></p><p><img src="https://cdn-images-1.medium.com/max/1600/1*hw9WW9_DqI2DLiK5GjOSig.png" alt="img"></p><p>Inverse Loss function that measures the difference between the real action and our predicted action</p><ul><li><em>Forward Model</em> (Red): Takes as input phi(st) and at and predict the feature representation phi(st+1) of st+1.</li></ul><p><img src="https://cdn-images-1.medium.com/max/1600/1*EsMzj_wLYR_kx1UZ2fC1dQ.png" alt="img"></p><p><img src="https://cdn-images-1.medium.com/max/1600/1*cmKEatcnl83GRZ8kJriBiQ.png" alt="img"></p><p>Forward Model Loss function</p><p>Then mathematically speaking, curiosity will be the difference between our predicted feature vector of the next state and the real feature vector of the next state.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-hRqX-e4OEcJlc6jp8rgRw.png" alt="img"></p><p>Finally the overall optimization problem of this module is a composition of Inverse Loss, Forward Loss.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*4BiRJ-_jGRF8N1HRFInBtQ.png" alt="img"></p><p>That‚Äôs was a lot of information and mathematics!</p><p>To recap:</p><ul><li>Because of extrinsic rewards implementation and sparse rewards problems, <strong>we want to create a reward that is intrinsic to the agent.</strong></li><li>To do that we created curiosity, <strong>which is the agent‚Äôs error in predicting the consequence of its action given its current state.</strong></li><li>Using curiosity will push our agent to <strong>favor transitions with high prediction error</strong> (which will be higher <strong>in areas where the agent has spent less time,</strong> or in areas with complex dynamics) and consequently better explore our environment.</li><li>But because we can‚Äôt predict the next state by predicting the next frame (too much complicated), we use a <strong>better feature representation that will keep only elements that can be controlled by our agent or affect our agent.</strong></li><li>To generate curiosity, we use Intrinsic Curiosity module that is composed of two models: <strong>Inverse Model</strong> that is used to learn the feature representation of state and next state and <strong>Forward Dynamics</strong> model used to generate the predicted feature representation of the next state.</li><li>Curiosity will be equal <strong>to the difference between predicted_phi(st+1) (Forward Dynamics model) and phi(st+1) (Inverse Dynamics model)</strong></li></ul><p>That‚Äôs all for today! Now that you understood the theory, you should read the two papers experiments results <a href="https://pathak22.github.io/noreward-rl/" target="_blank" rel="external">Curiosity-driven Exploration by Self-supervised Prediction</a> <a href="https://pathak22.github.io/large-scale-curiosity/" target="_blank" rel="external">and Large-Scale Study of Curiosity-Driven Learning.</a></p><p>Next time, we‚Äôll implement a PPO agent using curiosity as intrinsic reward to play Super Mario Bros.</p><hr><p>Source address is <a href="https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359" target="_blank" rel="external">here</a>,</p></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/deep-learning/" rel="tag"># deep learning</a> <a href="/tags/reinforcement-learning/" rel="tag"># reinforcement learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018/10/16/Validation-Checklist-in-Kaggle-Competition/" rel="next" title="Validation Checklist in Kaggle Competition"><i class="fa fa-chevron-left"></i> Validation Checklist in Kaggle Competition</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></article><div class="post-spread"><div class="addthis_inline_share_toolbox"><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-58f731508143741d" async></script></div></div></div></div><div class="comments" id="comments"><div id="hypercomments_widget"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">Table of Contents</li><li class="sidebar-nav-overview" data-target="site-overview">Overview</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">124</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">59</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Curiosity-Driven-Learning-made-easy-Part-I"><span class="nav-number">1.</span> <span class="nav-text">Curiosity-Driven Learning made easy Part I</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Two-main-problems-in-Reinforcement-Learning"><span class="nav-number">1.0.1.</span> <span class="nav-text">Two main problems in Reinforcement Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-new-reward-function-curiosity"><span class="nav-number">1.0.2.</span> <span class="nav-text">A new reward function: curiosity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Introducing-the-Intrinsic-Curiosity-Module"><span class="nav-number">1.0.3.</span> <span class="nav-text">Introducing the Intrinsic Curiosity Module</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-need-of-a-good-feature-space"><span class="nav-number">1.0.3.1.</span> <span class="nav-text">The need of a good feature space</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Intrinsic-Curiosity-Module-ICM"><span class="nav-number">1.0.3.2.</span> <span class="nav-text">Intrinsic Curiosity Module (ICM)</span></a></li></ol></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">Êú¨Á´ôËÆøÂÆ¢Êï∞</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>‰∫∫Ê¨°</span> <span class="site-pv"><i class="fa fa-eye">Êú¨Á´ôÊÄªËÆøÈóÆÈáè</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>Ê¨°</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),_hcwp.push({widget:"Stream",widget_id:89825,xid:"2018/10/17/Curiosity-Driven-Learning-made-easy-Part-I-Repost/"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var e=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),t=document.createElement("script");t.type="text/javascript",t.async=!0,t.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+e+"/widget.js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(t,a.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>