<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Abracadabra</title>
  <subtitle>Do it yourself</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-11-20T05:23:15.767Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Ewan Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Note of the DenseNet (contains TensorFlow and PyTorch Implementation)</title>
    <link href="http://yoursite.com/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/"/>
    <id>http://yoursite.com/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/</id>
    <published>2017-11-20T04:15:43.000Z</published>
    <updated>2017-11-20T05:23:15.767Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>The blog source:</strong></p><p><strong><a href="https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504" target="_blank" rel="external">https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504</a>.</strong></p><p>I have added the PyTorch implementation from</p><p><strong><a href="https://github.com/gpleiss/efficient_densenet_pytorch" target="_blank" rel="external">https://github.com/gpleiss/efficient_densenet_pytorch</a>.</strong></p></blockquote><p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">DenseNet</a>(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. It’s quite similar to <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet</a> but has some fundamental differences.</p><p>With all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*7WdURialIGTojNI9ltrplA.png" alt="img"></p><p><em>Error rates on various datasets(from source paper)</em></p><p>And for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*N_o10WxGx_e6vBSNnV5k1A.png" alt="img"></p><p><em>Сomparison of the DenseNet and ResNet Top-1 error rates on the ImageNet classification dataset as a function of learned parameters (left) and flops during test-time (right)(from source paper).</em></p><p>This post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:</p><ul><li>Why dense net differs from another convolution networks.</li><li>What difficulties I’ve met during the implementation of DenseNet in tensorflow.</li></ul><p>If you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the <a href="https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#aabd" target="_blank" rel="external">second chapter</a> or check the <a href="https://github.com/ikhlestov/vision_networks" target="_blank" rel="external">source code on GitHub</a>. If you not familiar with any topics but want to get some knowledge — I highly advise you <a href="http://cs231n.github.io/" target="_blank" rel="external">CS231n Stanford classes</a>.</p><h4 id="Compare-DenseNet-with-other-Convolution-Networks"><a href="#Compare-DenseNet-with-other-Convolution-Networks" class="headerlink" title="Compare DenseNet with other Convolution Networks"></a>Compare DenseNet with other Convolution Networks</h4><p>Usually, ConvNets work such way:<br>We have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.<br>So the output from the Lᵢ layer is input to the Lᵢ₊₁ layer. It seems like this:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*1F8wom5X1DeCj5BWeGfJbw.jpeg" alt="img"></p><p><em>source: &lt;<a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">http://cs231n.github.io/convolutional-networks/</a></em>&gt;</p><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet</a> architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the Lᵢ layer was obtained by summation of outputs from previous layers.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-C2QoqhfCjG8xZRu-UwO_Q.png" alt="img"></p><p>In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.<br>So, let’s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.<br>The output from Lᵢ layer will be (28, 28, 12).<br>But input to the Lᵢ₊₁ will be (28, 28, 24+12), for Lᵢ₊₂ (28, 28, 24 + 12 + 12) and so on.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*6JoB4BeIZyWDNGH5U63y_Q.png" alt="img"></p><p><em>Block of convolution layers with results concatenated</em></p><p>After a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).<br>All these N layers are named Block in the paper. There’s also batch normalization, nonlinearity and dropout inside the block.<br>To reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-CQyW4huxxxMYpffb72emg.png" alt="img"></p><p><em>Transition layer</em></p><p>Now we can again pass the image through the block with N convolutions.<br>With this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.<br>Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*SSn5H14SKhhaZZ5XYWN3Cg.jpeg" alt="img"></p><p><em>Full DenseNet example with 3 blocks from source paper</em></p><h4 id="Notes-about-implementation"><a href="#Notes-about-implementation" class="headerlink" title="Notes about implementation"></a>Notes about implementation</h4><p>In the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.</p><p>First of all, it was not clear how many blocks should be used depends on depth. After I’ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.</p><p>Second I’ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .</p><p>Despite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> block <span class="keyword">in</span> range(required_blocks):</div><div class="line">    output = build_block(output)</div><div class="line">    <span class="keyword">if</span> block != (required_blocks — <span class="number">1</span>):</div><div class="line">        output = transition_layer(output)</div></pre></td></tr></table></figure><p>For weights initialization authors proposed use MRSA initialization(as per<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="external">this paper</a>). In tensorflow this initialization can be easy implemented with<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer" target="_blank" rel="external">variance scaling initializer</a>.</p><p>In the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">batch norm -&gt; relu -&gt; conv 1x1 -&gt; dropout -&gt; batch norm -&gt; relu -&gt; conv 3x3 -&gt; dropout -&gt; output.</div></pre></td></tr></table></figure><p>Despite two conv filters, only last output will be concatenated to the main pool of features.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*FvW30tjOQ2hDQ7LdEkCRLQ.png" alt="img"></p><p>Also at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*N-nU5WXQcrNfhnQMwxquFA.png" alt="img"></p><p>Where theta — some reduction values, in the range (0, 1).</p><p>In case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.</p><p>Last, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used — just divide every image pixel by 255, so we have pixels values in the range [0, 1].</p><p>At first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next I’ve implemented per channel normalization… And networks began works even worse. It was not clear for me why. So I’ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.</p><p>And some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># without this line next slice assignment will silently fail!</span></div><div class="line"><span class="comment"># at least in numpy 1.12.0</span></div><div class="line">images = images.astype(‘float64’)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(channels):</div><div class="line">    images[:, :, :, i] = (</div><div class="line">        (images[:, :, :, i] — self.images_means[i]) /</div><div class="line">         self.images_stds[i])</div></pre></td></tr></table></figure><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>DenseNets are powerful neural nets that achieve state of the art performance on many datasets. And it’s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.</p><a id="more"></a><h4 id="Appendix-PyTorch-Implementation-naive-version-100-lines"><a href="#Appendix-PyTorch-Implementation-naive-version-100-lines" class="headerlink" title="Appendix: PyTorch Implementation (naive version ~100 lines)"></a>Appendix: PyTorch Implementation (naive version ~100 lines)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># This implementation is based on the DenseNet-BC implementation in torchvision</span></div><div class="line"><span class="comment"># https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DenseLayer</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_features, growth_rate, bn_size, drop_rate)</span>:</span></div><div class="line">        super(_DenseLayer, self).__init__()</div><div class="line">        self.add_module(<span class="string">'norm.1'</span>, nn.BatchNorm2d(num_input_features)),</div><div class="line">        self.add_module(<span class="string">'relu.1'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">        self.add_module(<span class="string">'conv.1'</span>, nn.Conv2d(num_input_features, bn_size *</div><div class="line">                        growth_rate, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        self.add_module(<span class="string">'norm.2'</span>, nn.BatchNorm2d(bn_size * growth_rate)),</div><div class="line">        self.add_module(<span class="string">'relu.2'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">        self.add_module(<span class="string">'conv.2'</span>, nn.Conv2d(bn_size * growth_rate, growth_rate,</div><div class="line">                        kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        self.drop_rate = drop_rate</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        new_features = super(_DenseLayer, self).forward(x)</div><div class="line">        <span class="keyword">if</span> self.drop_rate &gt; <span class="number">0</span>:</div><div class="line">            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)</div><div class="line">        <span class="keyword">return</span> torch.cat([x, new_features], <span class="number">1</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_Transition</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_features, num_output_features)</span>:</span></div><div class="line">        super(_Transition, self).__init__()</div><div class="line">        self.add_module(<span class="string">'norm'</span>, nn.BatchNorm2d(num_input_features))</div><div class="line">        self.add_module(<span class="string">'relu'</span>, nn.ReLU(inplace=<span class="keyword">True</span>))</div><div class="line">        self.add_module(<span class="string">'conv'</span>, nn.Conv2d(num_input_features, num_output_features,</div><div class="line">                                          kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>))</div><div class="line">        self.add_module(<span class="string">'pool'</span>, nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DenseBlock</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate)</span>:</span></div><div class="line">        super(_DenseBlock, self).__init__()</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</div><div class="line">            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)</div><div class="line">            self.add_module(<span class="string">'denselayer%d'</span> % (i + <span class="number">1</span>), layer)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseNet</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">r"""Densenet-BC model class, based on</span></div><div class="line">    `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`</div><div class="line">    Args:</div><div class="line">        growth_rate (int) - how many filters to add each layer (`k` in paper)</div><div class="line">        block_config (list of 3 or 4 ints) - how many layers in each pooling block</div><div class="line">        num_init_features (int) - the number of filters to learn in the first convolution layer</div><div class="line">        bn_size (int) - multiplicative factor for number of bottle neck layers</div><div class="line">            (i.e. bn_size * k features in the bottleneck layer)</div><div class="line">        drop_rate (float) - dropout rate after each dense layer</div><div class="line">        num_classes (int) - number of classification classes</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, growth_rate=<span class="number">12</span>, block_config=<span class="params">(<span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>)</span>, compression=<span class="number">0.5</span>,</span></span></div><div class="line">                 num_init_features=<span class="number">24</span>, bn_size=<span class="number">4</span>, drop_rate=<span class="number">0</span>, avgpool_size=<span class="number">8</span>,</div><div class="line">                 num_classes=<span class="number">10</span>):</div><div class="line"></div><div class="line">        super(DenseNet, self).__init__()</div><div class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt; compression &lt;= <span class="number">1</span>, <span class="string">'compression of densenet should be between 0 and 1'</span></div><div class="line">        self.avgpool_size = avgpool_size</div><div class="line"></div><div class="line">        <span class="comment"># First convolution</span></div><div class="line">        self.features = nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'conv0'</span>, nn.Conv2d(<span class="number">3</span>, num_init_features, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">        <span class="comment"># Each denseblock</span></div><div class="line">        num_features = num_init_features</div><div class="line">        <span class="keyword">for</span> i, num_layers <span class="keyword">in</span> enumerate(block_config):</div><div class="line">            block = _DenseBlock(num_layers=num_layers,</div><div class="line">                                num_input_features=num_features,</div><div class="line">                                bn_size=bn_size, growth_rate=growth_rate,</div><div class="line">                                drop_rate=drop_rate)</div><div class="line">            self.features.add_module(<span class="string">'denseblock%d'</span> % (i + <span class="number">1</span>), block)</div><div class="line">            num_features = num_features + num_layers * growth_rate</div><div class="line">            <span class="keyword">if</span> i != len(block_config) - <span class="number">1</span>:</div><div class="line">                trans = _Transition(num_input_features=num_features,</div><div class="line">                                    num_output_features=int(num_features</div><div class="line">                                                            * compression))</div><div class="line">                self.features.add_module(<span class="string">'transition%d'</span> % (i + <span class="number">1</span>), trans)</div><div class="line">                num_features = int(num_features * compression)</div><div class="line"></div><div class="line">        <span class="comment"># Final batch norm</span></div><div class="line">        self.features.add_module(<span class="string">'norm_final'</span>, nn.BatchNorm2d(num_features))</div><div class="line"></div><div class="line">        <span class="comment"># Linear layer</span></div><div class="line">        self.classifier = nn.Linear(num_features, num_classes)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        features = self.features(x)</div><div class="line">        out = F.relu(features, inplace=<span class="keyword">True</span>)</div><div class="line">        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(</div><div class="line">                           features.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">        out = self.classifier(out)</div><div class="line">        <span class="keyword">return</span> out</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;The blog source:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504&quot;&gt;https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;I have added the PyTorch implementation from&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/gpleiss/efficient_densenet_pytorch&quot;&gt;https://github.com/gpleiss/efficient_densenet_pytorch&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;DenseNet&lt;/a&gt;(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. It’s quite similar to &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt; but has some fundamental differences.&lt;/p&gt;&lt;p&gt;With all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*7WdURialIGTojNI9ltrplA.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Error rates on various datasets(from source paper)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;And for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*N_o10WxGx_e6vBSNnV5k1A.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Сomparison of the DenseNet and ResNet Top-1 error rates on the ImageNet classification dataset as a function of learned parameters (left) and flops during test-time (right)(from source paper).&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Why dense net differs from another convolution networks.&lt;/li&gt;&lt;li&gt;What difficulties I’ve met during the implementation of DenseNet in tensorflow.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the &lt;a href=&quot;https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#aabd&quot;&gt;second chapter&lt;/a&gt; or check the &lt;a href=&quot;https://github.com/ikhlestov/vision_networks&quot;&gt;source code on GitHub&lt;/a&gt;. If you not familiar with any topics but want to get some knowledge — I highly advise you &lt;a href=&quot;http://cs231n.github.io/&quot;&gt;CS231n Stanford classes&lt;/a&gt;.&lt;/p&gt;&lt;h4 id=&quot;Compare-DenseNet-with-other-Convolution-Networks&quot;&gt;&lt;a href=&quot;#Compare-DenseNet-with-other-Convolution-Networks&quot; class=&quot;headerlink&quot; title=&quot;Compare DenseNet with other Convolution Networks&quot;&gt;&lt;/a&gt;Compare DenseNet with other Convolution Networks&lt;/h4&gt;&lt;p&gt;Usually, ConvNets work such way:&lt;br&gt;We have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.&lt;br&gt;So the output from the Lᵢ layer is input to the Lᵢ₊₁ layer. It seems like this:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*1F8wom5X1DeCj5BWeGfJbw.jpeg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;source: &amp;lt;&lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;http://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/em&gt;&amp;gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt; architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the Lᵢ layer was obtained by summation of outputs from previous layers.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*-C2QoqhfCjG8xZRu-UwO_Q.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.&lt;br&gt;So, let’s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.&lt;br&gt;The output from Lᵢ layer will be (28, 28, 12).&lt;br&gt;But input to the Lᵢ₊₁ will be (28, 28, 24+12), for Lᵢ₊₂ (28, 28, 24 + 12 + 12) and so on.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*6JoB4BeIZyWDNGH5U63y_Q.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Block of convolution layers with results concatenated&lt;/em&gt;&lt;/p&gt;&lt;p&gt;After a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).&lt;br&gt;All these N layers are named Block in the paper. There’s also batch normalization, nonlinearity and dropout inside the block.&lt;br&gt;To reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*-CQyW4huxxxMYpffb72emg.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Transition layer&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Now we can again pass the image through the block with N convolutions.&lt;br&gt;With this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.&lt;br&gt;Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*SSn5H14SKhhaZZ5XYWN3Cg.jpeg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Full DenseNet example with 3 blocks from source paper&lt;/em&gt;&lt;/p&gt;&lt;h4 id=&quot;Notes-about-implementation&quot;&gt;&lt;a href=&quot;#Notes-about-implementation&quot; class=&quot;headerlink&quot; title=&quot;Notes about implementation&quot;&gt;&lt;/a&gt;Notes about implementation&lt;/h4&gt;&lt;p&gt;In the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.&lt;/p&gt;&lt;p&gt;First of all, it was not clear how many blocks should be used depends on depth. After I’ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.&lt;/p&gt;&lt;p&gt;Second I’ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .&lt;/p&gt;&lt;p&gt;Despite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; block &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(required_blocks):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    output = build_block(output)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; block != (required_blocks — &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        output = transition_layer(output)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;For weights initialization authors proposed use MRSA initialization(as per&lt;a href=&quot;https://arxiv.org/abs/1502.01852&quot;&gt;this paper&lt;/a&gt;). In tensorflow this initialization can be easy implemented with&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer&quot;&gt;variance scaling initializer&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;batch norm -&amp;gt; relu -&amp;gt; conv 1x1 -&amp;gt; dropout -&amp;gt; batch norm -&amp;gt; relu -&amp;gt; conv 3x3 -&amp;gt; dropout -&amp;gt; output.&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;Despite two conv filters, only last output will be concatenated to the main pool of features.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*FvW30tjOQ2hDQ7LdEkCRLQ.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;Also at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*N-nU5WXQcrNfhnQMwxquFA.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;Where theta — some reduction values, in the range (0, 1).&lt;/p&gt;&lt;p&gt;In case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.&lt;/p&gt;&lt;p&gt;Last, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used — just divide every image pixel by 255, so we have pixels values in the range [0, 1].&lt;/p&gt;&lt;p&gt;At first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next I’ve implemented per channel normalization… And networks began works even worse. It was not clear for me why. So I’ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.&lt;/p&gt;&lt;p&gt;And some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# without this line next slice assignment will silently fail!&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# at least in numpy 1.12.0&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;images = images.astype(‘float64’)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(channels):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    images[:, :, :, i] = (&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        (images[:, :, :, i] — self.images_means[i]) /&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;         self.images_stds[i])&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h4 id=&quot;Conclusion&quot;&gt;&lt;a href=&quot;#Conclusion&quot; class=&quot;headerlink&quot; title=&quot;Conclusion&quot;&gt;&lt;/a&gt;Conclusion&lt;/h4&gt;&lt;p&gt;DenseNets are powerful neural nets that achieve state of the art performance on many datasets. And it’s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec: The Skip-Gram Model</title>
    <link href="http://yoursite.com/2017/11/14/Word2Vec-The-Skip-Gram-Model/"/>
    <id>http://yoursite.com/2017/11/14/Word2Vec-The-Skip-Gram-Model/</id>
    <published>2017-11-14T13:41:00.000Z</published>
    <updated>2017-11-15T03:27:02.749Z</updated>
    
    <content type="html"><![CDATA[<p>This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual introductory and abstract insights about Word2Vec, and get into more of the details. Specifically here I’m diving into the skip gram neural network model.</p><h1 id="The-Model"><a href="#The-Model" class="headerlink" title="The Model"></a>The Model</h1><p>The skip-gram neural network model is actually surprisingly simple in its most basic form; I think it’s the all the little tweaks and enhancements that start to clutter the explanation.</p><p>Let’s start with a high-level insight about where we’re going. Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.</p><p>Another place you may have seen this trick is in unsupervised feature learning, where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer–it’s a trick for learning good image features without having labeled training data.</p><h1 id="The-Fake-Task"><a href="#The-Fake-Task" class="headerlink" title="The Fake Task"></a>The Fake Task</h1><p>So now we need to talk about this “fake” task that we’re going to build the neural network to perform, and then we’ll come back later to how this indirectly gives us those word vectors that we are really after.</p><p>We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.</p><p>When I say “nearby”, there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).</p><p>The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.</p><p>We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.</p><p><a href="http://mccormickml.com/assets/word2vec/training_data.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/training_data.png" alt="Training Data"></a></p><p>The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”.</p><h1 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h1><p>So how is this all represented?</p><p>First of all, you know you can’t feed a word just as a text string to a neural network, so we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words.</p><p>We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.</p><p>The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.</p><p>Here’s the architecture of our neural network.</p><p><a href="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png" alt="Skip-gram Neural Network Architecture"></a></p><p>There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.</p><p>When <em>training</em> this network on word pairs, the input is a one-hot vector representing the input word and the training output <em>is also a one-hot vector</em>representing the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, <em>not</em> a one-hot vector).</p><h1 id="The-Hidden-Layer"><a href="#The-Hidden-Layer" class="headerlink" title="The Hidden Layer"></a>The Hidden Layer</h1><p>For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).</p><p>300 features is what Google used in their published model trained on the Google news dataset (you can download it from <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="external">here</a>). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results).</p><p>If you look at the <em>rows</em> of this weight matrix, these are actually what will be our word vectors!</p><p><a href="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png" alt="Hidden Layer Weight Matrix"></a></p><p>So the end goal of all of this is really just to learn this hidden layer weight matrix – the output layer we’ll just toss when we’re done!</p><p>Let’s get back, though, to working through the definition of this model that we’re going to train.</p><p>Now, you might be asking yourself–“That one-hot vector is almost all zeros… what’s the effect of that?” If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just <em>select</em> the matrix row corresponding to the “1”. Here’s a small example to give you a visual.</p><p><a href="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png" alt="Effect of matrix multiplication with a one-hot vector"></a></p><p>This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word.</p><h1 id="The-Output-Layer"><a href="#The-Output-Layer" class="headerlink" title="The Output Layer"></a>The Output Layer</h1><p>The <code>1 x 300</code> word vector for “ants” then gets fed to the output layer. The output layer is a softmax regression classifier. There’s an in-depth tutorial on Softmax Regression <a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/" target="_blank" rel="external">here</a>, but the gist of it is that each output neuron (one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.</p><p>Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function <code>exp(x)</code> to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from <em>all</em> 10,000 output nodes.</p><p>Here’s an illustration of calculating the output of the output neuron for the word “car”.</p><p><a href="http://mccormickml.com/assets/word2vec/output_weights_function.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/output_weights_function.png" alt="Behavior of the output neuron"></a></p><p>Note that neural network does not know anything about the offset of the output word relative to the input word. It <em>does not</em> learn a different set of probabilities for the word before the input versus the word after. To understand the implication, let’s say that in our training corpus, <em>every single occurrence</em> of the word ‘York’ is preceded by the word ‘New’. That is, at least according to the training data, there is a 100% probability that ‘New’ will be in the vicinity of ‘York’. However, if we take the 10 words in the vicinity of ‘York’ and randomly pick one of them, the probability of it being ‘New’ <em>is not</em> 100%; you may have picked one of the other words in the vicinity.</p><h1 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h1><p>Ok, are you ready for an exciting bit of insight into this network?</p><p>If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if <em>the word vectors are similar</em>. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!</p><p>And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well.</p><p>This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts.</p><h1 id="More-Math-Details"><a href="#More-Math-Details" class="headerlink" title="More Math Details"></a>More Math Details</h1><p>For each word $t=1\cdots T$, predict surrounding words in a window of “radius” $m$ of every word.</p><h2 id="Objective-function"><a href="#Objective-function" class="headerlink" title="Objective function:"></a>Objective function:</h2><p>Maximize the probability of any context word given the current center word:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-14-Image-8.png" alt="Page-14-Image-8"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-14-Image-7.png" alt="Page-14-Image-7"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-16-Image-9.png" alt="Page-16-Image-9"></p><h2 id="The-Skip-Gram-Algorithm"><a href="#The-Skip-Gram-Algorithm" class="headerlink" title="The Skip-Gram Algorithm:"></a>The Skip-Gram Algorithm:</h2><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-19-Image-17.png" alt="Page-19-Image-17"></p><h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-24-Image-31.png" alt="Page-24-Image-31"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-25-Image-32.png" alt="Page-25-Image-32"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-26-Image-33.png" alt="Page-26-Image-33"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-27-Image-34.png" alt="Page-27-Image-34"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual i
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="natural language process" scheme="http://yoursite.com/tags/natural-language-process/"/>
    
  </entry>
  
  <entry>
    <title>Prioritized Experience Replay</title>
    <link href="http://yoursite.com/2017/10/30/Prioritized-Experience-Replay/"/>
    <id>http://yoursite.com/2017/10/30/Prioritized-Experience-Replay/</id>
    <published>2017-10-30T06:18:36.000Z</published>
    <updated>2017-10-30T06:25:34.813Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Prioritized-Experience-Replay"><a href="#Prioritized-Experience-Replay" class="headerlink" title="Prioritized Experience Replay"></a>Prioritized Experience Replay</h3><p>One of the possible improvements already acknowledged in the original research<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#fn-444-1" target="_blank" rel="external">2</a> lays in the way experience is used. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#fn-444-4" target="_blank" rel="external">3</a>(PER) is one strategy that tries to leverage this fact by changing the sampling distribution.</p><p>The main idea is that we prefer transitions that does not fit well to our current estimate of the Q function, because these are the transitions that we can learn most from. This reflects a simple intuition from our real world – if we encounter a situation that really differs from our expectation, we think about it over and over and change our model until it fits.</p><p>We can define an error of a sample $S = (s, a, r, s’)$ as a distance between the $Q(s, a)$ and its target $T(S)$:<br>$$<br>error = |Q(s, a) - T(S)|<br>$$<br>For DDQN described above, $T$ it would be:<br>$$<br>T(S) = r + \gamma \tilde{Q}(s’, argmax_a Q(s’, a))<br>$$<br>We will store this error in the agent’s memory along with every sample and update it with each learning step.</p><p>One of the possible approaches to PER is proportional prioritization. The error is first converted to priority using this formula:<br>$$<br>p = (error + \epsilon)^\alpha<br>$$<br>Epsilon $\epsilon$ is a small positive constant that ensures that no transition has zero priority.<br>Alpha, $0 \leq \alpha \leq 1$, controls the difference between high and low error. It determines how much prioritization is used. With $\alpha$ we would get the uniform case.</p><p>Priority is translated to probability of being chosen for replay. A sample $i$ has a probability of being picked during the experience replay determined by a formula:<br>$$<br>P_i = \frac{p_i}{\sum_k p_k}<br>$$<br>The algorithm is simple – during each learning step we will get a batch of samples with this probability distribution and train our network on it. We only need an effective way of storing these priorities and sampling from them.</p><h5 id="Initialization-and-new-transitions"><a href="#Initialization-and-new-transitions" class="headerlink" title="Initialization and new transitions"></a>Initialization and new transitions</h5><p>The original paper says that new transitions come without a known error<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#fn-444-4" target="_blank" rel="external">3</a>, but I argue that with definition given above, we can compute it with a simple forward pass as it arrives. It’s also effective, because high value transitions are discovered immediately.</p><p>Another thing is the initialization. Remember that before the learning itself, we fill the memory using random agent. But this agent does not use any neural network, so how could we estimate any error? We can use a fact that untrained neural network is likely to return a value around zero for every input. In this case the error formula becomes very simple:<br>$$<br>error = |Q(s, a) - T(S)| = |Q(s, a) - r - \gamma \tilde{Q}(s’, argmax_a Q(s’, a))| = | r |<br>$$<br>The error in this case is simply the reward experienced in a given sample. Indeed, the transitions where the agent experienced any reward intuitively seem to be very promising.</p><h5 id="Efficient-implementation"><a href="#Efficient-implementation" class="headerlink" title="Efficient implementation"></a>Efficient implementation</h5><p>So how do we store the experience and effectively sample from it?</p><p>A naive implementation would be to have all samples in an array sorted according to their priorities. A random number <em>s</em>, $0 \leq s \leq \sum_k p_k$, would be picked and the array would be walked left to right, summing up a priority of the current element until the sum is exceeded and that element is chosen. This will select a sample with the desired probability distribution.</p><p><img src="https://jaromiru.files.wordpress.com/2016/11/per_bar_1.png?w=700" alt="Sorted experience"></p><p>But this would have a terrible efficiency: $O(n log n)$ for insertion and update and O$(n) $for sampling.</p><p>A first important observation is that we don’t have to actually store the array sorted. Unsorted array would do just as well. Elements with higher priorities are still picked with higher probability.</p><p><img src="https://jaromiru.files.wordpress.com/2016/11/per_bar_2.png?w=700" alt="Unsorted experience"></p><p>This releases the need for sorting, improving the algorithm to <em>O(1)</em> for insertion and update.</p><p>But the <em>O(n)</em> for sampling is still too high. We can further improve our algorithm by using a different data structure. We can store our samples in unsorted sum tree – a binary tree data structure where the parent’s value is the sum of its children. The samples themselves are stored in the leaf nodes.</p><p>Update of a leaf node involves propagating a value difference up the tree, obtaining <em>O(log n)</em>. Sampling follows the thought process of the array case, but achieves <em>O(log n)</em>. For a value <em>s</em>, $0 \leq s \leq \sum_k p_k$, we use the following algorithm (pseudo code):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieve</span><span class="params">(n, s)</span>:</span></div><div class="line">    <span class="keyword">if</span> n <span class="keyword">is</span> leaf_node: <span class="keyword">return</span> n</div><div class="line"> </div><div class="line">    <span class="keyword">if</span> n.left.val &gt;= s: <span class="keyword">return</span> retrieve(n.left, s)</div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> retrieve(n.right, s - n.left.val)</div></pre></td></tr></table></figure><p>Following picture illustrates sampling from a tree with <em>s = 24</em>:</p><p><img src="https://jaromiru.files.wordpress.com/2016/11/sumtree.png?w=560" alt="Sampling from sum tree"></p><p>With this effective implementation we can use large memory sizes, with hundreds of thousands or millions of samples.</p><p>For known capacity, this sum tree data structure can be backed by an array. Its reference implementation containing 50 lines of code is available on <a href="https://github.com/jaara/AI-blog/blob/master/SumTree.py" target="_blank" rel="external">GitHub</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Prioritized-Experience-Replay&quot;&gt;&lt;a href=&quot;#Prioritized-Experience-Replay&quot; class=&quot;headerlink&quot; title=&quot;Prioritized Experience Replay&quot;&gt;&lt;/a
    
    </summary>
    
    
      <category term="Reinforcement Learning" scheme="http://yoursite.com/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu Server 16.04 Install Gnome and remote connect from Windows VNCViewer</title>
    <link href="http://yoursite.com/2017/10/26/Ubuntu-Server-16-04-Install-Gnome-and-remote-connect-from-Windows-VNCViewer/"/>
    <id>http://yoursite.com/2017/10/26/Ubuntu-Server-16-04-Install-Gnome-and-remote-connect-from-Windows-VNCViewer/</id>
    <published>2017-10-26T04:02:50.000Z</published>
    <updated>2017-10-30T06:04:58.307Z</updated>
    
    <content type="html"><![CDATA[<h3 id="第一步：装-Gnome-环境"><a href="#第一步：装-Gnome-环境" class="headerlink" title="第一步：装 Gnome 环境"></a>第一步：装 Gnome 环境</h3><p>首先按照如下命令安装 Gnome 环境。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get upgrade</div><div class="line">sudo apt-get install gnome</div><div class="line">sudo apt-get install ubuntu-gnome-desktop</div><div class="line">sudo apt-get install gnome-shell</div></pre></td></tr></table></figure><h3 id="第二步：安装-Gnome-界面管理工具"><a href="#第二步：安装-Gnome-界面管理工具" class="headerlink" title="第二步：安装 Gnome 界面管理工具"></a>第二步：安装 Gnome 界面管理工具</h3><p>安装 Gnome 桌面环境的配置工具。可以使用该工作对 Linux 进行很多配置，包括外观，工作台的数量等。后续安装的主题和图标都可以通过这个工具的 _外观（Appearance）_ 进行调整。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get gnome-tweak-tool</div></pre></td></tr></table></figure><h3 id="第三步：安装-Dash-to-Dock-工具条"><a href="#第三步：安装-Dash-to-Dock-工具条" class="headerlink" title="第三步：安装 Dash to Dock 工具条"></a>第三步：安装 Dash to Dock 工具条</h3><p>安装 Gnome 桌面环境下的 Dock 工具条，可提供 mac os 下dock类似的使用体验。</p><p>在任意浏览器打开 <a href="https://extensions.gnome.org/local/" target="_blank" rel="external">Gnome extensions</a>.</p><p>找到 _Dash to Dock_ 扩展栏，点开右面的 _[ON OFF]_ 选项。点击旁边的 _工具_ 选项，可进一步配置更多选项。</p><h3 id="第四步：安装-ARC-扁平化主题和图标"><a href="#第四步：安装-ARC-扁平化主题和图标" class="headerlink" title="第四步：安装 _ARC_ 扁平化主题和图标"></a>第四步：安装 _ARC_ 扁平化主题和图标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo add-apt-repository ppa:noobslab/themes</div><div class="line">sudo add-apt-repository ppa:noobslab/icons</div><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install arc-theme</div><div class="line">sudo apt-get install arc-icons</div></pre></td></tr></table></figure><h3 id="第五步：选装-Flat-Plat-扁平化主题"><a href="#第五步：选装-Flat-Plat-扁平化主题" class="headerlink" title="第五步：选装 _Flat Plat_ 扁平化主题"></a>第五步：选装 _Flat Plat_ 扁平化主题</h3><p>另一个扁平化主题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">curl -sL https://github.com/nana-4/Flat-Plat/archive/v20170323.tar.gz | tar xz</div><div class="line">cd Flat-Plat-20170323/</div><div class="line">sudo ./install.sh</div></pre></td></tr></table></figure><h3 id="第六步：安装vncserver"><a href="#第六步：安装vncserver" class="headerlink" title="第六步：安装vncserver"></a>第六步：安装vncserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install vnc4server</div><div class="line">sudo apt-get install gnome-panel gnome-settings-daemon metacity nautilus gnome-terminal</div><div class="line">cd ~/.vnc</div><div class="line">mv xstartup xstartup.bak</div><div class="line">vim xstartup</div></pre></td></tr></table></figure><p>使用以下配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">#!/bin/sh</div><div class="line"></div><div class="line">export XKL_XMODMAP_DISABLE=1</div><div class="line">unset SESSION_MANAGER</div><div class="line">unset DBUS_SESSION_BUS_ADDRESS</div><div class="line"></div><div class="line">[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup</div><div class="line">[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources</div><div class="line">xsetroot -solid grey</div><div class="line">vncconfig -iconic &amp;</div><div class="line"></div><div class="line">gnome-session &amp;</div><div class="line">gnome-panel &amp;</div><div class="line">gnome-settings-daemon &amp;</div><div class="line">metacity &amp;</div><div class="line">nautilus &amp;</div><div class="line">gnome-terminal &amp;</div></pre></td></tr></table></figure><h3 id="第七步：启动-vncserver"><a href="#第七步：启动-vncserver" class="headerlink" title="第七步：启动 vncserver"></a>第七步：启动 vncserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># ：1可以更改</div><div class="line">vncserver -geometry 1920x1080 -alwaysshared :1</div></pre></td></tr></table></figure><h3 id="第八步：在-Windows-上安装-VNCViewer"><a href="#第八步：在-Windows-上安装-VNCViewer" class="headerlink" title="第八步：在 Windows 上安装 VNCViewer"></a>第八步：在 Windows 上安装 <a href="https://www.realvnc.com/en/connect/download/viewer/" target="_blank" rel="external">VNCViewer</a></h3><p>启动只要 输入 ip:1 即可</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;第一步：装-Gnome-环境&quot;&gt;&lt;a href=&quot;#第一步：装-Gnome-环境&quot; class=&quot;headerlink&quot; title=&quot;第一步：装 Gnome 环境&quot;&gt;&lt;/a&gt;第一步：装 Gnome 环境&lt;/h3&gt;&lt;p&gt;首先按照如下命令安装 Gnome 环境。&lt;/
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Union Find</title>
    <link href="http://yoursite.com/2017/10/23/Union-Find/"/>
    <id>http://yoursite.com/2017/10/23/Union-Find/</id>
    <published>2017-10-23T05:41:42.000Z</published>
    <updated>2017-10-30T06:05:10.547Z</updated>
    
    <content type="html"><![CDATA[<h2 id="五分钟搞懂并查集"><a href="#五分钟搞懂并查集" class="headerlink" title="五分钟搞懂并查集"></a>五分钟搞懂并查集</h2><blockquote><p>转自：laserss<br><a href="http://blog.csdn.net/dellaserss/article/details/7724401/" target="_blank" rel="external">http://blog.csdn.net/dellaserss/article/details/7724401/</a></p></blockquote><p>并查集是我暑假从高手那里学到的一招，觉得真是太精妙的设计了。来看一个实例，杭电1232畅通工程。首先在地图上给你若干个城镇，这些城镇都可以看作点，然后告诉你哪些对城镇之间是有道路直接相连的。最后要解决的是整幅图的连通性问题。比如随意给你两个点，让你判断它们是否连通，或者问你整幅图一共有几个连通分支，也就是被分成了几个互相独立的块。</p><p>像畅通工程这题，问还需要修几条路，实质就是求有几个连通分支。如果是1个连通分支，说明整幅图上的点都连起来了，不用再修路了；如果是2个连通分支，则只要再修1条路，从两个分支中各选一个点，把它们连起来，那么所有的点都是连起来的了；如果是3个连通分支，则只要再修两条路……</p><p>以下面这组数据输入数据来说明</p><p>4 2 1 3 4 3</p><p>第一行告诉你，一共有4个点，2条路。下面两行告诉你，1、3之间有条路，4、3之间有条路。那么整幅图就被分成了1-3-4和2两部分。只要再加一条路，把2和其他任意一个点连起来，畅通工程就实现了，那么这个这组数据的输出结果就是1。好了，现在编程实现这个功能吧，城镇有几百个，路有不知道多少条，而且可能有回路。</p><p>这可如何是好？我以前也不会呀，自从用了并查集之后，嗨，效果还真好！我们全家都用它！并查集由一个整数型的数组和两个函数构成。数组pre[]记录了每个点的前导点是什么，函数find是查找，join是合并。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">int pre[1000 ];</div><div class="line">int find(int x)                                                                   //查找根节点</div><div class="line">&#123;  </div><div class="line">     int r=x; </div><div class="line">     while ( pre[r ] != r )                                                       //返回根节点 r </div><div class="line">           r=pre[r ];</div><div class="line"> </div><div class="line">     int i=x , j ;</div><div class="line">     while( i != r )                                                                 //路径压缩</div><div class="line">     &#123;</div><div class="line">          j = pre[ i ];     // 在改变上级之前用临时变量  j 记录下他的值 </div><div class="line">          pre[ i ]= r ;    //把上级改为根节点</div><div class="line">          i=j;</div><div class="line">     &#125;</div><div class="line">     return r ;</div><div class="line"> &#125;</div><div class="line"> </div><div class="line"> //判断x y是否连通，如果已经连通，就不用管了 //如果不连通，就把它们所在的连通分支合并起,</div><div class="line"> </div><div class="line"> void join(int x,int y)                     </div><div class="line"> &#123;</div><div class="line">     int fx=find(x),fy=find(y);</div><div class="line">     if(fx!=fy)</div><div class="line">         pre[fx ]=fy;</div><div class="line"> &#125;</div></pre></td></tr></table></figure><p>为了解释并查集的原理，我将举一个更有爱的例子。 话说江湖上散落着各式各样的大侠，有上千个之多。他们没有什么正当职业，整天背着剑在外面走来走去，碰到和自己不是一路人的，就免不了要打一架。但大侠们有一个优点就是讲义气，绝对不打自己的朋友。而且他们信奉“朋友的朋友就是我的朋友”，只要是能通过朋友关系串联起来的，不管拐了多少个弯，都认为是自己人。</p><p>这样一来，江湖上就形成了一个一个的群落，通过两两之间的朋友关系串联起来。而不在同一个群落的人，无论如何都无法通过朋友关系连起来，于是就可以放心往死了打。但是两个原本互不相识的人，如何判断是否属于一个朋友圈呢？</p><p>我们可以在每个朋友圈内推举出一个比较有名望的人，作为该圈子的代表人物，这样，每个圈子就可以这样命名“齐达内朋友之队”“罗纳尔多朋友之队”……两人只要互相对一下自己的队长是不是同一个人，就可以确定敌友关系了。</p><p>但是还有问题啊，大侠们只知道自己直接的朋友是谁，很多人压根就不认识队长，要判断自己的队长是谁，只能漫无目的的通过朋友的朋友关系问下去：“你是不是队长？你是不是队长？”这样一来，队长面子上挂不住了，而且效率太低，还有可能陷入无限循环中。</p><p>于是队长下令，重新组队。队内所有人实行分等级制度，形成树状结构，我队长就是根节点，下面分别是二级队员、三级队员。每个人只要记住自己的上级是谁就行了。遇到判断敌友的时候，只要一层层向上问，直到最高层，就可以在短时间内确定队长是谁了。</p><p>由于我们关心的只是两个人之间是否连通，至于他们是如何连通的，以及每个圈子内部的结构是怎样的，甚至队长是谁，并不重要。所以我们可以放任队长随意重新组队，只要不搞错敌友关系就好了。于是，门派产生了。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/QtPIxk7nOVdNRrpMREYRUAaor1hgv7kkWHTuGNGvAFRD5ibRSHjtSW8ZibABfibmYRrMWiaYJtrOnEQlL7UEhVt5PQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt=""></p><p>下面我们来看并查集的实现。 int pre[1000]; 这个数组，记录了每个大侠的上级是谁。大侠们从1或者0开始编号（依据题意而定），pre[15]=3就表示15号大侠的上级是3号大侠。如果一个人的上级就是他自己，那说明他就是掌门人了，查找到此为止。也有孤家寡人自成一派的，比如欧阳锋，那么他的上级就是他自己。</p><p>每个人都只认自己的上级。比如胡青牛同学只知道自己的上级是杨左使。张无忌是谁？不认识！要想知道自己的掌门是谁，只能一级级查上去。 find这个函数就是找掌门用的，意义再清楚不过了（路径压缩算法先不论，后面再说）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">int find(int x)                                          //查找我（x）的掌门</div><div class="line">&#123;</div><div class="line">    int r=x;                                               //委托 r 去找掌门</div><div class="line">    while (pre[r ]!=r)                                //如果r的上级不是r自己（也就是说找到的大侠他不是掌门 = =） </div><div class="line">    r=pre[r] ;                                           // r 就接着找他的上级，直到找到掌门为止。</div><div class="line">    return  r ;                                           //掌门驾到~~~</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>再来看看join函数，就是在两个点之间连一条线，这样一来，原先它们所在的两个板块的所有点就都可以互通了。这在图上很好办，画条线就行了。但我们现在是用并查集来描述武林中的状况的，一共只有一个pre[]数组，该如何实现呢？</p><p>还是举江湖的例子，假设现在武林中的形势如图所示。虚竹小和尚与周芷若MM是我非常喜欢的两个人物，他们的终极boss分别是玄慈方丈和灭绝师太，那明显就是两个阵营了。我不希望他们互相打架，就对他俩说：“你们两位拉拉勾，做好朋友吧。”他们看在我的面子上，同意了。这一同意可非同小可，整个少林和峨眉派的人就不能打架了。这么重大的变化，可如何实现呀，要改动多少地方？</p><p>其实非常简单，我对玄慈方丈说：“大师，麻烦你把你的上级改为灭绝师太吧。这样一来，两派原先的所有人员的终极boss都是师太，那还打个球啊！反正我们关心的只是连通性，门派内部的结构不要紧的。”玄慈一听肯定火大了：“我靠，凭什么是我变成她手下呀，怎么不反过来？我抗议！”抗议无效，上天安排的，最大。反正谁加入谁效果是一样的，我就随手指定了一个。这段函数的意思很明白了吧？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">void join(int x,int y)                                     //我想让虚竹和周芷若做朋友 </div><div class="line">&#123;</div><div class="line">    int fx=find(x),fy=find(y);                          //虚竹的老大是玄慈，芷若MM的老大是灭绝</div><div class="line">    if(fx!=fy)                                                  //玄慈和灭绝显然不是同一个人</div><div class="line">     pre[fx]=fy;                                              //方丈只好委委屈屈地当了师太的手下啦</div><div class="line">  </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>再来看看路径压缩算法。建立门派的过程是用join函数两个人两个人地连接起来的，谁当谁的手下完全随机。最后的树状结构会变成什么胎唇样，我也完全无法预计，一字排开也有可能。这样查找的效率就会比较低下。最理想的情况就是所有人的直接上级都是掌门，一共就两级结构，只要找一次就找到掌门了。哪怕不能完全做到，也最好尽量接近。这样就产生了路径压缩算法。</p><p>设想这样一个场景：两个互不相识的大侠碰面了，想知道能不能揍。 于是赶紧打电话问自己的上级：“你是不是掌门？” 上级说：“我不是呀，我的上级是谁谁谁，你问问他看看。” 一路问下去，原来两人的最终boss都是东厂曹公公。 “哎呀呀，原来是记己人，西礼西礼，在下三营六组白面葫芦娃!” “幸会幸会，在下九营十八组仙子狗尾巴花！” 两人高高兴兴地手拉手喝酒去了。 “等等等等，两位同学请留步，还有事情没完成呢！”我叫住他俩。 “哦，对了，还要做路径压缩。”两人醒悟。</p><p>白面葫芦娃打电话给他的上级六组长：“组长啊，我查过了，其习偶们的掌门是曹公公。不如偶们一起及接拜在曹公公手下吧，省得级别太低，以后查找掌门麻环。” “唔，有道理。” 白面葫芦娃接着打电话给刚才拜访过的三营长……仙子狗尾巴花也做了同样的事情。</p><p>这样，查询中所有涉及到的人物都聚集在曹公公的直接领导下。每次查询都做了优化处理，所以整个门派树的层数都会维持在比较低的水平上。路径压缩的代码，看得懂很好，看不懂也没关系，直接抄上用就行了。总之它所实现的功能就是这么个意思。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/QtPIxk7nOVdNRrpMREYRUAaor1hgv7kkRPXS7odI2JpZ10cHKV9dgGrsttLnxpSYwK6W8uYjqIffQKdP03IIhA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt=""></p><p>下面给出杭电1232畅通工程的解题代码，仅供大家参考，使用并查集来解决问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">#include&lt;iostream  </div><div class="line">using namespace std;  </div><div class="line">int  pre[1050];  </div><div class="line">bool t[1050];               //t 用于标记独立块的根结点  </div><div class="line"></div><div class="line">int Find(int x)  </div><div class="line">&#123;  </div><div class="line">    int r=x;  </div><div class="line">    while(r!=pre[r])  </div><div class="line">        r=pre[r];   </div><div class="line"></div><div class="line">    int i=x,j;  </div><div class="line">    while(pre[i]!=r)  </div><div class="line">    &#123;  </div><div class="line">        j=pre[i];  </div><div class="line">        pre[i]=r;  </div><div class="line">        i=j;  </div><div class="line">    &#125;  </div><div class="line"></div><div class="line">    return r;  </div><div class="line">&#125;  </div><div class="line"></div><div class="line">void mix(int x,int y)  </div><div class="line">&#123;  </div><div class="line">    int fx=Find(x),fy=Find(y);  </div><div class="line">    if(fx!=fy)  </div><div class="line">    &#123;  </div><div class="line">        pre[fy]=fx;  </div><div class="line">    &#125;  </div><div class="line">&#125;   </div><div class="line"></div><div class="line">int main()  </div><div class="line">&#123; </div><div class="line">    int N,M,a,b,i,j,ans;  </div><div class="line">    while(scanf(&quot;%d%d&quot;,&amp;N,&amp;M)&amp;&amp;N)  </div><div class="line">    &#123;  </div><div class="line">        for(i=1;i&lt;=N;i++)          //初始化   </div><div class="line">            pre[i]=i;  </div><div class="line">        for(i=1;i&lt;=M;i++)          //吸收并整理数据   </div><div class="line">        &#123;  </div><div class="line">            scanf(&quot;%d%d&quot;,&amp;a,&amp;b);  </div><div class="line">            mix(a,b);  </div><div class="line">        &#125;         </div><div class="line">        memset(t,0,sizeof(t));  </div><div class="line">        for(i=1;i&lt;=N;i++)          //标记根结点  </div><div class="line">        &#123;  </div><div class="line">            t[Find(i)]=1;  </div><div class="line">        &#125;  </div><div class="line">        for(ans=0,i=1;i&lt;=N;i++)  </div><div class="line">            if(t[i])  </div><div class="line">                ans++;  </div><div class="line">        printf(&quot;%d\n&quot;,ans-1);  </div><div class="line">    &#125;  </div><div class="line"></div><div class="line">    return 0;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;五分钟搞懂并查集&quot;&gt;&lt;a href=&quot;#五分钟搞懂并查集&quot; class=&quot;headerlink&quot; title=&quot;五分钟搞懂并查集&quot;&gt;&lt;/a&gt;五分钟搞懂并查集&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;转自：laserss&lt;br&gt;&lt;a href=&quot;http://blog
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>37 Reasons why your Neural Network is not working [Repost]</title>
    <link href="http://yoursite.com/2017/07/28/37-Reasons-why-your-Neural-Network-is-not-working/"/>
    <id>http://yoursite.com/2017/07/28/37-Reasons-why-your-Neural-Network-is-not-working/</id>
    <published>2017-07-28T06:23:50.000Z</published>
    <updated>2017-07-28T06:38:10.114Z</updated>
    
    <content type="html"><![CDATA[<p>The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then came the predictions: all zeroes, all background, nothing detected. “What did I do wrong?” — I asked my computer, who didn’t answer.</p><p>Where do you start checking if your model is outputting garbage (for example predicting the mean of all outputs, or it has really poor accuracy)?</p><p>A network might not be training for a number of reasons. Over the course of many debugging sessions, I would often find myself doing the same checks. I’ve compiled my experience along with the best ideas around in this handy list. I hope they would be of use to you, too.</p><h3 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h3><blockquote><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#b6fb" target="_blank" rel="external">0. How to use this guide?</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#678a" target="_blank" rel="external">I. Dataset issues</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#86fe" target="_blank" rel="external">II. Data Normalization/Augmentation issues</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#95eb" target="_blank" rel="external">III. Implementation issues</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#74de" target="_blank" rel="external">IV. Training issues</a></p></blockquote><h3 id="0-How-to-use-this-guide"><a href="#0-How-to-use-this-guide" class="headerlink" title="0. How to use this guide?"></a>0. How to use this guide?</h3><p>A lot of things can go wrong. But some of them are more likely to be broken than others. I usually start with this short list as an emergency first response:</p><ol><li>Start with a simple model that is known to work for this type of data (for example, VGG for images). Use a standard loss if possible.</li><li>Turn off all bells and whistles, e.g. regularization and data augmentation.</li><li>If finetuning a model, double check the preprocessing, for it should be the same as the original model’s training.</li><li>Verify that the input data is correct.</li><li>Start with a really small dataset (2–20 samples). Overfit on it and gradually add more data.</li><li>Start gradually adding back all the pieces that were omitted: augmentation/regularization, custom loss functions, try more complex models.</li></ol><p>If the steps above don’t do it, start going down the following big list and verify things one by one.</p><hr><h3 id="I-Dataset-issues"><a href="#I-Dataset-issues" class="headerlink" title="I. Dataset issues"></a>I. Dataset issues</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*xfIbyKKMDmjQF9JFuK2Ykg.png" alt="img">Source: <a href="http://dilbert.com/strip/2014-05-07" target="_blank" rel="external">http://dilbert.com/strip/2014-05-07</a></p><h4 id="1-Check-your-input-data"><a href="#1-Check-your-input-data" class="headerlink" title="1. Check your input data"></a>1. Check your input data</h4><p>Check if the input data you are feeding the network makes sense. For example, I’ve more than once mixed the width and the height of an image. Sometimes, I would feed all zeroes by mistake. Or I would use the same batch over and over. So print/display a couple of batches of input and target output and make sure they are OK.</p><h4 id="2-Try-random-input"><a href="#2-Try-random-input" class="headerlink" title="2. Try random input"></a>2. Try random input</h4><p>Try passing random numbers instead of actual data and see if the error behaves the same way. If it does, it’s a sure sign that your net is turning data into garbage at some point. Try debugging layer by layer /op by op/ and see where things go wrong.</p><h4 id="3-Check-the-data-loader"><a href="#3-Check-the-data-loader" class="headerlink" title="3. Check the data loader"></a>3. Check the data loader</h4><p>Your data might be fine but the code that passes the input to the net might be broken. Print the input of the first layer before any operations and check it.</p><h4 id="4-Make-sure-input-is-connected-to-output"><a href="#4-Make-sure-input-is-connected-to-output" class="headerlink" title="4. Make sure input is connected to output"></a>4. Make sure input is connected to output</h4><p>Check if a few input samples have the correct labels. Also make sure shuffling input samples works the same way for output labels.</p><h4 id="5-Is-the-relationship-between-input-and-output-too-random"><a href="#5-Is-the-relationship-between-input-and-output-too-random" class="headerlink" title="5. Is the relationship between input and output too random?"></a>5. Is the relationship between input and output too random?</h4><p>Maybe the non-random part of the relationship between the input and output is too small compared to the random part (one could argue that stock prices are like this). I.e. the input are not sufficiently related to the output. There isn’t an universal way to detect this as it depends on the nature of the data.</p><h4 id="6-Is-there-too-much-noise-in-the-dataset"><a href="#6-Is-there-too-much-noise-in-the-dataset" class="headerlink" title="6. Is there too much noise in the dataset?"></a>6. Is there too much noise in the dataset?</h4><p>This happened to me once when I scraped an image dataset off a food site. There were so many bad labels that the network couldn’t learn. Check a bunch of input samples manually and see if labels seem off.</p><p>The cutoff point is up for debate, as <a href="https://arxiv.org/pdf/1412.6596.pdf" target="_blank" rel="external">this paper</a> got above 50% accuracy on MNIST using 50% corrupted labels.</p><h4 id="7-Shuffle-the-dataset"><a href="#7-Shuffle-the-dataset" class="headerlink" title="7. Shuffle the dataset"></a>7. Shuffle the dataset</h4><p>If your dataset hasn’t been shuffled and has a particular order to it (ordered by label) this could negatively impact the learning. Shuffle your dataset to avoid this. Make sure you are shuffling input and labels together.</p><h4 id="8-Reduce-class-imbalance"><a href="#8-Reduce-class-imbalance" class="headerlink" title="8. Reduce class imbalance"></a>8. Reduce class imbalance</h4><p>Are there a 1000 class A images for every class B image? Then you might need to balance your loss function or <a href="http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/" target="_blank" rel="external">try other class imbalance approaches</a>.</p><h4 id="9-Do-you-have-enough-training-examples"><a href="#9-Do-you-have-enough-training-examples" class="headerlink" title="9. Do you have enough training examples?"></a>9. Do you have enough training examples?</h4><p>If you are training a net from scratch (i.e. not finetuning), you probably need lots of data. For image classification, <a href="https://stats.stackexchange.com/a/226693/30773" target="_blank" rel="external">people say</a> you need a 1000 images per class or more.</p><h4 id="10-Make-sure-your-batches-don’t-contain-a-single-label"><a href="#10-Make-sure-your-batches-don’t-contain-a-single-label" class="headerlink" title="10. Make sure your batches don’t contain a single label"></a>10. Make sure your batches don’t contain a single label</h4><p>This can happen in a sorted dataset (i.e. the first 10k samples contain the same class). Easily fixable by shuffling the dataset.</p><h4 id="11-Reduce-batch-size"><a href="#11-Reduce-batch-size" class="headerlink" title="11. Reduce batch size"></a>11. Reduce batch size</h4><p><a href="https://arxiv.org/abs/1609.04836" target="_blank" rel="external">This paper</a> points out that having a very large batch can reduce the generalization ability of the model.</p><h4 id="Addition-1-Use-standard-dataset-e-g-mnist-cifar10"><a href="#Addition-1-Use-standard-dataset-e-g-mnist-cifar10" class="headerlink" title="Addition 1. Use standard dataset (e.g. mnist, cifar10)"></a>Addition 1. Use standard dataset (e.g. mnist, cifar10)</h4><p>Thanks to @<a href="https://medium.com/@hengcherkeng" target="_blank" rel="external">hengcherkeng</a> for this one:</p><blockquote><p>When testing new network architecture or writing a new piece of code, use the standard datasets first, instead of your own data. This is because there are many reference results for these datasets and they are proved to be ‘solvable’. There will be no issues of label noise, train/test distribution difference , too much difficulty in dataset, etc.</p></blockquote><hr><h3 id="II-Data-Normalization-Augmentation"><a href="#II-Data-Normalization-Augmentation" class="headerlink" title="II. Data Normalization/Augmentation"></a>II. Data Normalization/Augmentation</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*UQLMfdKi5D4nNDN6Oxa5MA.png" alt="img"></p><h4 id="12-Standardize-the-features"><a href="#12-Standardize-the-features" class="headerlink" title="12. Standardize the features"></a><strong>12. Standardize</strong> the features</h4><p>Did you standardize your input to have zero mean and unit variance?</p><h4 id="13-Do-you-have-too-much-data-augmentation"><a href="#13-Do-you-have-too-much-data-augmentation" class="headerlink" title="13. Do you have too much data augmentation?"></a>13. Do you have too much data augmentation?</h4><p>Augmentation has a regularizing effect. Too much of this combined with other forms of regularization (weight L2, dropout, etc.) can cause the net to underfit.</p><h4 id="14-Check-the-preprocessing-of-your-pretrained-model"><a href="#14-Check-the-preprocessing-of-your-pretrained-model" class="headerlink" title="14. Check the preprocessing of your pretrained model"></a>14. Check the preprocessing of your pretrained model</h4><p>If you are using a pretrained model, make sure you are using the same normalization and preprocessing as the model was when training. For example, should an image pixel be in the range [0, 1], [-1, 1] or [0, 255]?</p><h4 id="15-Check-the-preprocessing-for-train-validation-test-set"><a href="#15-Check-the-preprocessing-for-train-validation-test-set" class="headerlink" title="15. Check the preprocessing for train/validation/test set"></a>15. Check the preprocessing for train/validation/test set</h4><p>CS231n points out a <a href="http://cs231n.github.io/neural-networks-2/#datapre" target="_blank" rel="external">common pitfall</a>:</p><blockquote><p>“… any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation/test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. “</p></blockquote><p>Also, check for different preprocessing in each sample or batch.</p><hr><h3 id="III-Implementation-issues"><a href="#III-Implementation-issues" class="headerlink" title="III. Implementation issues"></a>III. Implementation issues</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*EVy3hNSF4Nq7v7bNYOyNcQ.png" alt="img">Credit: <a href="https://xkcd.com/1838/" target="_blank" rel="external">https://xkcd.com/1838/</a></p><h4 id="16-Try-solving-a-simpler-version-of-the-problem"><a href="#16-Try-solving-a-simpler-version-of-the-problem" class="headerlink" title="16. Try solving a simpler version of the problem"></a>16. Try solving a simpler version of the problem</h4><p>This will help with finding where the issue is. For example, if the target output is an object class and coordinates, try limiting the prediction to object class only.</p><h4 id="17-Look-for-correct-loss-“at-chance”"><a href="#17-Look-for-correct-loss-“at-chance”" class="headerlink" title="17. Look for correct loss “at chance”"></a>17. Look for correct loss “at chance”</h4><p>Again from the excellent <a href="http://cs231n.github.io/neural-networks-3/#sanitycheck" target="_blank" rel="external">CS231n</a>: <em>Initialize with small parameters, without regularization. For example, if we have 10 classes, at chance means we will get the correct class 10% of the time, and the Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302.</em></p><p>After this, try increasing the regularization strength which should increase the loss.</p><h4 id="18-Check-your-loss-function"><a href="#18-Check-your-loss-function" class="headerlink" title="18. Check your loss function"></a>18. Check your loss function</h4><p>If you implemented your own loss function, check it for bugs and add unit tests. Often, my loss would be slightly incorrect and hurt the performance of the network in a subtle way.</p><h4 id="19-Verify-loss-input"><a href="#19-Verify-loss-input" class="headerlink" title="19. Verify loss input"></a>19. Verify loss input</h4><p>If you are using a loss function provided by your framework, make sure you are passing to it what it expects. For example, in PyTorch I would mix up the NLLLoss and CrossEntropyLoss as the former requires a softmax input and the latter doesn’t.</p><h4 id="20-Adjust-loss-weights"><a href="#20-Adjust-loss-weights" class="headerlink" title="20. Adjust loss weights"></a>20. Adjust loss weights</h4><p>If your loss is composed of several smaller loss functions, make sure their magnitude relative to each is correct. This might involve testing different combinations of loss weights.</p><h4 id="21-Monitor-other-metrics"><a href="#21-Monitor-other-metrics" class="headerlink" title="21. Monitor other metrics"></a>21. Monitor other metrics</h4><p>Sometimes the loss is not the best predictor of whether your network is training properly. If you can, use other metrics like accuracy.</p><h4 id="22-Test-any-custom-layers"><a href="#22-Test-any-custom-layers" class="headerlink" title="22. Test any custom layers"></a>22. Test any custom layers</h4><p>Did you implement any of the layers in the network yourself? Check and double-check to make sure they are working as intended.</p><h4 id="23-Check-for-“frozen”-layers-or-variables"><a href="#23-Check-for-“frozen”-layers-or-variables" class="headerlink" title="23. Check for “frozen” layers or variables"></a>23. Check for “frozen” layers or variables</h4><p>Check if you unintentionally disabled gradient updates for some layers/variables that should be learnable.</p><h4 id="24-Increase-network-size"><a href="#24-Increase-network-size" class="headerlink" title="24. Increase network size"></a>24. Increase network size</h4><p>Maybe the expressive power of your network is not enough to capture the target function. Try adding more layers or more hidden units in fully connected layers.</p><h4 id="25-Check-for-hidden-dimension-errors"><a href="#25-Check-for-hidden-dimension-errors" class="headerlink" title="25. Check for hidden dimension errors"></a>25. Check for hidden dimension errors</h4><p>If your input looks like (k, H, W) = (64, 64, 64) it’s easy to miss errors related to wrong dimensions. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network.</p><h4 id="26-Explore-Gradient-checking"><a href="#26-Explore-Gradient-checking" class="headerlink" title="26. Explore Gradient checking"></a>26. Explore Gradient checking</h4><p>If you implemented Gradient Descent by hand, gradient checking makes sure that your backpropagation works like it should. More info: <a href="http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/" target="_blank" rel="external">1</a> <a href="http://cs231n.github.io/neural-networks-3/#gradcheck" target="_blank" rel="external">2</a> <a href="https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking" target="_blank" rel="external">3</a>.</p><hr><h3 id="IV-Training-issues"><a href="#IV-Training-issues" class="headerlink" title="IV. Training issues"></a>IV. Training issues</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*gfcJD0eymh5SGuquzuvpig.png" alt="img">Credit: <a href="http://carlvondrick.com/ihog/" target="_blank" rel="external">http://carlvondrick.com/ihog/</a></p><h4 id="27-Solve-for-a-really-small-dataset"><a href="#27-Solve-for-a-really-small-dataset" class="headerlink" title="27. Solve for a really small dataset"></a>27. Solve for a really small dataset</h4><p><strong>Overfit a small subset of the data and make sure it works. </strong>For example, train with just 1 or 2 examples and see if your network can learn to differentiate these. Move on to more samples per class.</p><h4 id="28-Check-weights-initialization"><a href="#28-Check-weights-initialization" class="headerlink" title="28. Check weights initialization"></a>28. Check weights initialization</h4><p>If unsure, use <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="external">Xavier</a> or <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf" target="_blank" rel="external">He</a> initialization. Also, your initialization might be leading you to a bad local minimum, so try a different initialization and see if it helps.</p><h4 id="29-Change-your-hyperparameters"><a href="#29-Change-your-hyperparameters" class="headerlink" title="29. Change your hyperparameters"></a>29. Change your hyperparameters</h4><p>Maybe you using a particularly bad set of hyperparameters. If feasible, try a <a href="http://scikit-learn.org/stable/modules/grid_search.html" target="_blank" rel="external">grid search</a>.</p><h4 id="30-Reduce-regularization"><a href="#30-Reduce-regularization" class="headerlink" title="30. Reduce regularization"></a>30. Reduce regularization</h4><p>Too much regularization can cause the network to underfit badly. Reduce regularization such as dropout, batch norm, weight/bias L2 regularization, etc. In the excellent “<a href="http://course.fast.ai/" target="_blank" rel="external">Practical Deep Learning for coders</a>” course, <a href="https://twitter.com/jeremyphoward" target="_blank" rel="external">Jeremy Howard</a> advises getting rid of underfitting first. This means you overfit the training data sufficiently, and only then addressing overfitting.</p><h4 id="31-Give-it-time"><a href="#31-Give-it-time" class="headerlink" title="31. Give it time"></a>31. Give it time</h4><p>Maybe your network needs more time to train before it starts making meaningful predictions. If your loss is steadily decreasing, let it train some more.</p><h4 id="32-Switch-from-Train-to-Test-mode"><a href="#32-Switch-from-Train-to-Test-mode" class="headerlink" title="32. Switch from Train to Test mode"></a>32. Switch from Train to Test mode</h4><p>Some frameworks have layers like Batch Norm, Dropout, and other layers behave differently during training and testing. Switching to the appropriate mode might help your network to predict properly.</p><h4 id="33-Visualize-the-training"><a href="#33-Visualize-the-training" class="headerlink" title="33. Visualize the training"></a>33. Visualize the training</h4><ul><li>Monitor the activations, weights, and updates of each layer. Make sure their magnitudes match. For example, the magnitude of the updates to the parameters (weights and biases) <a href="https://cs231n.github.io/neural-networks-3/#summary" target="_blank" rel="external">should be 1-e3</a>.</li><li>Consider a visualization library like <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">Tensorboard</a> and <a href="https://github.com/torrvision/crayon" target="_blank" rel="external">Crayon</a>. In a pinch, you can also print weights/biases/activations.</li><li>Be on the lookout for layer activations with a mean much larger than 0. Try Batch Norm or ELUs.</li><li><a href="https://deeplearning4j.org/visualization#usingui" target="_blank" rel="external">Deeplearning4j</a> points out what to expect in histograms of weights and biases:</li></ul><blockquote><p>“For weights, these histograms should have an <strong>approximately Gaussian (normal) </strong>distribution, after some time. For biases, these histograms will generally start at 0, and will usually end up being <strong>approximately Gaussian</strong>(One exception to this is for LSTM). Keep an eye out for parameters that are diverging to +/- infinity. Keep an eye out for biases that become very large. This can sometimes occur in the output layer for classification if the distribution of classes is very imbalanced.”</p></blockquote><ul><li>Check layer updates, they should have a Gaussian distribution.</li></ul><h4 id="34-Try-a-different-optimizer"><a href="#34-Try-a-different-optimizer" class="headerlink" title="34. Try a different optimizer"></a>34. Try a different optimizer</h4><p>Your choice of optimizer shouldn’t prevent your network from training unless you have selected particularly bad hyperparameters. However, the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time. The paper which describes the algorithm you are using should specify the optimizer. If not, I tend to use Adam or plain SGD with momentum.</p><p>Check this <a href="http://ruder.io/optimizing-gradient-descent/" target="_blank" rel="external">excellent post</a> by Sebastian Ruder to learn more about gradient descent optimizers.</p><h4 id="35-Exploding-Vanishing-gradients"><a href="#35-Exploding-Vanishing-gradients" class="headerlink" title="35. Exploding / Vanishing gradients"></a>35. Exploding / Vanishing gradients</h4><ul><li>Check layer updates, as very large values can indicate exploding gradients. Gradient clipping may help.</li><li>Check layer activations. From <a href="https://deeplearning4j.org/visualization#usingui" target="_blank" rel="external">Deeplearning4j</a> comes a great guideline: <em>“A good standard deviation for the activations is on the order of 0.5 to 2.0. Significantly outside of this range may indicate vanishing or exploding activations.”</em></li></ul><h4 id="36-Increase-Decrease-Learning-Rate"><a href="#36-Increase-Decrease-Learning-Rate" class="headerlink" title="36. Increase/Decrease Learning Rate"></a>36. Increase/Decrease Learning Rate</h4><p>A low learning rate will cause your model to converge very slowly.</p><p>A high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution.</p><p>Play around with your current learning rate by multiplying it by 0.1 or 10.</p><h4 id="37-Overcoming-NaNs"><a href="#37-Overcoming-NaNs" class="headerlink" title="37. Overcoming NaNs"></a>37. Overcoming NaNs</h4><p>Getting a NaN (Non-a-Number) is a much bigger issue when training RNNs (from what I hear). Some approaches to fix it:</p><ul><li>Decrease the learning rate, especially if you are getting NaNs in the first 100 iterations.</li><li>NaNs can arise from division by zero or natural log of zero or negative number.</li><li>Russell Stewart has great pointers on <a href="http://russellsstewart.com/notes/0.html" target="_blank" rel="external">how to deal with NaNs</a>.</li><li>Try evaluating your network layer by layer and see where the NaNs appear.</li></ul><hr><h4 id="Resources"><a href="#Resources" class="headerlink" title="Resources:"></a><strong>Resources:</strong></h4><blockquote><p><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/</a><br><a href="http://russellsstewart.com/notes/0.html" target="_blank" rel="external">http://russellsstewart.com/notes/0.html</a><br><a href="https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class" target="_blank" rel="external">https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class</a><br><a href="https://deeplearning4j.org/visualization" target="_blank" rel="external">https://deeplearning4j.org/visualization</a><br><a href="https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/" target="_blank" rel="external">https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/</a><br><a href="https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phase" target="_blank" rel="external">https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phase</a><br><a href="http://book.caltech.edu/bookforum/showthread.php?t=4113" target="_blank" rel="external">http://book.caltech.edu/bookforum/showthread.php?t=4113</a><br><a href="https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134" target="_blank" rel="external">https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134</a><br><a href="https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithm" target="_blank" rel="external">https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithm</a></p></blockquote><p><strong>Origin post is <a href="*https://medium.com/@slavivanov/4020854bd607*">here</a>.</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch Tutorial (fork from official website)</title>
    <link href="http://yoursite.com/2017/07/24/PyTorch-Totorial-fork-from-official-website/"/>
    <id>http://yoursite.com/2017/07/24/PyTorch-Totorial-fork-from-official-website/</id>
    <published>2017-07-24T13:41:03.000Z</published>
    <updated>2017-07-25T02:09:05.948Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/tensor_tutorial.ipynb" target="_blank" rel="external">Tensor tutorial</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/autograd_tutorial.ipynb" target="_blank" rel="external">Autograd_tutorial</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/neural_networks_tutorial.ipynb" target="_blank" rel="external">Neural_networks_tutorial</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/cifar10_tutorial.ipynb" target="_blank" rel="external">CIFAR10_tutorial</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/tensor_tutorial.ipynb&quot; target=&quot;_blank&quot; rel=&quot;extern
    
    </summary>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Models Implemented By Tensorflow</title>
    <link href="http://yoursite.com/2017/07/24/Machine-Learning-Models-Implemented-By-Tensorflow/"/>
    <id>http://yoursite.com/2017/07/24/Machine-Learning-Models-Implemented-By-Tensorflow/</id>
    <published>2017-07-24T09:54:18.000Z</published>
    <updated>2017-07-24T09:55:57.021Z</updated>
    
    <content type="html"><![CDATA[<p>Project: <a href="https://github.com/ewanlee/finch/tree/master" target="_blank" rel="external">https://github.com/ewanlee/finch/tree/master</a></p><p>There are these algorithms in the tensorflow-models:</p><ul><li>Linear regression</li><li>Logistic regression</li><li>SVM</li><li>Autoencoder (MLP based and CNN based)</li><li>NMF</li><li>GAN</li><li>Conditional GAN</li><li>DCGAN</li><li>CNN</li><li>RNN (for classification and for regression)</li><li>Highway network (MLP based)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Project: &lt;a href=&quot;https://github.com/ewanlee/finch/tree/master&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/ewanlee/finch/tree/mast
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>sklearn-based feature engineering</title>
    <link href="http://yoursite.com/2017/07/20/sklearn-based-feature-engineering/"/>
    <id>http://yoursite.com/2017/07/20/sklearn-based-feature-engineering/</id>
    <published>2017-07-20T04:09:03.000Z</published>
    <updated>2017-07-20T04:23:44.320Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/feature-related/fe.ipynb" target="_blank" rel="external">feature engineering</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/feature-related/parallel.ipynb" target="_blank" rel="external">pipeline</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/feature-related/fe.ipynb&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;feature 
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>WGAN-GP [Repost]</title>
    <link href="http://yoursite.com/2017/07/18/WGAN-GP/"/>
    <id>http://yoursite.com/2017/07/18/WGAN-GP/</id>
    <published>2017-07-18T03:16:24.000Z</published>
    <updated>2017-07-18T13:49:35.700Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ewanlee.github.io/2017/04/29/The-awesome-Wasserstein-GAN/" target="_blank" rel="external">WGAN</a>存在着训练困难、收敛速度慢等问题。<a href="https://link.zhihu.com/?target=https%3A//www.reddit.com/r/MachineLearning/comments/5zd4c0/d_survey_whats_the_most_stable_regiment_for/dexfhxu/%3Futm_content%3Dpermalink%26utm_medium%3Dfront%26utm_source%3Dreddit%26utm_name%3DMachineLearning" target="_blank" rel="external">WGAN的作者Martin Arjovsky不久后就在reddit上表示他也意识到了这个问题</a>，认为关键在于原设计中Lipschitz限制的施加方式不对：</p><blockquote><p>I am now pretty convinced that the problems that happen sometimes in WGANs is due to the specific way of how weight clipping works. It’s just a terrible way of enforcing a Lipschitz constraint, and better ways are out there. I feel like apologizing for being too lazy and sticking to what could be done in one line of torch code.</p><p>A simple alternative (less than 5 lines of code) has been found by Montréal students. It works on quite a few settings (inc 100 layer resnets) with default hyperparameters. Arxiv coming this or next week, stay tuned.</p></blockquote><p>并在新论文中提出了相应的改进方案：</p><ul><li>论文：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.00028" target="_blank" rel="external">[1704.00028] Improved Training of Wasserstein GANs</a></li><li>Tensorflow实现：<ul><li><a href="https://github.com/brianherman/improved_wgan_training" target="_blank" rel="external">brianherman/improved_wgan_training</a> (Python 3)</li><li><a href="https://github.com/igul222/improved_wgan_training" target="_blank" rel="external">igul222/improved_wgan_training</a> (Python 2)</li></ul></li></ul><p><strong>首先回顾一下WGAN的关键部分——Lipschitz限制是什么。</strong>WGAN中，判别器D和生成器G的loss函数分别是<br>$$<br>\begin{align}<br>L(D) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] \\<br>L(G) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)]<br>\end{align}<br>$$<br>公式1表示判别器希望尽可能拉高真样本的分数，拉低假样本的分数，公式2表示生成器希望尽可能拉高假样本的分数。</p><p>Lipschitz限制则体现为，在整个样本空间$\mathcal{X}$上，要求判别器函数$D(x)$梯度的$L_p$ norm大于一个有限的常数K：<br>$$<br>| \nabla_x D(x) |_p \leq K, \forall x \in \mathcal{X}<br>$$<br>直观上解释，就是当输入的样本稍微变化后，判别器给出的分数不能发生太过剧烈的变化。在原来的论文中，这个限制具体是通过weight clipping的方式实现的：每当更新完一次判别器的参数之后，就检查判别器的所有参数的绝对值有没有超过一个阈值，比如0.01，有的话就把这些参数clip回 [-0.01, 0.01] 范围内。通过在训练过程中保证判别器的所有参数有界，就保证了判别器不能对两个略微不同的样本给出天差地别的分数值，从而间接实现了Lipschitz限制。</p><p><strong>然而weight clipping的实现方式存在两个严重问题：</strong></p><p>第一，如公式1所言，判别器loss希望尽可能拉大真假样本的分数差，然而weight clipping独立地限制每一个网络参数的取值范围，在这种情况下我们可以想象，最优的策略就是尽可能让所有参数走极端，要么取最大值（如0.01）要么取最小值（如-0.01）！为了验证这一点，作者统计了经过充分训练的判别器中所有网络参数的数值分布，发现真的集中在最大和最小两个极端上：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-clip.png" alt="weight-clipping"></p><p>这样带来的结果就是，判别器会非常倾向于学习一个简单的映射函数（想想看，几乎所有参数都是正负0.01，都已经可以直接视为一个<a href="https://link.zhihu.com/?target=http%3A//synchuman.baijia.baidu.com/article/385441" target="_blank" rel="external">二值神经网络**</a>了，太简单了）。而作为一个深层神经网络来说，这实在是对自身强大拟合能力的巨大浪费！判别器没能充分利用自身的模型能力，经过它回传给生成器的梯度也会跟着变差。</p><p>在正式介绍gradient penalty之前，我们可以先看看在它的指导下，同样充分训练判别器之后，参数的数值分布就合理得多了，判别器也能够充分利用自身模型的拟合能力：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-gp.png" alt="weight-gp"></p><p>第二个问题，weight clipping会导致很容易一不小心就梯度消失或者梯度爆炸。原因是判别器是一个多层网络，如果我们把clipping threshold设得稍微小了一点，每经过一层网络，梯度就变小一点点，多层之后就会指数衰减；反之，如果设得稍微大了一点，每经过一层网络，梯度变大一点点，多层之后就会指数爆炸。只有设得不大不小，才能让生成器获得恰到好处的回传梯度，然而在实际应用中这个平衡区域可能很狭窄，就会给调参工作带来麻烦。相比之下，gradient penalty就可以让梯度在后向传播的过程中保持平稳。论文通过下图体现了这一点，其中横轴代表判别器从低到高第几层，纵轴代表梯度回传到这一层之后的尺度大小（注意纵轴是对数刻度），c是clipping threshold：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-gradient-norm.png" alt="gradient-norm"></p><p><strong>说了这么多，gradient penalty到底是什么？</strong></p><p>前面提到，Lipschitz限制是要求判别器的梯度不超过K，那我们何不直接设置一个额外的loss项来体现这一点呢？比如说：<br>$$<br>ReLU[| \nabla_x D(x) |_p - K]<br>$$<br>不过，既然判别器希望尽可能拉大真假样本的分数差距，那自然是希望梯度越大越好，变化幅度越大越好，所以判别器在充分训练之后，其梯度norm其实就会是在K附近。知道了这一点，我们可以把上面的loss改成要求梯度norm离K越近越好，效果是类似的：<br>$$<br>[| \nabla_x D(x) |_p - K]^2<br>$$<br>究竟是公式4好还是公式5好，我看不出来，可能需要实验验证，反正论文作者选的是公式5。接着我们简单地把K定为1，再跟WGAN原来的判别器loss加权合并，就得到新的判别器loss：<br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \mathcal{X}} [| \nabla_x D(x) |_p - 1]^2<br>$$<br>这就是所谓的gradient penalty了吗？还没完。公式6有两个问题，首先是loss函数中存在梯度项，那么优化这个loss岂不是要算梯度的梯度？一些读者可能对此存在疑惑，不过这属于实现上的问题，放到后面说。</p><p>其次，3个loss项都是期望的形式，落到实现上肯定得变成采样的形式。前面两个期望的采样我们都熟悉，第一个期望是从真样本集里面采，第二个期望是从生成器的噪声输入分布采样后，再由生成器映射到样本空间。可是第三个分布要求我们在整个样本空间$\mathcal{X}$上采样，这完全不科学！由于所谓的维度灾难问题，如果要通过采样的方式在图片或自然语言这样的高维样本空间中估计期望值，所需样本量是指数级的，实际上没法做到。</p><p>所以，论文作者就非常机智地提出，我们其实没必要在整个样本空间上施加Lipschitz限制，只要重点抓住生成样本集中区域、真实样本集中区域以及夹在它们中间的区域就行了。具体来说，我们先随机采一对真假样本，还有一个0-1的随机数：<br>$$<br>x_r \sim P_r, x_g \sim P_g, \epsilon \sim Uniform[0, 1]<br>$$<br>然后在$x_r$和$x_g$的连线上随机插值采样：<br>$$<br>\hat{x} = \epsilon x_r + (1 - \epsilon) x_g<br>$$<br>把按照上述流程采样得到的$\hat{x}$所满足的分布记为$P_{\hat{x}}$, 就得到最终版本的判别器loss：<br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \hat{x}} [| \nabla_x D(x) |_p - 1]^2<br>$$<br><strong>这就是新论文所采用的gradient penalty方法，相应的新WGAN模型简称为WGAN-GP。</strong>我们可以做一个对比：</p><ul><li>weight clipping是对样本空间全局生效，但因为是间接限制判别器的梯度norm，会导致一不小心就梯度消失或者梯度爆炸；</li><li>gradient penalty只对真假样本集中区域、及其中间的过渡地带生效，但因为是直接把判别器的梯度norm限制在1附近，所以梯度可控性非常强，容易调整到合适的尺度大小。</li></ul><p>论文还讲了一些使用gradient penalty时需要注意的配套事项，这里只提一点：由于我们是对每个样本独立地施加梯度惩罚，所以判别器的模型架构中不能使用Batch Normalization，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择其他normalization方法，如Layer Normalization、Weight Normalization和Instance Normalization，这些方法就不会引入样本之间的依赖。论文推荐的是Layer Normalization。</p><p>实验表明，gradient penalty能够显著提高训练速度，解决了原始WGAN收敛缓慢的问题：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-exper.png" alt="exper"></p><p>虽然还是比不过DCGAN，但是因为WGAN不存在平衡判别器与生成器的问题，所以会比DCGAN更稳定，还是很有优势的。不过，作者凭什么能这么说？因为下面的实验体现出，在各种不同的网络架构下，其他GAN变种能不能训练好，可以说是一件相当看人品的事情，但是WGAN-GP全都能够训练好，尤其是最下面一行所对应的101层残差神经网络：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-image-gen.png" alt="image-gen"></p><p><strong>剩下的实验结果中，比较厉害的是第一次成功做到了“纯粹的”的文本GAN训练！</strong>我们知道在图像上训练GAN是不需要额外的有监督信息的，但是之前就没有人能够像训练图像GAN一样训练好一个文本GAN，要么依赖于预训练一个语言模型，要么就是利用已有的有监督ground truth提供指导信息。而现在WGAN-GP终于在无需任何有监督信息的情况下，生成出下图所示的英文字符序列：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-text-gen.png" alt="text-gen"></p><p>它是怎么做到的呢？我认为关键之处是对样本形式的更改。<strong>以前我们一般会把文本这样的离散序列样本表示为sequence of index，但是它把文本表示成sequence of probability vector。</strong>对于生成样本来说，我们可以取网络softmax层输出的词典概率分布向量，作为序列中每一个位置的内容；而对于真实样本来说，每个probability vector实际上就蜕化为我们熟悉的onehot vector。</p><p>但是如果按照传统GAN的思路来分析，这不是作死吗？一边是hard onehot vector，另一边是soft probability vector，判别器一下子就能够区分它们，生成器还怎么学习？没关系，对于WGAN来说，真假样本好不好区分并不是问题，WGAN只是拉近两个分布之间的Wasserstein距离，就算是一边是hard onehot另一边是soft probability也可以拉近，在训练过程中，概率向量中的有些项可能会慢慢变成0.8、0.9到接近1，整个向量也会接近onehot，最后我们要真正输出sequence of index形式的样本时，只需要对这些概率向量取argmax得到最大概率的index就行了。</p><p>新的样本表示形式+WGAN的分布拉近能力是一个“黄金组合”，但除此之外，还有其他因素帮助论文作者跑出上图的效果，包括：</p><ul><li>文本粒度为英文字符，而非英文单词，所以字典大小才二三十，大大减小了搜索空间</li><li>文本长度也才32</li><li>生成器用的不是常见的LSTM架构，而是多层反卷积网络，输入一个高斯噪声向量，直接一次性转换出所有32个字符</li></ul><p><strong>最后说回gradient penalty的实现问题。</strong>loss中本身包含梯度，优化loss就需要求梯度的梯度，这个功能并不是现在所有深度学习框架的标配功能，不过好在Tensorflow就有提供这个接口—<code>tf.gradients</code>。开头链接的GitHub源码中就是这么写的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># interpolates就是随机插值采样得到的图像，gradients就是loss中的梯度惩罚项</div><div class="line">gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]</div></pre></td></tr></table></figure><p>完整的loss是这样实现的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">gen_cost = -tf.reduce_mean(disc_fake)</div><div class="line">disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)</div><div class="line"></div><div class="line">alpha = tf.random_uniform(</div><div class="line">	shape=[BATCH_SIZE,<span class="number">1</span>], </div><div class="line">	minval=<span class="number">0.</span>,</div><div class="line">	maxval=<span class="number">1.</span></div><div class="line">)</div><div class="line">differences = fake_data - real_data</div><div class="line">interpolates = real_data + (alpha*differences)</div><div class="line">gradients = tf.gradients(Discriminator(interpolates), [interpolates])[<span class="number">0</span>]</div><div class="line">slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[<span class="number">1</span>]))</div><div class="line">gradient_penalty = tf.reduce_mean((slopes<span class="number">-1.</span>)**<span class="number">2</span>)</div><div class="line">disc_cost += LAMBDA*gradient_penalty</div><div class="line"></div><div class="line">gen_train_op = tf.train.AdamOptimizer(</div><div class="line">	learning_rate=<span class="number">1e-4</span>, </div><div class="line">	beta1=<span class="number">0.5</span>,</div><div class="line">	beta2=<span class="number">0.9</span></div><div class="line">).minimize(gen_cost, var_list=gen_params)</div><div class="line">disc_train_op = tf.train.AdamOptimizer(</div><div class="line">	learning_rate=<span class="number">1e-4</span>, </div><div class="line">	beta1=<span class="number">0.5</span>, </div><div class="line">	beta2=<span class="number">0.9</span></div><div class="line">).minimize(disc_cost, var_list=disc_params)</div></pre></td></tr></table></figure><p>对于我这样的PyTorch党就非常不幸了，高阶梯度的功能还在开发，感兴趣的PyTorch党可以订阅这个GitHub的pull request：<a href="https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/pull/1016" target="_blank" rel="external">Autograd refactor</a>，如果它被merged了话就可以在最新版中使用高阶梯度的功能实现gradient penalty了。</p><p>但是除了等待我们就没有别的办法了吗？<strong>其实可能是有的，我想到了一种近似方法来实现gradient penalty，只需要把微分换成差分：</strong><br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x_1 \sim \hat{x}, x_2 \sim \hat{x}} [ \frac{|D(x_1) - D(x_2)|}{| x_1 - x_2 |_p} - 1]^2<br>$$<br>也就是说，我们仍然是在分布 $P_{\hat{x}}$ 上随机采样，但是一次采两个，然后要求它们的连线斜率要接近1，这样理论上也可以起到跟公式9一样的效果，我自己在MNIST+MLP上简单验证过有作用，PyTorch党甚至Tensorflow党都可以尝试用一下。</p><hr><p><strong>作者：郑华滨链接：<a href="https://www.zhihu.com/question/52602529/answer/158727900" target="_blank" rel="external">https://www.zhihu.com/question/52602529/answer/158727900</a></strong></p><p><strong>来源：知乎</strong></p><p><strong>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://ewanlee.github.io/2017/04/29/The-awesome-Wasserstein-GAN/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;WGAN&lt;/a&gt;存在着训练困难、收敛速度慢等问题。&lt;a hr
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="WGAN" scheme="http://yoursite.com/tags/WGAN/"/>
    
  </entry>
  
  <entry>
    <title>Policy Gradient Methods</title>
    <link href="http://yoursite.com/2017/07/10/Policy-Gradient-Methods/"/>
    <id>http://yoursite.com/2017/07/10/Policy-Gradient-Methods/</id>
    <published>2017-07-10T09:23:16.000Z</published>
    <updated>2017-07-12T04:17:20.106Z</updated>
    
    <content type="html"><![CDATA[<p>Until now almost all the methods have learned the values of actions and then selected actions based on their estimated action values; their policies would not even exist without the action-value estimates. In this post we consider methods that instead learn a <em>parameterized policy</em> that can select actions without consulting a value function. A value function may still be used to <em>learn</em> the policy parameter, but is not required for action selection. We use the notation $\boldsymbol{\theta} \in \mathbb{R}^d$ for the policy’s parameter vector. Thus we write $\pi(a|s, \boldsymbol{\theta}) = \text{Pr}(A_t=a | S_t=s, \boldsymbol{\theta}_t=\boldsymbol{\theta})$ for the probability that action $a$ is taken at time $t$ given that the agent is in state $s$ at time $t$ with parameter $\boldsymbol{\theta}$. If a method uses a learned value function as well, then the value function’s weight vector is denoted $\mathbf{w} \in \mathbb{R}^m$, as in $\hat{v}(s, \mathbf{w})$.</p><p>In this chapter we consider methods for learning the policy parameter based on the gradient of some performance measure $J(\boldsymbol{\theta})$ with respect to the policy parameter. These methods seek to maximize performance, so their updates approximate gradient ascent in $J$ :<br>$$<br>\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \alpha \widehat{\nabla J(\boldsymbol{\theta}_t)}.<br>$$<br>All methods that follow this general schema we call <strong>policy gradient methods</strong>, whether or not they also learn an approximate value function. Methods that learn approximations to both policy and value functions are often called actor–critic methods, where ‘actor’ is a reference to the learned policy, and ‘critic’ refers to the learned value function, usually a state-value function.</p><h3 id="Policy-Approximation"><a href="#Policy-Approximation" class="headerlink" title="Policy Approximation"></a>Policy Approximation</h3><p>The most preferred actions in each state are given the highest probability of being selected, for example, according to an exponential softmax distribution:<br>$$<br>\pi(a|s, \boldsymbol{\theta}) = \frac{\exp(h(s, a, \boldsymbol{\theta}))}{\sum_b \exp(h(s, b, \boldsymbol{\theta}))}.<br>$$<br>For example, they might be computed by a deep neural network, where $\boldsymbol{\theta}$ is the vector of all the connection weights of the network. Or the preferences could simply be linear in features,<br>$$<br>h(s, a, \boldsymbol{\theta}) = \boldsymbol{\theta}^{\top} \mathbf{x}(s, a).<br>$$</p><h3 id="The-Policy-Gradient-Theorem"><a href="#The-Policy-Gradient-Theorem" class="headerlink" title="The Policy Gradient Theorem"></a>The Policy Gradient Theorem</h3><p>We deﬁne the performance measure as the value of the start state of the episode. We can simplify the notation without losing any meaningful generality by assuming that every episode starts in some particular (non-random) state s0. Then, in the episodic case we deﬁne performance as<br>$$<br>J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}}(s_0),<br>$$<br>where $ v_{\pi_{\boldsymbol{\theta}}}$ is the true value function for $\pi_{\boldsymbol{\theta}}$, the policy determined by $\boldsymbol{\theta}$.</p><p>The policy gradient theorem is that<br>$$<br>\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}),<br>$$<br>where $\mu_{\pi}(s)$ we mentioned in <a href="https://ewanlee.github.io/2017/07/05/On-policy-Prediction-with-Approximation/" target="_blank" rel="external">earlier</a>.</p><h3 id="REINFORCE-Monte-Carlo-Policy-Gradient"><a href="#REINFORCE-Monte-Carlo-Policy-Gradient" class="headerlink" title="REINFORCE: Monte Carlo Policy Gradient"></a>REINFORCE: Monte Carlo Policy Gradient</h3><p>$$<br>\begin{align}<br>\nabla J(\boldsymbol{\theta}) &amp;= \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}) \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a q_{\pi}(S_t, a) \nabla_{\boldsymbol{\theta}} \pi(a | S_t, \boldsymbol{\theta}) \Bigg] \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a \pi(a|S_t, \boldsymbol{\theta}) q_{\pi}(S_t, a) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})} \Bigg] \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t q_{\pi}(S_t, A_t) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \text{(replacing a by the sample } A_t \sim \pi \;) \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \;\;\; \;\;\; \;\;\; \;\; (\text{because } \mathbb{E}_{\pi}[G_t|S_t,A_t]=q_{\pi}(S_t,A_t)).<br>\end{align}<br>$$</p><p>So we get<br>$$<br>\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.<br>$$<br>This is shown explicitly in the boxed pseudocode below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/pgm/reinforce.png" alt="reinforce"></p><p>Notice that $\nabla \log x = \frac{\nabla x}{x}$.</p><h3 id="REINFORCE-with-Baseline"><a href="#REINFORCE-with-Baseline" class="headerlink" title="REINFORCE with Baseline"></a>REINFORCE with Baseline</h3><p>The policy gradient theorem can be generalized to include a comparison of the action value to an arbitrary <strong>baseline</strong> $b(s)$:</p><p>$$<br>\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a \big(q_{\pi}(s, a) - b(s)\big) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}).<br>$$<br>The baseline can be any function, even a random variable, as long as it does not vary with $a$; the equation remains true, because the subtracted quantity is zero:<br>$$<br>\sum_a b(s) \nabla_{\boldsymbol{\theta}} \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} \sum_a \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} 1 = 0 \;\;\;\; \forall s \in \mathcal{S}.<br>$$<br>The update rule that we end up with is a new version of REINFORCE that includes a general baseline:<br>$$<br>\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \big(G_t-b(S_t) \big) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.<br>$$<br>One natural choice for the baseline is an estimate of the state value, $\hat{v}(S_t, \mathbf{w})$, where $\mathbf{w} \in \mathbb{R}^m$ is a weight vector learned by one of the methods presented in previous posts. A complete pseudocode algorithm for REINFROCE with baseline is given in the box (use Monte Carlo method for learning the policy parameter and state-value weights).</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/pgm/reinforce_baseline.png" alt="reinforce_baseline"></p><h3 id="Actor-Critic-Methods"><a href="#Actor-Critic-Methods" class="headerlink" title="Actor-Critic Methods"></a>Actor-Critic Methods</h3><p>Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor-critic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating a state from the estimated values of subsequent states), but only as a baseline for the state being updated. In<br>order to gain these advantages in the case of policy gradient methods we use actor-critic methods with a true bootstrapping critic.</p><p>One-step actor-critic methods replace the full return of REINFORCE with the one-step return (and use a learned state-value function as the baseline) as follow:<br>$$<br>\begin{align}<br>\boldsymbol{\theta}_{t+1} &amp;\doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( G_{t:t+1} - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\<br>&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\<br>&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \delta_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})}.<br>\end{align}<br>$$<br>The natural state-value-function learning method to pair with this is semi-gradient TD(0). Pseudocode for the complete algorithm is given in the box below. Note that it is now a fully online, incremental algorithm, with states, actions, and rewards processed as they occur and then never revisited.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/pgm/one-step-ac.png" alt="one-step-ac"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Until now almost all the methods have learned the values of actions and then selected actions based on their estimated action values; the
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout</title>
    <link href="http://yoursite.com/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/"/>
    <id>http://yoursite.com/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/</id>
    <published>2017-07-09T03:34:18.000Z</published>
    <updated>2017-07-09T04:38:52.104Z</updated>
    
    <content type="html"><![CDATA[<p>In previous <a href="https://ewanlee.github.io/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" target="_blank" rel="external">blog</a>, we use the Keras to play the FlappyBird. Similarity, we will use another deep learning toolkit Tensorflow to develop the DQN and Double DQN and to play the another game Breakout (Atari 3600).</p><p>Here, we will use the OpenAI gym toolkit to construct out environment. Detail implementation is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env = gym.envs.make(<span class="string">"Breakout-v0"</span>)</div></pre></td></tr></table></figure><p>And then we look some demos:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"Action space size: &#123;&#125;"</span>.format(env.action_space.n))</div><div class="line"><span class="comment"># print(env.get_action_meanings())</span></div><div class="line"></div><div class="line">observation = env.reset()</div><div class="line">print(<span class="string">"Observation space shape: &#123;&#125;"</span>.format(observation.shape))</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">[env.step(<span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>)]</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">env.render(close=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/games/breakout-env.png" alt="breakout-env"></p><p>For deep learning purpose, we need to crop the image to a square image:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Check out what a cropped image looks like</span></div><div class="line">plt.imshow(observation[<span class="number">34</span>:<span class="number">-16</span>,:,:])</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/games/croped-breakout-image.png" alt="croped-breakout-image"></p><p>Not bad !</p><p>Ok, now let us to use the Tensorflow to develop the DQN algorithm first.</p><p>First of all, we need to reference some packages and initialize the environment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"></div><div class="line"><span class="keyword">import</span> gym</div><div class="line"><span class="keyword">from</span> gym.wrappers <span class="keyword">import</span> Monitor</div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">if</span> <span class="string">"../"</span> <span class="keyword">not</span> <span class="keyword">in</span> sys.path:</div><div class="line">  sys.path.append(<span class="string">"../"</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> lib <span class="keyword">import</span> plotting</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque, namedtuple</div><div class="line"></div><div class="line">env = gym.envs.make(<span class="string">"Breakout-v0"</span>)</div><div class="line"><span class="comment"># Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions</span></div><div class="line">VALID_ACTIONS = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div></pre></td></tr></table></figure><p>As mentioned above, we need to crop the image and preprocess the input before feed the raw image into the algorithm. So we define a <strong>StateProcessor</strong> class to do this.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StateProcessor</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Processes a raw Atari images. Resizes it and converts it to grayscale.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># Build the Tensorflow graph</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"state_processor"</span>):</div><div class="line">            self.input_state = tf.placeholder(shape=[<span class="number">210</span>, <span class="number">160</span>, <span class="number">3</span>], dtype=tf.uint8)</div><div class="line">            self.output = tf.image.rgb_to_grayscale(self.input_state)</div><div class="line">            self.output = tf.image.crop_to_bounding_box(self.output, <span class="number">34</span>, <span class="number">0</span>, <span class="number">160</span>, <span class="number">160</span>)</div><div class="line">            self.output = tf.image.resize_images(</div><div class="line">                self.output, [<span class="number">84</span>, <span class="number">84</span>], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</div><div class="line">            self.output = tf.squeeze(self.output)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, sess, state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Args:</div><div class="line">            sess: A Tensorflow session object</div><div class="line">            state: A [210, 160, 3] Atari RGB State</div><div class="line"></div><div class="line">        Returns:</div><div class="line">            A processed [84, 84, 1] state representing grayscale values.</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> sess.run(self.output, &#123; self.input_state: state &#125;)</div></pre></td></tr></table></figure><p>We first convert the image to gray image and then resize it to 84 by 84 (the size DQN paper used). Then, we construct the neural network to estimate the value function. The structure of the network as the same as the DQN paper.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Estimator</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""Q-Value Estimator neural network.</span></div><div class="line"></div><div class="line">    This network is used for both the Q-Network and the Target Network.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, scope=<span class="string">"estimator"</span>, summaries_dir=None)</span>:</span></div><div class="line">        self.scope = scope</div><div class="line">        <span class="comment"># Writes Tensorboard summaries to disk</span></div><div class="line">        self.summary_writer = <span class="keyword">None</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</div><div class="line">            <span class="comment"># Build the graph</span></div><div class="line">            self._build_model()</div><div class="line">            <span class="keyword">if</span> summaries_dir:</div><div class="line">                summary_dir = os.path.join(summaries_dir, <span class="string">"summaries_&#123;&#125;"</span>.format(scope))</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(summary_dir):</div><div class="line">                    os.makedirs(summary_dir)</div><div class="line">                self.summary_writer = tf.summary.FileWriter(summary_dir)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_model</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Builds the Tensorflow graph.</div><div class="line">        """</div><div class="line"></div><div class="line">        <span class="comment"># Placeholders for our input</span></div><div class="line">        <span class="comment"># Our input are 4 RGB frames of shape 160, 160 each</span></div><div class="line">        self.X_pl = tf.placeholder(shape=[<span class="keyword">None</span>, <span class="number">84</span>, <span class="number">84</span>, <span class="number">4</span>], dtype=tf.uint8, name=<span class="string">"X"</span>)</div><div class="line">        <span class="comment"># The TD target value</span></div><div class="line">        self.y_pl = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.float32, name=<span class="string">"y"</span>)</div><div class="line">        <span class="comment"># Integer id of which action was selected</span></div><div class="line">        self.actions_pl = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.int32, name=<span class="string">"actions"</span>)</div><div class="line"></div><div class="line">        X = tf.to_float(self.X_pl) / <span class="number">255.0</span></div><div class="line">        batch_size = tf.shape(self.X_pl)[<span class="number">0</span>]</div><div class="line"></div><div class="line">        <span class="comment"># Three convolutional layers</span></div><div class="line">        conv1 = tf.contrib.layers.conv2d(</div><div class="line">            X, <span class="number">32</span>, <span class="number">8</span>, <span class="number">4</span>, activation_fn=tf.nn.relu)</div><div class="line">        conv2 = tf.contrib.layers.conv2d(</div><div class="line">            conv1, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>, activation_fn=tf.nn.relu)</div><div class="line">        conv3 = tf.contrib.layers.conv2d(</div><div class="line">            conv2, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, activation_fn=tf.nn.relu)</div><div class="line"></div><div class="line">        <span class="comment"># Fully connected layers</span></div><div class="line">        flattened = tf.contrib.layers.flatten(conv3)</div><div class="line">        fc1 = tf.contrib.layers.fully_connected(flattened, <span class="number">512</span>)</div><div class="line">        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))</div><div class="line"></div><div class="line">        <span class="comment"># Get the predictions for the chosen actions only</span></div><div class="line">        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[<span class="number">1</span>] + self.actions_pl</div><div class="line">        self.action_predictions = tf.gather(tf.reshape(self.predictions, [<span class="number">-1</span>]), gather_indices)</div><div class="line"></div><div class="line">        <span class="comment"># Calcualte the loss</span></div><div class="line">        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)</div><div class="line">        self.loss = tf.reduce_mean(self.losses)</div><div class="line"></div><div class="line">        <span class="comment"># Optimizer Parameters from original paper</span></div><div class="line">        self.optimizer = tf.train.RMSPropOptimizer(<span class="number">0.00025</span>, <span class="number">0.99</span>, <span class="number">0.0</span>, <span class="number">1e-6</span>)</div><div class="line">        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())</div><div class="line"></div><div class="line">        <span class="comment"># Summaries for Tensorboard</span></div><div class="line">        self.summaries = tf.summary.merge([</div><div class="line">            tf.summary.scalar(<span class="string">"loss"</span>, self.loss),</div><div class="line">            tf.summary.histogram(<span class="string">"loss_hist"</span>, self.losses),</div><div class="line">            tf.summary.histogram(<span class="string">"q_values_hist"</span>, self.predictions),</div><div class="line">            tf.summary.scalar(<span class="string">"max_q_value"</span>, tf.reduce_max(self.predictions))</div><div class="line">        ])</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sess, s)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Predicts action values.</div><div class="line"></div><div class="line">        Args:</div><div class="line">          sess: Tensorflow session</div><div class="line">          s: State input of shape [batch_size, 4, 160, 160, 3]</div><div class="line"></div><div class="line">        Returns:</div><div class="line">          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated </div><div class="line">          action values.</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> sess.run(self.predictions, &#123; self.X_pl: s &#125;)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, sess, s, a, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Updates the estimator towards the given targets.</div><div class="line"></div><div class="line">        Args:</div><div class="line">          sess: Tensorflow session object</div><div class="line">          s: State input of shape [batch_size, 4, 160, 160, 3]</div><div class="line">          a: Chosen actions of shape [batch_size]</div><div class="line">          y: Targets of shape [batch_size]</div><div class="line"></div><div class="line">        Returns:</div><div class="line">          The calculated loss on the batch.</div><div class="line">        """</div><div class="line">        feed_dict = &#123; self.X_pl: s, self.y_pl: y, self.actions_pl: a &#125;</div><div class="line">        summaries, global_step, _, loss = sess.run(</div><div class="line">            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],</div><div class="line">            feed_dict)</div><div class="line">        <span class="keyword">if</span> self.summary_writer:</div><div class="line">            self.summary_writer.add_summary(summaries, global_step)</div><div class="line">        <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>As mentioned in DQN paper, there are two network that share the same parameters in DQN algorithm. We need to copy the parameters to the target network on each $t$ steps.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">copy_model_parameters</span><span class="params">(sess, estimator1, estimator2)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Copies the model parameters of one estimator to another.</div><div class="line"></div><div class="line">    Args:</div><div class="line">      sess: Tensorflow session instance</div><div class="line">      estimator1: Estimator to copy the paramters from</div><div class="line">      estimator2: Estimator to copy the parameters to</div><div class="line">    """</div><div class="line">    e1_params = [t <span class="keyword">for</span> t <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> t.name.startswith(estimator1.scope)]</div><div class="line">    e1_params = sorted(e1_params, key=<span class="keyword">lambda</span> v: v.name)</div><div class="line">    e2_params = [t <span class="keyword">for</span> t <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> t.name.startswith(estimator2.scope)]</div><div class="line">    e2_params = sorted(e2_params, key=<span class="keyword">lambda</span> v: v.name)</div><div class="line"></div><div class="line">    update_ops = []</div><div class="line">    <span class="keyword">for</span> e1_v, e2_v <span class="keyword">in</span> zip(e1_params, e2_params):</div><div class="line">        op = e2_v.assign(e1_v)</div><div class="line">        update_ops.append(op)</div><div class="line"></div><div class="line">    sess.run(update_ops)</div></pre></td></tr></table></figure><p>We also need a policy to take an action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(estimator, nA)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.</div><div class="line"></div><div class="line">    Args:</div><div class="line">        estimator: An estimator that returns q values for a given state</div><div class="line">        nA: Number of actions in the environment.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        A function that takes the (sess, observation, epsilon) as an argument and returns</div><div class="line">        the probabilities for each action in the form of a numpy array of length nA.</div><div class="line"></div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(sess, observation, epsilon)</span>:</span></div><div class="line">        A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">        q_values = estimator.predict(sess, np.expand_dims(observation, <span class="number">0</span>))[<span class="number">0</span>]</div><div class="line">        best_action = np.argmax(q_values)</div><div class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">        <span class="keyword">return</span> A</div><div class="line">    <span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Now let us to develop the DQN algorithm (we skip the details here because we explained it earlier).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">deep_q_learning</span><span class="params">(sess,</span></span></div><div class="line">                    env,</div><div class="line">                    q_estimator,</div><div class="line">                    target_estimator,</div><div class="line">                    state_processor,</div><div class="line">                    num_episodes,</div><div class="line">                    experiment_dir,</div><div class="line">                    replay_memory_size=<span class="number">500000</span>,</div><div class="line">                    replay_memory_init_size=<span class="number">50000</span>,</div><div class="line">                    update_target_estimator_every=<span class="number">10000</span>,</div><div class="line">                    discount_factor=<span class="number">0.99</span>,</div><div class="line">                    epsilon_start=<span class="number">1.0</span>,</div><div class="line">                    epsilon_end=<span class="number">0.1</span>,</div><div class="line">                    epsilon_decay_steps=<span class="number">500000</span>,</div><div class="line">                    batch_size=<span class="number">32</span>,</div><div class="line">                    record_video_every=<span class="number">50</span>):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm for fff-policy TD control using Function Approximation.</div><div class="line">    Finds the optimal greedy policy while following an epsilon-greedy policy.</div><div class="line"></div><div class="line">    Args:</div><div class="line">        sess: Tensorflow Session object</div><div class="line">        env: OpenAI environment</div><div class="line">        q_estimator: Estimator object used for the q values</div><div class="line">        target_estimator: Estimator object used for the targets</div><div class="line">        state_processor: A StateProcessor object</div><div class="line">        num_episodes: Number of episodes to run for</div><div class="line">        experiment_dir: Directory to save Tensorflow summaries in</div><div class="line">        replay_memory_size: Size of the replay memory</div><div class="line">        replay_memory_init_size: Number of random experiences to sampel when initializing </div><div class="line">          the reply memory.</div><div class="line">        update_target_estimator_every: Copy parameters from the Q estimator to the </div><div class="line">          target estimator every N steps</div><div class="line">        discount_factor: Lambda time discount factor</div><div class="line">        epsilon_start: Chance to sample a random action when taking an action.</div><div class="line">          Epsilon is decayed over time and this is the start value</div><div class="line">        epsilon_end: The final minimum value of epsilon after decaying is done</div><div class="line">        epsilon_decay_steps: Number of steps to decay epsilon over</div><div class="line">        batch_size: Size of batches to sample from the replay memory</div><div class="line">        record_video_every: Record a video every N episodes</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line"></div><div class="line">    Transition = namedtuple(<span class="string">"Transition"</span>, [<span class="string">"state"</span>, <span class="string">"action"</span>, <span class="string">"reward"</span>, <span class="string">"next_state"</span>, <span class="string">"done"</span>])</div><div class="line"></div><div class="line">    <span class="comment"># The replay memory</span></div><div class="line">    replay_memory = []</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))</div><div class="line"></div><div class="line">    <span class="comment"># Create directories for checkpoints and summaries</span></div><div class="line">    checkpoint_dir = os.path.join(experiment_dir, <span class="string">"checkpoints"</span>)</div><div class="line">    checkpoint_path = os.path.join(checkpoint_dir, <span class="string">"model"</span>)</div><div class="line">    monitor_path = os.path.join(experiment_dir, <span class="string">"monitor"</span>)</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):</div><div class="line">        os.makedirs(checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(monitor_path):</div><div class="line">        os.makedirs(monitor_path)</div><div class="line"></div><div class="line">    saver = tf.train.Saver()</div><div class="line">    <span class="comment"># Load a previous checkpoint if we find one</span></div><div class="line">    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> latest_checkpoint:</div><div class="line">        print(<span class="string">"Loading model checkpoint &#123;&#125;...\n"</span>.format(latest_checkpoint))</div><div class="line">        saver.restore(sess, latest_checkpoint)</div><div class="line">    </div><div class="line">    <span class="comment"># Get the current time step</span></div><div class="line">    total_t = sess.run(tf.contrib.framework.get_global_step())</div><div class="line"></div><div class="line">    <span class="comment"># The epsilon decay schedule</span></div><div class="line">    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)</div><div class="line"></div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(</div><div class="line">        q_estimator,</div><div class="line">        len(VALID_ACTIONS))</div><div class="line"></div><div class="line">    <span class="comment"># Populate the replay memory with initial experience</span></div><div class="line">    print(<span class="string">"Populating replay memory..."</span>)</div><div class="line">    state = env.reset()</div><div class="line">    state = state_processor.process(sess, state)</div><div class="line">    state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(replay_memory_init_size):</div><div class="line">        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps<span class="number">-1</span>)])</div><div class="line">        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])</div><div class="line">        next_state = state_processor.process(sess, next_state)</div><div class="line">        next_state = np.append(state[:,:,<span class="number">1</span>:], np.expand_dims(next_state, <span class="number">2</span>), axis=<span class="number">2</span>)</div><div class="line">        replay_memory.append(Transition(state, action, reward, next_state, done))</div><div class="line">        <span class="keyword">if</span> done:</div><div class="line">            state = env.reset()</div><div class="line">            state = state_processor.process(sess, state)</div><div class="line">            state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state = next_state</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment"># Record videos</span></div><div class="line">    <span class="comment"># Add env Monitor wrapper</span></div><div class="line">    env = Monitor(env, directory=monitor_path, video_callable=<span class="keyword">lambda</span> count: count % record_video_every == <span class="number">0</span>, resume=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line"></div><div class="line">        <span class="comment"># Save the current checkpoint</span></div><div class="line">        saver.save(tf.get_default_session(), checkpoint_path)</div><div class="line"></div><div class="line">        <span class="comment"># Reset the environment</span></div><div class="line">        state = env.reset()</div><div class="line">        state = state_processor.process(sess, state)</div><div class="line">        state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">        loss = <span class="keyword">None</span></div><div class="line"></div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line"></div><div class="line">            <span class="comment"># Epsilon for this time step</span></div><div class="line">            epsilon = epsilons[min(total_t, epsilon_decay_steps<span class="number">-1</span>)]</div><div class="line"></div><div class="line">            <span class="comment"># Add epsilon to Tensorboard</span></div><div class="line">            episode_summary = tf.Summary()</div><div class="line">            episode_summary.value.add(simple_value=epsilon, tag=<span class="string">"epsilon"</span>)</div><div class="line">            q_estimator.summary_writer.add_summary(episode_summary, total_t)</div><div class="line"></div><div class="line">            <span class="comment"># Maybe update the target estimator</span></div><div class="line">            <span class="keyword">if</span> total_t % update_target_estimator_every == <span class="number">0</span>:</div><div class="line">                copy_model_parameters(sess, q_estimator, target_estimator)</div><div class="line">                print(<span class="string">"\nCopied model parameters to target network."</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Print out which step we're on, useful for debugging.</span></div><div class="line">            print(<span class="string">"\rStep &#123;&#125; (&#123;&#125;) @ Episode &#123;&#125;/&#123;&#125;, loss: &#123;&#125;"</span>.format(</div><div class="line">                    t, total_t, i_episode + <span class="number">1</span>, num_episodes, loss), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line"></div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            action_probs = policy(sess, state, epsilon)</div><div class="line">            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])</div><div class="line">            next_state = state_processor.process(sess, next_state)</div><div class="line">            next_state = np.append(state[:,:,<span class="number">1</span>:], np.expand_dims(next_state, <span class="number">2</span>), axis=<span class="number">2</span>)</div><div class="line"></div><div class="line">            <span class="comment"># If our replay memory is full, pop the first element</span></div><div class="line">            <span class="keyword">if</span> len(replay_memory) == replay_memory_size:</div><div class="line">                replay_memory.pop(<span class="number">0</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Save transition to replay memory</span></div><div class="line">            replay_memory.append(Transition(state, action, reward, next_state, done))   </div><div class="line"></div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line"></div><div class="line">            <span class="comment"># Sample a minibatch from the replay memory</span></div><div class="line">            samples = random.sample(replay_memory, batch_size)</div><div class="line">            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))</div><div class="line"></div><div class="line">            <span class="comment"># Calculate q values and targets</span></div><div class="line">            q_values_next = target_estimator.predict(sess, next_states_batch)</div><div class="line">            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Perform gradient descent update</span></div><div class="line">            states_batch = np.array(states_batch)</div><div class="line">            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line"></div><div class="line">            state = next_state</div><div class="line">            total_t += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="comment"># Add summaries to tensorboard</span></div><div class="line">        episode_summary = tf.Summary()</div><div class="line">        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=<span class="string">"episode_reward"</span>, tag=<span class="string">"episode_reward"</span>)</div><div class="line">        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=<span class="string">"episode_length"</span>, tag=<span class="string">"episode_length"</span>)</div><div class="line">        q_estimator.summary_writer.add_summary(episode_summary, total_t)</div><div class="line">        q_estimator.summary_writer.flush()</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> total_t, plotting.EpisodeStats(</div><div class="line">            episode_lengths=stats.episode_lengths[:i_episode+<span class="number">1</span>],</div><div class="line">            episode_rewards=stats.episode_rewards[:i_episode+<span class="number">1</span>])</div><div class="line"></div><div class="line">    <span class="keyword">return</span> stats</div></pre></td></tr></table></figure><p>Finally, run it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">tf.reset_default_graph()</div><div class="line"></div><div class="line"><span class="comment"># Where we save our checkpoints and graphs</span></div><div class="line">experiment_dir = os.path.abspath(<span class="string">"./experiments/&#123;&#125;"</span>.format(env.spec.id))</div><div class="line"></div><div class="line"><span class="comment"># Create a glboal step variable</span></div><div class="line">global_step = tf.Variable(<span class="number">0</span>, name=<span class="string">'global_step'</span>, trainable=<span class="keyword">False</span>)</div><div class="line">    </div><div class="line"><span class="comment"># Create estimators</span></div><div class="line">q_estimator = Estimator(scope=<span class="string">"q"</span>, summaries_dir=experiment_dir)</div><div class="line">target_estimator = Estimator(scope=<span class="string">"target_q"</span>)</div><div class="line"></div><div class="line"><span class="comment"># State processor</span></div><div class="line">state_processor = StateProcessor()</div><div class="line"></div><div class="line"><span class="comment"># Run it!</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    <span class="keyword">for</span> t, stats <span class="keyword">in</span> deep_q_learning(sess,</div><div class="line">                                    env,</div><div class="line">                                    q_estimator=q_estimator,</div><div class="line">                                    target_estimator=target_estimator,</div><div class="line">                                    state_processor=state_processor,</div><div class="line">                                    experiment_dir=experiment_dir,</div><div class="line">                                    num_episodes=<span class="number">10000</span>,</div><div class="line">                                    replay_memory_size=<span class="number">500000</span>,</div><div class="line">                                    replay_memory_init_size=<span class="number">50000</span>,</div><div class="line">                                    update_target_estimator_every=<span class="number">10000</span>,</div><div class="line">                                    epsilon_start=<span class="number">1.0</span>,</div><div class="line">                                    epsilon_end=<span class="number">0.1</span>,</div><div class="line">                                    epsilon_decay_steps=<span class="number">500000</span>,</div><div class="line">                                    discount_factor=<span class="number">0.99</span>,</div><div class="line">                                    batch_size=<span class="number">32</span>):</div><div class="line"></div><div class="line">        print(<span class="string">"\nEpisode Reward: &#123;&#125;"</span>.format(stats.episode_rewards[<span class="number">-1</span>]))</div></pre></td></tr></table></figure><hr><p>Next, we will develop the Double-DQN algorithm. This algorithm only has a little changes.</p><p>In DQN <strong>q_learning</strong> method,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate q values and targets</span></div><div class="line">q_values_next = target_estimator.predict(sess, next_states_batch)</div><div class="line">targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p>we just change these codes to,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate q values and targets</span></div><div class="line"><span class="comment"># This is where Double Q-Learning comes in!</span></div><div class="line">q_values_next = q_estimator.predict(sess, next_states_batch)</div><div class="line">best_actions = np.argmax(q_values_next, axis=<span class="number">1</span>)</div><div class="line">q_values_next_target = target_estimator.predict(sess, next_states_batch)</div><div class="line">targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * q_values_next_target[np.arange(batch_size), best_actions]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In previous &lt;a href=&quot;https://ewanlee.github.io/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/&quot; target=&quot;_blank&quot; rel=
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="deep reinforcement learning" scheme="http://yoursite.com/tags/deep-reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Summary of Papers</title>
    <link href="http://yoursite.com/2017/07/08/Summary-of-the-papers/"/>
    <id>http://yoursite.com/2017/07/08/Summary-of-the-papers/</id>
    <published>2017-07-08T09:49:20.000Z</published>
    <updated>2017-11-20T05:22:55.042Z</updated>
    
    <content type="html"><![CDATA[<h3 id="CheXNet-Radiologist-Level-Pneumonia-Detection-on-Chest-X-Rays-with-Deep-Learning"><a href="#CheXNet-Radiologist-Level-Pneumonia-Detection-on-Chest-X-Rays-with-Deep-Learning" class="headerlink" title="CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning"></a>CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</h3><h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><ul><li>ChestX-ray14 dataset <a href="https://arxiv.org/abs/1705.02315" target="_blank" rel="external">Wang et al. 2017</a></li></ul><h4 id="Arch"><a href="#Arch" class="headerlink" title="Arch"></a>Arch</h4><ul><li><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">DenseNet</a> (121 layers)</li><li>Batch Normalization</li><li>The weights of the network are randomly initialized</li><li>Trained end-to-end using Adam with standard pa- rameters (β1 = 0.9 and β2 = 0.999)</li><li>Batch size = 16</li><li>Oversample the minority (positive) class <a href="https://arxiv.org/abs/1710.05381" target="_blank" rel="external">Buda et al., 2017</a></li><li>Use an initial learning rate of 0.01 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss.</li></ul><h4 id="Model-Interpretation"><a href="#Model-Interpretation" class="headerlink" title="Model Interpretation"></a>Model Interpretation</h4><p>To interpret the network predictions, we also produce heatmaps to visualize the areas of the image most in- dicative of the disease using class activation mappings (CAMs) <a href="http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf" target="_blank" rel="external">Zhou et al., 2016</a></p><h4 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/chexnet-paper/chexnet-paper-1.png" alt="1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/chexnet-paper/chexnet-paper-2.png" alt="2"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/chexnet-paper/chexnet-paper-3.png" alt="3"></p><h4 id="Related-Word"><a href="#Related-Word" class="headerlink" title="Related Word"></a>Related Word</h4><ul><li>Diabetic retinopathy detection <a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45732.pdf" target="_blank" rel="external">Gulshan et al., 2016</a></li><li>Skin cancer classification <a href="https://www.nature.com/articles/nature21056" target="_blank" rel="external">Esteva et al., 2017</a></li><li>Arrhythmia detection <a href="https://arxiv.org/pdf/1707.01836.pdf" target="_blank" rel="external">Rajpurkar et al., 2017</a></li><li>Hemorrhage identificatio <a href="https://pdfs.semanticscholar.org/8066/f31ae455b1180a6159e5a4f53d547c2a7263.pdf" target="_blank" rel="external">Grewal et al., 2017</a></li><li>Pulmonary tuberculosis classification <a href="http://pubs.rsna.org/doi/full/10.1148/radiol.2017162326" target="_blank" rel="external">Lakhani &amp; Sun- daram, 2017</a></li><li>Lung nodule detection <a href="http://pubs.rsna.org/doi/10.1148/radiol.2017162725?url_ver=Z39.88-2003&amp;rfr_id=ori%3Arid%3Acrossref.org&amp;rfr_dat=cr_pub%3Dpubmed&amp;" target="_blank" rel="external">Huang et al., 2017</a>, <a href="https://arxiv.org/pdf/1705.09850.pdf" target="_blank" rel="external">Islam et al. 2017</a><ul><li>studied the performance of various convolutional architectures on different ab- normalities using the publicly available OpenI dataset <a href="https://www.ncbi.nlm.nih.gov/pubmed/26133894" target="_blank" rel="external">Demner-Fushman et al., 2015</a></li></ul></li><li><a href="https://arxiv.org/pdf/1710.10501.pdf" target="_blank" rel="external">Yao et al. 2017</a> exploited statistical dependencies between la- bels in order make more accurate predictions, outper- forming <a href="https://arxiv.org/abs/1705.02315" target="_blank" rel="external">Wang et al. 2017</a> on 13 of 14 classes.</li></ul><hr><p>###Maximum Entropy Deep Inverse Reinforcement Learning [2016]</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/deep_irl_paper/deep_irl_1.png" alt="1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/deep_irl_paper/deep_irl_2.png" alt="1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/deep_irl_paper/deep_irl_3.png" alt="1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/deep_irl_paper/deep_irl_4.png" alt="1"></p><hr><h3 id="Maximum-Entropy-Inverse-Reinforcement-Learning-Brian-2008-Repost"><a href="#Maximum-Entropy-Inverse-Reinforcement-Learning-Brian-2008-Repost" class="headerlink" title="Maximum Entropy Inverse Reinforcement Learning Brian [2008][Repost]"></a>Maximum Entropy Inverse Reinforcement Learning Brian [2008][Repost]</h3><p>this is a summary of Ziebart et al’s 2008 paper: Maximum Entropy Inverse Reinforcement Learning <a href="https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf" target="_blank" rel="external">**</a>. I found this is a good way for me to distill the essence of the paper. Since the Maxent algorithm is mostly cited by the later papers in IRL/imitation learning, I would like to look into details of this algorithm. Code is available at <a href="https://github.com/stormmax/irl-imitation" target="_blank" rel="external">github</a>. Hope this post can also help others who are interested in IRL.</p><h4 id="Motivations"><a href="#Motivations" class="headerlink" title="Motivations"></a>Motivations</h4><p>The paper points out the limitations of the previous algorithms including LPIRL (Ng &amp; Russell 2000), structured maximum margin prediction (MMP, Ratliff, Bagnell &amp; Zinkevich 2006, Apprenticeship Learning vis IRL (Abbeel &amp; Ng 2004) about <strong>feature counts and IRL</strong></p><ul><li>Both IRL and the matching of feature counts are ambiguous.<ul><li>Each policy can be optimal for many reward functions.</li><li>many policies lead to the same feature counts.</li></ul></li><li>The ambiguity of suboptimality is unresolved.</li></ul><h4 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h4><p>An MDP is a tuple $(S,A,P_{sa},\gamma,R)$</p><ul><li>$S$ is a finite set of $N$ states.</li><li>$A={a_1,..,a_k}$ is a set of $k$ actions.</li><li>$P_{sa}(s^{\prime})$ is the state transition probability of landing at state $s^{\prime}$: $P(s,a,s^{\prime})$ upon taking the action aa at state $s$.</li><li>$\gamma \in [0,1)$ is the discount factor.</li><li>$R: S \rightarrow \mathbf{R}$ is the reward function.</li></ul><p>Maxent</p><ul><li>$\zeta:\{(s, a)\}$ is a trajectory.</li><li>$\mathbf{f_s} \in \mathbf{R}^k$ is the feature vector of the state $s$.</li><li>$\theta \in \mathbf{R}^k$ reward function parameters.</li><li>$P(\zeta)$ probability of the trajectory $\zeta$ to occur</li><li>$P(s)$ the probatility of visiting state $s$ (state visitation frequency), $P(\zeta) = \prod_{s\in\zeta} P(s)$.</li></ul><h4 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h4><ul><li>The reward of a trajectory is expressed as a linearly combination with feature counts</li></ul><p>$$<br>R(\zeta) = \theta ^T \mathbf{f}_{\zeta} = \sum_{s\in \zeta} \theta ^T \mathbf{f}_s<br>$$</p><ul><li>Principle of maximum entropy (Jaynes 1957): probability of a trajectory demonstrated by the expert is exponentially higher for higher rewards than lower rewards,</li></ul><p>$$<br>P(\zeta) \propto e^{R(\zeta)}<br>$$</p><h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><p>The Maxent algorithm learns from demonstrated expert trajectories with the objective being <strong>maximizing the likelihood</strong> of the demonstrated trajectories,<br>$$<br>\begin{align}<br>\theta^{\star} &amp;= \text{argmax}_{\theta} L(\theta) \\<br>&amp;= \text{argmax}_{\theta} \frac{1}{M}\text{log} P(\{\zeta\} | \theta)\\<br>&amp;= \text{argmax}_{\theta} \frac{1}{M}\text{log} \prod_{\zeta} P(\zeta|\theta)\\<br>&amp;= \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} \text{log} P(\zeta|\theta)\\<br>&amp;= \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} \text{log} \frac{e^{R(\zeta)}}{Z}\\<br>&amp;= \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} R(\zeta) - \text{log} Z\\<br>\end{align}<br>$$<br>Where $M$ is the number of trajectories, $Z$ is the normalization term,<br>$$<br>Z = \sum_{\zeta} e^{R(\zeta)}<br>$$<br>Then,<br>$$<br>\theta^{\star} = \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} R(\zeta) - \text{log} \sum_{\zeta} e^{R(\zeta)}\\<br>\theta^{\star} = \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} \theta ^T \mathbf{f}_{\zeta} - \text{log} \sum_{\zeta} e^{\theta ^T \mathbf{f}_{\zeta}}\\<br>$$<br>And the objective is convex! (with the second term being <a href="https://en.wikipedia.org/wiki/LogSumExp" target="_blank" rel="external">log-sum-exp</a>). We go ahead to differentiate the objective to find the gradients:<br>$$<br>\begin{align}<br>\nabla_{\theta} L &amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \frac{1}{\sum_\zeta e^{R(\zeta)}} \sum_{\zeta} (e^{R(\zeta)\frac{d R(\zeta)}{d\theta}})\\<br>&amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \frac{1}{\sum_\zeta e^{R(\zeta)}} \sum_{\zeta} (e^{R(\zeta)}\frac{d R(\zeta)<br>}{d\theta})\\<br>&amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \sum_{\zeta}\frac{e^{R(\zeta)}}{\sum_\zeta e^{R(\zeta)}} \mathbf{f}_{\zeta}\\<br>&amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \sum_{\zeta} P(\zeta | \theta) \mathbf{f}_{\zeta}\\<br>\end{align}<br>$$<br>Since the trajectories {ζ}{ζ} are consist of states,<br>$$<br>\nabla_{\theta} L = \frac{1}{M}\sum_s \mathbf{f}_{s} - \sum_{s} P(s | \theta) \mathbf{f}_{s}<br>$$<br>Where $\mathbf{f}_s$ is the feature vector for the state $s$. And<br>$$<br>P(s|\theta)<br>$$<br>is the state visitation frequency for state $s$.</p><p>So far the main body of the algorithm is described. The only thing left is to compute the state visitation frequency (SVF) vector. To do so, we can use the following dynamic programming algorithm (for convienience we use $P(s)$ to denote SVF on state $s$).</p><p>We use μt(s)μt(s) to denote the prob of visiting $s$ at $t$ (obviously,<br>$$<br>P(s) = \sum_t \mu_t(s)<br>$$</p><ul><li>solve the MDP using value iteration with the intermediate recovered rewards to get current optimal policy $\{\pi(a,s)\}$.</li><li>compute $\mu_1(s)$ using sampled trajectories</li><li>using DP to solve for the rest given optimal policy $\{\pi(a,s)\}$ and the transition dynamics $\{P_{sa}(s’)\}$</li></ul><p>For $t = 1,..,T$<br>$$<br>\mu_{t+1} (s) = \sum_{a}\sum_{s’} \mu_{t}(s’)\pi(a,s’)P_{sa}(s’)<br>$$</p><ul><li>And finally.</li></ul><p>$$<br>P(s) = \sum_t \mu_t(s)<br>$$</p><p>One key things to note is that, <strong>the algorithm solves MDP in each iteration</strong> of training.</p><p>If the transition dynamics $\{P_{sa}(s’)\}$ is unknown, we can actually using Monte Carlo to estimate the SVF with the trajectories. This is much more easier, so the details are omitted. Plugging the SVF back to the gradients, we can use iterative gradient descent to solve for the parameters $\theta$.</p><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>As a final summary of the algorithm, here is the slide from UC Berkeley’s <a href="https://www.youtube.com/watch?v=J2blDuU3X1I&amp;index=14&amp;list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX&amp;t=1954s" target="_blank" rel="external">CS 294, Deep Reinforcement Learning course</a>,</p><p><a href="http://178.79.149.207/assets/maxent/maxent_slide.jpg" target="_blank" rel="external"><img src="http://178.79.149.207/assets/maxent/maxent_slide.jpg" alt="Maxent IRL"></a></p><h4 id="Strengths-and-Limitations"><a href="#Strengths-and-Limitations" class="headerlink" title="Strengths and Limitations"></a>Strengths and Limitations</h4><ul><li>Strengths<ul><li>scales to neural network costs (overcome the drawbacks of linear costs)</li><li>efficient enough for real robots</li></ul></li><li>Limitations<ul><li>requires repeatedly solving the MDP</li><li>assumes known dynamics</li></ul></li></ul><h4 id="References"><a href="#References" class="headerlink" title="References"></a>References</h4><ul><li>Ziebart et al’s 2008 paper: Maximum Entropy Inverse Reinforcement Learning <a href="https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf" target="_blank" rel="external">**</a></li><li>UCB’s CS 294 DRL course, <a href="https://www.youtube.com/watch?v=J2blDuU3X1I&amp;index=14&amp;list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX&amp;t=1954s" target="_blank" rel="external">lecture on IRL</a></li></ul><h4 id="Code-Details"><a href="#Code-Details" class="headerlink" title="Code Details"></a>Code Details</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_state_visition_freq</span><span class="params">(P_a, gamma, trajs, policy, deterministic=True)</span>:</span></div><div class="line">  <span class="string">"""compute the expected states visition frequency p(s| theta, T) </span></div><div class="line">  using dynamic programming</div><div class="line">  inputs:</div><div class="line">    P_a     NxNxN_ACTIONS matrix - transition dynamics</div><div class="line">    gamma   float - discount factor</div><div class="line">    trajs   list of list of Steps - collected from expert</div><div class="line">    policy  Nx1 vector (or NxN_ACTIONS if deterministic=False) - policy</div><div class="line">  </div><div class="line">  returns:</div><div class="line">    p       Nx1 vector - state visitation frequencies</div><div class="line">  """</div><div class="line">  N_STATES, _, N_ACTIONS = np.shape(P_a)</div><div class="line"></div><div class="line">  T = len(trajs[<span class="number">0</span>])</div><div class="line">  <span class="comment"># mu[s, t] is the prob of visiting state s at time t</span></div><div class="line">  mu = np.zeros([N_STATES, T]) </div><div class="line"></div><div class="line">  <span class="keyword">for</span> traj <span class="keyword">in</span> trajs:</div><div class="line">    mu[traj[<span class="number">0</span>].cur_state, <span class="number">0</span>] += <span class="number">1</span></div><div class="line">  mu[:,<span class="number">0</span>] = mu[:,<span class="number">0</span>]/len(trajs)</div><div class="line"></div><div class="line">  <span class="keyword">for</span> s <span class="keyword">in</span> range(N_STATES):</div><div class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>):</div><div class="line">      <span class="keyword">if</span> deterministic:</div><div class="line">        mu[s, t+<span class="number">1</span>] = sum([mu[pre_s, t]*P_a[pre_s, s, int(policy[pre_s])] <span class="keyword">for</span> pre_s <span class="keyword">in</span> range(N_STATES)])</div><div class="line">      <span class="keyword">else</span>:</div><div class="line">        mu[s, t+<span class="number">1</span>] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] <span class="keyword">for</span> a1 <span class="keyword">in</span> range(N_ACTIONS)]) <span class="keyword">for</span> pre_s <span class="keyword">in</span> range(N_STATES)])</div><div class="line">  p = np.sum(mu, <span class="number">1</span>)</div><div class="line">  <span class="keyword">return</span> p</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">maxent_irl</span><span class="params">(feat_map, P_a, gamma, trajs, lr, n_iters)</span>:</span></div><div class="line">  <span class="string">"""</span></div><div class="line">  Maximum Entropy Inverse Reinforcement Learning (Maxent IRL)</div><div class="line">  inputs:</div><div class="line">    feat_map    NxD matrix - the features for each state</div><div class="line">    P_a         NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of </div><div class="line">                                       landing at state s1 when taking action </div><div class="line">                                       a at state s0</div><div class="line">    gamma       float - RL discount factor</div><div class="line">    trajs       a list of demonstrations</div><div class="line">    lr          float - learning rate</div><div class="line">    n_iters     int - number of optimization steps</div><div class="line">  returns</div><div class="line">    rewards     Nx1 vector - recoverred state rewards</div><div class="line">  """</div><div class="line">  N_STATES, _, N_ACTIONS = np.shape(P_a)</div><div class="line"></div><div class="line">  <span class="comment"># init parameters</span></div><div class="line">  theta = np.random.uniform(size=(feat_map.shape[<span class="number">1</span>],))</div><div class="line"></div><div class="line">  <span class="comment"># calc feature expectations</span></div><div class="line">  feat_exp = np.zeros([feat_map.shape[<span class="number">1</span>]])</div><div class="line">  <span class="keyword">for</span> episode <span class="keyword">in</span> trajs:</div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> episode:</div><div class="line">      feat_exp += feat_map[step.cur_state,:]</div><div class="line">  feat_exp = feat_exp/len(trajs)</div><div class="line"></div><div class="line">  <span class="comment"># training</span></div><div class="line">  <span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iters):</div><div class="line">  </div><div class="line">    <span class="keyword">if</span> iteration % (n_iters/<span class="number">20</span>) == <span class="number">0</span>:</div><div class="line">      <span class="keyword">print</span> <span class="string">'iteration: &#123;&#125;/&#123;&#125;'</span>.format(iteration, n_iters)</div><div class="line">    </div><div class="line">    <span class="comment"># compute reward function</span></div><div class="line">    rewards = np.dot(feat_map, theta)</div><div class="line"></div><div class="line">    <span class="comment"># compute policy</span></div><div class="line">    _, policy = value_iteration.value_iteration(P_a, rewards, gamma, error=<span class="number">0.01</span>, deterministic=<span class="keyword">False</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># compute state visition frequences</span></div><div class="line">    svf = compute_state_visition_freq(P_a, gamma, trajs, policy, deterministic=<span class="keyword">False</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># compute gradients</span></div><div class="line">    grad = feat_exp - feat_map.T.dot(svf)</div><div class="line"></div><div class="line">    <span class="comment"># update params</span></div><div class="line">    theta += lr * grad</div><div class="line"></div><div class="line">  rewards = np.dot(feat_map, theta)</div><div class="line">  <span class="comment"># return sigmoid(normalize(rewards))</span></div><div class="line">  <span class="keyword">return</span> normalize(rewards)</div></pre></td></tr></table></figure><hr><h3 id="Apprenticeship-Learning-via-Inverse-Reinforcement-Learning-ICML-2004"><a href="#Apprenticeship-Learning-via-Inverse-Reinforcement-Learning-ICML-2004" class="headerlink" title="Apprenticeship Learning via Inverse Reinforcement Learning [ICML 2004]"></a>Apprenticeship Learning via Inverse Reinforcement Learning [ICML 2004]</h3><h4 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h4><p>The reward function can be expressed as a linear combination of known features.</p><p>The value of a policy $\pi$:<br>$$<br>\begin{align}<br>E_{s_0 \sim D}[V^{\pi}(s_0)] &amp;= E[\sum_{t=0}^{\infty}\gamma^t R(s_t) | \pi] \\<br>&amp;= E[\sum_{t=0}^{\infty}\gamma^t w \cdot \phi(s_t)| \pi] \\<br>&amp;= w \cdot E[\sum_{t=0}^{\infty}\gamma^t \phi(s_t)| \pi],<br>\end{align}<br>$$<br>where vector of features $\phi: S \rightarrow [0, 1]^k$ or $\phi: S \times A \ \rightarrow [0, 1]^k$, $w \in \mathbb{R}^k$. In order to ensure that the rewards are bounded by 1, we also assume $| w |_1 \leq 1$.</p><p>The feature exceptions:<br>$$<br>\mu(\pi) = E[\sum_{t=0}^{\infty}\gamma^t \phi(s_t)| \pi] \in \mathbb{R}^k.<br>$$<br>So the value of a policy may be rewritten to<br>$$<br>E_{s_0 \sim D}[V^{\pi}(s_0)] = w \cdot \mu(\pi)<br>$$<br>We assume the expert policy is $\pi_E$ and we need to estimate the expert’s feature expectations $\mu_E = \mu(\pi_E)$. Specifically, given a set of $m$ trajectories $\{s_0^{(i)}, s_1^{(i)}, \cdots, \}_{i=1}^{m}$ generated by the expert, we denote the empirical estimate for $\mu_E$ by<br>$$<br>\hat{\mu}_E = \frac{1}{m}\sum_{i=1}^m\sum_{t=0}^m \gamma^t \phi(s_t^{(i)}).<br>$$<br>Furthermore, we could construct a new policy by linear combination some other policies. More specifically, we have<br>$$<br>\mu(\pi_3) = \lambda \mu(\pi_1) + (1 - \lambda) \mu(\pi_2) .<br>$$<br>Note that the randomization step selecting between $\pi_1$ and $\pi_2$ occurs only once at the start of a trajectory, and not on every step taken in the MDP.</p><h4 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm"></a>Algorithm</h4><p>We want to find a policy whose performance is close to that of the expert, that is, for any $w \in \mathbb{R}^k (|w|_1 \leq 1)$,<br>$$<br>|w^T\mu(\tilde{\pi}) - w^T\mu_E| \leq |w|_2 |w^T\mu(\tilde{\pi}) - w^T\mu_E|_2 \leq 1 \cdot \epsilon \leq \epsilon.<br>$$<br>So the problem is reduced to finding a policy $\tilde{\pi}$ that induces feature expectations $\mu(\tilde{\pi})$ close to $\mu_E$. We find such a policy as follows:</p><ol><li>Randomly pick some policy $\pi^{(0)}$. compute (or approximate via Monte Carlo) $\mu^{(0)} = \mu(\pi^{(0)})$, and set $i=1$.</li><li>Solve optimization problem (Solver as same as SVM) (*):</li><li>If $t^{(i)} \leq \epsilon$, then terminal.</li><li>Using the RL algorithm, compute the optimal policy $\pi^{(i)}$ for the MDP using reward $R = (w^{(i)})^T\phi$.</li><li>Compute (or estimate) $\mu^{(i)} = \mu(\pi^{(i)})$.</li><li>Set $i = i + 1$, and go back to step $2$.</li></ol><p>Finally, we can find the point closest to $\mu_E$ in the convex closure of $\mu^{(0)}, \cdots, \mu^{(n)}$ by solving the following QP:<br>$$<br>\min| \mu_E - \mu |_2, \; \text{s.t.} \; \mu = \sum_i \lambda_i \mu^{(i)}, \lambda_i \geq 0, \sum_i \lambda_i = 1.<br>$$<br>(*)<br>$$<br>\begin{align}<br>\max_{t, w} &amp;\;t \\<br>s.t. &amp;\; w^T\mu_E \geq w^T\mu^{(j)} + t, \; j = 0, \cdots, i-1 \\<br>&amp;\; |w |_2 \leq 1.<br>\end{align}<br>$$<br>Note that, step 2 could replace by a simpler way that no QP solver is needed:</p><ul><li>Set $\bar{\mu}^{(i-1)} = \bar{\mu}^{(i-2)} + \frac{(\mu^{(i-1)} - \bar{\mu}^{(i-2)})^T(\mu_E - \bar{\mu}^{(i-2)})}{(\mu^{(i-1)} - \bar{\mu}^{(i-2)})^T(\mu^{(i-1)} - \bar{\mu}^{(i-2)})} (\mu^{(i-1)} - \bar{\mu}^{(i-2)})^T$<ul><li>This computes the orthogonal projection of $\mu_E$ onto the line through $\bar{\mu}^{(i-2)}$ and $\mu^{(i-1)}$.</li></ul></li><li>Set $w^{(i)} = \mu_E - \bar{\mu}^{(i-1)}$</li><li>Set $t^{(i)} = | \mu_E - \bar{\mu}^{(i-1)} |_2$</li></ul><p>In the first iteration, we also set $w^{(1)} = \mu_E - \mu^{(0)}$ and $\bar{\mu}^{(0)} = \mu^{(0)}$.</p><hr><h3 id="Deep-Reinforcement-Learning-with-Double-Q-learning"><a href="#Deep-Reinforcement-Learning-with-Double-Q-learning" class="headerlink" title="Deep Reinforcement Learning with Double Q-learning"></a>Deep Reinforcement Learning with Double Q-learning</h3><p>The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having introduce additional networks. We therefore propose to evaluate the greedy policy according to the online network, but using the target network to estimate its value. In reference to both Double Q-learning and DQN, we refer to the resulting algorithm as Double DQN. Its update is the same as for DQN, but replacing the target $Y_t^{DQN}$ with<br>$$<br>Y_t^{\text{DoubleDQN}} \equiv R_{t+1} + \gamma Q(S_{t+1}, {\arg \max}_a Q(S_{t+1},a; \boldsymbol{\theta}_t),\boldsymbol{\theta}_t^{\mathbf{-}}).<br>$$<br>In comparison to Double Q-learning<br>$$<br>Y_t^{\text{DoubleDQN}} \equiv R_{t+1} + \gamma Q(S_{t+1}, {\arg \max}_a Q(S_{t+1},a; \boldsymbol{\theta}_t),\boldsymbol{\theta}_t^{\boldsymbol{\prime}}),<br>$$<br>the weights of the second network $\boldsymbol{\theta_t^{\prime}}$ are replaced with the weights of the target network $\boldsymbol{\theta_t^{-}}$ for the evaluation of the current greedy policy. The update to the target stays unchanged from DQN, and remains a periodic copy of the online network.</p><hr><h3 id="Prioritized-Experience-Replay"><a href="#Prioritized-Experience-Replay" class="headerlink" title="Prioritized Experience Replay"></a>Prioritized Experience Replay</h3><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/papersdouble-dqn-priori-exper-replay.png" alt="double-dqn-priori-exper-replay"></p><p>where $p_i &gt; 0$ is the priority of transition $i$. The exponent $\alpha$ determines how much prioritization is used, with $\alpha=0$ corresponding to the uniform case.</p><p>The first we consider is the direct, proportional prioritization where $p_i = |\delta_i| + \epsilon$, where $\delta_i$ is the TD-error of transition $i$ and $\epsilon$ is a small positive constant that prevent the edge-case of transitions not being revisited once their error is zero. The second variant is an indirect, rand-based prioritization where $p_i = \frac{1}{\text{rank}(i)}$, where $\text{rank}_i$ is the rank of transition $i$ when the replay memory is sorted according to $|\delta_i|$</p><hr><h3 id="Dueling-Network-Architectures-for-Deep-Reinforcement-Learning"><a href="#Dueling-Network-Architectures-for-Deep-Reinforcement-Learning" class="headerlink" title="Dueling Network Architectures for Deep Reinforcement Learning"></a>Dueling Network Architectures for Deep Reinforcement Learning</h3><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/papersduel-network-rl-arch.png" alt="duel-network-arch"></p><p>Let us consider the dueling network shown in above, where we make one stream of fully-connected layers output a scalar $V(s;\theta,\beta)$, and the other stream output an $\mathcal{A}$-dimensional vector $A(s, a; \theta, \alpha)$. Here, $\theta$ denotes the parameters of the convolutional layers, while $\alpha$ and $\beta$ are the parameters of the two streams of fully-connected layers. Using the definition of advantage, we might be tempted to construct the aggregating module as follows:<br>$$<br>Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + A(s, a; \theta, \alpha),<br>$$<br>Note that this expression applies to all $(s, a)$ instances; that is, to express equation above in matrix form we need to replicate the scalar, $V(s;\theta,\beta)$, $|\mathcal{A}|$ times.</p><p>Equation above is unidentifiable in the sense that given $Q$ we cannot recover V and A uniquely. To see this, add a constant to $V(s;\theta,\beta)$ and subtract the same constant from $A(s, a; \theta, \alpha)$. This constant cancels out resulting in the same $Q$ value. This lack of identifiability is mirrored by poor practical performance when this equation is used directly.</p><p>To address this issue of identifiability, we can replace the equation above to this one:<br>$$<br>Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \Big( A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a^{\prime}} A(s, a^{\prime}; \theta, \alpha) \Big).<br>$$</p><hr><h3 id="Learning-Tetris-Using-the-Noisy-Cross-Entropy-Method"><a href="#Learning-Tetris-Using-the-Noisy-Cross-Entropy-Method" class="headerlink" title="Learning Tetris Using the Noisy Cross-Entropy Method"></a>Learning Tetris Using the Noisy Cross-Entropy Method</h3><p>the paper works on solving Tetris with modified cross entropy method. original CE method in reinforcement learning usually results in early convergence.</p><h4 id="Cross-entropy-method-in-reinforcement-learning"><a href="#Cross-entropy-method-in-reinforcement-learning" class="headerlink" title="Cross entropy method in reinforcement learning"></a>Cross entropy method in reinforcement learning</h4><ul><li>first we start with a random uniform distribution <code>F_0</code></li><li>drawn from <code>F_0</code> and get N samples θ_0, θ_1, …</li><li>choose the top K samples that get the highest scores and use these selected sample(θ_0, θ_1, …) update distribution and get <code>F_1</code></li></ul><h4 id="keypoints"><a href="#keypoints" class="headerlink" title="keypoints"></a>keypoints</h4><ul><li>add noise to the cross-entropy method to prevent early converge</li><li>if we decrease the noise, which is only depend on time steps, the performance can even be better.</li><li>noise 👉 prevent early converge</li></ul><h4 id="note"><a href="#note" class="headerlink" title="note"></a>note</h4><ul><li>can view the noise apply to std as ensure enough exploration</li></ul><hr><h3 id="Doubly-Robust-Off-policy-Value-Evaluation-for-Reinforcement-Learning"><a href="#Doubly-Robust-Off-policy-Value-Evaluation-for-Reinforcement-Learning" class="headerlink" title="Doubly Robust Off-policy Value Evaluation for Reinforcement Learning"></a>Doubly Robust Off-policy Value Evaluation for Reinforcement Learning</h3><p>We study the off-policy value evaluation problem, where one aims to estimate the value of a policy with data collected by another policy.</p><p>There are roughly two classes of approaches to off-policy value evaluation. The first is to fit an MDP model from data via regression, and evaluate the policy against the model; The second class of approaches are based on the idea of importance sampling (IS), which corrects the mismatch between the distributions induces by the target policy ang by the behavior policy.</p><h4 id="Importance-Sampling-Estimators"><a href="#Importance-Sampling-Estimators" class="headerlink" title="Importance Sampling Estimators"></a>Importance Sampling Estimators</h4><h5 id="The-basic-IS-Estimator"><a href="#The-basic-IS-Estimator" class="headerlink" title="The basic IS Estimator"></a>The basic IS Estimator</h5><p>The IS estimator provides an unbiased estimate of $\pi_1$’s value by averaging the following function of each trajectory $(s_1, a_1, r_1, \cdots, s_{H+1})$ in the data: define the per-step importance ratio as $\rho_t := \pi_1(a_t|s_t)/\pi_0(a_t|s_t)$, and the cumulative importance ratio $\rho_{1:t}:=\prod_{t^{\prime}=1}^{t}\rho_{t^{\prime}}$; the basic (trajectory-wise) IS estimator, and an improved step-wise version are given as follows:<br>$$<br>V_{\text{IS}} := \rho_{1:H} \cdot (\sum_{t=1}^{H} \gamma^{t-1} r_t),<br>$$</p><p>$$<br>V_{\text{step-IS}} := \sum_{t=1}^{H} \gamma^{t-1}\rho_{1:t}r_t.<br>$$</p><p>Given a dataset $D$, the IS estimator is simply the average estimate over the trajectories, namely $\frac{1}{|D|}\sum_{i=1}V_{\text{IS}}^{(i)}$, where $|D|$ is the number of trajectories in $D$ and $V_{\text{IS}}^{(i)}$ is IS applied to the $i$-th trajectory.</p><h5 id="The-weighted-importance-sampling-WIS"><a href="#The-weighted-importance-sampling-WIS" class="headerlink" title="The weighted importance sampling (WIS)"></a>The weighted importance sampling (WIS)</h5><p>Define $w_t = \sum_{i=1}^{|D|}\rho_{1:t}^{(i)}/|D|$ as the average cumulative importance ratio at horizon $t$ in a dataset $D$, then for each trajectory in $D$, the estimates given by trajectory-wise and step-wise WIS are respectively<br>$$<br>V_{\text{WIS}} = \frac{\rho_{1:H}}{w_{H}}(\sum_{t=1}^H\gamma^{t-1}r_t),<br>$$</p><p>$$<br>V_{\text{step-WIS}} = \sum_{t=1}^H\gamma^{t-1}\frac{\rho_{1:t}}{w_t}r_t.<br>$$</p><h5 id="The-doubly-robust-estimator-for-contextual-bandits"><a href="#The-doubly-robust-estimator-for-contextual-bandits" class="headerlink" title="The doubly robust estimator for contextual bandits"></a>The doubly robust estimator for contextual bandits</h5><p>$$<br>V_{\text{DR}} := \widehat{V}(s) + \rho(r - \widehat{R}(s, a)),<br>$$</p><p>where $\rho := \frac{\pi_1(a|s)}{\pi_0(a|s)}$ and $\widehat{V}(s) := \sum_a \pi_1(a|s)\widehat{R}(s, a)$. If $\widehat{R}(s, a)$ is a good estimator of $r$, the magnitude of $r - \widehat{R}(s, a)$ can be much smaller than of $r$. Consequently, the variance of $\rho(r - \widehat{R}(s, a))$ tends to be smaller than that of $\rho r$, implying that DR often has a lower variance that IS (Dud´ık et al., 2011).</p><h5 id="DR-Estimator-for-the-Sequential-Setting"><a href="#DR-Estimator-for-the-Sequential-Setting" class="headerlink" title="DR Estimator for the Sequential Setting"></a>DR Estimator for the Sequential Setting</h5><p>A key observation is that Eqn.(6) can be written in a recursive form. Define $V_{\text{step-IS}}^0 := 0$, and for $t=1, \cdots, H$,<br>$$<br>V_{\text{step-IS}}^{H+1-t} := \rho_t (r_t + \gamma V_{\text{step-IS}}^{H-t}).<br>$$<br>We can apply the bandit DR estimator at each horizon, and obtain the following unbiased estimator: define $V_{\text{DR}}^0 := 0$, and<br>$$<br>V_{\text{DR}}^{H+1-t} := \widehat{V}(s_t) + \rho_t (r_t + \gamma V_{\text{DR}}^{H-t} - \widehat{Q}(s_t, a_t)).<br>$$<br>The DR estimator of the policy value is then $V_{\text{DR}} := V_{\text{DR}}^H$.</p><p>One modification of DR that further reduces the variance in state transitions is:<br>$$<br>V_{\text{DR-v2}}^{H+1-t} := \widehat{V}(s_t) + \rho_t (r_t + \gamma V_{\text{DR-v2}}^{H-t} - \widehat{Q}(s_t, a_t) - \gamma \widehat{V}(s_{t+1})).<br>$$</p><hr><h3 id="Continuous-State-Space-Models-for-Optimal-Sepsis-Treatment-a-Deep-Reinforcement-Learning-Approach"><a href="#Continuous-State-Space-Models-for-Optimal-Sepsis-Treatment-a-Deep-Reinforcement-Learning-Approach" class="headerlink" title="Continuous State-Space Models for Optimal Sepsis Treatment: a Deep Reinforcement Learning Approach"></a>Continuous State-Space Models for Optimal Sepsis Treatment: a Deep Reinforcement Learning Approach</h3><h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p>State Autoencoder + Deep Q-Network + Dueling Network + Double Q Learning + Prioritized Experience Replay</p><p><strong>Note: loss function have changed:</strong><br>$$<br>\mathcal{L}(\theta) = \mathbb{E}[(Q_{\text{double-target}} - Q(s, a;\theta))^2] - \lambda \cdot \max(|Q(s, a;\theta) - R_{\max}|, 0),<br>$$<br>where $R_{\max} = \pm15, Q_{\text{double-target}}=r+\gamma Q(s^{\prime}, \arg\max_{a^{\prime}}Q(s^{\prime}, a^{\prime};\theta);\theta)$. $\theta$ are the weights used to parameterize the main network, and $\theta^{\prime}$ are the weights used to parameterize the target network.</p><h4 id="Feature"><a href="#Feature" class="headerlink" title="Feature"></a>Feature</h4><h5 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h5><p>Data were aggregated into windows of <strong>4 hours</strong>, with the <strong>mean or sum</strong> being recorded (as appropriate) when several data points were present in one window. Variables with excessive missingness were removed, and any remaining missing values were imputed with <strong>k-nearest neighbors</strong>, yielding a 47 × 1 feature vector for each patient at each timestep. Values exceeding clinical limits were capped, and capped data were <strong>normalized</strong> per-feature to zero mean and unit variance.</p><h5 id="Feature-list"><a href="#Feature-list" class="headerlink" title="Feature list"></a>Feature list</h5><p>The physiological features used in our model are presented below.</p><p><strong>Demographics/Static</strong><br>Shock Index, Elixhauser, SIRS, Gender, Re-admission, GCS - Glasgow Coma Scale, SOFA - Sequential Organ Failure Assessment, Age</p><p><strong>Lab Values</strong><br>Albumin, Arterial pH, Calcium, Glucose, Hemoglobin, Magnesium, PTT - Partial Thromboplastin Time, Potassium, SGPT - Serum Glutamic-Pyruvic Transaminase, Arterial Blood Gas, BUN - Blood Urea Nitrogen, Chloride, Bicarbonate, INR -International Normalized Ratio, Sodium, Arterial Lactate, CO2, Creatinine, Ionised Calcium, PT - Prothrombin Time, Platelets Count, SGOT - Serum Glutamic-Oxaloacetic Transaminase, Total bilirubin, White Blood Cell Count</p><p><strong>Vital Signs</strong><br>Diastolic Blood Pressure, Systolic Blood Pressure, Mean Blood Pressure, PaCO2, PaO2, FiO2, PaO/FiO2 ratio, Respiratory Rate, Temperature (Celsius), Weight (kg), Heart Rate, SpO2</p><p><strong>Intake and Output Events</strong><br>Fluid Output - 4 hourly period, Total Fluid Output, Mechanical Ventilation</p><h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><h5 id="Discounted-Returns-vs-Mortality"><a href="#Discounted-Returns-vs-Mortality" class="headerlink" title="Discounted Returns vs. Mortality"></a>Discounted Returns vs. Mortality</h5><p>We bin Q-values obtained via SARSA (baseline) on the test set into discrete buckets, and for each, if it is part of a trajectory where a patient died, we assign it a label of 1; if the patient survived, we assign a label of 0.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/eva1.png" alt="eva1"></p><h5 id="State-Representation"><a href="#State-Representation" class="headerlink" title="State Representation"></a>State Representation</h5><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/eva2.png" alt="eva2"></p><h5 id="Doubly-Robust-off-policy-value-evaluation"><a href="#Doubly-Robust-off-policy-value-evaluation" class="headerlink" title="Doubly Robust off-policy value evaluation"></a>Doubly Robust off-policy value evaluation</h5><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/eva3.png" alt="eva3"></p><h5 id="Action-differences"><a href="#Action-differences" class="headerlink" title="Action differences"></a>Action differences</h5><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/eva4.png" alt="eva4"></p><h5 id="Action-vs-Mortality"><a href="#Action-vs-Mortality" class="headerlink" title="Action vs. Mortality"></a>Action vs. Mortality</h5><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/eva5.png" alt="eva5"></p><hr><h3 id="A-Reinforcement-Learning-Approach-to-Weaning-of-Mechanical-Ventilation-in-Intensive-Care-Units"><a href="#A-Reinforcement-Learning-Approach-to-Weaning-of-Mechanical-Ventilation-in-Intensive-Care-Units" class="headerlink" title="A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units"></a>A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units</h3><h4 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h4><h5 id="Preprocessing-using-Gaussian-Processes"><a href="#Preprocessing-using-Gaussian-Processes" class="headerlink" title="Preprocessing using Gaussian Processes"></a>Preprocessing using Gaussian Processes</h5><p>Denoting the observations of the vital signs by $v$ and the measurement time $t$, we model<br>$$<br>v = f(t) + \varepsilon,<br>$$<br>where $\varepsilon$ vector represents i.i.d Gaussian noise, and $f(t)$ is the latent noise-free function we would like to estimate. We put a GP prior on the latent function $f(t)$:<br>$$<br>f(t) \sim \mathcal{GP}(m(t), \mathcal{K}(t, t^{\prime})),<br>$$<br>where $m(t)$ is the mean function and $\mathcal{K}(t, t^{\prime})$ is the covariance function or kernel. In this work, we use a multi-output GP to account for temporal correlations between physiological signals during interpolation. We adapt the framework in Cheng et al. [2017] to impute the physiological signals jointly by estimating covariance structures between them, excluding the sparse prior settings (<strong>please read the paper for more details</strong>).</p><p><strong>Note: Just for continuous features.</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/gp.png" alt="gp"></p><h5 id="Reward-Function"><a href="#Reward-Function" class="headerlink" title="Reward Function"></a>Reward Function</h5><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/rwdf.png" alt="rwdf"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/rwdf2.png" alt="rwdf2"></p><h5 id="Fitted-Q-iteration-with-sampling-Omitted"><a href="#Fitted-Q-iteration-with-sampling-Omitted" class="headerlink" title="Fitted Q-iteration with sampling (Omitted)"></a>Fitted Q-iteration with sampling (Omitted)</h5><h4 id="Evaluations"><a href="#Evaluations" class="headerlink" title="Evaluations"></a>Evaluations</h4><h5 id="Features-importance"><a href="#Features-importance" class="headerlink" title="Features importance"></a>Features importance</h5><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/fi.png" alt="fi"></p><h5 id="The-outcome-of-difference"><a href="#The-outcome-of-difference" class="headerlink" title="The outcome of difference"></a>The outcome of difference</h5><p>We divide the 664 test admissions into six groups according to the fraction of<br>FQI policy actions that differ from the hospital’s policy: $\Delta_0$ comprises admissions in which the true and recommended policies agree perfectly, while those in $\Delta_5$ show the greatest deviation. Plotting the distribution of the number of reintubations and the mean accumulated reward over patient admissions respectively, for all patients in each set.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/summary-rl-paper/diff.png" alt="diff"></p><hr><h3 id="Deep-Reinforcement-Learning-Decision-Making-and-Control-ICML-2017-Tutorial"><a href="#Deep-Reinforcement-Learning-Decision-Making-and-Control-ICML-2017-Tutorial" class="headerlink" title="Deep Reinforcement Learning, Decision Making and Control (ICML 2017 Tutorial)"></a>Deep Reinforcement Learning, Decision Making and Control (ICML 2017 Tutorial)</h3><h4 id="Model-Free-RL"><a href="#Model-Free-RL" class="headerlink" title="Model-Free RL"></a>Model-Free RL</h4><h5 id="policy-gradients"><a href="#policy-gradients" class="headerlink" title="policy gradients"></a>policy gradients</h5><p><strong>REINFORCE</strong> algorithm:</p><ol><li>sample ${\tau^i}$ from $\pi_\theta(s_t|a_t)$</li><li>$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i(\sum_t \nabla_\theta \log \pi_\theta(a_t^i|s_t^i))(\sum_t r(s_t^i, a_t^i))$</li><li>$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$</li></ol><p><strong>Reduce variance</strong></p><p>$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i(\sum_t \nabla_\theta \log \pi_\theta(a_t^i|s_t^i))(\sum_{t^{\prime}=1}^T r(s_{t^{\prime}}^i, a_{t^{\prime}}^i))$</p><p>“reward to go”</p><p><strong>Baselines</strong></p><p>one baseline: average reward.</p><p>$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i\nabla_\theta \log \pi_\theta(\tau) [r(\tau) - b]$</p><p>$b = \frac{1}{N} \sum_{i=1}^N r(\tau)$</p><p><strong>Control variates</strong> (see also: Gu et al. 2016 (Q-Prop))</p><p>$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i(\sum_t \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)) \Big (\hat{Q}_t^i - b(s_{t^{\prime}}^i, a_{t^{\prime}}^i) \Big ) + \frac{1}{N} \sum_i \sum_t \nabla_\theta E_{a \sim \pi_\theta(a_t | s_t^i)}[b(s_t^i, a_t)]$</p><p><strong>covatriant/natural policy gradient</strong></p><p>natural gradient: pick $\alpha$</p><p>$\theta \leftarrow \theta + \alpha \mathbf{F}^{-1}\nabla_\theta J(\theta)$</p><p>$\mathbf{F} = E_{\pi_{\theta}}[\log\pi_{\theta}(a|s)\log\pi_{\theta}(a|s)^T]$</p><p>trust region policy optimization: pick $\epsilon$</p><p>$\theta^{\prime} \leftarrow \arg\max_{\theta^{\prime}} (\theta^{\prime} - \theta)^T \nabla_\theta J(\theta) \; \text{s.t.} (\theta^{\prime} - \theta)^T\mathbf{F}(\theta^{\prime} - \theta) \leq \epsilon$</p><p><strong>Policy gradients suggested readings</strong><br>• <strong>Classic papers</strong><br>​ • Williams (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning: introduces REINFORCE algorithm<br>​ • Baxter &amp; Bartlett (2001). Infinite-horizon policy-gradient estimation: temporally decomposed policy gradient (not the first paper on this! see actor-critic section later)<br>​ • Peters &amp; Schaal (2008). Reinforcement learning of motor skills with policy gradients: very accessible overview of optimal baselines and natural gradient<br>• <strong>Deep reinforcement learning policy gradient papers</strong><br>​ • L. &amp; Koltun (2013). Guided policy search: deep RL with importance sampled policy gradient (unrelated to later discussion of guided policy search)<br>​ • Schulman, L., Moritz, Jordan, Abbeel (2015). Trust region policy optimization: deep RL with natural policy gradient and adaptive step size<br>​ • Schulman, Wolski, Dhariwal, Radford, Klimov (2017). Proximal policy optimization algorithms: deep RL with importance sampled policy gradient</p><h5 id="Actor-Critic-algorithms"><a href="#Actor-Critic-algorithms" class="headerlink" title="Actor-Critic algorithms"></a>Actor-Critic algorithms</h5><p><strong>Value function fitting</strong></p><p><strong>batch actor-critic</strong> algorithm:</p><ol><li>sample $\{s_i, a_i\}$ from $\pi_\theta(a|s)$</li><li>fit $\hat{V_\phi}(s)$ to sampled reward sums (*)</li><li>evaluate $\hat{A^{\theta}}(s_i, a_i) = r(s_i, a_i) + \hat{V_\phi}(s_i^{\prime}) - \hat{V_\phi}(s_i)$</li><li>$\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log \pi_\theta(a^i|s^i) \hat{A}(s_i, a_i) $</li><li>$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$</li></ol><p>(*) $y_{i,t} \approx \sum_{t=1}^T r(s_t^i, a_{t}^i)$</p><p>$\mathcal{L}(\phi) = \frac{1}{2}\sum_i | \hat{V_\phi}(s_i) - y_i|^2$</p><p><strong>Discount factors</strong></p><p>$y_{i,t} \approx r(s_t^i, a_{t}^i) + \gamma \hat{V_\phi}(s_{t+1}^i)$ (0.99 works well)</p><p><strong>online actor-critic</strong> algorithm:</p><ol><li>take action $\mathbf{a} \sim \pi_\theta(a|s)$, get $(s, a, s^{\prime}, r)$</li><li>update $\hat{V_\phi^{\pi}}(s)$ using target $r + \gamma \hat{V_\phi^{\pi}}(s^{\prime})$</li><li>evaluate $\hat{A^{\theta}}(s_i, a_i) = r(s_i, a_i) + \hat{V_\phi}(s_i^{\prime}) - \hat{V_\phi}(s_i)$</li><li>$\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log \pi_\theta(a^i|s^i) \hat{A}(s_i, a_i) $</li><li>$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$</li></ol><p>Step 2 and 4 works best with a batch.</p><p>We can design better estimators (for both batch and online). See Schulman, Moritz, L. Jordan, Abbeel ‘16: Generalized advantage estimation.</p><p>We can use single network for actor and critic.</p><p><strong>Actor-critic suggested readings</strong></p><p>• <strong>Classic papers</strong><br>​ • Sutton, McAllester, Singh, Mansour (1999). Policy gradient methods for<br>reinforcement learning with function approximation: actor-critic algorithms with<br>value function approximation<br>• <strong>Deep reinforcement learning actor-critic papers</strong><br>​ • Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu (2016).<br>Asynchronous methods for deep reinforcement learning: A3C – parallel online<br>actor-critic<br>​ • Schulman, Moritz, L., Jordan, Abbeel (2016). High-dimensional continuous<br>control using generalized advantage estimation: batch-mode actor-critic with<br>blended Monte Carlo and function approximator returns<br>​ • Gu, Lillicrap, Ghahramani, Turner, L. (2017). Q-Prop: sample-efficient policy-<br>gradient with an off-policy critic: policy gradient with Q-function control variate</p><h5 id="Value-Functions"><a href="#Value-Functions" class="headerlink" title="Value Functions"></a>Value Functions</h5><p><strong>Q-Learning</strong></p><ol><li>take some action $a_i$ and observe $(s_i, a_i, s^{\prime}_i, r_i)$, add it to $\mathcal{R}$</li><li>sample mini-batch $(s_j, a_j, s^{\prime}_j, r_j)$ from $\mathcal{R}$ uniformly</li><li>compute $y_j = r_j + \gamma \max_{a^{\prime}_j} Q_{\phi^{\prime}} (s_j^{\prime},a_j^{\prime})$ using <em>target</em> network $Q_{\phi^{\prime}} $</li><li>$\phi \leftarrow \phi - \alpha \sum_j \frac{dQ_{\phi}}{d\phi} (Q_\phi(s_j, a_j) - y_j)$</li><li>update $\phi^{\prime}$: copy $\phi$ every $N$ steps, or Polyak average $\phi^{\prime} \leftarrow \tau\phi^{\prime} + (1-\tau)\phi$</li></ol><p><strong>Q-Learning with continuous actions</strong></p><p>Option 1: use function class that is easy to optimize (Gu, Lillicrap, Sutskever, L., ICML 2016)</p><p>$Q(s, a | \theta^Q) = A(s, a|\theta^A) + V(x|\theta^V)$</p><p>$A(s, a|\theta^A) = -\frac{1}{2}(s-\mu(x|\theta^{\mu}))^TP(s|\theta^P)(s-\mu(x|\theta^{\mu}))$</p><p>Option 2: learn an approximate maximizer <strong>DDPG (Lillicrap et al., ICLR 2016)</strong></p><p>$\mu_\theta(s) = a, \theta \leftarrow \arg\max_\theta Q_{\phi}(s, \mu_{\theta}(s))$</p><p>$y_j = r_j + \gamma \max_{a^{\prime}_j} Q_{\phi^{\prime}}(s_j^{\prime}, \mu_{\theta(s_j^{\prime})})$</p><p><strong>Q-learning suggested readings</strong><br>• <strong>Classic papers</strong><br>​ • Watkins. (1989). Learning from delayed rewards: introduces Q-learning<br>​ • Riedmiller. (2005). Neural fitted Q-iteration: batch-mode Q-learning with neural<br>networks<br>• <strong>Deep reinforcement learning Q-learning papers</strong><br>​ • Lange, Riedmiller. (2010). Deep auto-encoder neural networks in reinforcement<br>learning: early image-based Q-learning method using autoencoders to construct<br>embeddings<br>​ • Mnih et al. (2013). Human-level control through deep reinforcement learning: Q-<br>learning with convolutional networks for playing Atari.<br>​ • Van Hasselt, Guez, Silver. (2015). Deep reinforcement learning with double Q-learning: a very effective trick to improve performance of deep Q-learning.<br>​ • Lillicrap et al. (2016). Continuous control with deep reinforcement learning: continuous Q-learning with actor network for approximate maximization.<br>​ • Gu, Lillicrap, Stuskever, L. (2016). Continuous deep Q-learning with model-based<br>acceleration: continuous Q-learning with action-quadratic value functions.<br>​ • Wang, Schaul, Hessel, van Hasselt, Lanctot, de Freitas (2016). Dueling network<br>architectures for deep reinforcement learning: separates value and advantage estimation in Q-function.</p><h4 id="Soft-optimality-WIG"><a href="#Soft-optimality-WIG" class="headerlink" title="Soft optimality (WIG)"></a>Soft optimality (WIG)</h4><p><strong>RL inference in a graphical model</strong></p><p><strong>soft Q-learning</strong></p><p>$\phi \leftarrow \phi + \alpha \nabla_\phi Q_\phi (s, a) (r(s, a) + \gamma V(s^{\prime}) - Q_\phi(s, a))$</p><p>target value: $V(s^{\prime}) = \text{soft}\max_{a^{\prime}} Q_\phi(s^{\prime}, a^{\prime}) = \log \int \exp(Q_\phi(s^{\prime}, a^{\prime})da^{\prime}$</p><p>$\pi(a|s) = \exp(Q_\phi(s,a)-V(s)) = \exp(A(s, a))$</p><p><strong>policy gradient with soft optimality</strong> (WIG)</p><p><strong>Soft optimality suggested readings</strong><br>• Todorov. (2006). Linearly solvable Markov decision problems: one framework for<br>reasoning about soft optimality.<br>• Todorov. (2008). General duality between optimal control and estimation: primer on the equivalence between inference and control.<br>• Kappen. (2009). Optimal control as a graphical model inference problem: frames control as an inference problem in a graphical model.<br>• Ziebart. (2010). Modeling interaction via the principle of maximal causal entropy:<br>connection between soft optimality and maximum entropy modeling.<br>• Rawlik, Toussaint, Vijaykumar. (2013). On stochastic optimal control and reinforcement learning by approximate inference: temporal difference style algorithm with soft optimality.<br>• Haarnoja<em>, Tang</em>, Abbeel, L. (2017). Reinforcement learning with deep energy based<br>models: soft Q-learning algorithm, deep RL with continuous actions and soft optimality<br>• Nachum, Norouzi, Xu, Schuurmans. (2017). Bridging the gap between value and policy based reinforcement learning.<br>• Schulman, Abbeel, Chen. (2017). Equivalence between policy gradients and soft Q-<br>learning.</p><h4 id="Inverse-RL"><a href="#Inverse-RL" class="headerlink" title="Inverse RL"></a>Inverse RL</h4><h5 id="Maximum-Entropy-Inverse-RL-Ziebart-et-al-’08"><a href="#Maximum-Entropy-Inverse-RL-Ziebart-et-al-’08" class="headerlink" title="Maximum Entropy Inverse RL (Ziebart et al. ’08)"></a>Maximum Entropy Inverse RL (Ziebart et al. ’08)</h5><ol><li>Initialize $\psi$, gather demonstrations $\mathcal{D}$</li><li>Solve for optimal policy $\pi(a|s)$ w.r.t. reward $r_\psi$</li><li>Solve for state visitation frequencies $p(s|\psi)$</li><li>Compute gradient $\nabla_\psi \mathcal{L} = -\frac{1}{|\mathcal{D}|}\sum_{\tau_d \in \mathcal{D}} \frac{dr_\psi}{d\psi}(\tau_d)\sum_{s} \frac{dr_\psi}{d\psi}(s)$</li><li>Update $\psi$ with one gradient step using $\nabla_\psi\mathcal{L}$</li></ol><p><strong>guided cost learning &amp; generative adversarial imitation algorithm (Finn et al. ICML ’16, Ho &amp; Ermon NIPS ’16)</strong></p><p><strong>Suggested Reading on Inverse RL Classic Papers</strong></p><ul><li>Abbeel &amp; Ng ICML ’04. Apprenticeship Learning via Inverse Reinforcement Learning. Good introduction to inverse reinforcement learning</li><li>Ziebart et al. AAAI ’08. Maximum Entropy Inverse Reinforcement Learning.<br>Introduction of probabilistic method for inverse reinforcement learning<br>Modern Papers</li><li>Wulfmeier et al. arXiv ’16. Deep Maximum Entropy Inverse Reinforcement Learning. MaxEnt IRL using deep reward functions</li><li>Finn et al. ICML ’16. Guided Cost Learning. Sampling-based method for MaxEnt IRL<br>that handles unknown dynamics and deep reward functions</li><li>Ho &amp; Ermon NIPS ’16. Generative Adversarial Imitation Learning. IRL method<br>building on Abbeel &amp; Ng ’04 using generative adversarial networks</li></ul><p><strong>Further Reading on Inverse RL</strong></p><ul><li>MaxEnt-based IRL: Ziebart et al. AAAI ’08, Wulfmeier et al. arXiv ’16, Finn et al. ICML ‘16</li><li>Adversarial IRL: Ho &amp; Ermon NIPS ’16, Finn<em>, Christiano</em> et al. arXiv ’16, Baram et al. ICML ’17</li><li>Handling multimodality: Li et al. arXiv ’17, Hausman et al. arXiv ’17, Wang, Merel et al. ‘17</li><li>Handling domain shift: Stadie et al. ICLR ‘17</li></ul><h4 id="Model-bases-RL-WIS"><a href="#Model-bases-RL-WIS" class="headerlink" title="Model-bases RL (WIS)"></a>Model-bases RL (WIS)</h4><p><strong>Guided Policy Search (GPS)</strong></p><p><strong>Suggested Reading on Model-based RL</strong></p><ul><li>Tassa et al. IROS ’12. Synthesis and Stabilization of Complex Behaviors. Good</li></ul><p>introduction to MPC with a known model</p><ul><li><p>Levine<em>, Finn</em> et al. JMLR ’16. End-to-End Learning of Deep Visuomotor Policies.</p><p>Thorough paper on guided policy search for learning real robotic vision-based skills</p></li><li><p>Heess et al. NIPS ’15. Stochastic Value Gradients. Backdrop through dynamics to assist model-free learner</p></li><li><p>Watter et al. NIPS ’15. Embed-to-Control, Learn latent space and use model-baed RL in learned latent space to reach image of goal</p></li><li><p>Finn &amp; Levine ICRA ’17. Deep Visual Foresight for Planning Robot Motion. Plan using learned action-conditioned video prediction model</p></li></ul><p><strong>Further Reading on Model-based RL</strong></p><ul><li><strong>Use known model</strong>: Tassa et al. IROS ’12, Tan et al. TOG ’14, Mordatch et al. TOG ‘14</li><li><strong>Guided policy search</strong>: Levine<em>, Finn</em> et al. JMLR ’16, Mordatch et al. RSS ’14, NIPS ‘15</li><li><strong>Backprop through model</strong>: Deisenroth et al. ICML ’11, Heess et al. NIPS ’15, Mishra et al. ICML ’17, Degrave et al. ’17, Henaﬀ et al. ‘17</li></ul><ul><li><strong>MBRL in latent space</strong>: Watter et al. NIPS ’15, Finn et al. ICRA ‘16</li><li><strong>MPC with deep models</strong>: Lenz et al. RSS ’15, Finn &amp; Levine ICRA ‘17</li><li><strong>Combining Model-Based &amp; Model-Free:</strong><ul><li>use roll-outs from model as experience: Sutton ’90, Gu et al. ICML ‘16</li><li>use model as baseline: Chebotar et al. ICML ‘17</li><li>use model for exploration: Stadie et al. arXiv ’15, Oh et al. NIPS ’16</li><li>model-free policy with planning capabilities: Tamar et al. NIPS ’16, Pascanu et al. ‘17</li><li>model-based look-ahead: Guo et al. NIPS ’14, Silver et al. Nature ‘16</li></ul></li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;CheXNet-Radiologist-Level-Pneumonia-Detection-on-Chest-X-Rays-with-Deep-Learning&quot;&gt;&lt;a href=&quot;#CheXNet-Radiologist-Level-Pneumonia-Dete
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Using Keras and Deep Q-Network to Play FlappyBird (Repost)</title>
    <link href="http://yoursite.com/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/"/>
    <id>http://yoursite.com/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/</id>
    <published>2017-07-06T20:19:18.000Z</published>
    <updated>2017-07-06T20:34:48.861Z</updated>
    
    <content type="html"><![CDATA[<p>200 lines of python code to demonstrate DQN with Keras</p><p><img src="https://yanpanlau.github.io/img/animation1.gif" alt="img"></p><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>This project demonstrates how to use the Deep-Q Learning algorithm with Keras together to play FlappyBird.</p><p>This article is intended to target newcomers who are interested in Reinforcement Learning.</p><h1 id="Installation-Dependencies"><a href="#Installation-Dependencies" class="headerlink" title="Installation Dependencies:"></a>Installation Dependencies:</h1><p>(Update : 13 March 2017, code and weight file has been updated to support latest version of tensorflow and keras)</p><ul><li>Python 2.7</li><li>Keras 1.0</li><li>pygame</li><li>scikit-image</li></ul><h1 id="How-to-Run"><a href="#How-to-Run" class="headerlink" title="How to Run?"></a>How to Run?</h1><p><strong>CPU only/TensorFlow</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><p><strong>GPU version (Theano)</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">THEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=0.2 python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><p><strong>lib.cnmem=0.2</strong> means you assign only 20% of the GPU’s memory to the program.</p><p><strong>If you want to train the network from beginning, delete “model.h5” and run qlearn.py -m “Train”</strong></p><h1 id="What-is-Deep-Q-Network"><a href="#What-is-Deep-Q-Network" class="headerlink" title="What is Deep Q-Network?"></a>What is Deep Q-Network?</h1><p>Deep Q-Network is a learning algorithm developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the algorithm is generic enough to play various games.</p><p>The following post is a must-read for those who are interested in deep reinforcement learning.</p><p><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">Demystifying Deep Reinforcement Learning</a></p><h1 id="Code-Explanation-in-details"><a href="#Code-Explanation-in-details" class="headerlink" title="Code Explanation (in details)"></a>Code Explanation (in details)</h1><p>Let’s go though the example in <strong>qlearn.py</strong>, line by line. If you familiar with Keras and DQN, you can skip this session</p><p>The code simply does the following:</p><ol><li>The code receives the Game Screen Input in the form of a pixel array</li><li>The code does some image pre-processing</li><li>The processed image will be fed into a neural network (Convolution Neural Network), and the network will then decide the best action to execute (Flap or Not Flap)</li><li>The network will be trained millions of times, via an algorithm called Q-learning, to maximize the future expected reward.</li></ol><h3 id="Game-Screen-Input"><a href="#Game-Screen-Input" class="headerlink" title="Game Screen Input"></a>Game Screen Input</h3><p>First of all, the FlappyBird is already written in Python via pygame, so here is the code snippet to access the FlappyBird API</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import wrapped_flappy_bird as game</div><div class="line">x_t1_colored, r_t, terminal = game_state.frame_step(a_t)</div></pre></td></tr></table></figure><p>The idea is quite simple, the input is <strong>a_t</strong> (0 represent don’t flap, 1 represent flap), the API will give you the next frame <strong>x_t1_colored</strong>, the <strong>reward</strong> (0.1 if alive, +1 if pass the pipe, -1 if die) and <strong>terminal</strong> is a boolean flag indicates whether the game is FINISHED or NOT. We also followed DeepMind suggestion to clip the reward between [-1,+1] to improve the stability. I have not yet get a chance to test out different reward functions but it would be an interesting exercise to see how the performance is changed with different reward functions.</p><p>Interesting readers can modify the reward function in <strong>game/wrapped_flappy_bird.py”, under the function **def frame_step(self, input_actions)</strong></p><h3 id="Image-pre-processing"><a href="#Image-pre-processing" class="headerlink" title="Image pre-processing"></a>Image pre-processing</h3><p><img src="https://yanpanlau.github.io/img/bird.jpg" alt="img"></p><p>In order to make the code train faster, it is vital to do some image processing. Here are the key elements:</p><ol><li>I first convert the color image into grayscale</li><li>I crop down the image size into 80x80 pixel</li><li>I stack 4 frames together before I feed into neural network.</li></ol><p>Why do I need to stack 4 frames together? This is one way for the model to be able to infer the velocity information of the bird.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x_t1 = skimage.color.rgb2gray(x_t1_colored)</div><div class="line">x_t1 = skimage.transform.resize(x_t1,(<span class="number">80</span>,<span class="number">80</span>))</div><div class="line">x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(<span class="number">0</span>, <span class="number">255</span>))</div><div class="line"></div><div class="line">x_t1 = x_t1.reshape(<span class="number">1</span>, <span class="number">1</span>, x_t1.shape[<span class="number">0</span>], x_t1.shape[<span class="number">1</span>])</div><div class="line">s_t1 = np.append(x_t1, s_t[:, :<span class="number">3</span>, :, :], axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p><strong>x_t1</strong> is a single frame with shape (1x1x80x80) and <strong>s_t1</strong> is the stacked frame with shape (1x4x80x80). You might ask, why the input dimension is (1x4x80x80) but not (4x80x80)? Well, it is a requirement in Keras so let’s stick with it.</p><p>Note: Some readers may ask what is <strong>axis=1</strong>? It means that when I stack the frames, I want to stack on the “2nd” dimension. i.e. I am stacking under (1x<strong>4</strong>x80x80), the 2nd index.</p><h3 id="Convolution-Neural-Network"><a href="#Convolution-Neural-Network" class="headerlink" title="Convolution Neural Network"></a>Convolution Neural Network</h3><p>Now, we can input the pre-processed screen into the neural network, which is a convolution neural network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildmodel</span><span class="params">()</span>:</span></div><div class="line">    print(<span class="string">"Now we build the model"</span>)</div><div class="line">    model = Sequential()</div><div class="line">    model.add(Convolution2D(<span class="number">32</span>, <span class="number">8</span>, <span class="number">8</span>, subsample=(<span class="number">4</span>,<span class="number">4</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>,input_shape=(img_channels,img_rows,img_cols)))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">4</span>, <span class="number">4</span>, subsample=(<span class="number">2</span>,<span class="number">2</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, subsample=(<span class="number">1</span>,<span class="number">1</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Flatten())</div><div class="line">    model.add(Dense(<span class="number">512</span>, init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Dense(<span class="number">2</span>,init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)))</div><div class="line">   </div><div class="line">    adam = Adam(lr=<span class="number">1e-6</span>)</div><div class="line">    model.compile(loss=<span class="string">'mse'</span>,optimizer=adam)</div><div class="line">    print(<span class="string">"We finish building the model"</span>)</div><div class="line">    <span class="keyword">return</span> model</div></pre></td></tr></table></figure><p>The exact architecture is following : The input to the neural network consists of an 4x80x80 images. The first hidden layer convolves 32 filters of 8 x 8 with stride 4 and applies ReLU activation function. The 2nd layer convolves a 64 filters of 4 x 4 with stride 2 and applies ReLU activation function. The 3rd layer convolves a 64 filters of 3 x 3 with stride 1 and applies ReLU activation function. The final hidden layer is fully-connected consisted of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action.</p><p>So wait, what is convolution? The easiest way to understand a convolution is by thinking of it as a sliding window function apply to a matrix. The following gif file should help to understand.</p><p><img src="https://yanpanlau.github.io/img/Convolution_schematic.gif" alt="Convolution with 3×3 Filter. Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution"></p><p>You might ask what’s the purpose the convolution? It actually help computer to learn higher features like edges and shapes. The example below shows how the edges are stand out after a convolution filter is applied</p><p><img src="https://yanpanlau.github.io/img/generic-taj-convmatrix-edge-detect.jpg" alt="Using Convolution to detect Edges"></p><p>For more details about Convolution in Neural Network, please read <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_blank" rel="external">Understanding Convolution Neural Networks for NLP</a></p><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>Keras makes it very easy to build convolution neural network. However, there are few things I would like to highlight</p><p>A) It is important to choose a right initialization method. I choose normal distribution with $\sigma=0.01$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)</div></pre></td></tr></table></figure><p>B) The ordering of the dimension is important, the default setting is 4x80x80 (Theano setting), so if your input is 80x80x4 (Tensorflow setting) then you are in trouble because the dimension is wrong. <strong>Alert</strong>: If your input dimension is 80x80x4 (Tensorflow setting) you need to set <strong>dim_ordering = tf</strong> (tf means tensorflow, th means theano)</p><p>C) In Keras, <strong>subsample=(2,2)</strong> means you down sample the image size from (80x80) to (40x40). In ML literature it is often called “stride”</p><p>D) We have used an adaptive learning algorithm called ADAM to do the optimization. The learning rate is <strong>1-e6</strong>.</p><p>Interested readers who want to learn more various learning algoithms please read below</p><p><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">An overview of gradient descent optimization algorithms</a></p><h3 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h3><p>Finally, we can using the Q-learning algorithm to train the neural network.</p><p>So, what is Q-learning? In Q-learning the most important thing is the Q-function : Q(s, a) representing the maximum discounted future reward when we perform action <strong>a</strong> in state <strong>s</strong>. <strong>Q(s, a)</strong> gives you an estimation of how good to choose an action <strong>a</strong> in state <strong>s</strong>.</p><p>REPEAT : <strong>Q(s, a)</strong> representing the maximum discounted future reward when we perform action <strong>a</strong> in state <strong>s</strong></p><p>You might ask 1) Why Q-function is useful? 2) How can I get the Q-function?</p><p>Let me give you an analogy of the Q-function: Suppose you are playing a difficult RPG game and you don’t know how to play it well. If you have bought a strategy guide, which is an instruction book that contain hints or complete solutions to a specific video game, then playing that video game is easy. You just follow the guidiance from the strategy book. Here, Q-function is similar to a strategy guide. Suppose you are in state <strong>s</strong> and you need to decide whether you take action <strong>a</strong> or <strong>b</strong>. If you have this magical Q-function, the answers become really simple – pick the action with highest Q-value!</p><p>$$<br>{\pi(s) = {argmax}_{a} Q(s,a)}<br>$$<br>Here, $\pi$ represents the policy, which you will often see in the ML literature.</p><p>How do we get the Q-function? That’s where Q-learning is coming from. Let me quickly derive here:</p><p>Define total future reward from time <strong>t</strong> onward<br>$$<br>R_t = r_t + r_{t+1} + r_{t+2} … + r_n<br>$$<br>But because our environment is stochastic, we can never be sure, the more future we go, the more uncertain. Therefore, it is common to use <strong>discount future reward</strong> instead<br>$$<br>R_t = r_t + \gamma r_{t+1} + \gamma^{2} r_{t+2} … + \gamma^{n-t} r_n<br>$$<br>which, can be written as<br>$$<br>R_t = r_t + \gamma \ast R_{t+1}<br>$$<br>Recall the definition of Q-function (maximum discounted future reward if we choose action <strong>a</strong> in state <strong>s</strong>)<br>$$<br>Q(s_t, a_t) = max R_{t+1}<br>$$<br>therefore, we can rewrite the Q-function as below<br>$$<br>Q(s, a) = r + \gamma \ast max_{a^{‘}} Q(s^{\prime}, a^{\prime})<br>$$<br>In plain English, it means maximum future reward for this state and action (s,a) is the immediate reward r <strong>plus</strong> maximum future reward for the next state $s^{\prime}$ , action $a^{\prime}$</p><p>We could now use an iterative method to solve for the Q-function. Given a transition $(s, a, r, s^{\prime})$ , we are going to convert this episode into training set for the network. i.e. We want $r + \gamma max_a Q (s,a)$ to be equal to $Q(s,a)$. You can think of finding a Q-value is a regession task now, I have a estimator $r + \gamma max_a Q (s,a)$ and a predictor $Q(s,a)$, I can define the mean squared error (MSE), or the loss function, as below:</p><p>Define a loss function<br>$$<br>L = [r + \gamma max_{a^{\prime}} Q (s^{\prime},a^{\prime}) - Q(s,a)]^{2}<br>$$<br>Given a transition $(s, a, r, s^{\prime})$, how can I optimize my Q-function such that it returns the smallest mean squared error loss? If L getting smaller, we know the Q-function is getting converged into the optimal value, which is our “strategy book”.</p><p>Now, you might ask, where is the role of the neural network? This is where the <strong>DEEP Q-Learning</strong> comes in. You recall that $Q(s,a)$, is a stategy book, which contains millions or trillions of states and actions if you display as a table. The idea of the DQN is that I use the neural network to <strong>COMPRESS</strong> this Q-table, using some parameters \thetaθ <strong>(We called it weight in Neural Network)</strong>. So instead of handling a large table, I just need to worry the weights of the neural network. By smartly tuning the weight parameters, I can find the optimal Q-function via the various Neural Network training algorithm.<br>$$<br>Q(s,a) = f_{\theta}(s)<br>$$<br>where $f$ is our neural network with input $s$ and weight parameters $\theta$</p><p>Here is the code below to demonstrate how it works</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> t &gt; OBSERVE:</div><div class="line">    <span class="comment">#sample a minibatch to train on</span></div><div class="line">    minibatch = random.sample(D, BATCH)</div><div class="line"></div><div class="line">    inputs = np.zeros((BATCH, s_t.shape[<span class="number">1</span>], s_t.shape[<span class="number">2</span>], s_t.shape[<span class="number">3</span>]))   <span class="comment">#32, 80, 80, 4</span></div><div class="line">    targets = np.zeros((inputs.shape[<span class="number">0</span>], ACTIONS))                         <span class="comment">#32, 2</span></div><div class="line"></div><div class="line">    <span class="comment">#Now we do the experience replay</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(minibatch)):</div><div class="line">        state_t = minibatch[i][<span class="number">0</span>]</div><div class="line">        action_t = minibatch[i][<span class="number">1</span>]   <span class="comment">#This is action index</span></div><div class="line">        reward_t = minibatch[i][<span class="number">2</span>]</div><div class="line">        state_t1 = minibatch[i][<span class="number">3</span>]</div><div class="line">        terminal = minibatch[i][<span class="number">4</span>]</div><div class="line">        <span class="comment"># if terminated, only equals reward</span></div><div class="line"></div><div class="line">        inputs[i:i + <span class="number">1</span>] = state_t    <span class="comment">#I saved down s_t</span></div><div class="line"></div><div class="line">        targets[i] = model.predict(state_t)  <span class="comment"># Hitting each buttom probability</span></div><div class="line">        Q_sa = model.predict(state_t1)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> terminal:</div><div class="line">            targets[i, action_t] = reward_t</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)</div><div class="line"></div><div class="line">        loss += model.train_on_batch(inputs, targets)</div><div class="line"></div><div class="line">    s_t = s_t1</div><div class="line">    t = t + <span class="number">1</span></div></pre></td></tr></table></figure><h3 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h3><p>If you examine the code above, there is a comment called “Experience Replay”. Let me explain what it does: It was found that approximation of Q-value using non-linear functions like neural network is not very stable. The most important trick to solve this problem is called <strong>experience replay</strong>. During the gameplay all the episode $(s, a, r, s^{‘})$ are stored in replay memory <strong>D</strong>. (I use Python function deque() to store it). When training the network, random mini-batches from the replay memory are used instead of most the recent transition, which will greatly improve the stability.</p><h3 id="Exploration-vs-Exploitation"><a href="#Exploration-vs-Exploitation" class="headerlink" title="Exploration vs. Exploitation"></a>Exploration vs. Exploitation</h3><p>There is another issue in the reinforcement learning algorithm which called Exploration vs. Exploitation. How much of an agent’s time should be spent exploiting its existing known-good policy, and how much time should be focused on exploring new, possibility better, actions? We often encounter similar situations in real life too. For example, we face on which restaurant to go to on Saturday night. We all have a set of restaurants that we prefer, based on our policy/strategy book $Q(s,a)$. If we stick to our normal preference, there is a strong probability that we’ll pick a good restaurant. However, sometimes, we occasionally like to try new restaurants to see if they are better. The RL agents face the same problem. In order to maximize future reward, they need to balance the amount of time that they follow their current policy (this is called being “greedy”), and the time they spend exploring new possibilities that might be better. A popular approach is called $\epsilon$ greedy approach. Under this approach, the policy tells the agent to try a random action some percentage of the time, as defined by the variable $\epsilon$ (epsilon), which is a number between 0 and 1. The strategy will help the RL agent to occasionally try something new and see if we can achieve ultimate strategy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> random.random() &lt;= epsilon:</div><div class="line">    print(<span class="string">"----------Random Action----------"</span>)</div><div class="line">    action_index = random.randrange(ACTIONS)</div><div class="line">    a_t[action_index] = <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">    q = model.predict(s_t)       <span class="comment">#input a stack of 4 images, get the prediction</span></div><div class="line">    max_Q = np.argmax(q)</div><div class="line">    action_index = max_Q</div><div class="line">    a_t[max_Q] = <span class="number">1</span></div></pre></td></tr></table></figure><p>I think that’s it. I hope this blog will help you to understand how DQN works.</p><h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h3 id="My-training-is-very-slow"><a href="#My-training-is-very-slow" class="headerlink" title="My training is very slow"></a>My training is very slow</h3><p>You might need a GPU to accelerate the calculation. I used a TITAN X and train for at least 1 million frames to make it work</p><h1 id="Future-works-and-thoughts"><a href="#Future-works-and-thoughts" class="headerlink" title="Future works and thoughts"></a>Future works and thoughts</h1><ol><li>Current DQN depends on large experience replay. Is it possible to replace it or even remove it?</li><li>How can one decide on the optimal Convolution Neural Network?</li><li>Training is very slow, how to speed it up/to make the model converge faster?</li><li>What does the Neural Network actually learn? Is the knowledge transferable?</li></ol><p>I believe the questions are still not resolved and it’s an active research area in Machine Learning.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. <strong>Human-level Control through Deep Reinforcement Learning</strong>. Nature, 529-33, 2015.</p><h1 id="Disclaimer"><a href="#Disclaimer" class="headerlink" title="Disclaimer"></a>Disclaimer</h1><p>This work is highly based on the following repos:</p><p><a href="https://github.com/yenchenlin/DeepLearningFlappyBird" target="_blank" rel="external">https://github.com/yenchenlin/DeepLearningFlappyBird</a></p><p><a href="http://edersantana.github.io/articles/keras_rl/" target="_blank" rel="external">http://edersantana.github.io/articles/keras_rl/</a></p><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>I must thank to <a href="https://twitter.com/hardmaru" target="_blank" rel="external">@hardmaru</a> to encourage me to write this blog. I also thank to <a href="https://twitter.com/fchollet" target="_blank" rel="external">@fchollet</a> to help me on the weight initialization in Keras and <a href="https://twitter.com/edersantana" target="_blank" rel="external">@edersantana</a> his post on Keras and reinforcement learning which really help me to understand it.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;200 lines of python code to demonstrate DQN with Keras&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://yanpanlau.github.io/img/animation1.gif&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;h1 
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="deep reinforcement learning" scheme="http://yoursite.com/tags/deep-reinforcement-learning/"/>
    
      <category term="DQN" scheme="http://yoursite.com/tags/DQN/"/>
    
  </entry>
  
  <entry>
    <title>Demystifying Deep Reinforcement Learning (Repost)</title>
    <link href="http://yoursite.com/2017/07/07/Demystifying-Deep-Reinforcement-Learning/"/>
    <id>http://yoursite.com/2017/07/07/Demystifying-Deep-Reinforcement-Learning/</id>
    <published>2017-07-06T17:36:44.000Z</published>
    <updated>2017-07-06T17:45:40.451Z</updated>
    
    <content type="html"><![CDATA[<p>Two years ago, a small company in London called DeepMind uploaded their pioneering paper “<a href="http://arxiv.org/abs/1312.5602" target="_blank" rel="external">Playing Atari with Deep Reinforcement Learning</a>” to Arxiv. In this paper they demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable, because the games and the goals in every game were very different and designed to be challenging for humans. The same model architecture, without any change, was used to learn seven different games, and in three of them the algorithm performed even better than a human!</p><p>It has been hailed since then as the first step towards <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank" rel="external">general artificial intelligence</a> – an AI that can survive in a variety of environments, instead of being confined to strict realms such as playing chess. No wonder <a href="http://techcrunch.com/2014/01/26/google-deepmind/" target="_blank" rel="external">DeepMind was immediately bought by Google</a> and has been on the forefront of deep learning research ever since. In February 2015 their paper “<a href="http://www.nature.com/articles/nature14236" target="_blank" rel="external">Human-level control through deep reinforcement learning</a>” was featured on the cover of Nature, one of the most prestigious journals in science. In this paper they applied the same model to 49 different games and achieved superhuman performance in half of them.</p><p>Still, while deep models for supervised and unsupervised learning have seen widespread adoption in the community, deep reinforcement learning has remained a bit of a mystery. In this blog post I will be trying to demystify this technique and understand the rationale behind it. The intended audience is someone who already has background in machine learning and possibly in neural networks, but hasn’t had time to delve into reinforcement learning yet.</p><p>The roadmap ahead:</p><ol><li><strong>What are the main challenges in reinforcement learning?</strong> We will cover the credit assignment problem and the exploration-exploitation dilemma here.</li><li><strong>How to formalize reinforcement learning in mathematical terms?</strong> We will define Markov Decision Process and use it for reasoning about reinforcement learning.</li><li><strong>How do we form long-term strategies?</strong> We define “discounted future reward”, that forms the main basis for the algorithms in the next sections.</li><li><strong>How can we estimate or approximate the future reward?</strong> Simple table-based Q-learning algorithm is defined and explained here.</li><li><strong>What if our state space is too big?</strong> Here we see how Q-table can be replaced with a (deep) neural network.</li><li><strong>What do we need to make it actually work?</strong> Experience replay technique will be discussed here, that stabilizes the learning with neural networks.</li><li><strong>Are we done yet?</strong> Finally we will consider some simple solutions to the exploration-exploitation problem.</li></ol><h1 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h1><p>Consider the game Breakout. In this game you control a paddle at the bottom of the screen and have to bounce the ball back to clear all the bricks in the upper half of the screen. Each time you hit a brick, it disappears and your score increases – you get a reward.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.08.53-AM.png" alt="img">Figure 1: Atari Breakout game. Image credit: DeepMind.</p><p>Suppose you want to teach a neural network to play this game. Input to your network would be screen images, and output would be three actions: left, right or fire (to launch the ball). It would make sense to treat it as a classification problem – for each game screen you have to decide, whether you should move left, right or press fire. Sounds straightforward? Sure, but then you need training examples, and a lots of them. Of course you could go and record game sessions using expert players, but that’s not really how we learn. We don’t need somebody to tell us a million times which move to choose at each screen. We just need occasional feedback that we did the right thing and can then figure out everything else ourselves.</p><p>This is the task <strong>reinforcement learning </strong>tries to solve. Reinforcement learning lies somewhere in between supervised and unsupervised learning. Whereas in supervised learning one has a target label for each training example and in unsupervised learning one has no labels at all, in reinforcement learning one has sparse and time-delayed labels – the rewards. Based only on those rewards the agent has to learn to behave in the environment.</p><p>While the idea is quite intuitive, in practice there are numerous challenges. For example when you hit a brick and score a reward in the Breakout game, it often has nothing to do with the actions (paddle movements) you did just before getting the reward. All the hard work was already done, when you positioned the paddle correctly and bounced the ball back. This is called the <strong>credit assignment problem</strong> – i.e., which of the preceding actions was responsible for getting the reward and to what extent.</p><p>Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards? In the above Breakout game a simple strategy is to move to the left edge and wait there. When launched, the ball tends to fly left more often than right and you will easily score about 10 points before you die. Will you be satisfied with this or do you want more? This is called the <strong>explore-exploit dilemma</strong> – should you exploit the known working strategy or explore other, possibly better strategies.</p><p>Reinforcement learning is an important model of how we (and all animals in general) learn. Praise from our parents, grades in school, salary at work – these are all examples of rewards. Credit assignment problems and exploration-exploitation dilemmas come up every day both in business and in relationships. That’s why it is important to study this problem, and games form a wonderful sandbox for trying out new approaches.</p><h1 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h1><p>Now the question is, how do you formalize a reinforcement learning problem, so that you can reason about it? The most common method is to represent it as a Markov decision process.</p><p>Suppose you are an <strong>agent</strong>, situated in an <strong>environment</strong> (e.g. Breakout game). The environment is in a certain <strong>state</strong>(e.g. location of the paddle, location and direction of the ball, existence of every brick and so on). The agent can perform certain <strong>actions</strong> in the environment (e.g. move the paddle to the left or to the right). These actions sometimes result in a <strong>reward</strong> (e.g. increase in score). Actions transform the environment and lead to a new state, where the agent can perform another action, and so on. The rules for how you choose those actions are called <strong>policy</strong>. The environment in general is stochastic, which means the next state may be somewhat random (e.g. when you lose a ball and launch a new one, it goes towards a random direction).</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-12.01.04-PM.png" alt="img">Figure 2: <em>Left: </em>reinforcement learning problem. <em>Right: </em>Markov decision process.</p><p>The set of states and actions, together with rules for transitioning from one state to another, make up a <strong>Markov decision process</strong>. One <strong>episode</strong> of this process (e.g. one game) forms a finite sequence of states, actions and rewards:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.19-AM.png" alt="Screen Shot 2015-12-21 at 11.09.19 AM"></p><p>Here si represents the state, ai is the action and ri+1 is the reward after performing the action. The episode ends with <strong>terminal</strong> state sn (e.g. “game over” screen). A Markov decision process relies on the Markov assumption, that the probability of the next state si+1 depends only on current state si and action ai, but not on preceding states or actions.</p><h1 id="Discounted-Future-Reward"><a href="#Discounted-Future-Reward" class="headerlink" title="Discounted Future Reward"></a>Discounted Future Reward</h1><p>To perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. How should we go about that?</p><p>Given one run of the Markov decision process, we can easily calculate the <strong>total reward</strong> for one episode:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.26-AM.png" alt="Screen Shot 2015-12-21 at 11.09.26 AM"></p><p>Given that, the <strong>total future reward</strong> from time point <em>t</em> onward can be expressed as:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.32-AM.png" alt="Screen Shot 2015-12-21 at 11.09.32 AM"></p><p>But because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use <strong>discounted future reward </strong>instead:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.36-AM.png" alt="Screen Shot 2015-12-21 at 11.09.36 AM"></p><p>Here <em>γ</em> is the discount factor between 0 and 1 – the more into the future the reward is, the less we take it into consideration. It is easy to see, that discounted future reward at time step <em>t</em> can be expressed in terms of the same thing at time step <em>t+1</em>:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.40-AM.png" alt="Screen Shot 2015-12-21 at 11.09.40 AM"></p><p>If we set the discount factor <em>γ</em>=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like <em>γ=</em>0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor <em>γ</em>=1.</p><p>A good strategy for an agent would be to <strong>always choose an action that maximizes the (discounted) future reward</strong>.</p><h1 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h1><p>In Q-learning we define a function <em>Q(s, a)</em> representing <strong>the maximum discounted future reward when we perform action </strong>a<strong> in state </strong>s<strong>, and continue optimally from that point on.</strong></p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.47-AM.png" alt="Screen Shot 2015-12-21 at 11.09.47 AM"></p><p>The way to think about <em>Q(s, a)</em> is that it is “the best possible score at the end of the game after performing action a<strong>in state </strong>s<strong>“. It is called Q-function, because it represents the “quality” of a certain action in a given state.</strong></p><p>This may sound like quite a puzzling definition. How can we estimate the score at the end of game, if we know just the current state and action, and not the actions and rewards coming after that? We really can’t. But as a theoretical construct we can assume existence of such a function. Just close your eyes and repeat to yourself five times: “<em>Q(s, a) </em>exists, <em>Q(s, a) </em>exists, …”. Feel it?</p><p>If you’re still not convinced, then consider what the implications of having such a function would be. Suppose you are in state and pondering whether you should take action <em>a</em> or <em>b</em>. You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer becomes really simple – pick the action with the highest Q-value!</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.56-AM.png" alt="Screen Shot 2015-12-21 at 11.09.56 AM"></p><p>Here π represents the policy, the rule how we choose an action in each state.</p><p>OK, how do we get that Q-function then? Let’s focus on just one transition &lt;<em>s, a, r, s’</em>&gt;. Just like with discounted future rewards in the previous section, we can express the Q-value of state <em>s</em> and action <em>a</em> in terms of the Q-value of the next state <em>s’</em>.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.10.00-AM.png" alt="Screen Shot 2015-12-21 at 11.10.00 AM">This is called the <strong>Bellman equation</strong>. If you think about it, it is quite logical – maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state.</p><p>The main idea in Q-learning is that <strong>we can iteratively approximate the Q-function using the Bellman equation</strong>. In the simplest case the Q-function is implemented as a table, with states as rows and actions as columns. The gist of the Q-learning algorithm is as simple as the following<a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/#_ftn1" target="_blank" rel="external">[1]</a>:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.55-AM.png" alt="Screen Shot 2015-12-21 at 11.23.55 AM"></p><p><em>α</em> in the algorithm is a learning rate that controls how much of the difference between previous Q-value and newly proposed Q-value is taken into account. In particular, when <em>α</em>=1, then two <em>Q[s,a]</em> cancel and the update is exactly the same as the Bellman equation.</p><p>The max<em>a’</em> <em>Q</em>[<em>s’</em>,<em>a’</em>] that we use to update <em>Q</em>[<em>s</em>,<em>a</em>] is only an approximation and in early stages of learning it may be completely wrong. However the approximation get more and more accurate with every iteration and <a href="http://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/55/2015/12/ProofQlearning.pdf" target="_blank" rel="external">it has been shown</a>, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.</p><h1 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a>Deep Q Network</h1><p>The state of the environment in the Breakout game can be defined by the location of the paddle, location and direction of the ball and the presence or absence of each individual brick. This intuitive representation however is game specific. Could we come up with something more universal, that would be suitable for all the games? The obvious choice is screen pixels – they implicitly contain all of the relevant information about the game situation, except for the speed and direction of the ball. Two consecutive screens would have these covered as well.</p><p>If we apply the same preprocessing to game screens as in the DeepMind paper – take the four last screen images, resize them to 84×84 and convert to grayscale with 256 gray levels – we would have 25684x84x4 ≈ 1067970 possible game states. This means 1067970 rows in our imaginary Q-table – more than the number of atoms in the known universe! One could argue that many pixel combinations (and therefore states) never occur – we could possibly represent it as a sparse table containing only visited states. Even so, most of the states are very rarely visited and it would take a lifetime of the universe for the Q-table to converge. Ideally, we would also like to have a good guess for Q-values for states we have never seen before.</p><p>This is the point where deep learning steps in. Neural networks are exceptionally good at coming up with good features for highly structured data. We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. This approach has the advantage, that if we want to perform a Q-value update or pick the action with the highest Q-value, we only have to do one forward pass through the network and have all Q-values for all actions available immediately.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.27.12-AM.png" alt="img"></p><p>Figure 3: <em>Left: </em>Naive formulation of deep Q-network. <em>Right: </em>More optimized architecture of deep Q-network, used in DeepMind paper.</p><p>The network architecture that DeepMind used is as follows:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.28-AM.png" alt="Screen Shot 2015-12-21 at 11.23.28 AM"></p><p>This is a classical convolutional neural network with three convolutional layers, followed by two fully connected layers. People familiar with object recognition networks may notice that there are no pooling layers. But if you really think about it, pooling layers buy you translation invariance – the network becomes insensitive to the location of an object in the image. That makes perfectly sense for a classification task like ImageNet, but for games the location of the ball is crucial in determining the potential reward and we wouldn’t want to discard this information!</p><p>Input to the network are four 84×84 grayscale game screens. Outputs of the network are Q-values for each possible action (18 in Atari). Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/formula.png" alt="img"></p><p>Given a transition &lt;<em> s, a, r, s’</em> &gt;, the Q-table update rule in the previous algorithm must be replaced with the following:</p><ol><li>Do a feedforward pass for the current state <em>s</em> to get predicted Q-values for all actions.</li><li>Do a feedforward pass for the next state <em>s’ </em>and calculate maximum overall network outputs <em>max a’ Q(s’, a’).</em></li><li>Set Q-value target for action to <em>r + γmax a’ Q(s’, a’)</em> (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.</li><li>Update the weights using backpropagation.</li></ol><h1 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h1><p>By now we have an idea how to estimate the future reward in each state using Q-learning and approximate the Q-function using a convolutional neural network. But it turns out that approximation of Q-values using non-linear functions is not very stable. There is a whole bag of tricks that you have to use to actually make it converge. And it takes a long time, almost a week on a single GPU.</p><p>The most important trick is <strong>experience replay</strong>. During gameplay all the experiences &lt;<em> s, a, r, s’</em> &gt; are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.</p><h1 id="Exploration-Exploitation"><a href="#Exploration-Exploitation" class="headerlink" title="Exploration-Exploitation"></a>Exploration-Exploitation</h1><p>Q-learning attempts to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward. But we haven’t touched the exploration-exploitation dilemma yet…</p><p>Firstly observe, that when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude “exploration”. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is “greedy”, it settles with the first effective strategy it finds.</p><p>A simple and effective fix for the above problem is <strong>ε-greedy exploration</strong> – with probability <em>ε</em> choose a random action, otherwise go with the “greedy” action with the highest Q-value. In their system DeepMind actually decreases <em>ε</em> over time from 1 to 0.1 – in the beginning the system makes completely random moves to explore the state space maximally, and then it settles down to a fixed exploration rate.</p><h1 id="Deep-Q-learning-Algorithm"><a href="#Deep-Q-learning-Algorithm" class="headerlink" title="Deep Q-learning Algorithm"></a>Deep Q-learning Algorithm</h1><p>This gives us the final deep Q-learning algorithm with experience replay:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.43-AM-1.png" alt="Screen Shot 2015-12-21 at 11.23.43 AM"></p><p>There are many more tricks that DeepMind used to actually make it work – like target network, error clipping, reward clipping etc, but these are out of scope for this introduction.</p><p>The most amazing part of this algorithm is that it learns anything at all. Just think about it – because our Q-function is initialized randomly, it initially outputs complete garbage. And we are using this garbage (the maximum Q-value of the next state) as targets for the network, only occasionally folding in a tiny reward. That sounds insane, how could it learn anything meaningful at all? The fact is, that it does.</p><h1 id="Final-notes"><a href="#Final-notes" class="headerlink" title="Final notes"></a>Final notes</h1><p>Many improvements to deep Q-learning have been proposed since its first introduction – <a href="http://arxiv.org/abs/1509.06461" target="_blank" rel="external">Double Q-learning</a>, <a href="http://arxiv.org/abs/1511.05952" target="_blank" rel="external">Prioritized Experience Replay</a>, <a href="http://arxiv.org/abs/1511.06581" target="_blank" rel="external">Dueling Network Architecture</a> and <a href="http://arxiv.org/abs/1509.02971" target="_blank" rel="external">extension to continuous action space</a> to name a few. For latest advancements check out the <a href="http://rll.berkeley.edu/deeprlworkshop/" target="_blank" rel="external">NIPS 2015 deep reinforcement learning workshop</a> and <a href="https://cmt.research.microsoft.com/ICLR2016Conference/Protected/PublicComment.aspx" target="_blank" rel="external">ICLR 2016</a>(search for “reinforcement” in title). But beware, that <a href="http://www.google.com/patents/US20150100530" target="_blank" rel="external">deep Q-learning has been patented by Google</a>.</p><p>It is often said, that artificial intelligence is something we haven’t figured out yet. Once we know how it works, it doesn’t seem intelligent any more. But deep Q-networks still continue to amaze me. Watching them figure out a new game is like observing an animal in the wild – a rewarding experience by itself.</p><h1 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a>Credits</h1><p>Thanks to Ardi Tampuu, Tanel Pärnamaa, Jaan Aru, Ilya Kuzovkin, Arjun Bansal and Urs Köster for comments and suggestions on the drafts of this post.</p><h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><ul><li><a href="http://videolectures.net/rldm2015_silver_reinforcement_learning/" target="_blank" rel="external">David Silver’s lecture about deep reinforcement learning</a></li><li><a href="https://www.youtube.com/watch?v=b1a53hE0yQs" target="_blank" rel="external">Slightly awkward but accessible illustration of Q-learning</a></li><li><a href="http://rll.berkeley.edu/deeprlcourse/" target="_blank" rel="external">UC Berkley’s course on deep reinforcement learning</a></li><li><a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="external">David Silver’s reinforcement learning course</a></li><li><a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" target="_blank" rel="external">Nando de Freitas’ course on machine learning</a> (two lectures about reinforcement learning in the end)</li><li><a href="http://cs231n.github.io/" target="_blank" rel="external">Andrej Karpathy’s course on convolutional neural networks</a></li></ul><p><a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/#_ftnref1" target="_blank" rel="external">[1]</a> Algorithm adapted from <a href="http://artint.info/html/ArtInt_265.html" target="_blank" rel="external">http://artint.info/html/ArtInt_265.html</a><br>This blog was first published at: <a href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/</a></p><h4 id="This-is-the-part-1-of-my-series-on-deep-reinforcement-learning-Tune-in-next-week-for-“Deep-Reinforcement-Learning-with-Neon”-for-an-actual-implementation-with-Neon-deep-learning-toolkit"><a href="#This-is-the-part-1-of-my-series-on-deep-reinforcement-learning-Tune-in-next-week-for-“Deep-Reinforcement-Learning-with-Neon”-for-an-actual-implementation-with-Neon-deep-learning-toolkit" class="headerlink" title="This is the part 1 of my series on deep reinforcement learning. Tune in next week for “Deep Reinforcement Learning with Neon” for an actual implementation with Neon deep learning toolkit."></a>This is the part 1 of my series on deep reinforcement learning. Tune in next week for <a href="https://www.intelnervana.com/deep-reinforcement-learning-with-neon/" target="_blank" rel="external">“Deep Reinforcement Learning with Neon”</a> for an actual implementation with <a href="https://github.com/NervanaSystems/neon" target="_blank" rel="external">Neon</a> deep learning toolkit.</h4>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Two years ago, a small company in London called DeepMind uploaded their pioneering paper “&lt;a href=&quot;http://arxiv.org/abs/1312.5602&quot; target
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="deep reinforcement learning" scheme="http://yoursite.com/tags/deep-reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>On-policy Control with Approximation</title>
    <link href="http://yoursite.com/2017/07/06/On-policy-Control-with-Approximation/"/>
    <id>http://yoursite.com/2017/07/06/On-policy-Control-with-Approximation/</id>
    <published>2017-07-06T09:41:34.000Z</published>
    <updated>2017-07-06T17:06:06.685Z</updated>
    
    <content type="html"><![CDATA[<p>In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a finite-dimensional weight vector.</p><h3 id="Episodic-Semi-gradient-Control"><a href="#Episodic-Semi-gradient-Control" class="headerlink" title="Episodic Semi-gradient Control"></a>Episodic Semi-gradient Control</h3><p>The general gradient-descent update for action-value prediction is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>For example, the update for the one-step Sarsa method is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t)- \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>We call this method <strong>episode semi-gradient one-step sarsa</strong>.</p><p>To form control methods, we need to couple such action-value prediction methods with techniques for policy improvement and action selection. Suitable techniques applicable to continuous actions, or to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution. On the other hand, if the action set is discrete and not too large, then we can use the techniques already developed in previous chapters.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/episode-semi-grad-sarsa.png" alt="episode-semi-grad-sarsa"></p><h4 id="Example-Mountain-Car-Task"><a href="#Example-Mountain-Car-Task" class="headerlink" title="Example: Mountain-Car Task"></a>Example: Mountain-Car Task</h4><p>Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram in the below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mountain-car.png" alt="mountain-car"></p><p>The diﬃculty is that gravity is stronger than the car’s engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to ﬁrst move away from the goal and up the opposite slope on the left. Then, by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way. This is a simple example of a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. Many control methodologies have great diﬃculties with tasks of this kind unless explicitly aided by a human designer.</p><p>The reward in this problem is −1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (−1), and zero throttle (0). The car moves according to a simpliﬁed physics. Its position, $x_t$, and velocity, $\dot{x}_t$, are updated by<br>$$<br>\begin{align}<br>x_{t+1} &amp;\doteq bound \big[x_t + \dot{x}_{t+1} \big] \\<br>\dot{x}_{t+1} &amp;\doteq bound \big[\dot{x}_t + 0.001 A_t - 0.0025 \cos(3x_t) \big],<br>\end{align}<br>$$<br>where the bound operation enforces $-1.2 \leq x_{t+1} \leq 0.5 \;\; \text{and} \; -0.07 \leq \dot{x}_{t+1} \leq 0.07$. In addition, when $x_{t+1}$ reached the left bound, $\dot{x}_{t+1}$ was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position $x_t \in [−0.6, −0.4)$ and zero velocity. To convert the two continuous state variables to binary features, we used grid-tilings.</p><p>First of all, we define the environment of this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all possible actions</span></div><div class="line">ACTION_REVERSE = <span class="number">-1</span></div><div class="line">ACTION_ZERO = <span class="number">0</span></div><div class="line">ACTION_FORWARD = <span class="number">1</span></div><div class="line"><span class="comment"># order is important</span></div><div class="line">ACTIONS = [ACTION_REVERSE, ACTION_ZERO, ACTION_FORWARD]</div><div class="line"></div><div class="line"><span class="comment"># bound for position and velocity</span></div><div class="line">POSITION_MIN = <span class="number">-1.2</span></div><div class="line">POSITION_MAX = <span class="number">0.5</span></div><div class="line">VELOCITY_MIN = <span class="number">-0.07</span></div><div class="line">VELOCITY_MAX = <span class="number">0.07</span></div><div class="line"></div><div class="line"><span class="comment"># use optimistic initial value, so it's ok to set epsilon to 0</span></div><div class="line">EPSILON = <span class="number">0</span></div></pre></td></tr></table></figure><p>After take an action, we transition to a new state and get a reward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take an @action at @position and @velocity</span></div><div class="line"><span class="comment"># @return: new position, new velocity, reward (always -1)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(position, velocity, action)</span>:</span></div><div class="line">    newVelocity = velocity + <span class="number">0.001</span> * action - <span class="number">0.0025</span> * np.cos(<span class="number">3</span> * position)</div><div class="line">    newVelocity = min(max(VELOCITY_MIN, newVelocity), VELOCITY_MAX)</div><div class="line">    newPosition = position + newVelocity</div><div class="line">    newPosition = min(max(POSITION_MIN, newPosition), POSITION_MAX)</div><div class="line">    reward = <span class="number">-1.0</span></div><div class="line">    <span class="keyword">if</span> newPosition == POSITION_MIN:</div><div class="line">        newVelocity = <span class="number">0.0</span></div><div class="line">    <span class="keyword">return</span> newPosition, newVelocity, reward</div></pre></td></tr></table></figure><p>The $\varepsilon$-greedy policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get action at @position and @velocity based on epsilon greedy policy and @valueFunction</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(position, velocity, valueFunction)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(ACTIONS)</div><div class="line">    values = []</div><div class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">        values.append(valueFunction.value(position, velocity, action))</div><div class="line">    <span class="keyword">return</span> np.argmax(values) - <span class="number">1</span></div></pre></td></tr></table></figure><p>We need map out continuous state to discrete state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># wrapper class for state action value function</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValueFunction</span>:</span></div><div class="line">    <span class="comment"># In this example I use the tiling software instead of implementing standard tiling by myself</span></div><div class="line">    <span class="comment"># One important thing is that tiling is only a map from (state, action) to a series of indices</span></div><div class="line">    <span class="comment"># It doesn't matter whether the indices have meaning, only if this map satisfy some property</span></div><div class="line">    <span class="comment"># View the following webpage for more information</span></div><div class="line">    <span class="comment"># http://incompleteideas.net/sutton/tiles/tiles3.html</span></div><div class="line">    <span class="comment"># @maxSize: the maximum # of indices</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, stepSize, numOfTilings=<span class="number">8</span>, maxSize=<span class="number">2048</span>)</span>:</span></div><div class="line">        self.maxSize = maxSize</div><div class="line">        self.numOfTilings = numOfTilings</div><div class="line"></div><div class="line">        <span class="comment"># divide step size equally to each tiling</span></div><div class="line">        self.stepSize = stepSize / numOfTilings</div><div class="line"></div><div class="line">        self.hashTable = IHT(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># weight for each tile</span></div><div class="line">        self.weights = np.zeros(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># position and velocity needs scaling to satisfy the tile software</span></div><div class="line">        self.positionScale = self.numOfTilings / (POSITION_MAX - POSITION_MIN)</div><div class="line">        self.velocityScale = self.numOfTilings / (VELOCITY_MAX - VELOCITY_MIN)</div><div class="line"></div><div class="line">    <span class="comment"># get indices of active tiles for given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getActiveTiles</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="comment"># I think positionScale * (position - position_min) would be a good normalization.</span></div><div class="line">        <span class="comment"># However positionScale * position_min is a constant, so it's ok to ignore it.</span></div><div class="line">        activeTiles = tiles(self.hashTable, self.numOfTilings,</div><div class="line">                            [self.positionScale * position, self.velocityScale * velocity],</div><div class="line">                            [action])</div><div class="line">        <span class="keyword">return</span> activeTiles</div><div class="line"></div><div class="line">    <span class="comment"># estimate the value of given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="keyword">if</span> position == POSITION_MAX:</div><div class="line">            <span class="keyword">return</span> <span class="number">0.0</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        <span class="keyword">return</span> np.sum(self.weights[activeTiles])</div><div class="line"></div><div class="line">    <span class="comment"># learn with given state, action and target</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, position, velocity, action, target)</span>:</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        estimation = np.sum(self.weights[activeTiles])</div><div class="line">        delta = self.stepSize * (target - estimation)</div><div class="line">        <span class="keyword">for</span> activeTile <span class="keyword">in</span> activeTiles:</div><div class="line">            self.weights[activeTile] += delta</div><div class="line"></div><div class="line">    <span class="comment"># get # of steps to reach the goal under current state value function</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">costToGo</span><span class="params">(self, position, velocity)</span>:</span></div><div class="line">        costs = []</div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">            costs.append(self.value(position, velocity, action))</div><div class="line">        <span class="keyword">return</span> -np.max(costs)</div></pre></td></tr></table></figure><p>Because the one-step semi-gradient sarsa is a special case of n-step, so we develop the n-step algorithm<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \; n \geq 1,0 \leq t \leq T-n.<br>$$<br>The n-step equation is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ G_{t:t+n}- \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}), \;\;\; 0 \leq t \leq T.<br>$$<br>Complete pseudocode is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/n-step-sg-sarsa.png" alt="n-step-sg-sarsa"></p><p>So the code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># semi-gradient n-step Sarsa</span></div><div class="line"><span class="comment"># @valueFunction: state value function to learn</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">semiGradientNStepSarsa</span><span class="params">(valueFunction, n=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="comment"># start at a random position around the bottom of the valley</span></div><div class="line">    currentPosition = np.random.uniform(<span class="number">-0.6</span>, <span class="number">-0.4</span>)</div><div class="line">    <span class="comment"># initial velocity is 0</span></div><div class="line">    currentVelocity = <span class="number">0.0</span></div><div class="line">    <span class="comment"># get initial action</span></div><div class="line">    currentAction = getAction(currentPosition, currentVelocity, valueFunction)</div><div class="line"></div><div class="line">    <span class="comment"># track previous position, velocity, action and reward</span></div><div class="line">    positions = [currentPosition]</div><div class="line">    velocities = [currentVelocity]</div><div class="line">    actions = [currentAction]</div><div class="line">    rewards = [<span class="number">0.0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># take current action and go to the new state</span></div><div class="line">            newPostion, newVelocity, reward = takeAction(currentPosition, currentVelocity, currentAction)</div><div class="line">            <span class="comment"># choose new action</span></div><div class="line">            newAction = getAction(newPostion, newVelocity, valueFunction)</div><div class="line"></div><div class="line">            <span class="comment"># track new state and action</span></div><div class="line">            positions.append(newPostion)</div><div class="line">            velocities.append(newVelocity)</div><div class="line">            actions.append(newAction)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newPostion == POSITION_MAX:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += rewards[t]</div><div class="line">            <span class="comment"># add estimated state action value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += valueFunction.value(positions[updateTime + n],</div><div class="line">                                               velocities[updateTime + n],</div><div class="line">                                               actions[updateTime + n])</div><div class="line">            <span class="comment"># update the state value function</span></div><div class="line">            <span class="keyword">if</span> positions[updateTime] != POSITION_MAX:</div><div class="line">                valueFunction.learn(positions[updateTime], velocities[updateTime], actions[updateTime], returns)</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentPosition = newPostion</div><div class="line">        currentVelocity = newVelocity</div><div class="line">        currentAction = newAction</div><div class="line"></div><div class="line">    <span class="keyword">return</span> time</div></pre></td></tr></table></figure><p>Next, we use the method mentioned earlier to solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">episodes = <span class="number">9000</span></div><div class="line">targetEpisodes = [<span class="number">1</span><span class="number">-1</span>, <span class="number">12</span><span class="number">-1</span>, <span class="number">104</span><span class="number">-1</span>, <span class="number">1000</span><span class="number">-1</span>, episodes - <span class="number">1</span>]</div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alpha = <span class="number">0.3</span></div><div class="line">valueFunction = ValueFunction(alpha, numOfTilings)</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">    print(<span class="string">'episode:'</span>, episode)</div><div class="line">    semiGradientNStepSarsa(valueFunction)</div><div class="line">    <span class="keyword">if</span> episode <span class="keyword">in</span> targetEpisodes:</div><div class="line">        prettyPrint(valueFunction, <span class="string">'Episode: '</span> + str(episode + <span class="number">1</span>))</div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa.png" alt="mcar-sg-sarsa"></p><p>The result shows what typically happens while learning to solve this task with this form of function approximation. Shown is the negative of the value function (the cost-to-go function) learned on a single run. The initial action values were all zero, which was optimistic (all true values are negative in this task), causing extensive exploration to occur even though the exploration parameter, $\varepsilon$, was 0. All the states visited frequently are valued worse than unexplored states, because the actual rewards have been worse than what was (unrealistically) expected. This continually drives the agent away from wherever it has been, to explore new states, until a solution is found.</p><p>Next, let us test the performance of various step size (learning rate).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'alpha:'</span>, alphas[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'alpha = '</span>+str(alphas[i])+<span class="string">'/'</span>+str(numOfTilings))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha.png" alt="mcar-sg-sarsa-var-alpha"></p><p>And then, let us test the performance of one-step sarsa and multi-step sarsa on the Mountain Car task:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.5</span>, <span class="number">0.3</span>]</div><div class="line">nSteps = [<span class="number">1</span>, <span class="number">8</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nSteps[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index], nSteps[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-n.png" alt="mcar-sg-sarsa-var-n"></p><p>Finally, let me shows the results of a more detailed study of the eﬀect of the parameters $\alpha$ and $n$ on the rate of learning on this task.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">alphas = np.arange(<span class="number">0.25</span>, <span class="number">1.75</span>, <span class="number">0.25</span>)</div><div class="line">nSteps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">5</span>))</div><div class="line">episodes = <span class="number">50</span></div><div class="line">runs = <span class="number">5</span></div><div class="line"></div><div class="line">truncateStep = <span class="number">300</span></div><div class="line">steps = np.zeros((len(nSteps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> nStepIndex, nStep <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(nSteps)), nSteps):</div><div class="line">        <span class="keyword">for</span> alphaIndex, alpha <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(alphas)), alphas):</div><div class="line">            <span class="keyword">if</span> (nStep == <span class="number">8</span> <span class="keyword">and</span> alpha &gt; <span class="number">1</span>) <span class="keyword">or</span> \</div><div class="line">                    (nStep == <span class="number">16</span> <span class="keyword">and</span> alpha &gt; <span class="number">0.75</span>):</div><div class="line">                <span class="comment"># In these cases it won't converge, so ignore them</span></div><div class="line">                steps[nStepIndex, alphaIndex] += truncateStep * episodes</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            valueFunction = ValueFunction(alpha)</div><div class="line">            <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nStep, <span class="string">'alpha:'</span>, alpha, <span class="string">'episode:'</span>, episode)</div><div class="line">                step = semiGradientNStepSarsa(valueFunction, nStep)</div><div class="line">                steps[nStepIndex, alphaIndex] += step</div><div class="line"><span class="comment"># average over independent runs and episodes</span></div><div class="line">steps /= runs * episodes</div><div class="line"><span class="comment"># truncate high values for better display</span></div><div class="line">steps[steps &gt; truncateStep] = truncateStep</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(nSteps)):</div><div class="line">    plt.plot(alphas, steps[i, :], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha * number of tilings(8)'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha-n.png" alt="mcar-sg-sarsa-var-alpha-n"></p><h3 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h3><h4 id="Use-OpenAI-gym"><a href="#Use-OpenAI-gym" class="headerlink" title="Use OpenAI gym"></a>Use OpenAI gym</h4><p>Now, let us use the OpenAI gym toolkit to simply our Mountain Car task. As we know, we need to define the environment of task before develop the detail algorithm. It’s easy to make mistakes with some detail. So it is fantastic if we do not need to care about that. The gym toolkit help us to define the environment. Such as the Mountain Car task, there is a build-in model in the gym toolkit. We just need to write one row code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env = gym.envs.make(<span class="string">"MountainCar-v0"</span>)</div></pre></td></tr></table></figure><p>That is amazing!</p><p>We also can test the environment very convenience and get a pretty good user graphic:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">env.reset()</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line"><span class="comment"># for x in range(10000):</span></div><div class="line"><span class="comment">#     env.step(0)</span></div><div class="line"><span class="comment">#     plt.figure()</span></div><div class="line"><span class="comment">#     plt.imshow(env.render(mode='rgb_array'))  </span></div><div class="line">[env.step(<span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)]</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">env.render(close=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>These codes will return the result as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-gym-test.png" alt="mcar-gym-test"></p><p>Bravo~</p><p>Now, let us solve the task by to use the Q-Learning algorithm. Meanwhile, we will use the RBF kernel to construct the features and use the SGDRegressor gradient-descent method of scikit-learn package to update $\mathbf{w}$.</p><p>First of all, we need to normalized our features to accelerate the convergence speed and use the RBF kernel to reconstruct our feature space. The both methods (Normalization and Reconstruction) need to use some training set to train a model. Then we use this model to transform our raw data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Feature Preprocessing: Normalize to zero mean and unit variance</span></div><div class="line"><span class="comment"># We use a few samples from the observation space to do this</span></div><div class="line">observation_examples = np.array([env.observation_space.sample() <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)])</div><div class="line">scaler = sklearn.preprocessing.StandardScaler()</div><div class="line">scaler.fit(observation_examples)</div><div class="line"></div><div class="line"><span class="comment"># Used to converte a state to a featurizes represenation.</span></div><div class="line"><span class="comment"># We use RBF kernels with different variances to cover different parts of the space</span></div><div class="line">featurizer = sklearn.pipeline.FeatureUnion([</div><div class="line">        (<span class="string">"rbf1"</span>, RBFSampler(gamma=<span class="number">5.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf2"</span>, RBFSampler(gamma=<span class="number">2.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf3"</span>, RBFSampler(gamma=<span class="number">1.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf4"</span>, RBFSampler(gamma=<span class="number">0.5</span>, n_components=<span class="number">100</span>))</div><div class="line">        ])</div><div class="line">featurizer.fit(scaler.transform(observation_examples))</div></pre></td></tr></table></figure><p>Next, we define a class named Estimator to simply the gradient descent process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Estimator</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Value Function approximator. </div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># We create a separate model for each action in the environment's</span></div><div class="line">        <span class="comment"># action space. Alternatively we could somehow encode the action</span></div><div class="line">        <span class="comment"># into the features, but this way it's easier to code up.</span></div><div class="line">        self.models = []</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(env.action_space.n):</div><div class="line">            model = SGDRegressor(learning_rate=<span class="string">"constant"</span>)</div><div class="line">            <span class="comment"># We need to call partial_fit once to initialize the model</span></div><div class="line">            <span class="comment"># or we get a NotFittedError when trying to make a prediction</span></div><div class="line">            <span class="comment"># This is quite hacky.</span></div><div class="line">            model.partial_fit([self.featurize_state(env.reset())], [<span class="number">0</span>])</div><div class="line">            self.models.append(model)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">featurize_state</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Returns the featurized representation for a state.</div><div class="line">        """</div><div class="line">        scaled = scaler.transform([state])</div><div class="line">        featurized = featurizer.transform(scaled)</div><div class="line">        <span class="keyword">return</span> featurized[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, s, a=None)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Makes value function predictions.</div><div class="line">        </div><div class="line">        Args:</div><div class="line">            s: state to make a prediction for</div><div class="line">            a: (Optional) action to make a prediction for</div><div class="line">            </div><div class="line">        Returns</div><div class="line">            If an action a is given this returns a single number as the prediction.</div><div class="line">            If no action is given this returns a vector or predictions for all actions</div><div class="line">            in the environment where pred[i] is the prediction for action i.</div><div class="line">            </div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> a:</div><div class="line">            <span class="keyword">return</span> np.array([m.predict([features])[<span class="number">0</span>] <span class="keyword">for</span> m <span class="keyword">in</span> self.models])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self.models[a].predict([features])[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, s, a, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Updates the estimator parameters for a given state and action towards</div><div class="line">        the target y.</div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        self.models[a].partial_fit([features], [y])</div></pre></td></tr></table></figure><p>We also need a $\varepsilon$-greedy policy to select action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(estimator, epsilon, nA)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        estimator: An estimator that returns q values for a given state</div><div class="line">        epsilon: The probability to select a random action . float between 0 and 1.</div><div class="line">        nA: Number of actions in the environment.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A function that takes the observation as an argument and returns</div><div class="line">        the probabilities for each action in the form of a numpy array of length nA.</div><div class="line">    </div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></div><div class="line">        A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">        q_values = estimator.predict(observation)</div><div class="line">        best_action = np.argmax(q_values)</div><div class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">        <span class="keyword">return</span> A</div><div class="line">    <span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Then we develop the Q-Learning method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_learning</span><span class="params">(env, estimator, num_episodes, discount_factor=<span class="number">1.0</span>, epsilon=<span class="number">0.1</span>, epsilon_decay=<span class="number">1.0</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm for fff-policy TD control using Function Approximation.</div><div class="line">    Finds the optimal greedy policy while following an epsilon-greedy policy.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        estimator: Action-Value function estimator</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">        epsilon_decay: Each episode, epsilon is decayed by this factor</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))    </div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        </div><div class="line">        <span class="comment"># The policy we're following</span></div><div class="line">        policy = make_epsilon_greedy_policy(</div><div class="line">            estimator, epsilon * epsilon_decay**i_episode, env.action_space.n)</div><div class="line">        </div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="comment"># Also print reward for last episode</span></div><div class="line">        last_reward = stats.episode_rewards[i_episode - <span class="number">1</span>]</div><div class="line">        sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Reset the environment and pick the first action</span></div><div class="line">        state = env.reset()</div><div class="line">        </div><div class="line">        <span class="comment"># Only used for SARSA, not Q-Learning</span></div><div class="line">        next_action = <span class="keyword">None</span></div><div class="line">        </div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">                        </div><div class="line">            <span class="comment"># Choose an action to take</span></div><div class="line">            <span class="comment"># If we're using SARSA we already decided in the previous step</span></div><div class="line">            <span class="keyword">if</span> next_action <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                action_probs = policy(state)</div><div class="line">                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                action = next_action</div><div class="line">            </div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            next_state, reward, done, _ = env.step(action)</div><div class="line">    </div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            <span class="comment"># TD Update</span></div><div class="line">            q_values_next = estimator.predict(next_state)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for Q-Learning</span></div><div class="line">            <span class="comment"># Q-Value TD Target</span></div><div class="line">            td_target = reward + discount_factor * np.max(q_values_next)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for SARSA TD Target for on policy-training:</span></div><div class="line">            <span class="comment"># next_action_probs = policy(next_state)</span></div><div class="line">            <span class="comment"># next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)             </span></div><div class="line">            <span class="comment"># td_target = reward + discount_factor * q_values_next[next_action]</span></div><div class="line">            </div><div class="line">            <span class="comment"># Update the function approximator using our target</span></div><div class="line">            estimator.update(state, action, td_target)</div><div class="line">            </div><div class="line">            print(<span class="string">"\rStep &#123;&#125; @ Episode &#123;&#125;/&#123;&#125; (&#123;&#125;)"</span>.format(t, i_episode + <span class="number">1</span>, num_episodes, last_reward), end=<span class="string">""</span>)</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">                </div><div class="line">            state = next_state</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> stats</div></pre></td></tr></table></figure><p>Run this method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">estimator = Estimator()</div><div class="line"><span class="comment"># Note: For the Mountain Car we don't actually need an epsilon &gt; 0.0</span></div><div class="line"><span class="comment"># because our initial estimate for all states is too "optimistic" which leads</span></div><div class="line"><span class="comment"># to the exploration of all states.</span></div><div class="line">stats = q_learning(env, estimator, <span class="number">100</span>, epsilon=<span class="number">0.0</span>)</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-ql-gym.png" alt="mcar-ql-gym"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>On-policy Prediction with Approximation</title>
    <link href="http://yoursite.com/2017/07/05/On-policy-Prediction-with-Approximation/"/>
    <id>http://yoursite.com/2017/07/05/On-policy-Prediction-with-Approximation/</id>
    <published>2017-07-05T07:29:22.000Z</published>
    <updated>2017-07-06T09:39:43.665Z</updated>
    
    <content type="html"><![CDATA[<p>The novelty in this post is that the approximate value function is represented not as a table but as a parameterized functional form with weight vector $\mathbf{w} \in \mathbb{R}^d$. We will write $\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)$ for the approximated value of state $s$ given weight vector $\mathbf{w}$. For example, $\hat{v}$ might be a linear function in features of the state, with $\mathbf{w}$ the vector of feature weights. Consequently, when a single state is updated, the change generalizes from that state to affect the values of many other states.</p><h3 id="The-prediction-Objective-MSVE"><a href="#The-prediction-Objective-MSVE" class="headerlink" title="The prediction Objective (MSVE)"></a>The prediction Objective (MSVE)</h3><p>In the tabular case a continuous measure of prediction quality was not necessary because the learned value function could come to equal the true value function exactly. But with genuine approximation, an update at one state aﬀects many others, and it is not possible to get all states exactly correct. By assumption we have far more states than weights, so making one state’s estimate more accurate invariably means making others’ less accurate.</p><p>By the error in a state $s$ we mean the square of the diﬀerence between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_{\pi}(s)$. Weighting this over the state space by the distribution $\mu$, we obtain a natural objective function, the <strong>Mean Squared Value Error</strong>, or <strong>MSVE</strong>:<br>$$<br>\text{MSVE}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s) \Big[v_{\pi}(s) - \hat{v}(s, \mathbf{w}) \Big]^2.<br>$$<br>The square root of this measure, the root MSVE or RMSVE, gives a rough measure of how much the approximate values diﬀer from the true values and is often used in plots. Typically one chooses $\mu(s)$ to be the fraction of time spent in $s$ under the target policy $\pi$. This is called the <em>on-policy distribution</em>.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/on_policy_dist.png" alt="on-policy-distribution"></p><h3 id="Stochastic-gradient-Methods"><a href="#Stochastic-gradient-Methods" class="headerlink" title="Stochastic-gradient Methods"></a>Stochastic-gradient Methods</h3><p>We assume that states appear in examples with the same distribution, µ, over which we are trying to minimize the MSVE. A good strategy in this case is to try to minimize error on the observed examples. Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example by a small amount in the direction that would most reduce the error on that example:<br>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t - \frac{1}{2} \alpha \nabla \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big]^2 \\<br>&amp;= \mathbf{w}_t + \alpha \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).<br>\end{align}<br>$$<br>And<br>$$<br>\nabla f(\mathbf{w}) \doteq \Big( \frac{\partial f(\mathbf{w})}{\partial w_1}, \frac{\partial f(\mathbf{w})}{\partial w_2}, \cdots, \frac{\partial f(\mathbf{w})}{\partial w_d}\Big)^{\top}.<br>$$<br>Although $v_{\pi}(S_t)$ is unknown, but we can approximate it by substituting $U_t$ (the $t$th training example) in place of $v_{\pi}(S_t)$. This yields the following general SGD method for state-value prediction:<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).<br>$$<br>If $U_t$ is an <strong>unbiased</strong> estimate, that is, if $\mathbb{E}[U_t]=v_{\pi}(S_t)$, for each $t$, then $\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic approximation conditions for decreasing $\alpha$.</p><p>For example, suppose the states in the examples are the states generated by interaction (or simulated interaction) with the environment using policy $\pi$. Because the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t \doteq G_t$ is by deﬁnition an unbiased estimate of $v_{\pi}(S_t)$. Thus, the gradient-descent version of Monte Carlo state-value prediction is guaranteed to ﬁnd a locally optimal solution. Pseudocode for a complete algorithm<br>is shown in the box.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/gradient_mc.png" alt="gradient_mc"></p><h4 id="Example-State-Aggregation-on-the-1000-state-Random-Walk"><a href="#Example-State-Aggregation-on-the-1000-state-Random-Walk" class="headerlink" title="Example: State Aggregation on the 1000-state Random Walk"></a>Example: State Aggregation on the 1000-state Random Walk</h4><p>State aggregation is a simple form of generalizing function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group. The value of a state is estimated as its group’s component, and when the state is updated, that component alone is updated. State aggregation is a special case of SGD in which the gradient, $\nabla \hat{v}(S_t, \mathbf{w}_t)$, is <strong>1</strong> for $S_t$’s group’s component and <strong>0</strong> for the other components.</p><p>Consider a 1000-state version of the random walk task. The states are numbered from 1 to 1000, left to right, and all episodes begin near the center, in state 500. State transitions are from the current state to one of the 100 neighboring states to its left, or to one of the 100 neighboring states to its right, all with equal probability. Of course, if the current state is near an edge, then there may be fewer than 100 neighbors on that side of it. In this case, all the probability that would have gone into those missing neighbors goes into the probability of terminating on that side (thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right). As usual, termination on the left produces a reward of −1, and termination on the right produces a reward of +1. All other transitions have a reward of zero.</p><p>Now, let us solve this problem by gradient Monte Carlo algorithm. First of all, we define the environment of this problem.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># # of states except for terminal states</span></div><div class="line">N_STATES = <span class="number">1000</span></div><div class="line"></div><div class="line"><span class="comment"># true state values, just a promising guess</span></div><div class="line">trueStateValues = np.arange(<span class="number">-1001</span>, <span class="number">1003</span>, <span class="number">2</span>) / <span class="number">1001.0</span></div><div class="line"></div><div class="line"><span class="comment"># all states</span></div><div class="line">states = np.arange(<span class="number">1</span>, N_STATES + <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># start from a central state</span></div><div class="line">START_STATE = <span class="number">500</span></div><div class="line"></div><div class="line"><span class="comment"># terminal states</span></div><div class="line">END_STATES = [<span class="number">0</span>, N_STATES + <span class="number">1</span>]</div><div class="line"></div><div class="line"><span class="comment"># possible actions</span></div><div class="line">ACTION_LEFT = <span class="number">-1</span></div><div class="line">ACTION_RIGHT = <span class="number">1</span></div><div class="line">ACTIONS = [ACTION_LEFT, ACTION_RIGHT]</div><div class="line"></div><div class="line"><span class="comment"># maximum stride for an action</span></div><div class="line">STEP_RANGE = <span class="number">100</span></div></pre></td></tr></table></figure><p>We need a true value of each state, thus use the dynamic programming to get these value:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Dynamic programming to find the true state values, based on the promising guess above</span></div><div class="line"><span class="comment"># Assume all rewards are 0, given that we have already given value -1 and 1 to terminal states</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    oldTrueStateValues = np.copy(trueStateValues)</div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states:</div><div class="line">        trueStateValues[state] = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, STEP_RANGE + <span class="number">1</span>):</div><div class="line">                step *= action</div><div class="line">                newState = state + step</div><div class="line">                newState = max(min(newState, N_STATES + <span class="number">1</span>), <span class="number">0</span>)</div><div class="line">                <span class="comment"># asynchronous update for faster convergence</span></div><div class="line">                trueStateValues[state] += <span class="number">1.0</span> / (<span class="number">2</span> * STEP_RANGE) * trueStateValues[newState]</div><div class="line">    error = np.sum(np.abs(oldTrueStateValues - trueStateValues))</div><div class="line">    print(error)</div><div class="line">    <span class="keyword">if</span> error &lt; <span class="number">1e-2</span>:</div><div class="line">        <span class="keyword">break</span></div><div class="line"><span class="comment"># correct the state value for terminal states to 0</span></div><div class="line">trueStateValues[<span class="number">0</span>] = trueStateValues[<span class="number">-1</span>] = <span class="number">0</span></div></pre></td></tr></table></figure><p>The policy of episodes generation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take an @action at @state, return new state and reward for this transition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(state, action)</span>:</span></div><div class="line">    step = np.random.randint(<span class="number">1</span>, STEP_RANGE + <span class="number">1</span>)</div><div class="line">    step *= action</div><div class="line">    state += step</div><div class="line">    state = max(min(state, N_STATES + <span class="number">1</span>), <span class="number">0</span>)</div><div class="line">    <span class="keyword">if</span> state == <span class="number">0</span>:</div><div class="line">        reward = <span class="number">-1</span></div><div class="line">    <span class="keyword">elif</span> state == N_STATES + <span class="number">1</span>:</div><div class="line">        reward = <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        reward = <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> state, reward</div></pre></td></tr></table></figure><p>The reward after take an action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get an action, following random policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span></div></pre></td></tr></table></figure><p>And we have a special value function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># a wrapper class for aggregation value function</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValueFunction</span>:</span></div><div class="line">    <span class="comment"># @numOfGroups: # of aggregations</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, numOfGroups)</span>:</span></div><div class="line">        self.numOfGroups = numOfGroups</div><div class="line">        self.groupSize = N_STATES // numOfGroups</div><div class="line"></div><div class="line">        <span class="comment"># thetas</span></div><div class="line">        self.params = np.zeros(numOfGroups)</div><div class="line"></div><div class="line">    <span class="comment"># get the value of @state</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="keyword">if</span> state <span class="keyword">in</span> END_STATES:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        groupIndex = (state - <span class="number">1</span>) // self.groupSize</div><div class="line">        <span class="keyword">return</span> self.params[groupIndex]</div><div class="line"></div><div class="line">    <span class="comment"># update parameters</span></div><div class="line">    <span class="comment"># @delta: step size * (target - old estimation)</span></div><div class="line">    <span class="comment"># @state: state of current sample</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, delta, state)</span>:</span></div><div class="line">        groupIndex = (state - <span class="number">1</span>) // self.groupSize</div><div class="line">        self.params[groupIndex] += delta</div></pre></td></tr></table></figure><p>And the gradient MC algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># gradient Monte Carlo algorithm</span></div><div class="line"><span class="comment"># @valueFunction: an instance of class ValueFunction</span></div><div class="line"><span class="comment"># @alpha: step size</span></div><div class="line"><span class="comment"># @distribution: array to store the distribution statistics</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientMonteCarlo</span><span class="params">(valueFunction, alpha, distribution=None)</span>:</span></div><div class="line">    currentState = START_STATE</div><div class="line">    trajectory = [currentState]</div><div class="line"></div><div class="line">    <span class="comment"># We assume gamma = 1, so return is just the same as the latest reward</span></div><div class="line">    reward = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState <span class="keyword">not</span> <span class="keyword">in</span> END_STATES:</div><div class="line">        action = getAction()</div><div class="line">        newState, reward = takeAction(currentState, action)</div><div class="line">        trajectory.append(newState)</div><div class="line">        currentState = newState</div><div class="line"></div><div class="line">    <span class="comment"># Gradient update for each state in this trajectory</span></div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> trajectory[:<span class="number">-1</span>]:</div><div class="line">        delta = alpha * (reward - valueFunction.value(state))</div><div class="line">        valueFunction.update(delta, state)</div><div class="line">        <span class="keyword">if</span> distribution <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            distribution[state] += <span class="number">1</span></div></pre></td></tr></table></figure><p>Finally. let us solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">nEpisodes = int(<span class="number">1e5</span>)</div><div class="line">alpha = <span class="number">2e-5</span></div><div class="line"></div><div class="line"><span class="comment"># we have 10 aggregations in this example, each has 100 states</span></div><div class="line">valueFunction = ValueFunction(<span class="number">10</span>)</div><div class="line">distribution = np.zeros(N_STATES + <span class="number">2</span>)</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">    print(<span class="string">'episode:'</span>, episode)</div><div class="line">    gradientMonteCarlo(valueFunction, alpha, distribution)</div><div class="line"></div><div class="line">distribution /= np.sum(distribution)</div><div class="line">stateValues = [valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states]</div><div class="line"></div><div class="line">plt.figure(<span class="number">0</span>)</div><div class="line">plt.plot(states, stateValues, label=<span class="string">'Approximate MC value'</span>)</div><div class="line">plt.plot(states, trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'True value'</span>)</div><div class="line">plt.xlabel(<span class="string">'State'</span>)</div><div class="line">plt.ylabel(<span class="string">'Value'</span>)</div><div class="line">plt.legend()</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>)</div><div class="line">plt.plot(states, distribution[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'State distribution'</span>)</div><div class="line">plt.xlabel(<span class="string">'State'</span>)</div><div class="line">plt.ylabel(<span class="string">'Distribution'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_1_1.png" alt="distribution"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_1_2.png" alt="state_value"></p><h3 id="Semi-gradient-Methods"><a href="#Semi-gradient-Methods" class="headerlink" title="Semi-gradient Methods"></a>Semi-gradient Methods</h3><p>Bootstrapping target such as n-step returns $G_{t:t+n}$ or the DP target $\sum_{a, s^{\prime}, r} \pi(a|S_t)p(s^{\prime}, r|S_t, a)[r + \gamma \hat{v}(s^{\prime}, \mathbf{w}_t)]$ all depend on the current value of the weight vector $\mathbf{w}_t$, which implies that they will be biased and that they will not produce a true gradient=descent method. We call them <em>semi-gradient methods</em>.</p><p>Although semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear case. Moreover, they oﬀer important advantages which makes them often clearly preferred. One reason for this is that they are typically signiﬁcantly faster to learn. Another is that they enable learning to be continual and online, without waiting for the end of an episode. This enables them to be used on continuing problems and provides computational advantages. A prototypical semi-gradient method is semi-gradient TD(0), which uses $U_t \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w})$ as its target. Complete pseudocode for this method is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/semi_grad_td.png" alt="semi_grad_td"></p><h3 id="Linear-Methods"><a href="#Linear-Methods" class="headerlink" title="Linear Methods"></a>Linear Methods</h3><p>One of the most important special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot, \mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$. Corresponding to every state $s$, there is a real-valued vector of features $\mathbf{x}(s) \doteq (x_1(s),x_2(s),\cdots,x_d(s))^{\top}$, with the same number of components as $\mathbf{w}$. However the features are constructed, the approximate state-value function is given by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:<br>$$<br>\hat{v}(s, \mathbf{w}) \doteq \mathbf{w^{\top}x}(s) \doteq \sum_{i=1}^d w_i x_i(s).<br>$$<br>The individual functions $x_i:\mathcal{S} \rightarrow \mathbb{E}$ are called <strong>basis functions</strong>. The gradient of the approximate value function with respect to $\mathbf{w}$ in this case is<br>$$<br>\nabla \hat{v} (s, \mathbf{w}) = \mathbf{x}(s).<br>$$<br>The update at each time $t$ is</p><p>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t + \alpha \Big( R_{t+1} + \gamma \mathbf{w}_t^{\top}\mathbf{x}_{t+1} - \mathbf{w}_t^{\top}\mathbf{x}_{t}\Big)\mathbf{x}_t \\<br>&amp;= \mathbf{w}_t + \alpha \Big( R_{t+1}\mathbf{x}_t - \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \mathbf{w}_t\Big),<br>\end{align}<br>$$<br>where here we have used the notational shorthand $\mathbf{x}_t = \mathbf{x}(S_t)$. If the system converges, it must converge to the weight vector $\mathbf{w}_{TD}$ at which<br>$$<br>\mathbf{w}_{TD} \doteq \mathbf{A^{-1}b},<br>$$<br>where<br>$$<br>\mathbf{b} \doteq \mathbb{E}[R_{t+1}\mathbf{x}_t] \in \mathbb{R}^d \;\; \text{and} \;\; \mathbf{A} \doteq \mathbb{E}\big[ \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \big] \in \mathbb{R}^d \times \mathbb{R}^d.<br>$$<br>This quantity is called the TD <strong>fixedpoint</strong>. At this point we have:<br>$$<br>\text{MSVE}(\mathbf{w}_{TD}) \leq \frac{1}{1 - \gamma} \min_{\mathbf{w}} \text{MSVE}(\mathbf{w}).<br>$$<br>Now we use the state aggregation example again, but use the semi-gradient TD method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># semi-gradient n-step TD algorithm</span></div><div class="line"><span class="comment"># @valueFunction: an instance of class ValueFunction</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="comment"># @alpha: step size</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">semiGradientTemporalDifference</span><span class="params">(valueFunction, n, alpha)</span>:</span></div><div class="line">    <span class="comment"># initial starting state</span></div><div class="line">    currentState = START_STATE</div><div class="line"></div><div class="line">    <span class="comment"># arrays to store states and rewards for an episode</span></div><div class="line">    <span class="comment"># space isn't a major consideration, so I didn't use the mod trick</span></div><div class="line">    states = [currentState]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># choose an action randomly</span></div><div class="line">            action = getAction()</div><div class="line">            newState, reward = takeAction(currentState, action)</div><div class="line"></div><div class="line">            <span class="comment"># store new state and new reward</span></div><div class="line">            states.append(newState)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newState <span class="keyword">in</span> END_STATES:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += rewards[t]</div><div class="line">            <span class="comment"># add state value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += valueFunction.value(states[updateTime + n])</div><div class="line">            stateToUpdate = states[updateTime]</div><div class="line">            <span class="comment"># update the value function</span></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> stateToUpdate <span class="keyword">in</span> END_STATES:</div><div class="line">                delta = alpha * (returns - valueFunction.value(stateToUpdate))</div><div class="line">                valueFunction.update(delta, stateToUpdate)</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentState = newState</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">nEpisodes = int(<span class="number">1e5</span>)</div><div class="line">    alpha = <span class="number">2e-4</span></div><div class="line">    valueFunction = ValueFunction(<span class="number">10</span>)</div><div class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        print(<span class="string">'episode:'</span>, episode)</div><div class="line">        semiGradientTemporalDifference(valueFunction, <span class="number">1</span>, alpha)</div><div class="line"></div><div class="line">    stateValues = [valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states]</div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    plt.plot(states, stateValues, label=<span class="string">'Approximate TD value'</span>)</div><div class="line">    plt.plot(states, trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'True value'</span>)</div><div class="line">    plt.xlabel(<span class="string">'State'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Value'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_2_1.png" alt="semi_gradient_td"></p><p>We also could use the <a href="https://ewanlee.github.io/2017/07/04/n-step-TD/" target="_blank" rel="external">n-step semi-gradient TD method</a>. To obtain such quantitatively similar results we switched the state aggregation to 20 groups of 50 states each. The 20 groups are then quantitatively close to the 19 states of the tabular problem.</p><p>The semi-gradient n-step TD algorithm we used in this example is the natural extension of the tabular n-step TD algorithm. The key equation is<br>$$<br>\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \Big[ G_{t:t+n} - \hat{v}(S_t, \mathbf{w}_{t+n-1})\Big] \nabla \hat{v}(S_t, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T,<br>$$<br>where<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{v}(S_{t+n}, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T-n.<br>$$<br>Pseudocode for the complete algorithm is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/n_step_semi_gradient_td.png" alt="n_step_semi_gradient_td"></p><p>Now let us show the performance of different value of n:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># truncate value for better display</span></div><div class="line">truncateValue = <span class="number">0.55</span></div><div class="line"></div><div class="line"><span class="comment"># all possible steps</span></div><div class="line">steps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># all possible alphas</span></div><div class="line">alphas = np.arange(<span class="number">0</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="comment"># each run has 10 episodes</span></div><div class="line">episodes = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># perform 100 independent runs</span></div><div class="line">runs = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># track the errors for each (step, alpha) combination</span></div><div class="line">errors = np.zeros((len(steps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> stepInd, step <span class="keyword">in</span> zip(range(len(steps)), steps):</div><div class="line">        <span class="keyword">for</span> alphaInd, alpha <span class="keyword">in</span> zip(range(len(alphas)), alphas):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'step:'</span>, step, <span class="string">'alpha:'</span>, alpha)</div><div class="line">            <span class="comment"># we have 20 aggregations in this example</span></div><div class="line">            valueFunction = ValueFunction(<span class="number">20</span>)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                semiGradientTemporalDifference(valueFunction, step, alpha)</div><div class="line">                <span class="comment"># calculate the RMS error</span></div><div class="line">                currentStateValues = np.asarray([valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states])</div><div class="line">                errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], <span class="number">2</span>)) / N_STATES)</div><div class="line"><span class="comment"># take average</span></div><div class="line">errors /= episodes * runs</div><div class="line"><span class="comment"># truncate the error</span></div><div class="line">errors[errors &gt; truncateValue] = truncateValue</div><div class="line">plt.figure(<span class="number">3</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(steps)):</div><div class="line">    plt.plot(alphas, errors[i, :], label=<span class="string">'n = '</span> + str(steps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMS error'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_2_2.png" alt="n_step_semi_gradient_td_compare"></p><h3 id="Feature-Construction-for-Linear-Methods"><a href="#Feature-Construction-for-Linear-Methods" class="headerlink" title="Feature Construction for Linear Methods"></a>Feature Construction for Linear Methods</h3><p>Choosing features appropriate to the task is an important way of adding prior domain knowledge to reinforcement learning systems. Intuitively, the features should correspond to the natural features of the task, those along which generalization is most appropriate. If we are valuing geometric objects, for example, we might want to have features for each possible shape, color, size, or function. If we are valuing states of a mobile robot, then we might want to have features for locations, degrees of remaining battery power, recent sonar readings, and so on.</p><h4 id="Polynomials"><a href="#Polynomials" class="headerlink" title="Polynomials"></a>Polynomials</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/poly_feat.png" alt="poly"></p><h4 id="Fourier-Basis"><a href="#Fourier-Basis" class="headerlink" title="Fourier Basis"></a>Fourier Basis</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/fourier.png" alt="fourier"></p><p>Konidaris et al. (2011) found that when using Fourier cosine basis functions with a learning algorithm such as semi-gradient TD(0), or semi-gradient Sarsa, it is helpful to use a diﬀerent step-size parameter for each basis function. If $\alpha$ is the basic step-size parameter, they suggest setting the step-size parameter for basis function $x_i$ to $a_i=\alpha/\sqrt{(c_1^i)^2 + \cdots + (c_d^i)^2}$ (except when each $c_j^i=0$, in which case $\alpha_i=\alpha$).</p><p>Now, let us we compare the Fourier and polynomial bases on the 1000-state random walk example. <strong>In general, we do not recommend using the polynomial basis for online learning.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># a wrapper class for polynomial / Fourier -based value function</span></div><div class="line">POLYNOMIAL_BASES = <span class="number">0</span></div><div class="line">FOURIER_BASES = <span class="number">1</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasesValueFunction</span>:</span></div><div class="line">    <span class="comment"># @order: # of bases, each function also has one more constant parameter (called bias in machine learning)</span></div><div class="line">    <span class="comment"># @type: polynomial bases or Fourier bases</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, order, type)</span>:</span></div><div class="line">        self.order = order</div><div class="line">        self.weights = np.zeros(order + <span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># set up bases function</span></div><div class="line">        self.bases = []</div><div class="line">        <span class="keyword">if</span> type == POLYNOMIAL_BASES:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, order + <span class="number">1</span>):</div><div class="line">                self.bases.append(<span class="keyword">lambda</span> s, i=i: pow(s, i))</div><div class="line">        <span class="keyword">elif</span> type == FOURIER_BASES:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, order + <span class="number">1</span>):</div><div class="line">                self.bases.append(<span class="keyword">lambda</span> s, i=i: np.cos(i * np.pi * s))</div><div class="line"></div><div class="line">    <span class="comment"># get the value of @state</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="comment"># map the state space into [0, 1]</span></div><div class="line">        state /= float(N_STATES)</div><div class="line">        <span class="comment"># get the feature vector</span></div><div class="line">        feature = np.asarray([func(state) <span class="keyword">for</span> func <span class="keyword">in</span> self.bases])</div><div class="line">        <span class="keyword">return</span> np.dot(self.weights, feature)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, delta, state)</span>:</span></div><div class="line">        <span class="comment"># map the state space into [0, 1]</span></div><div class="line">        state /= float(N_STATES)</div><div class="line">        <span class="comment"># get derivative value</span></div><div class="line">        derivativeValue = np.asarray([func(state) <span class="keyword">for</span> func <span class="keyword">in</span> self.bases])</div><div class="line">        self.weights += delta * derivativeValue</div></pre></td></tr></table></figure><p>The function upper is used to construction the features of states (map states to features).</p><p>Next, we will compare different super-parameters’ (order) performance:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">1</span></div><div class="line"></div><div class="line">episodes = <span class="number">5000</span></div><div class="line"></div><div class="line"><span class="comment"># # of bases</span></div><div class="line">orders = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>]</div><div class="line"></div><div class="line">alphas = [<span class="number">1e-4</span>, <span class="number">5e-5</span>]</div><div class="line">labels = [[<span class="string">'polynomial basis'</span>] * <span class="number">3</span>, [<span class="string">'fourier basis'</span>] * <span class="number">3</span>]</div><div class="line"></div><div class="line"><span class="comment"># track errors for each episode</span></div><div class="line">errors = np.zeros((len(alphas), len(orders), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(orders)):</div><div class="line">        valueFunctions = [BasesValueFunction(orders[i], POLYNOMIAL_BASES), BasesValueFunction(orders[i], FOURIER_BASES)]</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">            <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'order:'</span>, orders[i], labels[j][i], <span class="string">'episode:'</span>, episode)</div><div class="line"></div><div class="line">                <span class="comment"># gradient Monte Carlo algorithm</span></div><div class="line">                gradientMonteCarlo(valueFunctions[j], alphas[j])</div><div class="line"></div><div class="line">                <span class="comment"># get state values under current value function</span></div><div class="line">                stateValues = [valueFunctions[j].value(state) <span class="keyword">for</span> state <span class="keyword">in</span> states]</div><div class="line"></div><div class="line">                <span class="comment"># get the root-mean-squared error</span></div><div class="line">                errors[j, i, episode] += np.sqrt(np.mean(np.power(trueStateValues[<span class="number">1</span>: <span class="number">-1</span>] - stateValues, <span class="number">2</span>)))</div><div class="line"></div><div class="line"><span class="comment"># average over independent runs</span></div><div class="line">errors /= runs</div><div class="line"></div><div class="line">plt.figure(<span class="number">5</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(orders)):</div><div class="line">        plt.plot(errors[i, j, :], label=labels[i][j]+<span class="string">' order = '</span> + str(orders[j]))</div><div class="line">plt.xlabel(<span class="string">'Episodes'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMSVE'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/poly_vs_four.png" alt="poly_vs_four"></p><h3 id="TODO-TILE-CODING"><a href="#TODO-TILE-CODING" class="headerlink" title="TODO: TILE CODING"></a>TODO: TILE CODING</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The novelty in this post is that the approximate value function is represented not as a table but as a parameterized functional form with
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>n-step TD</title>
    <link href="http://yoursite.com/2017/07/04/n-step-TD/"/>
    <id>http://yoursite.com/2017/07/04/n-step-TD/</id>
    <published>2017-07-04T03:16:06.000Z</published>
    <updated>2017-07-04T05:56:21.317Z</updated>
    
    <content type="html"><![CDATA[<p>In this post, we unify the Monte Carlo methods and the one-step TD methods. We first consider the prediction problem.</p><h3 id="n-step-TD-Prediction"><a href="#n-step-TD-Prediction" class="headerlink" title="n-step TD Prediction"></a>n-step TD Prediction</h3><p>Monte Carlo methods preform a backup for each state based on the entire sequence of observed rewards from that state until the end of the episode. One-step TD methods is based on just on next reward. So n-step TD methods perform a backup based on an intermediate number of rewards: more than one, but less than all of them until termination.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/ntd/nsteptd.png" alt="nstep_td"></p><p>More formally, consider the backup applied to state $S_t$ as a result of the state-reward sequence, $S_t, R_{t+1},S_{t+1}, R_{t+2}, \cdots, R_T, S_T$ (omitting the actions for simplicity). We know that in Monte Carlo backups the estimate of $v_{\pi}(S_t)$ updated in the direction of the complete return:<br>$$<br>G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T,<br>$$<br>where $T$ is the last time step of the episode. Let us call this quantity the <strong>target</strong> of the backup. Whereas in Monte Carlo backups the target is the return, in one-step backups the target is the first reward plus the discounted estimated value of the next state, which we call the one-step return:<br>$$<br>G_{t:t+1} \doteq R_{t+1} + \gamma V_t (S_{t+1}),<br>$$<br>where $V_t : \mathcal{S} \rightarrow \mathbb{R}$ here is an estimate at time $t$ of $v_{\pi}$. The subscripts on $G_{t:t+1}$ indicate that it is truncated return for time t using rewards up until time $t+1$. In the one-step return, $\gamma V_t (S_{t+1})$ takes the place of the other terms $ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$ of the full return. Our point now is that this idea makes just as much sense after two steps as it does after one. The target for a two-step backup is the two-step return:<br>$$<br>G_{t:t+2} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1} (S_{t+2}),<br>$$<br>where now $\gamma^2 V_{t+1}(S_{t+2})$ corrects for the absence of the terms $\gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$. Similarly, the target for an arbitrary n-step backup is the n-step return:<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n V_{t+n-1} (S_{t+n}),<br>$$<br>for all $n,t$ such that $n \ge 1$ and $0 \leq t \leq T-n$. If $t+n \ge T$, then all the missing terms are taken as zero, and the n-step return defined to be equal to the ordinary full return.</p><p>No real algorithm can use the n-step return until after it has seen $R_{t+n}$ and computed $V_{t+n-1}$. The first time these are available is $t+n$. The natural algorithm state-value learning algorithm for using n-step returns is thus<br>$$<br>V_{t+n}(S_t) \doteq V_{t+n-1}(S_t) + \alpha [G_{t:t+n} - V_{t+n-1}(S_t)], \;\;\;\;\;\; 0 \leq t \leq T<br>$$<br>while the values of all other states remain unchanged. Note that no changes at all are made during the first $n-1$ steps of each episode. Complete pseudocode is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/ntd/nstep_td_alg.png" alt="alg_box"></p><p>The worst error of the expected n-step return is guaranteed to be less than or equal to $\gamma^n$ times the worst error under $V_{t+n-1}$:<br>$$<br>\max_s \left |\mathbb{E}[G_{t:t+1}|S_t=s] - v_{\pi}(s) \right | \leq \gamma^n \max_s |V_{t+n-1}(s) - v_{\pi}(s)|,<br>$$<br>for all $n \geq 1$. This is called the <strong>error reduction property</strong> of n-step returns. The n-step TD methods thus form a family of sound methods, with one-step TD methods and Monte Carlo methods as extreme members.</p><h4 id="Example-n-step-TD-Methods-on-the-Random-Walk"><a href="#Example-n-step-TD-Methods-on-the-Random-Walk" class="headerlink" title="Example:  n-step TD Methods on the Random Walk"></a>Example: n-step TD Methods on the Random Walk</h4><p>Now we have a larger MDP (19 non-terminal states). First of all we need to define the new environment:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all states</span></div><div class="line">N_STATES = <span class="number">19</span></div><div class="line"></div><div class="line"><span class="comment"># discount</span></div><div class="line">GAMMA = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># initial state values</span></div><div class="line">stateValues = np.zeros(N_STATES + <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># all states but terminal states</span></div><div class="line">states = np.arange(<span class="number">1</span>, N_STATES + <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># start from the middle state</span></div><div class="line">START_STATE = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># two terminal states</span></div><div class="line"><span class="comment"># an action leading to the left terminal state has reward -1</span></div><div class="line"><span class="comment"># an action leading to the right terminal state has reward 1</span></div><div class="line">END_STATES = [<span class="number">0</span>, N_STATES + <span class="number">1</span>]</div><div class="line"></div><div class="line"><span class="comment"># true state value from bellman equation</span></div><div class="line">realStateValues = np.arange(<span class="number">-20</span>, <span class="number">22</span>, <span class="number">2</span>) / <span class="number">20.0</span></div><div class="line">realStateValues[<span class="number">0</span>] = realStateValues[<span class="number">-1</span>] = <span class="number">0</span></div></pre></td></tr></table></figure><p>And then develop the n-step TD algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># n-steps TD method</span></div><div class="line"><span class="comment"># @stateValues: values for each state, will be updated</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="comment"># @alpha: # step size</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">temporalDifference</span><span class="params">(stateValues, n, alpha)</span>:</span></div><div class="line">    <span class="comment"># initial starting state</span></div><div class="line">    currentState = START_STATE</div><div class="line"></div><div class="line">    <span class="comment"># arrays to store states and rewards for an episode</span></div><div class="line">    <span class="comment"># space isn't a major consideration, so I didn't use the mod trick</span></div><div class="line">    states = [currentState]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># choose an action randomly</span></div><div class="line">            <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">                newState = currentState + <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                newState = currentState - <span class="number">1</span></div><div class="line">            <span class="keyword">if</span> newState == <span class="number">0</span>:</div><div class="line">                reward = <span class="number">-1</span></div><div class="line">            <span class="keyword">elif</span> newState == <span class="number">20</span>:</div><div class="line">                reward = <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                reward = <span class="number">0</span></div><div class="line"></div><div class="line">            <span class="comment"># store new state and new reward</span></div><div class="line">            states.append(newState)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newState <span class="keyword">in</span> END_STATES:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += pow(GAMMA, t - updateTime - <span class="number">1</span>) * rewards[t]</div><div class="line">            <span class="comment"># add state value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += pow(GAMMA, n) * stateValues[states[(updateTime + n)]]</div><div class="line">            stateToUpdate = states[updateTime]</div><div class="line">            <span class="comment"># update the state value</span></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> stateToUpdate <span class="keyword">in</span> END_STATES:</div><div class="line">                stateValues[stateToUpdate] += alpha * (returns - stateValues[stateToUpdate])</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentState = newState</div></pre></td></tr></table></figure><p>Now, let us test the performance under different $n$ values and $\alpha$ values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># truncate value for better display</span></div><div class="line">truncateValue = <span class="number">0.55</span></div><div class="line"></div><div class="line"><span class="comment"># all possible steps</span></div><div class="line">steps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># all possible alphas</span></div><div class="line">alphas = np.arange(<span class="number">0</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="comment"># each run has 10 episodes</span></div><div class="line">episodes = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># perform 100 independent runs</span></div><div class="line">runs = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># track the errors for each (step, alpha) combination</span></div><div class="line">errors = np.zeros((len(steps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> stepInd, step <span class="keyword">in</span> zip(range(len(steps)), steps):</div><div class="line">        <span class="keyword">for</span> alphaInd, alpha <span class="keyword">in</span> zip(range(len(alphas)), alphas):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'step:'</span>, step, <span class="string">'alpha:'</span>, alpha)</div><div class="line">            currentStateValues = np.copy(stateValues)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                temporalDifference(currentStateValues, step, alpha)</div><div class="line">                <span class="comment"># calculate the RMS error</span></div><div class="line">                errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - realStateValues, <span class="number">2</span>)) / N_STATES)</div><div class="line"><span class="comment"># take average</span></div><div class="line">errors /= episodes * runs</div><div class="line"><span class="comment"># truncate the error</span></div><div class="line">errors[errors &gt; truncateValue] = truncateValue</div><div class="line">plt.figure()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(steps)):</div><div class="line">    plt.plot(alphas, errors[i, :], label=<span class="string">'n = '</span> + str(steps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMS error'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/ntd/n_step_td_random_walk.png" alt="n_step_td_random_walk_result"></p><h3 id="TODO-N-STEP-SARSA"><a href="#TODO-N-STEP-SARSA" class="headerlink" title="TODO: N-STEP SARSA"></a>TODO: N-STEP SARSA</h3><h3 id="TODO-N-STEP-OFF-POLICY-ALGORITHM"><a href="#TODO-N-STEP-OFF-POLICY-ALGORITHM" class="headerlink" title="TODO: N-STEP OFF-POLICY ALGORITHM"></a>TODO: N-STEP OFF-POLICY ALGORITHM</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this post, we unify the Monte Carlo methods and the one-step TD methods. We first consider the prediction problem.&lt;/p&gt;&lt;h3 id=&quot;n-step-T
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="TD" scheme="http://yoursite.com/tags/TD/"/>
    
  </entry>
  
  <entry>
    <title>Temporal-Difference Learning</title>
    <link href="http://yoursite.com/2017/07/02/Temporal-Difference-Learning/"/>
    <id>http://yoursite.com/2017/07/02/Temporal-Difference-Learning/</id>
    <published>2017-07-02T04:44:00.000Z</published>
    <updated>2017-07-04T02:36:15.327Z</updated>
    
    <content type="html"><![CDATA[<p>If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be <em>temporal-difference</em> (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome.</p><h3 id="TD-0"><a href="#TD-0" class="headerlink" title="TD(0)"></a><strong>TD(0)</strong></h3><p>Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environment is<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)],<br>$$<br>where $G_t$ is the <strong>actual return</strong> following time $t$. Let us call this method $constant\text{-}\alpha \ MC$. Notice that, if we are in a stationary environment (like <a href="https://ewanlee.github.io/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" target="_blank" rel="external">earlier</a>. For some reason, don’t use incremental implementation), the $\alpha$ is equals to $\frac{1}{N(S_t)}$. whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t+1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]<br>$$<br>immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_t$, whereas the target for the TD update is $R_{t+1} + \gamma V(S_{t+1})$. This TD method is called $TD(0)$, or <strong>one-step</strong> TD. The box below specifies TD(0) completely in procedural form.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td_0.png" alt="td_0"></p><p>TD(0)’s backup diagram is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td0_bg.png" alt="td0bg"></p><p>Because the TD(0) bases its update in part on an existing estimate, we say that it is a <em>bootstrapping</em> method, like DP. We know that<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [G_t \ | \ S_t=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} \ | \ S_t=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_t=s].<br>\end{align}<br>$$<br>Roughly speaking, Monte Carlo methods use an estimate of (3) as a target, whereas DP methods use an estimate of (5) as a target, The Monte Carlo target is an estimate because the expected value in (3) is not known; a sample return is used in place of the real expected return. The DP target is an estimate not because of the excepted value, which are assumed to be completely provided by a model of the environment (the environment is known for the DP methods), but because $v_{\pi}(S_{t+1})$ is not known and the current estimate, $V(S_{t+1})$, is used instead. The TD target is an estimate for both reasons.</p><p>Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \gamma V(S_{t+1})$. This quantity, called the <strong>TD error</strong>, arises in various forms throughout reinforcement learning:<br>$$<br>\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t).<br>$$<br>Notice that the TD error at each time is the error in the estimate <strong>made at that time</strong>. Because the TD error depends on the next state and the next reward, it is not actually available until one time step later. Also note that if the array $V$ does not change during the episode (as it does not in Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:<br>$$<br>\begin{align}<br>G_t - V(S_t) &amp;= R_{t+1} + \gamma G(S_{t+1}) - V(S_t) + \gamma V(S_{t+1} ) - \gamma V(S_{t+1}) \\<br>&amp;= \delta_t + \gamma (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(G_t-V(S_T)) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(0 -0) \\<br>&amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k.<br>\end{align}<br>$$<br>This identity is not exact if $V$ is updated during the episode (as it is in TD(0)), but if the step size is small then it may still hold approximately. Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning.</p><h4 id="Example-Random-walk"><a href="#Example-Random-walk" class="headerlink" title="Example: Random walk"></a><strong>Example: Random walk</strong></h4><p>In this example we empirically compare the prediction abilities of TD(0) and constant-$\alpha$ MC applied to the small Markov reward process shown in the upper part of the figure below. All episodes start in the center state, <strong>C</strong>, and the proceed either left or right by one state on each step, with equal probability. This behavior can be thought of as due to the combined effect of a fixed policy and an environment’s state-transition probabilities, but we do not care which; we are concerned only with predicting returns however they are generated. Episodes terminates on the right, a reward of +1 occurs; all other reward are zero. For example, a typical episode might consist of the following state-and-reward sequence: <strong>C, 0, B, 0, C, 0, D, 0, E, 1.</strong> Because this task is undiscounted, the true value of each state is the probability of terminating on the right if starting from that state. Thus, the true value of the center state is $v_{\pi}(\text{C}) = 0.5$. The true values of all the states, <strong>A</strong> through <strong>E</strong>, are $\frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}$, and $\frac{5}{6}$. In all cases the approximate value function was initialized to the intermediate value $V(s)=0.5$, for all $s$.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk.png" alt="random_walk"></p><p>Now, let us develop the codes to solve problem.</p><p>The first, we initialize some truth.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 0 is the left terminal state</span></div><div class="line"><span class="comment"># 6 is the right terminal state</span></div><div class="line"><span class="comment"># 1 ... 5 represents A ... E</span></div><div class="line">states = np.zeros(<span class="number">7</span>)</div><div class="line">states[<span class="number">1</span>:<span class="number">6</span>] = <span class="number">0.5</span></div><div class="line"><span class="comment"># For convenience, we assume all rewards are 0</span></div><div class="line"><span class="comment"># and the left terminal state has value 0, the right terminal state has value 1</span></div><div class="line"><span class="comment"># This trick has been used in Gambler's Problem</span></div><div class="line">states[<span class="number">6</span>] = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># set up true state values</span></div><div class="line">trueValue = np.zeros(<span class="number">7</span>)</div><div class="line">trueValue[<span class="number">1</span>:<span class="number">6</span>] = np.arange(<span class="number">1</span>, <span class="number">6</span>) / <span class="number">6.0</span></div><div class="line">trueValue[<span class="number">6</span>] = <span class="number">1</span></div><div class="line"></div><div class="line">ACTION_LEFT = <span class="number">0</span></div><div class="line">ACTION_RIGHT = <span class="number">1</span></div></pre></td></tr></table></figure><p>The below box is the TD(0) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">temporalDifference</span><span class="params">(states, alpha=<span class="number">0.1</span>, batch=False)</span>:</span></div><div class="line">    state = <span class="number">3</span></div><div class="line">    trajectory = [state]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        oldState = state</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == ACTION_LEFT:</div><div class="line">            state -= <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state += <span class="number">1</span></div><div class="line">        <span class="comment"># Assume all rewards are 0</span></div><div class="line">        reward = <span class="number">0</span></div><div class="line">        trajectory.append(state)</div><div class="line">        <span class="comment"># TD update</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> batch:</div><div class="line">            states[oldState] += alpha * (reward + states[state] - states[oldState])</div><div class="line">        <span class="keyword">if</span> state == <span class="number">6</span> <span class="keyword">or</span> state == <span class="number">0</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        rewards.append(reward)</div><div class="line">    <span class="keyword">return</span> trajectory, rewards</div></pre></td></tr></table></figure><p>And below box is the constant-$\alpha$ Monte Carlo algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarlo</span><span class="params">(states, alpha=<span class="number">0.1</span>, batch=False)</span>:</span></div><div class="line">    state = <span class="number">3</span></div><div class="line">    trajectory = [<span class="number">3</span>]</div><div class="line">    <span class="comment"># if end up with left terminal state, all returns are 0</span></div><div class="line">    <span class="comment"># if end up with right terminal state, all returns are 1</span></div><div class="line">    returns = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == ACTION_LEFT:</div><div class="line">            state -= <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state += <span class="number">1</span></div><div class="line">        trajectory.append(state)</div><div class="line">        <span class="keyword">if</span> state == <span class="number">6</span>:</div><div class="line">            returns = <span class="number">1.0</span></div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">elif</span> state == <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="keyword">break</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> batch:</div><div class="line">        <span class="keyword">for</span> state_ <span class="keyword">in</span> trajectory[:<span class="number">-1</span>]:</div><div class="line">            <span class="comment"># MC update</span></div><div class="line">            states[state_] += alpha * (returns - states[state_])</div><div class="line">    <span class="keyword">return</span> trajectory, [returns] * (len(trajectory) - <span class="number">1</span>)</div></pre></td></tr></table></figure><p>First of all, let us test the performance of the TD(0) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stateValue</span><span class="params">()</span>:</span></div><div class="line">    episodes = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]</div><div class="line">    currentStates = np.copy(states)</div><div class="line">    plt.figure(<span class="number">1</span>)</div><div class="line">    axisX = np.arange(<span class="number">0</span>, <span class="number">7</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, episodes[<span class="number">-1</span>] + <span class="number">1</span>):</div><div class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> episodes:</div><div class="line">            plt.plot(axisX, currentStates, label=str(i) + <span class="string">' episodes'</span>)</div><div class="line">        temporalDifference(currentStates)</div><div class="line">    plt.plot(axisX, trueValue, label=<span class="string">'true values'</span>)</div><div class="line">    plt.xlabel(<span class="string">'state'</span>)</div><div class="line">    plt.legend()</div><div class="line">    </div><div class="line">stateValue()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_td0.png" alt="random_walk_td0"></p><p>And then let us show the RMS error of the TD(0) algorithm and constant-$\alpha$ Monte Carlo algorithm, for various $\alpha$ values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMSError</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># I'm lazy here, so do not let same alpha value appear in both arrays</span></div><div class="line">    <span class="comment"># For example, if in TD you want to use alpha = 0.2, then in MC you can use alpha = 0.201</span></div><div class="line">    TDAlpha = [<span class="number">0.15</span>, <span class="number">0.1</span>, <span class="number">0.05</span>]</div><div class="line">    MCAlpha = [<span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.03</span>, <span class="number">0.04</span>]</div><div class="line">    episodes = <span class="number">100</span> + <span class="number">1</span></div><div class="line">    runs = <span class="number">100</span></div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    axisX = np.arange(<span class="number">0</span>, episodes)</div><div class="line">    <span class="keyword">for</span> alpha <span class="keyword">in</span> TDAlpha + MCAlpha:</div><div class="line">        totalErrors = np.zeros(episodes)</div><div class="line">        <span class="keyword">if</span> alpha <span class="keyword">in</span> TDAlpha:</div><div class="line">            method = <span class="string">'TD'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            method = <span class="string">'MC'</span></div><div class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">            errors = []</div><div class="line">            currentStates = np.copy(states)</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                errors.append(np.sqrt(np.sum(np.power(trueValue - currentStates, <span class="number">2</span>)) / <span class="number">5.0</span>))</div><div class="line">                <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                    temporalDifference(currentStates, alpha=alpha)</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    monteCarlo(currentStates, alpha=alpha)</div><div class="line">            totalErrors += np.asarray(errors)</div><div class="line">        totalErrors /= runs</div><div class="line">        plt.plot(axisX, totalErrors, label=method + <span class="string">', alpha='</span> + str(alpha))</div><div class="line">    plt.xlabel(<span class="string">'episodes'</span>)</div><div class="line">    plt.legend()</div><div class="line">    </div><div class="line">RMSError()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_rmse.png" alt="random_walk_error"></p><p>We can see, the TD method was consistently better than the MC method on this task.</p><p>Now, suppose that there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning method is to present the experience repeatedly until the method converges upon an answer. We call this <em>batch updating</em>.</p><h4 id="Example-Random-walk-under-batch-updating"><a href="#Example-Random-walk-under-batch-updating" class="headerlink" title="Example: Random walk under batch updating"></a><strong>Example: Random walk under batch updating</strong></h4><p>After each new episodes, all episodes seen so far were treated as a batch. They were repeatedly presented to the algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchUpdating</span><span class="params">(method, episodes, alpha=<span class="number">0.001</span>)</span>:</span></div><div class="line">    <span class="comment"># perform 100 independent runs</span></div><div class="line">    runs = <span class="number">100</span></div><div class="line">    totalErrors = np.zeros(episodes - <span class="number">1</span>)</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        currentStates = np.copy(states)</div><div class="line">        errors = []</div><div class="line">        <span class="comment"># track shown trajectories and reward/return sequences</span></div><div class="line">        trajectories = []</div><div class="line">        rewards = []</div><div class="line">        <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">1</span>, episodes):</div><div class="line">            print(<span class="string">'Run:'</span>, run, <span class="string">'Episode:'</span>, ep)</div><div class="line">            <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                trajectory_, rewards_ = temporalDifference(currentStates, batch=<span class="keyword">True</span>)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                trajectory_, rewards_ = monteCarlo(currentStates, batch=<span class="keyword">True</span>)</div><div class="line">            trajectories.append(trajectory_)</div><div class="line">            rewards.append(rewards_)</div><div class="line">            <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">                <span class="comment"># keep feeding our algorithm with trajectories seen so far until state value function converges</span></div><div class="line">                updates = np.zeros(<span class="number">7</span>)</div><div class="line">                <span class="keyword">for</span> trajectory_, rewards_ <span class="keyword">in</span> zip(trajectories, rewards):</div><div class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(trajectory_) - <span class="number">1</span>):</div><div class="line">                        <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                            updates[trajectory_[i]] += rewards_[i] + currentStates[trajectory_[i + <span class="number">1</span>]] - currentStates[trajectory_[i]]</div><div class="line">                        <span class="keyword">else</span>:</div><div class="line">                            updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]]</div><div class="line">                updates *= alpha</div><div class="line">                <span class="keyword">if</span> np.sum(np.abs(updates)) &lt; <span class="number">1e-3</span>:</div><div class="line">                    <span class="keyword">break</span></div><div class="line">                <span class="comment"># perform batch updating</span></div><div class="line">                currentStates += updates</div><div class="line">            <span class="comment"># calculate rms error</span></div><div class="line">            errors.append(np.sqrt(np.sum(np.power(currentStates - trueValue, <span class="number">2</span>)) / <span class="number">5.0</span>))</div><div class="line">        totalErrors += np.asarray(errors)</div><div class="line">    totalErrors /= runs</div><div class="line">    <span class="keyword">return</span> totalErrors</div></pre></td></tr></table></figure><p>Notice that the core codes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep feeding our algorithm with trajectories seen so far until state</span></div><div class="line">    <span class="comment"># value function converges</span></div><div class="line">    updates = np.zeros(<span class="number">7</span>)</div><div class="line">    <span class="keyword">for</span> trajectory_, rewards_ <span class="keyword">in</span> zip(trajectories, rewards):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(trajectory_) - <span class="number">1</span>):</div><div class="line">            <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                updates[trajectory_[i]] += rewards_[i] + \</div><div class="line">                    currentStates[trajectory_[i + <span class="number">1</span>]] - currentStates[trajectory_[i]]</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]]</div><div class="line">    updates *= alpha</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(updates)) &lt; <span class="number">1e-3</span>:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># perform batch updating</span></div><div class="line">    currentStates += updates</div></pre></td></tr></table></figure><p>Either TD methods or MC methods, the target is to minimize the TD error (or MC error, I say).</p><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/batch_update.png" alt="batch_update"></p><p>Under batch training, constant-$\alpha$ MC converges to value, $V(s)$, that are sample averages of the actual returns experienced after visiting each state $s$. These are optimal estimate in the sense that they minimize the mean-squared error from the actual returns in the training set. In this sense it is surprising that the batch TD method was able to perform better according to the root mean-squared error measure shown in the top figure. How is it that batch TD was able to perform better than this optimal methods? Consider the example in below box:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/example_6_4.png" alt="example6_4"></p><p>Example illustrates a general difference between the estimates founds by batch TD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the <strong>certainty-equivalence estimate</strong>.</p><h3 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a><strong>Sarsa</strong></h3><p>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\right]<br>$$</p><p>This update is done after every transition from a nonterminal state $S_t$. If $S_{t+1}$ is terminal, then $Q(S_{t+1}, A_{t+1})$ is defined as zero. This rule uses every element of the quintuple of events, $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that make up a transition from one state-action pair to the next. This quintuple gives rise to the name <em>Sarsa</em> for the algorithm. The backup diagram for Sarsa is as shown to the bottom.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/sarsabg.png" alt="sarsa_bg"></p><p>The general form of the Sarsa control algorithm is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png" alt="sarsa"></p><h4 id="Example-Windy-Gridworld"><a href="#Example-Windy-Gridworld" class="headerlink" title="Example: Windy Gridworld"></a><strong>Example: Windy Gridworld</strong></h4><p>The figure below is a standard grid-world, with start and goal states, but with one diﬀerence: there is a crosswind upward through the middle of the grid. The actions are the standard four—up, down,right, and left—but in the middle region the resultant next states are shifted upward by a “wind,” the strength of which varies from column to column. The strength of the wind is given below each column, in number of cells shifted upward. For example, if you are one cell to the right of the goal, then the action left takes you to the cell just above the goal. Let us treat this as an undiscounted episodic task, with constant rewards of −1 until the goal state is reached.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_gridworld.png" alt="windy_gridworld"></p><p>To demonstrate the problem clearly, we use the <a href="https://gym.openai.com/" target="_blank" rel="external">OpenAI gym</a> toolkit to develop the algorithm.</p><p>First of all, we need to define a environment (the windy grid world):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># represents every action as a integer</span></div><div class="line">UP = <span class="number">0</span></div><div class="line">RIGHT = <span class="number">1</span></div><div class="line">DOWN = <span class="number">2</span></div><div class="line">LEFT = <span class="number">3</span></div></pre></td></tr></table></figure><p>The environment is a class that inherit the gym default class <strong>discrete.DiscreteEnv</strong> (shows that the states are discrete):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">WindyGridworldEnv</span><span class="params">(discrete.DiscreteEnv)</span></span></div></pre></td></tr></table></figure><p>First we need to construct our world:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">    self.shape = (<span class="number">7</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="comment"># the number of all states</span></div><div class="line">    nS = np.prod(self.shape)</div><div class="line">    <span class="comment"># the number of all actions</span></div><div class="line">    nA = <span class="number">4</span></div><div class="line"></div><div class="line">    <span class="comment"># Wind strength</span></div><div class="line">    winds = np.zeros(self.shape)</div><div class="line">    winds[:,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>]] = <span class="number">1</span></div><div class="line">    winds[:,[<span class="number">6</span>,<span class="number">7</span>]] = <span class="number">2</span></div><div class="line"></div><div class="line">    <span class="comment"># Calculate transition probabilities</span></div><div class="line">    <span class="comment"># P is the transition matrix</span></div><div class="line">    P = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(nS):</div><div class="line">        position = np.unravel_index(s, self.shape)</div><div class="line">        P[s] = &#123; a : [] <span class="keyword">for</span> a <span class="keyword">in</span> range(nA) &#125;</div><div class="line">        P[s][UP] = self._calculate_transition_prob(position, [<span class="number">-1</span>, <span class="number">0</span>], winds)</div><div class="line">        P[s][RIGHT] = self._calculate_transition_prob(position, [<span class="number">0</span>, <span class="number">1</span>], winds)</div><div class="line">        P[s][DOWN] = self._calculate_transition_prob(position, [<span class="number">1</span>, <span class="number">0</span>], winds)</div><div class="line">        P[s][LEFT] = self._calculate_transition_prob(position, [<span class="number">0</span>, <span class="number">-1</span>], winds)</div><div class="line"></div><div class="line">    <span class="comment"># We always start in state (3, 0)</span></div><div class="line">    isd = np.zeros(nS)</div><div class="line">    isd[np.ravel_multi_index((<span class="number">3</span>,<span class="number">0</span>), self.shape)] = <span class="number">1.0</span></div><div class="line"></div><div class="line">    super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)</div></pre></td></tr></table></figure><p>This is natural, uh? Notice that there is a method called <strong>_calculate_transition_prob</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_calculate_transition_prob</span><span class="params">(self, current, delta, winds)</span>:</span></div><div class="line">        new_position = np.array(current) + np.array(delta) + np.array([<span class="number">-1</span>, <span class="number">0</span>]) * winds[tuple(current)]</div><div class="line">        new_position = self._limit_coordinates(new_position).astype(int)</div><div class="line">        new_state = np.ravel_multi_index(tuple(new_position), self.shape)</div><div class="line">        is_done = tuple(new_position) == (<span class="number">3</span>, <span class="number">7</span>)</div><div class="line">        <span class="keyword">return</span> [(<span class="number">1.0</span>, new_state, <span class="number">-1.0</span>, is_done)]</div></pre></td></tr></table></figure><p>and <strong>_limit_corrdinates</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_limit_coordinates</span><span class="params">(self, coord)</span>:</span></div><div class="line">    coord[<span class="number">0</span>] = min(coord[<span class="number">0</span>], self.shape[<span class="number">0</span>] - <span class="number">1</span>)</div><div class="line">    coord[<span class="number">0</span>] = max(coord[<span class="number">0</span>], <span class="number">0</span>)</div><div class="line">    coord[<span class="number">1</span>] = min(coord[<span class="number">1</span>], self.shape[<span class="number">1</span>] - <span class="number">1</span>)</div><div class="line">    coord[<span class="number">1</span>] = max(coord[<span class="number">1</span>], <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> coord</div></pre></td></tr></table></figure><p>It is worth to mention that the default gym environment class has some useful parameters: <strong>nS</strong>, <strong>nA</strong>, <strong>P</strong> and <strong>is_done</strong>. nS is the total number of states and nA is the total number of actions (here assume all states only could take the same fixed actions). P is the state transition matrix, the default environment class has a <strong>step</strong> method (accept a parameter <strong>action</strong>) that could generates episode automatically according the P and is_done that represents whether a state is terminal state or not.</p><p>Finally, we define a output method for pretty show the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_render</span><span class="params">(self, mode=<span class="string">'human'</span>, close=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> close:</div><div class="line">        <span class="keyword">return</span></div><div class="line"></div><div class="line">    outfile = StringIO() <span class="keyword">if</span> mode == <span class="string">'ansi'</span> <span class="keyword">else</span> sys.stdout</div><div class="line"></div><div class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(self.nS):</div><div class="line">        position = np.unravel_index(s, self.shape)</div><div class="line">        <span class="comment"># print(self.s)</span></div><div class="line">        <span class="keyword">if</span> self.s == s:</div><div class="line">            output = <span class="string">" x "</span></div><div class="line">        <span class="keyword">elif</span> position == (<span class="number">3</span>,<span class="number">7</span>):</div><div class="line">            output = <span class="string">" T "</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            output = <span class="string">" o "</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> position[<span class="number">1</span>] == <span class="number">0</span>:</div><div class="line">            output = output.lstrip()</div><div class="line">        <span class="keyword">if</span> position[<span class="number">1</span>] == self.shape[<span class="number">1</span>] - <span class="number">1</span>:</div><div class="line">            output = output.rstrip()</div><div class="line">            output += <span class="string">"\n"</span></div><div class="line"></div><div class="line">        outfile.write(output)</div><div class="line">    outfile.write(<span class="string">"\n"</span>)</div></pre></td></tr></table></figure><p>Then, let us test our model：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">env = WindyGridworldEnv()</div><div class="line"></div><div class="line">print(env.reset())</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">2</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_render.png" alt="windy_show"></p><p>Each state transition, the step method return a tuple <strong>(next_state, reward, is_done, some_extra_info)</strong>.</p><p>Next, we define the episodes generation policy:</p><p>def make_epsilon_greedy_policy(Q, epsilon, nA):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">Creates an epsilon-greedy policy based on a given Q-function and epsilon.</div><div class="line"></div><div class="line">Args:</div><div class="line">    Q: A dictionary that maps from state -&gt; action-values.</div><div class="line">        Each value is a numpy array of length nA (see below)</div><div class="line">    epsilon: The probability to select a random action . float between 0 and 1.</div><div class="line">    nA: Number of actions in the environment.</div><div class="line"></div><div class="line">Returns:</div><div class="line">    A function that takes the observation as an argument and returns</div><div class="line">    the probabilities for each action in the form of a numpy array of length nA.</div><div class="line"></div><div class="line">"""</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></div><div class="line">    A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">    best_action = np.argmax(Q[observation])</div><div class="line">    A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">    <span class="keyword">return</span> A</div><div class="line"><span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Now, let us implement the sarsa algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sarsa</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, alpha=<span class="number">0.5</span>, epsilon=<span class="number">0.1</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        alpha: TD learning rate.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A tuple (Q, stats).</div><div class="line">        Q is the optimal action-value function, a dictionary mapping state -&gt; action values.</div><div class="line">        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># The final action-value function.</span></div><div class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></div><div class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</div><div class="line">    </div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))</div><div class="line"></div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</div><div class="line">    </div><div class="line"></div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode + <span class="number">1</span>, num_episodes), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Implement this!</span></div><div class="line">        state = env.reset()</div><div class="line">        action_probs = policy(state)</div><div class="line">        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">        </div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">            next_state, reward, is_done, _ = env.step(action)</div><div class="line">            next_action_probs = policy(next_state)</div><div class="line">            </div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)</div><div class="line">            Q[state][action] += alpha * (reward + discount_factor * Q[next_state][next_action] - Q[state][action])</div><div class="line">            </div><div class="line">            <span class="keyword">if</span> is_done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">            </div><div class="line">            state = next_state</div><div class="line">            action = next_action</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Q, stats</div></pre></td></tr></table></figure><p>For understand easily, we put the pesudo-code here again:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png" alt="sarsa"></p><p>The results (with $\varepsilon=0.1,\ \alpha=0.5$) are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png" alt="sarsa_result"></p><p>The increasing slope (bottom figure) of the graph shows that the goal is reached more and more quickly over time. Note that Monte Carlo methods cannot easily be used on this task because termination is not guaranteed for all policies. If a policy was ever found that caused the agent to stay in the same state, then the next episode would never end. Step-by-step learning methods such as Sarsa do not have this problem because they quickly learn <strong>during the episode</strong> that such<br>policies are poor, and switch to something else.</p><h3 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h3><p>One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning, defined by<br>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)\right]<br>$$<br>The algorithm is shown in procedural form in the box below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning.png" alt="q_learning"></p><p>And below is the backup diagram:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_bg.png" alt="q_bg"></p><h4 id="Example-Cliff-Walking"><a href="#Example-Cliff-Walking" class="headerlink" title="Example: Cliff Walking"></a>Example: Cliff Walking</h4><p>This grid world example compares Sarsa and Q-learning, highlighting the difference between on-policy (Sarsa) and off-policy (Q-learning) methods. Consider the grid world shown in the figure below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_world.png" alt="cliff_world"></p><p>The same as earlier, we define the environment first. But the new environment just changes a little, so we just paste the code <a href="https://github.com/ewanlee/reinforcement-learning/blob/master/lib/envs/cliff_walking.py" target="_blank" rel="external">here</a>.</p><p>Let us test the environment first:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">env = CliffWalkingEnv()</div><div class="line"></div><div class="line">print(env.reset())</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">0</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">2</span>))</div><div class="line">env.render()</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_show.png" alt="cliff_walk_show"></p><p>Not bad.</p><p>Then, let us develop the Q-learning algorithm (the episodes generation policy is not change):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_learning</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, alpha=<span class="number">0.5</span>, epsilon=<span class="number">0.1</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy</div><div class="line">    while following an epsilon-greedy policy</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        alpha: TD learning rate.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A tuple (Q, episode_lengths).</div><div class="line">        Q is the optimal action-value function, a dictionary mapping state -&gt; action values.</div><div class="line">        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># The final action-value function.</span></div><div class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></div><div class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))    </div><div class="line">    </div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode + <span class="number">1</span>, num_episodes), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Reset the environment and pick the first action</span></div><div class="line">        state = env.reset()</div><div class="line">        </div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="comment"># total_reward = 0.0</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">            </div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            action_probs = policy(state)</div><div class="line">            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            next_state, reward, done, _ = env.step(action)</div><div class="line"></div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            <span class="comment"># TD Update</span></div><div class="line">            best_next_action = np.argmax(Q[next_state])    </div><div class="line">            td_target = reward + discount_factor * Q[next_state][best_next_action]</div><div class="line">            td_delta = td_target - Q[state][action]</div><div class="line">            Q[state][action] += alpha * td_delta</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">                </div><div class="line">            state = next_state</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Q, stats</div></pre></td></tr></table></figure><p>Results ($\varepsilon=0.1$) are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning_result.png" alt="q_learning_result"></p><p>For compare convenience, we put the result of Sarsa here again:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png" alt="sarsa_result"></p><p>We can see, for average, After an initial transient, Q-learning learns values for the optimal policy, that which travels right along the edge of the cliﬀ. Unfortunately, this results in its occasionally falling oﬀ the cliﬀ because of the ε-greedy action selection. Sarsa, on the other hand, takes the action selection into account and learns the longer but safer path through the upper part of the<br>grid. Although Q-learning actually learns the values of the optimal policy, its online performance is worse than that of Sarsa, which learns the roundabout policy. Of course, if ε were gradually reduced, then both methods would asymptotically converge to the optimal policy.</p><h3 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h3><p>Consider the learning algorithm that is just like Q-learning except that instead of the maximum over next state-action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule<br>$$<br>\begin{align}<br>Q(S_t, A_t) &amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \mathbb{E}[Q(S_{t+1}, A_{t+1} \ | \ S_{t+1})] - Q(S_t, A_t) \right ] \\<br>&amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1}, a) - Q(S_t, A_t) \right ],<br>\end{align}<br>$$<br>but that otherwise follows the schema of Q-learning. Its backup diagram is shown below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/esarsa_bg.png" alt="esarsa_bg"></p><p>For compare the results on the cliff-walking task with Excepted Sarsa with Sarsa and Q-learning, we develop another <a href="https://github.com/ewanlee/reinforcement-learning-an-introduction/blob/master/chapter06/CliffWalking.py" target="_blank" rel="external">codes</a> (here we are not use the OpenAI gym toolkit).</p><p>The first we define some truth of the environment:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># world height</span></div><div class="line">WORLD_HEIGHT = <span class="number">4</span></div><div class="line"></div><div class="line"><span class="comment"># world width</span></div><div class="line">WORLD_WIDTH = <span class="number">12</span></div><div class="line"></div><div class="line"><span class="comment"># probability for exploration</span></div><div class="line">EPSILON = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># step size</span></div><div class="line">ALPHA = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># gamma for Q-Learning and Expected Sarsa</span></div><div class="line">GAMMA = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">ACTION_UP = <span class="number">0</span></div><div class="line">ACTION_DOWN = <span class="number">1</span></div><div class="line">ACTION_LEFT = <span class="number">2</span></div><div class="line">ACTION_RIGHT = <span class="number">3</span></div><div class="line">actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]</div><div class="line"></div><div class="line"><span class="comment"># initial state action pair values</span></div><div class="line">stateActionValues = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, <span class="number">4</span>))</div><div class="line">startState = [<span class="number">3</span>, <span class="number">0</span>]</div><div class="line">goalState = [<span class="number">3</span>, <span class="number">11</span>]</div><div class="line"></div><div class="line"><span class="comment"># reward for each action in each state</span></div><div class="line">actionRewards = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, <span class="number">4</span>))</div><div class="line">actionRewards[:, :, :] = <span class="number">-1.0</span></div><div class="line">actionRewards[<span class="number">2</span>, <span class="number">1</span>:<span class="number">11</span>, ACTION_DOWN] = <span class="number">-100.0</span></div><div class="line">actionRewards[<span class="number">3</span>, <span class="number">0</span>, ACTION_RIGHT] = <span class="number">-100.0</span></div></pre></td></tr></table></figure><p>And then we define the state transitions:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># set up destinations for each action in each state</span></div><div class="line">actionDestination = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_HEIGHT):</div><div class="line">    actionDestination.append([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_WIDTH):</div><div class="line">        destinaion = dict()</div><div class="line">        destinaion[ACTION_UP] = [max(i - <span class="number">1</span>, <span class="number">0</span>), j]</div><div class="line">        destinaion[ACTION_LEFT] = [i, max(j - <span class="number">1</span>, <span class="number">0</span>)]</div><div class="line">        destinaion[ACTION_RIGHT] = [i, min(j + <span class="number">1</span>, WORLD_WIDTH - <span class="number">1</span>)]</div><div class="line">        <span class="keyword">if</span> i == <span class="number">2</span> <span class="keyword">and</span> <span class="number">1</span> &lt;= j &lt;= <span class="number">10</span>:</div><div class="line">            destinaion[ACTION_DOWN] = startState</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            destinaion[ACTION_DOWN] = [min(i + <span class="number">1</span>, WORLD_HEIGHT - <span class="number">1</span>), j]</div><div class="line">        actionDestination[<span class="number">-1</span>].append(destinaion)</div><div class="line">actionDestination[<span class="number">3</span>][<span class="number">0</span>][ACTION_RIGHT] = startState</div></pre></td></tr></table></figure><p>We also need a policy to generate the next action according to the current state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># choose an action based on epsilon greedy algorithm</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseAction</span><span class="params">(state, stateActionValues)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(actions)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> np.argmax(stateActionValues[state[<span class="number">0</span>], state[<span class="number">1</span>], :])</div></pre></td></tr></table></figure><p>The <strong>stateActionValues</strong> just is the Q.</p><p>Then, let us develop the Sarsa (and Excepted Sarsa) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># an episode with Sarsa</span></div><div class="line"><span class="comment"># @stateActionValues: values for state action pair, will be updated</span></div><div class="line"><span class="comment"># @expected: if True, will use expected Sarsa algorithm</span></div><div class="line"><span class="comment"># @stepSize: step size for updating</span></div><div class="line"><span class="comment"># @return: total rewards within this episode</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sarsa</span><span class="params">(stateActionValues, expected=False, stepSize=ALPHA)</span>:</span></div><div class="line">    currentState = startState</div><div class="line">    currentAction = chooseAction(currentState, stateActionValues)</div><div class="line">    rewards = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState != goalState:</div><div class="line">        newState = actionDestination[currentState[<span class="number">0</span>]][currentState[<span class="number">1</span>]][currentAction]</div><div class="line">        newAction = chooseAction(newState, stateActionValues)</div><div class="line">        reward = actionRewards[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction]</div><div class="line">        rewards += reward</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> expected:</div><div class="line">            valueTarget = stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], newAction]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># calculate the expected value of new state</span></div><div class="line">            valueTarget = <span class="number">0.0</span></div><div class="line">            actions_list = stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], :]</div><div class="line">            bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist()</div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                <span class="keyword">if</span> action <span class="keyword">in</span> bestActions:</div><div class="line">                    valueTarget += ((<span class="number">1.0</span> - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    valueTarget += EPSILON / len(actions) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div><div class="line">        valueTarget *= GAMMA</div><div class="line">        <span class="comment"># Sarsa update</span></div><div class="line">        stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction] += stepSize * (reward +</div><div class="line">            valueTarget - stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction])</div><div class="line">        currentState = newState</div><div class="line">        currentAction = newAction</div><div class="line">    <span class="keyword">return</span> rewards</div></pre></td></tr></table></figure><p>Because we develop the Sarsa algorithm earlier, so we just concentrate on the Excepted Sarsa algorithm here:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># calculate the expected value of new state</span></div><div class="line">valueTarget = <span class="number">0.0</span></div><div class="line">actions_list = stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], :]</div><div class="line">bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist()</div><div class="line"><span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">    <span class="keyword">if</span> action <span class="keyword">in</span> bestActions:</div><div class="line">        valueTarget += ((<span class="number">1.0</span> - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        valueTarget += EPSILON / len(actions) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div></pre></td></tr></table></figure><p>By the way, let us develop the Q-learning algorithm again:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># an episode with Q-Learning</span></div><div class="line"><span class="comment"># @stateActionValues: values for state action pair, will be updated</span></div><div class="line"><span class="comment"># @expected: if True, will use expected Sarsa algorithm</span></div><div class="line"><span class="comment"># @stepSize: step size for updating</span></div><div class="line"><span class="comment"># @return: total rewards within this episode</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">qLearning</span><span class="params">(stateActionValues, stepSize=ALPHA)</span>:</span></div><div class="line">    currentState = startState</div><div class="line">    rewards = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState != goalState:</div><div class="line">        currentAction = chooseAction(currentState, stateActionValues)</div><div class="line">        reward = actionRewards[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction]</div><div class="line">        rewards += reward</div><div class="line">        newState = actionDestination[currentState[<span class="number">0</span>]][currentState[<span class="number">1</span>]][currentAction]</div><div class="line">        <span class="comment"># Q-Learning update</span></div><div class="line">        stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction] += stepSize * (</div><div class="line">            reward + GAMMA * np.max(stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], :]) -</div><div class="line">            stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction])</div><div class="line">        currentState = newState</div><div class="line">    <span class="keyword">return</span> rewards</div></pre></td></tr></table></figure><p>Now we can see the optimal policy in each state of both algorithm (we are not mentioned earlier):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># print optimal policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">printOptimalPolicy</span><span class="params">(stateActionValues)</span>:</span></div><div class="line">    optimalPolicy = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_HEIGHT):</div><div class="line">        optimalPolicy.append([])</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_WIDTH):</div><div class="line">            <span class="keyword">if</span> [i, j] == goalState:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'G'</span>)</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            bestAction = np.argmax(stateActionValues[i, j, :])</div><div class="line">            <span class="keyword">if</span> bestAction == ACTION_UP:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'U'</span>)</div><div class="line">            <span class="keyword">elif</span> bestAction == ACTION_DOWN:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'D'</span>)</div><div class="line">            <span class="keyword">elif</span> bestAction == ACTION_LEFT:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'L'</span>)</div><div class="line">            <span class="keyword">elif</span> bestAction == ACTION_RIGHT:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'R'</span>)</div><div class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> optimalPolicy:</div><div class="line">        print(row)</div><div class="line"></div><div class="line"><span class="comment"># averaging the reward sums from 10 successive episodes</span></div><div class="line">averageRange = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># episodes of each run</span></div><div class="line">nEpisodes = <span class="number">500</span></div><div class="line"></div><div class="line"><span class="comment"># perform 20 independent runs</span></div><div class="line">runs = <span class="number">20</span></div><div class="line"></div><div class="line">rewardsSarsa = np.zeros(nEpisodes)</div><div class="line">rewardsQLearning = np.zeros(nEpisodes)</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    stateActionValuesSarsa = np.copy(stateActionValues)</div><div class="line">    stateActionValuesQLearning = np.copy(stateActionValues)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        <span class="comment"># cut off the value by -100 to draw the figure more elegantly</span></div><div class="line">        rewardsSarsa[i] += max(sarsa(stateActionValuesSarsa), <span class="number">-100</span>)</div><div class="line">        rewardsQLearning[i] += max(qLearning(stateActionValuesQLearning), <span class="number">-100</span>)</div><div class="line"></div><div class="line"><span class="comment"># averaging over independt runs</span></div><div class="line">rewardsSarsa /= runs</div><div class="line">rewardsQLearning /= runs</div><div class="line"></div><div class="line"><span class="comment"># averaging over successive episodes</span></div><div class="line">smoothedRewardsSarsa = np.copy(rewardsSarsa)</div><div class="line">smoothedRewardsQLearning = np.copy(rewardsQLearning)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(averageRange, nEpisodes):</div><div class="line">    smoothedRewardsSarsa[i] = np.mean(rewardsSarsa[i - averageRange: i + <span class="number">1</span>])</div><div class="line">    smoothedRewardsQLearning[i] = np.mean(rewardsQLearning[i - averageRange: i + <span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="comment"># display optimal policy</span></div><div class="line">print(<span class="string">'Sarsa Optimal Policy:'</span>)</div><div class="line">printOptimalPolicy(stateActionValuesSarsa)</div><div class="line">print(<span class="string">'Q-Learning Optimal Policy:'</span>)</div><div class="line">printOptimalPolicy(stateActionValuesQLearning)</div></pre></td></tr></table></figure><p>The results are as follows (emits the results of the changes of reward):</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_opti_policy.png" alt="cliff_walk_optimal_policy"></p><p>Now let us compare the three algorithms:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">stepSizes = np.arange(<span class="number">0.1</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line">    nEpisodes = <span class="number">1000</span></div><div class="line">    runs = <span class="number">10</span></div><div class="line"></div><div class="line">    ASY_SARSA = <span class="number">0</span></div><div class="line">    ASY_EXPECTED_SARSA = <span class="number">1</span></div><div class="line">    ASY_QLEARNING = <span class="number">2</span></div><div class="line">    INT_SARSA = <span class="number">3</span></div><div class="line">    INT_EXPECTED_SARSA = <span class="number">4</span></div><div class="line">    INT_QLEARNING = <span class="number">5</span></div><div class="line">    methods = range(<span class="number">0</span>, <span class="number">6</span>)</div><div class="line"></div><div class="line">    performace = np.zeros((<span class="number">6</span>, len(stepSizes)))</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        <span class="keyword">for</span> ind, stepSize <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(stepSizes)), stepSizes):</div><div class="line">            stateActionValuesSarsa = np.copy(stateActionValues)</div><div class="line">            stateActionValuesExpectedSarsa = np.copy(stateActionValues)</div><div class="line">            stateActionValuesQLearning = np.copy(stateActionValues)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'step size:'</span>, stepSize, <span class="string">'episode:'</span>, ep)</div><div class="line">                sarsaReward = sarsa(stateActionValuesSarsa, expected=<span class="keyword">False</span>, stepSize=stepSize)</div><div class="line">                expectedSarsaReward = sarsa(stateActionValuesExpectedSarsa, expected=<span class="keyword">True</span>, stepSize=stepSize)</div><div class="line">                qLearningReward = qLearning(stateActionValuesQLearning, stepSize=stepSize)</div><div class="line">                performace[ASY_SARSA, ind] += sarsaReward</div><div class="line">                performace[ASY_EXPECTED_SARSA, ind] += expectedSarsaReward</div><div class="line">                performace[ASY_QLEARNING, ind] += qLearningReward</div><div class="line"></div><div class="line">                <span class="keyword">if</span> ep &lt; <span class="number">100</span>:</div><div class="line">                    performace[INT_SARSA, ind] += sarsaReward</div><div class="line">                    performace[INT_EXPECTED_SARSA, ind] += expectedSarsaReward</div><div class="line">                    performace[INT_QLEARNING, ind] += qLearningReward</div><div class="line"></div><div class="line">    performace[:<span class="number">3</span>, :] /= nEpisodes * runs</div><div class="line">    performace[<span class="number">3</span>:, :] /= runs * <span class="number">100</span></div><div class="line">    labels = [<span class="string">'Asymptotic Sarsa'</span>, <span class="string">'Asymptotic Expected Sarsa'</span>, <span class="string">'Asymptotic Q-Learning'</span>,</div><div class="line">              <span class="string">'Interim Sarsa'</span>, <span class="string">'Interim Expected Sarsa'</span>, <span class="string">'Interim Q-Learning'</span>]</div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    <span class="keyword">for</span> method, label <span class="keyword">in</span> zip(methods, labels):</div><div class="line">        plt.plot(stepSizes, performace[method, :], label=label)</div><div class="line">    plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">    plt.ylabel(<span class="string">'reward per episode'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_3_results.png" alt="compare3algo_cliff_walk"></p><p>As an on-policy method, Expected Sarsa retains the signiﬁcant advantage of Sarsa over Q-learning on this problem. In addition, Expected Sarsa shows a signiﬁcant improvement over Sarsa over a wide range of values for the step-size parameter α. In cliﬀ walking the state transitions are all deterministic and all randomness comes from the policy. In such cases, Expected Sarsa can safely set α = 1 without suﬀering any degradation of asymptotic performance, whereas Sarsa can only perform well in the long run at a small value of α, at which short-term performance is poor. In this and other examples there is a consistent empirical advantage of Expected Sarsa over Sarsa. Except for the small additional computational cost, Expected Sarsa may completely dominate both of the other more-well-known TD control algorithms.</p><h3 id="Double-Q-learning"><a href="#Double-Q-learning" class="headerlink" title="Double Q-learning"></a>Double Q-learning</h3><p>All the control algorithms that we have discussed so far involve maximization in the construction of their target policies. For example, in Q-learning the target policy is the greedy policy given the current action values, which is deﬁned with a max, and in Sarsa the policy is often ε-greedy, which also involves a maximization operation. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a signiﬁcant positive bias. To see why, consider a single state s where there are many actions a whose true values, $q(s, a)$ are all zero but whose estimated values, $Q(s, a)$, are uncertain and thus distributed some above and some below zero. The maximum of the true values is zero, but the maximum of the estimates is positive, a positive bias. We call this maximization<br>bias.</p><h4 id="Example-Maximization-Bias"><a href="#Example-Maximization-Bias" class="headerlink" title="Example: Maximization Bias"></a>Example: Maximization Bias</h4><p>We have a small MDP:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/mb.png" alt="mb"></p><p>the expected return for any trajectory starting with left (from <strong>B</strong>) is −0.1, and thus taking left in state A is always a mistake. Nevertheless, our control methods may favor left because of maximization bias making B appear to have a positive value. The results (paste later) shows that Q-learning with ε-greedy action selection initially learns to strongly favor the left action on this example. Even at asymptote, Q-learning takes the left action about 5% more often than is optimal at our parameter settings (ε = 0.1, α = 0.1, and γ = 1).</p><p>We could use the Double Q-learning algorithm to avoid this problem. One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. Suppose we divided the plays in two sets and used them to learn two independent estimates.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/dbq.png" alt="dbq"></p><p>Of course there are also doubled versions of Sarsa and Expected Sarsa.</p><p>Now let us develop the both algorithms and compare their performance on the earlier example. First we define the problem environment:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># state A</span></div><div class="line">STATE_A = <span class="number">0</span></div><div class="line"></div><div class="line"><span class="comment"># state B</span></div><div class="line">STATE_B = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># use one terminal state</span></div><div class="line">STATE_TERMINAL = <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># starts from state A</span></div><div class="line">STATE_START = STATE_A</div><div class="line"></div><div class="line"><span class="comment"># possible actions in A</span></div><div class="line">ACTION_A_RIGHT = <span class="number">0</span></div><div class="line">ACTION_A_LEFT = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># possible actions in B, maybe 10 actions</span></div><div class="line">actionsOfB = range(<span class="number">0</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">stateActions = [[ACTION_A_RIGHT, ACTION_A_LEFT], actionsOfB]</div><div class="line"></div><div class="line"><span class="comment"># state action pair values, if a state is a terminal state, then the value is always 0</span></div><div class="line">stateActionValues = [np.zeros(<span class="number">2</span>), np.zeros(len(actionsOfB)), np.zeros(<span class="number">1</span>)]</div><div class="line"></div><div class="line"><span class="comment"># set up destination for each state and each action</span></div><div class="line">actionDestination = [[STATE_TERMINAL, STATE_B], [STATE_TERMINAL] * len(actionsOfB)]</div><div class="line"></div><div class="line"><span class="comment"># probability for exploration</span></div><div class="line">EPSILON = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># step size</span></div><div class="line">ALPHA = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># discount for max value</span></div><div class="line">GAMMA = <span class="number">1.0</span></div></pre></td></tr></table></figure><p>And we need a policy to take an action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># choose an action based on epsilon greedy algorithm</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseAction</span><span class="params">(state, stateActionValues)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(stateActions[state])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> argmax(stateActionValues[state])</div></pre></td></tr></table></figure><p>After take an action, we get the reward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take @action in @state, return the reward</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(state, action)</span>:</span></div><div class="line">    <span class="keyword">if</span> state == STATE_A:</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> np.random.normal(<span class="number">-0.1</span>, <span class="number">1</span>)</div></pre></td></tr></table></figure><p>Next, we develop the Double Q-learning algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># if there are two state action pair value array, use double Q-Learning</span></div><div class="line"><span class="comment"># otherwise use normal Q-Learning</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">qLearning</span><span class="params">(stateActionValues, stateActionValues2=None)</span>:</span></div><div class="line">    currentState = STATE_START</div><div class="line">    <span class="comment"># track the # of action left in state A</span></div><div class="line">    leftCount = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> currentState != STATE_TERMINAL:</div><div class="line">        <span class="keyword">if</span> stateActionValues2 <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            currentAction = chooseAction(currentState, stateActionValues)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># derive a action form Q1 and Q2</span></div><div class="line">            currentAction = chooseAction(currentState, [item1 + item2 <span class="keyword">for</span> item1, item2 <span class="keyword">in</span> zip(stateActionValues, stateActionValues2)])</div><div class="line">        <span class="keyword">if</span> currentState == STATE_A <span class="keyword">and</span> currentAction == ACTION_A_LEFT:</div><div class="line">            leftCount += <span class="number">1</span></div><div class="line">        reward = takeAction(currentState, currentAction)</div><div class="line">        newState = actionDestination[currentState][currentAction]</div><div class="line">        <span class="keyword">if</span> stateActionValues2 <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            currentStateActionValues = stateActionValues</div><div class="line">            targetValue = np.max(currentStateActionValues[newState])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">                currentStateActionValues = stateActionValues</div><div class="line">                anotherStateActionValues = stateActionValues2</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                currentStateActionValues = stateActionValues2</div><div class="line">                anotherStateActionValues = stateActionValues</div><div class="line">            bestAction = argmax(currentStateActionValues[newState])</div><div class="line">            targetValue = anotherStateActionValues[newState][bestAction]</div><div class="line"></div><div class="line">        <span class="comment"># Q-Learning update</span></div><div class="line">        currentStateActionValues[currentState][currentAction] += ALPHA * (</div><div class="line">            reward + GAMMA * targetValue - currentStateActionValues[currentState][currentAction])</div><div class="line">        currentState = newState</div><div class="line">    <span class="keyword">return</span> leftCount</div></pre></td></tr></table></figure><p>And now, let us solve the example problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># each independent run has 300 episodes</span></div><div class="line">    episodes = <span class="number">300</span></div><div class="line">    leftCountsQ = np.zeros(episodes)</div><div class="line">    leftCountsDoubleQ = np.zeros(episodes)</div><div class="line">    runs = <span class="number">1000</span></div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        print(<span class="string">'run:'</span>, run)</div><div class="line">        stateActionValuesQ = [np.copy(item) <span class="keyword">for</span> item <span class="keyword">in</span> stateActionValues]</div><div class="line">        stateActionValuesDoubleQ1 = [np.copy(item) <span class="keyword">for</span> item <span class="keyword">in</span> stateActionValues]</div><div class="line">        stateActionValuesDoubleQ2 = [np.copy(item) <span class="keyword">for</span> item <span class="keyword">in</span> stateActionValues]</div><div class="line">        leftCountsQ_ = [<span class="number">0</span>]</div><div class="line">        leftCountsDoubleQ_ = [<span class="number">0</span>]</div><div class="line">        <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            leftCountsQ_.append(leftCountsQ_[<span class="number">-1</span>] + qLearning(stateActionValuesQ))</div><div class="line">            leftCountsDoubleQ_.append(leftCountsDoubleQ_[<span class="number">-1</span>] + qLearning(stateActionValuesDoubleQ1, stateActionValuesDoubleQ2))</div><div class="line">        <span class="keyword">del</span> leftCountsQ_[<span class="number">0</span>]</div><div class="line">        <span class="keyword">del</span> leftCountsDoubleQ_[<span class="number">0</span>]</div><div class="line">        leftCountsQ += np.asarray(leftCountsQ_, dtype=<span class="string">'float'</span>) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">        leftCountsDoubleQ += np.asarray(leftCountsDoubleQ_, dtype=<span class="string">'float'</span>) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">    leftCountsQ /= runs</div><div class="line">    leftCountsDoubleQ /= runs</div><div class="line">    plt.figure()</div><div class="line">    plt.plot(leftCountsQ, label=<span class="string">'Q-Learning'</span>)</div><div class="line">    plt.plot(leftCountsDoubleQ, label=<span class="string">'Double Q-Learning'</span>)</div><div class="line">    plt.plot(np.ones(episodes) * <span class="number">0.05</span>, label=<span class="string">'Optimal'</span>)</div><div class="line">    plt.xlabel(<span class="string">'episodes'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% left actions from A'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Ok, results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/dbq_result.png" alt="dbq_result"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be &lt;em&gt;temporal-difference&lt;/em&gt; (TD)
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="TD" scheme="http://yoursite.com/tags/TD/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Resources</title>
    <link href="http://yoursite.com/2017/06/30/Reinforcement-Learning-Resources/"/>
    <id>http://yoursite.com/2017/06/30/Reinforcement-Learning-Resources/</id>
    <published>2017-06-30T13:07:44.000Z</published>
    <updated>2017-11-20T05:23:01.172Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Books"><a href="#Books" class="headerlink" title="Books"></a>Books</h1><ul><li>Sutton’s book has new <a href="https://github.com/ewanlee/RL-Resources/blob/master/book/bookdraft2017june.pdf" target="_blank" rel="external">update</a> (draft, version 2017) !</li><li><a href="https://github.com/ewanlee/RL-Resources/blob/master/book/RLAlgsInMDPs.pdf" target="_blank" rel="external">Algorithms for Reinforcement Learning (Morgan)</a></li></ul><h1 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h1><h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><ul><li><a href="https://arxiv.org/pdf/1509.06461.pdf" target="_blank" rel="external">Deep Reinforcement Learning with Double Q-learning</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Deep-Reinforcement-Learning-with-Double-Q-learning" target="_blank" rel="external">Summary</a></li><li><a href="https://ewanlee.github.io/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/" target="_blank" rel="external">Project</a></li></ul></li><li><a href="http://arxiv.org/abs/1511.05952" target="_blank" rel="external">Prioritized Experience Replay</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Prioritized-Experience-Replay" target="_blank" rel="external">Summary</a></li></ul></li><li><a href="https://arxiv.org/pdf/1511.06581.pdf" target="_blank" rel="external">Dueling Network Architectures for Deep Reinforcement Learning</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Dueling-Network-Architectures-for-Deep-Reinforcement-Learning" target="_blank" rel="external">Summary</a></li><li><a href="https://github.com/rlcode/reinforcement-learning/blob/master/3-atari/1-breakout/breakout_dueling_ddqn.py" target="_blank" rel="external">Project</a></li></ul></li><li><a href="http://iew3.technion.ac.il/CE/files/papers/Learning%20Tetris%20Using%20the%20Noisy%20Cross-Entropy%20Method.pdf" target="_blank" rel="external">Learning Tetris Using the Noisy Cross-Entropy Method</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Learning-Tetris-Using-the-Noisy-Cross-Entropy-Method" target="_blank" rel="external">Summary</a></li><li><a href="https://gist.github.com/andrewliao11/d52125b52f76a4af73433e1cf8405a8f" target="_blank" rel="external">Project</a></li></ul></li><li><a href="https://arxiv.org/abs/1511.03722" target="_blank" rel="external">Doubly Robust Off-policy Value Evaluation for Reinforcement Learning</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Doubly-Robust-Off-policy-Value-Evaluation-for-Reinforcement-Learning" target="_blank" rel="external">Summary</a></li></ul></li><li><a href="https://arxiv.org/abs/1705.08422" target="_blank" rel="external">Continuous State-Space Models for Optimal Sepsis Treatment: a Deep Reinforcement Learning Approach</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Continuous-State-Space-Models-for-Optimal-Sepsis-Treatment-a-Deep-Reinforcement-Learning-Approach" target="_blank" rel="external">Summary</a></li></ul></li><li><a href="https://arxiv.org/abs/1704.06300" target="_blank" rel="external">A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#A-Reinforcement-Learning-Approach-to-Weaning-of-Mechanical-Ventilation-in-Intensive-Care-Units" target="_blank" rel="external">Summary</a></li></ul></li><li><a href="https://drive.google.com/file/d/0B_j5EZzjlxchV2l3TGJPdTljM1k/view" target="_blank" rel="external">Deep Reinforcement Learning, Decision Making and Control (ICML 2017 Tutorial)</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Deep-Reinforcement-Learning-Decision-Making-and-Control-ICML-2017-Tutorial" target="_blank" rel="external">Summary</a></li></ul></li><li><a href="http://www.robots.ox.ac.uk/~mobile/Papers/DeepIRL_2015.pdf" target="_blank" rel="external">Maximum Entropy Deep Inverse Reinforcement Learning</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Maximum-Entropy-Deep-Inverse-Reinforcement-Learning" target="_blank" rel="external">Summary</a></li></ul></li><li><a href="https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf" target="_blank" rel="external">Maximum Entropy Inverse Reinforcement Learning</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Maximum-Entropy-Inverse-Reinforcement-Learning" target="_blank" rel="external">Summary</a></li></ul></li><li><a href="http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf" target="_blank" rel="external">Apprenticeship Learning via Inverse Reinforcement Learning</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Apprenticeship-Learning-via-Inverse-Reinforcement-Learning" target="_blank" rel="external">Summary</a></li></ul></li></ul><h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><ul><li><p><a href="https://arxiv.org/pdf/1711.05225.pdf" target="_blank" rel="external">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</a></p><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#CheXNet-Radiologist-Level-Pneumonia-Detection-on-Chest-X-Rays-with-Deep-Learning" target="_blank" rel="external">Summary</a></li></ul><p>​</p></li></ul><h1 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h1><ul><li><a href="https://ewanlee.github.io/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" target="_blank" rel="external">Using Keras and Deep Q-Network to Play FlappyBird</a></li></ul><h1 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h1><ul><li><a href="https://ewanlee.github.io/2017/07/07/Demystifying-Deep-Reinforcement-Learning/" target="_blank" rel="external">Demystifying Deep Reinforcement Learning</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Books&quot;&gt;&lt;a href=&quot;#Books&quot; class=&quot;headerlink&quot; title=&quot;Books&quot;&gt;&lt;/a&gt;Books&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;Sutton’s book has new &lt;a href=&quot;https://github.com/ewa
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
</feed>
