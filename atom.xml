<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Abracadabra</title>
  <subtitle>Do it yourself</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-01-21T10:35:43.214Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Ewan Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Seq2Seq with Attention and Beam Search [Repost]</title>
    <link href="http://yoursite.com/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/"/>
    <id>http://yoursite.com/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/</id>
    <published>2018-01-21T09:35:50.000Z</published>
    <updated>2018-01-21T10:35:43.214Z</updated>
    
    <content type="html"><![CDATA[<p>Source Post is <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">here</a></p><h2 id="Sequence-to-Sequence-basics"><a href="#Sequence-to-Sequence-basics" class="headerlink" title="Sequence to Sequence basics"></a>Sequence to Sequence basics</h2><p>Let’s explain the sequence to sequence framework as we’ll rely on it for our model. Let’s start with the simplest version on the translation task.</p><blockquote><p>As an example, let’s translate <code>how are you</code> in French <code>comment vas tu</code>.</p></blockquote><h3 id="Vanilla-Seq2Seq"><a href="#Vanilla-Seq2Seq" class="headerlink" title="Vanilla Seq2Seq"></a>Vanilla Seq2Seq</h3><p>The Seq2Seq framework relies on the <strong>encoder-decoder</strong> paradigm. The <strong>encoder</strong> <em>encodes</em> the input sequence, while the <strong>decoder</strong> <em>produces</em> the target sequence</p><p><strong>Encoder</strong></p><p>Our input sequence is <code>how are you</code>. Each word from the input sequence is associated to a vector $w \in \mathbb{R}^d$ (via a lookup table). In our case, we have 3 words, thus our input will be transformed into $[w_0, w_1, w_2] \in \mathbb{R}^{d \times 3}$. Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation $e$. Let’s write the hidden states $[e_0,e_1,e_2]$ (and thus $e=e_2$)</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg" alt="Vanilla Encoder"></p><p><em>Vanilla Encoder</em></p><p><strong>Decoder</strong></p><p>Now that we have a vector ee that captures the meaning of the input sequence, we’ll use it to generate the target sequence word by word. Feed to another LSTM cell: ee as hidden state and a special <em>start of sentence</em> vector $w_{\text{sos}}$ as input. The LSTM computes the next hidden state $h_0 \in \mathbb{R}^{h}$. Then, we apply some function $g : \mathbb{R}^h \mapsto \mathbb{R}^V$ so that $s_0 := g(h_0) \in \mathbb{R}^V$ is a vector of the same size as the vocabulary.<br>$$<br>% &lt;![CDATA[<br>\begin{align<em>}<br>h_0 &amp;= \operatorname{LSTM}\left(e, w_{sos} \right)\\<br>s_0 &amp;= g(h_0)\\<br>p_0 &amp;= \operatorname{softmax}(s_0)\\<br>i_0 &amp;= \operatorname{argmax}(p_0)\\<br>\end{align</em>} %]]&gt;<br>$$<br>Then, apply a softmax to $s_0$ to normalize it into a vector of probabilities $p_0 \in \mathbb{R}^V$ . Now, each entry of $p_0$ will measure how likely is each word in the vocabulary. Let’s say that the word <em>“comment”</em> has the highest probability (and thus $i_0 = \operatorname{argmax}(p_0)$) corresponds to the index of <em>“comment”</em>). Get a corresponding vector and $w_{i_0} = w_{comment}$ repeat the procedure: the LSTM will take $h_0$ as hidden state and $w_{comment}$ as input and will output a probability vector $p_1$ over the second word, etc.<br>$$<br>% &lt;![CDATA[<br>\begin{align<em>}<br>h_1 &amp;= \operatorname{LSTM}\left(h_0, w_{i_0} \right)\\<br>s_1 &amp;= g(h_1)\\<br>p_1 &amp;= \operatorname{softmax}(s_1)\\<br>i_1 &amp;= \operatorname{argmax}(p_1)<br>\end{align</em>} %]]&gt;<br>$$<br>The decoding stops when the predicted word is a special <em>end of sentence</em> token.</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg" alt="Vanilla Decoder"></p><p><em>Vanilla Decoder</em></p><blockquote><p>Intuitively, the hidden vector represents the “amount of meaning” that has not been decoded yet.</p></blockquote><p>The above method aims at modelling the distribution of the next word conditionned on the beginning of the sentence<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_1, \dots, y_{t}, x_0, \dots, x_n \right]<br>$$<br>by writing<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_t, h_{t}, e \right]<br>$$</p><h3 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h3><p>The previous model has been refined over the past few years and greatly benefited from what is known as <strong>attention</strong>. Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM. One way of performing attention is explained by <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Bahdanau et al.</a>. We slightly modify the reccurrence formula that we defined above by adding a new vector $c_t$ to the input of the LSTM<br>$$<br>% &lt;![CDATA[<br>\begin{align<em>}<br>h_{t} &amp;= \operatorname{LSTM}\left(h_{t-1}, [w_{i_{t-1}}, c_t] \right)\\<br>s_t &amp;= g(h_t)\\<br>p_t &amp;= \operatorname{softmax}(s_t)\\<br>i_t &amp;= \operatorname{argmax}(p_t)<br>\end{align</em>} %]]&gt;<br>$$<br>The vector ctct is the attention (or <strong>context</strong>) vector. We compute a new context vector at each decoding step. First, with a function $f (h_{t-1}, e_{t’}) \mapsto \alpha_{t’} \in \mathbb{R}$, compute a score for each hidden state $e_{t^{\prime}}$ of the encoder. Then, normalize the sequence of using a softmax and compute $c_t$ as the weighted average of the $e_{t^{\prime}}$.<br>$$<br>% &lt;![CDATA[<br>\begin{align<em>}<br>\alpha_{t’} &amp;= f(h_{t-1}, e_{t’}) \in \mathbb{R} &amp; \text{for all } t’\\<br>\bar{\alpha} &amp;= \operatorname{softmax} (\alpha)\\<br>c_t &amp;= \sum_{t’=0}^n \bar{\alpha}_{t’} e_{t’}<br>\end{align</em>} %]]&gt;<br>$$<br><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg" alt="Attention Mechanism"></p><p><em>Attention Mechanism</em></p><p>The choice of the function ff varies, but is usually one of the following<br>$$<br>% &lt;![CDATA[<br>f(h_{t-1}, e_{t’}) =<br>\begin{cases}<br>h_{t-1}^T e_{t’} &amp; \text{dot}\\<br>h_{t-1}^T W e_{t’} &amp; \text{general}\\<br>v^T \tanh \left(W [h_{t-1}, e_{t’}]\right) &amp; \text{concat}\\<br>\end{cases} %]]&gt;<br>$$<br>It turns out that the attention weighs $\bar{\alpha}$ can be easily interpreted. When generating the word <code>vas</code>(corresponding to <code>are</code> in English), we expect $\bar{\alpha} _ {\text{are}}$ are to be close to $1$ while $\bar{\alpha} _ {\text{how}}$ and $\bar{\alpha} _ {\text{you}}$ to be close to $0$. Intuitively, the context vector $c$ will be roughly equal to the hidden vector of <code>are</code> and it will help to generate the French word <code>vas</code>.</p><p>By putting the attention weights into a matrix (rows = input sequence, columns = output sequence), we would have access to the <strong>alignment</strong> between the words from the English and French sentences… (see <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="external">page 6</a>) There is still a lot of things to say about sequence to sequence models (for instance, it works better if the encoder processes the input sequence <em>backwards</em>…).</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><blockquote><p>What happens if the first time step is not sure about wether it should generate <code>comment</code> or <code>vas</code> (most likely case at the beginning of the training)? Then it would mess up the entire sequence, and the model will hardly learn anything…</p></blockquote><p>If we use the predicted token as input to the next step during training (as explained above), errors would accumulate and the model would rarely be exposed to the correct distribution of inputs, making training slow or impossible. To speedup things, one trick is to feed the actual output sequence (</p><figure class="highlight plain"><figcaption><span>```comment``` ```vas``` ```tu```) into the decoder’s LSTM and predict the next token at every position (```comment``` ```vas``` ```tu``` ```<eos>```).</eos></span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">![Training](https://guillaumegenthial.github.io/assets/img2latex/img2latex_training.svg)</div><div class="line"></div><div class="line">*Training*</div><div class="line"></div><div class="line">The decoder outputs vectors of probability over the vocabulary $p_i \in \mathbb&#123;R&#125;^V$ for each time step. Then, for a given target sequence $y_1, \dots, y_n$, we can compute its probability as the product of the probabilities of each token being produced at each relevant time step:</div><div class="line">$$</div><div class="line">\mathbb&#123;P&#125;\left(y_1, \dots, y_m \right) = \prod_&#123;i=1&#125;^m p_i [y_i]</div><div class="line">$$</div><div class="line">where $p_i [y_i]$ means that we extract the $y_i$-th entry of the probability vector $p_i$ from the $i$-th decoding step. In particular, we can compute the probability of the actual target sequence. A perfect system would give a probabilty of 1 to this target sequence, so we are going to train our network to maximize the probability of the target sequence, which is the same as minimizing</div><div class="line">$$</div><div class="line">% &lt;![CDATA[</div><div class="line">\begin&#123;align*&#125;</div><div class="line">-\log \mathbb&#123;P&#125; \left(y_1, \dots, y_m \right) &amp;= - \log \prod_&#123;i=1&#125;^m p_i [y_i]\\</div><div class="line">&amp;= - \sum_&#123;i=1&#125;^n \log p_i [y_i]\\</div><div class="line">\end&#123;align*&#125; %]]&gt;</div><div class="line">$$</div><div class="line">in our example, this is equal to</div><div class="line">$$</div><div class="line">% &lt;![CDATA[</div><div class="line">- \log p_1[\text&#123;comment&#125;] - \log p_2[\text&#123;vas&#125;] - \log p_3[\text&#123;tu&#125;] - \log p_4[\text&#123;&lt;eos&gt;&#125;] %]]&gt;</div><div class="line">$$</div><div class="line">and you recognize the standard cross entropy: we actually are minimizing the cross entropy between the target distribution (all one-hot vectors) and the predicted distribution outputed by our model (our vectors $p_i$).</div><div class="line"></div><div class="line">## Decoding</div><div class="line"></div><div class="line">The main takeaway from the discussion above is that for the same model, we can define different behaviors. In particular, we defined a specific behavior that speeds up training.</div><div class="line"></div><div class="line">&gt; What about inference/testing time then? Is there an other way to decode a sentence?</div><div class="line"></div><div class="line">There indeed are 2 main ways of performing decoding at testing time (translating a sentence for which we don’t have a translation). The first of these methods is the one covered at the beginning of the article: **greedy decoding**. It is the most natural way and it consists in feeding to the next step the most likely word predicted at the previous step.</div><div class="line"></div><div class="line">![Greedy Decoder - feeds the best token to the next step](https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg)</div><div class="line"></div><div class="line">*Greedy Decoder - feeds the best token to the next step*</div><div class="line"></div><div class="line">&gt; But didn’t we say that this behavior is likely to accumulate errors?</div><div class="line"></div><div class="line">Even after having trained the model, it can happen that the model makes a small error (and gives a small advantage to `vas` over `comment` for the first step of the decoding). This would mess up the entire decoding…</div><div class="line"></div><div class="line">There is a better way of performing decoding, called **Beam Search**. Instead of only predicting the token with the best score, we keep track of $k$ hypotheses (for example $k=5$, we refer to kk as the **beam size**). At each new time step, for these 5 hypotheses we have $V$ new possible tokens. It makes a total of $5V$ new hypotheses. Then, only keep the $5$ best ones, and so on… Formally, define $\mathcal&#123;H&#125;_t$ the set of hypotheses decoded at time step $t$.</div><div class="line">$$</div><div class="line">\mathcal&#123;H&#125;_ t := \&#123; (w^1_1, \dots, w^1_t), \dots, (w^k_1, \dots, w^k_t) \&#125;</div><div class="line">$$</div><div class="line">For instance if $k=2$, one possible $\mathcal&#123;H&#125;_2$ would be</div><div class="line">$$</div><div class="line">\mathcal&#123;H&#125;_ 2 := \&#123; (\text&#123;comment vas&#125;), (\text&#123;comment tu&#125;) \&#125;</div><div class="line">$$</div><div class="line">Now we consider all the possible candidates $\mathcal&#123;C&#125;_&#123;t+1&#125;$, produced from $\mathcal&#123;H&#125;_t$ by adding all possible new tokens</div><div class="line">$$</div><div class="line">\mathcal&#123;C&#125;_ &#123;t+1&#125; := \bigcup_&#123;i=1&#125;^k \&#123; (w^i_1, \dots, w^i_t, 1), \dots, (w^i_1, \dots, w^i_t, V) \&#125;</div><div class="line">$$</div><div class="line">and keep the $k$ highest scores (probability of the sequence). If we keep our example</div><div class="line">$$</div><div class="line">% &lt;![CDATA[</div><div class="line">\begin&#123;align*&#125;</div><div class="line">\mathcal&#123;C&#125;_ 3 =&amp; \&#123; (\text&#123;comment vas comment&#125;), (\text&#123;comment vas vas&#125;), (\text&#123;comment vas tu&#125;)\&#125;  \\</div><div class="line">\cup &amp; \&#123; (\text&#123;comment tu comment&#125;), \ \ (\text&#123;comment tu vas&#125;), \ \ (\text&#123;comment tu tu&#125;) \&#125;</div><div class="line">\end&#123;align*&#125; %]]&gt;</div><div class="line">$$</div><div class="line">and for instance we can imagine that the 2 best ones would be</div><div class="line">$$</div><div class="line">\mathcal&#123;H&#125;_ 3 := \&#123; (\text&#123;comment vas tu&#125;), (\text&#123;comment tu vas&#125;) \&#125;</div><div class="line">$$</div><div class="line">Once every hypothesis reached the `&lt;eos&gt;` token, we return the hypothesis with the highest score.</div><div class="line"></div><div class="line">&gt; If we use **beam search**, a small error at the first step might be rectified at the next step, as we keep the gold hypthesis in the beam!</div><div class="line"></div><div class="line">## Conclusion</div><div class="line"></div><div class="line">In this article we covered the seq2seq concepts. We showed that training is different than decoding. We covered two methods for decoding: **greedy** and **beam search**. While beam search generally achieves better results, it is not perfect and still suffers from **exposure bias**. During training, the model is never exposed to its errors! It also suffers from **Loss-Evaluation Mismatch**. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</div><div class="line"></div><div class="line">Now, let’s apply Seq2Seq for LaTeX generation from images!</div><div class="line"></div><div class="line">![Producing LaTeX code from an image](https://guillaumegenthial.github.io/assets/img2latex/img2latex_task.svg)</div><div class="line"></div><div class="line">*Producing LaTeX code from an image*</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">## Approach</div><div class="line"></div><div class="line">Previous part covered the concepts of **sequence-to-sequence** applied to Machine Translation. The same framework can be applied to our LaTeX generation problem. The input sequence would just be replaced by an image, preprocessed with some convolutional model adapted to OCR (in a sense, if we *unfold* the pixels of an image into a sequence, this is exactly the same problem). This idea proved to be efficient for image captioning (see the reference paper [Show, Attend and Tell](https://arxiv.org/abs/1502.03044)). Building on some [great work](https://arxiv.org/pdf/1609.04938v1.pdf) from the Harvard NLP group, my teammate [Romain](https://www.linkedin.com/in/romain-sauvestre-241171a2) and I chose to follow a similar approach.</div><div class="line"></div><div class="line">&gt; Keep the seq2seq framework but replace the encoder by a convolutional network over the image!</div><div class="line"></div><div class="line">Good Tensorflow implementations of such models were hard to find. Together with this post, I am releasing the [code](https://github.com/guillaumegenthial/im2latex) and hope some will find it useful. You can use it to train your own image captioning model or adapt it for a more advanced use. [The code](https://github.com/guillaumegenthial/im2latex) does **not** rely on the [Tensorflow Seq2Seq library](https://www.tensorflow.org/versions/master/api_guides/python/contrib.seq2seq) as it was not entirely ready at the time of the project and I also wanted more flexibility (but adopts a similar interface).</div><div class="line"></div><div class="line">## Data</div><div class="line"></div><div class="line">To train our model, we’ll need labeled examples: images of formulas along with the LaTeX code used to generate the images. A good source of LaTeX code is [arXiv](https://arxiv.org/), that has thousands of articles under the `.tex` format. After applying some heuristics to find equations in the `.tex` files, keeping only the ones that actually compile, the [Harvard NLP group](https://zenodo.org/record/56198#.WflVu0yZPLZ) extracted $\sim 100,000$ formulas.</div><div class="line"></div><div class="line">&gt; Wait… Don’t you have a problem as different LaTeX codes can give the same image?</div><div class="line"></div><div class="line">Good point: `(x^2 + 1)` and `\left( x^&#123;2&#125; + 1 \right)` indeed give the same output. That’s why Harvard’s paper found out that normalizing the data using a parser ([KaTeX](https://khan.github.io/KaTeX/)) improved performance. It forces adoption of some conventions, like writing `x ^ &#123; 2 &#125;` instead of `x^2`, etc. After normalization, they end up with a `.txt` file containing one formula per line that looks like</div><div class="line"></div><div class="line">```shell</div><div class="line">\alpha + \beta</div><div class="line">\frac &#123; 1 &#125; &#123; 2 &#125;</div><div class="line">\frac &#123; \alpha &#125; &#123; \beta &#125;</div><div class="line">1 + 2</div></pre></td></tr></table></figure><p></p><p>From this file, we’ll produce images <code>0.png</code>, <code>1.png</code>, etc. and a matching file mapping the image files to the indices (=line numbers) of the formulas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">0.png 0</div><div class="line">1.png 1</div><div class="line">2.png 2</div><div class="line">3.png 3</div></pre></td></tr></table></figure><p>The reason why we use this format is that it is flexible and allows you to use the pre-built <a href="https://zenodo.org/record/56198#.WflVu0yZPLZ" target="_blank" rel="external">dataset from Harvard</a> (You may need to use the preprocessing scripts as explained <a href="https://github.com/harvardnlp/im2markup" target="_blank" rel="external">here</a>). You’ll also need to have <code>pdflatex</code> and <code>ImageMagick</code> installed.</p><p>We also build a vocabulary, to map LaTeX tokens to indices that will be given as input to our model. If we keep the same data as above, our vocabulary will look like</p><p><code>+</code> <code>1</code> <code>2</code> <code>\alpha</code> <code>\beta</code> <code>\frac</code> <code>{</code> <code>}</code></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>Our model is going to rely on a variation of the Seq2Seq model, adapted to images. First, let’s define the input of our graph. Not surprisingly we get as input a batch of black-and-white images of shape $[H,W]$ and a batch of formulas (ids of the LaTeX tokens):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># batch of images, shape = (batch size, height, width, 1)</span></div><div class="line">img = tf.placeholder(tf.uint8, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'img'</span>)</div><div class="line"><span class="comment"># batch of formulas, shape = (batch size, length of the formula)</span></div><div class="line">formula = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>), name=<span class="string">'formula'</span>)</div><div class="line"><span class="comment"># for padding</span></div><div class="line">formula_length = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, ), name=<span class="string">'formula_length'</span>)</div></pre></td></tr></table></figure><blockquote><p>A special note on the type of the image input. You may have noticed that we use <code>tf.uint8</code>. This is because our image is encoded in grey-levels (integers from <code>0</code> to <code>255</code> - and $2^8=256$). Even if we could give a <code>tf.float32</code> Tensor as input to Tensorflow, this would be 4 times more expensive in terms of memory bandwith. As data starvation is one of the main bottlenecks of GPUs, this simple trick can save us some training time. For further improvement of the data pipeline, have a look at <a href="https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/data" target="_blank" rel="external">the new Tensorflow data pipeline</a>.</p></blockquote><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p><strong>High-level idea</strong> Apply some convolutional network on top of the image an flatten the output into a sequence of vectors $[e_1, \cdots, e_n]$, each of those corresponding to a region of the input image. These vectors will correspond to the hidden vectors of the LSTM that we used for translation.</p><blockquote><p>Once our image is transformed into a sequence, we can use the seq2seq model!</p></blockquote><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_encoder.svg" alt="Convolutional Encoder - produces a sequence of vectors"></p><p><em>Convolutional Encoder - produces a sequence of vectors</em></p><p>We need to extract features from our image, and for this, nothing has (<a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="external">yet</a>) been proven more effective than convolutions. Here, there is nothing much to say except that we pick some architecture that has been proven to be effective for Optical Character Recognition (OCR), which stacks convolutional layers and max-pooling to produce a Tensor of shape $[H^{\prime},W^{\prime},512]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># casting the image back to float32 on the GPU</span></div><div class="line">img = tf.cast(img, tf.float32) / <span class="number">255.</span></div><div class="line"></div><div class="line">out = tf.layers.conv2d(img, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line"><span class="comment"># encoder representation, shape = (batch size, height', width', 512)</span></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"VALID"</span>, activation=tf.nn.relu)</div></pre></td></tr></table></figure><p>Now that we have extracted some features from the image, let’s <strong>unfold</strong> the image to get a sequence so that we can use our sequence to sequence framework. We end up with a sequence of length $[H^{\prime},W^{\prime}]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">H, W = tf.shape(out)[<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">seq = tf.reshape(out, shape=[<span class="number">-1</span>, H*W, <span class="number">512</span>])</div></pre></td></tr></table></figure><blockquote><p>Don’t you loose a lot of structural information by reshaping? I’m afraid that when performing attention over the image, my decoder won’t be able to understand the location of each feature vector in the original image!</p></blockquote><p>It turns out that the model manages to work despite this issue, but that’s not completely satisfying. In the case of translation, the hidden states of the LSTM contained some positional information that was computed by the LSTM (after all, LSTM are by essence sequential). Can we fix this issue?</p><p><strong>Positional Embeddings</strong> I decided to follow the idea from <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external">Attention is All you Need</a> that adds <em>positional embeddings</em> to the image representation (<code>out</code>), and has the huge advantage of not adding any new trainable parameter to our model. The idea is that for each position of the image, we compute a vector of size $512$ such that its components are coscos or sinsin. More formally, the (2i)-th and (2i+1)-th entries of my positional embedding $v$ at position $p$ will be<br>$$<br>% &lt;![CDATA[<br>\begin{align<em>}<br>v_{2i} &amp;= \sin\left(p / f^{2i}\right)\\<br>v_{2i+1} &amp;= \cos\left(p / f^{2i}\right)\\<br>\end{align</em>} %]]&gt;<br>$$<br>where $f$ is some frequency parameter. Intuitively, because $\sin(a+b)$ and $\cos⁡(a+b)$ can be expressed in terms of $\sin⁡(b)$ , $\sin(a)$ , $\cos⁡(b)$ and $\cos⁡(a)$, there will be linear dependencies between the components of distant embeddings, authorizing the model to extract relative positioning information. Good news: the tensorflow code for this technique is available in the library <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="external">tensor2tensor</a>, so we just need to reuse the same function and transform our <code>out</code> with the following call</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">out = add_timing_signal_nd(out)</div></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Now that we have a sequence of vectors $[e_1, \dots, e_n]$ that represents our input image, let’s decode it! First, let’s explain what variant of the Seq2Seq framework we are going to use.</p><p><strong>First hidden vector of the decoder’s LSTM</strong> In the seq2seq framework, this is usually just the last hidden vector of the encoder’s LSTM. Here, we don’t have such a vector, so a good choice would be to learn to compute it with a matrix $W$ and a vector $b$<br>$$<br>h_0 = \tanh\left( W \cdot \left( \frac{1}{n} \sum_{i=1}^n e_i\right) + b \right)<br>$$<br>This can be done in Tensorflow with the following logic</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">img_mean = tf.reduce_mean(seq, axis=<span class="number">1</span>)</div><div class="line">W = tf.get_variable(<span class="string">"W"</span>, shape=[<span class="number">512</span>, <span class="number">512</span>])</div><div class="line">b = tf.get_variable(<span class="string">"b"</span>, shape=[<span class="number">512</span>])</div><div class="line">h = tf.tanh(tf.matmul(img_mean, W) + b)</div></pre></td></tr></table></figure><p><strong>Attention Mechanism</strong> We first need to compute a score $\alpha_{t^{\prime}}$ for each vector $e_{t^{\prime}}$ of the sequence. We use the following method<br>$$<br>% &lt;![CDATA[<br>\begin{align<em>}<br>\alpha_{t’} &amp;= \beta^T \tanh\left( W_1 \cdot e_{t’} + W_2 \cdot h_{t} \right)\\<br>\bar{\alpha} &amp;= \operatorname{softmax}\left(\alpha\right)\\<br>c_t &amp;= \sum_{i=1}^n \bar{\alpha}_{t’} e_{t’}\\<br>\end{align</em>} %]]&gt;<br>$$<br>This can be done in Tensorflow with the follwing code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># over the image, shape = (batch size, n, 512)</span></div><div class="line">W1_e = tf.layers.dense(inputs=seq, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># over the hidden vector, shape = (batch size, 512)</span></div><div class="line">W2_h = tf.layers.dense(inputs=h, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># sums the two contributions</span></div><div class="line">a = tf.tanh(W1_e + tf.expand_dims(W2_h, axis=<span class="number">1</span>))</div><div class="line">beta = tf.get_variable(<span class="string">"beta"</span>, shape=[<span class="number">512</span>, <span class="number">1</span>], dtype=tf.float32)</div><div class="line">a_flat = tf.reshape(a, shape=[<span class="number">-1</span>, <span class="number">512</span>])</div><div class="line">a_flat = tf.matmul(a_flat, beta)</div><div class="line">a = tf.reshape(a, shape=[<span class="number">-1</span>, n])</div><div class="line"></div><div class="line"><span class="comment"># compute weights</span></div><div class="line">a = tf.nn.softmax(a)</div><div class="line">a = tf.expand_dims(a, axis=<span class="number">-1</span>)</div><div class="line">c = tf.reduce_sum(a * seq, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Note that the line <code>W1_e = tf.layers.dense(inputs=seq, units=512, use_bias=False)</code> is common to every decoder time step, so we can just compute it once and for all. The dense layer with no bias is just a matrix multiplication.</p></blockquote><p>Now that we have our attention vector, let’s just add a small modification and compute an other vector $o_{t-1}$ (as in <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="external">Luong, Pham and Manning</a>) that we will use to make our final prediction and that we will feed as input to the LSTM at the next step. Here $w_{t−1}$ denotes the embedding of the token generated at the previous step.</p><blockquote><p>$o_t$ passes some information about the distribution from the previous time step as well as the confidence it had for the predicted token</p></blockquote><p>$$<br>% &lt;![CDATA[<br>\begin{align<em>}<br>h_t &amp;= \operatorname{LSTM}\left( h_{t-1}, [w_{t-1}, o_{t-1}] \right)\\<br>c_t &amp;= \operatorname{Attention}([e_1, \dots, e_n], h_t)\\<br>o_{t} &amp;= \tanh\left(W_3 \cdot [h_{t}, c_t] \right)\\<br>p_t &amp;= \operatorname{softmax}\left(W_4 \cdot o_{t} \right)\\<br>\end{align</em>} %]]&gt;<br>$$</p><p>and now the code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute o</span></div><div class="line">W3_o = tf.layers.dense(inputs=tf.concat([h, c], axis=<span class="number">-1</span>), units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line">o = tf.tanh(W3_o)</div><div class="line"></div><div class="line"><span class="comment"># compute the logits scores (before softmax)</span></div><div class="line">logits = tf.layers.dense(inputs=o, units=vocab_size, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># the softmax will be computed in the loss or somewhere else</span></div></pre></td></tr></table></figure><blockquote><p>If I read carefully, I notice that for the first step of the decoding process, we need to compute an $o_0$ too, right?</p></blockquote><p>This is a good point, and we just use the same technique that we used to generate $h_0$ but with different weights.</p><h2 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h2><blockquote><p>We’ll need to create 2 different outputs in the Tensorflow graph: one for training (that uses the <code>formula</code>and feeds the ground truth at each time step, see <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">part I</a>) and one for test time (that ignores everything about the actual <code>formula</code> and uses the prediction from the previous step).</p></blockquote><h3 id="AttentionCell"><a href="#AttentionCell" class="headerlink" title="AttentionCell"></a>AttentionCell</h3><p>We’ll need to encapsulate the reccurent logic into a custom cell that inherits <code>RNNCell</code>. Our custom cell will be able to call the LSTM cell (initialized in the <code>__init__</code>). It also has a special recurrent state that combines the LSTM state and the vector oo (as we need to pass it through). An elegant way is to define a namedtuple for this recurrent state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">AttentionState = collections.namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.lstm_cell = LSTMCell(<span class="number">512</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Args:</div><div class="line">            inputs: shape = (batch_size, dim_embeddings) embeddings from previous time step</div><div class="line">            cell_state: (AttentionState) state from previous time step</div><div class="line">        """</div><div class="line">        lstm_state, o = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([inputs, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># apply previous logic</span></div><div class="line">        c = ...</div><div class="line">        new_o  = ...</div><div class="line">        logits = ...</div><div class="line"></div><div class="line">        new_state = AttentionState(new_lstm_state, new_o)</div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><p>Then, to compute our output sequence, we just need to call the previous cell on the sequence of LaTeX tokens. We first produce the sequence of token embeddings to which we concatenate the special <code>&lt;sos&gt;</code> token. Then, we call <code>dynamic_rnn</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. get token embeddings</span></div><div class="line">E = tf.get_variable(<span class="string">"E"</span>, shape=[vocab_size, <span class="number">80</span>], dtype=tf.float32)</div><div class="line"><span class="comment"># special &lt;sos&gt; token</span></div><div class="line">start_token = tf.get_variable(<span class="string">"start_token"</span>, dtype=tf.float32, shape=[<span class="number">80</span>])</div><div class="line">tok_embeddings = tf.nn.embedding_lookup(E, formula)</div><div class="line"></div><div class="line"><span class="comment"># 2. add the special &lt;sos&gt; token embedding at the beggining of every formula</span></div><div class="line">start_token_ = tf.reshape(start_token, [<span class="number">1</span>, <span class="number">1</span>, dim])</div><div class="line">start_tokens = tf.tile(start_token_, multiples=[batch_size, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line"><span class="comment"># remove the &lt;eos&gt; that won't be used because we reached the end</span></div><div class="line">tok_embeddings = tf.concat([start_tokens, tok_embeddings[:, :<span class="number">-1</span>, :]], axis=<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 3. decode</span></div><div class="line">attn_cell = AttentionCell()</div><div class="line">seq_logits, _ = tf.nn.dynamic_rnn(attn_cell, tok_embeddings, initial_state=AttentionState(h_0, o_0))</div></pre></td></tr></table></figure><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>Code speaks for itself</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute - log(p_i[y_i]) for each time step, shape = (batch_size, formula length)</span></div><div class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=seq_logits, labels=formula)</div><div class="line"><span class="comment"># masking the losses</span></div><div class="line">mask = tf.sequence_mask(formula_length)</div><div class="line">losses = tf.boolean_mask(losses, mask)</div><div class="line"><span class="comment"># averaging the loss over the batch</span></div><div class="line">loss = tf.reduce_mean(losses)</div><div class="line"><span class="comment"># building the train op</span></div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate)</div><div class="line">train_op = optimizer.minimize(loss)</div></pre></td></tr></table></figure><p>and when iterating over the batches during training, <code>train_op</code> will be given to the <code>tf.Session</code> along with a <code>feed_dict</code> containing the data for the placeholders.</p><h2 id="Decoding-in-Tensorflow"><a href="#Decoding-in-Tensorflow" class="headerlink" title="Decoding in Tensorflow"></a>Decoding in Tensorflow</h2><blockquote><p>Let’s have a look at the Tensorflow implementation of the Greedy Method before dealing with Beam Search</p></blockquote><h3 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h3><p>While greedy decoding is easy to conceptualize, implementing it in Tensorflow is not straightforward, as you need to use the previous prediction and can’t use <code>dynamic_rnn</code> on the <code>formula</code>. There are basically <strong>2 ways of approaching the problem</strong></p><ol><li><p>Modify our <code>AttentionCell</code> and <code>AttentionState</code> so that <code>AttentionState</code> also contains the embedding of the predicted word at the previous time step,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">AttentionState = namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>, <span class="string">"embedding"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        lstm_state, o, embbeding = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([embedding, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># usual logic</span></div><div class="line">        logits = ...</div><div class="line">        <span class="comment"># compute new embeddding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        new_state = AttentionState(new_lstm_state, new_o, new_embedding)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><blockquote><p>This technique has a few downsides. It <strong>doesn’t use inputs</strong> (which used to be the embedding of the gold token from the <code>formula</code> and thus we would have to call <code>dynamic_rnn</code> on a “fake” sequence). Also, how do you know when to stop decoding, once you’ve reached the <code>&lt;eos&gt;</code> token?</p></blockquote></li><li><p>Implement a variant of <code>dynamic_rnn</code> that would not run on a sequence but feed the prediction from the previous time step to the cell, while having a maximum number of decoding steps. This would involve delving deeper into Tensorflow, using <code>tf.while_loop</code>. That’s the method we’re going to use as it fixes all the problems of the first technique. We eventually want something that looks like</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">attn_cell = AttentionCell(...)</div><div class="line"><span class="comment"># wrap the attention cell for decoding</span></div><div class="line">decoder_cell = GreedyDecoderCell(attn_cell)</div><div class="line"><span class="comment"># call a special dynamic_decode primitive</span></div><div class="line">test_outputs, _ = dynamic_decode(decoder_cell, max_length_formula+<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Much better isn’t it? Now let’s see what <code>GreedyDecoderCell</code> and <code>dynamic_decode</code> look like.</p></blockquote></li></ol><h3 id="Greedy-Decoder-Cell"><a href="#Greedy-Decoder-Cell" class="headerlink" title="Greedy Decoder Cell"></a>Greedy Decoder Cell</h3><p>We first wrap the attention cell in a <code>GreedyDecoderCell</code> that takes care of the greedy logic for us, without having to modify the <code>AttentionCell</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderOutput</span><span class="params">(collections.namedtuple<span class="params">(<span class="string">"DecoderOutput"</span>, <span class="params">(<span class="string">"logits"</span>, <span class="string">"ids"</span>)</span>)</span>)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GreedyDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># next step of attention cell</span></div><div class="line">        logits, new_state = self._attention_cell.step(embedding, state)</div><div class="line">        <span class="comment"># get ids of words predicted and get embedding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        <span class="comment"># create new state of decoder</span></div><div class="line">        new_output = DecoderOutput(logits, new_ids)</div><div class="line">        new_finished = tf.logical_or(finished, tf.equal(new_ids,</div><div class="line">                self._end_token))</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (new_output, new_state, new_embedding, new_finished)</div></pre></td></tr></table></figure><h3 id="Dynamic-Decode-primitive"><a href="#Dynamic-Decode-primitive" class="headerlink" title="Dynamic Decode primitive"></a>Dynamic Decode primitive</h3><p>We need to implement a function <code>dynamic_decode</code> that will recursively call the above <code>step</code> function. We do this with a <code>tf.while_loop</code> that stops when all the hypotheses reached <code>&lt;eos&gt;</code> or <code>time</code> is greater than the max number of iterations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamic_decode</span><span class="params">(decoder_cell, maximum_iterations)</span>:</span></div><div class="line">    <span class="comment"># initialize variables (details on github)</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">condition</span><span class="params">(time, unused_outputs_ta, unused_state, unused_inputs, finished)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.logical_not(tf.reduce_all(finished))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(time, outputs_ta, state, inputs, finished)</span>:</span></div><div class="line">        new_output, new_state, new_inputs, new_finished = decoder_cell.step(</div><div class="line">            time, state, inputs, finished)</div><div class="line">        <span class="comment"># store the outputs in TensorArrays (details on github)</span></div><div class="line">        new_finished = tf.logical_or(tf.greater_equal(time, maximum_iterations), new_finished)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (time + <span class="number">1</span>, outputs_ta, new_state, new_inputs, new_finished)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"rnn"</span>):</div><div class="line">        res = tf.while_loop(</div><div class="line">            condition,</div><div class="line">            body,</div><div class="line">            loop_vars=[initial_time, initial_outputs_ta, initial_state, initial_inputs, initial_finished])</div><div class="line"></div><div class="line">    <span class="comment"># return the final outputs (details on github)</span></div></pre></td></tr></table></figure><blockquote><p>Some details using <code>TensorArrays</code> or <code>nest.map_structure</code> have been omitted for clarity but may be found on <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/dynamic_decode.py" target="_blank" rel="external">github</a></p><p>Notice that we place the <code>tf.while_loop</code> inside a scope named <code>rnn</code>. This is because <code>dynamic_rnn</code>does the same thing and thus the weights of our LSTM are defined in that scope.</p></blockquote><h3 id="Beam-Search-Decoder-Cell"><a href="#Beam-Search-Decoder-Cell" class="headerlink" title="Beam Search Decoder Cell"></a>Beam Search Decoder Cell</h3><blockquote><p>We can follow the same approach as in the greedy method and use <code>dynamic_decode</code></p></blockquote><p>Let’s create a new wrapper for <code>AttentionCell</code> in the same way we did for <code>GreedyDecoderCell</code>. This time, the code is going to be more complicated and the following is just for intuition. Note that when selecting the top kk hypotheses from the set of candidates, we must know which “beginning” they used (=parent hypothesis).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BeamSearchDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># notice the same arguments as for GreedyDecoderCell</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># compute new logits</span></div><div class="line">        logits, new_cell_state = self._attention_cell.step(embedding, state.cell_state)</div><div class="line"></div><div class="line">        <span class="comment"># compute log probs of the step (- log p(w) for all words w)</span></div><div class="line">        <span class="comment"># shape = [batch_size, beam_size, vocab_size]</span></div><div class="line">        step_log_probs = tf.nn.log_softmax(new_logits)</div><div class="line"></div><div class="line">        <span class="comment"># compute scores for the (beam_size * vocabulary_size) new hypotheses</span></div><div class="line">        log_probs = state.log_probs + step_log_probs</div><div class="line"></div><div class="line">        <span class="comment"># get top k hypotheses</span></div><div class="line">        new_probs, indices = tf.nn.top_k(log_probs, self._beam_size)</div><div class="line"></div><div class="line">        <span class="comment"># get ids of next token along with the parent hypothesis</span></div><div class="line">        new_ids = ...</div><div class="line">        new_parents = ...</div><div class="line"></div><div class="line">        <span class="comment"># compute new embeddings, new_finished, new_cell state...</span></div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div></pre></td></tr></table></figure><blockquote><p>Look at <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/beam_search_decoder_cell.py" target="_blank" rel="external">github</a> for the details. The main idea is that we add a beam dimension to every tensor, but when feeding it into <code>AttentionCell</code> we merge the beam dimension with the batch dimension. There is also some trickery involved to compute the parents and the new ids using modulos.</p></blockquote><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I hope that you learned something with this post, either about the technique or Tensorflow. While the model achieves impressive performance (at least on short formulas with roughly 85% of the LaTeX being reconstructed), it still raises some questions that I list here:</p><p><em>How do we evaluate the performance of our model?</em>. We can use standard metrics from Machine Translation like <a href="https://en.wikipedia.org/wiki/BLEU" target="_blank" rel="external">BLEU</a> to evaluate how good the decoded LaTeX is compared to the reference. We can also choose to compile the predicted LaTeX sequence to get the image of the formula, and then compare this image to the orignal. As a formula is a sequence, computing the pixel-wise distance wouldn’t really make sense. A good idea is proposed by <a href="http://lstm.seas.harvard.edu/latex" target="_blank" rel="external">Harvard’s paper</a>. First, slice the image vertically. Then, compare the edit distance between these slices…</p><p><em>How to fix exposure bias?</em> While beam search generally achieves better results, it is not perfect and still suffers from <strong>exposure bias</strong>. During training, the model is never exposed to its errors! It also suffers from <strong>Loss-Evaluation Mismatch</strong>. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/ref.png" alt=""></p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/pred.png" alt=""></p><p><em>An Example of LaTeX generation - which one is the reference?</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Source Post is &lt;a href=&quot;https://guillaumegenthial.github.io/sequence-to-sequence.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;h2 id=
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Convolutions</title>
    <link href="http://yoursite.com/2018/01/18/Understanding-Convolutions/"/>
    <id>http://yoursite.com/2018/01/18/Understanding-Convolutions/</id>
    <published>2018-01-18T09:01:29.000Z</published>
    <updated>2018-01-18T10:26:33.245Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lessons-from-a-Dropped-Ball"><a href="#Lessons-from-a-Dropped-Ball" class="headerlink" title="Lessons from a Dropped Ball"></a>Lessons from a Dropped Ball</h2><p>Imagine we drop a ball from some height onto the ground, where it only has one dimension of motion. <em>How likely is it that a ball will go a distance $c$ if you drop it and then drop it again from above the point at which it landed?</em></p><p>Let’s break this down. After the first drop, it will land aa units away from the starting point with probability $f(a)$, where $f$ is the probability distribution.</p><p>Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed. The probability of the ball rolling $b$ units away from the new starting point is $g(b)$, where $g$ may be a different probability distribution if it’s dropped from a different height.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-fagb.png" alt="img"></p><p>If we fix the result of the first drop so we know the ball went distance aa, for the ball to go a total distance cc, the distance traveled in the second drop is also fixed at bb, where $a+b=c$. So the probability of this happening is simply $f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn1" target="_blank" rel="external">1</a></p><p>Let’s think about this with a specific discrete example. We want the total distance $c$ to be 3. If the first time it rolls, $a=2$, the second time it must roll $b=1$ in order to reach our total distance $a+b=3$. The probability of this is $f(2)⋅g(1)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-split-21.png" alt="img"></p><p>However, this isn’t the only way we could get to a total distance of 3. The ball could roll 1 units the first time, and 2 the second. Or 0 units the first time and all 3 the second. It could go any $a$ and $b$, as long as they add to 3.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-splits-12-03.png" alt="img"></p><p>The probabilities are $f(1)⋅g(2)$ and $f(0)⋅g(3)$, respectively.</p><p>In order to find the <em>total likelihood</em> of the ball reaching a total distance of $c$, we can’t consider only one possible way of reaching $c$. Instead, we consider <em>all the possible ways</em> of partitioning $c$ into two drops $a$ and $b$ and sum over the <em>probability of each way</em>.<br>$$<br>…\;\; f(0)\cdot g(3) ~+~ f(1)\cdot g(2) ~+~ f(2)\cdot g(1)\;\;…<br>$$<br>We already know that the probability for each case of $a+b=c$ is simply $f(a)⋅g(b)$. So, summing over every solution to $a+b=c$, we can denote the total likelihood as:<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Turns out, we’re doing a convolution! In particular, the convolution of $f$ and $g$, evluated at $c$ is defined:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)~~~~<br>$$<br>If we substitute $b=c−a$, we get:<br>$$<br>(f\ast g)(c) = \sum_a f(a) \cdot g(c-a)<br>$$<br>This is the standard definition<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn2" target="_blank" rel="external">2</a> of convolution.</p><p>To make this a bit more concrete, we can think about this in terms of positions the ball might land. After the first drop, it will land at an intermediate position aa with probability $f(a)$. If it lands at $a$, it has probability $g(c−a)$ of landing at a position $c$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-OnePath.png" alt="img"></p><p>To get the convolution, we consider all intermediate positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-SumPaths.png" alt="img"></p><h2 id="Visualizing-Convolutions"><a href="#Visualizing-Convolutions" class="headerlink" title="Visualizing Convolutions"></a>Visualizing Convolutions</h2><p>There’s a very nice trick that helps one think about convolutions more easily.</p><p>First, an observation. Suppose the probability that a ball lands a certain distance $x$ from where it started is $f(x)$. Then, afterwards, the probability that it started a distance $x$ from where it landed is $f(−x)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Reverse.png" alt="img"></p><p>If we know the ball lands at a position $c$ after the second drop, what is the probability that the previous position was $a$?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-BackProb.png" alt="img"></p><p>So the probability that the previous position was $a$ is $g(−(a−c))=g(c−a)$.</p><p>Now, consider the probability each intermediate position contributes to the ball finally landing at $c$. We know the probability of the first drop putting the ball into the intermediate position a is $f(a)$. We also know that the probability of it having been in $a$, if it lands at $c$ is $g(c−a)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate.png" alt="img"></p><p>Summing over the $a$s, we get the convolution.</p><p>The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value $c$ in a single picture. By shifting the bottom half around, we can evaluate the convolution at other values of $c$. This allows us to understand the convolution as a whole.</p><p>For example, we can see that it peaks when the distributions align.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Align.png" alt="img"></p><p>And shrinks as the intersection between the distributions gets smaller.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Sep.png" alt="img"></p><p>By using this trick in an animation, it really becomes possible to visually understand convolutions.</p><p>Below, we’re able to visualize the convolution of two box functions:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Wiki-BoxConvAnim.gif" alt="Wiki-BoxConvAnim"></p><p><em>From Wikipedia</em></p><p>Armed with this perspective, a lot of things become more intuitive.</p><p>Let’s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy.</p><h2 id="Higher-Dimensional-Convolutions"><a href="#Higher-Dimensional-Convolutions" class="headerlink" title="Higher Dimensional Convolutions"></a>Higher Dimensional Convolutions</h2><p>Convolutions are an extremely general idea. We can also use them in a higher number of dimensions.</p><p>Let’s consider our example of a falling ball again. Now, as it falls, it’s position shifts not only in one dimension, but in two.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-TwoDim.png" alt="img"></p><p>Convolution is the same as before:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Except, now $a$, $b$ and $c$ are vectors. To be more explicit,<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{\begin{array}{c}a_1+b_1=c_1\\a_2+b_2=c_2\end{array}} f(a_1,a_2) \cdot g(b_1,b_2)<br>$$<br>Or in the standard definition:<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{a_1, a_2} f(a_1, a_2) \cdot g(c_1-a_1,~ c_2-a_2)<br>$$<br>Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding.</p><p>One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a “kernel.”</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/RiverTrain-ImageConvDiagram.png" alt="RiverTrain-ImageConvDiagram"></p><p><em>From the <a href="http://intellabs.github.io/RiverTrail/tutorial/" target="_blank" rel="external">River Trail documentation</a></em></p><p>The kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over.</p><p>For example, by averaging a 3x3 box of pixels, we can blur an image. To do this, our kernel takes the value $\dfrac{1}{9}$ on each pixel in the box,</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Blur.png" alt="Gimp-Blur"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>We can also detect edges by taking the values −1−1 and 11 on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Edge.png" alt="Gimp-Edge"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>The gimp documentation has <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">many other examples</a>.</p><h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><p>So, how does convolution relate to convolutional neural networks?</p><p>Consider a 1-dimensional convolutional layer with inputs $\{x_n\}$ and outputs $\{y_n\}$, like we discussed in the <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" target="_blank" rel="external">previous post</a>:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY.png" alt="img"></p><p>As we observed, we can describe the outputs in terms of the inputs:<br>$$<br>y_n = A(x_{n}, x_{n+1}, …)<br>$$<br>Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.</p><p>Recall that a typical neuron in a neural network is described by:<br>$$<br>\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~…~ + b)<br>$$<br>Where $x_0$, $x_1$… are the inputs. The weights, $w_0$, $w_1$, … describe how the neuron connects to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn3" target="_blank" rel="external">3</a> Saying that multiple neurons are identical is the same thing as saying that the weights are the same.</p><p>It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.</p><p>Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:<br>$$<br>y = \sigma(Wx + b)<br>$$<br>For example, we get:<br>$$<br>y_0 = \sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 …)<br>$$</p><p>$$<br>y_1 = \sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 …)<br>$$</p><p>Each row of the matrix describes the weights connecting a neuron to its inputs.</p><p>Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY-W.png" alt="img"></p><p>Which corresponds to the equations:<br>$$<br>y_0 = \sigma(W_0x_0 + W_1x_1 -b)<br>$$</p><p>$$<br>y_1 = \sigma(W_0x_1 + W_1x_2 -b)<br>$$</p><p>So while, normally, a weight matrix connects every input to every neuron with different weights:<br>$$<br>W = \left[\begin{array}{ccccc}<br>W_{0,0} &amp; W_{0,1} &amp; W_{0,2} &amp; W_{0,3} &amp; …\\<br>W_{1,0} &amp; W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; …\\<br>W_{2,0} &amp; W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; …\\<br>W_{3,0} &amp; W_{3,1} &amp; W_{3,2} &amp; W_{3,3} &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don’t connect to many possible inputs, there’s lots of zeros.<br>$$<br>W = \left[\begin{array}{ccccc}<br>w_0 &amp; w_1 &amp; 0 &amp; 0 &amp; …\\<br>0 &amp; w_0 &amp; w_1 &amp; 0 &amp; …\\<br>0 &amp; 0 &amp; w_0 &amp; w_1 &amp; …\\<br>0 &amp; 0 &amp; 0 &amp; w_0 &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>Multiplying by the above matrix is the same thing as convolving with $[…0, w_1, w_0, 0…]$. The function sliding to different positions corresponds to having neurons at those positions.</p><p>What about two-dimensional convolutional layers?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv2-5x5-Conv2-XY.png" alt="img"></p><p>The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.</p><p>Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image.</p><ol><li><p>We want the probability of the ball rolling aa units the first time and also rolling bb units the second time. The distributions $P(A)=f(a)$ and $P(b)=g(b)$ are independent, with both distributions centered at 0. So $P(a,b)=P(a)∗P(b)=f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref1" target="_blank" rel="external">↩</a></p></li><li><p>The non-standard definition, which I haven’t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious.</p><p>For example, convolution is a commutative operation. That is, $f∗g=g∗f$. Why?</p><p>​<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b) ~~=~ \sum_{b+a=c} g(b) \cdot f(a)<br>$$<br>Convolution is also associative. That is, $(f∗g)∗h=f∗(g∗h)$. Why?</p><p>​<br>$$<br>\sum_{(a+b)+c=d} (f(a) \cdot g(b)) \cdot h(c) ~~=~ \sum_{a+(b+c)=d} f(a) \cdot (g(b) \cdot h(c))<br>$$<br>↩</p><p>​</p></li><li><p>There’s also the bias, which is the “threshold” for whether the neuron fires, but it’s much simpler and I don’t want to clutter this section talking about it.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref3" target="_blank" rel="external">↩</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Lessons-from-a-Dropped-Ball&quot;&gt;&lt;a href=&quot;#Lessons-from-a-Dropped-Ball&quot; class=&quot;headerlink&quot; title=&quot;Lessons from a Dropped Ball&quot;&gt;&lt;/a&gt;Lesso
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="CNN" scheme="http://yoursite.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>PCA With Tensorflow</title>
    <link href="http://yoursite.com/2018/01/17/PCA-With-Tensorflow/"/>
    <id>http://yoursite.com/2018/01/17/PCA-With-Tensorflow/</id>
    <published>2018-01-17T09:04:36.000Z</published>
    <updated>2018-01-17T09:07:11.626Z</updated>
    
    <content type="html"><![CDATA[<p>PCA (<strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis) is probably the oldest trick in the book.</p><p>PCA is well studied and there are numerous ways to get to the same solution, we will talk about two of them here, Eigen decomposition and Singular Value Decomposition (SVD) and then we will implement the SVD way in TensorFlow.</p><p>From now on, X will be our data matrix, of shape (n, p) where n is the number of examples, and p are the dimensions.</p><p>So given X, both methods will try to find, in their own way, a way to manipulate and decompose X in a manner that later on we could multiply the decomposed results to represent maximum information in less dimensions. I know I know, sounds horrible but I will spare you most of the math but keep the parts that contribute to the understanding of the method pros and cons.</p><p>So Eigen decomposition and SVD are both ways to decompose matrices, lets see how they help us in PCA and how they are connected.</p><p>Take a glance at the flow chart below and I will explain right after.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*xnomew0zpnxftxutG8xoFw.png" alt="img"></p><p>Figure 1 PCA workflow</p><p>So why should you care about this? Well there is something very fundamental about the two procedures that tells us a lot about PCA.</p><p>As you can see both methods are pure linear algebra, that basically tells us that using PCA is looking at the real data, from a different angle — this is unique to PCA since the other methods start with random representation of lower dimensional data and try to get it to behave like the high dimensional data.</p><p>Some other notable things are that all operations are linear and with SVD are super-super fast.</p><p>Also given the same data PCA will always give the same answer (which is not true about the other two methods).</p><p>Notice how in SVD we choose the r (r is the number of dimensions we want to reduce to) left most values of Σ to lower dimensionality?</p><p>Well there is something special about Σ .</p><p>Σ is a diagonal matrix, there are p (number of dimensions) diagonal values (called singular values) and their magnitude indicates how significant they are to preserving the information.</p><p>So we can choose to reduce dimensionality, to the number of dimensions that will preserve approx. given amount of percentage of the data and I will demonstrate that in the code (e.g. gives us the ability to reduce dimensionality with a constraint of losing a max of 15% of the data).</p><p>As you will see, coding this in TensorFlow is pretty simple — what we are are going to code is a class that has <code>fit</code> method and a <code>reduce</code> method which we will supply the dimensions to.</p><h3 id="CODE-PCA"><a href="#CODE-PCA" class="headerlink" title="CODE (PCA)"></a><strong>CODE (PCA)</strong></h3><p>Lets see how the <code>fit</code> method looks like, given <code>self.X</code> contains the data and <code>self.dtype=tf.float32</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></div><div class="line">    self.graph = tf.Graph()</div><div class="line">    <span class="keyword">with</span> self.graph.as_default():</div><div class="line">        self.X = tf.placeholder(self.dtype, shape=self.data.shape)</div><div class="line"></div><div class="line">        <span class="comment"># Perform SVD</span></div><div class="line">        singular_values, u, _ = tf.svd(self.X)</div><div class="line"></div><div class="line">        <span class="comment"># Create sigma matrix</span></div><div class="line">        sigma = tf.diag(singular_values)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session(graph=self.graph) <span class="keyword">as</span> session:</div><div class="line">        self.u, self.singular_values, self.sigma = session.run([u, singular_values, sigma],</div><div class="line">                                                               feed_dict=&#123;self.X: self.data&#125;)</div></pre></td></tr></table></figure><p>So the goal of <code>fit</code> is to create our Σ and U for later use.</p><p>We’ll start with the line <code>tf.svd</code> which gives us the singular values, which are the diagonal values of what was denoted as Σ in Figure 1, and the matrices U and V.</p><p>Then <code>tf.diag</code> is TensorFlow’s way of converting a 1D vector, to a diagonal matrix, which in our case will result in Σ.</p><p>At the end of the <code>fit</code> call we will have the singular values, Σ and U.</p><p>Now lets lets implement <code>reduce</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span><span class="params">(self, n_dimensions=None, keep_info=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> keep_info:</div><div class="line">        <span class="comment"># Normalize singular values</span></div><div class="line">        normalized_singular_values = self.singular_values / sum(self.singular_values)</div><div class="line"></div><div class="line">        <span class="comment"># Create the aggregated ladder of kept information per dimension</span></div><div class="line">        ladder = np.cumsum(normalized_singular_values)</div><div class="line"></div><div class="line">        <span class="comment"># Get the first index which is above the given information threshold</span></div><div class="line">        index = next(idx <span class="keyword">for</span> idx, value <span class="keyword">in</span> enumerate(ladder) <span class="keyword">if</span> value &gt;= keep_info) + <span class="number">1</span></div><div class="line">        n_dimensions = index</div><div class="line"></div><div class="line">    <span class="keyword">with</span> self.graph.as_default():</div><div class="line">        <span class="comment"># Cut out the relevant part from sigma</span></div><div class="line">        sigma = tf.slice(self.sigma, [<span class="number">0</span>, <span class="number">0</span>], [self.data.shape[<span class="number">1</span>], n_dimensions])</div><div class="line"></div><div class="line">        <span class="comment"># PCA</span></div><div class="line">        pca = tf.matmul(self.u, sigma)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session(graph=self.graph) <span class="keyword">as</span> session:</div><div class="line">        <span class="keyword">return</span> session.run(pca, feed_dict=&#123;self.X: self.data&#125;)</div></pre></td></tr></table></figure><p>So as you can see <code>reduce</code> gets either <code>keep_info</code> or <code>n_dimensions</code> (I didn’t implement the input check where <strong>only one must be supplied</strong>).</p><p>If we supply <code>n_dimensions</code> it will simply reduce to that number, but if we supply <code>keep_info</code> which should be a float between 0 and 1, we will preserve that much information from the original data (0.9 — preserve 90% of the data).</p><p>In the first ‘if’, we normalize and check how many singular values are needed, basically figuring out <code>n_dimensions</code> out of <code>keep_info</code>.</p><p>In the graph, we just slice the Σ (sigma) matrix for as much data as we need and perform the matrix multiplication.</p><p>So lets try it out on the iris dataset, which is (150, 4) dataset of 3 species of iris flowers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"></div><div class="line">tf_pca = TF_PCA(iris_dataset.data, iris_dataset.target)</div><div class="line">tf_pca.fit()</div><div class="line">pca = tf_pca.reduce(keep_info=<span class="number">0.9</span>)  <span class="comment"># Results in 2 dimensions</span></div><div class="line"></div><div class="line">color_mapping = &#123;<span class="number">0</span>: sns.xkcd_rgb[<span class="string">'bright purple'</span>], <span class="number">1</span>: sns.xkcd_rgb[<span class="string">'lime'</span>], <span class="number">2</span>: sns.xkcd_rgb[<span class="string">'ochre'</span>]&#125;</div><div class="line">colors = list(map(<span class="keyword">lambda</span> x: color_mapping[x], tf_pca.target))</div><div class="line"></div><div class="line">plt.scatter(pca[:, <span class="number">0</span>], pca[:, <span class="number">1</span>], c=colors)</div></pre></td></tr></table></figure><p><img src="https://cdn-images-1.medium.com/max/2000/1*-am5UfbZoJkUA4C8z5d0vQ.png" alt="img"></p><p>Figure 2 Iris dataset PCA 2 dimensional plot</p><p>Not so bad huh?</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCA (&lt;strong&gt;P&lt;/strong&gt;rincipal &lt;strong&gt;C&lt;/strong&gt;omponent &lt;strong&gt;A&lt;/strong&gt;nalysis) is probably the oldest trick in the book.&lt;/p&gt;&lt;p&gt;PCA
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Word Embedding - Approximating the Softmax [Repost]</title>
    <link href="http://yoursite.com/2018/01/16/Word-Embedding-Approximating-the-Softmax-Repost/"/>
    <id>http://yoursite.com/2018/01/16/Word-Embedding-Approximating-the-Softmax-Repost/</id>
    <published>2018-01-16T11:09:29.000Z</published>
    <updated>2018-01-16T12:28:19.322Z</updated>
    
    <content type="html"><![CDATA[<p>This is the second post in a series on word embeddings and representation learning. In the <a href="http://sebastianruder.com/word-embeddings-1/index.html" target="_blank" rel="external">previous post</a>, we gave an overview of word embedding models and introduced the classic neural language model by Bengio et al. (2003), the C&amp;W model by Collobert and Weston (2008), and the word2vec model by Mikolov et al. (2013). We observed that mitigating the complexity of computing the final softmax layer has been one of the main challenges in devising better word embedding models, a commonality with machine translation (MT) (Jean et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:10" target="_blank" rel="external">10</a>]) and language modelling (Jozefowicz et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:6" target="_blank" rel="external">6</a>]).</p><p>In this post, we will thus focus on giving an overview of various approximations to the softmax layer that have been proposed over the last years, some of which have so far only been employed in the context of language modelling or MT. We will postpone the discussion of additional hyperparameters to the subsequent post.</p><p>Let us know partially re-introduce the previous post’s notation both for consistency and to facilitate comparison as well as introduce some new notation: We assume a training corpus containing a sequence of $T$ training words $w_1,w_2,w_3, \cdots ,w_T$ that belong to a vocabulary $V$ whose size is $|V|$. Our models generally consider a context $c$ of $n$ words. We associate every word with an input embedding $v_w$ (the eponymous word embedding in the Embedding Layer) with $d$ dimensions and an output embedding $v_{w^{\prime}}$ (the representation of the word in the weight matrix of the softmax layer). We finally optimize an objective function $J_{\theta}$ with regard to our model parameters $\theta$.</p><p>Recall that the softmax calculates the probability of a word $w$ given its context $c$ and can be computed using the following equation:<br>$$<br>p(w|c) = \frac{\exp(h^{\text{T}} v_{w^{\prime}})}{\sum_{w_i \in V} \exp(h^{\text{T}}v_{w_i}^{\prime})}<br>$$<br>where $h$ is the output vector of the penultimate network layer. Note that we use $c$ for the context as mentioned above and drop the index $t$ of the target word $w_t$ for simplicity. Computing the softmax is expensive as the inner product between $h$ and the output embedding of every word $w_i$ in the vocabulary $V$ needs to be computed as part of the sum in the denominator in order to obtain the normalized probability of the target word $w$ given its context $c$.</p><p>In the following we will discuss different strategies that have been proposed to approximate the softmax. These approaches can be grouped into softmax-based and sampling-based approaches. Softmax-based approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency. Sampling-based approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax.</p><h1 id="Softmax-based-Approaches"><a href="#Softmax-based-Approaches" class="headerlink" title="Softmax-based Approaches"></a>Softmax-based Approaches</h1><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p>Hierarchical softmax (H-Softmax) is an approximation inspired by binary trees that was proposed by Morin and Bengio [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:3" target="_blank" rel="external">3</a>]. H-Softmax essentially replaces the flat softmax layer with a hierarchical layer that has the words as leaves, as can be seen in Figure 1.</p><p>This allows us to decompose calculating the probability of one word into a sequence of probability calculations, which saves us from having to calculate the expensive normalization over all words. Replacing a softmax layer with H-Softmax can yield speedups for word prediction tasks of at least $50 \times $ and is thus critical for low-latency tasks such as real-time communication in <a href="http://googleresearch.blogspot.ie/2016/05/chat-smarter-with-allo.html" target="_blank" rel="external">Google’s new messenger app Allo</a>.</p><p><img src="http://ruder.io/content/images/2016/06/hierarchical_softmax_example.png" alt="Hierarchical softmax"></p><p>Figure 1: Hierarchical softmax (<a href="https://www.quora.com/Word2vec-How-can-hierarchical-soft-max-training-method-of-CBOW-guarantee-its-self-consistence" target="_blank" rel="external">Quora</a>)</p><p>We can think of the regular softmax as a tree of depth 11, with each word in $V$ as a leaf node. Computing the softmax probability of one word then requires normalizing over the probabilities of all $|V|$ leaves. If we instead structure the softmax as a binary tree, with the words as leaf nodes, then we only need to follow the path to the leaf node of that word, without having to consider any of the other nodes.</p><p>Since a balanced binary tree has a depth of $\log_2(|V|)$ we only need to evaluate at most $\log_2(|V|)$ nodes to obtain the final probability of a word. Note that this probability is already normalized, as the probabilities of all leaves in a binary tree sum to 11 and thus form a probability distribution. To informally verify this, we can reason that at a tree’s root node (Node 0) in Figure 1), the probabilities of branching decisions must sum to 11. At each subsequent node, the probability mass is then split among its children, until it eventually ends up at the leaf nodes, i.e. the words. Since no probability is lost along the way and since all words are leaves, the probabilities of all words must necessarily sum to 11 and hence the hierarchical softmax defines a normalized probability distribution over all words in $V$.</p><p>To get a bit more concrete, as we go through the tree, we have to be able to calculate the probability of taking the left or right branch at every junction. For this reason, we assign a representation to every node. In contrast to the regular softmax, we thus no longer have output embeddings $v^{\prime}_w$ for every word $w$ – instead, we have embeddings $v^{\prime}_n$ for every node $n$. As we have $|V|−1$ nodes and each one possesses a unique representation, the number of parameters of H-Softmax is almost the same as for the regular softmax. We can now calculate the probability of going right (or left) at a given node $n$ given the context $c$ the following way:<br>$$<br>p(\text{right}|n,c) = \sigma(h^{\text{T}}v^{\prime}_n).<br>$$<br>This is almost the same as the computations in the regular softmax; now instead of computing the dot product between $h$ and the output word embedding $v^{\prime}_w$, we compute the dot product between $h$ and the embedding $v^{\prime}_w$ of each node in the tree; additionally, instead of computing a probability distribution over the entire vocabulary words, we output just one probability, the probability of going right at node $n$ in this case, with the sigmoid function. Conversely, the probability of turning left is simply $1−p(\text{right} | n,c)$.</p><p><img src="http://ruder.io/content/images/2016/05/hierarchical_softmax.png" alt="Hierarchical softmax"></p><p>Figure 2: Hierarchical softmax computations (<a href="https://www.youtube.com/watch?v=B95LTf2rVWM" target="_blank" rel="external">Hugo Lachorelle’s Youtube lectures</a>)</p><p>The probability of a word ww given its context cc is then simply the product of the probabilities of taking right and left turns respectively that lead to its leaf node. To illustrate this, given the context “the”, “dog”, “and”, “the”, the probability of the word “cat” in Figure 2 can be computed as the product of the probability of turning left at node 1, turning right at node 2, and turning right at node 5. Hugo Lachorelle gives a more detailed account in his excellent <a href="https://www.youtube.com/watch?v=B95LTf2rVWM" target="_blank" rel="external">lecture video</a>. Rong [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:7" target="_blank" rel="external">7</a>] also does a good job of explaining these concepts and also derives the derivatives of H-Softmax.</p><p>Obviously, the structure of the tree is of significance. Intuitively, we should be able to achieve better performance, if we make it easier for the model to learn the binary predictors at every node, e.g. by enabling it to assign similar probabilities to similar paths. Based on this idea, Morin and Bengio use the synsets in WordNet as clusters for the tree. However, they still report inferior performance to the regular softmax. Mnih and Hinton [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:8" target="_blank" rel="external">8</a>] learn the tree structure with a clustering algorithm that recursively partitions the words in two clusters and allows them to achieve the same performance as the regular softmax at a fraction of the computation.</p><p>Notably, we are only able to obtain this speed-up during training, when we know the word we want to predict (and consequently its path) in advance. During testing, when we need to find the most likely prediction, we still need to calculate the probability of all words, although narrowing down the choices in advance helps here.</p><p>In practice, instead of using “right” and “left” in order to designate nodes, we can index every node with a bit vector that corresponds to the path it takes to reach that node. In Figure 2, if we assume a <code>0</code> bit for turning left and a <code>1</code> bit for turning right, we can thus represent the path to “cat” as <code>011</code>.</p><p>Recall that the path length in a balanced binary tree is $\log_2|V|$. If we set $|V|=10000$, this amounts to an average path length of about $13.3$. Analogously, we can represent every word by the bit vector of its path that is on average $13.3$ bits long. In information theory, this is referred to as an information content of $13.3$ bits per word.</p><h3 id="A-note-on-the-information-content-of-words"><a href="#A-note-on-the-information-content-of-words" class="headerlink" title="A note on the information content of words"></a>A note on the information content of words</h3><p>Recall that the information content $I(w)$ of a word $w$ is the negative logarithm of its probability $p(w)$:<br>$$<br>I(w) = − \log_2p(w)<br>$$<br>The entropy $H$ of all words in a corpus is then the expectation of the information content of all words in the vocabulary:<br>$$<br>H= \sum_{i \in V} p(w_i) I(w_i)<br>$$<br>We can also conceive of the entropy of a data source as the average number of bits needed to encode it. For a fair coin flip, we need $1$ bit per flip, whereas we need $0$ bits for a data source that always emits the same symbol. For a balanced binary tree, where we treat every word equally, the word entropy $H$ equals the information content $I(w)$ of every word $w$, as each word has the same probability. The average word entropy $H$ in a balanced binary tree with $|V|=10000$ thus coincides with its average path length:<br>$$<br>H = − \sum_{i \in V}\frac{1}{10000} \log_2 \frac{⁡1}{10000} = 13.3.<br>$$<br>We saw before that the structure of the tree is important. Notably, we can leverage the tree structure not only to gain better performance, but also to speed up computation: If we manage to encode more information into the tree, we can get away with taking shorter paths for less informative words. Morin and Bengio point out that leveraging word probabilities should work even better; as some words are more likely to occur than others, they can be encoded using less information. They note that the word entropy of their corpus (with $|V|=10,000$) is about $9.16$.</p><p>Thus, by taking into account frequencies, we can reduce the average number of bits per word in the corpus from $13.3$ to $9.16$ in this case, which amounts to a speed-up of 31%. A <a href="https://en.wikipedia.org/wiki/Huffman_coding" target="_blank" rel="external">Huffman tree</a>, which is used by Mikolov et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:1" target="_blank" rel="external">1</a>] for their hierarchical softmax, generates such a coding by assigning fewer bits to more common symbols. For instance, “the”, the most common word in the English language, would be assigned the shortest bit code in the tree, the second most frequent word would be assigned the second-shortest bit code, and so on. While we still need the same number of codes to designate all words, when we predict the words in a corpus, short codes appear now a lot more often, and we consequently need fewer bits to represent each word on average.</p><p>A coding such as Huffman coding is also known as entropy encoding, as the length of each codeword is approximately proportional to the entropy of each symbol as we have observed. Shannon [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:5" target="_blank" rel="external">5</a>] establishes in his experiments that the lower bound on the information rate in English is between $0.6$ to $1.3$ bits per character; given an average word length of $4.5$, this amounts to $2.7$ - $5.85$ bits per word.</p><p>To tie this back to language modelling (which we already talked about in the previous post): perplexity, the evaluation measure of language modelling, is $2^H$ where $H$ is the entropy. A unigram entropy of $9.16$ thus entails a still very high perplexity of $2^{9.16}=572.0$. We can render this value more tangible by observing that a model with a perplexity of $572$ is as confused by the data as if it had to choose among $572$ possibilities for each word uniformly and independently.</p><p>To put this into context: The state-of-the-art language model by Jozefowicz et al. (2016) achieves a perplexity of $24.2$ per word on the 1B Word Benchmark. Such a model would thus require an average of around 4.604.60 bits to encode each word, as $2^{4.60}=24.2$, which is incredibly close to the experimental lower bounds documented by Shannon. If and how we could use such a model to construct a better hierarchical softmax layer is still left to be explored.</p><h2 id="Differentiated-Softmax"><a href="#Differentiated-Softmax" class="headerlink" title="Differentiated Softmax"></a>Differentiated Softmax</h2><p>Chen et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:9" target="_blank" rel="external">9</a>] introduce a variation on the traditional softmax layer, the Differentiated Softmax (D-Softmax). D-Softmax is based on the intuition that not all words require the same number of parameters: Many occurrences of frequent words allow us to fit many parameters to them, while extremely rare words might only allow to fit a few.</p><p>In order to do this, instead of the dense matrix of the regular softmax layer of size $d×|V|$ containing the output word embeddings $v^{\prime}_w \in \mathbb{R}^d$, they use a sparse matrix. They then arrange $v′w$ in blocks sorted by frequency, with the embeddings in each block being of a certain dimensionality $d_k$. The number of blocks and their embedding sizes are hyperparameters that can be tuned.</p><p><img src="http://ruder.io/content/images/2016/05/differentiated_softmax_1.png" alt="Differentiated softmax"></p><p>Figure 3: Differentiated softmax (Chen et al. (2015))</p><p>In Figure 3, embeddings in partition $A$ are of dimensionality $d_A$ (these are embeddings of frequent words, as they are allocated more parameters), while embeddings in partitions $B$ and $C$ have $d_B$ and $d_C$ dimensions respectively. Note that all areas not part of any partition, i.e. the non-shaded areas in Figure 1, are set to $0$.</p><p>The output of the previous hidden layer $h$ is treated as a concatenation of features corresponding to each partition of the dimensionality of that partition, e.g. $h$ in Figure 3 is made up of partitions of size $d_A$, $d_B$, and $d_B$ respectively. Instead of computing the matrix-vector product between the entire output embedding matrix and $h$ as in the regular softmax, D-Softmax then computes the product of each partition and its corresponding section in $h$.</p><p>As many words will only require comparatively few parameters, the complexity of computing the softmax is reduced, which speeds up training. In contrast to H-Softmax, this speed-up persists during testing. Chen et al. (2015) observe that D-Softmax is the fastest method when testing, while being one of the most accurate. However, as it assigns fewer parameters to rare words, D-Softmax does a worse job at modelling them.</p><h2 id="CNN-Softmax"><a href="#CNN-Softmax" class="headerlink" title="CNN-Softmax"></a>CNN-Softmax</h2><p>Another modification to the traditional softmax layer is inspired by recent work by Kim et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:13" target="_blank" rel="external">13</a>] who produce input word embeddings $v_w$ via a character-level CNN. Jozefowicz et al. (2016) in turn suggest to do the same thing for the output word embeddings $v^{\prime}_w$ via a character-level CNN – and refer to this as CNN-Softmax. Note that if we have a CNN at the input and at the output as in Figure 4, the CNN generating the output word embeddings $v^{\prime}_w$ is necessarily different from the CNN generating the input word embeddings $v_w$, just as the input and output word embedding matrices would be different.</p><p><img src="http://ruder.io/content/images/2016/05/cnn-softmax_1.png" alt="CNN-Softmax"></p><p>Figure 4: CNN-Softmax (Jozefowicz et al. (2016))</p><p>While this still requires computing the regular softmax normalization, this approach drastically reduces the number of parameters of the model: Instead of storing an embedding matrix of $d \times |V|$, we now only need to keep track of the parameters of the CNN. During testing, the output word embeddings $v^{\prime}_w$ can be pre-computed, so that there is no loss in performance.</p><p>However, as characters are represented in a continuous space and as the resulting model tends to learn a smooth function mapping characters to a word embedding, character-based models often find it difficult to differentiate between similarly spelled words with different meanings. To mitigate this, the authors add a correction factor that is learned per word, which significantly reduces the performance gap between regular and CNN-softmax. By adjusting the dimensionality of the correction term, the authors are able to trade-off model size versus performance.</p><p>The authors also note that instead of using a CNN-softmax, the output of the previous layer hh can be fed to a character-level LSTM, which predicts the output word one character at a time. Instead of a softmax over words, a softmax outputting a probability distribution over characters would thus be used at every time step. They, however, fail to achieve competitive performance with this layer. Ling et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:14" target="_blank" rel="external">14</a>] use a similar layer for machine translation and achieve competitive results.</p><h1 id="Sampling-based-Approaches"><a href="#Sampling-based-Approaches" class="headerlink" title="Sampling-based Approaches"></a>Sampling-based Approaches</h1><p>While the approaches discussed so far still maintain the overall structure of the softmax, sampling-based approaches on the other hand completely do away with the softmax layer. They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. However, sampling-based approaches are only useful at training time – during inference, the full softmax still needs to be computed to obtain a normalised probability.</p><p>In order to gain some intuitions about the softmax denominator’s impact on the loss, we will derive the gradient of our loss function $J_{\theta}$ w.r.t. the parameters of our model $\theta$.<br>During training, we aim to minimize the cross-entropy loss of our model for every word $w$ in the training set. This is simply the negative logarithm of the output of our softmax. If you are unsure of this connection, have a look at <a href="http://cs231n.github.io/linear-classify/#softmax-classifier" target="_blank" rel="external">Karpathy’s explanation</a> to gain some more intuitions about the connection between softmax and cross-entropy. The loss of our model is then the following:<br>$$<br>J_{\theta} = − \log \frac{\exp(h^{\text{T}} v^{\prime}_w)}{\sum_{w_i \in V} \exp(h^{\text{T}} v^{\prime}_{w_i})}.<br>$$<br>Note that in practice $J_{\theta}$ would be the average of all negative log-probabilities over the whole corpus. To facilitate the derivation, we decompose $J_{\theta}$ into a sum as $\log \frac{x}{y} = \log x − \log y$:<br>$$<br>J_\theta = - \: h^\top v^{\prime}_{w} + \text{log} \sum_{w_i \in V} \text{exp}(h^\top v’_{w_i})<br>$$<br>For brevity and to conform with the notation of Bengio and Senécal [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:4" target="_blank" rel="external">4</a>, <a href="http://ruder.io/word-embeddings-softmax/index.html#fn:15" target="_blank" rel="external">15</a>] (note that in the first paper, they compute the gradient of the <em>positive</em> logarithm), we replace the dot product $h^\top v’_{w}$ with $- \mathcal{E}(w)$. Our loss then looks like the following:<br>$$<br>J_\theta = \: \mathcal{E}(w) + \text{log} \sum_{w_i \in V} \text{exp}( - \mathcal{E}(w_i))<br>$$<br>For back-propagation, we can now compute the gradient $\nabla$i of $J_{\theta}$ w.r.t. our model’s parameters $\theta$:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \nabla_\theta \text{log} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))<br>$$<br>As the gradient of $\log x$ is $\dfrac{1}{x}$, an application of the chain rule yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)<br>$$<br>We can now move the gradient inside the sum:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \nabla_\theta \: \text{exp}(- \mathcal{E}(w_i))<br>$$<br>As the gradient of $\exp(x)​$ is just $\exp(x)​$, another application of the chain rule yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)) \nabla_\theta (- \mathcal{E}(w_i))<br>$$<br>We can rewrite this as:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} \dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta (- \mathcal{E}(w_i))<br>$$<br>Note that $\dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))}$ is just the softmax probability $P(w_i)$ of $w_i$ (we omit the dependence on the context cc here for brevity). Replacing it yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} P(w_i) \nabla_\theta (- \mathcal{E}(w_i))<br>$$<br>Finally, repositioning the negative coefficient in front of the sum yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) - \sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i)<br>$$<br>Bengio and Senécal (2003) note that the gradient essentially has two parts: a positive reinforcement for the target word $w$ (the first term in the above equation) and a negative reinforcement for all other words $w_i$, which is weighted by their probability (the second term). As we can see, this negative reinforcement is just the expectation $\mathbb{E}_{w_i \sim P}$ of the gradient of $\mathcal{E}$ for all words $w_i$ in $V$:<br>$$<br>\sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i) = \mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)]<br>$$<br>The crux of most sampling-based approach now is to approximate this negative reinforcement in some way to make it easier to compute, since we don’t want to sum over the probabilities for all words in $V$.</p><h2 id="Importance-Sampling"><a href="#Importance-Sampling" class="headerlink" title="Importance Sampling"></a>Importance Sampling</h2><p>We can approximate the expected value $E$ of any probability distribution using the Monte Carlo method, i.e. by taking the mean of random samples of the probability distribution. If we knew the network’s distribution, i.e. $P(w)$, we could thus directly sample mm words $w_1, \cdots ,w_m$ from it and approximate the above expectation with:<br>$$<br>\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{m} \sum\limits^m_{i=1} \nabla_\theta \mathcal{E}(w_i)<br>$$<br>However, in order to sample from the probability distribution $P$, we need to compute $P$, which is just what we wanted to avoid in the first place. We therefore have find some other distribution $Q$ (we call this the proposal distribution), from which it is cheap to sample and which can be used as the basis of Monte-Carlo sampling. Preferably, $Q$ should also be similar to $P$, since we want our approximated expectation to be as accurate as possible. A straightforward choice in the case of language modelling is to simply use the unigram distribution of the training set for $Q$.</p><p>This is essentially what classical Importance Sampling (IS) does: It uses Monte-Carlo sampling to approximate a target distribution $P$ via a proposal distribution $Q$. However, this still requires computing $P(w)$ for every word ww that is sampled. To avoid this, Bengio and Senécal (2003) use a biased estimator that was first proposed by Liu [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:16" target="_blank" rel="external">16</a>]. This estimator can be used when $P(w)$ is computed as a product, which is the case here, since every division can be transformed into a multiplication.</p><p>Essentially, instead of weighting the gradient $\nabla_\theta \mathcal{E}(w_i)$ with the expensive to compute probability $P_{w_i}$, we weight it with a factor that leverages the proposal distribution $Q$. For biased IS, this factor is $\dfrac{1}{R}r(w_i)$ where $r(w) = \dfrac{\text{exp}(- \mathcal{E}(w))}{Q(w)}$ and $R = \sum^m_{j=1} r(w_j)$.</p><p>Note that we use $r$ and $R$ instead of $w$ and $W$ as in Bengio and Senécal (2003, 2008) to avoid name clashes. As we can see, we still compute the numerator of the softmax, but replace the normalisation in the denominator with the proposal distribution $Q$. Our biased estimator that approximates the expectation thus looks like the following:<br>$$<br>\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{R} \sum\limits^m_{i=1} r(w_i) \nabla_\theta \: \mathcal{E}(w_i)<br>$$<br>Note that the fewer samples we use, the worse is our approximation. We additionally need to adjust our sample size during training, as the network’s distribution $P$ might diverge from the unigram distribution $Q$ during training, which leads to divergence of the model, if the sample size that is used is too small. Consequently, Bengio and Senécal introduce a measure to calculate the effective sample size in order to protect against possible divergence. Finally, the authors report a speed-up factor of $19$ over the regular softmax for this method.</p><h2 id="Adaptive-Importance-Sampling"><a href="#Adaptive-Importance-Sampling" class="headerlink" title="Adaptive Importance Sampling"></a>Adaptive Importance Sampling</h2><p>Bengio and Senécal (2008) note that for Importance Sampling, substituting more complex distributions, e.g. bigram and trigram distributions, later in training to combat the divergence of the unigram distribution $Q$ from the model’s true distribution $P$ does not help, as n-gram distributions seem to be quite different from the distribution of trained neural language models. As an alternative, they propose an n-gram distribution that is adapted during training to follow the target distribution $P$ more closely. To this end, they interpolate a bigram distribution and a unigram distribution according to some mixture function, whose parameters they train with SGD for different frequency bins to minimize the Kullback-Leibler divergence between the distribution $Q$ and the target distribution $P$. For experiments, they report a speed-up factor of about $100$.</p><h2 id="Target-Sampling"><a href="#Target-Sampling" class="headerlink" title="Target Sampling"></a>Target Sampling</h2><p>Jean et al. (2015) propose to use Adaptive Importance Sampling for machine translation. In order to make the method more suitable for processing on a GPU with limited memory, they limit the number of target words that need to be sampled from. They do this by partitioning the training set and including only a fixed number of sample words in every partition, which form a subset $V^{\prime}$ of the vocabulary.</p><p>This essentially means that a separate proposal distribution $Q_i$ can be used for every partition ii of the training set, which assigns equal probability to all words included in the vocabulary subset $V’_i$ and zero probability to all other words.</p><h2 id="Noise-Contrastive-Estimation"><a href="#Noise-Contrastive-Estimation" class="headerlink" title="Noise Contrastive Estimation"></a>Noise Contrastive Estimation</h2><p>Noise Contrastive Estimation (NCE) (Gutmann and Hyvärinen) [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:17" target="_blank" rel="external">17</a>] is proposed by Mnih and Teh [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:18" target="_blank" rel="external">18</a>] as a more stable sampling method than Importance Sampling (IS), as we have seen that IS poses the risk of having the proposal distribution $Q$ diverge from the distribution $P$ that should be optimized. In contrast to the former, NCE does not try to estimate the probability of a word directly. Instead, it uses an auxiliary loss that also optimises the goal of maximizing the probability of correct words.</p><p>Recall the pairwise-ranking criterion of Collobert and Weston (2008) that ranks positive windows higher than “corrupted” windows, which we discussed in the <a href="http://sebastianruder.com/word-embeddings-1/index.html" target="_blank" rel="external">previous post</a>. NCE does a similar thing: We train a model to differentiate the target word from noise. We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive, genuine data from noise samples, as can be seen in Figure 4 below.</p><p><img src="http://ruder.io/content/images/2016/06/negative_sampling.png" alt="Noise Contrastive Estimation"></p><p>Figure 4: Noise Contrastive Estimation (Stephan Gouws’ PhD dissertation [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:24" target="_blank" rel="external">24</a>])</p><p>For every word $w_i$ given its context $c_i$ of $n$ previous words $w_{t-1} , \cdots , w_{t-n+1}$ in the training set, we thus generate $k$ noise samples $w~ik$ from a noise distribution $Q$. As in IS, we can sample from the unigram distribution of the training set. As we need labels to perform our binary classification task, we designate all correct words $w_i$ given their context $c_i$ as true ($y=1$) and all noise samples $\tilde{w}_{ik}$ as false ($y=0$).</p><p>We can now use logistic regression to minimize the negative log-likelihood, i.e. cross-entropy of our training examples against the noise (conversely, we could also maximize the <em>positive</em> log-likelihood as some papers do):<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \mathbb{E}_{\tilde{w}_{ik} \sim Q} [ \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]]<br>$$<br>Instead of computing the expectation $\mathbb{E}_{\tilde{w}_{ik} \sim Q}$ of our noise samples, which would still require summing over all words in $V$ to predict the normalised probability of a negative label, we can again take the mean with the Monte Carlo approximation:<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \sum_{j=1}^k \dfrac{1}{k} \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]<br>$$<br>which reduces to:<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + \: \sum_{j=1}^k \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]<br>$$<br>By generating $k$ noise samples for every genuine word $w_i$ given its context $c$, we are effectively sampling words from two different distributions: Correct words are sampled from the empirical distribution of the training set $P_{\text{train}}$ and depend on their context $c$, whereas noise samples come from the noise distribution $Q$. We can thus represent the probability of sampling either a positive or a noise sample as a mixture of those two distributions, which are weighted based on the number of samples that come from each:<br>$$<br>P(y, w \: | \: c) = \dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)<br>$$<br>Given this mixture, we can now calculate the probability that a sample came from the training $P_{\text{train}}$ distribution as a conditional probability of $y$ given $w$ and $c$:<br>$$<br>P(y=1\:|\:w,c)= \dfrac{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)}{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)}<br>$$<br>which can be simplified to:<br>$$<br>P(y=1\:|\:w,c)= \dfrac{P_{\text{train}}(w \: | \: c)}{P_{\text{train}}(w \: | \: c) + k \: Q(w)}<br>$$<br>As we don’t know $P_{\text{train}}$ (which is what we would like to calculate), we replace $P_{\text{train}}$ with the probability of our model $P$:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{P(w \: | \: c)}{P(w \: | \: c) + k \: Q(w)}<br>$$<br>The probability of predicting a noise sample ($y=0$) is then simply $P(y=0\:|\:w,c) = 1 - P(y=1\:|\:w,c)$. Note that computing $P(w|c)$, i.e. the probability of a word $w$ given its context $c$ is essentially the definition of our softmax:<br>$$<br>P(w \: | \: c) = \dfrac{\text{exp}({h^\top v’_{w}})}{\sum_{w_i \in V} \text{exp}({h^\top v’_{w_i}})}<br>$$<br>For notational brevity and unambiguity, let us designate the denominator of the softmax with $Z(c)$, since the denominator only depends on $h$, which is generated from $c$ (assuming a fixed $V$). The softmax then looks like this:</p><p>$$<br>P(w \: | \: c) = \dfrac{\text{exp}({h^\top v’_{w}})}{Z(c)}<br>$$<br>Having to compute $P(w|c)$ means that – again – we need to compute $Z(c)$, which requires us to sum over the probabilities of all words in $V$. In the case of NCE, there exists a neat trick to circumvent this issue: We can treat the normalisation denominator $Z(c)$ as a parameter that the model can learn.</p><p>Mnih and Teh (2012) and Vaswani et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:20" target="_blank" rel="external">20</a>] actually keep $Z(c)$ fixed at $1$, which they report does not affect the model’s performance. This assumption has the nice side-effect of reducing the model’s parameters, while ensuring that the model self-normalises by not depending on the explicit normalisation in $Z(c)$. Indeed, Zoph et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:19" target="_blank" rel="external">19</a>] find that even when learned, $Z(c)$ is close to $1$ and has low variance.</p><p>If we thus set $Z(c)$ to $1$ in the above softmax equation, we are left with the following probability of word $w$ given a context $c$:</p><p>$$<br>P(w \: | \: c) = \text{exp}({h^\top v’_{w}})<br>$$<br>We can now insert this term in the above equation to compute $P(y=1|w,c)$:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + k \: Q(w)}<br>$$<br>Inserting this term in turn in our logistic regression objective finally yields the full NCE loss:</p><p>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{\text{exp}({h^\top v’_{w_i}})}{\text{exp}({h^\top v’_{w_i}}) + k \: Q(w_i)} + \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{\text{exp}({h^\top v’_{\tilde{w}_{ij}}})}{\text{exp}({h^\top v’_{\tilde{w}_{ij}}}) + k \: Q(\tilde{w}_{ij})})]<br>$$<br>Note that NCE has nice theoretical guarantees: It can be shown that as we increase the number of noise samples $k$, the NCE derivative tends towards the gradient of the softmax function. Mnih and Teh (2012) report that $25$ noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about $45$. For more information on NCE, Chris Dyer has published some excellent notes [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:21" target="_blank" rel="external">21</a>].</p><p>One caveat of NCE is that as typically different noise samples are sampled for every training word ww, the noise samples and their gradients cannot be stored in dense matrices, which reduces the benefit of using NCE with GPUs, as it cannot benefit from fast dense matrix multiplications. Jozefowicz et al. (2016) and Zoph et al. (2016) independently propose to share noise samples across all training words in a mini-batch, so that NCE gradients can be computed with dense matrix operations, which are more efficient on GPUs.</p><h3 id="Similarity-between-NCE-and-IS"><a href="#Similarity-between-NCE-and-IS" class="headerlink" title="Similarity between NCE and IS"></a>Similarity between NCE and IS</h3><p>Jozefowicz et al. (2016) show that NCE and IS are not only similar as both are sampling-based approaches, but are strongly connected. While NCE uses a binary classification task, they show that IS can be described similarly using a surrogate loss function: Instead of performing binary classification with a logistic loss function like NCE, IS then optimises a multi-class classification problem with a softmax and cross-entropy loss function. They observe that as IS performs multi-class classification, it may be a better choice for language modelling, as the loss leads to tied updates between the data and noise samples rather than independent updates as with NCE. Indeed, Jozefowicz et al. (2016) use IS for language modelling and obtain state-of-the-art performance (as mentioned above) on the 1B Word benchmark.</p><h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p>Negative Sampling (NEG), the objective that has been popularised by Mikolov et al. (2013), can be seen as an approximation to NCE. As we have mentioned above, NCE can be shown to approximate the loss of the softmax as the number of samples kk increases. NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modelling.</p><p>NEG also uses a logistic loss function to minimise the negative log-likelihood of words in the training set. Recall that NCE calculated the probability that a word $w$ comes from the empirical training distribution $P_{\text{train}}$ given a context $c$ as follows:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + k \: Q(w)}<br>$$<br>The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible. For this reason, it sets the most expensive term, $kQ(w)$ to $1$, which leaves us with:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + 1}<br>$$<br>$kQ(w)=1$ is exactly then true, when $k=|V|$ and $Q$ is a uniform distribution. In this case, NEG is equivalent to NCE. The reason we set $kQ(w)=1$ and not to some other constant can be seen by rewriting the equation, as $P(y=1|w,c)$ can be transformed into the sigmoid function:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{1}{1 + \text{exp}({-h^\top v’_{w}})}<br>$$<br>If we now insert this back into the logistic regression loss from before, we get:</p><p>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v’_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{1}{1 + \text{exp}({-h^\top v’_{\tilde{w}_{ij}}})}]<br>$$<br>By simplifying slightly, we obtain:</p><p>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v’_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (\dfrac{1}{1 + \text{exp}({h^\top v’_{\tilde{w}_{ij}}})}]<br>$$<br>Setting $\sigma(x) = \dfrac{1}{1 + \text{exp}({-x})}$ finally yields the NEG loss:<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \sigma(h^\top v’_{w_i}) + \: \sum_{j=1}^k \:\text{log} \: \sigma(-h^\top v’_{\tilde{w}_{ij}})]<br>$$<br>To conform with the notation of Mikolov et al. (2013), $h$ must be replaced with $v_{w_{I}}$, v′wivwi′ with v′wOvwO′ and vw~ijvw~ij with v′wivwi′. Also, in contrast to Mikolov’s NEG objective, we a) optimise the objective over the whole corpus, b) minimise negative log-likelihood instead of maximising positive log-likelihood (as mentioned before), and c) have already replaced the expectation Ew~ik∼QEw~ik∼Q with its Monte Carlo approximation. For more insights on the derivation of NEG, have a look at Goldberg and Levy’s notes [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:22" target="_blank" rel="external">22</a>].</p><p>We have seen that NEG is only equivalent to NCE when $k=|V|$ and $Q$ is uniform. In all other cases, NEG only approximates NCE, which means that it will not directly optimise the likelihood of correct words, which is key for language modelling. While NEG may thus be useful for learning word embeddings, its lack of asymptotic consistency guarantees makes it inappropriate for language modelling.</p><h2 id="Self-Normalisation"><a href="#Self-Normalisation" class="headerlink" title="Self-Normalisation"></a>Self-Normalisation</h2><p>Even though the self-normalisation technique proposed by Devlin et al. <a href="http://ruder.io/word-embeddings-softmax/index.html#fn:23" target="_blank" rel="external">23</a> is not a sampling-based approach, it provides further intuitions on self-normalisation of language models, which we briefly touched upon. We previously mentioned in passing that by setting the denominator $Z(c)$ of the NCE loss to $1$, the model essentially self-normalises. This is a useful property as it allows us to skip computing the expensive normalisation in $Z(c)$.</p><p>Recall that our loss function $J_{\theta}$ minimises the negative log-likelihood of all words $w_i$ in our training data:</p><p>$$<br>J_\theta = - \sum\limits_i [\text{log} \: \dfrac{\text{exp}({h^\top v’_{w_i}})}{Z(c)}]<br>$$<br>We can decompose the softmax into a sum as we did before:</p><p>$$<br>J_\theta \: P(w \: | \: c) = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c)]<br>$$<br>If we are able to constrain our model so that it sets $Z(c)=1$ or similarly $\log Z(c)=0$, then we can avoid computing the normalisation in $Z(c)$ altogether. Devlin et al. (2014) thus propose to add a squared error penalty term to the loss function that encourages the model to keep $\log Z(c)$ as close as possible to $0$:</p><p>$$<br>J_\theta = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c) - \alpha \:(\text{log}(Z(c)) - 0)^2]<br>$$<br>which can be rewritten as:</p><p>$$<br>J_\theta = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c) - \alpha \: \text{log}^2 Z(c)]<br>$$<br>where αα allows us to trade-off between model accuracy and mean self-normalisation. By doing this, we can essentially guarantee that $Z(c)$ will be as close to $1$ as we want. At decoding time in their MT system, Devlin et al. (2014) then set the denominator of the softmax to $1$ and only use the numerator for computing P(w|c)P(w|c) together with their penalty term:</p><p>$$<br>J_\theta = - \sum\limits_i [h^\top v’_{w_i} - \alpha \: \text{log}^2 Z(c)]<br>$$<br>They report that self-normalisation achieves a speed-up factor of about $15$, while only resulting in a small degradation of BLEU scores compared to a regular non-self-normalizing neural language model.</p><h2 id="Infrequent-Normalisation"><a href="#Infrequent-Normalisation" class="headerlink" title="Infrequent Normalisation"></a>Infrequent Normalisation</h2><p>Andreas and Klein [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:11" target="_blank" rel="external">11</a>] suggest that it should even be sufficient to only normalise a fraction of the training examples and still obtain approximate self-normalising behaviour. They thus propose Infrequent Normalisation (IN), which down-samples the penalty term, making this a sampling-based approach.</p><p>Let us first decompose the sum of the previous loss $J_{\theta}$ into two separate sums:</p><p>$$<br>J_\theta = - \sum\limits_i h^\top v’_{w_i} + \alpha \sum\limits_i \text{log}^2 Z(c)<br>$$<br>We can now down-sample the second term by only computing the normalisation for a subset $C$ of words $w_j$ and thus of contexts $c_j$ (as $Z(c)$ only depends on the context $c$) in the training data:</p><p>$$<br>J_\theta = - \sum\limits_i h^\top v’_{w_i} + \dfrac{\alpha}{\gamma} \sum\limits_{c_j \in C} \text{log}^2 Z(c_j)<br>$$<br>where $\gamma$ controls the size of the subset $C$. Andreas and Klein (2015) suggest that IF combines the strengths of NCE and self-normalisation as it does not require computing the normalisation for all training examples (which NCE avoids entirely), but like self-normalisation allows trading-off between the accuracy of the model and how well normalisation is approximated. They observe a speed-up factor of $10$ when normalising only a tenth of the training set, with no noticeable performance penalty.</p><h3 id="Other-Approaches"><a href="#Other-Approaches" class="headerlink" title="Other Approaches"></a>Other Approaches</h3><p>So far, we have focused exclusively on approximating or even entirely avoiding the computation of the softmax denominator $Z(c)$, as it is the most expensive term in the computation. We have thus not paid particular attention to $h^\top v’_{w}$, i.e. the dot-product between the penultimate layer representation hh and output word embedding $v^{\prime}_w$. Vijayanarasimhan et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:12" target="_blank" rel="external">12</a>] propose fast locality-sensitive hashing to approximate $h^\top v^{\prime}_{w}$. However, while this technique accelerates the model at test time, during training, these speed-ups virtually vanish as embeddings must be re-indexed and the batch size increases.</p><h1 id="Which-Approach-to-Choose"><a href="#Which-Approach-to-Choose" class="headerlink" title="Which Approach to Choose?"></a>Which Approach to Choose?</h1><p>Having reviewed the most popular softmax-based and sampling-based approaches, we have shown that there are plenty of alternatives to the good ol’ softmax and almost all of them promise a significant speed-up and equivalent or at most marginally deteriorated performance. This naturally poses the question which approach is the best for a particular task.</p><table><thead><tr><th>Approach</th><th>Speed-upfactor</th><th>Duringtraining?</th><th>Duringtesting?</th><th>Performance(small vocab)</th><th>Performance(large vocab)</th><th>Proportion ofparameters</th></tr></thead><tbody><tr><td>Softmax</td><td>1x</td><td>-</td><td>-</td><td>very good</td><td>very poor</td><td>100%</td></tr><tr><td>Hierarchical Softmax</td><td>25x (50-100x)</td><td>X</td><td>-</td><td>very poor</td><td>very good</td><td>100%</td></tr><tr><td>Differentiated Softmax</td><td>2x</td><td>X</td><td>X</td><td>very good</td><td>very good</td><td>&lt; 100%</td></tr><tr><td>CNN-Softmax</td><td>-</td><td>X</td><td>-</td><td>-</td><td>bad - good</td><td>30%</td></tr><tr><td>Importance Sampling</td><td>(19x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>AdaptiveImportance Sampling</td><td>(100x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>Target Sampling</td><td>2x</td><td>X</td><td>-</td><td>good</td><td>bad</td><td>100%</td></tr><tr><td>Noise ContrastiveEstimation</td><td>8x (45x)</td><td>X</td><td>-</td><td>very bad</td><td>very bad</td><td>100%</td></tr><tr><td>Negative Sampling</td><td>(50-100x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>Self-Normalisation</td><td>(15x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>InfrequentNormalisation</td><td>6x (10x)</td><td>X</td><td>-</td><td>very good</td><td>good</td><td>100%</td></tr></tbody></table><p>Table 1: Comparison of approaches to approximate the softmax for language modelling.</p><p>We compare the performance of the approaches we discussed in this post for language modelling in Table 1. Speed-up factors and performance are based on the experiments by Chen et al. (2015), while we show speed-up factors reported by the authors of the original papers in brackets. The third and fourth columns indicate if the speed-up is achieved during training and testing respectively. Note that divergence of speed-up factors might be due to unoptimised implementations or the fact that the original authors might not have had access to GPUs, which benefit the regular softmax more than some of the other approaches. Performance for approaches where no comparison is available should largely be analogous to similar approaches, i.e. Self-Normalisation should achieve similar performance as Infrequent Normalisation and Importance Sampling and Adaptive Importance Sampling should achieve similar performance as Target Sampling. The performance of CNN-Softmax is as reported by Jozefowicz et al. (2016) and ranges from bad to good depending on the size of the correction. Of all approaches, only CNN-Softmax achieves a substantial reduction in parameters as the other approaches still require storing output embeddings. Differentiated Softmax reduces parameters by being able to store a sparse weight matrix.</p><p>As it always is, there is no clear winner that beats all other approaches on all datasets or tasks. For language modelling, the regular softmax still achieves very good performance on small vocabulary datasets, such as the Penn Treebank, and even performs well on medium datasets, such as Gigaword, but does very poorly on large vocabulary datasets, e.g. the 1B Word Benchmark. Target Sampling, Hierarchical Softmax, and Infrequent Normalisation in turn do better with large vocabularies.</p><p>Differentiated Softmax generally does well for both small and large vocabularies and is the only approach that ensures a speed-up at test time. Interestingly, Hierarchical Softmax (HS) performs very poorly with small vocabularies. However, of all methods, HS is the fastest and processes most training examples in a given time frame. While NCE performs well with large vocabularies, it is generally worse than the other methods. Negative Sampling does not work well for language modelling, but it is generally superior for learning word representations, as attested by word2vec’s success. Note that all results should be taken with a grain of salt: Chen et al. (2015) report having difficulties using Noise Contrastive Estimation in practice; Kim et al. (2016) use Hierarchical Softmax to achieve state-of-the-art with a small vocabulary, while Importance Sampling is used by the state-of-the-art language model by Jozefowicz et al. (2016) on a dataset with a large vocabulary.</p><p>Finally, if you are looking to actually use the described methods, TensorFlow has <a href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#candidate-sampling" target="_blank" rel="external">implementations</a> for a few sampling-based approaches and also explains the differences between some of them <a href="https://www.tensorflow.org/extras/candidate_sampling.pdf" target="_blank" rel="external">here</a>.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>This overview of different methods to approximate the softmax attempted to provide you with intuitions that can not only be applied to improve and speed-up learning word representations, but are also relevant for language modelling and machine translation. As we have seen, most of these approaches are closely related and are driven by one uniting factor: the necessity to approximate the expensive normalisation in the denominator of the softmax. With these approaches in mind, I hope you feel now better equipped to train and understand your models and that you might even feel ready to work on learning better word representations yourself.</p><p>As we have seen, learning word representations is a vast field and many factors are relevant for success. In the previous blog post, we looked at the architectures of popular models and in this blog post, we investigated more closely a key component, the softmax layer. In the next one, we will introduce GloVe, a method that relies on matrix factorisation rather than language modelling, and turn our attention to other hyperparameters that are essential for successfully learning word embeddings.</p><p><strong>As always, let me know about any mistakes I made and approaches I missed in the comments below.</strong></p><h1 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h1><p>If you found this blog post helpful, please consider citing it as:</p><p><em>Sebastian Ruder. On word embeddings - Part 2: Approximating the Softmax. <a href="http://ruder.io/word-embeddings-softmax" target="_blank" rel="external">http://ruder.io/word-embeddings-softmax</a>, 2016.</em></p><h1 id="Other-blog-posts-on-word-embeddings"><a href="#Other-blog-posts-on-word-embeddings" class="headerlink" title="Other blog posts on word embeddings"></a>Other blog posts on word embeddings</h1><p>If you want to learn more about word embeddings, these other blog posts on word embeddings are also available:</p><ul><li><a href="http://sebastianruder.com/word-embeddings-1/index.html" target="_blank" rel="external">On word embeddings - Part 1</a></li><li><a href="http://sebastianruder.com/secret-word2vec/index.html" target="_blank" rel="external">On word embeddings - Part 3: The secret ingredients of word2vec</a></li><li><a href="http://sebastianruder.com/cross-lingual-embeddings/index.html" target="_blank" rel="external">Unofficial Part 4: A survey of cross-lingual embedding models</a></li><li><a href="http://ruder.io/word-embeddings-2017/index.html" target="_blank" rel="external">Unofficial Part 5: Word embeddings in 2017 - Trends and future directions</a></li></ul><h1 id="Translations"><a href="#Translations" class="headerlink" title="Translations"></a>Translations</h1><p>This blog post has been translated into the following languages:</p><ul><li><a href="http://geek.csdn.net/news/detail/135736" target="_blank" rel="external">Chinese</a></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:1" target="_blank" rel="external"></a></li><li>Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:2" target="_blank" rel="external"></a></li><li>Morin, F., &amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats, 5. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:3" target="_blank" rel="external"></a></li><li>Bengio, Y., &amp; Senécal, J.-S. (2003). Quick Training of Probabilistic Neural Nets by Importance Sampling. AISTATS. <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/submit_aistats2003.pdf" target="_blank" rel="external">http://www.iro.umontreal.ca/~lisa/pointeurs/submit_aistats2003.pdf</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:4" target="_blank" rel="external"></a></li><li>Shannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. <a href="http://doi.org/10.1002/j.1538-7305.1951.tb01366.x" target="_blank" rel="external">http://doi.org/10.1002/j.1538-7305.1951.tb01366.x</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:5" target="_blank" rel="external"></a></li><li>Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from <a href="http://arxiv.org/abs/1602.02410" target="_blank" rel="external">http://arxiv.org/abs/1602.02410</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:6" target="_blank" rel="external"></a></li><li>Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738, 1–19. Retrieved from <a href="http://arxiv.org/abs/1411.2738" target="_blank" rel="external">http://arxiv.org/abs/1411.2738</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:7" target="_blank" rel="external"></a></li><li>Mnih, A., &amp; Hinton, G. E. (2008). A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, 1–8. Retrieved from <a href="http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf" target="_blank" rel="external">http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:8" target="_blank" rel="external"></a></li><li>Chen, W., Grangier, D., &amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. Retrieved from <a href="http://arxiv.org/abs/1512.04906" target="_blank" rel="external">http://arxiv.org/abs/1512.04906</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:9" target="_blank" rel="external"></a></li><li>Jean, S., Cho, K., Memisevic, R., &amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from <a href="http://www.aclweb.org/anthology/P15-1001" target="_blank" rel="external">http://www.aclweb.org/anthology/P15-1001</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:10" target="_blank" rel="external"></a></li><li>Andreas, J., &amp; Klein, D. (2015). When and why are log-linear models self-normalizing? Naacl-2015, 244–249. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:11" target="_blank" rel="external"></a></li><li>Vijayanarasimhan, S., Shlens, J., Monga, R., &amp; Yagnik, J. (2015). Deep Networks With Large Output Spaces. Iclr, 1–9. Retrieved from <a href="http://arxiv.org/abs/1412.7479" target="_blank" rel="external">http://arxiv.org/abs/1412.7479</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:12" target="_blank" rel="external"></a></li><li>Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from <a href="http://arxiv.org/abs/1508.06615" target="_blank" rel="external">http://arxiv.org/abs/1508.06615</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:13" target="_blank" rel="external"></a></li><li>Ling, W., Trancoso, I., Dyer, C., &amp; Black, A. W. (2016). Character-based Neural Machine Translation. ICLR, 1–11. Retrieved from <a href="http://arxiv.org/abs/1511.04586" target="_blank" rel="external">http://arxiv.org/abs/1511.04586</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:14" target="_blank" rel="external"></a></li><li>Bengio, Y., &amp; Senécal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713–722. <a href="http://doi.org/10.1109/TNN.2007.912312" target="_blank" rel="external">http://doi.org/10.1109/TNN.2007.912312</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:15" target="_blank" rel="external"></a></li><li>Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer. <a href="http://doi.org/10.1017/CBO9781107415324.004" target="_blank" rel="external">http://doi.org/10.1017/CBO9781107415324.004</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:16" target="_blank" rel="external"></a></li><li>Gutmann, M., &amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. International Conference on Artificial Intelligence and Statistics, 1–8. Retrieved from <a href="http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf" target="_blank" rel="external">http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:17" target="_blank" rel="external"></a></li><li>Mnih, A., &amp; Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICML’12), 1751–1758. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:18" target="_blank" rel="external"></a></li><li>Zoph, B., Vaswani, A., May, J., &amp; Knight, K. (2016). Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. NAACL. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:19" target="_blank" rel="external"></a></li><li>Vaswani, A., Zhao, Y., Fossum, V., &amp; Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), (October), 1387–1392. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:20" target="_blank" rel="external"></a></li><li>Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. Arxiv preprint. Retrieved from <a href="http://arxiv.org/abs/1410.8251" target="_blank" rel="external">http://arxiv.org/abs/1410.8251</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:21" target="_blank" rel="external"></a></li><li>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method. arXiv Preprint arXiv:1402.3722, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722" target="_blank" rel="external">http://arxiv.org/abs/1402.3722</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:22" target="_blank" rel="external"></a></li><li>Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., &amp; Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. Proc. ACL’2014, 1370–1380. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:23" target="_blank" rel="external"></a></li><li>Gouws, S. (2016). Training neural word embeddings for transfer learning and translation (Doctoral dissertation, Stellenbosch: Stellenbosch University). <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:24" target="_blank" rel="external"></a></li></ol><p>Credit for the cover image goes to <a href="http://stephangouws.com/" target="_blank" rel="external">Stephan Gouws</a> who included the image in his <a href="http://scholar.sun.ac.za/handle/10019.1/98758" target="_blank" rel="external">PhD dissertation</a> and in the <a href="https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html" target="_blank" rel="external">Tensorflow word2vec tutorial</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is the second post in a series on word embeddings and representation learning. In the &lt;a href=&quot;http://sebastianruder.com/word-embedd
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>DropConnect Implementation in Python and TensorFlow [Repost]</title>
    <link href="http://yoursite.com/2018/01/15/DropConnect-Implementation-in-Python-and-TensorFlow-Repost/"/>
    <id>http://yoursite.com/2018/01/15/DropConnect-Implementation-in-Python-and-TensorFlow-Repost/</id>
    <published>2018-01-15T14:36:19.000Z</published>
    <updated>2018-01-15T14:38:56.183Z</updated>
    
    <content type="html"><![CDATA[<p>Source post is <a href="https://nickcdryan.wordpress.com/2017/06/13/dropconnect-implementation-in-python-and-tensorflow/" target="_blank" rel="external">here</a>.</p><hr><p>I wouldn’t expect DropConnect to appear in TensorFlow, Keras, or Theano since, as far as I know, it’s used pretty rarely and doesn’t seem as well-studied or demonstrably more useful than its cousin, Dropout. However, there don’t seem to be any implementations out there, so I’ll provide a few ways of doing so.</p><p><img src="https://nickcdryan.files.wordpress.com/2017/06/screen-shot-2017-06-13-at-3-01-19-am.png?w=840" alt="Screen Shot 2017-06-13 at 3.01.19 AM"></p><p>For the briefest of refreshers, DropConnect (<a href="http://proceedings.mlr.press/v28/wan13.pdf" target="_blank" rel="external">Wan et al.</a>) regularizes networks like Dropout. Instead of dropping neurons, DropConnect regularizes by randomly dropping a subset of weights. A binary mask drawn from a Bernoulli distribution is applied to the original weight matrix (we’re just setting some connections to 0 with a certain probability):</p><p><img src="https://s0.wp.com/latex.php?latex=output+%3D+a%28%28M%C2%A0%5Codot+W%29v%29&amp;bg=ffffff&amp;fg=1a1a1a&amp;s=0" alt="output = a((M \odot W)v)"></p><p>where a is an activation function, v is input matrix, W is weight matrix, <img src="https://s0.wp.com/latex.php?latex=%5Codot&amp;bg=ffffff&amp;fg=1a1a1a&amp;s=0" alt="\odot"> is Hadamard (element-wise multiplication), and M is the binary mask drawn from a Bernoulli distribution with probability p.</p><p>Pure Python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> operator</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask_size_helper</span><span class="params">(args)</span>:</span></div><div class="line">    <span class="comment"># multiply n dimensions to get array size</span></div><div class="line">    <span class="keyword">return</span> reduce(operator.mul, args) </div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dropconnect_mask</span><span class="params">(dc_keep_prob, dimensions)</span>:</span></div><div class="line">    <span class="comment"># get binary mask of size=*dimensions from binomial dist. with dc_keep_prob = prob of drawing a 1</span></div><div class="line">    mask_vector = np.random.binomial(<span class="number">1</span>, dc_keep_prob, mask_size_helper(dimensions))</div><div class="line">    <span class="comment"># reshape mask to correct dimensions (we could just broadcast, but that's messy)</span></div><div class="line">    mask_array = mask_vector.reshape(dimensions)</div><div class="line">    <span class="keyword">return</span> mask_array</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropconnect</span><span class="params">(W, dc_keep_prob)</span>:</span></div><div class="line">    dimensions = W.shape</div><div class="line">    <span class="keyword">return</span> W * create_dropconnect_mask(dc_keep_prob, dimensions)</div></pre></td></tr></table></figure><p>TensorFlow (unnecessarily hard way):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropconnect</span><span class="params">(W, p)</span>:</span></div><div class="line">    M_vector = tf.multinomial(tf.log([[<span class="number">1</span>-p, p]]), np.prod(W_shape))</div><div class="line">    M = tf.reshape(M_vector, W_shape)</div><div class="line">    M = tf.cast(M, tf.float32)</div><div class="line">    <span class="keyword">return</span> M * W</div></pre></td></tr></table></figure><p>TensorFlow (easy way / recommended):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropconnect</span><span class="params">(W, p)</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.nn.dropout(W, keep_prob=p) * p</div></pre></td></tr></table></figure><p>Yes, sadly after a good amount of time spent searching for existing implementations and then creating my own, I took a look at the <a href="https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/ops/nn_ops.py" target="_blank" rel="external">dropout source code</a> and found that plain old dropout does the job so long as you remember to scale the weight matrix back down by keep_prob. After realizing that a connection weight matrix used for DropConnect is compatible input for the layer of neurons used in dropout, the only actual implementation difference between Dropout and DropConnect on TensorFlow is whether or not the weights in the masked matrix get scaled up (to preserve the expected sum).</p><p>I find DropConnect interesting, not so much as a regularization method but for some novel extensions that I’d like to try. I’ve played around with using keep_prob in our new DropConnect function as a trainable variable in the graph so that, if you incorporate keep_prob into the loss function in a way that creates interesting gradients, you can punish your network for the amount of connections it makes between neurons.</p><p>More interesting would be to see if we can induce modularity in the network by persisting dropped connections. That is, instead of randomly dropping an entirely new subset of connections at each training example, connections would drop and stay dropped perhaps as a result of the input data class or the connection’s contribution to deeper layers. For another post…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Source post is &lt;a href=&quot;https://nickcdryan.wordpress.com/2017/06/13/dropconnect-implementation-in-python-and-tensorflow/&quot; target=&quot;_blank&quot;
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Short Video Title Classification</title>
    <link href="http://yoursite.com/2017/12/06/Short-Video-Title-Classification-Problem/"/>
    <id>http://yoursite.com/2017/12/06/Short-Video-Title-Classification-Problem/</id>
    <published>2017-12-06T07:22:42.000Z</published>
    <updated>2017-12-06T07:27:52.484Z</updated>
    
    <content type="html"><![CDATA[<p>本文档是<a href="https://github.com/ewanlee/video_title_classification" target="_blank" rel="external">Github项目</a>的流程解释文档，具体实现请移步。</p><p>本项目解决的是视频短标题的多分类问题，目前涉及到33个类，所采用的算法包括TextCNN，TextRNN，TextRCNN以及HAN。目前效果最好的是TextCNN算法。</p><p>项目流程大体框架如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/1.png" alt="1"></p><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>数据预处理部分主要涉及到的文件有：</p><ul><li><code>ordered_set.py</code></li><li><code>preprocess.py</code></li></ul><p>大致流程如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/2.png" alt="2"></p><h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><p>初始的文件包括三个：</p><ul><li><code>all_video_info.txt</code> 该文件是后两个数据的合并，作为数据预处理算法输入</li><li><code>all_video_info_month_day.txt</code>（这里的month和day由具体数值替换）这类文件包含多个，<strong>只使用最新的</strong>，是正式的标题数据， 包括已标记的以及未标记的</li><li><code>add_signed_video_info.txt</code> 该文件是从其他数据库中选取的经人工标注的数据，只含有已标记的标题</li></ul><p>所有文件的格式都是一样的，每一行代表一个样本，分为四列，中间用制表符间隔。</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/3.png" alt="3"></p><p>其中第一列代表视频URL；第二列为该视频类别是否经过算法修改，最开始全都为0；第三列为视频标签；第四列为视频标题。</p><p>视频标签的映射表如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/4.png" alt="4"></p><p>在数据加载部分，我们将数据分为有标记数据以及无标记数据，有标记数据将用来训练以及测试分类器，然后用训练好的分类器预测无标记数据的标签。</p><p>分类的依据首先是根据视频标签是否为0，如果为0，代表视频是未标记的。其次，已标记的数据中有些类别是会对算法造成干扰，这里我们也将其去掉。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>load_data</code>方法。</p><h2 id="去除特殊符号"><a href="#去除特殊符号" class="headerlink" title="去除特殊符号"></a>去除特殊符号</h2><p>由于视频标题中存在一些表情等特殊符号，在这个阶段将其去掉。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>remove_emoji</code>方法。</p><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>本项目采用结巴分词作为分词器。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>cut</code>方法。</p><h2 id="去停止词"><a href="#去停止词" class="headerlink" title="去停止词"></a>去停止词</h2><p>本项目采用了<code>data/stopword.dic</code>文件中的停止词表，值得注意的是，句子去停止词前后去停止词后，单词的相对顺序保持不变。这里我们采用了有序集合（具体实现在<code>ordered_set.py</code>文件中）实现。</p><p>经过这一步之后，句子中重复的非停止词将只会取一次。但是由于视频标题较短，出现重复词的概率非常小，因此不会有太大影响。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>remove_stop_words</code>方法。</p><h2 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h2><p>将所有视频标题经过分词后的单词汇总起来建立一个词典，供后续句子建模使用。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>vocab_build</code>方法。</p><h2 id="句子建模"><a href="#句子建模" class="headerlink" title="句子建模"></a>句子建模</h2><p>将分词后的视频标题中的每个词替换为其在词典中的序号，这样每个标题将会转换为由一串数组构成的向量。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>word2index</code>方法。</p><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>之前提到过，本文一共运用了四种深度学习模型，采用tensorflow框架，训练过程中涉及到的文件分为两类：</p><ul><li>模型文件， 包括<code>textcnn.py</code>, <code>textrnn.py</code>, <code>textrcnn.py</code>以及<code>han.py</code></li><li>训练文件，包括<code>train_cnn.py</code>, <code>train_rnn.py</code>, <code>train_rcnn.py</code>以及<code>train_han.py</code></li></ul><p>模型文件定义了具体的模型，本篇文档将不会具体地讲解实现代码，只会从理论层面介绍模型。训练文件包含了算法的训练过程，由于不同算法的训练流程一致，这里单挑TextCNN讲解。</p><p>下面开始介绍模型，如果只关注实现可以跳过到训练部分。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>分布式表示（Distributed Representation）是Hinton 在1986年提出的，基本思想是将每个词表达成 n 维稠密、连续的实数向量，与之相对的one-hot encoding向量空间只有一个维度是1，其余都是0。分布式表示最大的优点是具备非常powerful的特征表达能力，比如 n 维向量每维 k 个值，可以表征 $k^n$个概念。事实上，不管是神经网络的隐层，还是多个潜在变量的概率主题模型，都是应用分布式表示。下图是03年Bengio在 <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a> 的网络结构：</p><p><img src="https://pic3.zhimg.com/50/v2-dc007baa415cf1674df6d323419cc2de_hd.jpg" alt="img"></p><p>这篇文章提出的神经网络语言模型（NNLM，Neural Probabilistic Language Model）采用的是文本分布式表示，即每个词表示为稠密的实数向量。NNLM模型的目标是构建语言模型：</p><p><img src="https://pic1.zhimg.com/50/v2-855f785d33895960712509982199c4b4_hd.jpg" alt="img"></p><p>词的分布式表示即词向量（word embedding）是训练语言模型的一个附加产物，即图中的Matrix C。</p><p>尽管Hinton 86年就提出了词的分布式表示，Bengio 03年便提出了NNLM，词向量真正火起来是google Mikolov 13年发表的两篇word2vec的文章 <a href="http://ttic.uchicago.edu/%7Ehaotang/speech/1301.3781.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a>和<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="external">Distributed Representations of Words and Phrases and their Compositionality</a>，更重要的是发布了简单好用的<strong>word2vec工具包</strong>，在语义维度上得到了很好的验证，极大的推进了文本分析的进程。下图是文中提出的CBOW 和 Skip-Gram两个模型的结构，基本类似于NNLM，不同的是模型去掉了非线性隐层，预测目标不同，CBOW是上下文词预测当前词，Skip-Gram则相反。</p><p><img src="https://pic2.zhimg.com/50/v2-04bfc01157c1c3ae1480299947315251_hd.jpg" alt="img"></p><p>除此之外，提出了Hierarchical Softmax 和 Negative Sample两个方法，很好的解决了计算有效性，事实上这两个方法都没有严格的理论证明，有些trick之处，非常的实用主义。详细的过程不再阐述了，有兴趣深入理解word2vec的，推荐读读这篇很不错的paper: <a href="http://www-personal.umich.edu/%7Eronxin/pdf/w2vexp.pdf" target="_blank" rel="external">word2vec Parameter Learning Explained</a>。额外多提一点，实际上word2vec学习的向量和真正语义还有差距，更多学到的是具备相似上下文的词，比如“good” “bad”相似度也很高，反而是文本分类任务输入有监督的语义能够学到更好的语义表示。</p><p>至此，文本的表示通过词向量的表示方式，把文本数据从高纬度高稀疏的神经网络难处理的方式，变成了类似图像、语音的的连续稠密数据。深度学习算法本身有很强的数据迁移性，很多之前在图像领域很适用的深度学习算法比如CNN等也可以很好的迁移到文本领域了，</p><h3 id="深度学习文本分类模型"><a href="#深度学习文本分类模型" class="headerlink" title="深度学习文本分类模型"></a>深度学习文本分类模型</h3><h4 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h4><p>本篇文章的题图选用的就是14年这篇文章提出的TextCNN的结构（见下图）。卷积神经网络<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" target="_blank" rel="external">CNN Convolutional Neural Network</a>最初在图像领域取得了巨大成功，CNN原理就不讲了，核心点在于可以<strong>捕捉局部相关性</strong>，具体到文本分类任务中可以利用CNN来提取句子中类似 n-gram 的关键信息。</p><p><img src="https://pic1.zhimg.com/50/v2-ab904178abf9241329e3e2d0fa7c0584_hd.jpg" alt="img"></p><p>TextCNN的详细过程原理图见下：</p><p><img src="https://pic3.zhimg.com/50/v2-bb10ad5bbdc5294d3041662f887e60a6_hd.jpg" alt="img"></p><p><strong>TextCNN详细过程</strong>：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。</p><p><strong>特征</strong>：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。</p><p><strong>通道（Channels）</strong>：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。</p><p><strong>一维卷积（conv-1d）</strong>：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。</p><p><strong>Pooling层</strong>：利用CNN解决文本分类问题的文章还是很多的，比如这篇 <a href="https://arxiv.org/pdf/1404.2188.pdf" target="_blank" rel="external">A Convolutional Neural Network for Modelling Sentences</a> 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">“ 我觉得这个地方景色还不错，但是人也实在太多了 ”</div></pre></td></tr></table></figure><p>虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。</p><h4 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h4><p>尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。</p><p>RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，<a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf" target="_blank" rel="external">Recurrent Neural Network for Text Classification with Multi-Task Learning</a>文中介绍了RNN用于分类问题的设计，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。</p><p><img src="https://pic3.zhimg.com/50/v2-92e49aef6626add56e85c2ee1b36e9aa_hd.jpg" alt="img"></p><h4 id="TextRCNN-TextCNN-TextRNN"><a href="#TextRCNN-TextCNN-TextRNN" class="headerlink" title="TextRCNN (TextCNN + TextRNN)"></a>TextRCNN (TextCNN + TextRNN)</h4><p>我们参考的是中科院15年发表在AAAI上的这篇文章 Recurrent Convolutional Neural Networks for Text Classification 的结构：</p><p><img src="https://pic3.zhimg.com/50/v2-263209ce34c0941fece21de00065aa92_hd.jpg" alt="img"></p><p>利用前向和后向RNN得到每个词的前向和后向上下文的表示：</p><p><img src="https://pic1.zhimg.com/50/v2-d97b136cbb9cd98354521a827e0fd8b4_hd.jpg" alt="img"></p><p>这样词的表示就变成词向量和前向后向上下文向量concat起来的形式了，即：</p><p><img src="https://pic4.zhimg.com/50/v2-16378ac29633452e7093288fd98d3f73_hd.jpg" alt="img"></p><p>最后再接跟TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1就可以了，不再需要更大 filter_size 获得更大视野，这里词的表示也可以只用双向RNN输出。</p><h4 id="HAN-TextRNN-Attention"><a href="#HAN-TextRNN-Attention" class="headerlink" title="HAN (TextRNN + Attention)"></a>HAN (TextRNN + Attention)</h4><p>CNN和RNN用在文本分类任务中尽管效果显著，但都有一个不足的地方就是不够直观，可解释性不好，特别是在分析badcase时候感受尤其深刻。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够很直观的给出每个词对结果的贡献，基本成了Seq2Seq模型的标配了。</p><p><strong>Attention机制介绍</strong>：</p><p>详细介绍Attention恐怕需要一小篇文章的篇幅，感兴趣的可参考14年这篇paper <a href="https://arxiv.org/pdf/1409.0473v7.pdf" target="_blank" rel="external">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>。</p><p>以机器翻译为例简单介绍下，下图中$x_t$是源语言的一个词，$y_t$是目标语言的一个词，机器翻译的任务就是给定源序列得到目标序列。翻译$y_t$的过程产生取决于上一个词 $y_{t-1}$ 和源语言的词的表示 $h_{j}$($x_{j}$) 的 bi-RNN 模型的表示），而每个词所占的权重是不一样的。比如源语言是中文 “我 / 是 / 中国人” 目标语言 “i / am / Chinese”，翻译出“Chinese”时候显然取决于“中国人”，而与“我 / 是”基本无关。下图公式, $\alpha _{ij}$则是翻译英文第$i$个词时，中文第$j$个词的贡献，也就是注意力。显然在翻译“Chinese”时，“中国人”的注意力值非常大。</p><p><img src="https://pic3.zhimg.com/50/v2-de9146388978dfe7ef467993b9cf12ae_hd.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/50/v2-0ebc7b64a7d34a908b8d82d87c92f6b8_hd.jpg" alt="img"></p><p>Attention的核心point是在翻译每个目标词（或 预测商品标题文本所属类别）所用的上下文是不同的，这样的考虑显然是更合理的。</p><p><strong>TextRNN + Attention 模型</strong>：</p><p>我们参考了这篇文章 <a href="https://www.cs.cmu.edu/%7Ediyiy/docs/naacl16.pdf" target="_blank" rel="external">Hierarchical Attention Networks for Document Classification</a>，下图是模型的网络结构图，它一方面用层次化的结构保留了文档的结构，另一方面在word-level和sentence-level。标题场景只需要 word-level 这一层的 Attention 即可。</p><p><img src="https://pic3.zhimg.com/50/v2-4ff2c8099ccf0b2d8eb963a0ac248296_hd.jpg" alt="img"></p><p>加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。</p><h2 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h2><p>现在来详细讲解训练过程，涉及到的文件<code>train_cnn.py</code>, <code>utils.py</code>, <code>textcnn.py</code></p><p>注意到<code>train_cnn.py</code>文件最后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span>  __name__ == <span class="string">'__main__'</span>:</div><div class="line">    os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">'1'</span></div><div class="line">    tf.app.run()</div></pre></td></tr></table></figure><p>其中第一行是指定只用一个GPU。第二行是tensorflow的一个运行框架，<code>run</code>会运行文件内的<code>main</code>方法，并且传入文件最开始设定的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># configuration</span></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">"num_classes"</span>, <span class="number">33</span>, <span class="string">"number of label"</span>)</div><div class="line">tf.app.flags.DEFINE_float(<span class="string">"learning_rate"</span>, <span class="number">0.01</span>, <span class="string">"learning rate"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"batch_size"</span>, <span class="number">64</span>, <span class="string">"Batch size for training/evaluating."</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"decay_steps"</span>, <span class="number">1000</span>, <span class="string">"how many steps before decay learning rate."</span>)</div><div class="line">tf.app.flags.DEFINE_float(</div><div class="line">  <span class="string">"decay_rate"</span>, <span class="number">0.95</span>, <span class="string">"Rate of decay for learning rate."</span>)</div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">  <span class="string">"ckpt_dir"</span>, <span class="string">"text_cnn_title_desc_checkpoint/"</span>, <span class="string">"checkpoint location for the model"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"sentence_len"</span>, <span class="number">15</span>, <span class="string">"max sentence length"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">"embed_size"</span>, <span class="number">64</span>, <span class="string">"embedding size"</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"is_training"</span>, <span class="keyword">True</span>, <span class="string">"is traning.true:tranining,false:testing/inference"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"num_epochs"</span>, <span class="number">30</span>, <span class="string">"number of epochs to run."</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"validate_every"</span>, <span class="number">1</span>, <span class="string">"Validate every validate_every epochs."</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"use_embedding"</span>, <span class="keyword">True</span>, <span class="string">"whether to use embedding or not."</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"num_filters"</span>, <span class="number">256</span>, <span class="string">"number of filters"</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"multi_label_flag"</span>, <span class="keyword">False</span>, <span class="string">"use multi label or single label."</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"just_train"</span>, <span class="keyword">False</span>, <span class="string">"whether use all data to train or not."</span>)</div></pre></td></tr></table></figure><p>第一个参数代表参数名（调用这个参数的方法：<code>FLAGS.name</code>），第二个参数是默认值，第三个参数是描述。值得说明的是这里有一个<code>just_train</code>参数，它代表是否将测试集放入训练集一起训练，一般在用模型最终确定之后。</p><p>所以运行<code>python train_cnn.py</code>就是启动训练过程，同时可以传入参数，方法为<code>python train_cnn.py --name value</code>, 这里的name就是文件定义的参数名，value就是你要设定的值。如果不传入参数，则参数为默认值。</p><p>下面我们来看一下<code>main</code>函数，流程如下：</p><p>​ <img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/5.png" alt="5"></p><h3 id="数据加载-1"><a href="#数据加载-1" class="headerlink" title="数据加载"></a>数据加载</h3><p>这个过程主要是调用<code>train_test_loader</code>方法切分训练集与测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">X_train, X_val, y_train, y_val, n_classes = </div><div class="line">	train_test_loader(FLAGS.just_train)</div></pre></td></tr></table></figure><h3 id="词典加载"><a href="#词典加载" class="headerlink" title="词典加载"></a>词典加载</h3><p>加载数据预处理过程中建立的词典。目的是用来从预训练的词向量词典中拿出对应的词向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> open(<span class="string">'data/vocab.dic'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</div><div class="line">    vocab = pickle.load(f)</div><div class="line">vocab_size = len(vocab) + <span class="number">1</span></div><div class="line">print(<span class="string">'size of vocabulary: &#123;&#125;'</span>.format(vocab_size))</div></pre></td></tr></table></figure><p>这里将词典的长度加一是为了给一个特殊词“空”加入位置，“空”的作用是填充短标题，让所有标题长度一样。</p><h3 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h3><p>这个阶段就是将所有标题长度变成一致，短了就填充，长了就截断。标题长度是一个参数，可以设置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># padding sentences</span></div><div class="line">    X_train = pad_sequences(X_train, maxlen=FLAGS.sentence_len, </div><div class="line">    	value=float(vocab_size - <span class="number">1</span>))</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.just_train:</div><div class="line">        X_val = pad_sequences(</div><div class="line">         	X_val, maxlen=FLAGS.sentence_len, value=float(vocab_size - <span class="number">1</span>))</div></pre></td></tr></table></figure><h3 id="模型实例化"><a href="#模型实例化" class="headerlink" title="模型实例化"></a>模型实例化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">textcnn = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes,</div><div class="line">                FLAGS.learning_rate, FLAGS.batch_size,</div><div class="line">                FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sentence_len,</div><div class="line">                vocab_size, FLAGS.embed_size, FLAGS.is_training, </div><div class="line">                multi_label_flag=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>如果有之前训练到一半的模型，那我们就加载那个模型的参数，继续训练，否则进行参数初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Initialize save</span></div><div class="line">        saver = tf.train.Saver()</div><div class="line">        <span class="keyword">if</span> os.path.exists(FLAGS.ckpt_dir + <span class="string">'checkpoint'</span>):</div><div class="line">            print(<span class="string">'restoring variables from checkpoint'</span>)</div><div class="line">            saver.restore(</div><div class="line">            	sess, </div><div class="line">            	tf.train.latest_checkpoint(FLAGS.ckpt_dir))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">'Initializing Variables'</span>)</div><div class="line">            sess.run(tf.global_variables_initializer())</div><div class="line">            <span class="keyword">if</span> FLAGS.use_embedding:</div><div class="line">                assign_pretrained_word_embedding(</div><div class="line">                	sess, vocab, vocab_size, textcnn)</div></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>模型训练过程中包括两个循环，第一个是大循环，表示遍历所有训练数据多少遍。第二个是mini-batch循环，小循环走过一遍代表遍历了所有训练数据一遍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(curr_epoch, total_epochs):</div><div class="line">            loss, acc, counter = <span class="number">.0</span>, <span class="number">.0</span>, <span class="number">0</span></div><div class="line">            <span class="keyword">for</span> start, end <span class="keyword">in</span> zip(</div><div class="line">                    range(<span class="number">0</span>, number_of_training_data, batch_size),</div><div class="line">                    range(batch_size, number_of_training_data, </div><div class="line">                    batch_size)):</div></pre></td></tr></table></figure><p>下面就是将训练数据喂到模型中:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">feed_dict = &#123;textcnn.input_x: X_train[start:end], 		</div><div class="line">			textcnn.dropout_keep_prob: <span class="number">0.5</span>&#125;</div></pre></td></tr></table></figure><p>第二个参数是模型相关的dropout参数，用于减少过拟合，范围是(0, 1]，基本不用改变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">curr_loss, curr_acc, _ = sess.run(</div><div class="line">                        [textcnn.loss_val, textcnn.accuracy, </div><div class="line">                        textcnn.train_op], feed_dict)</div></pre></td></tr></table></figure><p>这一步就是得到这一小部分训练数据对应的准确率以及loss。</p><p>然后每经过<code>validate_every</code>个大循环的训练，在测试集上看看模型性能。如果性能比上一次更好，就保存模型，否则就退出，因为算法开始发散了。</p><p>模型训练完毕检查性能之后，如果模型可行，下一步就将所有数据用于训练，也即运行以下命令<code>python train_cnn.py --just_train True</code>。这个过程会迭代固定的20个大循环。训练完毕之后，下面的预测过程将使用这个模型。</p><h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><p>预测涉及到的文件<code>predict_cnn.py</code>以及<code>utils.py</code></p><p>预测的流程和训练差不多，只不过不再进行多次对数据集的遍历，只进行对未标记数据进行一次遍历，拿到结果之后，由于算法输出的结果是[0, 32]这样一个序号，我们需要转化为中文标签。</p><p>具体参照代码，不再赘述。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>【1】<a href="https://zhuanlan.zhihu.com/p/25928551" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25928551</a></p><p>【2】<a href="https://github.com/brightmart/text_classification" target="_blank" rel="external">https://github.com/brightmart/text_classification</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文档是&lt;a href=&quot;https://github.com/ewanlee/video_title_classification&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github项目&lt;/a&gt;的流程解释文档，具体实现请移步。&lt;/p&gt;&lt;p&gt;本项目
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>BFG Repo-Cleaner</title>
    <link href="http://yoursite.com/2017/11/29/BFG-Repo-Cleaner/"/>
    <id>http://yoursite.com/2017/11/29/BFG-Repo-Cleaner/</id>
    <published>2017-11-29T11:31:19.000Z</published>
    <updated>2017-11-29T11:32:30.550Z</updated>
    
    <content type="html"><![CDATA[<h1 id="An-alternative-to-git-filter-branch"><a href="#An-alternative-to-git-filter-branch" class="headerlink" title="An alternative to git-filter-branch"></a>An alternative to git-filter-branch</h1><p>The BFG is a simpler, faster alternative to <a href="http://git-scm.com/docs/git-filter-branch" target="_blank" rel="external"><code>git-filter-branch</code></a> for cleansing bad data out of your Git repository history:</p><ul><li>Removing <strong>Crazy Big Files</strong></li><li>Removing <strong>Passwords</strong>, <strong>Credentials</strong> &amp; other <strong>Private data</strong></li></ul><p>The <code>git-filter-branch</code> command is enormously powerful and can do things that the BFG can’t - but the BFG is <em>much</em> better for the tasks above, because:</p><ul><li><a href="https://rtyley.github.io/bfg-repo-cleaner/#speed" target="_blank" rel="external">Faster</a> : <strong>10 - 720x</strong> faster</li><li><a href="https://rtyley.github.io/bfg-repo-cleaner/#examples" target="_blank" rel="external">Simpler</a> : The BFG isn’t particularily clever, but <em>is</em> focused on making the above tasks easy</li><li>Beautiful : If you need to, you can use the beautiful Scala language to customise the BFG. Which has got to be better than Bash scripting at least some of the time.</li></ul><h1 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h1><p>First clone a fresh copy of your repo, using the <a href="http://stackoverflow.com/q/3959924/438886" target="_blank" rel="external"><code>--mirror</code></a> flag:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git clone --mirror git://example.com/some-big-repo.git</div></pre></td></tr></table></figure><p>This is a <a href="http://git-scm.com/docs/gitglossary.html#def_bare_repository" target="_blank" rel="external">bare</a> repo, which means your normal files won’t be visible, but it is a <em>full</em> copy of the Git database of your repository, and at this point you should <strong>make a backup of it</strong> to ensure you don’t lose anything.</p><p>Now you can run the BFG to clean your repository up:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -jar bfg.jar --strip-blobs-bigger-than 100M some-big-repo.git</div></pre></td></tr></table></figure><p>The BFG will update your commits and all branches and tags so they are clean, but it doesn’t physically delete the unwanted stuff. Examine the repo to make sure your history has been updated, and then use the standard <a href="http://git-scm.com/docs/git-gc" target="_blank" rel="external"><code>git gc</code></a> command to strip out the unwanted dirty data, which Git will now recognise as surplus to requirements:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ cd some-big-repo.git</div><div class="line">$ git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive</div></pre></td></tr></table></figure><p>Finally, once you’re happy with the updated state of your repo, push it back up <em>(note that because your clone command used the –mirror flag, this push will update *<em>all*</em> refs on your remote server)</em>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git push</div></pre></td></tr></table></figure><p>At this point, you’re ready for everyone to ditch their old copies of the repo and do fresh clones of the nice, new pristine data. It’s best to delete all old clones, as they’ll have dirty history that you <em>don’t</em> want to risk pushing back into your newly cleaned repo.</p><h1 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h1><p>In all these examples <code>bfg</code> is an alias for <code>java -jar bfg.jar</code>.</p><p>Delete all files named ‘id_rsa’ or ‘id_dsa’ :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --delete-files id_&#123;dsa,rsa&#125;  my-repo.git</div></pre></td></tr></table></figure><p>Remove all blobs bigger than 50 megabytes :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --strip-blobs-bigger-than 50M  my-repo.git</div></pre></td></tr></table></figure><p>Replace all passwords listed in a file <em>(prefix lines ‘regex:’ or ‘glob:’ if required)</em> with <code>***REMOVED***</code>wherever they occur in your repository :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --replace-text passwords.txt  my-repo.git</div></pre></td></tr></table></figure><p>Remove all folders or files named ‘.git’ - a <a href="https://github.com/git/git/blob/d29e9c89d/fsck.c#L228-L229" target="_blank" rel="external">reserved filename</a> in Git. These often <a href="http://stackoverflow.com/q/16821649/438886" target="_blank" rel="external">become a problem</a>when migrating to Git from other source-control systems like Mercurial :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --delete-folders .git --delete-files .git  --no-blob-protection  my-repo.git</div></pre></td></tr></table></figure><p>For further command-line options, you can run the BFG without any arguments, which will output <a href="https://repository.sonatype.org/service/local/artifact/maven/redirect?r=central-proxy&amp;g=com.madgag&amp;a=bfg&amp;v=LATEST&amp;e=txt" target="_blank" rel="external">text like this</a>.</p><h1 id="Your-current-files-are-sacred…"><a href="#Your-current-files-are-sacred…" class="headerlink" title="Your current files are sacred…"></a>Your <em>current</em> files are sacred…</h1><p>The BFG treats you like a reformed alcoholic: you’ve made some mistakes in the past, but now you’ve cleaned up your act. Thus the BFG assumes that your latest commit is a <em>good</em> one, with none of the dirty files you want removing from your history still in it. This assumption by the BFG protects your work, and gives you peace of mind knowing that the BFG is <em>only</em> changing your repo history, not meddling with the <em>current</em> files of your project.</p><p>By default the <code>HEAD</code> branch is protected, and while its history will be cleaned, the very latest commit (the ‘tip’) is a <strong>protected commit</strong> and its file-hierarchy won’t be changed at all.</p><p>If you want to protect the tips of several branches or tags (not just HEAD), just name them for the BFG:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --strip-biggest-blobs 100 --protect-blobs-from master,maint,next repo.git</div></pre></td></tr></table></figure><p>Note:</p><ul><li>Cleaning Git repos is about <em>completely</em> eradicating bad stuff from history. If something ‘bad’ (like a 10MB file, when you’re specifying <code>--strip-blobs-bigger-than 5M</code>) is in a protected commit, it <em>won’t</em> be deleted - it’ll persist in your repository, <a href="https://github.com/rtyley/bfg-repo-cleaner/issues/53#issuecomment-50088997" target="_blank" rel="external">even if the BFG deletes if from earlier commits</a>. If you want the BFG to delete something <strong>you need to make sure your current commits are clean</strong>.</li><li>Note that although the files in those protected commits won’t be changed, when those commits follow on from earlier dirty commits, their commit ids <strong>will</strong> change, to reflect the changed history - only the SHA-1 id of the filesystem-tree will remain the same.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;An-alternative-to-git-filter-branch&quot;&gt;&lt;a href=&quot;#An-alternative-to-git-filter-branch&quot; class=&quot;headerlink&quot; title=&quot;An alternative to git-
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Understanding LSTM Networks [repost]</title>
    <link href="http://yoursite.com/2017/11/28/Understanding-LSTM-Networks-repost/"/>
    <id>http://yoursite.com/2017/11/28/Understanding-LSTM-Networks-repost/</id>
    <published>2017-11-28T07:51:36.000Z</published>
    <updated>2017-11-28T07:53:14.945Z</updated>
    
    <content type="html"><![CDATA[<p>source post is <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">here</a>.</p><h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p>Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.</p><p>Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.</p><p>Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" alt="img"></p><p><strong>Recurrent Neural Networks have loops.</strong></p><p>In the above diagram, a chunk of neural network, AA, looks at some input xtxt and outputs a value htht. A loop allows information to be passed from one step of the network to the next.</p><p>These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" alt="An unrolled recurrent neural network."></p><p><strong>An unrolled recurrent neural network.</strong></p><p>This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.</p><p>And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. But they really are pretty amazing.</p><p>Essential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore.</p><h2 id="The-Problem-of-Long-Term-Dependencies"><a href="#The-Problem-of-Long-Term-Dependencies" class="headerlink" title="The Problem of Long-Term Dependencies"></a>The Problem of Long-Term Dependencies</h2><p>One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends.</p><p>Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the <em>sky</em>,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" alt="img"></p><p>But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent <em>French</em>.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.</p><p>Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" alt="Neural networks struggle with long term dependencies."></p><p>In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="external">Hochreiter (1991) [German]</a> and <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="external">Bengio, et al. (1994)</a>, who found some pretty fundamental reasons why it might be difficult.</p><p>Thankfully, LSTMs don’t have this problem!</p><h2 id="LSTM-Networks"><a href="#LSTM-Networks" class="headerlink" title="LSTM Networks"></a>LSTM Networks</h2><p>Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">Hochreiter &amp; Schmidhuber (1997)</a>, and were refined and popularized by many people in following work.<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1" target="_blank" rel="external">1</a> They work tremendously well on a large variety of problems, and are now widely used.</p><p>LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</p><p>All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p><p><strong>The repeating module in a standard RNN contains a single layer.</strong></p><p>LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.</p><p><strong>The repeating module in an LSTM contains four interacting layers.</strong></p><p>Don’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" alt="img"></p><p>In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.</p><h2 id="The-Core-Idea-Behind-LSTMs"><a href="#The-Core-Idea-Behind-LSTMs" class="headerlink" title="The Core Idea Behind LSTMs"></a>The Core Idea Behind LSTMs</h2><p>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.</p><p>The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" alt="img"></p><p>The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.</p><p>Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" alt="img"></p><p>The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”</p><p>An LSTM has three of these gates, to protect and control the cell state.</p><h2 id="Step-by-Step-LSTM-Walk-Through"><a href="#Step-by-Step-LSTM-Walk-Through" class="headerlink" title="Step-by-Step LSTM Walk Through"></a>Step-by-Step LSTM Walk Through</h2><p>The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1ht−1and xtxt, and outputs a number between 00 and 11 for each number in the cell state Ct−1Ct−1. A 11represents “completely keep this” while a 00 represents “completely get rid of this.”</p><p>Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt="img"></p><p>The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, C~tC~t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.</p><p>In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt="img"></p><p>It’s now time to update the old cell state, Ct−1Ct−1, into the new cell state CtCt. The previous steps already decided what to do, we just need to actually do it.</p><p>We multiply the old state by ftft, forgetting the things we decided to forget earlier. Then we add it∗C~tit∗C~t. This is the new candidate values, scaled by how much we decided to update each state value.</p><p>In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt="img"></p><p>Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanhtanh (to push the values to be between −1−1 and 11) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.</p><p>For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt="img"></p><h2 id="Variants-on-Long-Short-Term-Memory"><a href="#Variants-on-Long-Short-Term-Memory" class="headerlink" title="Variants on Long Short Term Memory"></a>Variants on Long Short Term Memory</h2><p>What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them.</p><p>One popular LSTM variant, introduced by <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="external">Gers &amp; Schmidhuber (2000)</a>, is adding “peephole connections.” This means that we let the gate layers look at the cell state.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png" alt="img"></p><p>The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.</p><p>Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png" alt="img"></p><p>A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">Cho, et al. (2014)</a>. It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png" alt="A gated recurrent unit neural network."></p><p>These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="external">Yao, et al. (2015)</a>. There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="external">Koutnik, et al. (2014)</a>.</p><p>Which of these variants is best? Do the differences matter? <a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">Greff, et al. (2015)</a> do a nice comparison of popular variants, finding that they’re all about the same. <a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="external">Jozefowicz, et al. (2015)</a>tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!</p><p>Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.</p><p>LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, <a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="external">Xu, <em>et al.</em>(2015)</a> do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner…</p><p>Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by <a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="external">Kalchbrenner, <em>et al.</em> (2015)</a> seem extremely promising. Work using RNNs in generative models – such as <a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="external">Gregor, <em>et al.</em> (2015)</a>, <a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="external">Chung, <em>et al.</em> (2015)</a>, or <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="external">Bayer &amp; Osendorfer (2015)</a> – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;source post is &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;Re
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>A Simple Multi-Class Classification Task: Keras and Scikit-Learn</title>
    <link href="http://yoursite.com/2017/11/21/A-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn/"/>
    <id>http://yoursite.com/2017/11/21/A-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn/</id>
    <published>2017-11-21T08:59:17.000Z</published>
    <updated>2017-11-21T09:05:39.613Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Problem-Description"><a href="#1-Problem-Description" class="headerlink" title="1. Problem Description"></a>1. Problem Description</h2><p>In this tutorial, we will use the standard machine learning problem called the <a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="external">iris flowers dataset</a>.</p><p>This dataset is well studied and is a good problem for practicing on neural networks because all of the 4 input variables are numeric and have the same scale in centimeters. Each instance describes the properties of an observed flower measurements and the output variable is specific iris species.</p><p>This is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species. This is an important type of problem on which to practice with neural networks because the three class values require specialized handling.</p><p>The iris flower dataset is a well-studied problem and a such we can <a href="http://www.is.umk.pl/projects/rules.html#Iris" target="_blank" rel="external">expect to achieve a model accuracy</a> in the range of 95% to 97%. This provides a good target to aim for when developing our models.</p><p>You can <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" target="_blank" rel="external">download the iris flowers dataset</a> from the UCI Machine Learning repository and place it in your current working directory with the filename “<em>iris.csv</em>“.</p><p>Need help with Deep Learning in Python?Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with sample code).Click to sign-up now and also get a free PDF Ebook version of the course.<a href="https://machinelearningmastery.leadpages.co/leadbox/142d6e873f72a2%3A164f8be4f346dc/5657382461898752/" target="_blank" rel="external">Start Your FREE Mini-Course Now!</a></p><h2 id="2-Import-Classes-and-Functions"><a href="#2-Import-Classes-and-Functions" class="headerlink" title="2. Import Classes and Functions"></a>2. Import Classes and Functions</h2><p>We can begin by importing all of the classes and functions we will need in this tutorial.</p><p>This includes both the functionality we require from Keras, but also data loading from <a href="http://pandas.pydata.org/" target="_blank" rel="external">pandas</a>as well as data preparation and model evaluation from <a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="keyword">import</span> pandas</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">from</span> keras.wrappers.scikit_learn <span class="keyword">import</span> KerasClassifier</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</div></pre></td></tr></table></figure><h2 id="3-Initialize-Random-Number-Generator"><a href="#3-Initialize-Random-Number-Generator" class="headerlink" title="3. Initialize Random Number Generator"></a>3. Initialize Random Number Generator</h2><p>Next, we need to initialize the random number generator to a constant value (7).</p><p>This is important to ensure that the results we achieve from this model can be achieved again precisely. It ensures that the stochastic process of training a neural network model can be reproduced.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># fix random seed for reproducibility</span></div><div class="line">seed = <span class="number">7</span></div><div class="line">numpy.random.seed(seed)</div></pre></td></tr></table></figure><h2 id="4-Load-The-Dataset"><a href="#4-Load-The-Dataset" class="headerlink" title="4. Load The Dataset"></a>4. Load The Dataset</h2><p>The dataset can be loaded directly. Because the output variable contains strings, it is easiest to load the data using pandas. We can then split the attributes (columns) into input variables (X) and output variables (Y).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># load dataset</span></div><div class="line">dataframe = pandas.read_csv(<span class="string">"iris.csv"</span>, header=<span class="keyword">None</span>)</div><div class="line">dataset = dataframe.values</div><div class="line">X = dataset[:,<span class="number">0</span>:<span class="number">4</span>].astype(float)</div><div class="line">Y = dataset[:,<span class="number">4</span>]</div></pre></td></tr></table></figure><h2 id="5-Encode-The-Output-Variable"><a href="#5-Encode-The-Output-Variable" class="headerlink" title="5. Encode The Output Variable"></a>5. Encode The Output Variable</h2><p>The output variable contains three different string values.</p><p>When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not.</p><p>This is called <a href="https://en.wikipedia.org/wiki/One-hot" target="_blank" rel="external">one hot encoding</a> or creating dummy variables from a categorical variable.</p><p>For example, in this problem three class values are Iris-setosa, Iris-versicolor and Iris-virginica. If we had the observations:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Iris-setosa</div><div class="line">Iris-versicolor</div><div class="line">Iris-virginica</div></pre></td></tr></table></figure><p>We can turn this into a one-hot encoded binary matrix for each data instance that would look as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Iris-setosa,	Iris-versicolor,	Iris-virginica</div><div class="line"><span class="number">1</span>,		<span class="number">0</span>,			<span class="number">0</span></div><div class="line"><span class="number">0</span>,		<span class="number">1</span>, 			<span class="number">0</span></div><div class="line"><span class="number">0</span>, 		<span class="number">0</span>, 			<span class="number">1</span></div></pre></td></tr></table></figure><p>We can do this by first encoding the strings consistently to integers using the scikit-learn class LabelEncoder. Then convert the vector of integers to a one hot encoding using the Keras function to_categorical().</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># encode class values as integers</span></div><div class="line">encoder = LabelEncoder()</div><div class="line">encoder.fit(Y)</div><div class="line">encoded_Y = encoder.transform(Y)</div><div class="line"><span class="comment"># convert integers to dummy variables (i.e. one hot encoded)</span></div><div class="line">dummy_y = np_utils.to_categorical(encoded_Y)</div></pre></td></tr></table></figure><h2 id="6-Define-The-Neural-Network-Model"><a href="#6-Define-The-Neural-Network-Model" class="headerlink" title="6. Define The Neural Network Model"></a>6. Define The Neural Network Model</h2><p>The Keras library provides wrapper classes to allow you to use neural network models developed with Keras in scikit-learn.</p><p>There is a KerasClassifier class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.</p><p>Below is a function that will create a baseline neural network for the iris classification problem. It creates a simple fully connected network with one hidden layer that contains 8 neurons.</p><p>The hidden layer uses a rectifier activation function which is a good practice. Because we used a one-hot encoding for our iris dataset, the output layer must create 3 output values, one for each class. The output value with the largest value will be taken as the class predicted by the model.</p><p>The network topology of this simple one-layer neural network can be summarized as:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">4 inputs -&gt; [8 hidden nodes] -&gt; 3 outputs</div></pre></td></tr></table></figure><p>Note that we use a “<em>softmax</em>” activation function in the output layer. This is to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities.</p><p>Finally, the network uses the efficient Adam gradient descent optimization algorithm with a logarithmic loss function, which is called “<em>categorical_crossentropy</em>” in Keras.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># define baseline model</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_model</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment"># create model</span></div><div class="line">	model = Sequential()</div><div class="line">	model.add(Dense(<span class="number">8</span>, input_dim=<span class="number">4</span>, activation=<span class="string">'relu'</span>))</div><div class="line">	model.add(Dense(<span class="number">3</span>, activation=<span class="string">'softmax'</span>))</div><div class="line">	<span class="comment"># Compile model</span></div><div class="line">	model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">	<span class="keyword">return</span> model</div></pre></td></tr></table></figure><p>We can now create our KerasClassifier for use in scikit-learn.</p><p>We can also pass arguments in the construction of the KerasClassifier class that will be passed on to the fit() function internally used to train the neural network. Here, we pass the number of epochs as 200 and batch size as 5 to use when training the model. Debugging is also turned off when training by setting verbose to 0.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">estimator = KerasClassifier(</div><div class="line">  build_fn=baseline_model, epochs=<span class="number">200</span>, batch_size=<span class="number">5</span>, verbose=<span class="number">0</span>)</div></pre></td></tr></table></figure><h2 id="7-Evaluate-The-Model-with-k-Fold-Cross-Validation"><a href="#7-Evaluate-The-Model-with-k-Fold-Cross-Validation" class="headerlink" title="7. Evaluate The Model with k-Fold Cross Validation"></a>7. Evaluate The Model with k-Fold Cross Validation</h2><p>We can now evaluate the neural network model on our training data.</p><p>The scikit-learn has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating machine learning models is k-fold cross validation.</p><p>First we can define the model evaluation procedure. Here, we set the number of folds to be 10 (an excellent default) and to shuffle the data before partitioning it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kfold = KFold(n_splits=<span class="number">10</span>, shuffle=<span class="keyword">True</span>, random_state=seed)</div></pre></td></tr></table></figure><p>Now we can evaluate our model (estimator) on our dataset (X and dummy_y) using a 10-fold cross-validation procedure (kfold).</p><p>Evaluating the model only takes approximately 10 seconds and returns an object that describes the evaluation of the 10 constructed models for each of the splits of the dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">results = cross_val_score(estimator, X, dummy_y, cv=kfold)</div><div class="line">print(<span class="string">"Baseline: %.2f%% (%.2f%%)"</span> % (results.mean()*<span class="number">100</span>, results.std()*<span class="number">100</span>))</div></pre></td></tr></table></figure><p>The results are summarized as both the mean and standard deviation of the model accuracy on the dataset. This is a reasonable estimation of the performance of the model on unseen data. It is also within the realm of known top results for this problem.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Accuracy: <span class="number">97.33</span>% (<span class="number">4.42</span>%)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Problem-Description&quot;&gt;&lt;a href=&quot;#1-Problem-Description&quot; class=&quot;headerlink&quot; title=&quot;1. Problem Description&quot;&gt;&lt;/a&gt;1. Problem Description
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>The Right Way to Oversample in Predictive Modeling</title>
    <link href="http://yoursite.com/2017/11/21/The-Right-Way-to-Oversample-in-Predictive-Modeling/"/>
    <id>http://yoursite.com/2017/11/21/The-Right-Way-to-Oversample-in-Predictive-Modeling/</id>
    <published>2017-11-21T01:10:22.000Z</published>
    <updated>2017-11-21T08:57:45.929Z</updated>
    
    <content type="html"><![CDATA[<p>The Source Blog: <a href="https://beckernick.github.io/oversampling-modeling/" target="_blank" rel="external">https://beckernick.github.io/oversampling-modeling/</a></p><p>Imbalanced datasets spring up everywhere. Amazon wants to classify fake reviews, banks want to predict fraudulent credit card charges, and, as of this November, Facebook researchers are probably wondering if they can predict which news articles are fake.</p><p>In each of these cases, only a small fraction of observations are actually positives. I’d guess that only 1 in 10,000 credit card charges are fraudulent, at most. Recently, oversampling the minority class observations has become a common approach to improve the quality of predictive modeling. By oversampling, models are sometimes better able to learn patterns that differentiate classes.</p><p>However, this post isn’t about how this can improve modeling. Instead, it’s about how the <strong>*timing*</strong> of oversampling can affect the generalization ability of a model. Since one of the primary goals of model validation is to estimate how it will perform on unseen data, oversampling correctly is critical.</p><h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>I’m going to try to predict whether someone will default on or a creditor will have to charge off a loan, using data from Lending Club. I’ll start by importing some modules and loading the data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</div><div class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">loans = pd.read_csv(<span class="string">'../lending-club-data.csv.zip'</span>)</div><div class="line">loans.iloc[<span class="number">0</span>]</div></pre></td></tr></table></figure><p>There’s a lot of cool person and loan-specific information in this dataset. The target variable is <code>bad_loans</code>, which is 1 if the loan was charged off or the lessee defaulted, and 0 otherwise. I know this dataset should be imbalanced (most loans are paid off), but how imbalanced is it?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loans.bad_loans.value_counts()</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">0</span>    <span class="number">99457</span></div><div class="line"><span class="number">1</span>    <span class="number">23150</span></div><div class="line">Name: bad_loans, dtype: int64</div></pre></td></tr></table></figure><p>Charge offs occurred or people defaulted on about 19% of loans, so there’s some imbalance in the data but it’s not terrible. I’ll remove a few observations with missing values for a payment-to-income ratio and then pick a handful of features to use in a random forest model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loans = loans[~loans.payment_inc_ratio.isnull()]</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">model_variables = [<span class="string">'grade'</span>, <span class="string">'home_ownership'</span>,<span class="string">'emp_length_num'</span>, <span class="string">'sub_grade'</span>,<span class="string">'short_emp'</span>, <span class="string">'dti'</span>, <span class="string">'term'</span>, <span class="string">'purpose'</span>, <span class="string">'int_rate'</span>, <span class="string">'last_delinq_none'</span>, <span class="string">'last_major_derog_none'</span>, <span class="string">'revol_util'</span>, <span class="string">'total_rec_late_fee'</span>, <span class="string">'payment_inc_ratio'</span>, <span class="string">'bad_loans'</span>]</div><div class="line"></div><div class="line">loans_data_relevent = loans[model_variables]</div></pre></td></tr></table></figure><p>Next, I need to one-hot encode the categorical features as binary variables to use them in sklearn’s random forest classifier.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loans_relevant_enconded = pd.get_dummies(loans_data_relevent)</div></pre></td></tr></table></figure><h1 id="Creating-the-Training-and-Test-Sets"><a href="#Creating-the-Training-and-Test-Sets" class="headerlink" title="Creating the Training and Test Sets"></a>Creating the Training and Test Sets</h1><p>With the data prepared, I can create a training dataset and a test dataset. I’ll use the training dataset to build and validate the model, and treat the test dataset as the unseen new data I’d see if the model were in production.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">training_features, test_features, \</div><div class="line">training_target, test_target, = train_test_split(loans_relevant_enconded.drop([<span class="string">'bad_loans'</span>], axis=<span class="number">1</span>),                        loans_relevant_enconded[<span class="string">'bad_loans'</span>],</div><div class="line">			    test_size = <span class="number">.1</span>,</div><div class="line">			    random_state=<span class="number">12</span>)</div></pre></td></tr></table></figure><h1 id="The-Wrong-Way-to-Oversample"><a href="#The-Wrong-Way-to-Oversample" class="headerlink" title="The Wrong Way to Oversample"></a>The Wrong Way to Oversample</h1><p>With my training data created, I’ll upsample the bad loans using the <a href="https://www.jair.org/media/953/live-953-2037-jair.pdf" target="_blank" rel="external">SMOTE algorithm</a> (Synthetic Minority Oversampling Technique). At a high level, SMOTE creates synthetic observations of the minority class (bad loans) by:</p><ol><li>Finding the k-nearest-neighbors for minority class observations (finding similar observations)</li><li>Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.</li></ol><p>After upsampling to a class ratio of 1.0, I should have a balanced dataset. There’s no need (and often it’s not smart) to balance the classes, but it magnifies the issue caused by incorrectly timed oversampling.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sm = SMOTE(random_state=<span class="number">12</span>, ratio = <span class="number">1.0</span>)</div><div class="line">x_res, y_res = sm.fit_sample(training_features, training_target)</div><div class="line"><span class="keyword">print</span> training_target.value_counts(), np.bincount(y_res)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">0</span>    <span class="number">89493</span></div><div class="line"><span class="number">1</span>    <span class="number">20849</span></div><div class="line">Name: bad_loans, dtype: int64 [<span class="number">89493</span> <span class="number">89493</span>]</div></pre></td></tr></table></figure><p>After upsampling, I’ll split the data into separate training and validation sets and build a random forest model to classify the bad loans.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res,</div><div class="line">                                                    y_res,</div><div class="line">                                                    test_size = <span class="number">.1</span>,</div><div class="line">                                                    random_state=<span class="number">12</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">clf_rf = RandomForestClassifier(n_estimators=<span class="number">25</span>, random_state=<span class="number">12</span>)</div><div class="line">clf_rf.fit(x_train_res, y_train_res)</div><div class="line">clf_rf.score(x_val_res, y_val_res)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">0.88468629532376108</span></div></pre></td></tr></table></figure><p>88% accuracy looks good, but I’m not just interested in accuracy. I also want to know how well I can specifically classify bad loans, since they’re more important. In statistics, this is called <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" target="_blank" rel="external">recall</a>, and it’s the number of correctly predicted “positives” divided by the total number of “positives”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">recall_score(y_val_res, clf_rf.predict(x_val_res))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">0.81192097332291546</span></div></pre></td></tr></table></figure><p>81% recall. That means the model correctly identified 81% of the total bad loans. That’s pretty great. But is this actually representative of how the model will perform? To find out, I’ll calculate the accuracy and recall for the model on the test dataset I created initially.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> clf_rf.score(test_features, test_target)</div><div class="line"><span class="keyword">print</span> recall_score(test_target, clf_rf.predict(test_features))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">0.801973737868</span></div><div class="line"><span class="number">0.129943502825</span></div></pre></td></tr></table></figure><p>Only 80% accuracy and 13% recall on the test data. That’s a <strong>huge</strong> difference!</p><h1 id="What-Happened"><a href="#What-Happened" class="headerlink" title="What Happened?"></a>What Happened?</h1><p>By oversampling before splitting into training and validation datasets, I “bleed” information from the validation set into the training of the model.</p><p>To see how this works, think about the case of simple oversampling (where I just duplicate observations). If I upsample a dataset before splitting it into a train and validation set, I could end up with the same observation in both datasets. As a result, a complex enough model will be able to perfectly predict the value for those observations when predicting on the validation set, inflating the accuracy and recall.</p><p>When upsampling using SMOTE, I don’t create duplicate observations. However, because the SMOTE algorithm uses the nearest neighbors of observations to create synthetic data, it still bleeds information. If the nearest neighbors of minority class observations in the training set end up in the validation set, their information is partially captured by the synthetic data in the training set. Since I’m splitting the data randomly, we’d expect to have this happen. As a result, the model will be better able to predict validation set values than completely new data.</p><h1 id="The-Right-Way-to-Oversample"><a href="#The-Right-Way-to-Oversample" class="headerlink" title="The Right Way to Oversample"></a>The Right Way to Oversample</h1><p>Okay, so I’ve gone through the wrong way to oversample. Now I’ll go through the right way: oversampling on only the training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x_train, x_val, y_train, y_val = \</div><div class="line">			train_test_split(training_features, 											    training_target,</div><div class="line">						    test_size = <span class="number">.1</span>,</div><div class="line">                              random_state=<span class="number">12</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sm = SMOTE(random_state=<span class="number">12</span>, ratio = <span class="number">1.0</span>)</div><div class="line">x_train_res, y_train_res = sm.fit_sample(x_train, y_train)</div></pre></td></tr></table></figure><p>By oversampling only on the training data, none of the information in the validation data is being used to create synthetic observations. So these results should be generalizable. Let’s see if that’s true.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">clf_rf = RandomForestClassifier(n_estimators=<span class="number">25</span>, random_state=<span class="number">12</span>)</div><div class="line">clf_rf.fit(x_train_res, y_train_res)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> <span class="string">'Validation Results'</span></div><div class="line"><span class="keyword">print</span> clf_rf.score(x_val, y_val)</div><div class="line"><span class="keyword">print</span> recall_score(y_val, clf_rf.predict(x_val))</div><div class="line"><span class="keyword">print</span> <span class="string">'\nTest Results'</span></div><div class="line"><span class="keyword">print</span> clf_rf.score(test_features, test_target)</div><div class="line"><span class="keyword">print</span> recall_score(test_target, clf_rf.predict(test_features))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Validation Results</div><div class="line"><span class="number">0.800362483009</span></div><div class="line"><span class="number">0.138195777351</span></div><div class="line"></div><div class="line">Test Results</div><div class="line"><span class="number">0.803278688525</span></div><div class="line"><span class="number">0.142546718818</span></div></pre></td></tr></table></figure><p>The validation results closely match the unseen test data results, which is exactly what I would want to see after putting a model into production.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Oversampling is a well-known way to potentially improve models trained on imbalanced data. But it’s important to remember that oversampling incorrectly can lead to thinking a model will generalize better than it actually does. Random forests are great because the model architecture reduces overfitting (see <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf" target="_blank" rel="external">Brieman 2001</a> for a proof), but poor sampling practices can still lead to false conclusions about the quality of a model.</p><p>When the model is in production, it’s predicting on unseen data. The main point of model validation is to estimate how the model will generalize to new data. If the decision to put a model into production is based on how it performs on a validation set, it’s critical that oversampling is done correctly.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The Source Blog: &lt;a href=&quot;https://beckernick.github.io/oversampling-modeling/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://beckernick.github.i
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Note of the DenseNet (contains TensorFlow and PyTorch Implementation)</title>
    <link href="http://yoursite.com/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/"/>
    <id>http://yoursite.com/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/</id>
    <published>2017-11-20T04:15:43.000Z</published>
    <updated>2017-11-20T05:23:15.767Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>The blog source:</strong></p><p><strong><a href="https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504" target="_blank" rel="external">https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504</a>.</strong></p><p>I have added the PyTorch implementation from</p><p><strong><a href="https://github.com/gpleiss/efficient_densenet_pytorch" target="_blank" rel="external">https://github.com/gpleiss/efficient_densenet_pytorch</a>.</strong></p></blockquote><p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">DenseNet</a>(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. It’s quite similar to <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet</a> but has some fundamental differences.</p><p>With all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*7WdURialIGTojNI9ltrplA.png" alt="img"></p><p><em>Error rates on various datasets(from source paper)</em></p><p>And for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*N_o10WxGx_e6vBSNnV5k1A.png" alt="img"></p><p><em>Сomparison of the DenseNet and ResNet Top-1 error rates on the ImageNet classification dataset as a function of learned parameters (left) and flops during test-time (right)(from source paper).</em></p><p>This post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:</p><ul><li>Why dense net differs from another convolution networks.</li><li>What difficulties I’ve met during the implementation of DenseNet in tensorflow.</li></ul><p>If you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the <a href="https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#aabd" target="_blank" rel="external">second chapter</a> or check the <a href="https://github.com/ikhlestov/vision_networks" target="_blank" rel="external">source code on GitHub</a>. If you not familiar with any topics but want to get some knowledge — I highly advise you <a href="http://cs231n.github.io/" target="_blank" rel="external">CS231n Stanford classes</a>.</p><h4 id="Compare-DenseNet-with-other-Convolution-Networks"><a href="#Compare-DenseNet-with-other-Convolution-Networks" class="headerlink" title="Compare DenseNet with other Convolution Networks"></a>Compare DenseNet with other Convolution Networks</h4><p>Usually, ConvNets work such way:<br>We have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.<br>So the output from the Lᵢ layer is input to the Lᵢ₊₁ layer. It seems like this:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*1F8wom5X1DeCj5BWeGfJbw.jpeg" alt="img"></p><p><em>source: &lt;<a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">http://cs231n.github.io/convolutional-networks/</a></em>&gt;</p><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet</a> architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the Lᵢ layer was obtained by summation of outputs from previous layers.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-C2QoqhfCjG8xZRu-UwO_Q.png" alt="img"></p><p>In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.<br>So, let’s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.<br>The output from Lᵢ layer will be (28, 28, 12).<br>But input to the Lᵢ₊₁ will be (28, 28, 24+12), for Lᵢ₊₂ (28, 28, 24 + 12 + 12) and so on.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*6JoB4BeIZyWDNGH5U63y_Q.png" alt="img"></p><p><em>Block of convolution layers with results concatenated</em></p><p>After a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).<br>All these N layers are named Block in the paper. There’s also batch normalization, nonlinearity and dropout inside the block.<br>To reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-CQyW4huxxxMYpffb72emg.png" alt="img"></p><p><em>Transition layer</em></p><p>Now we can again pass the image through the block with N convolutions.<br>With this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.<br>Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*SSn5H14SKhhaZZ5XYWN3Cg.jpeg" alt="img"></p><p><em>Full DenseNet example with 3 blocks from source paper</em></p><h4 id="Notes-about-implementation"><a href="#Notes-about-implementation" class="headerlink" title="Notes about implementation"></a>Notes about implementation</h4><p>In the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.</p><p>First of all, it was not clear how many blocks should be used depends on depth. After I’ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.</p><p>Second I’ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .</p><p>Despite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> block <span class="keyword">in</span> range(required_blocks):</div><div class="line">    output = build_block(output)</div><div class="line">    <span class="keyword">if</span> block != (required_blocks — <span class="number">1</span>):</div><div class="line">        output = transition_layer(output)</div></pre></td></tr></table></figure><p>For weights initialization authors proposed use MRSA initialization(as per<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="external">this paper</a>). In tensorflow this initialization can be easy implemented with<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer" target="_blank" rel="external">variance scaling initializer</a>.</p><p>In the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">batch norm -&gt; relu -&gt; conv 1x1 -&gt; dropout -&gt; batch norm -&gt; relu -&gt; conv 3x3 -&gt; dropout -&gt; output.</div></pre></td></tr></table></figure><p>Despite two conv filters, only last output will be concatenated to the main pool of features.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*FvW30tjOQ2hDQ7LdEkCRLQ.png" alt="img"></p><p>Also at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*N-nU5WXQcrNfhnQMwxquFA.png" alt="img"></p><p>Where theta — some reduction values, in the range (0, 1).</p><p>In case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.</p><p>Last, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used — just divide every image pixel by 255, so we have pixels values in the range [0, 1].</p><p>At first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next I’ve implemented per channel normalization… And networks began works even worse. It was not clear for me why. So I’ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.</p><p>And some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># without this line next slice assignment will silently fail!</span></div><div class="line"><span class="comment"># at least in numpy 1.12.0</span></div><div class="line">images = images.astype(‘float64’)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(channels):</div><div class="line">    images[:, :, :, i] = (</div><div class="line">        (images[:, :, :, i] — self.images_means[i]) /</div><div class="line">         self.images_stds[i])</div></pre></td></tr></table></figure><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>DenseNets are powerful neural nets that achieve state of the art performance on many datasets. And it’s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.</p><a id="more"></a><h4 id="Appendix-PyTorch-Implementation-naive-version-100-lines"><a href="#Appendix-PyTorch-Implementation-naive-version-100-lines" class="headerlink" title="Appendix: PyTorch Implementation (naive version ~100 lines)"></a>Appendix: PyTorch Implementation (naive version ~100 lines)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># This implementation is based on the DenseNet-BC implementation in torchvision</span></div><div class="line"><span class="comment"># https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DenseLayer</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_features, growth_rate, bn_size, drop_rate)</span>:</span></div><div class="line">        super(_DenseLayer, self).__init__()</div><div class="line">        self.add_module(<span class="string">'norm.1'</span>, nn.BatchNorm2d(num_input_features)),</div><div class="line">        self.add_module(<span class="string">'relu.1'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">        self.add_module(<span class="string">'conv.1'</span>, nn.Conv2d(num_input_features, bn_size *</div><div class="line">                        growth_rate, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        self.add_module(<span class="string">'norm.2'</span>, nn.BatchNorm2d(bn_size * growth_rate)),</div><div class="line">        self.add_module(<span class="string">'relu.2'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">        self.add_module(<span class="string">'conv.2'</span>, nn.Conv2d(bn_size * growth_rate, growth_rate,</div><div class="line">                        kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        self.drop_rate = drop_rate</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        new_features = super(_DenseLayer, self).forward(x)</div><div class="line">        <span class="keyword">if</span> self.drop_rate &gt; <span class="number">0</span>:</div><div class="line">            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)</div><div class="line">        <span class="keyword">return</span> torch.cat([x, new_features], <span class="number">1</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_Transition</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_features, num_output_features)</span>:</span></div><div class="line">        super(_Transition, self).__init__()</div><div class="line">        self.add_module(<span class="string">'norm'</span>, nn.BatchNorm2d(num_input_features))</div><div class="line">        self.add_module(<span class="string">'relu'</span>, nn.ReLU(inplace=<span class="keyword">True</span>))</div><div class="line">        self.add_module(<span class="string">'conv'</span>, nn.Conv2d(num_input_features, num_output_features,</div><div class="line">                                          kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>))</div><div class="line">        self.add_module(<span class="string">'pool'</span>, nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DenseBlock</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate)</span>:</span></div><div class="line">        super(_DenseBlock, self).__init__()</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</div><div class="line">            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)</div><div class="line">            self.add_module(<span class="string">'denselayer%d'</span> % (i + <span class="number">1</span>), layer)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseNet</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">r"""Densenet-BC model class, based on</span></div><div class="line">    `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`</div><div class="line">    Args:</div><div class="line">        growth_rate (int) - how many filters to add each layer (`k` in paper)</div><div class="line">        block_config (list of 3 or 4 ints) - how many layers in each pooling block</div><div class="line">        num_init_features (int) - the number of filters to learn in the first convolution layer</div><div class="line">        bn_size (int) - multiplicative factor for number of bottle neck layers</div><div class="line">            (i.e. bn_size * k features in the bottleneck layer)</div><div class="line">        drop_rate (float) - dropout rate after each dense layer</div><div class="line">        num_classes (int) - number of classification classes</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, growth_rate=<span class="number">12</span>, block_config=<span class="params">(<span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>)</span>, compression=<span class="number">0.5</span>,</span></span></div><div class="line">                 num_init_features=<span class="number">24</span>, bn_size=<span class="number">4</span>, drop_rate=<span class="number">0</span>, avgpool_size=<span class="number">8</span>,</div><div class="line">                 num_classes=<span class="number">10</span>):</div><div class="line"></div><div class="line">        super(DenseNet, self).__init__()</div><div class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt; compression &lt;= <span class="number">1</span>, <span class="string">'compression of densenet should be between 0 and 1'</span></div><div class="line">        self.avgpool_size = avgpool_size</div><div class="line"></div><div class="line">        <span class="comment"># First convolution</span></div><div class="line">        self.features = nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'conv0'</span>, nn.Conv2d(<span class="number">3</span>, num_init_features, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">        <span class="comment"># Each denseblock</span></div><div class="line">        num_features = num_init_features</div><div class="line">        <span class="keyword">for</span> i, num_layers <span class="keyword">in</span> enumerate(block_config):</div><div class="line">            block = _DenseBlock(num_layers=num_layers,</div><div class="line">                                num_input_features=num_features,</div><div class="line">                                bn_size=bn_size, growth_rate=growth_rate,</div><div class="line">                                drop_rate=drop_rate)</div><div class="line">            self.features.add_module(<span class="string">'denseblock%d'</span> % (i + <span class="number">1</span>), block)</div><div class="line">            num_features = num_features + num_layers * growth_rate</div><div class="line">            <span class="keyword">if</span> i != len(block_config) - <span class="number">1</span>:</div><div class="line">                trans = _Transition(num_input_features=num_features,</div><div class="line">                                    num_output_features=int(num_features</div><div class="line">                                                            * compression))</div><div class="line">                self.features.add_module(<span class="string">'transition%d'</span> % (i + <span class="number">1</span>), trans)</div><div class="line">                num_features = int(num_features * compression)</div><div class="line"></div><div class="line">        <span class="comment"># Final batch norm</span></div><div class="line">        self.features.add_module(<span class="string">'norm_final'</span>, nn.BatchNorm2d(num_features))</div><div class="line"></div><div class="line">        <span class="comment"># Linear layer</span></div><div class="line">        self.classifier = nn.Linear(num_features, num_classes)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        features = self.features(x)</div><div class="line">        out = F.relu(features, inplace=<span class="keyword">True</span>)</div><div class="line">        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(</div><div class="line">                           features.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">        out = self.classifier(out)</div><div class="line">        <span class="keyword">return</span> out</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;The blog source:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504&quot;&gt;https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;I have added the PyTorch implementation from&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/gpleiss/efficient_densenet_pytorch&quot;&gt;https://github.com/gpleiss/efficient_densenet_pytorch&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;DenseNet&lt;/a&gt;(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. It’s quite similar to &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt; but has some fundamental differences.&lt;/p&gt;&lt;p&gt;With all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*7WdURialIGTojNI9ltrplA.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Error rates on various datasets(from source paper)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;And for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*N_o10WxGx_e6vBSNnV5k1A.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Сomparison of the DenseNet and ResNet Top-1 error rates on the ImageNet classification dataset as a function of learned parameters (left) and flops during test-time (right)(from source paper).&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Why dense net differs from another convolution networks.&lt;/li&gt;&lt;li&gt;What difficulties I’ve met during the implementation of DenseNet in tensorflow.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the &lt;a href=&quot;https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#aabd&quot;&gt;second chapter&lt;/a&gt; or check the &lt;a href=&quot;https://github.com/ikhlestov/vision_networks&quot;&gt;source code on GitHub&lt;/a&gt;. If you not familiar with any topics but want to get some knowledge — I highly advise you &lt;a href=&quot;http://cs231n.github.io/&quot;&gt;CS231n Stanford classes&lt;/a&gt;.&lt;/p&gt;&lt;h4 id=&quot;Compare-DenseNet-with-other-Convolution-Networks&quot;&gt;&lt;a href=&quot;#Compare-DenseNet-with-other-Convolution-Networks&quot; class=&quot;headerlink&quot; title=&quot;Compare DenseNet with other Convolution Networks&quot;&gt;&lt;/a&gt;Compare DenseNet with other Convolution Networks&lt;/h4&gt;&lt;p&gt;Usually, ConvNets work such way:&lt;br&gt;We have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.&lt;br&gt;So the output from the Lᵢ layer is input to the Lᵢ₊₁ layer. It seems like this:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*1F8wom5X1DeCj5BWeGfJbw.jpeg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;source: &amp;lt;&lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;http://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/em&gt;&amp;gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt; architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the Lᵢ layer was obtained by summation of outputs from previous layers.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*-C2QoqhfCjG8xZRu-UwO_Q.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.&lt;br&gt;So, let’s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.&lt;br&gt;The output from Lᵢ layer will be (28, 28, 12).&lt;br&gt;But input to the Lᵢ₊₁ will be (28, 28, 24+12), for Lᵢ₊₂ (28, 28, 24 + 12 + 12) and so on.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*6JoB4BeIZyWDNGH5U63y_Q.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Block of convolution layers with results concatenated&lt;/em&gt;&lt;/p&gt;&lt;p&gt;After a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).&lt;br&gt;All these N layers are named Block in the paper. There’s also batch normalization, nonlinearity and dropout inside the block.&lt;br&gt;To reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*-CQyW4huxxxMYpffb72emg.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Transition layer&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Now we can again pass the image through the block with N convolutions.&lt;br&gt;With this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.&lt;br&gt;Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*SSn5H14SKhhaZZ5XYWN3Cg.jpeg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Full DenseNet example with 3 blocks from source paper&lt;/em&gt;&lt;/p&gt;&lt;h4 id=&quot;Notes-about-implementation&quot;&gt;&lt;a href=&quot;#Notes-about-implementation&quot; class=&quot;headerlink&quot; title=&quot;Notes about implementation&quot;&gt;&lt;/a&gt;Notes about implementation&lt;/h4&gt;&lt;p&gt;In the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.&lt;/p&gt;&lt;p&gt;First of all, it was not clear how many blocks should be used depends on depth. After I’ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.&lt;/p&gt;&lt;p&gt;Second I’ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .&lt;/p&gt;&lt;p&gt;Despite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; block &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(required_blocks):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    output = build_block(output)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; block != (required_blocks — &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        output = transition_layer(output)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;For weights initialization authors proposed use MRSA initialization(as per&lt;a href=&quot;https://arxiv.org/abs/1502.01852&quot;&gt;this paper&lt;/a&gt;). In tensorflow this initialization can be easy implemented with&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer&quot;&gt;variance scaling initializer&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;batch norm -&amp;gt; relu -&amp;gt; conv 1x1 -&amp;gt; dropout -&amp;gt; batch norm -&amp;gt; relu -&amp;gt; conv 3x3 -&amp;gt; dropout -&amp;gt; output.&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;Despite two conv filters, only last output will be concatenated to the main pool of features.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*FvW30tjOQ2hDQ7LdEkCRLQ.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;Also at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*N-nU5WXQcrNfhnQMwxquFA.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;Where theta — some reduction values, in the range (0, 1).&lt;/p&gt;&lt;p&gt;In case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.&lt;/p&gt;&lt;p&gt;Last, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used — just divide every image pixel by 255, so we have pixels values in the range [0, 1].&lt;/p&gt;&lt;p&gt;At first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next I’ve implemented per channel normalization… And networks began works even worse. It was not clear for me why. So I’ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.&lt;/p&gt;&lt;p&gt;And some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# without this line next slice assignment will silently fail!&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# at least in numpy 1.12.0&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;images = images.astype(‘float64’)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(channels):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    images[:, :, :, i] = (&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        (images[:, :, :, i] — self.images_means[i]) /&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;         self.images_stds[i])&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h4 id=&quot;Conclusion&quot;&gt;&lt;a href=&quot;#Conclusion&quot; class=&quot;headerlink&quot; title=&quot;Conclusion&quot;&gt;&lt;/a&gt;Conclusion&lt;/h4&gt;&lt;p&gt;DenseNets are powerful neural nets that achieve state of the art performance on many datasets. And it’s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec: The Skip-Gram Model</title>
    <link href="http://yoursite.com/2017/11/14/Word2Vec-The-Skip-Gram-Model/"/>
    <id>http://yoursite.com/2017/11/14/Word2Vec-The-Skip-Gram-Model/</id>
    <published>2017-11-14T13:41:00.000Z</published>
    <updated>2017-11-15T03:27:02.749Z</updated>
    
    <content type="html"><![CDATA[<p>This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual introductory and abstract insights about Word2Vec, and get into more of the details. Specifically here I’m diving into the skip gram neural network model.</p><h1 id="The-Model"><a href="#The-Model" class="headerlink" title="The Model"></a>The Model</h1><p>The skip-gram neural network model is actually surprisingly simple in its most basic form; I think it’s the all the little tweaks and enhancements that start to clutter the explanation.</p><p>Let’s start with a high-level insight about where we’re going. Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.</p><p>Another place you may have seen this trick is in unsupervised feature learning, where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer–it’s a trick for learning good image features without having labeled training data.</p><h1 id="The-Fake-Task"><a href="#The-Fake-Task" class="headerlink" title="The Fake Task"></a>The Fake Task</h1><p>So now we need to talk about this “fake” task that we’re going to build the neural network to perform, and then we’ll come back later to how this indirectly gives us those word vectors that we are really after.</p><p>We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.</p><p>When I say “nearby”, there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).</p><p>The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.</p><p>We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.</p><p><a href="http://mccormickml.com/assets/word2vec/training_data.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/training_data.png" alt="Training Data"></a></p><p>The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”.</p><h1 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h1><p>So how is this all represented?</p><p>First of all, you know you can’t feed a word just as a text string to a neural network, so we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words.</p><p>We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.</p><p>The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.</p><p>Here’s the architecture of our neural network.</p><p><a href="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png" alt="Skip-gram Neural Network Architecture"></a></p><p>There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.</p><p>When <em>training</em> this network on word pairs, the input is a one-hot vector representing the input word and the training output <em>is also a one-hot vector</em>representing the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, <em>not</em> a one-hot vector).</p><h1 id="The-Hidden-Layer"><a href="#The-Hidden-Layer" class="headerlink" title="The Hidden Layer"></a>The Hidden Layer</h1><p>For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).</p><p>300 features is what Google used in their published model trained on the Google news dataset (you can download it from <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="external">here</a>). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results).</p><p>If you look at the <em>rows</em> of this weight matrix, these are actually what will be our word vectors!</p><p><a href="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png" alt="Hidden Layer Weight Matrix"></a></p><p>So the end goal of all of this is really just to learn this hidden layer weight matrix – the output layer we’ll just toss when we’re done!</p><p>Let’s get back, though, to working through the definition of this model that we’re going to train.</p><p>Now, you might be asking yourself–“That one-hot vector is almost all zeros… what’s the effect of that?” If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just <em>select</em> the matrix row corresponding to the “1”. Here’s a small example to give you a visual.</p><p><a href="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png" alt="Effect of matrix multiplication with a one-hot vector"></a></p><p>This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word.</p><h1 id="The-Output-Layer"><a href="#The-Output-Layer" class="headerlink" title="The Output Layer"></a>The Output Layer</h1><p>The <code>1 x 300</code> word vector for “ants” then gets fed to the output layer. The output layer is a softmax regression classifier. There’s an in-depth tutorial on Softmax Regression <a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/" target="_blank" rel="external">here</a>, but the gist of it is that each output neuron (one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.</p><p>Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function <code>exp(x)</code> to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from <em>all</em> 10,000 output nodes.</p><p>Here’s an illustration of calculating the output of the output neuron for the word “car”.</p><p><a href="http://mccormickml.com/assets/word2vec/output_weights_function.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/output_weights_function.png" alt="Behavior of the output neuron"></a></p><p>Note that neural network does not know anything about the offset of the output word relative to the input word. It <em>does not</em> learn a different set of probabilities for the word before the input versus the word after. To understand the implication, let’s say that in our training corpus, <em>every single occurrence</em> of the word ‘York’ is preceded by the word ‘New’. That is, at least according to the training data, there is a 100% probability that ‘New’ will be in the vicinity of ‘York’. However, if we take the 10 words in the vicinity of ‘York’ and randomly pick one of them, the probability of it being ‘New’ <em>is not</em> 100%; you may have picked one of the other words in the vicinity.</p><h1 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h1><p>Ok, are you ready for an exciting bit of insight into this network?</p><p>If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if <em>the word vectors are similar</em>. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!</p><p>And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well.</p><p>This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts.</p><h1 id="More-Math-Details"><a href="#More-Math-Details" class="headerlink" title="More Math Details"></a>More Math Details</h1><p>For each word $t=1\cdots T$, predict surrounding words in a window of “radius” $m$ of every word.</p><h2 id="Objective-function"><a href="#Objective-function" class="headerlink" title="Objective function:"></a>Objective function:</h2><p>Maximize the probability of any context word given the current center word:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-14-Image-8.png" alt="Page-14-Image-8"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-14-Image-7.png" alt="Page-14-Image-7"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-16-Image-9.png" alt="Page-16-Image-9"></p><h2 id="The-Skip-Gram-Algorithm"><a href="#The-Skip-Gram-Algorithm" class="headerlink" title="The Skip-Gram Algorithm:"></a>The Skip-Gram Algorithm:</h2><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-19-Image-17.png" alt="Page-19-Image-17"></p><h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-24-Image-31.png" alt="Page-24-Image-31"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-25-Image-32.png" alt="Page-25-Image-32"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-26-Image-33.png" alt="Page-26-Image-33"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-27-Image-34.png" alt="Page-27-Image-34"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual i
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="natural language process" scheme="http://yoursite.com/tags/natural-language-process/"/>
    
  </entry>
  
  <entry>
    <title>Prioritized Experience Replay</title>
    <link href="http://yoursite.com/2017/10/30/Prioritized-Experience-Replay/"/>
    <id>http://yoursite.com/2017/10/30/Prioritized-Experience-Replay/</id>
    <published>2017-10-30T06:18:36.000Z</published>
    <updated>2017-10-30T06:25:34.813Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Prioritized-Experience-Replay"><a href="#Prioritized-Experience-Replay" class="headerlink" title="Prioritized Experience Replay"></a>Prioritized Experience Replay</h3><p>One of the possible improvements already acknowledged in the original research<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#fn-444-1" target="_blank" rel="external">2</a> lays in the way experience is used. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#fn-444-4" target="_blank" rel="external">3</a>(PER) is one strategy that tries to leverage this fact by changing the sampling distribution.</p><p>The main idea is that we prefer transitions that does not fit well to our current estimate of the Q function, because these are the transitions that we can learn most from. This reflects a simple intuition from our real world – if we encounter a situation that really differs from our expectation, we think about it over and over and change our model until it fits.</p><p>We can define an error of a sample $S = (s, a, r, s’)$ as a distance between the $Q(s, a)$ and its target $T(S)$:<br>$$<br>error = |Q(s, a) - T(S)|<br>$$<br>For DDQN described above, $T$ it would be:<br>$$<br>T(S) = r + \gamma \tilde{Q}(s’, argmax_a Q(s’, a))<br>$$<br>We will store this error in the agent’s memory along with every sample and update it with each learning step.</p><p>One of the possible approaches to PER is proportional prioritization. The error is first converted to priority using this formula:<br>$$<br>p = (error + \epsilon)^\alpha<br>$$<br>Epsilon $\epsilon$ is a small positive constant that ensures that no transition has zero priority.<br>Alpha, $0 \leq \alpha \leq 1$, controls the difference between high and low error. It determines how much prioritization is used. With $\alpha$ we would get the uniform case.</p><p>Priority is translated to probability of being chosen for replay. A sample $i$ has a probability of being picked during the experience replay determined by a formula:<br>$$<br>P_i = \frac{p_i}{\sum_k p_k}<br>$$<br>The algorithm is simple – during each learning step we will get a batch of samples with this probability distribution and train our network on it. We only need an effective way of storing these priorities and sampling from them.</p><h5 id="Initialization-and-new-transitions"><a href="#Initialization-and-new-transitions" class="headerlink" title="Initialization and new transitions"></a>Initialization and new transitions</h5><p>The original paper says that new transitions come without a known error<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#fn-444-4" target="_blank" rel="external">3</a>, but I argue that with definition given above, we can compute it with a simple forward pass as it arrives. It’s also effective, because high value transitions are discovered immediately.</p><p>Another thing is the initialization. Remember that before the learning itself, we fill the memory using random agent. But this agent does not use any neural network, so how could we estimate any error? We can use a fact that untrained neural network is likely to return a value around zero for every input. In this case the error formula becomes very simple:<br>$$<br>error = |Q(s, a) - T(S)| = |Q(s, a) - r - \gamma \tilde{Q}(s’, argmax_a Q(s’, a))| = | r |<br>$$<br>The error in this case is simply the reward experienced in a given sample. Indeed, the transitions where the agent experienced any reward intuitively seem to be very promising.</p><h5 id="Efficient-implementation"><a href="#Efficient-implementation" class="headerlink" title="Efficient implementation"></a>Efficient implementation</h5><p>So how do we store the experience and effectively sample from it?</p><p>A naive implementation would be to have all samples in an array sorted according to their priorities. A random number <em>s</em>, $0 \leq s \leq \sum_k p_k$, would be picked and the array would be walked left to right, summing up a priority of the current element until the sum is exceeded and that element is chosen. This will select a sample with the desired probability distribution.</p><p><img src="https://jaromiru.files.wordpress.com/2016/11/per_bar_1.png?w=700" alt="Sorted experience"></p><p>But this would have a terrible efficiency: $O(n log n)$ for insertion and update and O$(n) $for sampling.</p><p>A first important observation is that we don’t have to actually store the array sorted. Unsorted array would do just as well. Elements with higher priorities are still picked with higher probability.</p><p><img src="https://jaromiru.files.wordpress.com/2016/11/per_bar_2.png?w=700" alt="Unsorted experience"></p><p>This releases the need for sorting, improving the algorithm to <em>O(1)</em> for insertion and update.</p><p>But the <em>O(n)</em> for sampling is still too high. We can further improve our algorithm by using a different data structure. We can store our samples in unsorted sum tree – a binary tree data structure where the parent’s value is the sum of its children. The samples themselves are stored in the leaf nodes.</p><p>Update of a leaf node involves propagating a value difference up the tree, obtaining <em>O(log n)</em>. Sampling follows the thought process of the array case, but achieves <em>O(log n)</em>. For a value <em>s</em>, $0 \leq s \leq \sum_k p_k$, we use the following algorithm (pseudo code):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieve</span><span class="params">(n, s)</span>:</span></div><div class="line">    <span class="keyword">if</span> n <span class="keyword">is</span> leaf_node: <span class="keyword">return</span> n</div><div class="line"> </div><div class="line">    <span class="keyword">if</span> n.left.val &gt;= s: <span class="keyword">return</span> retrieve(n.left, s)</div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> retrieve(n.right, s - n.left.val)</div></pre></td></tr></table></figure><p>Following picture illustrates sampling from a tree with <em>s = 24</em>:</p><p><img src="https://jaromiru.files.wordpress.com/2016/11/sumtree.png?w=560" alt="Sampling from sum tree"></p><p>With this effective implementation we can use large memory sizes, with hundreds of thousands or millions of samples.</p><p>For known capacity, this sum tree data structure can be backed by an array. Its reference implementation containing 50 lines of code is available on <a href="https://github.com/jaara/AI-blog/blob/master/SumTree.py" target="_blank" rel="external">GitHub</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Prioritized-Experience-Replay&quot;&gt;&lt;a href=&quot;#Prioritized-Experience-Replay&quot; class=&quot;headerlink&quot; title=&quot;Prioritized Experience Replay&quot;&gt;&lt;/a
    
    </summary>
    
    
      <category term="Reinforcement Learning" scheme="http://yoursite.com/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu Server 16.04 Install Gnome and remote connect from Windows VNCViewer</title>
    <link href="http://yoursite.com/2017/10/26/Ubuntu-Server-16-04-Install-Gnome-and-remote-connect-from-Windows-VNCViewer/"/>
    <id>http://yoursite.com/2017/10/26/Ubuntu-Server-16-04-Install-Gnome-and-remote-connect-from-Windows-VNCViewer/</id>
    <published>2017-10-26T04:02:50.000Z</published>
    <updated>2017-10-30T06:04:58.307Z</updated>
    
    <content type="html"><![CDATA[<h3 id="第一步：装-Gnome-环境"><a href="#第一步：装-Gnome-环境" class="headerlink" title="第一步：装 Gnome 环境"></a>第一步：装 Gnome 环境</h3><p>首先按照如下命令安装 Gnome 环境。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get upgrade</div><div class="line">sudo apt-get install gnome</div><div class="line">sudo apt-get install ubuntu-gnome-desktop</div><div class="line">sudo apt-get install gnome-shell</div></pre></td></tr></table></figure><h3 id="第二步：安装-Gnome-界面管理工具"><a href="#第二步：安装-Gnome-界面管理工具" class="headerlink" title="第二步：安装 Gnome 界面管理工具"></a>第二步：安装 Gnome 界面管理工具</h3><p>安装 Gnome 桌面环境的配置工具。可以使用该工作对 Linux 进行很多配置，包括外观，工作台的数量等。后续安装的主题和图标都可以通过这个工具的 _外观（Appearance）_ 进行调整。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get gnome-tweak-tool</div></pre></td></tr></table></figure><h3 id="第三步：安装-Dash-to-Dock-工具条"><a href="#第三步：安装-Dash-to-Dock-工具条" class="headerlink" title="第三步：安装 Dash to Dock 工具条"></a>第三步：安装 Dash to Dock 工具条</h3><p>安装 Gnome 桌面环境下的 Dock 工具条，可提供 mac os 下dock类似的使用体验。</p><p>在任意浏览器打开 <a href="https://extensions.gnome.org/local/" target="_blank" rel="external">Gnome extensions</a>.</p><p>找到 _Dash to Dock_ 扩展栏，点开右面的 _[ON OFF]_ 选项。点击旁边的 _工具_ 选项，可进一步配置更多选项。</p><h3 id="第四步：安装-ARC-扁平化主题和图标"><a href="#第四步：安装-ARC-扁平化主题和图标" class="headerlink" title="第四步：安装 _ARC_ 扁平化主题和图标"></a>第四步：安装 _ARC_ 扁平化主题和图标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo add-apt-repository ppa:noobslab/themes</div><div class="line">sudo add-apt-repository ppa:noobslab/icons</div><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install arc-theme</div><div class="line">sudo apt-get install arc-icons</div></pre></td></tr></table></figure><h3 id="第五步：选装-Flat-Plat-扁平化主题"><a href="#第五步：选装-Flat-Plat-扁平化主题" class="headerlink" title="第五步：选装 _Flat Plat_ 扁平化主题"></a>第五步：选装 _Flat Plat_ 扁平化主题</h3><p>另一个扁平化主题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">curl -sL https://github.com/nana-4/Flat-Plat/archive/v20170323.tar.gz | tar xz</div><div class="line">cd Flat-Plat-20170323/</div><div class="line">sudo ./install.sh</div></pre></td></tr></table></figure><h3 id="第六步：安装vncserver"><a href="#第六步：安装vncserver" class="headerlink" title="第六步：安装vncserver"></a>第六步：安装vncserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install vnc4server</div><div class="line">sudo apt-get install gnome-panel gnome-settings-daemon metacity nautilus gnome-terminal</div><div class="line">cd ~/.vnc</div><div class="line">mv xstartup xstartup.bak</div><div class="line">vim xstartup</div></pre></td></tr></table></figure><p>使用以下配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">#!/bin/sh</div><div class="line"></div><div class="line">export XKL_XMODMAP_DISABLE=1</div><div class="line">unset SESSION_MANAGER</div><div class="line">unset DBUS_SESSION_BUS_ADDRESS</div><div class="line"></div><div class="line">[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup</div><div class="line">[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources</div><div class="line">xsetroot -solid grey</div><div class="line">vncconfig -iconic &amp;</div><div class="line"></div><div class="line">gnome-session &amp;</div><div class="line">gnome-panel &amp;</div><div class="line">gnome-settings-daemon &amp;</div><div class="line">metacity &amp;</div><div class="line">nautilus &amp;</div><div class="line">gnome-terminal &amp;</div></pre></td></tr></table></figure><h3 id="第七步：启动-vncserver"><a href="#第七步：启动-vncserver" class="headerlink" title="第七步：启动 vncserver"></a>第七步：启动 vncserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># ：1可以更改</div><div class="line">vncserver -geometry 1920x1080 -alwaysshared :1</div></pre></td></tr></table></figure><h3 id="第八步：在-Windows-上安装-VNCViewer"><a href="#第八步：在-Windows-上安装-VNCViewer" class="headerlink" title="第八步：在 Windows 上安装 VNCViewer"></a>第八步：在 Windows 上安装 <a href="https://www.realvnc.com/en/connect/download/viewer/" target="_blank" rel="external">VNCViewer</a></h3><p>启动只要 输入 ip:1 即可</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;第一步：装-Gnome-环境&quot;&gt;&lt;a href=&quot;#第一步：装-Gnome-环境&quot; class=&quot;headerlink&quot; title=&quot;第一步：装 Gnome 环境&quot;&gt;&lt;/a&gt;第一步：装 Gnome 环境&lt;/h3&gt;&lt;p&gt;首先按照如下命令安装 Gnome 环境。&lt;/
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Union Find</title>
    <link href="http://yoursite.com/2017/10/23/Union-Find/"/>
    <id>http://yoursite.com/2017/10/23/Union-Find/</id>
    <published>2017-10-23T05:41:42.000Z</published>
    <updated>2017-10-30T06:05:10.547Z</updated>
    
    <content type="html"><![CDATA[<h2 id="五分钟搞懂并查集"><a href="#五分钟搞懂并查集" class="headerlink" title="五分钟搞懂并查集"></a>五分钟搞懂并查集</h2><blockquote><p>转自：laserss<br><a href="http://blog.csdn.net/dellaserss/article/details/7724401/" target="_blank" rel="external">http://blog.csdn.net/dellaserss/article/details/7724401/</a></p></blockquote><p>并查集是我暑假从高手那里学到的一招，觉得真是太精妙的设计了。来看一个实例，杭电1232畅通工程。首先在地图上给你若干个城镇，这些城镇都可以看作点，然后告诉你哪些对城镇之间是有道路直接相连的。最后要解决的是整幅图的连通性问题。比如随意给你两个点，让你判断它们是否连通，或者问你整幅图一共有几个连通分支，也就是被分成了几个互相独立的块。</p><p>像畅通工程这题，问还需要修几条路，实质就是求有几个连通分支。如果是1个连通分支，说明整幅图上的点都连起来了，不用再修路了；如果是2个连通分支，则只要再修1条路，从两个分支中各选一个点，把它们连起来，那么所有的点都是连起来的了；如果是3个连通分支，则只要再修两条路……</p><p>以下面这组数据输入数据来说明</p><p>4 2 1 3 4 3</p><p>第一行告诉你，一共有4个点，2条路。下面两行告诉你，1、3之间有条路，4、3之间有条路。那么整幅图就被分成了1-3-4和2两部分。只要再加一条路，把2和其他任意一个点连起来，畅通工程就实现了，那么这个这组数据的输出结果就是1。好了，现在编程实现这个功能吧，城镇有几百个，路有不知道多少条，而且可能有回路。</p><p>这可如何是好？我以前也不会呀，自从用了并查集之后，嗨，效果还真好！我们全家都用它！并查集由一个整数型的数组和两个函数构成。数组pre[]记录了每个点的前导点是什么，函数find是查找，join是合并。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">int pre[1000 ];</div><div class="line">int find(int x)                                                                   //查找根节点</div><div class="line">&#123;  </div><div class="line">     int r=x; </div><div class="line">     while ( pre[r ] != r )                                                       //返回根节点 r </div><div class="line">           r=pre[r ];</div><div class="line"> </div><div class="line">     int i=x , j ;</div><div class="line">     while( i != r )                                                                 //路径压缩</div><div class="line">     &#123;</div><div class="line">          j = pre[ i ];     // 在改变上级之前用临时变量  j 记录下他的值 </div><div class="line">          pre[ i ]= r ;    //把上级改为根节点</div><div class="line">          i=j;</div><div class="line">     &#125;</div><div class="line">     return r ;</div><div class="line"> &#125;</div><div class="line"> </div><div class="line"> //判断x y是否连通，如果已经连通，就不用管了 //如果不连通，就把它们所在的连通分支合并起,</div><div class="line"> </div><div class="line"> void join(int x,int y)                     </div><div class="line"> &#123;</div><div class="line">     int fx=find(x),fy=find(y);</div><div class="line">     if(fx!=fy)</div><div class="line">         pre[fx ]=fy;</div><div class="line"> &#125;</div></pre></td></tr></table></figure><p>为了解释并查集的原理，我将举一个更有爱的例子。 话说江湖上散落着各式各样的大侠，有上千个之多。他们没有什么正当职业，整天背着剑在外面走来走去，碰到和自己不是一路人的，就免不了要打一架。但大侠们有一个优点就是讲义气，绝对不打自己的朋友。而且他们信奉“朋友的朋友就是我的朋友”，只要是能通过朋友关系串联起来的，不管拐了多少个弯，都认为是自己人。</p><p>这样一来，江湖上就形成了一个一个的群落，通过两两之间的朋友关系串联起来。而不在同一个群落的人，无论如何都无法通过朋友关系连起来，于是就可以放心往死了打。但是两个原本互不相识的人，如何判断是否属于一个朋友圈呢？</p><p>我们可以在每个朋友圈内推举出一个比较有名望的人，作为该圈子的代表人物，这样，每个圈子就可以这样命名“齐达内朋友之队”“罗纳尔多朋友之队”……两人只要互相对一下自己的队长是不是同一个人，就可以确定敌友关系了。</p><p>但是还有问题啊，大侠们只知道自己直接的朋友是谁，很多人压根就不认识队长，要判断自己的队长是谁，只能漫无目的的通过朋友的朋友关系问下去：“你是不是队长？你是不是队长？”这样一来，队长面子上挂不住了，而且效率太低，还有可能陷入无限循环中。</p><p>于是队长下令，重新组队。队内所有人实行分等级制度，形成树状结构，我队长就是根节点，下面分别是二级队员、三级队员。每个人只要记住自己的上级是谁就行了。遇到判断敌友的时候，只要一层层向上问，直到最高层，就可以在短时间内确定队长是谁了。</p><p>由于我们关心的只是两个人之间是否连通，至于他们是如何连通的，以及每个圈子内部的结构是怎样的，甚至队长是谁，并不重要。所以我们可以放任队长随意重新组队，只要不搞错敌友关系就好了。于是，门派产生了。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/QtPIxk7nOVdNRrpMREYRUAaor1hgv7kkWHTuGNGvAFRD5ibRSHjtSW8ZibABfibmYRrMWiaYJtrOnEQlL7UEhVt5PQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt=""></p><p>下面我们来看并查集的实现。 int pre[1000]; 这个数组，记录了每个大侠的上级是谁。大侠们从1或者0开始编号（依据题意而定），pre[15]=3就表示15号大侠的上级是3号大侠。如果一个人的上级就是他自己，那说明他就是掌门人了，查找到此为止。也有孤家寡人自成一派的，比如欧阳锋，那么他的上级就是他自己。</p><p>每个人都只认自己的上级。比如胡青牛同学只知道自己的上级是杨左使。张无忌是谁？不认识！要想知道自己的掌门是谁，只能一级级查上去。 find这个函数就是找掌门用的，意义再清楚不过了（路径压缩算法先不论，后面再说）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">int find(int x)                                          //查找我（x）的掌门</div><div class="line">&#123;</div><div class="line">    int r=x;                                               //委托 r 去找掌门</div><div class="line">    while (pre[r ]!=r)                                //如果r的上级不是r自己（也就是说找到的大侠他不是掌门 = =） </div><div class="line">    r=pre[r] ;                                           // r 就接着找他的上级，直到找到掌门为止。</div><div class="line">    return  r ;                                           //掌门驾到~~~</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>再来看看join函数，就是在两个点之间连一条线，这样一来，原先它们所在的两个板块的所有点就都可以互通了。这在图上很好办，画条线就行了。但我们现在是用并查集来描述武林中的状况的，一共只有一个pre[]数组，该如何实现呢？</p><p>还是举江湖的例子，假设现在武林中的形势如图所示。虚竹小和尚与周芷若MM是我非常喜欢的两个人物，他们的终极boss分别是玄慈方丈和灭绝师太，那明显就是两个阵营了。我不希望他们互相打架，就对他俩说：“你们两位拉拉勾，做好朋友吧。”他们看在我的面子上，同意了。这一同意可非同小可，整个少林和峨眉派的人就不能打架了。这么重大的变化，可如何实现呀，要改动多少地方？</p><p>其实非常简单，我对玄慈方丈说：“大师，麻烦你把你的上级改为灭绝师太吧。这样一来，两派原先的所有人员的终极boss都是师太，那还打个球啊！反正我们关心的只是连通性，门派内部的结构不要紧的。”玄慈一听肯定火大了：“我靠，凭什么是我变成她手下呀，怎么不反过来？我抗议！”抗议无效，上天安排的，最大。反正谁加入谁效果是一样的，我就随手指定了一个。这段函数的意思很明白了吧？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">void join(int x,int y)                                     //我想让虚竹和周芷若做朋友 </div><div class="line">&#123;</div><div class="line">    int fx=find(x),fy=find(y);                          //虚竹的老大是玄慈，芷若MM的老大是灭绝</div><div class="line">    if(fx!=fy)                                                  //玄慈和灭绝显然不是同一个人</div><div class="line">     pre[fx]=fy;                                              //方丈只好委委屈屈地当了师太的手下啦</div><div class="line">  </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>再来看看路径压缩算法。建立门派的过程是用join函数两个人两个人地连接起来的，谁当谁的手下完全随机。最后的树状结构会变成什么胎唇样，我也完全无法预计，一字排开也有可能。这样查找的效率就会比较低下。最理想的情况就是所有人的直接上级都是掌门，一共就两级结构，只要找一次就找到掌门了。哪怕不能完全做到，也最好尽量接近。这样就产生了路径压缩算法。</p><p>设想这样一个场景：两个互不相识的大侠碰面了，想知道能不能揍。 于是赶紧打电话问自己的上级：“你是不是掌门？” 上级说：“我不是呀，我的上级是谁谁谁，你问问他看看。” 一路问下去，原来两人的最终boss都是东厂曹公公。 “哎呀呀，原来是记己人，西礼西礼，在下三营六组白面葫芦娃!” “幸会幸会，在下九营十八组仙子狗尾巴花！” 两人高高兴兴地手拉手喝酒去了。 “等等等等，两位同学请留步，还有事情没完成呢！”我叫住他俩。 “哦，对了，还要做路径压缩。”两人醒悟。</p><p>白面葫芦娃打电话给他的上级六组长：“组长啊，我查过了，其习偶们的掌门是曹公公。不如偶们一起及接拜在曹公公手下吧，省得级别太低，以后查找掌门麻环。” “唔，有道理。” 白面葫芦娃接着打电话给刚才拜访过的三营长……仙子狗尾巴花也做了同样的事情。</p><p>这样，查询中所有涉及到的人物都聚集在曹公公的直接领导下。每次查询都做了优化处理，所以整个门派树的层数都会维持在比较低的水平上。路径压缩的代码，看得懂很好，看不懂也没关系，直接抄上用就行了。总之它所实现的功能就是这么个意思。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/QtPIxk7nOVdNRrpMREYRUAaor1hgv7kkRPXS7odI2JpZ10cHKV9dgGrsttLnxpSYwK6W8uYjqIffQKdP03IIhA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt=""></p><p>下面给出杭电1232畅通工程的解题代码，仅供大家参考，使用并查集来解决问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">#include&lt;iostream  </div><div class="line">using namespace std;  </div><div class="line">int  pre[1050];  </div><div class="line">bool t[1050];               //t 用于标记独立块的根结点  </div><div class="line"></div><div class="line">int Find(int x)  </div><div class="line">&#123;  </div><div class="line">    int r=x;  </div><div class="line">    while(r!=pre[r])  </div><div class="line">        r=pre[r];   </div><div class="line"></div><div class="line">    int i=x,j;  </div><div class="line">    while(pre[i]!=r)  </div><div class="line">    &#123;  </div><div class="line">        j=pre[i];  </div><div class="line">        pre[i]=r;  </div><div class="line">        i=j;  </div><div class="line">    &#125;  </div><div class="line"></div><div class="line">    return r;  </div><div class="line">&#125;  </div><div class="line"></div><div class="line">void mix(int x,int y)  </div><div class="line">&#123;  </div><div class="line">    int fx=Find(x),fy=Find(y);  </div><div class="line">    if(fx!=fy)  </div><div class="line">    &#123;  </div><div class="line">        pre[fy]=fx;  </div><div class="line">    &#125;  </div><div class="line">&#125;   </div><div class="line"></div><div class="line">int main()  </div><div class="line">&#123; </div><div class="line">    int N,M,a,b,i,j,ans;  </div><div class="line">    while(scanf(&quot;%d%d&quot;,&amp;N,&amp;M)&amp;&amp;N)  </div><div class="line">    &#123;  </div><div class="line">        for(i=1;i&lt;=N;i++)          //初始化   </div><div class="line">            pre[i]=i;  </div><div class="line">        for(i=1;i&lt;=M;i++)          //吸收并整理数据   </div><div class="line">        &#123;  </div><div class="line">            scanf(&quot;%d%d&quot;,&amp;a,&amp;b);  </div><div class="line">            mix(a,b);  </div><div class="line">        &#125;         </div><div class="line">        memset(t,0,sizeof(t));  </div><div class="line">        for(i=1;i&lt;=N;i++)          //标记根结点  </div><div class="line">        &#123;  </div><div class="line">            t[Find(i)]=1;  </div><div class="line">        &#125;  </div><div class="line">        for(ans=0,i=1;i&lt;=N;i++)  </div><div class="line">            if(t[i])  </div><div class="line">                ans++;  </div><div class="line">        printf(&quot;%d\n&quot;,ans-1);  </div><div class="line">    &#125;  </div><div class="line"></div><div class="line">    return 0;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;五分钟搞懂并查集&quot;&gt;&lt;a href=&quot;#五分钟搞懂并查集&quot; class=&quot;headerlink&quot; title=&quot;五分钟搞懂并查集&quot;&gt;&lt;/a&gt;五分钟搞懂并查集&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;转自：laserss&lt;br&gt;&lt;a href=&quot;http://blog
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>37 Reasons why your Neural Network is not working [Repost]</title>
    <link href="http://yoursite.com/2017/07/28/37-Reasons-why-your-Neural-Network-is-not-working/"/>
    <id>http://yoursite.com/2017/07/28/37-Reasons-why-your-Neural-Network-is-not-working/</id>
    <published>2017-07-28T06:23:50.000Z</published>
    <updated>2017-07-28T06:38:10.114Z</updated>
    
    <content type="html"><![CDATA[<p>The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then came the predictions: all zeroes, all background, nothing detected. “What did I do wrong?” — I asked my computer, who didn’t answer.</p><p>Where do you start checking if your model is outputting garbage (for example predicting the mean of all outputs, or it has really poor accuracy)?</p><p>A network might not be training for a number of reasons. Over the course of many debugging sessions, I would often find myself doing the same checks. I’ve compiled my experience along with the best ideas around in this handy list. I hope they would be of use to you, too.</p><h3 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h3><blockquote><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#b6fb" target="_blank" rel="external">0. How to use this guide?</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#678a" target="_blank" rel="external">I. Dataset issues</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#86fe" target="_blank" rel="external">II. Data Normalization/Augmentation issues</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#95eb" target="_blank" rel="external">III. Implementation issues</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#74de" target="_blank" rel="external">IV. Training issues</a></p></blockquote><h3 id="0-How-to-use-this-guide"><a href="#0-How-to-use-this-guide" class="headerlink" title="0. How to use this guide?"></a>0. How to use this guide?</h3><p>A lot of things can go wrong. But some of them are more likely to be broken than others. I usually start with this short list as an emergency first response:</p><ol><li>Start with a simple model that is known to work for this type of data (for example, VGG for images). Use a standard loss if possible.</li><li>Turn off all bells and whistles, e.g. regularization and data augmentation.</li><li>If finetuning a model, double check the preprocessing, for it should be the same as the original model’s training.</li><li>Verify that the input data is correct.</li><li>Start with a really small dataset (2–20 samples). Overfit on it and gradually add more data.</li><li>Start gradually adding back all the pieces that were omitted: augmentation/regularization, custom loss functions, try more complex models.</li></ol><p>If the steps above don’t do it, start going down the following big list and verify things one by one.</p><hr><h3 id="I-Dataset-issues"><a href="#I-Dataset-issues" class="headerlink" title="I. Dataset issues"></a>I. Dataset issues</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*xfIbyKKMDmjQF9JFuK2Ykg.png" alt="img">Source: <a href="http://dilbert.com/strip/2014-05-07" target="_blank" rel="external">http://dilbert.com/strip/2014-05-07</a></p><h4 id="1-Check-your-input-data"><a href="#1-Check-your-input-data" class="headerlink" title="1. Check your input data"></a>1. Check your input data</h4><p>Check if the input data you are feeding the network makes sense. For example, I’ve more than once mixed the width and the height of an image. Sometimes, I would feed all zeroes by mistake. Or I would use the same batch over and over. So print/display a couple of batches of input and target output and make sure they are OK.</p><h4 id="2-Try-random-input"><a href="#2-Try-random-input" class="headerlink" title="2. Try random input"></a>2. Try random input</h4><p>Try passing random numbers instead of actual data and see if the error behaves the same way. If it does, it’s a sure sign that your net is turning data into garbage at some point. Try debugging layer by layer /op by op/ and see where things go wrong.</p><h4 id="3-Check-the-data-loader"><a href="#3-Check-the-data-loader" class="headerlink" title="3. Check the data loader"></a>3. Check the data loader</h4><p>Your data might be fine but the code that passes the input to the net might be broken. Print the input of the first layer before any operations and check it.</p><h4 id="4-Make-sure-input-is-connected-to-output"><a href="#4-Make-sure-input-is-connected-to-output" class="headerlink" title="4. Make sure input is connected to output"></a>4. Make sure input is connected to output</h4><p>Check if a few input samples have the correct labels. Also make sure shuffling input samples works the same way for output labels.</p><h4 id="5-Is-the-relationship-between-input-and-output-too-random"><a href="#5-Is-the-relationship-between-input-and-output-too-random" class="headerlink" title="5. Is the relationship between input and output too random?"></a>5. Is the relationship between input and output too random?</h4><p>Maybe the non-random part of the relationship between the input and output is too small compared to the random part (one could argue that stock prices are like this). I.e. the input are not sufficiently related to the output. There isn’t an universal way to detect this as it depends on the nature of the data.</p><h4 id="6-Is-there-too-much-noise-in-the-dataset"><a href="#6-Is-there-too-much-noise-in-the-dataset" class="headerlink" title="6. Is there too much noise in the dataset?"></a>6. Is there too much noise in the dataset?</h4><p>This happened to me once when I scraped an image dataset off a food site. There were so many bad labels that the network couldn’t learn. Check a bunch of input samples manually and see if labels seem off.</p><p>The cutoff point is up for debate, as <a href="https://arxiv.org/pdf/1412.6596.pdf" target="_blank" rel="external">this paper</a> got above 50% accuracy on MNIST using 50% corrupted labels.</p><h4 id="7-Shuffle-the-dataset"><a href="#7-Shuffle-the-dataset" class="headerlink" title="7. Shuffle the dataset"></a>7. Shuffle the dataset</h4><p>If your dataset hasn’t been shuffled and has a particular order to it (ordered by label) this could negatively impact the learning. Shuffle your dataset to avoid this. Make sure you are shuffling input and labels together.</p><h4 id="8-Reduce-class-imbalance"><a href="#8-Reduce-class-imbalance" class="headerlink" title="8. Reduce class imbalance"></a>8. Reduce class imbalance</h4><p>Are there a 1000 class A images for every class B image? Then you might need to balance your loss function or <a href="http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/" target="_blank" rel="external">try other class imbalance approaches</a>.</p><h4 id="9-Do-you-have-enough-training-examples"><a href="#9-Do-you-have-enough-training-examples" class="headerlink" title="9. Do you have enough training examples?"></a>9. Do you have enough training examples?</h4><p>If you are training a net from scratch (i.e. not finetuning), you probably need lots of data. For image classification, <a href="https://stats.stackexchange.com/a/226693/30773" target="_blank" rel="external">people say</a> you need a 1000 images per class or more.</p><h4 id="10-Make-sure-your-batches-don’t-contain-a-single-label"><a href="#10-Make-sure-your-batches-don’t-contain-a-single-label" class="headerlink" title="10. Make sure your batches don’t contain a single label"></a>10. Make sure your batches don’t contain a single label</h4><p>This can happen in a sorted dataset (i.e. the first 10k samples contain the same class). Easily fixable by shuffling the dataset.</p><h4 id="11-Reduce-batch-size"><a href="#11-Reduce-batch-size" class="headerlink" title="11. Reduce batch size"></a>11. Reduce batch size</h4><p><a href="https://arxiv.org/abs/1609.04836" target="_blank" rel="external">This paper</a> points out that having a very large batch can reduce the generalization ability of the model.</p><h4 id="Addition-1-Use-standard-dataset-e-g-mnist-cifar10"><a href="#Addition-1-Use-standard-dataset-e-g-mnist-cifar10" class="headerlink" title="Addition 1. Use standard dataset (e.g. mnist, cifar10)"></a>Addition 1. Use standard dataset (e.g. mnist, cifar10)</h4><p>Thanks to @<a href="https://medium.com/@hengcherkeng" target="_blank" rel="external">hengcherkeng</a> for this one:</p><blockquote><p>When testing new network architecture or writing a new piece of code, use the standard datasets first, instead of your own data. This is because there are many reference results for these datasets and they are proved to be ‘solvable’. There will be no issues of label noise, train/test distribution difference , too much difficulty in dataset, etc.</p></blockquote><hr><h3 id="II-Data-Normalization-Augmentation"><a href="#II-Data-Normalization-Augmentation" class="headerlink" title="II. Data Normalization/Augmentation"></a>II. Data Normalization/Augmentation</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*UQLMfdKi5D4nNDN6Oxa5MA.png" alt="img"></p><h4 id="12-Standardize-the-features"><a href="#12-Standardize-the-features" class="headerlink" title="12. Standardize the features"></a><strong>12. Standardize</strong> the features</h4><p>Did you standardize your input to have zero mean and unit variance?</p><h4 id="13-Do-you-have-too-much-data-augmentation"><a href="#13-Do-you-have-too-much-data-augmentation" class="headerlink" title="13. Do you have too much data augmentation?"></a>13. Do you have too much data augmentation?</h4><p>Augmentation has a regularizing effect. Too much of this combined with other forms of regularization (weight L2, dropout, etc.) can cause the net to underfit.</p><h4 id="14-Check-the-preprocessing-of-your-pretrained-model"><a href="#14-Check-the-preprocessing-of-your-pretrained-model" class="headerlink" title="14. Check the preprocessing of your pretrained model"></a>14. Check the preprocessing of your pretrained model</h4><p>If you are using a pretrained model, make sure you are using the same normalization and preprocessing as the model was when training. For example, should an image pixel be in the range [0, 1], [-1, 1] or [0, 255]?</p><h4 id="15-Check-the-preprocessing-for-train-validation-test-set"><a href="#15-Check-the-preprocessing-for-train-validation-test-set" class="headerlink" title="15. Check the preprocessing for train/validation/test set"></a>15. Check the preprocessing for train/validation/test set</h4><p>CS231n points out a <a href="http://cs231n.github.io/neural-networks-2/#datapre" target="_blank" rel="external">common pitfall</a>:</p><blockquote><p>“… any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation/test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. “</p></blockquote><p>Also, check for different preprocessing in each sample or batch.</p><hr><h3 id="III-Implementation-issues"><a href="#III-Implementation-issues" class="headerlink" title="III. Implementation issues"></a>III. Implementation issues</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*EVy3hNSF4Nq7v7bNYOyNcQ.png" alt="img">Credit: <a href="https://xkcd.com/1838/" target="_blank" rel="external">https://xkcd.com/1838/</a></p><h4 id="16-Try-solving-a-simpler-version-of-the-problem"><a href="#16-Try-solving-a-simpler-version-of-the-problem" class="headerlink" title="16. Try solving a simpler version of the problem"></a>16. Try solving a simpler version of the problem</h4><p>This will help with finding where the issue is. For example, if the target output is an object class and coordinates, try limiting the prediction to object class only.</p><h4 id="17-Look-for-correct-loss-“at-chance”"><a href="#17-Look-for-correct-loss-“at-chance”" class="headerlink" title="17. Look for correct loss “at chance”"></a>17. Look for correct loss “at chance”</h4><p>Again from the excellent <a href="http://cs231n.github.io/neural-networks-3/#sanitycheck" target="_blank" rel="external">CS231n</a>: <em>Initialize with small parameters, without regularization. For example, if we have 10 classes, at chance means we will get the correct class 10% of the time, and the Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302.</em></p><p>After this, try increasing the regularization strength which should increase the loss.</p><h4 id="18-Check-your-loss-function"><a href="#18-Check-your-loss-function" class="headerlink" title="18. Check your loss function"></a>18. Check your loss function</h4><p>If you implemented your own loss function, check it for bugs and add unit tests. Often, my loss would be slightly incorrect and hurt the performance of the network in a subtle way.</p><h4 id="19-Verify-loss-input"><a href="#19-Verify-loss-input" class="headerlink" title="19. Verify loss input"></a>19. Verify loss input</h4><p>If you are using a loss function provided by your framework, make sure you are passing to it what it expects. For example, in PyTorch I would mix up the NLLLoss and CrossEntropyLoss as the former requires a softmax input and the latter doesn’t.</p><h4 id="20-Adjust-loss-weights"><a href="#20-Adjust-loss-weights" class="headerlink" title="20. Adjust loss weights"></a>20. Adjust loss weights</h4><p>If your loss is composed of several smaller loss functions, make sure their magnitude relative to each is correct. This might involve testing different combinations of loss weights.</p><h4 id="21-Monitor-other-metrics"><a href="#21-Monitor-other-metrics" class="headerlink" title="21. Monitor other metrics"></a>21. Monitor other metrics</h4><p>Sometimes the loss is not the best predictor of whether your network is training properly. If you can, use other metrics like accuracy.</p><h4 id="22-Test-any-custom-layers"><a href="#22-Test-any-custom-layers" class="headerlink" title="22. Test any custom layers"></a>22. Test any custom layers</h4><p>Did you implement any of the layers in the network yourself? Check and double-check to make sure they are working as intended.</p><h4 id="23-Check-for-“frozen”-layers-or-variables"><a href="#23-Check-for-“frozen”-layers-or-variables" class="headerlink" title="23. Check for “frozen” layers or variables"></a>23. Check for “frozen” layers or variables</h4><p>Check if you unintentionally disabled gradient updates for some layers/variables that should be learnable.</p><h4 id="24-Increase-network-size"><a href="#24-Increase-network-size" class="headerlink" title="24. Increase network size"></a>24. Increase network size</h4><p>Maybe the expressive power of your network is not enough to capture the target function. Try adding more layers or more hidden units in fully connected layers.</p><h4 id="25-Check-for-hidden-dimension-errors"><a href="#25-Check-for-hidden-dimension-errors" class="headerlink" title="25. Check for hidden dimension errors"></a>25. Check for hidden dimension errors</h4><p>If your input looks like (k, H, W) = (64, 64, 64) it’s easy to miss errors related to wrong dimensions. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network.</p><h4 id="26-Explore-Gradient-checking"><a href="#26-Explore-Gradient-checking" class="headerlink" title="26. Explore Gradient checking"></a>26. Explore Gradient checking</h4><p>If you implemented Gradient Descent by hand, gradient checking makes sure that your backpropagation works like it should. More info: <a href="http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/" target="_blank" rel="external">1</a> <a href="http://cs231n.github.io/neural-networks-3/#gradcheck" target="_blank" rel="external">2</a> <a href="https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking" target="_blank" rel="external">3</a>.</p><hr><h3 id="IV-Training-issues"><a href="#IV-Training-issues" class="headerlink" title="IV. Training issues"></a>IV. Training issues</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*gfcJD0eymh5SGuquzuvpig.png" alt="img">Credit: <a href="http://carlvondrick.com/ihog/" target="_blank" rel="external">http://carlvondrick.com/ihog/</a></p><h4 id="27-Solve-for-a-really-small-dataset"><a href="#27-Solve-for-a-really-small-dataset" class="headerlink" title="27. Solve for a really small dataset"></a>27. Solve for a really small dataset</h4><p><strong>Overfit a small subset of the data and make sure it works. </strong>For example, train with just 1 or 2 examples and see if your network can learn to differentiate these. Move on to more samples per class.</p><h4 id="28-Check-weights-initialization"><a href="#28-Check-weights-initialization" class="headerlink" title="28. Check weights initialization"></a>28. Check weights initialization</h4><p>If unsure, use <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="external">Xavier</a> or <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf" target="_blank" rel="external">He</a> initialization. Also, your initialization might be leading you to a bad local minimum, so try a different initialization and see if it helps.</p><h4 id="29-Change-your-hyperparameters"><a href="#29-Change-your-hyperparameters" class="headerlink" title="29. Change your hyperparameters"></a>29. Change your hyperparameters</h4><p>Maybe you using a particularly bad set of hyperparameters. If feasible, try a <a href="http://scikit-learn.org/stable/modules/grid_search.html" target="_blank" rel="external">grid search</a>.</p><h4 id="30-Reduce-regularization"><a href="#30-Reduce-regularization" class="headerlink" title="30. Reduce regularization"></a>30. Reduce regularization</h4><p>Too much regularization can cause the network to underfit badly. Reduce regularization such as dropout, batch norm, weight/bias L2 regularization, etc. In the excellent “<a href="http://course.fast.ai/" target="_blank" rel="external">Practical Deep Learning for coders</a>” course, <a href="https://twitter.com/jeremyphoward" target="_blank" rel="external">Jeremy Howard</a> advises getting rid of underfitting first. This means you overfit the training data sufficiently, and only then addressing overfitting.</p><h4 id="31-Give-it-time"><a href="#31-Give-it-time" class="headerlink" title="31. Give it time"></a>31. Give it time</h4><p>Maybe your network needs more time to train before it starts making meaningful predictions. If your loss is steadily decreasing, let it train some more.</p><h4 id="32-Switch-from-Train-to-Test-mode"><a href="#32-Switch-from-Train-to-Test-mode" class="headerlink" title="32. Switch from Train to Test mode"></a>32. Switch from Train to Test mode</h4><p>Some frameworks have layers like Batch Norm, Dropout, and other layers behave differently during training and testing. Switching to the appropriate mode might help your network to predict properly.</p><h4 id="33-Visualize-the-training"><a href="#33-Visualize-the-training" class="headerlink" title="33. Visualize the training"></a>33. Visualize the training</h4><ul><li>Monitor the activations, weights, and updates of each layer. Make sure their magnitudes match. For example, the magnitude of the updates to the parameters (weights and biases) <a href="https://cs231n.github.io/neural-networks-3/#summary" target="_blank" rel="external">should be 1-e3</a>.</li><li>Consider a visualization library like <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">Tensorboard</a> and <a href="https://github.com/torrvision/crayon" target="_blank" rel="external">Crayon</a>. In a pinch, you can also print weights/biases/activations.</li><li>Be on the lookout for layer activations with a mean much larger than 0. Try Batch Norm or ELUs.</li><li><a href="https://deeplearning4j.org/visualization#usingui" target="_blank" rel="external">Deeplearning4j</a> points out what to expect in histograms of weights and biases:</li></ul><blockquote><p>“For weights, these histograms should have an <strong>approximately Gaussian (normal) </strong>distribution, after some time. For biases, these histograms will generally start at 0, and will usually end up being <strong>approximately Gaussian</strong>(One exception to this is for LSTM). Keep an eye out for parameters that are diverging to +/- infinity. Keep an eye out for biases that become very large. This can sometimes occur in the output layer for classification if the distribution of classes is very imbalanced.”</p></blockquote><ul><li>Check layer updates, they should have a Gaussian distribution.</li></ul><h4 id="34-Try-a-different-optimizer"><a href="#34-Try-a-different-optimizer" class="headerlink" title="34. Try a different optimizer"></a>34. Try a different optimizer</h4><p>Your choice of optimizer shouldn’t prevent your network from training unless you have selected particularly bad hyperparameters. However, the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time. The paper which describes the algorithm you are using should specify the optimizer. If not, I tend to use Adam or plain SGD with momentum.</p><p>Check this <a href="http://ruder.io/optimizing-gradient-descent/" target="_blank" rel="external">excellent post</a> by Sebastian Ruder to learn more about gradient descent optimizers.</p><h4 id="35-Exploding-Vanishing-gradients"><a href="#35-Exploding-Vanishing-gradients" class="headerlink" title="35. Exploding / Vanishing gradients"></a>35. Exploding / Vanishing gradients</h4><ul><li>Check layer updates, as very large values can indicate exploding gradients. Gradient clipping may help.</li><li>Check layer activations. From <a href="https://deeplearning4j.org/visualization#usingui" target="_blank" rel="external">Deeplearning4j</a> comes a great guideline: <em>“A good standard deviation for the activations is on the order of 0.5 to 2.0. Significantly outside of this range may indicate vanishing or exploding activations.”</em></li></ul><h4 id="36-Increase-Decrease-Learning-Rate"><a href="#36-Increase-Decrease-Learning-Rate" class="headerlink" title="36. Increase/Decrease Learning Rate"></a>36. Increase/Decrease Learning Rate</h4><p>A low learning rate will cause your model to converge very slowly.</p><p>A high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution.</p><p>Play around with your current learning rate by multiplying it by 0.1 or 10.</p><h4 id="37-Overcoming-NaNs"><a href="#37-Overcoming-NaNs" class="headerlink" title="37. Overcoming NaNs"></a>37. Overcoming NaNs</h4><p>Getting a NaN (Non-a-Number) is a much bigger issue when training RNNs (from what I hear). Some approaches to fix it:</p><ul><li>Decrease the learning rate, especially if you are getting NaNs in the first 100 iterations.</li><li>NaNs can arise from division by zero or natural log of zero or negative number.</li><li>Russell Stewart has great pointers on <a href="http://russellsstewart.com/notes/0.html" target="_blank" rel="external">how to deal with NaNs</a>.</li><li>Try evaluating your network layer by layer and see where the NaNs appear.</li></ul><hr><h4 id="Resources"><a href="#Resources" class="headerlink" title="Resources:"></a><strong>Resources:</strong></h4><blockquote><p><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/</a><br><a href="http://russellsstewart.com/notes/0.html" target="_blank" rel="external">http://russellsstewart.com/notes/0.html</a><br><a href="https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class" target="_blank" rel="external">https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class</a><br><a href="https://deeplearning4j.org/visualization" target="_blank" rel="external">https://deeplearning4j.org/visualization</a><br><a href="https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/" target="_blank" rel="external">https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/</a><br><a href="https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phase" target="_blank" rel="external">https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phase</a><br><a href="http://book.caltech.edu/bookforum/showthread.php?t=4113" target="_blank" rel="external">http://book.caltech.edu/bookforum/showthread.php?t=4113</a><br><a href="https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134" target="_blank" rel="external">https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134</a><br><a href="https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithm" target="_blank" rel="external">https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithm</a></p></blockquote><p><strong>Origin post is <a href="*https://medium.com/@slavivanov/4020854bd607*">here</a>.</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch Tutorial (fork from official website)</title>
    <link href="http://yoursite.com/2017/07/24/PyTorch-Totorial-fork-from-official-website/"/>
    <id>http://yoursite.com/2017/07/24/PyTorch-Totorial-fork-from-official-website/</id>
    <published>2017-07-24T13:41:03.000Z</published>
    <updated>2017-07-25T02:09:05.948Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/tensor_tutorial.ipynb" target="_blank" rel="external">Tensor tutorial</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/autograd_tutorial.ipynb" target="_blank" rel="external">Autograd_tutorial</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/neural_networks_tutorial.ipynb" target="_blank" rel="external">Neural_networks_tutorial</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/cifar10_tutorial.ipynb" target="_blank" rel="external">CIFAR10_tutorial</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/tensor_tutorial.ipynb&quot; target=&quot;_blank&quot; rel=&quot;extern
    
    </summary>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Models Implemented By Tensorflow</title>
    <link href="http://yoursite.com/2017/07/24/Machine-Learning-Models-Implemented-By-Tensorflow/"/>
    <id>http://yoursite.com/2017/07/24/Machine-Learning-Models-Implemented-By-Tensorflow/</id>
    <published>2017-07-24T09:54:18.000Z</published>
    <updated>2017-07-24T09:55:57.021Z</updated>
    
    <content type="html"><![CDATA[<p>Project: <a href="https://github.com/ewanlee/finch/tree/master" target="_blank" rel="external">https://github.com/ewanlee/finch/tree/master</a></p><p>There are these algorithms in the tensorflow-models:</p><ul><li>Linear regression</li><li>Logistic regression</li><li>SVM</li><li>Autoencoder (MLP based and CNN based)</li><li>NMF</li><li>GAN</li><li>Conditional GAN</li><li>DCGAN</li><li>CNN</li><li>RNN (for classification and for regression)</li><li>Highway network (MLP based)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Project: &lt;a href=&quot;https://github.com/ewanlee/finch/tree/master&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/ewanlee/finch/tree/mast
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>sklearn-based feature engineering</title>
    <link href="http://yoursite.com/2017/07/20/sklearn-based-feature-engineering/"/>
    <id>http://yoursite.com/2017/07/20/sklearn-based-feature-engineering/</id>
    <published>2017-07-20T04:09:03.000Z</published>
    <updated>2017-07-20T04:23:44.320Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/feature-related/fe.ipynb" target="_blank" rel="external">feature engineering</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/feature-related/parallel.ipynb" target="_blank" rel="external">pipeline</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/feature-related/fe.ipynb&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;feature 
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>WGAN-GP [Repost]</title>
    <link href="http://yoursite.com/2017/07/18/WGAN-GP/"/>
    <id>http://yoursite.com/2017/07/18/WGAN-GP/</id>
    <published>2017-07-18T03:16:24.000Z</published>
    <updated>2017-07-18T13:49:35.700Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ewanlee.github.io/2017/04/29/The-awesome-Wasserstein-GAN/" target="_blank" rel="external">WGAN</a>存在着训练困难、收敛速度慢等问题。<a href="https://link.zhihu.com/?target=https%3A//www.reddit.com/r/MachineLearning/comments/5zd4c0/d_survey_whats_the_most_stable_regiment_for/dexfhxu/%3Futm_content%3Dpermalink%26utm_medium%3Dfront%26utm_source%3Dreddit%26utm_name%3DMachineLearning" target="_blank" rel="external">WGAN的作者Martin Arjovsky不久后就在reddit上表示他也意识到了这个问题</a>，认为关键在于原设计中Lipschitz限制的施加方式不对：</p><blockquote><p>I am now pretty convinced that the problems that happen sometimes in WGANs is due to the specific way of how weight clipping works. It’s just a terrible way of enforcing a Lipschitz constraint, and better ways are out there. I feel like apologizing for being too lazy and sticking to what could be done in one line of torch code.</p><p>A simple alternative (less than 5 lines of code) has been found by Montréal students. It works on quite a few settings (inc 100 layer resnets) with default hyperparameters. Arxiv coming this or next week, stay tuned.</p></blockquote><p>并在新论文中提出了相应的改进方案：</p><ul><li>论文：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.00028" target="_blank" rel="external">[1704.00028] Improved Training of Wasserstein GANs</a></li><li>Tensorflow实现：<ul><li><a href="https://github.com/brianherman/improved_wgan_training" target="_blank" rel="external">brianherman/improved_wgan_training</a> (Python 3)</li><li><a href="https://github.com/igul222/improved_wgan_training" target="_blank" rel="external">igul222/improved_wgan_training</a> (Python 2)</li></ul></li></ul><p><strong>首先回顾一下WGAN的关键部分——Lipschitz限制是什么。</strong>WGAN中，判别器D和生成器G的loss函数分别是<br>$$<br>\begin{align}<br>L(D) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] \\<br>L(G) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)]<br>\end{align}<br>$$<br>公式1表示判别器希望尽可能拉高真样本的分数，拉低假样本的分数，公式2表示生成器希望尽可能拉高假样本的分数。</p><p>Lipschitz限制则体现为，在整个样本空间$\mathcal{X}$上，要求判别器函数$D(x)$梯度的$L_p$ norm大于一个有限的常数K：<br>$$<br>| \nabla_x D(x) |_p \leq K, \forall x \in \mathcal{X}<br>$$<br>直观上解释，就是当输入的样本稍微变化后，判别器给出的分数不能发生太过剧烈的变化。在原来的论文中，这个限制具体是通过weight clipping的方式实现的：每当更新完一次判别器的参数之后，就检查判别器的所有参数的绝对值有没有超过一个阈值，比如0.01，有的话就把这些参数clip回 [-0.01, 0.01] 范围内。通过在训练过程中保证判别器的所有参数有界，就保证了判别器不能对两个略微不同的样本给出天差地别的分数值，从而间接实现了Lipschitz限制。</p><p><strong>然而weight clipping的实现方式存在两个严重问题：</strong></p><p>第一，如公式1所言，判别器loss希望尽可能拉大真假样本的分数差，然而weight clipping独立地限制每一个网络参数的取值范围，在这种情况下我们可以想象，最优的策略就是尽可能让所有参数走极端，要么取最大值（如0.01）要么取最小值（如-0.01）！为了验证这一点，作者统计了经过充分训练的判别器中所有网络参数的数值分布，发现真的集中在最大和最小两个极端上：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-clip.png" alt="weight-clipping"></p><p>这样带来的结果就是，判别器会非常倾向于学习一个简单的映射函数（想想看，几乎所有参数都是正负0.01，都已经可以直接视为一个<a href="https://link.zhihu.com/?target=http%3A//synchuman.baijia.baidu.com/article/385441" target="_blank" rel="external">二值神经网络**</a>了，太简单了）。而作为一个深层神经网络来说，这实在是对自身强大拟合能力的巨大浪费！判别器没能充分利用自身的模型能力，经过它回传给生成器的梯度也会跟着变差。</p><p>在正式介绍gradient penalty之前，我们可以先看看在它的指导下，同样充分训练判别器之后，参数的数值分布就合理得多了，判别器也能够充分利用自身模型的拟合能力：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-gp.png" alt="weight-gp"></p><p>第二个问题，weight clipping会导致很容易一不小心就梯度消失或者梯度爆炸。原因是判别器是一个多层网络，如果我们把clipping threshold设得稍微小了一点，每经过一层网络，梯度就变小一点点，多层之后就会指数衰减；反之，如果设得稍微大了一点，每经过一层网络，梯度变大一点点，多层之后就会指数爆炸。只有设得不大不小，才能让生成器获得恰到好处的回传梯度，然而在实际应用中这个平衡区域可能很狭窄，就会给调参工作带来麻烦。相比之下，gradient penalty就可以让梯度在后向传播的过程中保持平稳。论文通过下图体现了这一点，其中横轴代表判别器从低到高第几层，纵轴代表梯度回传到这一层之后的尺度大小（注意纵轴是对数刻度），c是clipping threshold：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-gradient-norm.png" alt="gradient-norm"></p><p><strong>说了这么多，gradient penalty到底是什么？</strong></p><p>前面提到，Lipschitz限制是要求判别器的梯度不超过K，那我们何不直接设置一个额外的loss项来体现这一点呢？比如说：<br>$$<br>ReLU[| \nabla_x D(x) |_p - K]<br>$$<br>不过，既然判别器希望尽可能拉大真假样本的分数差距，那自然是希望梯度越大越好，变化幅度越大越好，所以判别器在充分训练之后，其梯度norm其实就会是在K附近。知道了这一点，我们可以把上面的loss改成要求梯度norm离K越近越好，效果是类似的：<br>$$<br>[| \nabla_x D(x) |_p - K]^2<br>$$<br>究竟是公式4好还是公式5好，我看不出来，可能需要实验验证，反正论文作者选的是公式5。接着我们简单地把K定为1，再跟WGAN原来的判别器loss加权合并，就得到新的判别器loss：<br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \mathcal{X}} [| \nabla_x D(x) |_p - 1]^2<br>$$<br>这就是所谓的gradient penalty了吗？还没完。公式6有两个问题，首先是loss函数中存在梯度项，那么优化这个loss岂不是要算梯度的梯度？一些读者可能对此存在疑惑，不过这属于实现上的问题，放到后面说。</p><p>其次，3个loss项都是期望的形式，落到实现上肯定得变成采样的形式。前面两个期望的采样我们都熟悉，第一个期望是从真样本集里面采，第二个期望是从生成器的噪声输入分布采样后，再由生成器映射到样本空间。可是第三个分布要求我们在整个样本空间$\mathcal{X}$上采样，这完全不科学！由于所谓的维度灾难问题，如果要通过采样的方式在图片或自然语言这样的高维样本空间中估计期望值，所需样本量是指数级的，实际上没法做到。</p><p>所以，论文作者就非常机智地提出，我们其实没必要在整个样本空间上施加Lipschitz限制，只要重点抓住生成样本集中区域、真实样本集中区域以及夹在它们中间的区域就行了。具体来说，我们先随机采一对真假样本，还有一个0-1的随机数：<br>$$<br>x_r \sim P_r, x_g \sim P_g, \epsilon \sim Uniform[0, 1]<br>$$<br>然后在$x_r$和$x_g$的连线上随机插值采样：<br>$$<br>\hat{x} = \epsilon x_r + (1 - \epsilon) x_g<br>$$<br>把按照上述流程采样得到的$\hat{x}$所满足的分布记为$P_{\hat{x}}$, 就得到最终版本的判别器loss：<br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \hat{x}} [| \nabla_x D(x) |_p - 1]^2<br>$$<br><strong>这就是新论文所采用的gradient penalty方法，相应的新WGAN模型简称为WGAN-GP。</strong>我们可以做一个对比：</p><ul><li>weight clipping是对样本空间全局生效，但因为是间接限制判别器的梯度norm，会导致一不小心就梯度消失或者梯度爆炸；</li><li>gradient penalty只对真假样本集中区域、及其中间的过渡地带生效，但因为是直接把判别器的梯度norm限制在1附近，所以梯度可控性非常强，容易调整到合适的尺度大小。</li></ul><p>论文还讲了一些使用gradient penalty时需要注意的配套事项，这里只提一点：由于我们是对每个样本独立地施加梯度惩罚，所以判别器的模型架构中不能使用Batch Normalization，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择其他normalization方法，如Layer Normalization、Weight Normalization和Instance Normalization，这些方法就不会引入样本之间的依赖。论文推荐的是Layer Normalization。</p><p>实验表明，gradient penalty能够显著提高训练速度，解决了原始WGAN收敛缓慢的问题：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-exper.png" alt="exper"></p><p>虽然还是比不过DCGAN，但是因为WGAN不存在平衡判别器与生成器的问题，所以会比DCGAN更稳定，还是很有优势的。不过，作者凭什么能这么说？因为下面的实验体现出，在各种不同的网络架构下，其他GAN变种能不能训练好，可以说是一件相当看人品的事情，但是WGAN-GP全都能够训练好，尤其是最下面一行所对应的101层残差神经网络：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-image-gen.png" alt="image-gen"></p><p><strong>剩下的实验结果中，比较厉害的是第一次成功做到了“纯粹的”的文本GAN训练！</strong>我们知道在图像上训练GAN是不需要额外的有监督信息的，但是之前就没有人能够像训练图像GAN一样训练好一个文本GAN，要么依赖于预训练一个语言模型，要么就是利用已有的有监督ground truth提供指导信息。而现在WGAN-GP终于在无需任何有监督信息的情况下，生成出下图所示的英文字符序列：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-text-gen.png" alt="text-gen"></p><p>它是怎么做到的呢？我认为关键之处是对样本形式的更改。<strong>以前我们一般会把文本这样的离散序列样本表示为sequence of index，但是它把文本表示成sequence of probability vector。</strong>对于生成样本来说，我们可以取网络softmax层输出的词典概率分布向量，作为序列中每一个位置的内容；而对于真实样本来说，每个probability vector实际上就蜕化为我们熟悉的onehot vector。</p><p>但是如果按照传统GAN的思路来分析，这不是作死吗？一边是hard onehot vector，另一边是soft probability vector，判别器一下子就能够区分它们，生成器还怎么学习？没关系，对于WGAN来说，真假样本好不好区分并不是问题，WGAN只是拉近两个分布之间的Wasserstein距离，就算是一边是hard onehot另一边是soft probability也可以拉近，在训练过程中，概率向量中的有些项可能会慢慢变成0.8、0.9到接近1，整个向量也会接近onehot，最后我们要真正输出sequence of index形式的样本时，只需要对这些概率向量取argmax得到最大概率的index就行了。</p><p>新的样本表示形式+WGAN的分布拉近能力是一个“黄金组合”，但除此之外，还有其他因素帮助论文作者跑出上图的效果，包括：</p><ul><li>文本粒度为英文字符，而非英文单词，所以字典大小才二三十，大大减小了搜索空间</li><li>文本长度也才32</li><li>生成器用的不是常见的LSTM架构，而是多层反卷积网络，输入一个高斯噪声向量，直接一次性转换出所有32个字符</li></ul><p><strong>最后说回gradient penalty的实现问题。</strong>loss中本身包含梯度，优化loss就需要求梯度的梯度，这个功能并不是现在所有深度学习框架的标配功能，不过好在Tensorflow就有提供这个接口—<code>tf.gradients</code>。开头链接的GitHub源码中就是这么写的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># interpolates就是随机插值采样得到的图像，gradients就是loss中的梯度惩罚项</div><div class="line">gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]</div></pre></td></tr></table></figure><p>完整的loss是这样实现的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">gen_cost = -tf.reduce_mean(disc_fake)</div><div class="line">disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)</div><div class="line"></div><div class="line">alpha = tf.random_uniform(</div><div class="line">	shape=[BATCH_SIZE,<span class="number">1</span>], </div><div class="line">	minval=<span class="number">0.</span>,</div><div class="line">	maxval=<span class="number">1.</span></div><div class="line">)</div><div class="line">differences = fake_data - real_data</div><div class="line">interpolates = real_data + (alpha*differences)</div><div class="line">gradients = tf.gradients(Discriminator(interpolates), [interpolates])[<span class="number">0</span>]</div><div class="line">slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[<span class="number">1</span>]))</div><div class="line">gradient_penalty = tf.reduce_mean((slopes<span class="number">-1.</span>)**<span class="number">2</span>)</div><div class="line">disc_cost += LAMBDA*gradient_penalty</div><div class="line"></div><div class="line">gen_train_op = tf.train.AdamOptimizer(</div><div class="line">	learning_rate=<span class="number">1e-4</span>, </div><div class="line">	beta1=<span class="number">0.5</span>,</div><div class="line">	beta2=<span class="number">0.9</span></div><div class="line">).minimize(gen_cost, var_list=gen_params)</div><div class="line">disc_train_op = tf.train.AdamOptimizer(</div><div class="line">	learning_rate=<span class="number">1e-4</span>, </div><div class="line">	beta1=<span class="number">0.5</span>, </div><div class="line">	beta2=<span class="number">0.9</span></div><div class="line">).minimize(disc_cost, var_list=disc_params)</div></pre></td></tr></table></figure><p>对于我这样的PyTorch党就非常不幸了，高阶梯度的功能还在开发，感兴趣的PyTorch党可以订阅这个GitHub的pull request：<a href="https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/pull/1016" target="_blank" rel="external">Autograd refactor</a>，如果它被merged了话就可以在最新版中使用高阶梯度的功能实现gradient penalty了。</p><p>但是除了等待我们就没有别的办法了吗？<strong>其实可能是有的，我想到了一种近似方法来实现gradient penalty，只需要把微分换成差分：</strong><br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x_1 \sim \hat{x}, x_2 \sim \hat{x}} [ \frac{|D(x_1) - D(x_2)|}{| x_1 - x_2 |_p} - 1]^2<br>$$<br>也就是说，我们仍然是在分布 $P_{\hat{x}}$ 上随机采样，但是一次采两个，然后要求它们的连线斜率要接近1，这样理论上也可以起到跟公式9一样的效果，我自己在MNIST+MLP上简单验证过有作用，PyTorch党甚至Tensorflow党都可以尝试用一下。</p><hr><p><strong>作者：郑华滨链接：<a href="https://www.zhihu.com/question/52602529/answer/158727900" target="_blank" rel="external">https://www.zhihu.com/question/52602529/answer/158727900</a></strong></p><p><strong>来源：知乎</strong></p><p><strong>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://ewanlee.github.io/2017/04/29/The-awesome-Wasserstein-GAN/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;WGAN&lt;/a&gt;存在着训练困难、收敛速度慢等问题。&lt;a hr
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="WGAN" scheme="http://yoursite.com/tags/WGAN/"/>
    
  </entry>
  
</feed>
