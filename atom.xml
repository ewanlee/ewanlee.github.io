<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Abracadabra</title>
  <subtitle>Do it yourself</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-01-29T13:52:24.514Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Ewan Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A Visual Guide to Evolution Strategies</title>
    <link href="http://yoursite.com/2018/01/29/A-Visual-Guide-to-Evolution-Strategies/"/>
    <id>http://yoursite.com/2018/01/29/A-Visual-Guide-to-Evolution-Strategies/</id>
    <published>2018-01-29T13:23:43.000Z</published>
    <updated>2018-01-29T13:52:24.514Z</updated>
    
    <content type="html"><![CDATA[<p>Source post is <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/" target="_blank" rel="external">here</a>.</p><p><img src="http://blog.otoro.net/assets/20171031/es_bear.jpeg" alt="img"><br><em>Survival of the fittest.</em></p><p>In this post I explain how evolution strategies (ES) work with the aid of a few visual examples. I try to keep the equations light, and I provide links to original articles if the reader wishes to understand more details. This is the first post in a series of articles, where I plan to show how to apply these algorithms to a range of tasks from MNIST, OpenAI Gym, Roboschool to PyBullet environments.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Neural network models are highly expressive and flexible, and if we are able to find a suitable set of model parameters, we can use neural nets to solve many challenging problems. Deep learning’s success largely comes from the ability to use the backpropagation algorithm to efficiently calculate the gradient of an objective function over each model parameter. With these gradients, we can efficiently search over the parameter space to find a solution that is often good enough for our neural net to accomplish difficult tasks.</p><p>However, there are many problems where the backpropagation algorithm cannot be used. For example, in reinforcement learning (RL) problems, we can also train a neural network to make decisions to perform a sequence of actions to accomplish some task in an environment. However, it is not trivial to estimate the gradient of reward signals given to the agent in the future to an action performed by the agent right now, especially if the reward is realised many timesteps in the future. Even if we are able to calculate accurate gradients, there is also the issue of being stuck in a local optimum, which exists many for RL tasks.</p><p><img src="http://blog.otoro.net/assets/20171031/biped/biped_local_optima.gif" alt="img"></p><p><em>Stuck in a local optimum.</em></p><p>A whole area within RL is devoted to studying this credit-assignment problem, and great progress has been made in recent years. However, credit assignment is still difficult when the reward signals are sparse. In the real world, rewards can be sparse and noisy. Sometimes we are given just a single reward, like a bonus check at the end of the year, and depending on our employer, it may be difficult to figure out exactly why it is so low. For these problems, rather than rely on a very noisy and possibly meaningless gradient estimate of the future to our policy, we might as well just ignore any gradient information, and attempt to use black-box optimisation techniques such as genetic algorithms (GA) or ES.</p><p>OpenAI published a paper called <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a> where they showed that evolution strategies, while being less data efficient than RL, offer many benefits. The ability to abandon gradient calculation allows such algorithms to be evaluated more efficiently. It is also easy to distribute the computation for an ES algorithm to thousands of machines for parallel computation. By running the algorithm from scratch many times, they also showed that policies discovered using ES tend to be more diverse compared to policies discovered by RL algorithms.</p><p>I would like to point out that even for the problem of identifying a machine learning model, such as designing a neural net’s architecture, is one where we cannot directly compute gradients. While <a href="https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html" target="_blank" rel="external">RL</a>, <a href="https://arxiv.org/abs/1703.00548" target="_blank" rel="external">Evolution</a>, <a href="http://blog.otoro.net/2016/05/07/backprop-neat/" target="_blank" rel="external">GA</a> etc., can be applied to search in the space of model architectures, in this post, I will focus only on applying these algorithms to search for parameters of a pre-defined model.</p><h2 id="What-is-an-Evolution-Strategy"><a href="#What-is-an-Evolution-Strategy" class="headerlink" title="What is an Evolution Strategy?"></a>What is an Evolution Strategy?</h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/8/8b/Rastrigin_function.png" alt="img"><br><em>Two-dimensional Rastrigin function has many local optima (Source: Wikipedia</em>).</p><p>The diagrams below are top-down plots of <em>shifted</em> 2D <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization" target="_blank" rel="external">Schaffer and Rastrigin</a> functions, two of several simple toy problems used for testing continuous black-box optimisation algorithms. Lighter regions of the plots represent higher values of $F(x,y)$. As you can see, there are many local optimums in this function. Our job is to find a set of <em>model parameters</em> $(x, y)$, such that $F(x,y)$ is as close as possible to the global maximum.</p><p><em>Schaffer-2D Function</em><br><img src="http://blog.otoro.net/assets/20171031/schaffer/schaffer_label.png" alt="img"></p><p><em>Rastrigin-2D Function</em><br><img src="http://blog.otoro.net/assets/20171031/rastrigin/rastrigin_label.png" alt="img"></p><p>Although there are many definitions of evolution strategies, we can define an evolution strategy as an algorithm that provides the user a set of candidate solutions to evaluate a problem. The evaluation is based on an <em>objective function</em> that takes a given solution and returns a single <em>fitness</em> value. Based on the fitness results of the current solutions, the algorithm will then produce the next generation of candidate solutions that is more likely to produce even better results than the current generation. The iterative process will stop once the best known solution is satisfactory for the user.</p><p>Given an evolution strategy algorithm called <code>EvolutionStrategy</code>, we can use in the following way:</p><hr><p><code>solver = EvolutionStrategy()</code></p><p><code>while True:</code></p><p><code># ask the ES to give us a set of candidate solutions</code><br><code>solutions = solver.ask()</code></p><p><code># create an array to hold the fitness results.</code><br><code>fitness_list = np.zeros(solver.popsize)</code></p><p><code># evaluate the fitness for each given solution.</code><br><code>for i in range(solver.popsize):</code><br><code>fitness_list[i] = evaluate(solutions[i])</code></p><p><code># give list of fitness results back to ES</code><br><code>solver.tell(fitness_list)</code></p><p><code># get best parameter, fitness from ES</code><br><code>best_solution, best_fitness = solver.result()</code></p><p><code>if best_fitness &gt; MY_REQUIRED_FITNESS:</code><br><code>break</code></p><hr><p>Although the size of the population is usually held constant for each generation, they don’t need to be. The ES can generate as many candidate solutions as we want, because the solutions produced by an ES are <em>sampled</em> from a distribution whose parameters are being updated by the ES at each generation. I will explain this sampling process with an example of a simple evolution strategy.</p><h2 id="Simple-Evolution-Strategy"><a href="#Simple-Evolution-Strategy" class="headerlink" title="Simple Evolution Strategy"></a>Simple Evolution Strategy</h2><p>One of the simplest evolution strategy we can imagine will just sample a set of solutions from a Normal distribution, with a mean \muμand a fixed standard deviation \sigmaσ. In our 2D problem, \mu = (\mu_x, \mu_y)μ=(μx,μy) and \sigma = (\sigma_x, \sigma_y)σ=(σx,σy). Initially, \muμ is set at the origin. After the fitness results are evaluated, we set \muμ to the best solution in the population, and sample the next generation of solutions around this new mean. This is how the algorithm behaves over 20 generations on the two problems mentioned earlier:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/simplees.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/simplees.gif" alt="img"></p><p>In the visualisation above, the green dot indicates the mean of the distribution at each generation, the blue dots are the sampled solutions, and the red dot is the best solution found so far by our algorithm.</p><p>This simple algorithm will generally only work for simple problems. Given its greedy nature, it throws away all but the best solution, and can be prone to be stuck at a local optimum for more complicated problems. It would be beneficial to sample the next generation from a probability distribution that represents a more diverse set of ideas, rather than just from the best solution from the current generation.</p><h2 id="Simple-Genetic-Algorithm"><a href="#Simple-Genetic-Algorithm" class="headerlink" title="Simple Genetic Algorithm"></a>Simple Genetic Algorithm</h2><p>One of the oldest black-box optimisation algorithms is the genetic algorithm. There are many variations with many degrees of sophistication, but I will illustrate the simplest version here.</p><p>The idea is quite simple: keep only 10% of the best performing solutions in the current generation, and let the rest of the population die. In the next generation, to sample a new solution is to randomly select two solutions from the survivors of the previous generation, and recombine their parameters to form a new solution. This <em>crossover</em> recombination process uses a coin toss to determine which parent to take each parameter from. In the case of our 2D toy function, our new solution might inherit xx or yy from either parents with 50% chance. Gaussian noise with a fixed standard deviation will also be injected into each new solution after this recombination process.</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/simplega.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/simplega.gif" alt="img"></p><p>The figure above illustrates how the simple genetic algorithm works. The green dots represent members of the elite population from the previous generation, the blue dots are the offsprings to form the set of candidate solutions, and the red dot is the best solution.</p><p>Genetic algorithms help diversity by keeping track of a diverse set of candidate solutions to reproduce the next generation. However, in practice, most of the solutions in the elite surviving population tend to converge to a local optimum over time. There are more sophisticated variations of GA out there, such as <a href="http://people.idsia.ch/~juergen/gomez08a.pdf" target="_blank" rel="external">CoSyNe</a>, <a href="http://blog.otoro.net/2015/03/10/esp-algorithm-for-double-pendulum/" target="_blank" rel="external">ESP</a>, and <a href="http://blog.otoro.net/2016/05/07/backprop-neat/" target="_blank" rel="external">NEAT</a>, where the idea is to cluster similar solutions in the population together into different species, to maintain better diversity over time.</p><h2 id="Covariance-Matrix-Adaptation-Evolution-Strategy-CMA-ES"><a href="#Covariance-Matrix-Adaptation-Evolution-Strategy-CMA-ES" class="headerlink" title="Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)"></a>Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)</h2><p>A shortcoming of both the Simple ES and Simple GA is that our standard deviation noise parameter is fixed. There are times when we want to explore more and increase the standard deviation of our search space, and there are times when we are confident we are close to a good optima and just want to fine tune the solution. We basically want our search process to behave like this:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/cmaes.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes.gif" alt="img"></p><p>Amazing isn’it it? The search process shown in the figure above is produced by <a href="https://en.wikipedia.org/wiki/CMA-ES" target="_blank" rel="external">Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)</a>. CMA-ES an algorithm that can take the results of each generation, and adaptively increase or decrease the search space for the next generation. It will not only adapt for the mean $\mu$ and sigma $\sigma$ parameters, but will calculate the entire covariance matrix of the parameter space. At each generation, CMA-ES provides the parameters of a multi-variate normal distribution to sample solutions from. So how does it know how to increase or decrease the search space?</p><p>Before we discuss its methodology, let’s review how to estimate a <a href="https://en.wikipedia.org/wiki/Covariance_matrix" target="_blank" rel="external">covariance matrix</a>. This will be important to understand CMA-ES’s methodology later on. If we want to estimate the covariance matrix of our entire sampled population of size of $N$, we can do so using the set of equations below to calculate the maximum likelihood estimate of a covariance matrix $C$. We first calculate the means of each of the $x_i$ and $y_i$ in our population:<br>$$<br>\mu_x = \frac{1}{N} \sum_{i=1}^{N}x_i,<br>$$</p><p>$$<br>\mu_y = \frac{1}{N} \sum_{i=1}^{N}y_i.<br>$$</p><p>The terms of the 2x2 covariance matrix $C$ will be:<br>$$<br>\begin{align}<br>\sigma_x^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)^2, \\</p><p>\sigma_y^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(y_i - \mu_y)^2, \\</p><p>\sigma_{xy} &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y).<br>\end{align}<br>$$<br>Of course, these resulting mean estimates $\mu_x$ and $\mu_y$, and covariance terms $\sigma_x$, $\sigma_y$, $\sigma_{xy}$ will just be an estimate to the actual covariance matrix that we originally sampled from, and not particularly useful to us.</p><p>CMA-ES modifies the above covariance calculation formula in a clever way to make it adapt well to an optimisation problem. I will go over how it does this step-by-step. Firstly, it focuses on the best $N_{best}$ solutions in the current generation. For simplicity let’s set $N_{best}$ to be the best 25% of solutions. After sorting the solutions based on fitness, we calculate the mean $\mu^{(g+1)}$ of the next generation $(g+1)$ as the average of only the best 25% of the solutions in current population $(g)$, i.e.:<br>$$<br>\begin{align}<br>\mu_x^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}x_i, \\</p><p>\mu_y^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}y_i.<br>\end{align}<br>$$<br>Next, we use only the best 25% of the solutions to estimate the covariance matrix $C^{(g+1)}$ of the next generation, but the clever <em>hack</em> here is that it uses the <em>current</em> generation’s $\mu^{(g)}$, rather than the updated $\mu^{(g+1)}$ parameters that we had just calculated, in the calculation:<br>$$<br>\begin{align}<br>\sigma_x^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})^2, \\</p><p>\sigma_y^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(y_i - \mu_y^{(g)})^2, \\</p><p>\sigma_{xy}^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})(y_i - \mu_y^{(g)}).<br>\end{align}<br>$$<br>Armed with a set of $\mu_x$, $\mu_y$, $\sigma_x$, $\sigma_y$, and $\sigma_{xy}$ parameters for the next generation $(g+1)$, we can now sample the next generation of candidate solutions.</p><p>Below is a set of figures to visually illustrate how it uses the results from the current generation $(g)$ to construct the solutions in the next generation $(g+1)$:</p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step1.png" alt="img"></p><p><em>Step 1</em></p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step2.png" alt="img"></p><p><em>Step 2</em></p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step3.png" alt="img"></p><p><em>Step 3</em></p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step4.png" alt="img"></p><p><em>Step 4</em></p><ol><li>Calculate the fitness score of each candidate solution in generation $(g)$.</li><li>Isolates the best 25% of the population in generation $(g)$, in purple.</li><li>Using only the best solutions, along with the mean $\mu^{(g)}$ of the current generation (the green dot), calculate the covariance matrix $C^{(g+1)}$ of the next generation.</li><li>Sample a new set of candidate solutions using the updated mean $\mu^{(g+1)}$ and covariance matrix $C^{(g+1)}$.</li></ol><p>Let’s visualise the scheme one more time, on the entire search process on both problems:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/cmaes2.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes2.gif" alt="img"></p><p>Because CMA-ES can adapt both its mean and covariance matrix using information from the best solutions, it can decide to cast a wider net when the best solutions are far away, or narrow the search space when the best solutions are close by. My description of the CMA-ES algorithm for a 2D toy problem is highly simplified to get the idea across. For more details, I suggest reading the <a href="https://arxiv.org/abs/1604.00772" target="_blank" rel="external">CMA-ES Tutorial</a> prepared by Nikolaus Hansen, the author of CMA-ES.</p><p>This algorithm is one of the most popular gradient-free optimisation algorithms out there, and has been the algorithm of choice for many researchers and practitioners alike. The only real drawback is the performance if the number of model parameters we need to solve for is large, as the covariance calculation is $O(N^2)$, although recently there has been approximations to make it $O(N)$. CMA-ES is my algorithm of choice when the search space is less than a thousand parameters. I found it still usable up to ~ 10K parameters if I’m willing to be patient.</p><h2 id="Natural-Evolution-Strategies"><a href="#Natural-Evolution-Strategies" class="headerlink" title="Natural Evolution Strategies"></a>Natural Evolution Strategies</h2><hr><p><em>Imagine if you had built an artificial life simulator, and you sample a different neural network to control the behavior of each ant inside an ant colony. Using the Simple Evolution Strategy for this task will optimise for traits and behaviours that benefit individual ants, and with each successive generation, our population will be full of alpha ants who only care about their own well-being.</em></p><p><em>Instead of using a rule that is based on the survival of the fittest ants, what if you take an alternative approach where you take the sum of all fitness values of the entire ant population, and optimise for this sum instead to maximise the well-being of the entire ant population over successive generations? Well, you would end up creating a Marxist utopia.</em></p><hr><p>A perceived weakness of the algorithms mentioned so far is that they discard the majority of the solutions and only keep the best solutions. Weak solutions contain information about what <em>not</em> to do, and this is valuable information to calculate a better estimate for the next generation.</p><p>Many people who studied RL are familiar with the <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> paper. In this 1992 paper, Williams outlined an approach to estimate the gradient of the expected rewards with respect to the model parameters of a policy neural network. This paper also proposed using REINFORCE as an Evolution Strategy, in Section 6 of the paper. This special case of <em>REINFORCE-ES</em> was expanded later on in <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Parameter-Exploring Policy Gradients</a> (PEPG, 2009) and <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a> (NES, 2014).</p><p>In this approach, we want to use all of the information from each member of the population, good or bad, for estimating a gradient signal that can move the entire population to a better direction in the next generation. Since we are estimating a gradient, we can also use this gradient in a standard SGD update rule typically used for deep learning. We can even use this estimated gradient with Momentum SGD, RMSProp, or Adam if we want to.</p><p>The idea is to maximise the <em>expected value</em> of the fitness score of a sampled solution. If the expected result is good enough, then the best performing member within a sampled population will be even better, so optimising for the expectation might be a sensible approach. Maximising the expected fitness score of a sampled solution is almost the same as maximising the total fitness score of the entire population.</p><p>If $z$ is a solution vector sampled from a probability distribution function $\pi(z, \theta)$, we can define the expected value of the objective function $F$ as:<br>$$<br>J(\theta) = E_{\theta}[F(z)] = \int F(z) \; \pi(z, \theta) \; dz,<br>$$<br>where $\theta$ are the parameters of the probability distribution function. For example, if $\pi$ is a normal distribution, then $\theta$ would be \muμand $\sigma$. For our simple 2D toy problems, each ensemble $z$ is a 2D vector $(x, y)$.</p><p>The <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">NES paper</a> contains a nice derivation of the gradient of $J(\theta)$ with respect to $\theta$. Using the same <em>log-likelihood trick</em> as in the REINFORCE algorithm allows us to calculate the gradient of $J(\theta)$:<br>$$<br>\nabla_{\theta} J(\theta) = E_{\theta}[ \; F(z) \; \nabla_{\theta} \log \pi(z, \theta) \; ].<br>$$<br>In a population size of $N$, where we have solutions $z^1, z^2, … z^N$, we can estimate this gradient as a summation:<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).<br>$$<br>With this gradient $\nabla_{\theta} J(\theta)$, we can use a learning rate parameter \alphaα (such as 0.01) and start optimising the $\theta$ parameters of pdf $\pi$ so that our sampled solutions will likely get higher fitness scores on the objective function $F$. Using SGD (or Adam), we can update $\theta$ for the next generation:<br>$$<br>\theta \rightarrow \theta + \alpha \nabla_{\theta} J(\theta),<br>$$<br>and sample a new set of candidate solutions $z$ from this updated pdf, and continue until we arrive at a satisfactory solution.</p><p>In Section 6 of the <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> paper, Williams derived closed-form formulas of the gradient $\nabla_{\theta} \log \pi(z^i, \theta)$, for the special case where $ \pi(z, \theta)$ is a factored multi-variate normal distribution (i.e., the correlation parameters are zero). In this special case, $\theta$ are the $\mu$ and $\sigma$ vectors. Therefore, each element of a solution can be sampled from a univariate normal distribution $z_j \sim N(\mu_j, \sigma_j)$.</p><p>The closed-form formulas for $\nabla_{\theta} \log N(z^i, \theta)$, for each individual element of vector $\theta$ on each solution $i$ in the population can be derived as:<br>$$<br>\nabla_{\mu_{j}} \log N(z^i, \mu, \sigma) = \frac{z_j^i - \mu_j}{\sigma_j},<br>$$</p><p>$$<br>\nabla_{\sigma_{j}} \log N(z^i, \mu, \sigma) = \frac{(z_j^i - \mu_j)^2 - \sigma_j^2}{\sigma_j^3}.<br>$$</p><p>For clarity, I use the index of jj, to count across parameter space, and this is not to be confused with superscript $i$, used to count across each sampled member of the population. For our 2D problems, $z_1 = x, z_2 = y, \mu_1 = \mu_x, \mu_2 = \mu_y, \sigma_1 = \sigma_x, \sigma_2 = \sigma_y$ in this context.</p><p>These two formulas can be plugged back into the approximate gradient formula to derive explicit update rules for \muμ and \sigmaσ. In the papers mentioned above, they derived more explicit update rules, incorporated a <em>baseline</em>, and introduced other tricks such as antithetic sampling in PEPG, which is what my implementation is based on. NES proposed incorporating the inverse of the Fisher Information Matrix into the gradient update rule. But the concept is basically the same as other ES algorithms, where we update the mean and standard deviation of a multi-variate normal distribution at each new generation, and sample a new set of solutions from the updated distribution. Below is a visualization of this algorithm in action, following the formulas described above:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/pepg.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/pepg.gif" alt="img"></p><p>We see that this algorithm is able to dynamically change the $\sigma$’s to explore or fine tune the solution space as needed. Unlike CMA-ES, there is no correlation structure in our implementation, so we don’t get the diagonal ellipse samples, only the vertical or horizontal ones, although in principle we can derive update rules to incorporate the entire covariance matrix if we needed to, at the expense of computational efficiency.</p><p>I like this algorithm because like CMA-ES, the $\sigma$’s can adapt so our search space can be expanded or narrowed over time. Because the correlation parameter is not used in this implementation, the efficiency of the algorithm is $O(N)$ so I use PEPG if the performance of CMA-ES becomes an issue. I usually use PEPG when the number of model parameters exceed several thousand.</p><h2 id="OpenAI-Evolution-Strategy"><a href="#OpenAI-Evolution-Strategy" class="headerlink" title="OpenAI Evolution Strategy"></a>OpenAI Evolution Strategy</h2><p>In OpenAI’s <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">paper</a>, they implement an evolution strategy that is a special case of the REINFORCE-ES algorithm outlined earlier. In particular, \sigmaσ is fixed to a constant number, and only the \muμ parameter is updated at each generation. Below is how this strategy looks like, with a constant \sigmaσ parameter:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/openes.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/oes.gif" alt="img"></p><p>In addition to the simplification, this paper also proposed a modification of the update rule that is suitable for parallel computation across different worker machines. In their update rule, a large grid of random numbers have been pre-computed using a fixed seed. By doing this, each worker can reproduce the parameters of every other worker over time, and each worker needs only to communicate a single number, the final fitness result, to all of the other workers. This is important if we want to scale evolution strategies to thousands or even a million workers located on different machines, since while it may not be feasible to transmit an entire solution vector a million times at each generation update, it may be feasible to transmit only the final fitness results. In the paper, they showed that by using 1440 workers on Amazon EC2 they were able to solve the Mujoco Humanoid walking task in ~ 10 minutes.</p><p>I think in principle, this parallel update rule should work with the original algorithm where they can also adapt $\sigma$, but perhaps in practice, they wanted to keep the number of moving parts to a minimum for large-scale parallel computing experiments. This inspiring paper also discussed many other practical aspects of deploying ES for RL-style tasks, and I highly recommend going through it to learn more.</p><h2 id="Fitness-Shaping"><a href="#Fitness-Shaping" class="headerlink" title="Fitness Shaping"></a>Fitness Shaping</h2><p>Most of the algorithms above are usually combined with a <em>fitness shaping</em> method, such as the rank-based fitness shaping method I will discuss here. Fitness shaping allows us to avoid outliers in the population from dominating the approximate gradient calculation mentioned earlier:<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).<br>$$<br>If a particular $F(z^m)$ is much larger than other $F(z^i)$ in the population, then the gradient might become dominated by this outliers and increase the chance of the algorithm being stuck in a local optimum. To mitigate this, one can apply a rank transformation of the fitness. Rather than use the actual fitness function, we would rank the results and use an augmented fitness function which is proportional to the solution’s rank in the population. Below is a comparison of what the original set of fitness may look like, and what the ranked fitness looks like:</p><p><img src="http://blog.otoro.net/assets/20171031/ranked_fitness.svg" alt="img"></p><p>What this means is supposed we have a population size of 101. We would evaluate each population to the actual fitness function, and then sort the solutions based by their fitness. We will assign an augmented fitness value of -0.50 to the worse performer, -0.49 to the second worse solution, …, 0.49 to the second best solution, and finally a fitness value of 0.50 to the best solution. This augmented set of fitness values will be used to calculate the gradient update, instead of the actual fitness values. In a way, it is a similar to just applying Batch Normalization to the results, but more direct. There are alternative methods for fitness shaping but they all basically give similar results in the end.</p><p>I find fitness shaping to be very useful for RL tasks if the objective function is non-deterministic for a given policy network, which is often the cases on RL environments where maps are randomly generated and various opponents have random policies. It is less useful for optimising for well-behaved functions that are deterministic, and the use of fitness shaping can sometimes slow down the time it takes to find a good solution.</p><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><p>Although ES might be a way to search for more novel solutions that are difficult for gradient-based methods to find, it still vastly underperforms gradient-based methods on many problems where we can calculate high quality gradients. For instance, only an idiot would attempt to use a genetic algorithm for image classification. But sometimes <a href="https://blog.openai.com/nonlinear-computation-in-linear-networks/" target="_blank" rel="external">such people</a> do exist in the world, and sometimes these explorations can be fruitful!</p><p>Since all ML algorithms should be tested on MNIST, I also tried to apply these various ES algorithms to find weights for a small, simple 2-layer convnet used to classify MNIST, just to see where we stand compared to SGD. The convnet only has ~ 11k parameters so we can accommodate the slower CMA-ES algorithm. The code and the experiments are available <a href="https://github.com/hardmaru/pytorch_notebooks/tree/master/mnist_es" target="_blank" rel="external">here</a>.</p><p>Below are the results for various ES methods, using a population size of 101, over 300 epochs. We keep track of the model parameters that performed best on the entire training set at the end of each epoch, and evaluate this model once on the test set after 300 epochs. It is interesting how sometimes the test set’s accuracy is higher than the training set for the models that have lower scores.</p><table><thead><tr><th>Method</th><th>Train Set</th><th>Test Set</th></tr></thead><tbody><tr><td>Adam (BackProp) Baseline</td><td>99.8</td><td>98.9</td></tr><tr><td>Simple GA</td><td>82.1</td><td>82.4</td></tr><tr><td>CMA-ES</td><td>98.4</td><td>98.1</td></tr><tr><td>OpenAI-ES</td><td>96.0</td><td>96.2</td></tr><tr><td>PEPG</td><td>98.5</td><td>98.0</td></tr></tbody></table><p><img src="http://blog.otoro.net/assets/20171031/mnist_results.svg" alt="img"></p><p>We should take these results with a grain of salt, since they are based on a single run, rather than the average of 5-10 runs. The results based on a single-run seem to indicate that CMA-ES is the best at the MNIST task, but the PEPG algorithm is not that far off. Both of these algorithms achieved ~ 98% test accuracy, 1% lower than the SGD/ADAM baseline. Perhaps the ability to dynamically alter its covariance matrix, and standard deviation parameters over each generation allowed it to fine-tune its weights better than OpenAI’s simpler variation.</p><h2 id="Try-It-Yourself"><a href="#Try-It-Yourself" class="headerlink" title="Try It Yourself"></a>Try It Yourself</h2><p>There are probably open source implementations of all of the algorithms described in this article. The author of CMA-ES, Nikolaus Hansen, has been maintaining a numpy-based implementation of <a href="https://github.com/CMA-ES/pycma" target="_blank" rel="external">CMA-ES</a> with lots of bells and whistles. His python implementation introduced me to the training loop interface described earlier. Since this interface is quite easy to use, I also implemented the other algorithms such as Simple Genetic Algorithm, PEPG, and OpenAI’s ES using the same interface, and put it in a small python file called <code>es.py</code>, and also wrapped the original CMA-ES library in this small library. This way, I can quickly compare different ES algorithms by just changing one line:</p><hr><p><code>import es</code></p><p><code>#solver = es.SimpleGA(...)</code><br><code>#solver = es.PEPG(...)</code><br><code>#solver = es.OpenES(...)</code><br><code>solver = es.CMAES(...)</code></p><p><code>while True:</code></p><p><code>solutions = solver.ask()</code></p><p><code>fitness_list = np.zeros(solver.popsize)</code></p><p><code>for i in range(solver.popsize):</code><br><code>fitness_list[i] = evaluate(solutions[i])</code></p><p><code>solver.tell(fitness_list)</code></p><p><code>result = solver.result()</code></p><p><code>if result[1] &gt; MY_REQUIRED_FITNESS:</code><br><code>break</code></p><hr><p>You can look at <code>es.py</code> on <a href="https://github.com/hardmaru/estool/blob/master/es.py" target="_blank" rel="external">GitHub</a> and the IPython notebook <a href="https://github.com/hardmaru/estool/blob/master/simple_es_example.ipynb" target="_blank" rel="external">examples</a> using the various ES algorithms.</p><p>In this <a href="https://github.com/hardmaru/estool/blob/master/simple_es_example.ipynb" target="_blank" rel="external">IPython notebook</a> that accompanies <code>es.py</code>, I show how to use the ES solvers in <code>es.py</code> to solve a 100-Dimensional version of the Rastrigin function with even more local optimum points. The 100-D version is somewhat more challenging than the trivial 2D version used to produce the visualizations in this article. Below is a comparison of the performance for various algorithms discussed:</p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin10d.svg" alt="img"></p><p>On this 100-D Rastrigin problem, none of the optimisers got to the global optimum solution, although CMA-ES comes close. CMA-ES blows everything else away. PEPG is in 2nd place, and OpenAI-ES / Genetic Algorithm falls behind. I had to use an annealing schedule to gradually lower \sigmaσ for OpenAI-ES to make it perform better for this task.</p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin_cma_solution.png" alt="img"></p><p><em>Final solution that CMA-ES discovered for 100-D Rastrigin function.Global optimal solution is a 100-dimensional vector of exactly 10.</em></p><h2 id="References-and-Other-Links"><a href="#References-and-Other-Links" class="headerlink" title="References and Other Links"></a>References and Other Links</h2><p>Below are a few links to information related to evolutionary computing which I found useful or inspiring.</p><p>Image Credits of <a href="https://www.reddit.com/r/CryptoMarkets/comments/6qpla3/investing_in_icos_results_may_vary/" target="_blank" rel="external">Lemmings Jumping off a Cliff</a>. Your results may vary when investing in ICOs.</p><p>CMA-ES: <a href="https://github.com/CMA-ES" target="_blank" rel="external">Official Reference Implementation</a> on GitHub, <a href="https://arxiv.org/abs/1604.00772" target="_blank" rel="external">Tutorial</a>, Original CMA-ES <a href="http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaartic.pdf" target="_blank" rel="external">Paper</a> from 2001, Overview <a href="https://www.slideshare.net/OsamaSalaheldin2/cmaes-presentation" target="_blank" rel="external">Slides</a></p><p><a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</a> (REINFORCE), 1992.</p><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Parameter-Exploring Policy Gradients</a>, 2009.</p><p><a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a>, 2014.</p><p><a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a>, OpenAI, 2017.</p><p>Risto Miikkulainen’s <a href="http://nn.cs.utexas.edu/downloads/slides/miikkulainen.ijcnn13.pdf" target="_blank" rel="external">Slides</a> on Neuroevolution.</p><p>A Neuroevolution Approach to <a href="http://www.cs.utexas.edu/~ai-lab/?atari" target="_blank" rel="external">General Atari Game Playing</a>, 2013.</p><p>Kenneth Stanley’s Talk on <a href="https://youtu.be/dXQPL9GooyI" target="_blank" rel="external">Why Greatness Cannot Be Planned: The Myth of the Objective</a>, 2015.</p><p><a href="https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning" target="_blank" rel="external">Neuroevolution</a>: A Different Kind of Deep Learning. The quest to evolve neural networks through evolutionary algorithms.</p><p><a href="http://people.idsia.ch/~juergen/compressednetworksearch.html" target="_blank" rel="external">Compressed Network Search</a> Finds Complex Neural Controllers with a Million Weights.</p><p>Karl Sims <a href="https://youtu.be/JBgG_VSP7f8" target="_blank" rel="external">Evolved Virtual Creatures</a>, 1994.</p><p>Evolved <a href="https://youtu.be/euFvRfQRbLI" target="_blank" rel="external">Step Climbing</a> Creatures.</p><p>Super Mario World Agent <a href="https://youtu.be/qv6UVOQ0F44" target="_blank" rel="external">Mario I/O</a>, Mario Kart 64 <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/(https://github.com/nicknlsn/MarioKart64NEAT" target="_blank" rel="external">Controller using</a>) using <a href="https://www.cs.ucf.edu/~kstanley/neat.html" target="_blank" rel="external">NEAT Algorithm</a>.</p><p><a href="http://www.bionik.tu-berlin.de/institut/xstart.htm" target="_blank" rel="external">Ingo Rechenberg</a>, the inventor of Evolution Strategies.</p><p>A Tutorial on <a href="https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/" target="_blank" rel="external">Differential Evolution</a> with Python.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Source post is &lt;a href=&quot;http://blog.otoro.net/2017/10/29/visual-evolution-strategies/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;im
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Genetic Algorithm</title>
    <link href="http://yoursite.com/2018/01/23/Introduction-to-Genetic-Algorithm/"/>
    <id>http://yoursite.com/2018/01/23/Introduction-to-Genetic-Algorithm/</id>
    <published>2018-01-23T07:13:09.000Z</published>
    <updated>2018-01-23T07:14:51.190Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Intuition-behind-Genetic-Algorithms"><a href="#1-Intuition-behind-Genetic-Algorithms" class="headerlink" title="1. Intuition behind Genetic Algorithms"></a>1. Intuition behind Genetic Algorithms</h2><p>Let’s start with the famous quote by Charles Darwin:</p><blockquote><p><em>It is not the strongest of the species that survives, nor the most intelligent , but the one most responsive to change.</em></p></blockquote><p>You must be thinking what has this quote got to do with genetic algorithm? Actually, the entire concept of a genetic algorithm is based on the above line.</p><p>Let us understand with a basic example:</p><p>Let’s take a hypothetical situation where, you are head of a country, and in order to keep your city safe from bad things, you implement a policy like this.</p><ul><li>You select all the good people, and ask them to extend their generation by having their children.</li><li>This repeats for a few generations.</li><li>You will notice that now you have an entire population of good people.</li></ul><p>Now, that may not be entirely possible, but this example was just to help you understand the concept. So the basic idea was that we changed the input (i.e. population) such that we get better output (i.e. better country).</p><p>Now, I suppose you have got some intuition that the concept of a genetic algorithm is somewhat related to biology. So let’s us quickly grasp some little concepts, so that we can draw a parallel line between them.</p><h2 id="2-Biological-Inspiration"><a href="#2-Biological-Inspiration" class="headerlink" title="2. Biological Inspiration"></a>2. Biological Inspiration</h2><p>I am sure you would remember:</p><p><em>Cells are the basic building block of all living things.</em></p><p>Therefore in each cell, there is the same set of chromosomes. Chromosome are basically the strings of DNA.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153917/dna-300x194.png" alt="img"></p><p>Traditionally, these chromosomes are represented in binary as strings of 0’s and 1’s.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153928/gene.png" alt="img"></p><p>Source : <a href="https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_fundamentals.htm" target="_blank" rel="external">link</a></p><p>A chromosome consists of genes, commonly referred as blocks of DNA, where each gene encodes a specific trait, for example hair color or eye color.</p><p>I wanted you to recall these basics concept of biology before going further. Let’s get back and understand what actually is a genetic algorithm?</p><h2 id="3-What-is-a-Genetic-Algorithm"><a href="#3-What-is-a-Genetic-Algorithm" class="headerlink" title="3. What is a Genetic Algorithm?"></a>3. What is a Genetic Algorithm?</h2><p>Let’s get back to the example we discussed above and summarize what we did.</p><ol><li>Firstly, we defined our initial population as our countrymen.</li><li>We defined a function to classify whether is a person is good or bad.</li><li>Then we selected good people for mating to produce their off-springs.</li><li>And finally, these off-springs replace the bad people from the population and this process repeats.</li></ol><p>This is how genetic algorithm actually works, which basically tries to mimic the human evolution to some extent.</p><p>So to formalize a definition of a genetic algorithm, we can say that it is an optimization technique, which tries to find out such values of input so that we get the best output values or results.</p><p>The working of a genetic algorithm is also derived from biology, which is as shown in the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154007/steps-210x300.png" alt="img"></p><p>Source: <a href="https://www.neuraldesigner.com/blog/genetic_algorithms_for_feature_selection" target="_blank" rel="external">link</a></p><p>So, let us try to understand the steps one by one.</p><h2 id="4-Steps-Involved-in-Genetic-Algorithm"><a href="#4-Steps-Involved-in-Genetic-Algorithm" class="headerlink" title="4. Steps Involved in Genetic Algorithm"></a><strong>4. Steps Involved in Genetic Algorithm</strong></h2><p>Here, to make things easier, let us understand it by the famous <a href="https://en.wikipedia.org/wiki/Knapsack_problem" target="_blank" rel="external">Knapsack problem</a>.</p><p>If you haven’t come across this problem, let me introduce my version of this problem.</p><p>Let’s say, you are going to spend a month in the wilderness. Only thing you are carrying is the backpack which can hold a maximum weight of <strong>30 kg</strong>. Now you have different survival items, each having its own “Survival Points” (which are given for each item in the table). So, your objective is maximise the survival points.</p><p>Here is the table giving details about each item.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154400/table1-300x99.png" alt="img"></p><h3 id="4-1-Initialisation"><a href="#4-1-Initialisation" class="headerlink" title="4.1 Initialisation"></a>4.1 Initialisation</h3><p>To solve this problem using genetic algorithm, our first step would be defining our population. So our population will contain individuals, each having their own set of chromosomes.</p><p>We know that, chromosomes are binary strings, where for this problem 1 would mean that the following item is taken and 0 meaning that it is dropped.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22170200/Capture1-300x186.png" alt="img"></p><p>This set of chromosome is considered as our initial population.</p><h3 id="4-2-Fitness-Function"><a href="#4-2-Fitness-Function" class="headerlink" title="4.2 Fitness Function"></a>4.2 Fitness Function</h3><p>Let us calculate fitness points for our first two chromosomes.</p><p>For A1 chromosome [100110],</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/24105541/table-300x74.png" alt="img"></p><p>Similarly for A2 chromosome [001110],</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154417/table3-300x74.png" alt="img"></p><p>So, for this problem, our chromosome will be considered as more fit when it contains more survival points.</p><p>Therefore chromosome 1 is more fit than chromosome 2.</p><h3 id="4-3-Selection"><a href="#4-3-Selection" class="headerlink" title="4.3 Selection"></a>4.3 Selection</h3><p>Now, we can select fit chromosomes from our population which can mate and create their off-springs.</p><p>General thought is that we should select the fit chromosomes and allow them to produce off-springs. But that would lead to chromosomes that are more close to one another in a few next generation, and therefore less diversity.</p><p>Therefore, we generally use Roulette Wheel Selection method.</p><p>Don’t be afraid of name, just take a look at the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153953/roulette-wheel-300x188.jpg" alt="img"></p><p>I suppose we all have seen this, either in real or in movies. So, let’s build our roulette wheel.</p><p>Consider a wheel, and let’s divide that into m divisions, where m is the number of chromosomes in our populations. The area occupied by each chromosome will be proportional to its fitness value.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22155317/table4-300x72.png" alt="img"></p><p>Based on these values, let us create our roulette wheel.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22171149/roulette-300x204.png" alt="img"></p><p>So, now this wheel is rotated and the region of wheel which comes in front of the fixed point is chosen as the parent. For the second parent, the same process is repeated.</p><p>Sometimes we mark two fixed point as shown in the figure below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22171418/stio-300x194.png" alt="img"></p><p>So, in this method we can get both our parents in one go. This method is known as Stochastic Universal Selection method.</p><h3 id="4-4-Crossover"><a href="#4-4-Crossover" class="headerlink" title="4.4 Crossover"></a>4.4 Crossover</h3><p>So in this previous step, we have selected parent chromosomes that will produce off-springs. So in biological terms, crossover is nothing but reproduction.</p><p>So let us find the crossover of chromosome 1 and 4, which were selected in the previous step. Take a look at the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22173337/one-point-300x93.png" alt="img"></p><p>This is the most basic form of crossover, known as one point crossover. Here we select a random crossover point and the tails of both the chromosomes are swapped to produce a new off-springs.</p><p>If you take two crossover point, then it will called as multi point crossover which is as shown below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22174145/multi-point-300x101.png" alt="img"></p><h3 id="4-5-Mutation"><a href="#4-5-Mutation" class="headerlink" title="4.5 Mutation"></a>4.5 Mutation</h3><p>Now if you think in the biological sense, are the children produced have the same traits as their parents? The answer is NO. During their growth, there is some change in the genes of children which makes them different from its parents.</p><p>This process is known as mutation, which may be defined as a random tweak in the chromosome, which also promotes the idea of diversity in the population.</p><p>A simple method of mutation is shown in the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22174928/mutation-300x56.png" alt="img"></p><p>So the entire process is summarise as shown in the figure.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22175311/gadiagram-300x196.png" alt="img"></p><p>Source : <a href="http://www.jade-cheng.com/au/coalhmm/optimization/" target="_blank" rel="external">link</a></p><p>The off-springs thus produced are again validated using our fitness function, and if considered fit then will replace the less fit chromosomes from the population.</p><p>But the question is how we will get to know that we have reached our best possible solution?</p><p>So basically there are different termination conditions, which are listed below:</p><ol><li>There is no improvement in the population for over x iterations.</li><li>We have already predefined an absolute number of generation for our algorithm.</li><li>When our fitness function has reached a predefined value.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Intuition-behind-Genetic-Algorithms&quot;&gt;&lt;a href=&quot;#1-Intuition-behind-Genetic-Algorithms&quot; class=&quot;headerlink&quot; title=&quot;1. Intuition behin
    
    </summary>
    
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]</title>
    <link href="http://yoursite.com/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/"/>
    <id>http://yoursite.com/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/</id>
    <published>2018-01-22T08:49:43.000Z</published>
    <updated>2018-01-22T08:53:40.634Z</updated>
    
    <content type="html"><![CDATA[<p>Source blog is <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">here</a>.</p><hr><p>We’ve <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">discovered</a> that <strong>evolution strategies (ES)</strong>, an optimization technique that’s been known for decades, rivals the performance of standard <strong>reinforcement learning (RL)</strong>techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL’s inconveniences.</p><p>In particular, ES is simpler to implement (there is no need for <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">backpropagation</a>), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer <a href="https://www.quora.com/What-are-hyperparameters-in-machine-learning" target="_blank" rel="external">hyperparameters</a>. This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on <a href="https://en.wikipedia.org/wiki/Finite_difference" target="_blank" rel="external">finite differences</a> along a few random directions at each step.</p><p>Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">“AlexNet” paper</a> showed how to design, scale and train convolutional neural networks (CNNs) to achieve extremely strong results on image recognition tasks, at a time when most researchers thought that CNNs were not a promising approach to computer vision. Similarly, in 2013, the <a href="https://arxiv.org/abs/1312.5602" target="_blank" rel="external">Deep Q-Learning paper</a> showed how to combine Q-Learning with CNNs to successfully solve Atari games, reinvigorating RL as a research field with exciting experimental (rather than theoretical) results. Likewise, our work demonstrates that ES achieves strong performance on RL benchmarks, dispelling the common belief that ES methods are impossible to apply to high dimensional problems.</p><p>ES is easy to implement and scale. Running on a computing cluster of 80 machines and 1,440 CPU cores, our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes about 10 hours). Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour.</p><p>In what follows, we’ll first briefly describe the conventional RL approach, contrast that with our ES approach, discuss the tradeoffs between ES and RL, and finally highlight some of our experiments.</p><h4 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h4><p>Let’s briefly look at how RL works. Suppose we are given some environment (e.g. a game) that we’d like to train an agent on. To describe the behavior of the agent, we define a policy function (the brain of the agent), which computes how the agent should act in any given situation. In practice, the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions. A typical policy function might have about 1,000,000 parameters, so our task comes down to finding the precise setting of these parameters such that the policy plays well (i.e. wins a lot of games).</p><p><img src="https://blog.openai.com/content/images/2017/03/first-graphic-1.png" alt="img"></p><p><em>Above: In the game of Pong, the policy could take the pixels of the screen and compute the probability of moving the player’s paddle (in green, on right) Up, Down, or neither.</em></p><p>The training process for the policy works as follows. Starting from a random initialization, we let the agent interact with the environment for a while and collect episodes of interaction (e.g. each episode is one game of Pong). We thus obtain a complete recording of what happened: what sequence of states we encountered, what actions we took in each state, and what the reward was at each step. As an example, below is a diagram of three episodes that each took 10 time steps in a hypothetical environment. Each rectangle is a state, and rectangles are colored green if the reward was positive (e.g. we just got the ball past our opponent) and red if the reward was negative (e.g. we missed the ball):</p><p><img src="https://blog.openai.com/content/images/2017/03/second-graphic-1.png" alt="img"></p><p>This diagram suggests a recipe for how we can improve the policy; whatever we happened to do leading up to the green states was good, and whatever we happened to do in the states leading up to the red areas was bad. We can then use backpropagation to compute a small update on the network’s parameters that would make the green actions more likely in those states in the future, and the red actions less likely in those states in the future. We expect that the updated policy works a bit better as a result. We then iterate the process: collect another batch of episodes, do another update, etc.</p><p><strong>Exploration by injecting noise in the actions.</strong> The policies we usually use in RL are stochastic, in that they only compute probabilities of taking any action. This way, during the course of training, the agent may find itself in a particular state many times, and at different times it will take different actions due to the sampling. This provides the signal needed for learning; some of those actions will lead to good outcomes, and get encouraged, and some of them will not work out, and get discouraged. We therefore say that we introduce exploration into the learning process by injecting noise into the agent’s actions, which we do by sampling from the action distribution at each time step. This will be in contrast to ES, which we describe next.</p><h4 id="Evolution-Strategies"><a href="#Evolution-Strategies" class="headerlink" title="Evolution Strategies"></a>Evolution Strategies</h4><p><strong>On “Evolution”.</strong> Before we dive into the ES approach, it is important to note that despite the word “evolution”, ES has very little to do with biological evolution. Early versions of these techniques may have been inspired by biological evolution and the approach can, on an abstract level, be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations. However, the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black-box stochastic optimization techniques.</p><p><strong>Black-box optimization.</strong> In ES, we forget entirely that there is an agent, an environment, that there are neural networks involved, or that interactions take place over time, etc. The whole setup is that 1,000,000 numbers (which happen to describe the parameters of the policy network) go in, 1 number comes out (the total reward), and we want to find the best setting of the 1,000,000 numbers. Mathematically, we would say that we are optimizing a function <code>f(w)</code> with respect to the input vector <code>w</code>(the parameters / weights of the network), but we make no assumptions about the structure of <code>f</code>, except that we can evaluate it (hence “black box”).</p><p><strong>The ES algorithm.</strong> Intuitively, the optimization is a “guess and check” process, where we start with some random parameters and then repeatedly 1) tweak the guess a bit randomly, and 2) move our guess slightly towards whatever tweaks worked better. Concretely, at each step we take a parameter vector <code>w</code> and generate a population of, say, 100 slightly different parameter vectors <code>w1 ... w100</code> by jittering <code>w</code> with gaussian noise. We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while, and add up all the rewards in each case. The updated parameter vector then becomes the weighted sum of the 100 vectors, where each weight is proportional to the total reward (i.e. we want the more successful candidates to have a higher weight). Mathematically, you’ll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences, except we only do it along 100 random directions. Yet another way to see it is that we’re still doing RL (Policy Gradients, or <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> specifically), where the agent’s actions are to emit entire parameter vectors using a gaussian policy.</p><p><img src="https://blog.openai.com/content/images/2017/03/evo.png" alt="img"></p><p><em>Above: ES optimization process, in a setting with only two parameters and a reward function (red = high, blue = low). At each iteration we show the current parameter value (in white), a population of jittered samples (in black), and the estimated gradient (white arrow). We keep moving the parameters to the top of the arrow until we converge to a local optimum. You can reproduce this figure with this notebook.</em></p><p><strong>Code sample.</strong> To make the core algorithm concrete and to highlight its simplicity, here is a short example of optimizing a quadratic function using ES (or see this <a href="https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d" target="_blank" rel="external">longer version</a> with more comments):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># simple example: minimize a quadratic around some solution point</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">solution = np.array([<span class="number">0.5</span>, <span class="number">0.1</span>, <span class="number">-0.3</span>])</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(w)</span>:</span> <span class="keyword">return</span> -np.sum((w - solution)**<span class="number">2</span>)</div><div class="line"></div><div class="line">npop = <span class="number">50</span>      <span class="comment"># population size</span></div><div class="line">sigma = <span class="number">0.1</span>    <span class="comment"># noise standard deviation</span></div><div class="line">alpha = <span class="number">0.001</span>  <span class="comment"># learning rate</span></div><div class="line">w = np.random.randn(<span class="number">3</span>) <span class="comment"># initial guess</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</div><div class="line">  N = np.random.randn(npop, <span class="number">3</span>)</div><div class="line">  R = np.zeros(npop)</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(npop):</div><div class="line">    w_try = w + sigma*N[j]</div><div class="line">    R[j] = f(w_try)</div><div class="line">  A = (R - np.mean(R)) / np.std(R)</div><div class="line">  w = w + alpha/(npop*sigma) * np.dot(N.T, A)</div></pre></td></tr></table></figure><p><strong>Injecting noise in the parameters.</strong> Notice that the objective is identical to the one that RL optimizes: the expected reward. However, RL injects noise in the action space and uses backpropagation to compute the parameter updates, while ES injects noise directly in the parameter space. Another way to describe this is that RL is a “guess and check” on actions, while ES is a “guess and check” on parameters. Since we’re injecting noise in the parameters, it is possible to use deterministic policies (and we do, in our experiments). It is also possible to add noise in both actions and parameters to potentially combine the two approaches.</p><h4 id="Tradeoffs-between-ES-and-RL"><a href="#Tradeoffs-between-ES-and-RL" class="headerlink" title="Tradeoffs between ES and RL"></a>Tradeoffs between ES and RL</h4><p>ES enjoys multiple advantages over RL algorithms (some of them are a little technical):</p><ul><li><strong>No need for backpropagation</strong>. ES only requires the forward pass of the policy and does not require backpropagation (or value function estimation), which makes the code shorter and between 2-3 times faster in practice. On memory-constrained systems, it is also not necessary to keep a record of the episodes for a later update. There is also no need to worry about exploding gradients in RNNs. Lastly, we can explore a much larger function class of policies, including networks that are not differentiable (such as in binary networks), or ones that include complex modules (e.g. pathfinding, or various optimization layers).</li><li><strong>Highly parallelizable.</strong> ES only requires workers to communicate a few scalars between each other, while in RL it is necessary to synchronize entire parameter vectors (which can be millions of numbers). Intuitively, this is because we control the random seeds on each worker, so each worker can locally reconstruct the perturbations of the other workers. Thus, all that we need to communicate between workers is the reward of each perturbation. As a result, we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization.</li><li><strong>Higher robustness.</strong> Several hyperparameters that are difficult to set in RL implementations are side-stepped in ES. For example, RL is not “scale-free”, so one can achieve very different learning outcomes (including a complete failure) with different settings of the frame-skip hyperparameter in Atari. As we show in our work, ES works about equally well with any frame-skip.</li><li><strong>Structured exploration.</strong> Some RL algorithms (especially policy gradients) initialize with random policies, which often manifests as random jitter on spot for a long time. This effect is mitigated in Q-Learning due to epsilon-greedy policies, where the max operation can cause the agents to perform some consistent action for a while (e.g. holding down a left arrow). This is more likely to do something in a game than if the agent jitters on spot, as is the case with policy gradients. Similar to Q-learning, ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration.</li><li><strong>Credit assignment over long time scales.</strong> By studying both ES and RL gradient estimators mathematically we can see that ES is an attractive choice especially when the number of time steps in an episode is long, where actions have longlasting effects, or if no good value function estimates are available.</li></ul><p>Conversely, we also found some challenges to applying ES in practice. One core problem is that in order for ES to work, adding noise in parameters must lead to different outcomes to obtain some gradient signal. As we elaborate on in our paper, we found that the use of virtual batchnorm can help alleviate this problem, but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary. As an example of a related difficulty, we found that in Montezuma’s Revenge, one is very unlikely to get the key in the first level with a random network, while this is occasionally possible with random actions.</p><h4 id="ES-is-competitive-with-RL"><a href="#ES-is-competitive-with-RL" class="headerlink" title="ES is competitive with RL"></a>ES is competitive with RL</h4><p>We compared the performance of ES and RL on two standard RL benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo task (see examples below) contains a physically-simulated articulated figure, where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward. Below are some example agents trained on three MuJoCo control tasks, where the objective is to move forward:</p><p><img src="https://blog.openai.com/content/images/2017/03/out.gif" alt="img"></p><p>We usually compare the performance of algorithms by looking at their efficiency of learning from data; as a function of how many states we’ve seen, what is our average reward? Here are the example learning curves that we obtain, in comparison to RL (the <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="external">TRPO</a> algorithm in this case):</p><p><img src="https://blog.openai.com/content/images/2017/03/es_vs_trpo_full.png" alt="img"></p><p><strong>Data efficiency comparison</strong>. The comparisons above show that ES (orange) can reach a comparable performance to TRPO (blue), although it doesn’t quite match or surpass it in all cases. Moreover, by scanning horizontally we can see that ES is less efficient, but no worse than about a factor of 10 (note the x-axis is in log scale).</p><p><strong>Wall clock comparison</strong>. Instead of looking at the raw number of states seen, one can argue that the most important metric to look at is the wall clock time: how long (in number of seconds) does it take to solve a given problem? This quantity ultimately dictates the achievable speed of iteration for a researcher. Since ES requires negligible communication between workers, we were able to solve one of the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 machines in only 10 minutes. As a comparison, in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours. It is also possible that the performance of RL could also improve with more algorithmic and engineering effort, but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements.</p><p>Below are a few videos of 3D humanoid walkers trained with ES. As we can see, the results have quite a bit of variety, based on which local minimum the optimization ends up converging into.</p><p><img src="https://blog.openai.com/content/images/2017/03/out-1.gif" alt="img"></p><p>On Atari, ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day. Below are some result snippets on Pong, Seaquest and Beamrider. These videos show the preprocessed frames, which is exactly what the agent sees when it is playing:</p><p><img src="https://blog.openai.com/content/images/2017/03/atari.gif" alt="img"></p><p>In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels.</p><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p>ES is an algorithm from the neuroevolution literature, which has a long history in AI and a complete literature review is beyond the scope of this post. However, we encourage an interested reader to look at <a href="https://en.wikipedia.org/wiki/Neuroevolution" target="_blank" rel="external">Wikipedia</a>, <a href="http://www.scholarpedia.org/article/Neuroevolution" target="_blank" rel="external">Scholarpedia</a>, and Jürgen Schmidhuber’s <a href="https://arxiv.org/abs/1404.7828" target="_blank" rel="external">review article (Section 6.6)</a>. The work that most closely informed our approach is <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a> by Wierstra et al. 2014. Compared to this work and much of the work it has inspired, our focus is specifically on scaling these algorithms to large-scale, distributed settings, finding components that make the algorithms work better with deep neural networks (e.g. <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="external">virtual batch norm</a>), and evaluating them on modern RL benchmarks.</p><p>It is also worth noting that neuroevolution-related approaches have seen some recent resurgence in the machine learning literature, for example with <a href="https://arxiv.org/abs/1609.09106" target="_blank" rel="external">HyperNetworks</a>, <a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="external">“Large-Scale Evolution of Image Classifiers”</a> and <a href="https://arxiv.org/abs/1606.02580" target="_blank" rel="external">“Convolution by Evolution”</a>.</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Our work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent-environment benchmarks, while offering significant benefits related to code complexity and ease of scaling to large-scale distributed settings. We also expect that more exciting work can be done by revisiting other ideas from this line of work, such as indirect encoding methods, or evolving the network structure in addition to the parameters.</p><p><strong>Note on supervised learning</strong>. It is also important to note that supervised learning problems (e.g. image classification, speech recognition, or most other tasks in the industry), where one can compute the exact gradient of the loss function with backpropagation, are not directly impacted by these findings. For example, in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1,000 times slower than using backpropagation. It is only in RL settings, where one has to estimate the gradient of the expected reward by sampling, where ES becomes competitive.</p><p><strong>Code release</strong>. Finally, if you’d like to try running ES yourself, we encourage you to dive into the full details by reading <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">our paper</a> or looking at our code on this <a href="https://github.com/openai/evolution-strategies-starter" target="_blank" rel="external">Github repo</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Source blog is &lt;a href=&quot;https://blog.openai.com/evolution-strategies/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;We’ve &lt;a href=&quot;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Seq2Seq with Attention and Beam Search [Repost]</title>
    <link href="http://yoursite.com/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/"/>
    <id>http://yoursite.com/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/</id>
    <published>2018-01-21T09:35:50.000Z</published>
    <updated>2018-01-21T10:53:57.584Z</updated>
    
    <content type="html"><![CDATA[<p>Source Post is <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">here</a></p><h2 id="Sequence-to-Sequence-basics"><a href="#Sequence-to-Sequence-basics" class="headerlink" title="Sequence to Sequence basics"></a>Sequence to Sequence basics</h2><p>Let’s explain the sequence to sequence framework as we’ll rely on it for our model. Let’s start with the simplest version on the translation task.</p><blockquote><p>As an example, let’s translate <code>how are you</code> in French <code>comment vas tu</code>.</p></blockquote><h3 id="Vanilla-Seq2Seq"><a href="#Vanilla-Seq2Seq" class="headerlink" title="Vanilla Seq2Seq"></a>Vanilla Seq2Seq</h3><p>The Seq2Seq framework relies on the <strong>encoder-decoder</strong> paradigm. The <strong>encoder</strong> <em>encodes</em> the input sequence, while the <strong>decoder</strong> <em>produces</em> the target sequence</p><p><strong>Encoder</strong></p><p>Our input sequence is <code>how are you</code>. Each word from the input sequence is associated to a vector $w \in \mathbb{R}^d$ (via a lookup table). In our case, we have 3 words, thus our input will be transformed into $[w_0, w_1, w_2] \in \mathbb{R}^{d \times 3}$. Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation $e$. Let’s write the hidden states $[e_0,e_1,e_2]$ (and thus $e=e_2$)</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg" alt="Vanilla Encoder"></p><p><em>Vanilla Encoder</em></p><p><strong>Decoder</strong></p><p>Now that we have a vector ee that captures the meaning of the input sequence, we’ll use it to generate the target sequence word by word. Feed to another LSTM cell: ee as hidden state and a special <em>start of sentence</em> vector $w_{\text{sos}}$ as input. The LSTM computes the next hidden state $h_0 \in \mathbb{R}^{h}$. Then, we apply some function $g : \mathbb{R}^h \mapsto \mathbb{R}^V$ so that $s_0 := g(h_0) \in \mathbb{R}^V$ is a vector of the same size as the vocabulary.<br>$$<br>\begin{align}<br>h_0 &amp;= \operatorname{LSTM}\left(e, w_{sos} \right)\\<br>s_0 &amp;= g(h_0)\\<br>p_0 &amp;= \operatorname{softmax}(s_0)\\<br>i_0 &amp;= \operatorname{argmax}(p_0)\\<br>\end{align}<br>$$<br>Then, apply a softmax to $s_0$ to normalize it into a vector of probabilities $p_0 \in \mathbb{R}^V$ . Now, each entry of $p_0$ will measure how likely is each word in the vocabulary. Let’s say that the word <em>“comment”</em> has the highest probability (and thus $i_0 = \operatorname{argmax}(p_0)$) corresponds to the index of <em>“comment”</em>). Get a corresponding vector and $w_{i_0} = w_{comment}$ repeat the procedure: the LSTM will take $h_0$ as hidden state and $w_{comment}$ as input and will output a probability vector $p_1$ over the second word, etc.<br>$$<br>\begin{align}<br>h_1 &amp;= \operatorname{LSTM}\left(h_0, w_{i_0} \right)\\<br>s_1 &amp;= g(h_1)\\<br>p_1 &amp;= \operatorname{softmax}(s_1)\\<br>i_1 &amp;= \operatorname{argmax}(p_1)<br>\end{align}<br>$$<br>The decoding stops when the predicted word is a special <em>end of sentence</em> token.</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg" alt="Vanilla Decoder"></p><p><em>Vanilla Decoder</em></p><blockquote><p>Intuitively, the hidden vector represents the “amount of meaning” that has not been decoded yet.</p></blockquote><p>The above method aims at modelling the distribution of the next word conditionned on the beginning of the sentence<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_1, \dots, y_{t}, x_0, \dots, x_n \right]<br>$$<br>by writing<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_t, h_{t}, e \right]<br>$$</p><h3 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h3><p>The previous model has been refined over the past few years and greatly benefited from what is known as <strong>attention</strong>. Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM. One way of performing attention is explained by <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Bahdanau et al.</a>. We slightly modify the reccurrence formula that we defined above by adding a new vector $c_t$ to the input of the LSTM<br>$$<br>\begin{align}<br>h_{t} &amp;= \operatorname{LSTM}\left(h_{t-1}, [w_{i_{t-1}}, c_t] \right)\\<br>s_t &amp;= g(h_t)\\<br>p_t &amp;= \operatorname{softmax}(s_t)\\<br>i_t &amp;= \operatorname{argmax}(p_t)<br>\end{align}<br>$$<br>The vector ctct is the attention (or <strong>context</strong>) vector. We compute a new context vector at each decoding step. First, with a function $f (h_{t-1}, e_{t’}) \mapsto \alpha_{t’} \in \mathbb{R}$, compute a score for each hidden state $e_{t^{\prime}}$ of the encoder. Then, normalize the sequence of using a softmax and compute $c_t$ as the weighted average of the $e_{t^{\prime}}$.<br>$$<br>\begin{align}<br>\alpha_{t’} &amp;= f(h_{t-1}, e_{t’}) \in \mathbb{R} &amp; \text{for all } t’\\<br>\bar{\alpha} &amp;= \operatorname{softmax} (\alpha)\\<br>c_t &amp;= \sum_{t’=0}^n \bar{\alpha}_{t’} e_{t’}<br>\end{align}<br>$$<br><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg" alt="Attention Mechanism"></p><p><em>Attention Mechanism</em></p><p>The choice of the function ff varies, but is usually one of the following<br>$$<br>f(h_{t-1}, e_{t’}) =<br>\begin{cases}<br>h_{t-1}^T e_{t’} &amp; \text{dot}\\<br>h_{t-1}^T W e_{t’} &amp; \text{general}\\<br>v^T \tanh \left(W [h_{t-1}, e_{t’}]\right) &amp; \text{concat}\\<br>\end{cases}<br>$$<br>It turns out that the attention weighs $\bar{\alpha}$ can be easily interpreted. When generating the word <code>vas</code>(corresponding to <code>are</code> in English), we expect $\bar{\alpha} _ {\text{are}}$ are to be close to $1$ while $\bar{\alpha} _ {\text{how}}$ and $\bar{\alpha} _ {\text{you}}$ to be close to $0$. Intuitively, the context vector $c$ will be roughly equal to the hidden vector of <code>are</code> and it will help to generate the French word <code>vas</code>.</p><p>By putting the attention weights into a matrix (rows = input sequence, columns = output sequence), we would have access to the <strong>alignment</strong> between the words from the English and French sentences… (see <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="external">page 6</a>) There is still a lot of things to say about sequence to sequence models (for instance, it works better if the encoder processes the input sequence <em>backwards</em>…).</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><blockquote><p>What happens if the first time step is not sure about wether it should generate <code>comment</code> or <code>vas</code> (most likely case at the beginning of the training)? Then it would mess up the entire sequence, and the model will hardly learn anything…</p></blockquote><p>If we use the predicted token as input to the next step during training (as explained above), errors would accumulate and the model would rarely be exposed to the correct distribution of inputs, making training slow or impossible. To speedup things, one trick is to feed the actual output sequence (<code>&lt;sos&gt;</code> <code>comment</code> <code>vas</code> <code>tu</code>) into the decoder’s LSTM and predict the next token at every position (<code>comment</code> <code>vas</code> <code>tu</code> <code>&lt;eos&gt;</code>).</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_training.svg" alt="Training"></p><p><em>Training</em></p><p>The decoder outputs vectors of probability over the vocabulary $p_i \in \mathbb{R}^V$ for each time step. Then, for a given target sequence $y_1, \dots, y_n$, we can compute its probability as the product of the probabilities of each token being produced at each relevant time step:<br>$$<br>\mathbb{P}\left(y_1, \dots, y_m \right) = \prod_{i=1}^m p_i [y_i]<br>$$<br>where $p_i [y_i]$ means that we extract the $y_i$-th entry of the probability vector $p_i$ from the $i$-th decoding step. In particular, we can compute the probability of the actual target sequence. A perfect system would give a probabilty of 1 to this target sequence, so we are going to train our network to maximize the probability of the target sequence, which is the same as minimizing<br>$$<br>\begin{align}<br>-\log \mathbb{P} \left(y_1, \dots, y_m \right) &amp;= - \log \prod_{i=1}^m p_i [y_i]\\<br>&amp;= - \sum_{i=1}^n \log p_i [y_i]\\<br>\end{align}<br>$$<br>in our example, this is equal to<br>$$</p><ul><li>\log p_1[\text{comment}] - \log p_2[\text{vas}] - \log p_3[\text{tu}] - \log p_4[\text{<eos>}]<br>$$<br>and you recognize the standard cross entropy: we actually are minimizing the cross entropy between the target distribution (all one-hot vectors) and the predicted distribution outputed by our model (our vectors $p_i$).</eos></li></ul><h2 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h2><p>The main takeaway from the discussion above is that for the same model, we can define different behaviors. In particular, we defined a specific behavior that speeds up training.</p><blockquote><p>What about inference/testing time then? Is there an other way to decode a sentence?</p></blockquote><p>There indeed are 2 main ways of performing decoding at testing time (translating a sentence for which we don’t have a translation). The first of these methods is the one covered at the beginning of the article: <strong>greedy decoding</strong>. It is the most natural way and it consists in feeding to the next step the most likely word predicted at the previous step.</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg" alt="Greedy Decoder - feeds the best token to the next step"></p><p><em>Greedy Decoder - feeds the best token to the next step</em></p><blockquote><p>But didn’t we say that this behavior is likely to accumulate errors?</p></blockquote><p>Even after having trained the model, it can happen that the model makes a small error (and gives a small advantage to <code>vas</code> over <code>comment</code> for the first step of the decoding). This would mess up the entire decoding…</p><p>There is a better way of performing decoding, called <strong>Beam Search</strong>. Instead of only predicting the token with the best score, we keep track of $k$ hypotheses (for example $k=5$, we refer to kk as the <strong>beam size</strong>). At each new time step, for these 5 hypotheses we have $V$ new possible tokens. It makes a total of $5V$ new hypotheses. Then, only keep the $5$ best ones, and so on… Formally, define $\mathcal{H}_t$ the set of hypotheses decoded at time step $t$.<br>$$<br>\mathcal{H}_ t := \{ (w^1_1, \dots, w^1_t), \dots, (w^k_1, \dots, w^k_t) \}<br>$$<br>For instance if $k=2$, one possible $\mathcal{H}_2$ would be<br>$$<br>\mathcal{H}_ 2 := \{ (\text{comment vas}), (\text{comment tu}) \}<br>$$<br>Now we consider all the possible candidates $\mathcal{C}_{t+1}$, produced from $\mathcal{H}_t$ by adding all possible new tokens<br>$$<br>\mathcal{C}_ {t+1} := \bigcup_{i=1}^k \{ (w^i_1, \dots, w^i_t, 1), \dots, (w^i_1, \dots, w^i_t, V) \}<br>$$<br>and keep the $k$ highest scores (probability of the sequence). If we keep our example<br>$$<br>\begin{align}<br>\mathcal{C}_ 3 =&amp; \{ (\text{comment vas comment}), (\text{comment vas vas}), (\text{comment vas tu})\} \\<br>\cup &amp; \{ (\text{comment tu comment}), \ \ (\text{comment tu vas}), \ \ (\text{comment tu tu}) \}<br>\end{align}<br>$$<br>and for instance we can imagine that the 2 best ones would be<br>$$<br>\mathcal{H}_ 3 := \{ (\text{comment vas tu}), (\text{comment tu vas}) \}<br>$$<br>Once every hypothesis reached the <code>&lt;eos&gt;</code> token, we return the hypothesis with the highest score.</p><blockquote><p>If we use <strong>beam search</strong>, a small error at the first step might be rectified at the next step, as we keep the gold hypthesis in the beam!</p></blockquote><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this article we covered the seq2seq concepts. We showed that training is different than decoding. We covered two methods for decoding: <strong>greedy</strong> and <strong>beam search</strong>. While beam search generally achieves better results, it is not perfect and still suffers from <strong>exposure bias</strong>. During training, the model is never exposed to its errors! It also suffers from <strong>Loss-Evaluation Mismatch</strong>. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</p><p>Now, let’s apply Seq2Seq for LaTeX generation from images!</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_task.svg" alt="Producing LaTeX code from an image"></p><p><em>Producing LaTeX code from an image</em></p><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>Previous part covered the concepts of <strong>sequence-to-sequence</strong> applied to Machine Translation. The same framework can be applied to our LaTeX generation problem. The input sequence would just be replaced by an image, preprocessed with some convolutional model adapted to OCR (in a sense, if we <em>unfold</em> the pixels of an image into a sequence, this is exactly the same problem). This idea proved to be efficient for image captioning (see the reference paper <a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Show, Attend and Tell</a>). Building on some <a href="https://arxiv.org/pdf/1609.04938v1.pdf" target="_blank" rel="external">great work</a> from the Harvard NLP group, my teammate <a href="https://www.linkedin.com/in/romain-sauvestre-241171a2" target="_blank" rel="external">Romain</a> and I chose to follow a similar approach.</p><blockquote><p>Keep the seq2seq framework but replace the encoder by a convolutional network over the image!</p></blockquote><p>Good Tensorflow implementations of such models were hard to find. Together with this post, I am releasing the <a href="https://github.com/guillaumegenthial/im2latex" target="_blank" rel="external">code</a> and hope some will find it useful. You can use it to train your own image captioning model or adapt it for a more advanced use. <a href="https://github.com/guillaumegenthial/im2latex" target="_blank" rel="external">The code</a> does <strong>not</strong> rely on the <a href="https://www.tensorflow.org/versions/master/api_guides/python/contrib.seq2seq" target="_blank" rel="external">Tensorflow Seq2Seq library</a> as it was not entirely ready at the time of the project and I also wanted more flexibility (but adopts a similar interface).</p><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>To train our model, we’ll need labeled examples: images of formulas along with the LaTeX code used to generate the images. A good source of LaTeX code is <a href="https://arxiv.org/" target="_blank" rel="external">arXiv</a>, that has thousands of articles under the <code>.tex</code> format. After applying some heuristics to find equations in the <code>.tex</code> files, keeping only the ones that actually compile, the <a href="https://zenodo.org/record/56198#.WflVu0yZPLZ" target="_blank" rel="external">Harvard NLP group</a> extracted $\sim 100,000$ formulas.</p><blockquote><p>Wait… Don’t you have a problem as different LaTeX codes can give the same image?</p></blockquote><p>Good point: <code>(x^2 + 1)</code> and <code>\left( x^{2} + 1 \right)</code> indeed give the same output. That’s why Harvard’s paper found out that normalizing the data using a parser (<a href="https://khan.github.io/KaTeX/" target="_blank" rel="external">KaTeX</a>) improved performance. It forces adoption of some conventions, like writing <code>x ^ { 2 }</code> instead of <code>x^2</code>, etc. After normalization, they end up with a <code>.txt</code> file containing one formula per line that looks like</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">\alpha + \beta</div><div class="line">\frac &#123; 1 &#125; &#123; 2 &#125;</div><div class="line">\frac &#123; \alpha &#125; &#123; \beta &#125;</div><div class="line">1 + 2</div></pre></td></tr></table></figure><p>From this file, we’ll produce images <code>0.png</code>, <code>1.png</code>, etc. and a matching file mapping the image files to the indices (=line numbers) of the formulas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">0.png 0</div><div class="line">1.png 1</div><div class="line">2.png 2</div><div class="line">3.png 3</div></pre></td></tr></table></figure><p>The reason why we use this format is that it is flexible and allows you to use the pre-built <a href="https://zenodo.org/record/56198#.WflVu0yZPLZ" target="_blank" rel="external">dataset from Harvard</a> (You may need to use the preprocessing scripts as explained <a href="https://github.com/harvardnlp/im2markup" target="_blank" rel="external">here</a>). You’ll also need to have <code>pdflatex</code> and <code>ImageMagick</code> installed.</p><p>We also build a vocabulary, to map LaTeX tokens to indices that will be given as input to our model. If we keep the same data as above, our vocabulary will look like</p><p><code>+</code> <code>1</code> <code>2</code> <code>\alpha</code> <code>\beta</code> <code>\frac</code> <code>{</code> <code>}</code></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>Our model is going to rely on a variation of the Seq2Seq model, adapted to images. First, let’s define the input of our graph. Not surprisingly we get as input a batch of black-and-white images of shape $[H,W]$ and a batch of formulas (ids of the LaTeX tokens):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># batch of images, shape = (batch size, height, width, 1)</span></div><div class="line">img = tf.placeholder(tf.uint8, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'img'</span>)</div><div class="line"><span class="comment"># batch of formulas, shape = (batch size, length of the formula)</span></div><div class="line">formula = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>), name=<span class="string">'formula'</span>)</div><div class="line"><span class="comment"># for padding</span></div><div class="line">formula_length = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, ), name=<span class="string">'formula_length'</span>)</div></pre></td></tr></table></figure><blockquote><p>A special note on the type of the image input. You may have noticed that we use <code>tf.uint8</code>. This is because our image is encoded in grey-levels (integers from <code>0</code> to <code>255</code> - and $2^8=256$). Even if we could give a <code>tf.float32</code> Tensor as input to Tensorflow, this would be 4 times more expensive in terms of memory bandwith. As data starvation is one of the main bottlenecks of GPUs, this simple trick can save us some training time. For further improvement of the data pipeline, have a look at <a href="https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/data" target="_blank" rel="external">the new Tensorflow data pipeline</a>.</p></blockquote><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p><strong>High-level idea</strong> Apply some convolutional network on top of the image an flatten the output into a sequence of vectors $[e_1, \cdots, e_n]$, each of those corresponding to a region of the input image. These vectors will correspond to the hidden vectors of the LSTM that we used for translation.</p><blockquote><p>Once our image is transformed into a sequence, we can use the seq2seq model!</p></blockquote><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_encoder.svg" alt="Convolutional Encoder - produces a sequence of vectors"></p><p><em>Convolutional Encoder - produces a sequence of vectors</em></p><p>We need to extract features from our image, and for this, nothing has (<a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="external">yet</a>) been proven more effective than convolutions. Here, there is nothing much to say except that we pick some architecture that has been proven to be effective for Optical Character Recognition (OCR), which stacks convolutional layers and max-pooling to produce a Tensor of shape $[H^{\prime},W^{\prime},512]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># casting the image back to float32 on the GPU</span></div><div class="line">img = tf.cast(img, tf.float32) / <span class="number">255.</span></div><div class="line"></div><div class="line">out = tf.layers.conv2d(img, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line"><span class="comment"># encoder representation, shape = (batch size, height', width', 512)</span></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"VALID"</span>, activation=tf.nn.relu)</div></pre></td></tr></table></figure><p>Now that we have extracted some features from the image, let’s <strong>unfold</strong> the image to get a sequence so that we can use our sequence to sequence framework. We end up with a sequence of length $[H^{\prime},W^{\prime}]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">H, W = tf.shape(out)[<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">seq = tf.reshape(out, shape=[<span class="number">-1</span>, H*W, <span class="number">512</span>])</div></pre></td></tr></table></figure><blockquote><p>Don’t you loose a lot of structural information by reshaping? I’m afraid that when performing attention over the image, my decoder won’t be able to understand the location of each feature vector in the original image!</p></blockquote><p>It turns out that the model manages to work despite this issue, but that’s not completely satisfying. In the case of translation, the hidden states of the LSTM contained some positional information that was computed by the LSTM (after all, LSTM are by essence sequential). Can we fix this issue?</p><p><strong>Positional Embeddings</strong> I decided to follow the idea from <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external">Attention is All you Need</a> that adds <em>positional embeddings</em> to the image representation (<code>out</code>), and has the huge advantage of not adding any new trainable parameter to our model. The idea is that for each position of the image, we compute a vector of size $512$ such that its components are coscos or sinsin. More formally, the (2i)-th and (2i+1)-th entries of my positional embedding $v$ at position $p$ will be<br>$$<br>\begin{align}<br>v_{2i} &amp;= \sin\left(p / f^{2i}\right)\\<br>v_{2i+1} &amp;= \cos\left(p / f^{2i}\right)\\<br>\end{align}<br>$$<br>where $f$ is some frequency parameter. Intuitively, because $\sin(a+b)$ and $\cos⁡(a+b)$ can be expressed in terms of $\sin⁡(b)$ , $\sin(a)$ , $\cos⁡(b)$ and $\cos⁡(a)$, there will be linear dependencies between the components of distant embeddings, authorizing the model to extract relative positioning information. Good news: the tensorflow code for this technique is available in the library <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="external">tensor2tensor</a>, so we just need to reuse the same function and transform our <code>out</code> with the following call</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">out = add_timing_signal_nd(out)</div></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Now that we have a sequence of vectors $[e_1, \dots, e_n]$ that represents our input image, let’s decode it! First, let’s explain what variant of the Seq2Seq framework we are going to use.</p><p><strong>First hidden vector of the decoder’s LSTM</strong> In the seq2seq framework, this is usually just the last hidden vector of the encoder’s LSTM. Here, we don’t have such a vector, so a good choice would be to learn to compute it with a matrix $W$ and a vector $b$<br>$$<br>h_0 = \tanh\left( W \cdot \left( \frac{1}{n} \sum_{i=1}^n e_i\right) + b \right)<br>$$<br>This can be done in Tensorflow with the following logic</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">img_mean = tf.reduce_mean(seq, axis=<span class="number">1</span>)</div><div class="line">W = tf.get_variable(<span class="string">"W"</span>, shape=[<span class="number">512</span>, <span class="number">512</span>])</div><div class="line">b = tf.get_variable(<span class="string">"b"</span>, shape=[<span class="number">512</span>])</div><div class="line">h = tf.tanh(tf.matmul(img_mean, W) + b)</div></pre></td></tr></table></figure><p><strong>Attention Mechanism</strong> We first need to compute a score $\alpha_{t^{\prime}}$ for each vector $e_{t^{\prime}}$ of the sequence. We use the following method<br>$$<br>\begin{align}<br>\alpha_{t’} &amp;= \beta^T \tanh\left( W_1 \cdot e_{t’} + W_2 \cdot h_{t} \right)\\<br>\bar{\alpha} &amp;= \operatorname{softmax}\left(\alpha\right)\\<br>c_t &amp;= \sum_{i=1}^n \bar{\alpha}_{t’} e_{t’}\\<br>\end{align}<br>$$<br>This can be done in Tensorflow with the follwing code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># over the image, shape = (batch size, n, 512)</span></div><div class="line">W1_e = tf.layers.dense(inputs=seq, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># over the hidden vector, shape = (batch size, 512)</span></div><div class="line">W2_h = tf.layers.dense(inputs=h, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># sums the two contributions</span></div><div class="line">a = tf.tanh(W1_e + tf.expand_dims(W2_h, axis=<span class="number">1</span>))</div><div class="line">beta = tf.get_variable(<span class="string">"beta"</span>, shape=[<span class="number">512</span>, <span class="number">1</span>], dtype=tf.float32)</div><div class="line">a_flat = tf.reshape(a, shape=[<span class="number">-1</span>, <span class="number">512</span>])</div><div class="line">a_flat = tf.matmul(a_flat, beta)</div><div class="line">a = tf.reshape(a, shape=[<span class="number">-1</span>, n])</div><div class="line"></div><div class="line"><span class="comment"># compute weights</span></div><div class="line">a = tf.nn.softmax(a)</div><div class="line">a = tf.expand_dims(a, axis=<span class="number">-1</span>)</div><div class="line">c = tf.reduce_sum(a * seq, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Note that the line <code>W1_e = tf.layers.dense(inputs=seq, units=512, use_bias=False)</code> is common to every decoder time step, so we can just compute it once and for all. The dense layer with no bias is just a matrix multiplication.</p></blockquote><p>Now that we have our attention vector, let’s just add a small modification and compute an other vector $o_{t-1}$ (as in <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="external">Luong, Pham and Manning</a>) that we will use to make our final prediction and that we will feed as input to the LSTM at the next step. Here $w_{t−1}$ denotes the embedding of the token generated at the previous step.</p><blockquote><p>$o_t$ passes some information about the distribution from the previous time step as well as the confidence it had for the predicted token</p></blockquote><p>$$<br>\begin{align}<br>h_t &amp;= \operatorname{LSTM}\left( h_{t-1}, [w_{t-1}, o_{t-1}] \right)\\<br>c_t &amp;= \operatorname{Attention}([e_1, \dots, e_n], h_t)\\<br>o_{t} &amp;= \tanh\left(W_3 \cdot [h_{t}, c_t] \right)\\<br>p_t &amp;= \operatorname{softmax}\left(W_4 \cdot o_{t} \right)\\<br>\end{align}<br>$$</p><p>and now the code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute o</span></div><div class="line">W3_o = tf.layers.dense(inputs=tf.concat([h, c], axis=<span class="number">-1</span>), units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line">o = tf.tanh(W3_o)</div><div class="line"></div><div class="line"><span class="comment"># compute the logits scores (before softmax)</span></div><div class="line">logits = tf.layers.dense(inputs=o, units=vocab_size, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># the softmax will be computed in the loss or somewhere else</span></div></pre></td></tr></table></figure><blockquote><p>If I read carefully, I notice that for the first step of the decoding process, we need to compute an $o_0$ too, right?</p></blockquote><p>This is a good point, and we just use the same technique that we used to generate $h_0$ but with different weights.</p><h2 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h2><blockquote><p>We’ll need to create 2 different outputs in the Tensorflow graph: one for training (that uses the <code>formula</code>and feeds the ground truth at each time step, see <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">part I</a>) and one for test time (that ignores everything about the actual <code>formula</code> and uses the prediction from the previous step).</p></blockquote><h3 id="AttentionCell"><a href="#AttentionCell" class="headerlink" title="AttentionCell"></a>AttentionCell</h3><p>We’ll need to encapsulate the reccurent logic into a custom cell that inherits <code>RNNCell</code>. Our custom cell will be able to call the LSTM cell (initialized in the <code>__init__</code>). It also has a special recurrent state that combines the LSTM state and the vector oo (as we need to pass it through). An elegant way is to define a namedtuple for this recurrent state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">AttentionState = collections.namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.lstm_cell = LSTMCell(<span class="number">512</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Args:</div><div class="line">            inputs: shape = (batch_size, dim_embeddings) embeddings from previous time step</div><div class="line">            cell_state: (AttentionState) state from previous time step</div><div class="line">        """</div><div class="line">        lstm_state, o = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([inputs, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># apply previous logic</span></div><div class="line">        c = ...</div><div class="line">        new_o  = ...</div><div class="line">        logits = ...</div><div class="line"></div><div class="line">        new_state = AttentionState(new_lstm_state, new_o)</div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><p>Then, to compute our output sequence, we just need to call the previous cell on the sequence of LaTeX tokens. We first produce the sequence of token embeddings to which we concatenate the special <code>&lt;sos&gt;</code> token. Then, we call <code>dynamic_rnn</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. get token embeddings</span></div><div class="line">E = tf.get_variable(<span class="string">"E"</span>, shape=[vocab_size, <span class="number">80</span>], dtype=tf.float32)</div><div class="line"><span class="comment"># special &lt;sos&gt; token</span></div><div class="line">start_token = tf.get_variable(<span class="string">"start_token"</span>, dtype=tf.float32, shape=[<span class="number">80</span>])</div><div class="line">tok_embeddings = tf.nn.embedding_lookup(E, formula)</div><div class="line"></div><div class="line"><span class="comment"># 2. add the special &lt;sos&gt; token embedding at the beggining of every formula</span></div><div class="line">start_token_ = tf.reshape(start_token, [<span class="number">1</span>, <span class="number">1</span>, dim])</div><div class="line">start_tokens = tf.tile(start_token_, multiples=[batch_size, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line"><span class="comment"># remove the &lt;eos&gt; that won't be used because we reached the end</span></div><div class="line">tok_embeddings = tf.concat([start_tokens, tok_embeddings[:, :<span class="number">-1</span>, :]], axis=<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 3. decode</span></div><div class="line">attn_cell = AttentionCell()</div><div class="line">seq_logits, _ = tf.nn.dynamic_rnn(attn_cell, tok_embeddings, initial_state=AttentionState(h_0, o_0))</div></pre></td></tr></table></figure><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>Code speaks for itself</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute - log(p_i[y_i]) for each time step, shape = (batch_size, formula length)</span></div><div class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=seq_logits, labels=formula)</div><div class="line"><span class="comment"># masking the losses</span></div><div class="line">mask = tf.sequence_mask(formula_length)</div><div class="line">losses = tf.boolean_mask(losses, mask)</div><div class="line"><span class="comment"># averaging the loss over the batch</span></div><div class="line">loss = tf.reduce_mean(losses)</div><div class="line"><span class="comment"># building the train op</span></div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate)</div><div class="line">train_op = optimizer.minimize(loss)</div></pre></td></tr></table></figure><p>and when iterating over the batches during training, <code>train_op</code> will be given to the <code>tf.Session</code> along with a <code>feed_dict</code> containing the data for the placeholders.</p><h2 id="Decoding-in-Tensorflow"><a href="#Decoding-in-Tensorflow" class="headerlink" title="Decoding in Tensorflow"></a>Decoding in Tensorflow</h2><blockquote><p>Let’s have a look at the Tensorflow implementation of the Greedy Method before dealing with Beam Search</p></blockquote><h3 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h3><p>While greedy decoding is easy to conceptualize, implementing it in Tensorflow is not straightforward, as you need to use the previous prediction and can’t use <code>dynamic_rnn</code> on the <code>formula</code>. There are basically <strong>2 ways of approaching the problem</strong></p><ol><li><p>Modify our <code>AttentionCell</code> and <code>AttentionState</code> so that <code>AttentionState</code> also contains the embedding of the predicted word at the previous time step,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">AttentionState = namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>, <span class="string">"embedding"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        lstm_state, o, embbeding = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([embedding, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># usual logic</span></div><div class="line">        logits = ...</div><div class="line">        <span class="comment"># compute new embeddding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        new_state = AttentionState(new_lstm_state, new_o, new_embedding)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><blockquote><p>This technique has a few downsides. It <strong>doesn’t use inputs</strong> (which used to be the embedding of the gold token from the <code>formula</code> and thus we would have to call <code>dynamic_rnn</code> on a “fake” sequence). Also, how do you know when to stop decoding, once you’ve reached the <code>&lt;eos&gt;</code> token?</p></blockquote></li><li><p>Implement a variant of <code>dynamic_rnn</code> that would not run on a sequence but feed the prediction from the previous time step to the cell, while having a maximum number of decoding steps. This would involve delving deeper into Tensorflow, using <code>tf.while_loop</code>. That’s the method we’re going to use as it fixes all the problems of the first technique. We eventually want something that looks like</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">attn_cell = AttentionCell(...)</div><div class="line"><span class="comment"># wrap the attention cell for decoding</span></div><div class="line">decoder_cell = GreedyDecoderCell(attn_cell)</div><div class="line"><span class="comment"># call a special dynamic_decode primitive</span></div><div class="line">test_outputs, _ = dynamic_decode(decoder_cell, max_length_formula+<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Much better isn’t it? Now let’s see what <code>GreedyDecoderCell</code> and <code>dynamic_decode</code> look like.</p></blockquote></li></ol><h3 id="Greedy-Decoder-Cell"><a href="#Greedy-Decoder-Cell" class="headerlink" title="Greedy Decoder Cell"></a>Greedy Decoder Cell</h3><p>We first wrap the attention cell in a <code>GreedyDecoderCell</code> that takes care of the greedy logic for us, without having to modify the <code>AttentionCell</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderOutput</span><span class="params">(collections.namedtuple<span class="params">(<span class="string">"DecoderOutput"</span>, <span class="params">(<span class="string">"logits"</span>, <span class="string">"ids"</span>)</span>)</span>)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GreedyDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># next step of attention cell</span></div><div class="line">        logits, new_state = self._attention_cell.step(embedding, state)</div><div class="line">        <span class="comment"># get ids of words predicted and get embedding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        <span class="comment"># create new state of decoder</span></div><div class="line">        new_output = DecoderOutput(logits, new_ids)</div><div class="line">        new_finished = tf.logical_or(finished, tf.equal(new_ids,</div><div class="line">                self._end_token))</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (new_output, new_state, new_embedding, new_finished)</div></pre></td></tr></table></figure><h3 id="Dynamic-Decode-primitive"><a href="#Dynamic-Decode-primitive" class="headerlink" title="Dynamic Decode primitive"></a>Dynamic Decode primitive</h3><p>We need to implement a function <code>dynamic_decode</code> that will recursively call the above <code>step</code> function. We do this with a <code>tf.while_loop</code> that stops when all the hypotheses reached <code>&lt;eos&gt;</code> or <code>time</code> is greater than the max number of iterations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamic_decode</span><span class="params">(decoder_cell, maximum_iterations)</span>:</span></div><div class="line">    <span class="comment"># initialize variables (details on github)</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">condition</span><span class="params">(time, unused_outputs_ta, unused_state, unused_inputs, finished)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.logical_not(tf.reduce_all(finished))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(time, outputs_ta, state, inputs, finished)</span>:</span></div><div class="line">        new_output, new_state, new_inputs, new_finished = decoder_cell.step(</div><div class="line">            time, state, inputs, finished)</div><div class="line">        <span class="comment"># store the outputs in TensorArrays (details on github)</span></div><div class="line">        new_finished = tf.logical_or(tf.greater_equal(time, maximum_iterations), new_finished)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (time + <span class="number">1</span>, outputs_ta, new_state, new_inputs, new_finished)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"rnn"</span>):</div><div class="line">        res = tf.while_loop(</div><div class="line">            condition,</div><div class="line">            body,</div><div class="line">            loop_vars=[initial_time, initial_outputs_ta, initial_state, initial_inputs, initial_finished])</div><div class="line"></div><div class="line">    <span class="comment"># return the final outputs (details on github)</span></div></pre></td></tr></table></figure><blockquote><p>Some details using <code>TensorArrays</code> or <code>nest.map_structure</code> have been omitted for clarity but may be found on <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/dynamic_decode.py" target="_blank" rel="external">github</a></p><p>Notice that we place the <code>tf.while_loop</code> inside a scope named <code>rnn</code>. This is because <code>dynamic_rnn</code>does the same thing and thus the weights of our LSTM are defined in that scope.</p></blockquote><h3 id="Beam-Search-Decoder-Cell"><a href="#Beam-Search-Decoder-Cell" class="headerlink" title="Beam Search Decoder Cell"></a>Beam Search Decoder Cell</h3><blockquote><p>We can follow the same approach as in the greedy method and use <code>dynamic_decode</code></p></blockquote><p>Let’s create a new wrapper for <code>AttentionCell</code> in the same way we did for <code>GreedyDecoderCell</code>. This time, the code is going to be more complicated and the following is just for intuition. Note that when selecting the top kk hypotheses from the set of candidates, we must know which “beginning” they used (=parent hypothesis).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BeamSearchDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># notice the same arguments as for GreedyDecoderCell</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># compute new logits</span></div><div class="line">        logits, new_cell_state = self._attention_cell.step(embedding, state.cell_state)</div><div class="line"></div><div class="line">        <span class="comment"># compute log probs of the step (- log p(w) for all words w)</span></div><div class="line">        <span class="comment"># shape = [batch_size, beam_size, vocab_size]</span></div><div class="line">        step_log_probs = tf.nn.log_softmax(new_logits)</div><div class="line"></div><div class="line">        <span class="comment"># compute scores for the (beam_size * vocabulary_size) new hypotheses</span></div><div class="line">        log_probs = state.log_probs + step_log_probs</div><div class="line"></div><div class="line">        <span class="comment"># get top k hypotheses</span></div><div class="line">        new_probs, indices = tf.nn.top_k(log_probs, self._beam_size)</div><div class="line"></div><div class="line">        <span class="comment"># get ids of next token along with the parent hypothesis</span></div><div class="line">        new_ids = ...</div><div class="line">        new_parents = ...</div><div class="line"></div><div class="line">        <span class="comment"># compute new embeddings, new_finished, new_cell state...</span></div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div></pre></td></tr></table></figure><blockquote><p>Look at <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/beam_search_decoder_cell.py" target="_blank" rel="external">github</a> for the details. The main idea is that we add a beam dimension to every tensor, but when feeding it into <code>AttentionCell</code> we merge the beam dimension with the batch dimension. There is also some trickery involved to compute the parents and the new ids using modulos.</p></blockquote><h2 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I hope that you learned something with this post, either about the technique or Tensorflow. While the model achieves impressive performance (at least on short formulas with roughly 85% of the LaTeX being reconstructed), it still raises some questions that I list here:</p><p><em>How do we evaluate the performance of our model?</em>. We can use standard metrics from Machine Translation like <a href="https://en.wikipedia.org/wiki/BLEU" target="_blank" rel="external">BLEU</a> to evaluate how good the decoded LaTeX is compared to the reference. We can also choose to compile the predicted LaTeX sequence to get the image of the formula, and then compare this image to the orignal. As a formula is a sequence, computing the pixel-wise distance wouldn’t really make sense. A good idea is proposed by <a href="http://lstm.seas.harvard.edu/latex" target="_blank" rel="external">Harvard’s paper</a>. First, slice the image vertically. Then, compare the edit distance between these slices…</p><p><em>How to fix exposure bias?</em> While beam search generally achieves better results, it is not perfect and still suffers from <strong>exposure bias</strong>. During training, the model is never exposed to its errors! It also suffers from <strong>Loss-Evaluation Mismatch</strong>. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/ref.png" alt=""></p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/pred.png" alt=""></p><p><em>An Example of LaTeX generation - which one is the reference?</em></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Source Post is &lt;a href=&quot;https://guillaumegenthial.github.io/sequence-to-sequence.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;h2 id=
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Convolutions</title>
    <link href="http://yoursite.com/2018/01/18/Understanding-Convolutions/"/>
    <id>http://yoursite.com/2018/01/18/Understanding-Convolutions/</id>
    <published>2018-01-18T09:01:29.000Z</published>
    <updated>2018-01-18T10:26:33.245Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lessons-from-a-Dropped-Ball"><a href="#Lessons-from-a-Dropped-Ball" class="headerlink" title="Lessons from a Dropped Ball"></a>Lessons from a Dropped Ball</h2><p>Imagine we drop a ball from some height onto the ground, where it only has one dimension of motion. <em>How likely is it that a ball will go a distance $c$ if you drop it and then drop it again from above the point at which it landed?</em></p><p>Let’s break this down. After the first drop, it will land aa units away from the starting point with probability $f(a)$, where $f$ is the probability distribution.</p><p>Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed. The probability of the ball rolling $b$ units away from the new starting point is $g(b)$, where $g$ may be a different probability distribution if it’s dropped from a different height.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-fagb.png" alt="img"></p><p>If we fix the result of the first drop so we know the ball went distance aa, for the ball to go a total distance cc, the distance traveled in the second drop is also fixed at bb, where $a+b=c$. So the probability of this happening is simply $f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn1" target="_blank" rel="external">1</a></p><p>Let’s think about this with a specific discrete example. We want the total distance $c$ to be 3. If the first time it rolls, $a=2$, the second time it must roll $b=1$ in order to reach our total distance $a+b=3$. The probability of this is $f(2)⋅g(1)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-split-21.png" alt="img"></p><p>However, this isn’t the only way we could get to a total distance of 3. The ball could roll 1 units the first time, and 2 the second. Or 0 units the first time and all 3 the second. It could go any $a$ and $b$, as long as they add to 3.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-splits-12-03.png" alt="img"></p><p>The probabilities are $f(1)⋅g(2)$ and $f(0)⋅g(3)$, respectively.</p><p>In order to find the <em>total likelihood</em> of the ball reaching a total distance of $c$, we can’t consider only one possible way of reaching $c$. Instead, we consider <em>all the possible ways</em> of partitioning $c$ into two drops $a$ and $b$ and sum over the <em>probability of each way</em>.<br>$$<br>…\;\; f(0)\cdot g(3) ~+~ f(1)\cdot g(2) ~+~ f(2)\cdot g(1)\;\;…<br>$$<br>We already know that the probability for each case of $a+b=c$ is simply $f(a)⋅g(b)$. So, summing over every solution to $a+b=c$, we can denote the total likelihood as:<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Turns out, we’re doing a convolution! In particular, the convolution of $f$ and $g$, evluated at $c$ is defined:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)~~~~<br>$$<br>If we substitute $b=c−a$, we get:<br>$$<br>(f\ast g)(c) = \sum_a f(a) \cdot g(c-a)<br>$$<br>This is the standard definition<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn2" target="_blank" rel="external">2</a> of convolution.</p><p>To make this a bit more concrete, we can think about this in terms of positions the ball might land. After the first drop, it will land at an intermediate position aa with probability $f(a)$. If it lands at $a$, it has probability $g(c−a)$ of landing at a position $c$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-OnePath.png" alt="img"></p><p>To get the convolution, we consider all intermediate positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-SumPaths.png" alt="img"></p><h2 id="Visualizing-Convolutions"><a href="#Visualizing-Convolutions" class="headerlink" title="Visualizing Convolutions"></a>Visualizing Convolutions</h2><p>There’s a very nice trick that helps one think about convolutions more easily.</p><p>First, an observation. Suppose the probability that a ball lands a certain distance $x$ from where it started is $f(x)$. Then, afterwards, the probability that it started a distance $x$ from where it landed is $f(−x)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Reverse.png" alt="img"></p><p>If we know the ball lands at a position $c$ after the second drop, what is the probability that the previous position was $a$?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-BackProb.png" alt="img"></p><p>So the probability that the previous position was $a$ is $g(−(a−c))=g(c−a)$.</p><p>Now, consider the probability each intermediate position contributes to the ball finally landing at $c$. We know the probability of the first drop putting the ball into the intermediate position a is $f(a)$. We also know that the probability of it having been in $a$, if it lands at $c$ is $g(c−a)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate.png" alt="img"></p><p>Summing over the $a$s, we get the convolution.</p><p>The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value $c$ in a single picture. By shifting the bottom half around, we can evaluate the convolution at other values of $c$. This allows us to understand the convolution as a whole.</p><p>For example, we can see that it peaks when the distributions align.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Align.png" alt="img"></p><p>And shrinks as the intersection between the distributions gets smaller.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Sep.png" alt="img"></p><p>By using this trick in an animation, it really becomes possible to visually understand convolutions.</p><p>Below, we’re able to visualize the convolution of two box functions:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Wiki-BoxConvAnim.gif" alt="Wiki-BoxConvAnim"></p><p><em>From Wikipedia</em></p><p>Armed with this perspective, a lot of things become more intuitive.</p><p>Let’s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy.</p><h2 id="Higher-Dimensional-Convolutions"><a href="#Higher-Dimensional-Convolutions" class="headerlink" title="Higher Dimensional Convolutions"></a>Higher Dimensional Convolutions</h2><p>Convolutions are an extremely general idea. We can also use them in a higher number of dimensions.</p><p>Let’s consider our example of a falling ball again. Now, as it falls, it’s position shifts not only in one dimension, but in two.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-TwoDim.png" alt="img"></p><p>Convolution is the same as before:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Except, now $a$, $b$ and $c$ are vectors. To be more explicit,<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{\begin{array}{c}a_1+b_1=c_1\\a_2+b_2=c_2\end{array}} f(a_1,a_2) \cdot g(b_1,b_2)<br>$$<br>Or in the standard definition:<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{a_1, a_2} f(a_1, a_2) \cdot g(c_1-a_1,~ c_2-a_2)<br>$$<br>Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding.</p><p>One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a “kernel.”</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/RiverTrain-ImageConvDiagram.png" alt="RiverTrain-ImageConvDiagram"></p><p><em>From the <a href="http://intellabs.github.io/RiverTrail/tutorial/" target="_blank" rel="external">River Trail documentation</a></em></p><p>The kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over.</p><p>For example, by averaging a 3x3 box of pixels, we can blur an image. To do this, our kernel takes the value $\dfrac{1}{9}$ on each pixel in the box,</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Blur.png" alt="Gimp-Blur"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>We can also detect edges by taking the values −1−1 and 11 on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Edge.png" alt="Gimp-Edge"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>The gimp documentation has <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">many other examples</a>.</p><h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><p>So, how does convolution relate to convolutional neural networks?</p><p>Consider a 1-dimensional convolutional layer with inputs $\{x_n\}$ and outputs $\{y_n\}$, like we discussed in the <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" target="_blank" rel="external">previous post</a>:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY.png" alt="img"></p><p>As we observed, we can describe the outputs in terms of the inputs:<br>$$<br>y_n = A(x_{n}, x_{n+1}, …)<br>$$<br>Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.</p><p>Recall that a typical neuron in a neural network is described by:<br>$$<br>\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~…~ + b)<br>$$<br>Where $x_0$, $x_1$… are the inputs. The weights, $w_0$, $w_1$, … describe how the neuron connects to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn3" target="_blank" rel="external">3</a> Saying that multiple neurons are identical is the same thing as saying that the weights are the same.</p><p>It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.</p><p>Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:<br>$$<br>y = \sigma(Wx + b)<br>$$<br>For example, we get:<br>$$<br>y_0 = \sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 …)<br>$$</p><p>$$<br>y_1 = \sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 …)<br>$$</p><p>Each row of the matrix describes the weights connecting a neuron to its inputs.</p><p>Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY-W.png" alt="img"></p><p>Which corresponds to the equations:<br>$$<br>y_0 = \sigma(W_0x_0 + W_1x_1 -b)<br>$$</p><p>$$<br>y_1 = \sigma(W_0x_1 + W_1x_2 -b)<br>$$</p><p>So while, normally, a weight matrix connects every input to every neuron with different weights:<br>$$<br>W = \left[\begin{array}{ccccc}<br>W_{0,0} &amp; W_{0,1} &amp; W_{0,2} &amp; W_{0,3} &amp; …\\<br>W_{1,0} &amp; W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; …\\<br>W_{2,0} &amp; W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; …\\<br>W_{3,0} &amp; W_{3,1} &amp; W_{3,2} &amp; W_{3,3} &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don’t connect to many possible inputs, there’s lots of zeros.<br>$$<br>W = \left[\begin{array}{ccccc}<br>w_0 &amp; w_1 &amp; 0 &amp; 0 &amp; …\\<br>0 &amp; w_0 &amp; w_1 &amp; 0 &amp; …\\<br>0 &amp; 0 &amp; w_0 &amp; w_1 &amp; …\\<br>0 &amp; 0 &amp; 0 &amp; w_0 &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>Multiplying by the above matrix is the same thing as convolving with $[…0, w_1, w_0, 0…]$. The function sliding to different positions corresponds to having neurons at those positions.</p><p>What about two-dimensional convolutional layers?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv2-5x5-Conv2-XY.png" alt="img"></p><p>The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.</p><p>Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image.</p><ol><li><p>We want the probability of the ball rolling aa units the first time and also rolling bb units the second time. The distributions $P(A)=f(a)$ and $P(b)=g(b)$ are independent, with both distributions centered at 0. So $P(a,b)=P(a)∗P(b)=f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref1" target="_blank" rel="external">↩</a></p></li><li><p>The non-standard definition, which I haven’t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious.</p><p>For example, convolution is a commutative operation. That is, $f∗g=g∗f$. Why?</p><p>​<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b) ~~=~ \sum_{b+a=c} g(b) \cdot f(a)<br>$$<br>Convolution is also associative. That is, $(f∗g)∗h=f∗(g∗h)$. Why?</p><p>​<br>$$<br>\sum_{(a+b)+c=d} (f(a) \cdot g(b)) \cdot h(c) ~~=~ \sum_{a+(b+c)=d} f(a) \cdot (g(b) \cdot h(c))<br>$$<br>↩</p><p>​</p></li><li><p>There’s also the bias, which is the “threshold” for whether the neuron fires, but it’s much simpler and I don’t want to clutter this section talking about it.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref3" target="_blank" rel="external">↩</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Lessons-from-a-Dropped-Ball&quot;&gt;&lt;a href=&quot;#Lessons-from-a-Dropped-Ball&quot; class=&quot;headerlink&quot; title=&quot;Lessons from a Dropped Ball&quot;&gt;&lt;/a&gt;Lesso
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="CNN" scheme="http://yoursite.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>PCA With Tensorflow</title>
    <link href="http://yoursite.com/2018/01/17/PCA-With-Tensorflow/"/>
    <id>http://yoursite.com/2018/01/17/PCA-With-Tensorflow/</id>
    <published>2018-01-17T09:04:36.000Z</published>
    <updated>2018-01-17T09:07:11.626Z</updated>
    
    <content type="html"><![CDATA[<p>PCA (<strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis) is probably the oldest trick in the book.</p><p>PCA is well studied and there are numerous ways to get to the same solution, we will talk about two of them here, Eigen decomposition and Singular Value Decomposition (SVD) and then we will implement the SVD way in TensorFlow.</p><p>From now on, X will be our data matrix, of shape (n, p) where n is the number of examples, and p are the dimensions.</p><p>So given X, both methods will try to find, in their own way, a way to manipulate and decompose X in a manner that later on we could multiply the decomposed results to represent maximum information in less dimensions. I know I know, sounds horrible but I will spare you most of the math but keep the parts that contribute to the understanding of the method pros and cons.</p><p>So Eigen decomposition and SVD are both ways to decompose matrices, lets see how they help us in PCA and how they are connected.</p><p>Take a glance at the flow chart below and I will explain right after.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*xnomew0zpnxftxutG8xoFw.png" alt="img"></p><p>Figure 1 PCA workflow</p><p>So why should you care about this? Well there is something very fundamental about the two procedures that tells us a lot about PCA.</p><p>As you can see both methods are pure linear algebra, that basically tells us that using PCA is looking at the real data, from a different angle — this is unique to PCA since the other methods start with random representation of lower dimensional data and try to get it to behave like the high dimensional data.</p><p>Some other notable things are that all operations are linear and with SVD are super-super fast.</p><p>Also given the same data PCA will always give the same answer (which is not true about the other two methods).</p><p>Notice how in SVD we choose the r (r is the number of dimensions we want to reduce to) left most values of Σ to lower dimensionality?</p><p>Well there is something special about Σ .</p><p>Σ is a diagonal matrix, there are p (number of dimensions) diagonal values (called singular values) and their magnitude indicates how significant they are to preserving the information.</p><p>So we can choose to reduce dimensionality, to the number of dimensions that will preserve approx. given amount of percentage of the data and I will demonstrate that in the code (e.g. gives us the ability to reduce dimensionality with a constraint of losing a max of 15% of the data).</p><p>As you will see, coding this in TensorFlow is pretty simple — what we are are going to code is a class that has <code>fit</code> method and a <code>reduce</code> method which we will supply the dimensions to.</p><h3 id="CODE-PCA"><a href="#CODE-PCA" class="headerlink" title="CODE (PCA)"></a><strong>CODE (PCA)</strong></h3><p>Lets see how the <code>fit</code> method looks like, given <code>self.X</code> contains the data and <code>self.dtype=tf.float32</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></div><div class="line">    self.graph = tf.Graph()</div><div class="line">    <span class="keyword">with</span> self.graph.as_default():</div><div class="line">        self.X = tf.placeholder(self.dtype, shape=self.data.shape)</div><div class="line"></div><div class="line">        <span class="comment"># Perform SVD</span></div><div class="line">        singular_values, u, _ = tf.svd(self.X)</div><div class="line"></div><div class="line">        <span class="comment"># Create sigma matrix</span></div><div class="line">        sigma = tf.diag(singular_values)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session(graph=self.graph) <span class="keyword">as</span> session:</div><div class="line">        self.u, self.singular_values, self.sigma = session.run([u, singular_values, sigma],</div><div class="line">                                                               feed_dict=&#123;self.X: self.data&#125;)</div></pre></td></tr></table></figure><p>So the goal of <code>fit</code> is to create our Σ and U for later use.</p><p>We’ll start with the line <code>tf.svd</code> which gives us the singular values, which are the diagonal values of what was denoted as Σ in Figure 1, and the matrices U and V.</p><p>Then <code>tf.diag</code> is TensorFlow’s way of converting a 1D vector, to a diagonal matrix, which in our case will result in Σ.</p><p>At the end of the <code>fit</code> call we will have the singular values, Σ and U.</p><p>Now lets lets implement <code>reduce</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span><span class="params">(self, n_dimensions=None, keep_info=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> keep_info:</div><div class="line">        <span class="comment"># Normalize singular values</span></div><div class="line">        normalized_singular_values = self.singular_values / sum(self.singular_values)</div><div class="line"></div><div class="line">        <span class="comment"># Create the aggregated ladder of kept information per dimension</span></div><div class="line">        ladder = np.cumsum(normalized_singular_values)</div><div class="line"></div><div class="line">        <span class="comment"># Get the first index which is above the given information threshold</span></div><div class="line">        index = next(idx <span class="keyword">for</span> idx, value <span class="keyword">in</span> enumerate(ladder) <span class="keyword">if</span> value &gt;= keep_info) + <span class="number">1</span></div><div class="line">        n_dimensions = index</div><div class="line"></div><div class="line">    <span class="keyword">with</span> self.graph.as_default():</div><div class="line">        <span class="comment"># Cut out the relevant part from sigma</span></div><div class="line">        sigma = tf.slice(self.sigma, [<span class="number">0</span>, <span class="number">0</span>], [self.data.shape[<span class="number">1</span>], n_dimensions])</div><div class="line"></div><div class="line">        <span class="comment"># PCA</span></div><div class="line">        pca = tf.matmul(self.u, sigma)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session(graph=self.graph) <span class="keyword">as</span> session:</div><div class="line">        <span class="keyword">return</span> session.run(pca, feed_dict=&#123;self.X: self.data&#125;)</div></pre></td></tr></table></figure><p>So as you can see <code>reduce</code> gets either <code>keep_info</code> or <code>n_dimensions</code> (I didn’t implement the input check where <strong>only one must be supplied</strong>).</p><p>If we supply <code>n_dimensions</code> it will simply reduce to that number, but if we supply <code>keep_info</code> which should be a float between 0 and 1, we will preserve that much information from the original data (0.9 — preserve 90% of the data).</p><p>In the first ‘if’, we normalize and check how many singular values are needed, basically figuring out <code>n_dimensions</code> out of <code>keep_info</code>.</p><p>In the graph, we just slice the Σ (sigma) matrix for as much data as we need and perform the matrix multiplication.</p><p>So lets try it out on the iris dataset, which is (150, 4) dataset of 3 species of iris flowers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"></div><div class="line">tf_pca = TF_PCA(iris_dataset.data, iris_dataset.target)</div><div class="line">tf_pca.fit()</div><div class="line">pca = tf_pca.reduce(keep_info=<span class="number">0.9</span>)  <span class="comment"># Results in 2 dimensions</span></div><div class="line"></div><div class="line">color_mapping = &#123;<span class="number">0</span>: sns.xkcd_rgb[<span class="string">'bright purple'</span>], <span class="number">1</span>: sns.xkcd_rgb[<span class="string">'lime'</span>], <span class="number">2</span>: sns.xkcd_rgb[<span class="string">'ochre'</span>]&#125;</div><div class="line">colors = list(map(<span class="keyword">lambda</span> x: color_mapping[x], tf_pca.target))</div><div class="line"></div><div class="line">plt.scatter(pca[:, <span class="number">0</span>], pca[:, <span class="number">1</span>], c=colors)</div></pre></td></tr></table></figure><p><img src="https://cdn-images-1.medium.com/max/2000/1*-am5UfbZoJkUA4C8z5d0vQ.png" alt="img"></p><p>Figure 2 Iris dataset PCA 2 dimensional plot</p><p>Not so bad huh?</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PCA (&lt;strong&gt;P&lt;/strong&gt;rincipal &lt;strong&gt;C&lt;/strong&gt;omponent &lt;strong&gt;A&lt;/strong&gt;nalysis) is probably the oldest trick in the book.&lt;/p&gt;&lt;p&gt;PCA
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Word Embedding - Approximating the Softmax [Repost]</title>
    <link href="http://yoursite.com/2018/01/16/Word-Embedding-Approximating-the-Softmax-Repost/"/>
    <id>http://yoursite.com/2018/01/16/Word-Embedding-Approximating-the-Softmax-Repost/</id>
    <published>2018-01-16T11:09:29.000Z</published>
    <updated>2018-01-16T12:28:19.322Z</updated>
    
    <content type="html"><![CDATA[<p>This is the second post in a series on word embeddings and representation learning. In the <a href="http://sebastianruder.com/word-embeddings-1/index.html" target="_blank" rel="external">previous post</a>, we gave an overview of word embedding models and introduced the classic neural language model by Bengio et al. (2003), the C&amp;W model by Collobert and Weston (2008), and the word2vec model by Mikolov et al. (2013). We observed that mitigating the complexity of computing the final softmax layer has been one of the main challenges in devising better word embedding models, a commonality with machine translation (MT) (Jean et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:10" target="_blank" rel="external">10</a>]) and language modelling (Jozefowicz et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:6" target="_blank" rel="external">6</a>]).</p><p>In this post, we will thus focus on giving an overview of various approximations to the softmax layer that have been proposed over the last years, some of which have so far only been employed in the context of language modelling or MT. We will postpone the discussion of additional hyperparameters to the subsequent post.</p><p>Let us know partially re-introduce the previous post’s notation both for consistency and to facilitate comparison as well as introduce some new notation: We assume a training corpus containing a sequence of $T$ training words $w_1,w_2,w_3, \cdots ,w_T$ that belong to a vocabulary $V$ whose size is $|V|$. Our models generally consider a context $c$ of $n$ words. We associate every word with an input embedding $v_w$ (the eponymous word embedding in the Embedding Layer) with $d$ dimensions and an output embedding $v_{w^{\prime}}$ (the representation of the word in the weight matrix of the softmax layer). We finally optimize an objective function $J_{\theta}$ with regard to our model parameters $\theta$.</p><p>Recall that the softmax calculates the probability of a word $w$ given its context $c$ and can be computed using the following equation:<br>$$<br>p(w|c) = \frac{\exp(h^{\text{T}} v_{w^{\prime}})}{\sum_{w_i \in V} \exp(h^{\text{T}}v_{w_i}^{\prime})}<br>$$<br>where $h$ is the output vector of the penultimate network layer. Note that we use $c$ for the context as mentioned above and drop the index $t$ of the target word $w_t$ for simplicity. Computing the softmax is expensive as the inner product between $h$ and the output embedding of every word $w_i$ in the vocabulary $V$ needs to be computed as part of the sum in the denominator in order to obtain the normalized probability of the target word $w$ given its context $c$.</p><p>In the following we will discuss different strategies that have been proposed to approximate the softmax. These approaches can be grouped into softmax-based and sampling-based approaches. Softmax-based approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency. Sampling-based approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax.</p><h1 id="Softmax-based-Approaches"><a href="#Softmax-based-Approaches" class="headerlink" title="Softmax-based Approaches"></a>Softmax-based Approaches</h1><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p>Hierarchical softmax (H-Softmax) is an approximation inspired by binary trees that was proposed by Morin and Bengio [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:3" target="_blank" rel="external">3</a>]. H-Softmax essentially replaces the flat softmax layer with a hierarchical layer that has the words as leaves, as can be seen in Figure 1.</p><p>This allows us to decompose calculating the probability of one word into a sequence of probability calculations, which saves us from having to calculate the expensive normalization over all words. Replacing a softmax layer with H-Softmax can yield speedups for word prediction tasks of at least $50 \times $ and is thus critical for low-latency tasks such as real-time communication in <a href="http://googleresearch.blogspot.ie/2016/05/chat-smarter-with-allo.html" target="_blank" rel="external">Google’s new messenger app Allo</a>.</p><p><img src="http://ruder.io/content/images/2016/06/hierarchical_softmax_example.png" alt="Hierarchical softmax"></p><p>Figure 1: Hierarchical softmax (<a href="https://www.quora.com/Word2vec-How-can-hierarchical-soft-max-training-method-of-CBOW-guarantee-its-self-consistence" target="_blank" rel="external">Quora</a>)</p><p>We can think of the regular softmax as a tree of depth 11, with each word in $V$ as a leaf node. Computing the softmax probability of one word then requires normalizing over the probabilities of all $|V|$ leaves. If we instead structure the softmax as a binary tree, with the words as leaf nodes, then we only need to follow the path to the leaf node of that word, without having to consider any of the other nodes.</p><p>Since a balanced binary tree has a depth of $\log_2(|V|)$ we only need to evaluate at most $\log_2(|V|)$ nodes to obtain the final probability of a word. Note that this probability is already normalized, as the probabilities of all leaves in a binary tree sum to 11 and thus form a probability distribution. To informally verify this, we can reason that at a tree’s root node (Node 0) in Figure 1), the probabilities of branching decisions must sum to 11. At each subsequent node, the probability mass is then split among its children, until it eventually ends up at the leaf nodes, i.e. the words. Since no probability is lost along the way and since all words are leaves, the probabilities of all words must necessarily sum to 11 and hence the hierarchical softmax defines a normalized probability distribution over all words in $V$.</p><p>To get a bit more concrete, as we go through the tree, we have to be able to calculate the probability of taking the left or right branch at every junction. For this reason, we assign a representation to every node. In contrast to the regular softmax, we thus no longer have output embeddings $v^{\prime}_w$ for every word $w$ – instead, we have embeddings $v^{\prime}_n$ for every node $n$. As we have $|V|−1$ nodes and each one possesses a unique representation, the number of parameters of H-Softmax is almost the same as for the regular softmax. We can now calculate the probability of going right (or left) at a given node $n$ given the context $c$ the following way:<br>$$<br>p(\text{right}|n,c) = \sigma(h^{\text{T}}v^{\prime}_n).<br>$$<br>This is almost the same as the computations in the regular softmax; now instead of computing the dot product between $h$ and the output word embedding $v^{\prime}_w$, we compute the dot product between $h$ and the embedding $v^{\prime}_w$ of each node in the tree; additionally, instead of computing a probability distribution over the entire vocabulary words, we output just one probability, the probability of going right at node $n$ in this case, with the sigmoid function. Conversely, the probability of turning left is simply $1−p(\text{right} | n,c)$.</p><p><img src="http://ruder.io/content/images/2016/05/hierarchical_softmax.png" alt="Hierarchical softmax"></p><p>Figure 2: Hierarchical softmax computations (<a href="https://www.youtube.com/watch?v=B95LTf2rVWM" target="_blank" rel="external">Hugo Lachorelle’s Youtube lectures</a>)</p><p>The probability of a word ww given its context cc is then simply the product of the probabilities of taking right and left turns respectively that lead to its leaf node. To illustrate this, given the context “the”, “dog”, “and”, “the”, the probability of the word “cat” in Figure 2 can be computed as the product of the probability of turning left at node 1, turning right at node 2, and turning right at node 5. Hugo Lachorelle gives a more detailed account in his excellent <a href="https://www.youtube.com/watch?v=B95LTf2rVWM" target="_blank" rel="external">lecture video</a>. Rong [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:7" target="_blank" rel="external">7</a>] also does a good job of explaining these concepts and also derives the derivatives of H-Softmax.</p><p>Obviously, the structure of the tree is of significance. Intuitively, we should be able to achieve better performance, if we make it easier for the model to learn the binary predictors at every node, e.g. by enabling it to assign similar probabilities to similar paths. Based on this idea, Morin and Bengio use the synsets in WordNet as clusters for the tree. However, they still report inferior performance to the regular softmax. Mnih and Hinton [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:8" target="_blank" rel="external">8</a>] learn the tree structure with a clustering algorithm that recursively partitions the words in two clusters and allows them to achieve the same performance as the regular softmax at a fraction of the computation.</p><p>Notably, we are only able to obtain this speed-up during training, when we know the word we want to predict (and consequently its path) in advance. During testing, when we need to find the most likely prediction, we still need to calculate the probability of all words, although narrowing down the choices in advance helps here.</p><p>In practice, instead of using “right” and “left” in order to designate nodes, we can index every node with a bit vector that corresponds to the path it takes to reach that node. In Figure 2, if we assume a <code>0</code> bit for turning left and a <code>1</code> bit for turning right, we can thus represent the path to “cat” as <code>011</code>.</p><p>Recall that the path length in a balanced binary tree is $\log_2|V|$. If we set $|V|=10000$, this amounts to an average path length of about $13.3$. Analogously, we can represent every word by the bit vector of its path that is on average $13.3$ bits long. In information theory, this is referred to as an information content of $13.3$ bits per word.</p><h3 id="A-note-on-the-information-content-of-words"><a href="#A-note-on-the-information-content-of-words" class="headerlink" title="A note on the information content of words"></a>A note on the information content of words</h3><p>Recall that the information content $I(w)$ of a word $w$ is the negative logarithm of its probability $p(w)$:<br>$$<br>I(w) = − \log_2p(w)<br>$$<br>The entropy $H$ of all words in a corpus is then the expectation of the information content of all words in the vocabulary:<br>$$<br>H= \sum_{i \in V} p(w_i) I(w_i)<br>$$<br>We can also conceive of the entropy of a data source as the average number of bits needed to encode it. For a fair coin flip, we need $1$ bit per flip, whereas we need $0$ bits for a data source that always emits the same symbol. For a balanced binary tree, where we treat every word equally, the word entropy $H$ equals the information content $I(w)$ of every word $w$, as each word has the same probability. The average word entropy $H$ in a balanced binary tree with $|V|=10000$ thus coincides with its average path length:<br>$$<br>H = − \sum_{i \in V}\frac{1}{10000} \log_2 \frac{⁡1}{10000} = 13.3.<br>$$<br>We saw before that the structure of the tree is important. Notably, we can leverage the tree structure not only to gain better performance, but also to speed up computation: If we manage to encode more information into the tree, we can get away with taking shorter paths for less informative words. Morin and Bengio point out that leveraging word probabilities should work even better; as some words are more likely to occur than others, they can be encoded using less information. They note that the word entropy of their corpus (with $|V|=10,000$) is about $9.16$.</p><p>Thus, by taking into account frequencies, we can reduce the average number of bits per word in the corpus from $13.3$ to $9.16$ in this case, which amounts to a speed-up of 31%. A <a href="https://en.wikipedia.org/wiki/Huffman_coding" target="_blank" rel="external">Huffman tree</a>, which is used by Mikolov et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:1" target="_blank" rel="external">1</a>] for their hierarchical softmax, generates such a coding by assigning fewer bits to more common symbols. For instance, “the”, the most common word in the English language, would be assigned the shortest bit code in the tree, the second most frequent word would be assigned the second-shortest bit code, and so on. While we still need the same number of codes to designate all words, when we predict the words in a corpus, short codes appear now a lot more often, and we consequently need fewer bits to represent each word on average.</p><p>A coding such as Huffman coding is also known as entropy encoding, as the length of each codeword is approximately proportional to the entropy of each symbol as we have observed. Shannon [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:5" target="_blank" rel="external">5</a>] establishes in his experiments that the lower bound on the information rate in English is between $0.6$ to $1.3$ bits per character; given an average word length of $4.5$, this amounts to $2.7$ - $5.85$ bits per word.</p><p>To tie this back to language modelling (which we already talked about in the previous post): perplexity, the evaluation measure of language modelling, is $2^H$ where $H$ is the entropy. A unigram entropy of $9.16$ thus entails a still very high perplexity of $2^{9.16}=572.0$. We can render this value more tangible by observing that a model with a perplexity of $572$ is as confused by the data as if it had to choose among $572$ possibilities for each word uniformly and independently.</p><p>To put this into context: The state-of-the-art language model by Jozefowicz et al. (2016) achieves a perplexity of $24.2$ per word on the 1B Word Benchmark. Such a model would thus require an average of around 4.604.60 bits to encode each word, as $2^{4.60}=24.2$, which is incredibly close to the experimental lower bounds documented by Shannon. If and how we could use such a model to construct a better hierarchical softmax layer is still left to be explored.</p><h2 id="Differentiated-Softmax"><a href="#Differentiated-Softmax" class="headerlink" title="Differentiated Softmax"></a>Differentiated Softmax</h2><p>Chen et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:9" target="_blank" rel="external">9</a>] introduce a variation on the traditional softmax layer, the Differentiated Softmax (D-Softmax). D-Softmax is based on the intuition that not all words require the same number of parameters: Many occurrences of frequent words allow us to fit many parameters to them, while extremely rare words might only allow to fit a few.</p><p>In order to do this, instead of the dense matrix of the regular softmax layer of size $d×|V|$ containing the output word embeddings $v^{\prime}_w \in \mathbb{R}^d$, they use a sparse matrix. They then arrange $v′w$ in blocks sorted by frequency, with the embeddings in each block being of a certain dimensionality $d_k$. The number of blocks and their embedding sizes are hyperparameters that can be tuned.</p><p><img src="http://ruder.io/content/images/2016/05/differentiated_softmax_1.png" alt="Differentiated softmax"></p><p>Figure 3: Differentiated softmax (Chen et al. (2015))</p><p>In Figure 3, embeddings in partition $A$ are of dimensionality $d_A$ (these are embeddings of frequent words, as they are allocated more parameters), while embeddings in partitions $B$ and $C$ have $d_B$ and $d_C$ dimensions respectively. Note that all areas not part of any partition, i.e. the non-shaded areas in Figure 1, are set to $0$.</p><p>The output of the previous hidden layer $h$ is treated as a concatenation of features corresponding to each partition of the dimensionality of that partition, e.g. $h$ in Figure 3 is made up of partitions of size $d_A$, $d_B$, and $d_B$ respectively. Instead of computing the matrix-vector product between the entire output embedding matrix and $h$ as in the regular softmax, D-Softmax then computes the product of each partition and its corresponding section in $h$.</p><p>As many words will only require comparatively few parameters, the complexity of computing the softmax is reduced, which speeds up training. In contrast to H-Softmax, this speed-up persists during testing. Chen et al. (2015) observe that D-Softmax is the fastest method when testing, while being one of the most accurate. However, as it assigns fewer parameters to rare words, D-Softmax does a worse job at modelling them.</p><h2 id="CNN-Softmax"><a href="#CNN-Softmax" class="headerlink" title="CNN-Softmax"></a>CNN-Softmax</h2><p>Another modification to the traditional softmax layer is inspired by recent work by Kim et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:13" target="_blank" rel="external">13</a>] who produce input word embeddings $v_w$ via a character-level CNN. Jozefowicz et al. (2016) in turn suggest to do the same thing for the output word embeddings $v^{\prime}_w$ via a character-level CNN – and refer to this as CNN-Softmax. Note that if we have a CNN at the input and at the output as in Figure 4, the CNN generating the output word embeddings $v^{\prime}_w$ is necessarily different from the CNN generating the input word embeddings $v_w$, just as the input and output word embedding matrices would be different.</p><p><img src="http://ruder.io/content/images/2016/05/cnn-softmax_1.png" alt="CNN-Softmax"></p><p>Figure 4: CNN-Softmax (Jozefowicz et al. (2016))</p><p>While this still requires computing the regular softmax normalization, this approach drastically reduces the number of parameters of the model: Instead of storing an embedding matrix of $d \times |V|$, we now only need to keep track of the parameters of the CNN. During testing, the output word embeddings $v^{\prime}_w$ can be pre-computed, so that there is no loss in performance.</p><p>However, as characters are represented in a continuous space and as the resulting model tends to learn a smooth function mapping characters to a word embedding, character-based models often find it difficult to differentiate between similarly spelled words with different meanings. To mitigate this, the authors add a correction factor that is learned per word, which significantly reduces the performance gap between regular and CNN-softmax. By adjusting the dimensionality of the correction term, the authors are able to trade-off model size versus performance.</p><p>The authors also note that instead of using a CNN-softmax, the output of the previous layer hh can be fed to a character-level LSTM, which predicts the output word one character at a time. Instead of a softmax over words, a softmax outputting a probability distribution over characters would thus be used at every time step. They, however, fail to achieve competitive performance with this layer. Ling et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:14" target="_blank" rel="external">14</a>] use a similar layer for machine translation and achieve competitive results.</p><h1 id="Sampling-based-Approaches"><a href="#Sampling-based-Approaches" class="headerlink" title="Sampling-based Approaches"></a>Sampling-based Approaches</h1><p>While the approaches discussed so far still maintain the overall structure of the softmax, sampling-based approaches on the other hand completely do away with the softmax layer. They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. However, sampling-based approaches are only useful at training time – during inference, the full softmax still needs to be computed to obtain a normalised probability.</p><p>In order to gain some intuitions about the softmax denominator’s impact on the loss, we will derive the gradient of our loss function $J_{\theta}$ w.r.t. the parameters of our model $\theta$.<br>During training, we aim to minimize the cross-entropy loss of our model for every word $w$ in the training set. This is simply the negative logarithm of the output of our softmax. If you are unsure of this connection, have a look at <a href="http://cs231n.github.io/linear-classify/#softmax-classifier" target="_blank" rel="external">Karpathy’s explanation</a> to gain some more intuitions about the connection between softmax and cross-entropy. The loss of our model is then the following:<br>$$<br>J_{\theta} = − \log \frac{\exp(h^{\text{T}} v^{\prime}_w)}{\sum_{w_i \in V} \exp(h^{\text{T}} v^{\prime}_{w_i})}.<br>$$<br>Note that in practice $J_{\theta}$ would be the average of all negative log-probabilities over the whole corpus. To facilitate the derivation, we decompose $J_{\theta}$ into a sum as $\log \frac{x}{y} = \log x − \log y$:<br>$$<br>J_\theta = - \: h^\top v^{\prime}_{w} + \text{log} \sum_{w_i \in V} \text{exp}(h^\top v’_{w_i})<br>$$<br>For brevity and to conform with the notation of Bengio and Senécal [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:4" target="_blank" rel="external">4</a>, <a href="http://ruder.io/word-embeddings-softmax/index.html#fn:15" target="_blank" rel="external">15</a>] (note that in the first paper, they compute the gradient of the <em>positive</em> logarithm), we replace the dot product $h^\top v’_{w}$ with $- \mathcal{E}(w)$. Our loss then looks like the following:<br>$$<br>J_\theta = \: \mathcal{E}(w) + \text{log} \sum_{w_i \in V} \text{exp}( - \mathcal{E}(w_i))<br>$$<br>For back-propagation, we can now compute the gradient $\nabla$i of $J_{\theta}$ w.r.t. our model’s parameters $\theta$:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \nabla_\theta \text{log} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))<br>$$<br>As the gradient of $\log x$ is $\dfrac{1}{x}$, an application of the chain rule yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)<br>$$<br>We can now move the gradient inside the sum:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \nabla_\theta \: \text{exp}(- \mathcal{E}(w_i))<br>$$<br>As the gradient of $\exp(x)​$ is just $\exp(x)​$, another application of the chain rule yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)) \nabla_\theta (- \mathcal{E}(w_i))<br>$$<br>We can rewrite this as:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} \dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta (- \mathcal{E}(w_i))<br>$$<br>Note that $\dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))}$ is just the softmax probability $P(w_i)$ of $w_i$ (we omit the dependence on the context cc here for brevity). Replacing it yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} P(w_i) \nabla_\theta (- \mathcal{E}(w_i))<br>$$<br>Finally, repositioning the negative coefficient in front of the sum yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) - \sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i)<br>$$<br>Bengio and Senécal (2003) note that the gradient essentially has two parts: a positive reinforcement for the target word $w$ (the first term in the above equation) and a negative reinforcement for all other words $w_i$, which is weighted by their probability (the second term). As we can see, this negative reinforcement is just the expectation $\mathbb{E}_{w_i \sim P}$ of the gradient of $\mathcal{E}$ for all words $w_i$ in $V$:<br>$$<br>\sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i) = \mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)]<br>$$<br>The crux of most sampling-based approach now is to approximate this negative reinforcement in some way to make it easier to compute, since we don’t want to sum over the probabilities for all words in $V$.</p><h2 id="Importance-Sampling"><a href="#Importance-Sampling" class="headerlink" title="Importance Sampling"></a>Importance Sampling</h2><p>We can approximate the expected value $E$ of any probability distribution using the Monte Carlo method, i.e. by taking the mean of random samples of the probability distribution. If we knew the network’s distribution, i.e. $P(w)$, we could thus directly sample mm words $w_1, \cdots ,w_m$ from it and approximate the above expectation with:<br>$$<br>\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{m} \sum\limits^m_{i=1} \nabla_\theta \mathcal{E}(w_i)<br>$$<br>However, in order to sample from the probability distribution $P$, we need to compute $P$, which is just what we wanted to avoid in the first place. We therefore have find some other distribution $Q$ (we call this the proposal distribution), from which it is cheap to sample and which can be used as the basis of Monte-Carlo sampling. Preferably, $Q$ should also be similar to $P$, since we want our approximated expectation to be as accurate as possible. A straightforward choice in the case of language modelling is to simply use the unigram distribution of the training set for $Q$.</p><p>This is essentially what classical Importance Sampling (IS) does: It uses Monte-Carlo sampling to approximate a target distribution $P$ via a proposal distribution $Q$. However, this still requires computing $P(w)$ for every word ww that is sampled. To avoid this, Bengio and Senécal (2003) use a biased estimator that was first proposed by Liu [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:16" target="_blank" rel="external">16</a>]. This estimator can be used when $P(w)$ is computed as a product, which is the case here, since every division can be transformed into a multiplication.</p><p>Essentially, instead of weighting the gradient $\nabla_\theta \mathcal{E}(w_i)$ with the expensive to compute probability $P_{w_i}$, we weight it with a factor that leverages the proposal distribution $Q$. For biased IS, this factor is $\dfrac{1}{R}r(w_i)$ where $r(w) = \dfrac{\text{exp}(- \mathcal{E}(w))}{Q(w)}$ and $R = \sum^m_{j=1} r(w_j)$.</p><p>Note that we use $r$ and $R$ instead of $w$ and $W$ as in Bengio and Senécal (2003, 2008) to avoid name clashes. As we can see, we still compute the numerator of the softmax, but replace the normalisation in the denominator with the proposal distribution $Q$. Our biased estimator that approximates the expectation thus looks like the following:<br>$$<br>\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{R} \sum\limits^m_{i=1} r(w_i) \nabla_\theta \: \mathcal{E}(w_i)<br>$$<br>Note that the fewer samples we use, the worse is our approximation. We additionally need to adjust our sample size during training, as the network’s distribution $P$ might diverge from the unigram distribution $Q$ during training, which leads to divergence of the model, if the sample size that is used is too small. Consequently, Bengio and Senécal introduce a measure to calculate the effective sample size in order to protect against possible divergence. Finally, the authors report a speed-up factor of $19$ over the regular softmax for this method.</p><h2 id="Adaptive-Importance-Sampling"><a href="#Adaptive-Importance-Sampling" class="headerlink" title="Adaptive Importance Sampling"></a>Adaptive Importance Sampling</h2><p>Bengio and Senécal (2008) note that for Importance Sampling, substituting more complex distributions, e.g. bigram and trigram distributions, later in training to combat the divergence of the unigram distribution $Q$ from the model’s true distribution $P$ does not help, as n-gram distributions seem to be quite different from the distribution of trained neural language models. As an alternative, they propose an n-gram distribution that is adapted during training to follow the target distribution $P$ more closely. To this end, they interpolate a bigram distribution and a unigram distribution according to some mixture function, whose parameters they train with SGD for different frequency bins to minimize the Kullback-Leibler divergence between the distribution $Q$ and the target distribution $P$. For experiments, they report a speed-up factor of about $100$.</p><h2 id="Target-Sampling"><a href="#Target-Sampling" class="headerlink" title="Target Sampling"></a>Target Sampling</h2><p>Jean et al. (2015) propose to use Adaptive Importance Sampling for machine translation. In order to make the method more suitable for processing on a GPU with limited memory, they limit the number of target words that need to be sampled from. They do this by partitioning the training set and including only a fixed number of sample words in every partition, which form a subset $V^{\prime}$ of the vocabulary.</p><p>This essentially means that a separate proposal distribution $Q_i$ can be used for every partition ii of the training set, which assigns equal probability to all words included in the vocabulary subset $V’_i$ and zero probability to all other words.</p><h2 id="Noise-Contrastive-Estimation"><a href="#Noise-Contrastive-Estimation" class="headerlink" title="Noise Contrastive Estimation"></a>Noise Contrastive Estimation</h2><p>Noise Contrastive Estimation (NCE) (Gutmann and Hyvärinen) [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:17" target="_blank" rel="external">17</a>] is proposed by Mnih and Teh [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:18" target="_blank" rel="external">18</a>] as a more stable sampling method than Importance Sampling (IS), as we have seen that IS poses the risk of having the proposal distribution $Q$ diverge from the distribution $P$ that should be optimized. In contrast to the former, NCE does not try to estimate the probability of a word directly. Instead, it uses an auxiliary loss that also optimises the goal of maximizing the probability of correct words.</p><p>Recall the pairwise-ranking criterion of Collobert and Weston (2008) that ranks positive windows higher than “corrupted” windows, which we discussed in the <a href="http://sebastianruder.com/word-embeddings-1/index.html" target="_blank" rel="external">previous post</a>. NCE does a similar thing: We train a model to differentiate the target word from noise. We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive, genuine data from noise samples, as can be seen in Figure 4 below.</p><p><img src="http://ruder.io/content/images/2016/06/negative_sampling.png" alt="Noise Contrastive Estimation"></p><p>Figure 4: Noise Contrastive Estimation (Stephan Gouws’ PhD dissertation [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:24" target="_blank" rel="external">24</a>])</p><p>For every word $w_i$ given its context $c_i$ of $n$ previous words $w_{t-1} , \cdots , w_{t-n+1}$ in the training set, we thus generate $k$ noise samples $w~ik$ from a noise distribution $Q$. As in IS, we can sample from the unigram distribution of the training set. As we need labels to perform our binary classification task, we designate all correct words $w_i$ given their context $c_i$ as true ($y=1$) and all noise samples $\tilde{w}_{ik}$ as false ($y=0$).</p><p>We can now use logistic regression to minimize the negative log-likelihood, i.e. cross-entropy of our training examples against the noise (conversely, we could also maximize the <em>positive</em> log-likelihood as some papers do):<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \mathbb{E}_{\tilde{w}_{ik} \sim Q} [ \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]]<br>$$<br>Instead of computing the expectation $\mathbb{E}_{\tilde{w}_{ik} \sim Q}$ of our noise samples, which would still require summing over all words in $V$ to predict the normalised probability of a negative label, we can again take the mean with the Monte Carlo approximation:<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \sum_{j=1}^k \dfrac{1}{k} \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]<br>$$<br>which reduces to:<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + \: \sum_{j=1}^k \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]<br>$$<br>By generating $k$ noise samples for every genuine word $w_i$ given its context $c$, we are effectively sampling words from two different distributions: Correct words are sampled from the empirical distribution of the training set $P_{\text{train}}$ and depend on their context $c$, whereas noise samples come from the noise distribution $Q$. We can thus represent the probability of sampling either a positive or a noise sample as a mixture of those two distributions, which are weighted based on the number of samples that come from each:<br>$$<br>P(y, w \: | \: c) = \dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)<br>$$<br>Given this mixture, we can now calculate the probability that a sample came from the training $P_{\text{train}}$ distribution as a conditional probability of $y$ given $w$ and $c$:<br>$$<br>P(y=1\:|\:w,c)= \dfrac{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)}{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)}<br>$$<br>which can be simplified to:<br>$$<br>P(y=1\:|\:w,c)= \dfrac{P_{\text{train}}(w \: | \: c)}{P_{\text{train}}(w \: | \: c) + k \: Q(w)}<br>$$<br>As we don’t know $P_{\text{train}}$ (which is what we would like to calculate), we replace $P_{\text{train}}$ with the probability of our model $P$:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{P(w \: | \: c)}{P(w \: | \: c) + k \: Q(w)}<br>$$<br>The probability of predicting a noise sample ($y=0$) is then simply $P(y=0\:|\:w,c) = 1 - P(y=1\:|\:w,c)$. Note that computing $P(w|c)$, i.e. the probability of a word $w$ given its context $c$ is essentially the definition of our softmax:<br>$$<br>P(w \: | \: c) = \dfrac{\text{exp}({h^\top v’_{w}})}{\sum_{w_i \in V} \text{exp}({h^\top v’_{w_i}})}<br>$$<br>For notational brevity and unambiguity, let us designate the denominator of the softmax with $Z(c)$, since the denominator only depends on $h$, which is generated from $c$ (assuming a fixed $V$). The softmax then looks like this:</p><p>$$<br>P(w \: | \: c) = \dfrac{\text{exp}({h^\top v’_{w}})}{Z(c)}<br>$$<br>Having to compute $P(w|c)$ means that – again – we need to compute $Z(c)$, which requires us to sum over the probabilities of all words in $V$. In the case of NCE, there exists a neat trick to circumvent this issue: We can treat the normalisation denominator $Z(c)$ as a parameter that the model can learn.</p><p>Mnih and Teh (2012) and Vaswani et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:20" target="_blank" rel="external">20</a>] actually keep $Z(c)$ fixed at $1$, which they report does not affect the model’s performance. This assumption has the nice side-effect of reducing the model’s parameters, while ensuring that the model self-normalises by not depending on the explicit normalisation in $Z(c)$. Indeed, Zoph et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:19" target="_blank" rel="external">19</a>] find that even when learned, $Z(c)$ is close to $1$ and has low variance.</p><p>If we thus set $Z(c)$ to $1$ in the above softmax equation, we are left with the following probability of word $w$ given a context $c$:</p><p>$$<br>P(w \: | \: c) = \text{exp}({h^\top v’_{w}})<br>$$<br>We can now insert this term in the above equation to compute $P(y=1|w,c)$:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + k \: Q(w)}<br>$$<br>Inserting this term in turn in our logistic regression objective finally yields the full NCE loss:</p><p>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{\text{exp}({h^\top v’_{w_i}})}{\text{exp}({h^\top v’_{w_i}}) + k \: Q(w_i)} + \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{\text{exp}({h^\top v’_{\tilde{w}_{ij}}})}{\text{exp}({h^\top v’_{\tilde{w}_{ij}}}) + k \: Q(\tilde{w}_{ij})})]<br>$$<br>Note that NCE has nice theoretical guarantees: It can be shown that as we increase the number of noise samples $k$, the NCE derivative tends towards the gradient of the softmax function. Mnih and Teh (2012) report that $25$ noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about $45$. For more information on NCE, Chris Dyer has published some excellent notes [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:21" target="_blank" rel="external">21</a>].</p><p>One caveat of NCE is that as typically different noise samples are sampled for every training word ww, the noise samples and their gradients cannot be stored in dense matrices, which reduces the benefit of using NCE with GPUs, as it cannot benefit from fast dense matrix multiplications. Jozefowicz et al. (2016) and Zoph et al. (2016) independently propose to share noise samples across all training words in a mini-batch, so that NCE gradients can be computed with dense matrix operations, which are more efficient on GPUs.</p><h3 id="Similarity-between-NCE-and-IS"><a href="#Similarity-between-NCE-and-IS" class="headerlink" title="Similarity between NCE and IS"></a>Similarity between NCE and IS</h3><p>Jozefowicz et al. (2016) show that NCE and IS are not only similar as both are sampling-based approaches, but are strongly connected. While NCE uses a binary classification task, they show that IS can be described similarly using a surrogate loss function: Instead of performing binary classification with a logistic loss function like NCE, IS then optimises a multi-class classification problem with a softmax and cross-entropy loss function. They observe that as IS performs multi-class classification, it may be a better choice for language modelling, as the loss leads to tied updates between the data and noise samples rather than independent updates as with NCE. Indeed, Jozefowicz et al. (2016) use IS for language modelling and obtain state-of-the-art performance (as mentioned above) on the 1B Word benchmark.</p><h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p>Negative Sampling (NEG), the objective that has been popularised by Mikolov et al. (2013), can be seen as an approximation to NCE. As we have mentioned above, NCE can be shown to approximate the loss of the softmax as the number of samples kk increases. NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modelling.</p><p>NEG also uses a logistic loss function to minimise the negative log-likelihood of words in the training set. Recall that NCE calculated the probability that a word $w$ comes from the empirical training distribution $P_{\text{train}}$ given a context $c$ as follows:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + k \: Q(w)}<br>$$<br>The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible. For this reason, it sets the most expensive term, $kQ(w)$ to $1$, which leaves us with:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + 1}<br>$$<br>$kQ(w)=1$ is exactly then true, when $k=|V|$ and $Q$ is a uniform distribution. In this case, NEG is equivalent to NCE. The reason we set $kQ(w)=1$ and not to some other constant can be seen by rewriting the equation, as $P(y=1|w,c)$ can be transformed into the sigmoid function:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{1}{1 + \text{exp}({-h^\top v’_{w}})}<br>$$<br>If we now insert this back into the logistic regression loss from before, we get:</p><p>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v’_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{1}{1 + \text{exp}({-h^\top v’_{\tilde{w}_{ij}}})}]<br>$$<br>By simplifying slightly, we obtain:</p><p>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v’_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (\dfrac{1}{1 + \text{exp}({h^\top v’_{\tilde{w}_{ij}}})}]<br>$$<br>Setting $\sigma(x) = \dfrac{1}{1 + \text{exp}({-x})}$ finally yields the NEG loss:<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \sigma(h^\top v’_{w_i}) + \: \sum_{j=1}^k \:\text{log} \: \sigma(-h^\top v’_{\tilde{w}_{ij}})]<br>$$<br>To conform with the notation of Mikolov et al. (2013), $h$ must be replaced with $v_{w_{I}}$, v′wivwi′ with v′wOvwO′ and vw~ijvw~ij with v′wivwi′. Also, in contrast to Mikolov’s NEG objective, we a) optimise the objective over the whole corpus, b) minimise negative log-likelihood instead of maximising positive log-likelihood (as mentioned before), and c) have already replaced the expectation Ew~ik∼QEw~ik∼Q with its Monte Carlo approximation. For more insights on the derivation of NEG, have a look at Goldberg and Levy’s notes [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:22" target="_blank" rel="external">22</a>].</p><p>We have seen that NEG is only equivalent to NCE when $k=|V|$ and $Q$ is uniform. In all other cases, NEG only approximates NCE, which means that it will not directly optimise the likelihood of correct words, which is key for language modelling. While NEG may thus be useful for learning word embeddings, its lack of asymptotic consistency guarantees makes it inappropriate for language modelling.</p><h2 id="Self-Normalisation"><a href="#Self-Normalisation" class="headerlink" title="Self-Normalisation"></a>Self-Normalisation</h2><p>Even though the self-normalisation technique proposed by Devlin et al. <a href="http://ruder.io/word-embeddings-softmax/index.html#fn:23" target="_blank" rel="external">23</a> is not a sampling-based approach, it provides further intuitions on self-normalisation of language models, which we briefly touched upon. We previously mentioned in passing that by setting the denominator $Z(c)$ of the NCE loss to $1$, the model essentially self-normalises. This is a useful property as it allows us to skip computing the expensive normalisation in $Z(c)$.</p><p>Recall that our loss function $J_{\theta}$ minimises the negative log-likelihood of all words $w_i$ in our training data:</p><p>$$<br>J_\theta = - \sum\limits_i [\text{log} \: \dfrac{\text{exp}({h^\top v’_{w_i}})}{Z(c)}]<br>$$<br>We can decompose the softmax into a sum as we did before:</p><p>$$<br>J_\theta \: P(w \: | \: c) = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c)]<br>$$<br>If we are able to constrain our model so that it sets $Z(c)=1$ or similarly $\log Z(c)=0$, then we can avoid computing the normalisation in $Z(c)$ altogether. Devlin et al. (2014) thus propose to add a squared error penalty term to the loss function that encourages the model to keep $\log Z(c)$ as close as possible to $0$:</p><p>$$<br>J_\theta = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c) - \alpha \:(\text{log}(Z(c)) - 0)^2]<br>$$<br>which can be rewritten as:</p><p>$$<br>J_\theta = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c) - \alpha \: \text{log}^2 Z(c)]<br>$$<br>where αα allows us to trade-off between model accuracy and mean self-normalisation. By doing this, we can essentially guarantee that $Z(c)$ will be as close to $1$ as we want. At decoding time in their MT system, Devlin et al. (2014) then set the denominator of the softmax to $1$ and only use the numerator for computing P(w|c)P(w|c) together with their penalty term:</p><p>$$<br>J_\theta = - \sum\limits_i [h^\top v’_{w_i} - \alpha \: \text{log}^2 Z(c)]<br>$$<br>They report that self-normalisation achieves a speed-up factor of about $15$, while only resulting in a small degradation of BLEU scores compared to a regular non-self-normalizing neural language model.</p><h2 id="Infrequent-Normalisation"><a href="#Infrequent-Normalisation" class="headerlink" title="Infrequent Normalisation"></a>Infrequent Normalisation</h2><p>Andreas and Klein [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:11" target="_blank" rel="external">11</a>] suggest that it should even be sufficient to only normalise a fraction of the training examples and still obtain approximate self-normalising behaviour. They thus propose Infrequent Normalisation (IN), which down-samples the penalty term, making this a sampling-based approach.</p><p>Let us first decompose the sum of the previous loss $J_{\theta}$ into two separate sums:</p><p>$$<br>J_\theta = - \sum\limits_i h^\top v’_{w_i} + \alpha \sum\limits_i \text{log}^2 Z(c)<br>$$<br>We can now down-sample the second term by only computing the normalisation for a subset $C$ of words $w_j$ and thus of contexts $c_j$ (as $Z(c)$ only depends on the context $c$) in the training data:</p><p>$$<br>J_\theta = - \sum\limits_i h^\top v’_{w_i} + \dfrac{\alpha}{\gamma} \sum\limits_{c_j \in C} \text{log}^2 Z(c_j)<br>$$<br>where $\gamma$ controls the size of the subset $C$. Andreas and Klein (2015) suggest that IF combines the strengths of NCE and self-normalisation as it does not require computing the normalisation for all training examples (which NCE avoids entirely), but like self-normalisation allows trading-off between the accuracy of the model and how well normalisation is approximated. They observe a speed-up factor of $10$ when normalising only a tenth of the training set, with no noticeable performance penalty.</p><h3 id="Other-Approaches"><a href="#Other-Approaches" class="headerlink" title="Other Approaches"></a>Other Approaches</h3><p>So far, we have focused exclusively on approximating or even entirely avoiding the computation of the softmax denominator $Z(c)$, as it is the most expensive term in the computation. We have thus not paid particular attention to $h^\top v’_{w}$, i.e. the dot-product between the penultimate layer representation hh and output word embedding $v^{\prime}_w$. Vijayanarasimhan et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:12" target="_blank" rel="external">12</a>] propose fast locality-sensitive hashing to approximate $h^\top v^{\prime}_{w}$. However, while this technique accelerates the model at test time, during training, these speed-ups virtually vanish as embeddings must be re-indexed and the batch size increases.</p><h1 id="Which-Approach-to-Choose"><a href="#Which-Approach-to-Choose" class="headerlink" title="Which Approach to Choose?"></a>Which Approach to Choose?</h1><p>Having reviewed the most popular softmax-based and sampling-based approaches, we have shown that there are plenty of alternatives to the good ol’ softmax and almost all of them promise a significant speed-up and equivalent or at most marginally deteriorated performance. This naturally poses the question which approach is the best for a particular task.</p><table><thead><tr><th>Approach</th><th>Speed-upfactor</th><th>Duringtraining?</th><th>Duringtesting?</th><th>Performance(small vocab)</th><th>Performance(large vocab)</th><th>Proportion ofparameters</th></tr></thead><tbody><tr><td>Softmax</td><td>1x</td><td>-</td><td>-</td><td>very good</td><td>very poor</td><td>100%</td></tr><tr><td>Hierarchical Softmax</td><td>25x (50-100x)</td><td>X</td><td>-</td><td>very poor</td><td>very good</td><td>100%</td></tr><tr><td>Differentiated Softmax</td><td>2x</td><td>X</td><td>X</td><td>very good</td><td>very good</td><td>&lt; 100%</td></tr><tr><td>CNN-Softmax</td><td>-</td><td>X</td><td>-</td><td>-</td><td>bad - good</td><td>30%</td></tr><tr><td>Importance Sampling</td><td>(19x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>AdaptiveImportance Sampling</td><td>(100x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>Target Sampling</td><td>2x</td><td>X</td><td>-</td><td>good</td><td>bad</td><td>100%</td></tr><tr><td>Noise ContrastiveEstimation</td><td>8x (45x)</td><td>X</td><td>-</td><td>very bad</td><td>very bad</td><td>100%</td></tr><tr><td>Negative Sampling</td><td>(50-100x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>Self-Normalisation</td><td>(15x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>InfrequentNormalisation</td><td>6x (10x)</td><td>X</td><td>-</td><td>very good</td><td>good</td><td>100%</td></tr></tbody></table><p>Table 1: Comparison of approaches to approximate the softmax for language modelling.</p><p>We compare the performance of the approaches we discussed in this post for language modelling in Table 1. Speed-up factors and performance are based on the experiments by Chen et al. (2015), while we show speed-up factors reported by the authors of the original papers in brackets. The third and fourth columns indicate if the speed-up is achieved during training and testing respectively. Note that divergence of speed-up factors might be due to unoptimised implementations or the fact that the original authors might not have had access to GPUs, which benefit the regular softmax more than some of the other approaches. Performance for approaches where no comparison is available should largely be analogous to similar approaches, i.e. Self-Normalisation should achieve similar performance as Infrequent Normalisation and Importance Sampling and Adaptive Importance Sampling should achieve similar performance as Target Sampling. The performance of CNN-Softmax is as reported by Jozefowicz et al. (2016) and ranges from bad to good depending on the size of the correction. Of all approaches, only CNN-Softmax achieves a substantial reduction in parameters as the other approaches still require storing output embeddings. Differentiated Softmax reduces parameters by being able to store a sparse weight matrix.</p><p>As it always is, there is no clear winner that beats all other approaches on all datasets or tasks. For language modelling, the regular softmax still achieves very good performance on small vocabulary datasets, such as the Penn Treebank, and even performs well on medium datasets, such as Gigaword, but does very poorly on large vocabulary datasets, e.g. the 1B Word Benchmark. Target Sampling, Hierarchical Softmax, and Infrequent Normalisation in turn do better with large vocabularies.</p><p>Differentiated Softmax generally does well for both small and large vocabularies and is the only approach that ensures a speed-up at test time. Interestingly, Hierarchical Softmax (HS) performs very poorly with small vocabularies. However, of all methods, HS is the fastest and processes most training examples in a given time frame. While NCE performs well with large vocabularies, it is generally worse than the other methods. Negative Sampling does not work well for language modelling, but it is generally superior for learning word representations, as attested by word2vec’s success. Note that all results should be taken with a grain of salt: Chen et al. (2015) report having difficulties using Noise Contrastive Estimation in practice; Kim et al. (2016) use Hierarchical Softmax to achieve state-of-the-art with a small vocabulary, while Importance Sampling is used by the state-of-the-art language model by Jozefowicz et al. (2016) on a dataset with a large vocabulary.</p><p>Finally, if you are looking to actually use the described methods, TensorFlow has <a href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#candidate-sampling" target="_blank" rel="external">implementations</a> for a few sampling-based approaches and also explains the differences between some of them <a href="https://www.tensorflow.org/extras/candidate_sampling.pdf" target="_blank" rel="external">here</a>.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>This overview of different methods to approximate the softmax attempted to provide you with intuitions that can not only be applied to improve and speed-up learning word representations, but are also relevant for language modelling and machine translation. As we have seen, most of these approaches are closely related and are driven by one uniting factor: the necessity to approximate the expensive normalisation in the denominator of the softmax. With these approaches in mind, I hope you feel now better equipped to train and understand your models and that you might even feel ready to work on learning better word representations yourself.</p><p>As we have seen, learning word representations is a vast field and many factors are relevant for success. In the previous blog post, we looked at the architectures of popular models and in this blog post, we investigated more closely a key component, the softmax layer. In the next one, we will introduce GloVe, a method that relies on matrix factorisation rather than language modelling, and turn our attention to other hyperparameters that are essential for successfully learning word embeddings.</p><p><strong>As always, let me know about any mistakes I made and approaches I missed in the comments below.</strong></p><h1 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h1><p>If you found this blog post helpful, please consider citing it as:</p><p><em>Sebastian Ruder. On word embeddings - Part 2: Approximating the Softmax. <a href="http://ruder.io/word-embeddings-softmax" target="_blank" rel="external">http://ruder.io/word-embeddings-softmax</a>, 2016.</em></p><h1 id="Other-blog-posts-on-word-embeddings"><a href="#Other-blog-posts-on-word-embeddings" class="headerlink" title="Other blog posts on word embeddings"></a>Other blog posts on word embeddings</h1><p>If you want to learn more about word embeddings, these other blog posts on word embeddings are also available:</p><ul><li><a href="http://sebastianruder.com/word-embeddings-1/index.html" target="_blank" rel="external">On word embeddings - Part 1</a></li><li><a href="http://sebastianruder.com/secret-word2vec/index.html" target="_blank" rel="external">On word embeddings - Part 3: The secret ingredients of word2vec</a></li><li><a href="http://sebastianruder.com/cross-lingual-embeddings/index.html" target="_blank" rel="external">Unofficial Part 4: A survey of cross-lingual embedding models</a></li><li><a href="http://ruder.io/word-embeddings-2017/index.html" target="_blank" rel="external">Unofficial Part 5: Word embeddings in 2017 - Trends and future directions</a></li></ul><h1 id="Translations"><a href="#Translations" class="headerlink" title="Translations"></a>Translations</h1><p>This blog post has been translated into the following languages:</p><ul><li><a href="http://geek.csdn.net/news/detail/135736" target="_blank" rel="external">Chinese</a></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:1" target="_blank" rel="external"></a></li><li>Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:2" target="_blank" rel="external"></a></li><li>Morin, F., &amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats, 5. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:3" target="_blank" rel="external"></a></li><li>Bengio, Y., &amp; Senécal, J.-S. (2003). Quick Training of Probabilistic Neural Nets by Importance Sampling. AISTATS. <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/submit_aistats2003.pdf" target="_blank" rel="external">http://www.iro.umontreal.ca/~lisa/pointeurs/submit_aistats2003.pdf</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:4" target="_blank" rel="external"></a></li><li>Shannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. <a href="http://doi.org/10.1002/j.1538-7305.1951.tb01366.x" target="_blank" rel="external">http://doi.org/10.1002/j.1538-7305.1951.tb01366.x</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:5" target="_blank" rel="external"></a></li><li>Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from <a href="http://arxiv.org/abs/1602.02410" target="_blank" rel="external">http://arxiv.org/abs/1602.02410</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:6" target="_blank" rel="external"></a></li><li>Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738, 1–19. Retrieved from <a href="http://arxiv.org/abs/1411.2738" target="_blank" rel="external">http://arxiv.org/abs/1411.2738</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:7" target="_blank" rel="external"></a></li><li>Mnih, A., &amp; Hinton, G. E. (2008). A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, 1–8. Retrieved from <a href="http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf" target="_blank" rel="external">http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:8" target="_blank" rel="external"></a></li><li>Chen, W., Grangier, D., &amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. Retrieved from <a href="http://arxiv.org/abs/1512.04906" target="_blank" rel="external">http://arxiv.org/abs/1512.04906</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:9" target="_blank" rel="external"></a></li><li>Jean, S., Cho, K., Memisevic, R., &amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from <a href="http://www.aclweb.org/anthology/P15-1001" target="_blank" rel="external">http://www.aclweb.org/anthology/P15-1001</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:10" target="_blank" rel="external"></a></li><li>Andreas, J., &amp; Klein, D. (2015). When and why are log-linear models self-normalizing? Naacl-2015, 244–249. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:11" target="_blank" rel="external"></a></li><li>Vijayanarasimhan, S., Shlens, J., Monga, R., &amp; Yagnik, J. (2015). Deep Networks With Large Output Spaces. Iclr, 1–9. Retrieved from <a href="http://arxiv.org/abs/1412.7479" target="_blank" rel="external">http://arxiv.org/abs/1412.7479</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:12" target="_blank" rel="external"></a></li><li>Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from <a href="http://arxiv.org/abs/1508.06615" target="_blank" rel="external">http://arxiv.org/abs/1508.06615</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:13" target="_blank" rel="external"></a></li><li>Ling, W., Trancoso, I., Dyer, C., &amp; Black, A. W. (2016). Character-based Neural Machine Translation. ICLR, 1–11. Retrieved from <a href="http://arxiv.org/abs/1511.04586" target="_blank" rel="external">http://arxiv.org/abs/1511.04586</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:14" target="_blank" rel="external"></a></li><li>Bengio, Y., &amp; Senécal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713–722. <a href="http://doi.org/10.1109/TNN.2007.912312" target="_blank" rel="external">http://doi.org/10.1109/TNN.2007.912312</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:15" target="_blank" rel="external"></a></li><li>Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer. <a href="http://doi.org/10.1017/CBO9781107415324.004" target="_blank" rel="external">http://doi.org/10.1017/CBO9781107415324.004</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:16" target="_blank" rel="external"></a></li><li>Gutmann, M., &amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. International Conference on Artificial Intelligence and Statistics, 1–8. Retrieved from <a href="http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf" target="_blank" rel="external">http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:17" target="_blank" rel="external"></a></li><li>Mnih, A., &amp; Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICML’12), 1751–1758. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:18" target="_blank" rel="external"></a></li><li>Zoph, B., Vaswani, A., May, J., &amp; Knight, K. (2016). Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. NAACL. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:19" target="_blank" rel="external"></a></li><li>Vaswani, A., Zhao, Y., Fossum, V., &amp; Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), (October), 1387–1392. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:20" target="_blank" rel="external"></a></li><li>Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. Arxiv preprint. Retrieved from <a href="http://arxiv.org/abs/1410.8251" target="_blank" rel="external">http://arxiv.org/abs/1410.8251</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:21" target="_blank" rel="external"></a></li><li>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method. arXiv Preprint arXiv:1402.3722, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722" target="_blank" rel="external">http://arxiv.org/abs/1402.3722</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:22" target="_blank" rel="external"></a></li><li>Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., &amp; Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. Proc. ACL’2014, 1370–1380. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:23" target="_blank" rel="external"></a></li><li>Gouws, S. (2016). Training neural word embeddings for transfer learning and translation (Doctoral dissertation, Stellenbosch: Stellenbosch University). <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:24" target="_blank" rel="external"></a></li></ol><p>Credit for the cover image goes to <a href="http://stephangouws.com/" target="_blank" rel="external">Stephan Gouws</a> who included the image in his <a href="http://scholar.sun.ac.za/handle/10019.1/98758" target="_blank" rel="external">PhD dissertation</a> and in the <a href="https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html" target="_blank" rel="external">Tensorflow word2vec tutorial</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is the second post in a series on word embeddings and representation learning. In the &lt;a href=&quot;http://sebastianruder.com/word-embedd
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>DropConnect Implementation in Python and TensorFlow [Repost]</title>
    <link href="http://yoursite.com/2018/01/15/DropConnect-Implementation-in-Python-and-TensorFlow-Repost/"/>
    <id>http://yoursite.com/2018/01/15/DropConnect-Implementation-in-Python-and-TensorFlow-Repost/</id>
    <published>2018-01-15T14:36:19.000Z</published>
    <updated>2018-01-15T14:38:56.183Z</updated>
    
    <content type="html"><![CDATA[<p>Source post is <a href="https://nickcdryan.wordpress.com/2017/06/13/dropconnect-implementation-in-python-and-tensorflow/" target="_blank" rel="external">here</a>.</p><hr><p>I wouldn’t expect DropConnect to appear in TensorFlow, Keras, or Theano since, as far as I know, it’s used pretty rarely and doesn’t seem as well-studied or demonstrably more useful than its cousin, Dropout. However, there don’t seem to be any implementations out there, so I’ll provide a few ways of doing so.</p><p><img src="https://nickcdryan.files.wordpress.com/2017/06/screen-shot-2017-06-13-at-3-01-19-am.png?w=840" alt="Screen Shot 2017-06-13 at 3.01.19 AM"></p><p>For the briefest of refreshers, DropConnect (<a href="http://proceedings.mlr.press/v28/wan13.pdf" target="_blank" rel="external">Wan et al.</a>) regularizes networks like Dropout. Instead of dropping neurons, DropConnect regularizes by randomly dropping a subset of weights. A binary mask drawn from a Bernoulli distribution is applied to the original weight matrix (we’re just setting some connections to 0 with a certain probability):</p><p><img src="https://s0.wp.com/latex.php?latex=output+%3D+a%28%28M%C2%A0%5Codot+W%29v%29&amp;bg=ffffff&amp;fg=1a1a1a&amp;s=0" alt="output = a((M \odot W)v)"></p><p>where a is an activation function, v is input matrix, W is weight matrix, <img src="https://s0.wp.com/latex.php?latex=%5Codot&amp;bg=ffffff&amp;fg=1a1a1a&amp;s=0" alt="\odot"> is Hadamard (element-wise multiplication), and M is the binary mask drawn from a Bernoulli distribution with probability p.</p><p>Pure Python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> operator</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask_size_helper</span><span class="params">(args)</span>:</span></div><div class="line">    <span class="comment"># multiply n dimensions to get array size</span></div><div class="line">    <span class="keyword">return</span> reduce(operator.mul, args) </div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dropconnect_mask</span><span class="params">(dc_keep_prob, dimensions)</span>:</span></div><div class="line">    <span class="comment"># get binary mask of size=*dimensions from binomial dist. with dc_keep_prob = prob of drawing a 1</span></div><div class="line">    mask_vector = np.random.binomial(<span class="number">1</span>, dc_keep_prob, mask_size_helper(dimensions))</div><div class="line">    <span class="comment"># reshape mask to correct dimensions (we could just broadcast, but that's messy)</span></div><div class="line">    mask_array = mask_vector.reshape(dimensions)</div><div class="line">    <span class="keyword">return</span> mask_array</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropconnect</span><span class="params">(W, dc_keep_prob)</span>:</span></div><div class="line">    dimensions = W.shape</div><div class="line">    <span class="keyword">return</span> W * create_dropconnect_mask(dc_keep_prob, dimensions)</div></pre></td></tr></table></figure><p>TensorFlow (unnecessarily hard way):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropconnect</span><span class="params">(W, p)</span>:</span></div><div class="line">    M_vector = tf.multinomial(tf.log([[<span class="number">1</span>-p, p]]), np.prod(W_shape))</div><div class="line">    M = tf.reshape(M_vector, W_shape)</div><div class="line">    M = tf.cast(M, tf.float32)</div><div class="line">    <span class="keyword">return</span> M * W</div></pre></td></tr></table></figure><p>TensorFlow (easy way / recommended):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropconnect</span><span class="params">(W, p)</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.nn.dropout(W, keep_prob=p) * p</div></pre></td></tr></table></figure><p>Yes, sadly after a good amount of time spent searching for existing implementations and then creating my own, I took a look at the <a href="https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/ops/nn_ops.py" target="_blank" rel="external">dropout source code</a> and found that plain old dropout does the job so long as you remember to scale the weight matrix back down by keep_prob. After realizing that a connection weight matrix used for DropConnect is compatible input for the layer of neurons used in dropout, the only actual implementation difference between Dropout and DropConnect on TensorFlow is whether or not the weights in the masked matrix get scaled up (to preserve the expected sum).</p><p>I find DropConnect interesting, not so much as a regularization method but for some novel extensions that I’d like to try. I’ve played around with using keep_prob in our new DropConnect function as a trainable variable in the graph so that, if you incorporate keep_prob into the loss function in a way that creates interesting gradients, you can punish your network for the amount of connections it makes between neurons.</p><p>More interesting would be to see if we can induce modularity in the network by persisting dropped connections. That is, instead of randomly dropping an entirely new subset of connections at each training example, connections would drop and stay dropped perhaps as a result of the input data class or the connection’s contribution to deeper layers. For another post…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Source post is &lt;a href=&quot;https://nickcdryan.wordpress.com/2017/06/13/dropconnect-implementation-in-python-and-tensorflow/&quot; target=&quot;_blank&quot;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Short Video Title Classification</title>
    <link href="http://yoursite.com/2017/12/06/Short-Video-Title-Classification-Problem/"/>
    <id>http://yoursite.com/2017/12/06/Short-Video-Title-Classification-Problem/</id>
    <published>2017-12-06T07:22:42.000Z</published>
    <updated>2017-12-06T07:27:52.484Z</updated>
    
    <content type="html"><![CDATA[<p>本文档是<a href="https://github.com/ewanlee/video_title_classification" target="_blank" rel="external">Github项目</a>的流程解释文档，具体实现请移步。</p><p>本项目解决的是视频短标题的多分类问题，目前涉及到33个类，所采用的算法包括TextCNN，TextRNN，TextRCNN以及HAN。目前效果最好的是TextCNN算法。</p><p>项目流程大体框架如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/1.png" alt="1"></p><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>数据预处理部分主要涉及到的文件有：</p><ul><li><code>ordered_set.py</code></li><li><code>preprocess.py</code></li></ul><p>大致流程如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/2.png" alt="2"></p><h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><p>初始的文件包括三个：</p><ul><li><code>all_video_info.txt</code> 该文件是后两个数据的合并，作为数据预处理算法输入</li><li><code>all_video_info_month_day.txt</code>（这里的month和day由具体数值替换）这类文件包含多个，<strong>只使用最新的</strong>，是正式的标题数据， 包括已标记的以及未标记的</li><li><code>add_signed_video_info.txt</code> 该文件是从其他数据库中选取的经人工标注的数据，只含有已标记的标题</li></ul><p>所有文件的格式都是一样的，每一行代表一个样本，分为四列，中间用制表符间隔。</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/3.png" alt="3"></p><p>其中第一列代表视频URL；第二列为该视频类别是否经过算法修改，最开始全都为0；第三列为视频标签；第四列为视频标题。</p><p>视频标签的映射表如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/4.png" alt="4"></p><p>在数据加载部分，我们将数据分为有标记数据以及无标记数据，有标记数据将用来训练以及测试分类器，然后用训练好的分类器预测无标记数据的标签。</p><p>分类的依据首先是根据视频标签是否为0，如果为0，代表视频是未标记的。其次，已标记的数据中有些类别是会对算法造成干扰，这里我们也将其去掉。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>load_data</code>方法。</p><h2 id="去除特殊符号"><a href="#去除特殊符号" class="headerlink" title="去除特殊符号"></a>去除特殊符号</h2><p>由于视频标题中存在一些表情等特殊符号，在这个阶段将其去掉。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>remove_emoji</code>方法。</p><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>本项目采用结巴分词作为分词器。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>cut</code>方法。</p><h2 id="去停止词"><a href="#去停止词" class="headerlink" title="去停止词"></a>去停止词</h2><p>本项目采用了<code>data/stopword.dic</code>文件中的停止词表，值得注意的是，句子去停止词前后去停止词后，单词的相对顺序保持不变。这里我们采用了有序集合（具体实现在<code>ordered_set.py</code>文件中）实现。</p><p>经过这一步之后，句子中重复的非停止词将只会取一次。但是由于视频标题较短，出现重复词的概率非常小，因此不会有太大影响。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>remove_stop_words</code>方法。</p><h2 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h2><p>将所有视频标题经过分词后的单词汇总起来建立一个词典，供后续句子建模使用。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>vocab_build</code>方法。</p><h2 id="句子建模"><a href="#句子建模" class="headerlink" title="句子建模"></a>句子建模</h2><p>将分词后的视频标题中的每个词替换为其在词典中的序号，这样每个标题将会转换为由一串数组构成的向量。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>word2index</code>方法。</p><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>之前提到过，本文一共运用了四种深度学习模型，采用tensorflow框架，训练过程中涉及到的文件分为两类：</p><ul><li>模型文件， 包括<code>textcnn.py</code>, <code>textrnn.py</code>, <code>textrcnn.py</code>以及<code>han.py</code></li><li>训练文件，包括<code>train_cnn.py</code>, <code>train_rnn.py</code>, <code>train_rcnn.py</code>以及<code>train_han.py</code></li></ul><p>模型文件定义了具体的模型，本篇文档将不会具体地讲解实现代码，只会从理论层面介绍模型。训练文件包含了算法的训练过程，由于不同算法的训练流程一致，这里单挑TextCNN讲解。</p><p>下面开始介绍模型，如果只关注实现可以跳过到训练部分。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>分布式表示（Distributed Representation）是Hinton 在1986年提出的，基本思想是将每个词表达成 n 维稠密、连续的实数向量，与之相对的one-hot encoding向量空间只有一个维度是1，其余都是0。分布式表示最大的优点是具备非常powerful的特征表达能力，比如 n 维向量每维 k 个值，可以表征 $k^n$个概念。事实上，不管是神经网络的隐层，还是多个潜在变量的概率主题模型，都是应用分布式表示。下图是03年Bengio在 <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a> 的网络结构：</p><p><img src="https://pic3.zhimg.com/50/v2-dc007baa415cf1674df6d323419cc2de_hd.jpg" alt="img"></p><p>这篇文章提出的神经网络语言模型（NNLM，Neural Probabilistic Language Model）采用的是文本分布式表示，即每个词表示为稠密的实数向量。NNLM模型的目标是构建语言模型：</p><p><img src="https://pic1.zhimg.com/50/v2-855f785d33895960712509982199c4b4_hd.jpg" alt="img"></p><p>词的分布式表示即词向量（word embedding）是训练语言模型的一个附加产物，即图中的Matrix C。</p><p>尽管Hinton 86年就提出了词的分布式表示，Bengio 03年便提出了NNLM，词向量真正火起来是google Mikolov 13年发表的两篇word2vec的文章 <a href="http://ttic.uchicago.edu/%7Ehaotang/speech/1301.3781.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a>和<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="external">Distributed Representations of Words and Phrases and their Compositionality</a>，更重要的是发布了简单好用的<strong>word2vec工具包</strong>，在语义维度上得到了很好的验证，极大的推进了文本分析的进程。下图是文中提出的CBOW 和 Skip-Gram两个模型的结构，基本类似于NNLM，不同的是模型去掉了非线性隐层，预测目标不同，CBOW是上下文词预测当前词，Skip-Gram则相反。</p><p><img src="https://pic2.zhimg.com/50/v2-04bfc01157c1c3ae1480299947315251_hd.jpg" alt="img"></p><p>除此之外，提出了Hierarchical Softmax 和 Negative Sample两个方法，很好的解决了计算有效性，事实上这两个方法都没有严格的理论证明，有些trick之处，非常的实用主义。详细的过程不再阐述了，有兴趣深入理解word2vec的，推荐读读这篇很不错的paper: <a href="http://www-personal.umich.edu/%7Eronxin/pdf/w2vexp.pdf" target="_blank" rel="external">word2vec Parameter Learning Explained</a>。额外多提一点，实际上word2vec学习的向量和真正语义还有差距，更多学到的是具备相似上下文的词，比如“good” “bad”相似度也很高，反而是文本分类任务输入有监督的语义能够学到更好的语义表示。</p><p>至此，文本的表示通过词向量的表示方式，把文本数据从高纬度高稀疏的神经网络难处理的方式，变成了类似图像、语音的的连续稠密数据。深度学习算法本身有很强的数据迁移性，很多之前在图像领域很适用的深度学习算法比如CNN等也可以很好的迁移到文本领域了，</p><h3 id="深度学习文本分类模型"><a href="#深度学习文本分类模型" class="headerlink" title="深度学习文本分类模型"></a>深度学习文本分类模型</h3><h4 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h4><p>本篇文章的题图选用的就是14年这篇文章提出的TextCNN的结构（见下图）。卷积神经网络<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" target="_blank" rel="external">CNN Convolutional Neural Network</a>最初在图像领域取得了巨大成功，CNN原理就不讲了，核心点在于可以<strong>捕捉局部相关性</strong>，具体到文本分类任务中可以利用CNN来提取句子中类似 n-gram 的关键信息。</p><p><img src="https://pic1.zhimg.com/50/v2-ab904178abf9241329e3e2d0fa7c0584_hd.jpg" alt="img"></p><p>TextCNN的详细过程原理图见下：</p><p><img src="https://pic3.zhimg.com/50/v2-bb10ad5bbdc5294d3041662f887e60a6_hd.jpg" alt="img"></p><p><strong>TextCNN详细过程</strong>：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。</p><p><strong>特征</strong>：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。</p><p><strong>通道（Channels）</strong>：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。</p><p><strong>一维卷积（conv-1d）</strong>：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。</p><p><strong>Pooling层</strong>：利用CNN解决文本分类问题的文章还是很多的，比如这篇 <a href="https://arxiv.org/pdf/1404.2188.pdf" target="_blank" rel="external">A Convolutional Neural Network for Modelling Sentences</a> 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">“ 我觉得这个地方景色还不错，但是人也实在太多了 ”</div></pre></td></tr></table></figure><p>虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。</p><h4 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h4><p>尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。</p><p>RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，<a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf" target="_blank" rel="external">Recurrent Neural Network for Text Classification with Multi-Task Learning</a>文中介绍了RNN用于分类问题的设计，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。</p><p><img src="https://pic3.zhimg.com/50/v2-92e49aef6626add56e85c2ee1b36e9aa_hd.jpg" alt="img"></p><h4 id="TextRCNN-TextCNN-TextRNN"><a href="#TextRCNN-TextCNN-TextRNN" class="headerlink" title="TextRCNN (TextCNN + TextRNN)"></a>TextRCNN (TextCNN + TextRNN)</h4><p>我们参考的是中科院15年发表在AAAI上的这篇文章 Recurrent Convolutional Neural Networks for Text Classification 的结构：</p><p><img src="https://pic3.zhimg.com/50/v2-263209ce34c0941fece21de00065aa92_hd.jpg" alt="img"></p><p>利用前向和后向RNN得到每个词的前向和后向上下文的表示：</p><p><img src="https://pic1.zhimg.com/50/v2-d97b136cbb9cd98354521a827e0fd8b4_hd.jpg" alt="img"></p><p>这样词的表示就变成词向量和前向后向上下文向量concat起来的形式了，即：</p><p><img src="https://pic4.zhimg.com/50/v2-16378ac29633452e7093288fd98d3f73_hd.jpg" alt="img"></p><p>最后再接跟TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1就可以了，不再需要更大 filter_size 获得更大视野，这里词的表示也可以只用双向RNN输出。</p><h4 id="HAN-TextRNN-Attention"><a href="#HAN-TextRNN-Attention" class="headerlink" title="HAN (TextRNN + Attention)"></a>HAN (TextRNN + Attention)</h4><p>CNN和RNN用在文本分类任务中尽管效果显著，但都有一个不足的地方就是不够直观，可解释性不好，特别是在分析badcase时候感受尤其深刻。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够很直观的给出每个词对结果的贡献，基本成了Seq2Seq模型的标配了。</p><p><strong>Attention机制介绍</strong>：</p><p>详细介绍Attention恐怕需要一小篇文章的篇幅，感兴趣的可参考14年这篇paper <a href="https://arxiv.org/pdf/1409.0473v7.pdf" target="_blank" rel="external">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>。</p><p>以机器翻译为例简单介绍下，下图中$x_t$是源语言的一个词，$y_t$是目标语言的一个词，机器翻译的任务就是给定源序列得到目标序列。翻译$y_t$的过程产生取决于上一个词 $y_{t-1}$ 和源语言的词的表示 $h_{j}$($x_{j}$) 的 bi-RNN 模型的表示），而每个词所占的权重是不一样的。比如源语言是中文 “我 / 是 / 中国人” 目标语言 “i / am / Chinese”，翻译出“Chinese”时候显然取决于“中国人”，而与“我 / 是”基本无关。下图公式, $\alpha _{ij}$则是翻译英文第$i$个词时，中文第$j$个词的贡献，也就是注意力。显然在翻译“Chinese”时，“中国人”的注意力值非常大。</p><p><img src="https://pic3.zhimg.com/50/v2-de9146388978dfe7ef467993b9cf12ae_hd.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/50/v2-0ebc7b64a7d34a908b8d82d87c92f6b8_hd.jpg" alt="img"></p><p>Attention的核心point是在翻译每个目标词（或 预测商品标题文本所属类别）所用的上下文是不同的，这样的考虑显然是更合理的。</p><p><strong>TextRNN + Attention 模型</strong>：</p><p>我们参考了这篇文章 <a href="https://www.cs.cmu.edu/%7Ediyiy/docs/naacl16.pdf" target="_blank" rel="external">Hierarchical Attention Networks for Document Classification</a>，下图是模型的网络结构图，它一方面用层次化的结构保留了文档的结构，另一方面在word-level和sentence-level。标题场景只需要 word-level 这一层的 Attention 即可。</p><p><img src="https://pic3.zhimg.com/50/v2-4ff2c8099ccf0b2d8eb963a0ac248296_hd.jpg" alt="img"></p><p>加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。</p><h2 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h2><p>现在来详细讲解训练过程，涉及到的文件<code>train_cnn.py</code>, <code>utils.py</code>, <code>textcnn.py</code></p><p>注意到<code>train_cnn.py</code>文件最后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span>  __name__ == <span class="string">'__main__'</span>:</div><div class="line">    os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">'1'</span></div><div class="line">    tf.app.run()</div></pre></td></tr></table></figure><p>其中第一行是指定只用一个GPU。第二行是tensorflow的一个运行框架，<code>run</code>会运行文件内的<code>main</code>方法，并且传入文件最开始设定的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># configuration</span></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">"num_classes"</span>, <span class="number">33</span>, <span class="string">"number of label"</span>)</div><div class="line">tf.app.flags.DEFINE_float(<span class="string">"learning_rate"</span>, <span class="number">0.01</span>, <span class="string">"learning rate"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"batch_size"</span>, <span class="number">64</span>, <span class="string">"Batch size for training/evaluating."</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"decay_steps"</span>, <span class="number">1000</span>, <span class="string">"how many steps before decay learning rate."</span>)</div><div class="line">tf.app.flags.DEFINE_float(</div><div class="line">  <span class="string">"decay_rate"</span>, <span class="number">0.95</span>, <span class="string">"Rate of decay for learning rate."</span>)</div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">  <span class="string">"ckpt_dir"</span>, <span class="string">"text_cnn_title_desc_checkpoint/"</span>, <span class="string">"checkpoint location for the model"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"sentence_len"</span>, <span class="number">15</span>, <span class="string">"max sentence length"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">"embed_size"</span>, <span class="number">64</span>, <span class="string">"embedding size"</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"is_training"</span>, <span class="keyword">True</span>, <span class="string">"is traning.true:tranining,false:testing/inference"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"num_epochs"</span>, <span class="number">30</span>, <span class="string">"number of epochs to run."</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"validate_every"</span>, <span class="number">1</span>, <span class="string">"Validate every validate_every epochs."</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"use_embedding"</span>, <span class="keyword">True</span>, <span class="string">"whether to use embedding or not."</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"num_filters"</span>, <span class="number">256</span>, <span class="string">"number of filters"</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"multi_label_flag"</span>, <span class="keyword">False</span>, <span class="string">"use multi label or single label."</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"just_train"</span>, <span class="keyword">False</span>, <span class="string">"whether use all data to train or not."</span>)</div></pre></td></tr></table></figure><p>第一个参数代表参数名（调用这个参数的方法：<code>FLAGS.name</code>），第二个参数是默认值，第三个参数是描述。值得说明的是这里有一个<code>just_train</code>参数，它代表是否将测试集放入训练集一起训练，一般在用模型最终确定之后。</p><p>所以运行<code>python train_cnn.py</code>就是启动训练过程，同时可以传入参数，方法为<code>python train_cnn.py --name value</code>, 这里的name就是文件定义的参数名，value就是你要设定的值。如果不传入参数，则参数为默认值。</p><p>下面我们来看一下<code>main</code>函数，流程如下：</p><p>​ <img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/5.png" alt="5"></p><h3 id="数据加载-1"><a href="#数据加载-1" class="headerlink" title="数据加载"></a>数据加载</h3><p>这个过程主要是调用<code>train_test_loader</code>方法切分训练集与测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">X_train, X_val, y_train, y_val, n_classes = </div><div class="line">	train_test_loader(FLAGS.just_train)</div></pre></td></tr></table></figure><h3 id="词典加载"><a href="#词典加载" class="headerlink" title="词典加载"></a>词典加载</h3><p>加载数据预处理过程中建立的词典。目的是用来从预训练的词向量词典中拿出对应的词向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> open(<span class="string">'data/vocab.dic'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</div><div class="line">    vocab = pickle.load(f)</div><div class="line">vocab_size = len(vocab) + <span class="number">1</span></div><div class="line">print(<span class="string">'size of vocabulary: &#123;&#125;'</span>.format(vocab_size))</div></pre></td></tr></table></figure><p>这里将词典的长度加一是为了给一个特殊词“空”加入位置，“空”的作用是填充短标题，让所有标题长度一样。</p><h3 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h3><p>这个阶段就是将所有标题长度变成一致，短了就填充，长了就截断。标题长度是一个参数，可以设置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># padding sentences</span></div><div class="line">    X_train = pad_sequences(X_train, maxlen=FLAGS.sentence_len, </div><div class="line">    	value=float(vocab_size - <span class="number">1</span>))</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.just_train:</div><div class="line">        X_val = pad_sequences(</div><div class="line">         	X_val, maxlen=FLAGS.sentence_len, value=float(vocab_size - <span class="number">1</span>))</div></pre></td></tr></table></figure><h3 id="模型实例化"><a href="#模型实例化" class="headerlink" title="模型实例化"></a>模型实例化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">textcnn = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes,</div><div class="line">                FLAGS.learning_rate, FLAGS.batch_size,</div><div class="line">                FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sentence_len,</div><div class="line">                vocab_size, FLAGS.embed_size, FLAGS.is_training, </div><div class="line">                multi_label_flag=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>如果有之前训练到一半的模型，那我们就加载那个模型的参数，继续训练，否则进行参数初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Initialize save</span></div><div class="line">        saver = tf.train.Saver()</div><div class="line">        <span class="keyword">if</span> os.path.exists(FLAGS.ckpt_dir + <span class="string">'checkpoint'</span>):</div><div class="line">            print(<span class="string">'restoring variables from checkpoint'</span>)</div><div class="line">            saver.restore(</div><div class="line">            	sess, </div><div class="line">            	tf.train.latest_checkpoint(FLAGS.ckpt_dir))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">'Initializing Variables'</span>)</div><div class="line">            sess.run(tf.global_variables_initializer())</div><div class="line">            <span class="keyword">if</span> FLAGS.use_embedding:</div><div class="line">                assign_pretrained_word_embedding(</div><div class="line">                	sess, vocab, vocab_size, textcnn)</div></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>模型训练过程中包括两个循环，第一个是大循环，表示遍历所有训练数据多少遍。第二个是mini-batch循环，小循环走过一遍代表遍历了所有训练数据一遍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(curr_epoch, total_epochs):</div><div class="line">            loss, acc, counter = <span class="number">.0</span>, <span class="number">.0</span>, <span class="number">0</span></div><div class="line">            <span class="keyword">for</span> start, end <span class="keyword">in</span> zip(</div><div class="line">                    range(<span class="number">0</span>, number_of_training_data, batch_size),</div><div class="line">                    range(batch_size, number_of_training_data, </div><div class="line">                    batch_size)):</div></pre></td></tr></table></figure><p>下面就是将训练数据喂到模型中:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">feed_dict = &#123;textcnn.input_x: X_train[start:end], 		</div><div class="line">			textcnn.dropout_keep_prob: <span class="number">0.5</span>&#125;</div></pre></td></tr></table></figure><p>第二个参数是模型相关的dropout参数，用于减少过拟合，范围是(0, 1]，基本不用改变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">curr_loss, curr_acc, _ = sess.run(</div><div class="line">                        [textcnn.loss_val, textcnn.accuracy, </div><div class="line">                        textcnn.train_op], feed_dict)</div></pre></td></tr></table></figure><p>这一步就是得到这一小部分训练数据对应的准确率以及loss。</p><p>然后每经过<code>validate_every</code>个大循环的训练，在测试集上看看模型性能。如果性能比上一次更好，就保存模型，否则就退出，因为算法开始发散了。</p><p>模型训练完毕检查性能之后，如果模型可行，下一步就将所有数据用于训练，也即运行以下命令<code>python train_cnn.py --just_train True</code>。这个过程会迭代固定的20个大循环。训练完毕之后，下面的预测过程将使用这个模型。</p><h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><p>预测涉及到的文件<code>predict_cnn.py</code>以及<code>utils.py</code></p><p>预测的流程和训练差不多，只不过不再进行多次对数据集的遍历，只进行对未标记数据进行一次遍历，拿到结果之后，由于算法输出的结果是[0, 32]这样一个序号，我们需要转化为中文标签。</p><p>具体参照代码，不再赘述。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>【1】<a href="https://zhuanlan.zhihu.com/p/25928551" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25928551</a></p><p>【2】<a href="https://github.com/brightmart/text_classification" target="_blank" rel="external">https://github.com/brightmart/text_classification</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文档是&lt;a href=&quot;https://github.com/ewanlee/video_title_classification&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Github项目&lt;/a&gt;的流程解释文档，具体实现请移步。&lt;/p&gt;&lt;p&gt;本项目
    
    </summary>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>BFG Repo-Cleaner</title>
    <link href="http://yoursite.com/2017/11/29/BFG-Repo-Cleaner/"/>
    <id>http://yoursite.com/2017/11/29/BFG-Repo-Cleaner/</id>
    <published>2017-11-29T11:31:19.000Z</published>
    <updated>2017-11-29T11:32:30.550Z</updated>
    
    <content type="html"><![CDATA[<h1 id="An-alternative-to-git-filter-branch"><a href="#An-alternative-to-git-filter-branch" class="headerlink" title="An alternative to git-filter-branch"></a>An alternative to git-filter-branch</h1><p>The BFG is a simpler, faster alternative to <a href="http://git-scm.com/docs/git-filter-branch" target="_blank" rel="external"><code>git-filter-branch</code></a> for cleansing bad data out of your Git repository history:</p><ul><li>Removing <strong>Crazy Big Files</strong></li><li>Removing <strong>Passwords</strong>, <strong>Credentials</strong> &amp; other <strong>Private data</strong></li></ul><p>The <code>git-filter-branch</code> command is enormously powerful and can do things that the BFG can’t - but the BFG is <em>much</em> better for the tasks above, because:</p><ul><li><a href="https://rtyley.github.io/bfg-repo-cleaner/#speed" target="_blank" rel="external">Faster</a> : <strong>10 - 720x</strong> faster</li><li><a href="https://rtyley.github.io/bfg-repo-cleaner/#examples" target="_blank" rel="external">Simpler</a> : The BFG isn’t particularily clever, but <em>is</em> focused on making the above tasks easy</li><li>Beautiful : If you need to, you can use the beautiful Scala language to customise the BFG. Which has got to be better than Bash scripting at least some of the time.</li></ul><h1 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h1><p>First clone a fresh copy of your repo, using the <a href="http://stackoverflow.com/q/3959924/438886" target="_blank" rel="external"><code>--mirror</code></a> flag:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git clone --mirror git://example.com/some-big-repo.git</div></pre></td></tr></table></figure><p>This is a <a href="http://git-scm.com/docs/gitglossary.html#def_bare_repository" target="_blank" rel="external">bare</a> repo, which means your normal files won’t be visible, but it is a <em>full</em> copy of the Git database of your repository, and at this point you should <strong>make a backup of it</strong> to ensure you don’t lose anything.</p><p>Now you can run the BFG to clean your repository up:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -jar bfg.jar --strip-blobs-bigger-than 100M some-big-repo.git</div></pre></td></tr></table></figure><p>The BFG will update your commits and all branches and tags so they are clean, but it doesn’t physically delete the unwanted stuff. Examine the repo to make sure your history has been updated, and then use the standard <a href="http://git-scm.com/docs/git-gc" target="_blank" rel="external"><code>git gc</code></a> command to strip out the unwanted dirty data, which Git will now recognise as surplus to requirements:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ cd some-big-repo.git</div><div class="line">$ git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive</div></pre></td></tr></table></figure><p>Finally, once you’re happy with the updated state of your repo, push it back up <em>(note that because your clone command used the –mirror flag, this push will update *<em>all*</em> refs on your remote server)</em>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git push</div></pre></td></tr></table></figure><p>At this point, you’re ready for everyone to ditch their old copies of the repo and do fresh clones of the nice, new pristine data. It’s best to delete all old clones, as they’ll have dirty history that you <em>don’t</em> want to risk pushing back into your newly cleaned repo.</p><h1 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h1><p>In all these examples <code>bfg</code> is an alias for <code>java -jar bfg.jar</code>.</p><p>Delete all files named ‘id_rsa’ or ‘id_dsa’ :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --delete-files id_&#123;dsa,rsa&#125;  my-repo.git</div></pre></td></tr></table></figure><p>Remove all blobs bigger than 50 megabytes :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --strip-blobs-bigger-than 50M  my-repo.git</div></pre></td></tr></table></figure><p>Replace all passwords listed in a file <em>(prefix lines ‘regex:’ or ‘glob:’ if required)</em> with <code>***REMOVED***</code>wherever they occur in your repository :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --replace-text passwords.txt  my-repo.git</div></pre></td></tr></table></figure><p>Remove all folders or files named ‘.git’ - a <a href="https://github.com/git/git/blob/d29e9c89d/fsck.c#L228-L229" target="_blank" rel="external">reserved filename</a> in Git. These often <a href="http://stackoverflow.com/q/16821649/438886" target="_blank" rel="external">become a problem</a>when migrating to Git from other source-control systems like Mercurial :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --delete-folders .git --delete-files .git  --no-blob-protection  my-repo.git</div></pre></td></tr></table></figure><p>For further command-line options, you can run the BFG without any arguments, which will output <a href="https://repository.sonatype.org/service/local/artifact/maven/redirect?r=central-proxy&amp;g=com.madgag&amp;a=bfg&amp;v=LATEST&amp;e=txt" target="_blank" rel="external">text like this</a>.</p><h1 id="Your-current-files-are-sacred…"><a href="#Your-current-files-are-sacred…" class="headerlink" title="Your current files are sacred…"></a>Your <em>current</em> files are sacred…</h1><p>The BFG treats you like a reformed alcoholic: you’ve made some mistakes in the past, but now you’ve cleaned up your act. Thus the BFG assumes that your latest commit is a <em>good</em> one, with none of the dirty files you want removing from your history still in it. This assumption by the BFG protects your work, and gives you peace of mind knowing that the BFG is <em>only</em> changing your repo history, not meddling with the <em>current</em> files of your project.</p><p>By default the <code>HEAD</code> branch is protected, and while its history will be cleaned, the very latest commit (the ‘tip’) is a <strong>protected commit</strong> and its file-hierarchy won’t be changed at all.</p><p>If you want to protect the tips of several branches or tags (not just HEAD), just name them for the BFG:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --strip-biggest-blobs 100 --protect-blobs-from master,maint,next repo.git</div></pre></td></tr></table></figure><p>Note:</p><ul><li>Cleaning Git repos is about <em>completely</em> eradicating bad stuff from history. If something ‘bad’ (like a 10MB file, when you’re specifying <code>--strip-blobs-bigger-than 5M</code>) is in a protected commit, it <em>won’t</em> be deleted - it’ll persist in your repository, <a href="https://github.com/rtyley/bfg-repo-cleaner/issues/53#issuecomment-50088997" target="_blank" rel="external">even if the BFG deletes if from earlier commits</a>. If you want the BFG to delete something <strong>you need to make sure your current commits are clean</strong>.</li><li>Note that although the files in those protected commits won’t be changed, when those commits follow on from earlier dirty commits, their commit ids <strong>will</strong> change, to reflect the changed history - only the SHA-1 id of the filesystem-tree will remain the same.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;An-alternative-to-git-filter-branch&quot;&gt;&lt;a href=&quot;#An-alternative-to-git-filter-branch&quot; class=&quot;headerlink&quot; title=&quot;An alternative to git-
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Understanding LSTM Networks [repost]</title>
    <link href="http://yoursite.com/2017/11/28/Understanding-LSTM-Networks-repost/"/>
    <id>http://yoursite.com/2017/11/28/Understanding-LSTM-Networks-repost/</id>
    <published>2017-11-28T07:51:36.000Z</published>
    <updated>2017-11-28T07:53:14.945Z</updated>
    
    <content type="html"><![CDATA[<p>source post is <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">here</a>.</p><h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p>Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.</p><p>Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.</p><p>Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" alt="img"></p><p><strong>Recurrent Neural Networks have loops.</strong></p><p>In the above diagram, a chunk of neural network, AA, looks at some input xtxt and outputs a value htht. A loop allows information to be passed from one step of the network to the next.</p><p>These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" alt="An unrolled recurrent neural network."></p><p><strong>An unrolled recurrent neural network.</strong></p><p>This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.</p><p>And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. But they really are pretty amazing.</p><p>Essential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore.</p><h2 id="The-Problem-of-Long-Term-Dependencies"><a href="#The-Problem-of-Long-Term-Dependencies" class="headerlink" title="The Problem of Long-Term Dependencies"></a>The Problem of Long-Term Dependencies</h2><p>One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends.</p><p>Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the <em>sky</em>,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" alt="img"></p><p>But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent <em>French</em>.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.</p><p>Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" alt="Neural networks struggle with long term dependencies."></p><p>In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="external">Hochreiter (1991) [German]</a> and <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="external">Bengio, et al. (1994)</a>, who found some pretty fundamental reasons why it might be difficult.</p><p>Thankfully, LSTMs don’t have this problem!</p><h2 id="LSTM-Networks"><a href="#LSTM-Networks" class="headerlink" title="LSTM Networks"></a>LSTM Networks</h2><p>Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">Hochreiter &amp; Schmidhuber (1997)</a>, and were refined and popularized by many people in following work.<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1" target="_blank" rel="external">1</a> They work tremendously well on a large variety of problems, and are now widely used.</p><p>LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</p><p>All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p><p><strong>The repeating module in a standard RNN contains a single layer.</strong></p><p>LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.</p><p><strong>The repeating module in an LSTM contains four interacting layers.</strong></p><p>Don’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" alt="img"></p><p>In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.</p><h2 id="The-Core-Idea-Behind-LSTMs"><a href="#The-Core-Idea-Behind-LSTMs" class="headerlink" title="The Core Idea Behind LSTMs"></a>The Core Idea Behind LSTMs</h2><p>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.</p><p>The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" alt="img"></p><p>The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.</p><p>Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" alt="img"></p><p>The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”</p><p>An LSTM has three of these gates, to protect and control the cell state.</p><h2 id="Step-by-Step-LSTM-Walk-Through"><a href="#Step-by-Step-LSTM-Walk-Through" class="headerlink" title="Step-by-Step LSTM Walk Through"></a>Step-by-Step LSTM Walk Through</h2><p>The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1ht−1and xtxt, and outputs a number between 00 and 11 for each number in the cell state Ct−1Ct−1. A 11represents “completely keep this” while a 00 represents “completely get rid of this.”</p><p>Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt="img"></p><p>The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, C~tC~t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.</p><p>In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt="img"></p><p>It’s now time to update the old cell state, Ct−1Ct−1, into the new cell state CtCt. The previous steps already decided what to do, we just need to actually do it.</p><p>We multiply the old state by ftft, forgetting the things we decided to forget earlier. Then we add it∗C~tit∗C~t. This is the new candidate values, scaled by how much we decided to update each state value.</p><p>In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt="img"></p><p>Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanhtanh (to push the values to be between −1−1 and 11) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.</p><p>For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt="img"></p><h2 id="Variants-on-Long-Short-Term-Memory"><a href="#Variants-on-Long-Short-Term-Memory" class="headerlink" title="Variants on Long Short Term Memory"></a>Variants on Long Short Term Memory</h2><p>What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them.</p><p>One popular LSTM variant, introduced by <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="external">Gers &amp; Schmidhuber (2000)</a>, is adding “peephole connections.” This means that we let the gate layers look at the cell state.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png" alt="img"></p><p>The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.</p><p>Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png" alt="img"></p><p>A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">Cho, et al. (2014)</a>. It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png" alt="A gated recurrent unit neural network."></p><p>These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="external">Yao, et al. (2015)</a>. There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="external">Koutnik, et al. (2014)</a>.</p><p>Which of these variants is best? Do the differences matter? <a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">Greff, et al. (2015)</a> do a nice comparison of popular variants, finding that they’re all about the same. <a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="external">Jozefowicz, et al. (2015)</a>tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!</p><p>Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.</p><p>LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, <a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="external">Xu, <em>et al.</em>(2015)</a> do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner…</p><p>Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by <a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="external">Kalchbrenner, <em>et al.</em> (2015)</a> seem extremely promising. Work using RNNs in generative models – such as <a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="external">Gregor, <em>et al.</em> (2015)</a>, <a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="external">Chung, <em>et al.</em> (2015)</a>, or <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="external">Bayer &amp; Osendorfer (2015)</a> – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;source post is &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;h2 id=&quot;Re
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>A Simple Multi-Class Classification Task: Keras and Scikit-Learn</title>
    <link href="http://yoursite.com/2017/11/21/A-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn/"/>
    <id>http://yoursite.com/2017/11/21/A-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn/</id>
    <published>2017-11-21T08:59:17.000Z</published>
    <updated>2017-11-21T09:05:39.613Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Problem-Description"><a href="#1-Problem-Description" class="headerlink" title="1. Problem Description"></a>1. Problem Description</h2><p>In this tutorial, we will use the standard machine learning problem called the <a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="external">iris flowers dataset</a>.</p><p>This dataset is well studied and is a good problem for practicing on neural networks because all of the 4 input variables are numeric and have the same scale in centimeters. Each instance describes the properties of an observed flower measurements and the output variable is specific iris species.</p><p>This is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species. This is an important type of problem on which to practice with neural networks because the three class values require specialized handling.</p><p>The iris flower dataset is a well-studied problem and a such we can <a href="http://www.is.umk.pl/projects/rules.html#Iris" target="_blank" rel="external">expect to achieve a model accuracy</a> in the range of 95% to 97%. This provides a good target to aim for when developing our models.</p><p>You can <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" target="_blank" rel="external">download the iris flowers dataset</a> from the UCI Machine Learning repository and place it in your current working directory with the filename “<em>iris.csv</em>“.</p><p>Need help with Deep Learning in Python?Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with sample code).Click to sign-up now and also get a free PDF Ebook version of the course.<a href="https://machinelearningmastery.leadpages.co/leadbox/142d6e873f72a2%3A164f8be4f346dc/5657382461898752/" target="_blank" rel="external">Start Your FREE Mini-Course Now!</a></p><h2 id="2-Import-Classes-and-Functions"><a href="#2-Import-Classes-and-Functions" class="headerlink" title="2. Import Classes and Functions"></a>2. Import Classes and Functions</h2><p>We can begin by importing all of the classes and functions we will need in this tutorial.</p><p>This includes both the functionality we require from Keras, but also data loading from <a href="http://pandas.pydata.org/" target="_blank" rel="external">pandas</a>as well as data preparation and model evaluation from <a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="keyword">import</span> pandas</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">from</span> keras.wrappers.scikit_learn <span class="keyword">import</span> KerasClassifier</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</div></pre></td></tr></table></figure><h2 id="3-Initialize-Random-Number-Generator"><a href="#3-Initialize-Random-Number-Generator" class="headerlink" title="3. Initialize Random Number Generator"></a>3. Initialize Random Number Generator</h2><p>Next, we need to initialize the random number generator to a constant value (7).</p><p>This is important to ensure that the results we achieve from this model can be achieved again precisely. It ensures that the stochastic process of training a neural network model can be reproduced.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># fix random seed for reproducibility</span></div><div class="line">seed = <span class="number">7</span></div><div class="line">numpy.random.seed(seed)</div></pre></td></tr></table></figure><h2 id="4-Load-The-Dataset"><a href="#4-Load-The-Dataset" class="headerlink" title="4. Load The Dataset"></a>4. Load The Dataset</h2><p>The dataset can be loaded directly. Because the output variable contains strings, it is easiest to load the data using pandas. We can then split the attributes (columns) into input variables (X) and output variables (Y).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># load dataset</span></div><div class="line">dataframe = pandas.read_csv(<span class="string">"iris.csv"</span>, header=<span class="keyword">None</span>)</div><div class="line">dataset = dataframe.values</div><div class="line">X = dataset[:,<span class="number">0</span>:<span class="number">4</span>].astype(float)</div><div class="line">Y = dataset[:,<span class="number">4</span>]</div></pre></td></tr></table></figure><h2 id="5-Encode-The-Output-Variable"><a href="#5-Encode-The-Output-Variable" class="headerlink" title="5. Encode The Output Variable"></a>5. Encode The Output Variable</h2><p>The output variable contains three different string values.</p><p>When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not.</p><p>This is called <a href="https://en.wikipedia.org/wiki/One-hot" target="_blank" rel="external">one hot encoding</a> or creating dummy variables from a categorical variable.</p><p>For example, in this problem three class values are Iris-setosa, Iris-versicolor and Iris-virginica. If we had the observations:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Iris-setosa</div><div class="line">Iris-versicolor</div><div class="line">Iris-virginica</div></pre></td></tr></table></figure><p>We can turn this into a one-hot encoded binary matrix for each data instance that would look as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Iris-setosa,	Iris-versicolor,	Iris-virginica</div><div class="line"><span class="number">1</span>,		<span class="number">0</span>,			<span class="number">0</span></div><div class="line"><span class="number">0</span>,		<span class="number">1</span>, 			<span class="number">0</span></div><div class="line"><span class="number">0</span>, 		<span class="number">0</span>, 			<span class="number">1</span></div></pre></td></tr></table></figure><p>We can do this by first encoding the strings consistently to integers using the scikit-learn class LabelEncoder. Then convert the vector of integers to a one hot encoding using the Keras function to_categorical().</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># encode class values as integers</span></div><div class="line">encoder = LabelEncoder()</div><div class="line">encoder.fit(Y)</div><div class="line">encoded_Y = encoder.transform(Y)</div><div class="line"><span class="comment"># convert integers to dummy variables (i.e. one hot encoded)</span></div><div class="line">dummy_y = np_utils.to_categorical(encoded_Y)</div></pre></td></tr></table></figure><h2 id="6-Define-The-Neural-Network-Model"><a href="#6-Define-The-Neural-Network-Model" class="headerlink" title="6. Define The Neural Network Model"></a>6. Define The Neural Network Model</h2><p>The Keras library provides wrapper classes to allow you to use neural network models developed with Keras in scikit-learn.</p><p>There is a KerasClassifier class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.</p><p>Below is a function that will create a baseline neural network for the iris classification problem. It creates a simple fully connected network with one hidden layer that contains 8 neurons.</p><p>The hidden layer uses a rectifier activation function which is a good practice. Because we used a one-hot encoding for our iris dataset, the output layer must create 3 output values, one for each class. The output value with the largest value will be taken as the class predicted by the model.</p><p>The network topology of this simple one-layer neural network can be summarized as:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">4 inputs -&gt; [8 hidden nodes] -&gt; 3 outputs</div></pre></td></tr></table></figure><p>Note that we use a “<em>softmax</em>” activation function in the output layer. This is to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities.</p><p>Finally, the network uses the efficient Adam gradient descent optimization algorithm with a logarithmic loss function, which is called “<em>categorical_crossentropy</em>” in Keras.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># define baseline model</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_model</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment"># create model</span></div><div class="line">	model = Sequential()</div><div class="line">	model.add(Dense(<span class="number">8</span>, input_dim=<span class="number">4</span>, activation=<span class="string">'relu'</span>))</div><div class="line">	model.add(Dense(<span class="number">3</span>, activation=<span class="string">'softmax'</span>))</div><div class="line">	<span class="comment"># Compile model</span></div><div class="line">	model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">	<span class="keyword">return</span> model</div></pre></td></tr></table></figure><p>We can now create our KerasClassifier for use in scikit-learn.</p><p>We can also pass arguments in the construction of the KerasClassifier class that will be passed on to the fit() function internally used to train the neural network. Here, we pass the number of epochs as 200 and batch size as 5 to use when training the model. Debugging is also turned off when training by setting verbose to 0.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">estimator = KerasClassifier(</div><div class="line">  build_fn=baseline_model, epochs=<span class="number">200</span>, batch_size=<span class="number">5</span>, verbose=<span class="number">0</span>)</div></pre></td></tr></table></figure><h2 id="7-Evaluate-The-Model-with-k-Fold-Cross-Validation"><a href="#7-Evaluate-The-Model-with-k-Fold-Cross-Validation" class="headerlink" title="7. Evaluate The Model with k-Fold Cross Validation"></a>7. Evaluate The Model with k-Fold Cross Validation</h2><p>We can now evaluate the neural network model on our training data.</p><p>The scikit-learn has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating machine learning models is k-fold cross validation.</p><p>First we can define the model evaluation procedure. Here, we set the number of folds to be 10 (an excellent default) and to shuffle the data before partitioning it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kfold = KFold(n_splits=<span class="number">10</span>, shuffle=<span class="keyword">True</span>, random_state=seed)</div></pre></td></tr></table></figure><p>Now we can evaluate our model (estimator) on our dataset (X and dummy_y) using a 10-fold cross-validation procedure (kfold).</p><p>Evaluating the model only takes approximately 10 seconds and returns an object that describes the evaluation of the 10 constructed models for each of the splits of the dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">results = cross_val_score(estimator, X, dummy_y, cv=kfold)</div><div class="line">print(<span class="string">"Baseline: %.2f%% (%.2f%%)"</span> % (results.mean()*<span class="number">100</span>, results.std()*<span class="number">100</span>))</div></pre></td></tr></table></figure><p>The results are summarized as both the mean and standard deviation of the model accuracy on the dataset. This is a reasonable estimation of the performance of the model on unseen data. It is also within the realm of known top results for this problem.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Accuracy: <span class="number">97.33</span>% (<span class="number">4.42</span>%)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Problem-Description&quot;&gt;&lt;a href=&quot;#1-Problem-Description&quot; class=&quot;headerlink&quot; title=&quot;1. Problem Description&quot;&gt;&lt;/a&gt;1. Problem Description
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>The Right Way to Oversample in Predictive Modeling</title>
    <link href="http://yoursite.com/2017/11/21/The-Right-Way-to-Oversample-in-Predictive-Modeling/"/>
    <id>http://yoursite.com/2017/11/21/The-Right-Way-to-Oversample-in-Predictive-Modeling/</id>
    <published>2017-11-21T01:10:22.000Z</published>
    <updated>2017-11-21T08:57:45.929Z</updated>
    
    <content type="html"><![CDATA[<p>The Source Blog: <a href="https://beckernick.github.io/oversampling-modeling/" target="_blank" rel="external">https://beckernick.github.io/oversampling-modeling/</a></p><p>Imbalanced datasets spring up everywhere. Amazon wants to classify fake reviews, banks want to predict fraudulent credit card charges, and, as of this November, Facebook researchers are probably wondering if they can predict which news articles are fake.</p><p>In each of these cases, only a small fraction of observations are actually positives. I’d guess that only 1 in 10,000 credit card charges are fraudulent, at most. Recently, oversampling the minority class observations has become a common approach to improve the quality of predictive modeling. By oversampling, models are sometimes better able to learn patterns that differentiate classes.</p><p>However, this post isn’t about how this can improve modeling. Instead, it’s about how the <strong>*timing*</strong> of oversampling can affect the generalization ability of a model. Since one of the primary goals of model validation is to estimate how it will perform on unseen data, oversampling correctly is critical.</p><h1 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h1><p>I’m going to try to predict whether someone will default on or a creditor will have to charge off a loan, using data from Lending Club. I’ll start by importing some modules and loading the data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</div><div class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">loans = pd.read_csv(<span class="string">'../lending-club-data.csv.zip'</span>)</div><div class="line">loans.iloc[<span class="number">0</span>]</div></pre></td></tr></table></figure><p>There’s a lot of cool person and loan-specific information in this dataset. The target variable is <code>bad_loans</code>, which is 1 if the loan was charged off or the lessee defaulted, and 0 otherwise. I know this dataset should be imbalanced (most loans are paid off), but how imbalanced is it?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loans.bad_loans.value_counts()</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">0</span>    <span class="number">99457</span></div><div class="line"><span class="number">1</span>    <span class="number">23150</span></div><div class="line">Name: bad_loans, dtype: int64</div></pre></td></tr></table></figure><p>Charge offs occurred or people defaulted on about 19% of loans, so there’s some imbalance in the data but it’s not terrible. I’ll remove a few observations with missing values for a payment-to-income ratio and then pick a handful of features to use in a random forest model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loans = loans[~loans.payment_inc_ratio.isnull()]</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">model_variables = [<span class="string">'grade'</span>, <span class="string">'home_ownership'</span>,<span class="string">'emp_length_num'</span>, <span class="string">'sub_grade'</span>,<span class="string">'short_emp'</span>, <span class="string">'dti'</span>, <span class="string">'term'</span>, <span class="string">'purpose'</span>, <span class="string">'int_rate'</span>, <span class="string">'last_delinq_none'</span>, <span class="string">'last_major_derog_none'</span>, <span class="string">'revol_util'</span>, <span class="string">'total_rec_late_fee'</span>, <span class="string">'payment_inc_ratio'</span>, <span class="string">'bad_loans'</span>]</div><div class="line"></div><div class="line">loans_data_relevent = loans[model_variables]</div></pre></td></tr></table></figure><p>Next, I need to one-hot encode the categorical features as binary variables to use them in sklearn’s random forest classifier.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loans_relevant_enconded = pd.get_dummies(loans_data_relevent)</div></pre></td></tr></table></figure><h1 id="Creating-the-Training-and-Test-Sets"><a href="#Creating-the-Training-and-Test-Sets" class="headerlink" title="Creating the Training and Test Sets"></a>Creating the Training and Test Sets</h1><p>With the data prepared, I can create a training dataset and a test dataset. I’ll use the training dataset to build and validate the model, and treat the test dataset as the unseen new data I’d see if the model were in production.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">training_features, test_features, \</div><div class="line">training_target, test_target, = train_test_split(loans_relevant_enconded.drop([<span class="string">'bad_loans'</span>], axis=<span class="number">1</span>),                        loans_relevant_enconded[<span class="string">'bad_loans'</span>],</div><div class="line">			    test_size = <span class="number">.1</span>,</div><div class="line">			    random_state=<span class="number">12</span>)</div></pre></td></tr></table></figure><h1 id="The-Wrong-Way-to-Oversample"><a href="#The-Wrong-Way-to-Oversample" class="headerlink" title="The Wrong Way to Oversample"></a>The Wrong Way to Oversample</h1><p>With my training data created, I’ll upsample the bad loans using the <a href="https://www.jair.org/media/953/live-953-2037-jair.pdf" target="_blank" rel="external">SMOTE algorithm</a> (Synthetic Minority Oversampling Technique). At a high level, SMOTE creates synthetic observations of the minority class (bad loans) by:</p><ol><li>Finding the k-nearest-neighbors for minority class observations (finding similar observations)</li><li>Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.</li></ol><p>After upsampling to a class ratio of 1.0, I should have a balanced dataset. There’s no need (and often it’s not smart) to balance the classes, but it magnifies the issue caused by incorrectly timed oversampling.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sm = SMOTE(random_state=<span class="number">12</span>, ratio = <span class="number">1.0</span>)</div><div class="line">x_res, y_res = sm.fit_sample(training_features, training_target)</div><div class="line"><span class="keyword">print</span> training_target.value_counts(), np.bincount(y_res)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">0</span>    <span class="number">89493</span></div><div class="line"><span class="number">1</span>    <span class="number">20849</span></div><div class="line">Name: bad_loans, dtype: int64 [<span class="number">89493</span> <span class="number">89493</span>]</div></pre></td></tr></table></figure><p>After upsampling, I’ll split the data into separate training and validation sets and build a random forest model to classify the bad loans.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res,</div><div class="line">                                                    y_res,</div><div class="line">                                                    test_size = <span class="number">.1</span>,</div><div class="line">                                                    random_state=<span class="number">12</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">clf_rf = RandomForestClassifier(n_estimators=<span class="number">25</span>, random_state=<span class="number">12</span>)</div><div class="line">clf_rf.fit(x_train_res, y_train_res)</div><div class="line">clf_rf.score(x_val_res, y_val_res)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">0.88468629532376108</span></div></pre></td></tr></table></figure><p>88% accuracy looks good, but I’m not just interested in accuracy. I also want to know how well I can specifically classify bad loans, since they’re more important. In statistics, this is called <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" target="_blank" rel="external">recall</a>, and it’s the number of correctly predicted “positives” divided by the total number of “positives”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">recall_score(y_val_res, clf_rf.predict(x_val_res))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="number">0.81192097332291546</span></div></pre></td></tr></table></figure><p>81% recall. That means the model correctly identified 81% of the total bad loans. That’s pretty great. But is this actually representative of how the model will perform? To find out, I’ll calculate the accuracy and recall for the model on the test dataset I created initially.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> clf_rf.score(test_features, test_target)</div><div class="line"><span class="keyword">print</span> recall_score(test_target, clf_rf.predict(test_features))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">0.801973737868</span></div><div class="line"><span class="number">0.129943502825</span></div></pre></td></tr></table></figure><p>Only 80% accuracy and 13% recall on the test data. That’s a <strong>huge</strong> difference!</p><h1 id="What-Happened"><a href="#What-Happened" class="headerlink" title="What Happened?"></a>What Happened?</h1><p>By oversampling before splitting into training and validation datasets, I “bleed” information from the validation set into the training of the model.</p><p>To see how this works, think about the case of simple oversampling (where I just duplicate observations). If I upsample a dataset before splitting it into a train and validation set, I could end up with the same observation in both datasets. As a result, a complex enough model will be able to perfectly predict the value for those observations when predicting on the validation set, inflating the accuracy and recall.</p><p>When upsampling using SMOTE, I don’t create duplicate observations. However, because the SMOTE algorithm uses the nearest neighbors of observations to create synthetic data, it still bleeds information. If the nearest neighbors of minority class observations in the training set end up in the validation set, their information is partially captured by the synthetic data in the training set. Since I’m splitting the data randomly, we’d expect to have this happen. As a result, the model will be better able to predict validation set values than completely new data.</p><h1 id="The-Right-Way-to-Oversample"><a href="#The-Right-Way-to-Oversample" class="headerlink" title="The Right Way to Oversample"></a>The Right Way to Oversample</h1><p>Okay, so I’ve gone through the wrong way to oversample. Now I’ll go through the right way: oversampling on only the training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x_train, x_val, y_train, y_val = \</div><div class="line">			train_test_split(training_features, 											    training_target,</div><div class="line">						    test_size = <span class="number">.1</span>,</div><div class="line">                              random_state=<span class="number">12</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sm = SMOTE(random_state=<span class="number">12</span>, ratio = <span class="number">1.0</span>)</div><div class="line">x_train_res, y_train_res = sm.fit_sample(x_train, y_train)</div></pre></td></tr></table></figure><p>By oversampling only on the training data, none of the information in the validation data is being used to create synthetic observations. So these results should be generalizable. Let’s see if that’s true.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">clf_rf = RandomForestClassifier(n_estimators=<span class="number">25</span>, random_state=<span class="number">12</span>)</div><div class="line">clf_rf.fit(x_train_res, y_train_res)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> <span class="string">'Validation Results'</span></div><div class="line"><span class="keyword">print</span> clf_rf.score(x_val, y_val)</div><div class="line"><span class="keyword">print</span> recall_score(y_val, clf_rf.predict(x_val))</div><div class="line"><span class="keyword">print</span> <span class="string">'\nTest Results'</span></div><div class="line"><span class="keyword">print</span> clf_rf.score(test_features, test_target)</div><div class="line"><span class="keyword">print</span> recall_score(test_target, clf_rf.predict(test_features))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Validation Results</div><div class="line"><span class="number">0.800362483009</span></div><div class="line"><span class="number">0.138195777351</span></div><div class="line"></div><div class="line">Test Results</div><div class="line"><span class="number">0.803278688525</span></div><div class="line"><span class="number">0.142546718818</span></div></pre></td></tr></table></figure><p>The validation results closely match the unseen test data results, which is exactly what I would want to see after putting a model into production.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Oversampling is a well-known way to potentially improve models trained on imbalanced data. But it’s important to remember that oversampling incorrectly can lead to thinking a model will generalize better than it actually does. Random forests are great because the model architecture reduces overfitting (see <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf" target="_blank" rel="external">Brieman 2001</a> for a proof), but poor sampling practices can still lead to false conclusions about the quality of a model.</p><p>When the model is in production, it’s predicting on unseen data. The main point of model validation is to estimate how the model will generalize to new data. If the decision to put a model into production is based on how it performs on a validation set, it’s critical that oversampling is done correctly.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The Source Blog: &lt;a href=&quot;https://beckernick.github.io/oversampling-modeling/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://beckernick.github.i
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Note of the DenseNet (contains TensorFlow and PyTorch Implementation)</title>
    <link href="http://yoursite.com/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/"/>
    <id>http://yoursite.com/2017/11/20/Note-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation/</id>
    <published>2017-11-20T04:15:43.000Z</published>
    <updated>2017-11-20T05:23:15.767Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>The blog source:</strong></p><p><strong><a href="https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504" target="_blank" rel="external">https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504</a>.</strong></p><p>I have added the PyTorch implementation from</p><p><strong><a href="https://github.com/gpleiss/efficient_densenet_pytorch" target="_blank" rel="external">https://github.com/gpleiss/efficient_densenet_pytorch</a>.</strong></p></blockquote><p><a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="external">DenseNet</a>(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. It’s quite similar to <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet</a> but has some fundamental differences.</p><p>With all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*7WdURialIGTojNI9ltrplA.png" alt="img"></p><p><em>Error rates on various datasets(from source paper)</em></p><p>And for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*N_o10WxGx_e6vBSNnV5k1A.png" alt="img"></p><p><em>Сomparison of the DenseNet and ResNet Top-1 error rates on the ImageNet classification dataset as a function of learned parameters (left) and flops during test-time (right)(from source paper).</em></p><p>This post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:</p><ul><li>Why dense net differs from another convolution networks.</li><li>What difficulties I’ve met during the implementation of DenseNet in tensorflow.</li></ul><p>If you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the <a href="https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#aabd" target="_blank" rel="external">second chapter</a> or check the <a href="https://github.com/ikhlestov/vision_networks" target="_blank" rel="external">source code on GitHub</a>. If you not familiar with any topics but want to get some knowledge — I highly advise you <a href="http://cs231n.github.io/" target="_blank" rel="external">CS231n Stanford classes</a>.</p><h4 id="Compare-DenseNet-with-other-Convolution-Networks"><a href="#Compare-DenseNet-with-other-Convolution-Networks" class="headerlink" title="Compare DenseNet with other Convolution Networks"></a>Compare DenseNet with other Convolution Networks</h4><p>Usually, ConvNets work such way:<br>We have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.<br>So the output from the Lᵢ layer is input to the Lᵢ₊₁ layer. It seems like this:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*1F8wom5X1DeCj5BWeGfJbw.jpeg" alt="img"></p><p><em>source: &lt;<a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">http://cs231n.github.io/convolutional-networks/</a></em>&gt;</p><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet</a> architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the Lᵢ layer was obtained by summation of outputs from previous layers.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-C2QoqhfCjG8xZRu-UwO_Q.png" alt="img"></p><p>In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.<br>So, let’s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.<br>The output from Lᵢ layer will be (28, 28, 12).<br>But input to the Lᵢ₊₁ will be (28, 28, 24+12), for Lᵢ₊₂ (28, 28, 24 + 12 + 12) and so on.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*6JoB4BeIZyWDNGH5U63y_Q.png" alt="img"></p><p><em>Block of convolution layers with results concatenated</em></p><p>After a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).<br>All these N layers are named Block in the paper. There’s also batch normalization, nonlinearity and dropout inside the block.<br>To reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-CQyW4huxxxMYpffb72emg.png" alt="img"></p><p><em>Transition layer</em></p><p>Now we can again pass the image through the block with N convolutions.<br>With this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.<br>Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*SSn5H14SKhhaZZ5XYWN3Cg.jpeg" alt="img"></p><p><em>Full DenseNet example with 3 blocks from source paper</em></p><h4 id="Notes-about-implementation"><a href="#Notes-about-implementation" class="headerlink" title="Notes about implementation"></a>Notes about implementation</h4><p>In the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.</p><p>First of all, it was not clear how many blocks should be used depends on depth. After I’ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.</p><p>Second I’ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .</p><p>Despite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> block <span class="keyword">in</span> range(required_blocks):</div><div class="line">    output = build_block(output)</div><div class="line">    <span class="keyword">if</span> block != (required_blocks — <span class="number">1</span>):</div><div class="line">        output = transition_layer(output)</div></pre></td></tr></table></figure><p>For weights initialization authors proposed use MRSA initialization(as per<a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="external">this paper</a>). In tensorflow this initialization can be easy implemented with<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer" target="_blank" rel="external">variance scaling initializer</a>.</p><p>In the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">batch norm -&gt; relu -&gt; conv 1x1 -&gt; dropout -&gt; batch norm -&gt; relu -&gt; conv 3x3 -&gt; dropout -&gt; output.</div></pre></td></tr></table></figure><p>Despite two conv filters, only last output will be concatenated to the main pool of features.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*FvW30tjOQ2hDQ7LdEkCRLQ.png" alt="img"></p><p>Also at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*N-nU5WXQcrNfhnQMwxquFA.png" alt="img"></p><p>Where theta — some reduction values, in the range (0, 1).</p><p>In case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.</p><p>Last, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used — just divide every image pixel by 255, so we have pixels values in the range [0, 1].</p><p>At first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next I’ve implemented per channel normalization… And networks began works even worse. It was not clear for me why. So I’ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.</p><p>And some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># without this line next slice assignment will silently fail!</span></div><div class="line"><span class="comment"># at least in numpy 1.12.0</span></div><div class="line">images = images.astype(‘float64’)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(channels):</div><div class="line">    images[:, :, :, i] = (</div><div class="line">        (images[:, :, :, i] — self.images_means[i]) /</div><div class="line">         self.images_stds[i])</div></pre></td></tr></table></figure><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>DenseNets are powerful neural nets that achieve state of the art performance on many datasets. And it’s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.</p><a id="more"></a><h4 id="Appendix-PyTorch-Implementation-naive-version-100-lines"><a href="#Appendix-PyTorch-Implementation-naive-version-100-lines" class="headerlink" title="Appendix: PyTorch Implementation (naive version ~100 lines)"></a>Appendix: PyTorch Implementation (naive version ~100 lines)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># This implementation is based on the DenseNet-BC implementation in torchvision</span></div><div class="line"><span class="comment"># https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DenseLayer</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_features, growth_rate, bn_size, drop_rate)</span>:</span></div><div class="line">        super(_DenseLayer, self).__init__()</div><div class="line">        self.add_module(<span class="string">'norm.1'</span>, nn.BatchNorm2d(num_input_features)),</div><div class="line">        self.add_module(<span class="string">'relu.1'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">        self.add_module(<span class="string">'conv.1'</span>, nn.Conv2d(num_input_features, bn_size *</div><div class="line">                        growth_rate, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        self.add_module(<span class="string">'norm.2'</span>, nn.BatchNorm2d(bn_size * growth_rate)),</div><div class="line">        self.add_module(<span class="string">'relu.2'</span>, nn.ReLU(inplace=<span class="keyword">True</span>)),</div><div class="line">        self.add_module(<span class="string">'conv.2'</span>, nn.Conv2d(bn_size * growth_rate, growth_rate,</div><div class="line">                        kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        self.drop_rate = drop_rate</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        new_features = super(_DenseLayer, self).forward(x)</div><div class="line">        <span class="keyword">if</span> self.drop_rate &gt; <span class="number">0</span>:</div><div class="line">            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)</div><div class="line">        <span class="keyword">return</span> torch.cat([x, new_features], <span class="number">1</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_Transition</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_input_features, num_output_features)</span>:</span></div><div class="line">        super(_Transition, self).__init__()</div><div class="line">        self.add_module(<span class="string">'norm'</span>, nn.BatchNorm2d(num_input_features))</div><div class="line">        self.add_module(<span class="string">'relu'</span>, nn.ReLU(inplace=<span class="keyword">True</span>))</div><div class="line">        self.add_module(<span class="string">'conv'</span>, nn.Conv2d(num_input_features, num_output_features,</div><div class="line">                                          kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>))</div><div class="line">        self.add_module(<span class="string">'pool'</span>, nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">_DenseBlock</span><span class="params">(nn.Sequential)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate)</span>:</span></div><div class="line">        super(_DenseBlock, self).__init__()</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num_layers):</div><div class="line">            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)</div><div class="line">            self.add_module(<span class="string">'denselayer%d'</span> % (i + <span class="number">1</span>), layer)</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseNet</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">r"""Densenet-BC model class, based on</span></div><div class="line">    `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;`</div><div class="line">    Args:</div><div class="line">        growth_rate (int) - how many filters to add each layer (`k` in paper)</div><div class="line">        block_config (list of 3 or 4 ints) - how many layers in each pooling block</div><div class="line">        num_init_features (int) - the number of filters to learn in the first convolution layer</div><div class="line">        bn_size (int) - multiplicative factor for number of bottle neck layers</div><div class="line">            (i.e. bn_size * k features in the bottleneck layer)</div><div class="line">        drop_rate (float) - dropout rate after each dense layer</div><div class="line">        num_classes (int) - number of classification classes</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, growth_rate=<span class="number">12</span>, block_config=<span class="params">(<span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>)</span>, compression=<span class="number">0.5</span>,</span></span></div><div class="line">                 num_init_features=<span class="number">24</span>, bn_size=<span class="number">4</span>, drop_rate=<span class="number">0</span>, avgpool_size=<span class="number">8</span>,</div><div class="line">                 num_classes=<span class="number">10</span>):</div><div class="line"></div><div class="line">        super(DenseNet, self).__init__()</div><div class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt; compression &lt;= <span class="number">1</span>, <span class="string">'compression of densenet should be between 0 and 1'</span></div><div class="line">        self.avgpool_size = avgpool_size</div><div class="line"></div><div class="line">        <span class="comment"># First convolution</span></div><div class="line">        self.features = nn.Sequential(OrderedDict([</div><div class="line">            (<span class="string">'conv0'</span>, nn.Conv2d(<span class="number">3</span>, num_init_features, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)),</div><div class="line">        ]))</div><div class="line"></div><div class="line">        <span class="comment"># Each denseblock</span></div><div class="line">        num_features = num_init_features</div><div class="line">        <span class="keyword">for</span> i, num_layers <span class="keyword">in</span> enumerate(block_config):</div><div class="line">            block = _DenseBlock(num_layers=num_layers,</div><div class="line">                                num_input_features=num_features,</div><div class="line">                                bn_size=bn_size, growth_rate=growth_rate,</div><div class="line">                                drop_rate=drop_rate)</div><div class="line">            self.features.add_module(<span class="string">'denseblock%d'</span> % (i + <span class="number">1</span>), block)</div><div class="line">            num_features = num_features + num_layers * growth_rate</div><div class="line">            <span class="keyword">if</span> i != len(block_config) - <span class="number">1</span>:</div><div class="line">                trans = _Transition(num_input_features=num_features,</div><div class="line">                                    num_output_features=int(num_features</div><div class="line">                                                            * compression))</div><div class="line">                self.features.add_module(<span class="string">'transition%d'</span> % (i + <span class="number">1</span>), trans)</div><div class="line">                num_features = int(num_features * compression)</div><div class="line"></div><div class="line">        <span class="comment"># Final batch norm</span></div><div class="line">        self.features.add_module(<span class="string">'norm_final'</span>, nn.BatchNorm2d(num_features))</div><div class="line"></div><div class="line">        <span class="comment"># Linear layer</span></div><div class="line">        self.classifier = nn.Linear(num_features, num_classes)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        features = self.features(x)</div><div class="line">        out = F.relu(features, inplace=<span class="keyword">True</span>)</div><div class="line">        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(</div><div class="line">                           features.size(<span class="number">0</span>), <span class="number">-1</span>)</div><div class="line">        out = self.classifier(out)</div><div class="line">        <span class="keyword">return</span> out</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;The blog source:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504&quot;&gt;https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;I have added the PyTorch implementation from&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/gpleiss/efficient_densenet_pytorch&quot;&gt;https://github.com/gpleiss/efficient_densenet_pytorch&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;DenseNet&lt;/a&gt;(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. It’s quite similar to &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt; but has some fundamental differences.&lt;/p&gt;&lt;p&gt;With all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*7WdURialIGTojNI9ltrplA.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Error rates on various datasets(from source paper)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;And for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*N_o10WxGx_e6vBSNnV5k1A.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Сomparison of the DenseNet and ResNet Top-1 error rates on the ImageNet classification dataset as a function of learned parameters (left) and flops during test-time (right)(from source paper).&lt;/em&gt;&lt;/p&gt;&lt;p&gt;This post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Why dense net differs from another convolution networks.&lt;/li&gt;&lt;li&gt;What difficulties I’ve met during the implementation of DenseNet in tensorflow.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;If you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the &lt;a href=&quot;https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504#aabd&quot;&gt;second chapter&lt;/a&gt; or check the &lt;a href=&quot;https://github.com/ikhlestov/vision_networks&quot;&gt;source code on GitHub&lt;/a&gt;. If you not familiar with any topics but want to get some knowledge — I highly advise you &lt;a href=&quot;http://cs231n.github.io/&quot;&gt;CS231n Stanford classes&lt;/a&gt;.&lt;/p&gt;&lt;h4 id=&quot;Compare-DenseNet-with-other-Convolution-Networks&quot;&gt;&lt;a href=&quot;#Compare-DenseNet-with-other-Convolution-Networks&quot; class=&quot;headerlink&quot; title=&quot;Compare DenseNet with other Convolution Networks&quot;&gt;&lt;/a&gt;Compare DenseNet with other Convolution Networks&lt;/h4&gt;&lt;p&gt;Usually, ConvNets work such way:&lt;br&gt;We have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.&lt;br&gt;So the output from the Lᵢ layer is input to the Lᵢ₊₁ layer. It seems like this:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*1F8wom5X1DeCj5BWeGfJbw.jpeg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;source: &amp;lt;&lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;http://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/em&gt;&amp;gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;ResNet&lt;/a&gt; architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the Lᵢ layer was obtained by summation of outputs from previous layers.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*-C2QoqhfCjG8xZRu-UwO_Q.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.&lt;br&gt;So, let’s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.&lt;br&gt;The output from Lᵢ layer will be (28, 28, 12).&lt;br&gt;But input to the Lᵢ₊₁ will be (28, 28, 24+12), for Lᵢ₊₂ (28, 28, 24 + 12 + 12) and so on.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*6JoB4BeIZyWDNGH5U63y_Q.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Block of convolution layers with results concatenated&lt;/em&gt;&lt;/p&gt;&lt;p&gt;After a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).&lt;br&gt;All these N layers are named Block in the paper. There’s also batch normalization, nonlinearity and dropout inside the block.&lt;br&gt;To reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*-CQyW4huxxxMYpffb72emg.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Transition layer&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Now we can again pass the image through the block with N convolutions.&lt;br&gt;With this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.&lt;br&gt;Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*SSn5H14SKhhaZZ5XYWN3Cg.jpeg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Full DenseNet example with 3 blocks from source paper&lt;/em&gt;&lt;/p&gt;&lt;h4 id=&quot;Notes-about-implementation&quot;&gt;&lt;a href=&quot;#Notes-about-implementation&quot; class=&quot;headerlink&quot; title=&quot;Notes about implementation&quot;&gt;&lt;/a&gt;Notes about implementation&lt;/h4&gt;&lt;p&gt;In the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.&lt;/p&gt;&lt;p&gt;First of all, it was not clear how many blocks should be used depends on depth. After I’ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.&lt;/p&gt;&lt;p&gt;Second I’ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .&lt;/p&gt;&lt;p&gt;Despite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; block &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(required_blocks):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    output = build_block(output)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; block != (required_blocks — &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        output = transition_layer(output)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;For weights initialization authors proposed use MRSA initialization(as per&lt;a href=&quot;https://arxiv.org/abs/1502.01852&quot;&gt;this paper&lt;/a&gt;). In tensorflow this initialization can be easy implemented with&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/contrib/layers/variance_scaling_initializer&quot;&gt;variance scaling initializer&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;In the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;batch norm -&amp;gt; relu -&amp;gt; conv 1x1 -&amp;gt; dropout -&amp;gt; batch norm -&amp;gt; relu -&amp;gt; conv 3x3 -&amp;gt; dropout -&amp;gt; output.&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;Despite two conv filters, only last output will be concatenated to the main pool of features.&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*FvW30tjOQ2hDQ7LdEkCRLQ.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;Also at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*N-nU5WXQcrNfhnQMwxquFA.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;Where theta — some reduction values, in the range (0, 1).&lt;/p&gt;&lt;p&gt;In case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.&lt;/p&gt;&lt;p&gt;Last, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used — just divide every image pixel by 255, so we have pixels values in the range [0, 1].&lt;/p&gt;&lt;p&gt;At first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next I’ve implemented per channel normalization… And networks began works even worse. It was not clear for me why. So I’ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.&lt;/p&gt;&lt;p&gt;And some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# without this line next slice assignment will silently fail!&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# at least in numpy 1.12.0&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;images = images.astype(‘float64’)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(channels):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    images[:, :, :, i] = (&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        (images[:, :, :, i] — self.images_means[i]) /&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;         self.images_stds[i])&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h4 id=&quot;Conclusion&quot;&gt;&lt;a href=&quot;#Conclusion&quot; class=&quot;headerlink&quot; title=&quot;Conclusion&quot;&gt;&lt;/a&gt;Conclusion&lt;/h4&gt;&lt;p&gt;DenseNets are powerful neural nets that achieve state of the art performance on many datasets. And it’s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec: The Skip-Gram Model</title>
    <link href="http://yoursite.com/2017/11/14/Word2Vec-The-Skip-Gram-Model/"/>
    <id>http://yoursite.com/2017/11/14/Word2Vec-The-Skip-Gram-Model/</id>
    <published>2017-11-14T13:41:00.000Z</published>
    <updated>2017-11-15T03:27:02.749Z</updated>
    
    <content type="html"><![CDATA[<p>This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual introductory and abstract insights about Word2Vec, and get into more of the details. Specifically here I’m diving into the skip gram neural network model.</p><h1 id="The-Model"><a href="#The-Model" class="headerlink" title="The Model"></a>The Model</h1><p>The skip-gram neural network model is actually surprisingly simple in its most basic form; I think it’s the all the little tweaks and enhancements that start to clutter the explanation.</p><p>Let’s start with a high-level insight about where we’re going. Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn.</p><p>Another place you may have seen this trick is in unsupervised feature learning, where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer–it’s a trick for learning good image features without having labeled training data.</p><h1 id="The-Fake-Task"><a href="#The-Fake-Task" class="headerlink" title="The Fake Task"></a>The Fake Task</h1><p>So now we need to talk about this “fake” task that we’re going to build the neural network to perform, and then we’ll come back later to how this indirectly gives us those word vectors that we are really after.</p><p>We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.</p><p>When I say “nearby”, there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).</p><p>The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.</p><p>We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.</p><p><a href="http://mccormickml.com/assets/word2vec/training_data.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/training_data.png" alt="Training Data"></a></p><p>The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”.</p><h1 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h1><p>So how is this all represented?</p><p>First of all, you know you can’t feed a word just as a text string to a neural network, so we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words.</p><p>We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.</p><p>The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.</p><p>Here’s the architecture of our neural network.</p><p><a href="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png" alt="Skip-gram Neural Network Architecture"></a></p><p>There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.</p><p>When <em>training</em> this network on word pairs, the input is a one-hot vector representing the input word and the training output <em>is also a one-hot vector</em>representing the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, <em>not</em> a one-hot vector).</p><h1 id="The-Hidden-Layer"><a href="#The-Hidden-Layer" class="headerlink" title="The Hidden Layer"></a>The Hidden Layer</h1><p>For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).</p><p>300 features is what Google used in their published model trained on the Google news dataset (you can download it from <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="external">here</a>). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results).</p><p>If you look at the <em>rows</em> of this weight matrix, these are actually what will be our word vectors!</p><p><a href="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png" alt="Hidden Layer Weight Matrix"></a></p><p>So the end goal of all of this is really just to learn this hidden layer weight matrix – the output layer we’ll just toss when we’re done!</p><p>Let’s get back, though, to working through the definition of this model that we’re going to train.</p><p>Now, you might be asking yourself–“That one-hot vector is almost all zeros… what’s the effect of that?” If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just <em>select</em> the matrix row corresponding to the “1”. Here’s a small example to give you a visual.</p><p><a href="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png" alt="Effect of matrix multiplication with a one-hot vector"></a></p><p>This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word.</p><h1 id="The-Output-Layer"><a href="#The-Output-Layer" class="headerlink" title="The Output Layer"></a>The Output Layer</h1><p>The <code>1 x 300</code> word vector for “ants” then gets fed to the output layer. The output layer is a softmax regression classifier. There’s an in-depth tutorial on Softmax Regression <a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/" target="_blank" rel="external">here</a>, but the gist of it is that each output neuron (one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.</p><p>Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function <code>exp(x)</code> to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from <em>all</em> 10,000 output nodes.</p><p>Here’s an illustration of calculating the output of the output neuron for the word “car”.</p><p><a href="http://mccormickml.com/assets/word2vec/output_weights_function.png" target="_blank" rel="external"><img src="http://mccormickml.com/assets/word2vec/output_weights_function.png" alt="Behavior of the output neuron"></a></p><p>Note that neural network does not know anything about the offset of the output word relative to the input word. It <em>does not</em> learn a different set of probabilities for the word before the input versus the word after. To understand the implication, let’s say that in our training corpus, <em>every single occurrence</em> of the word ‘York’ is preceded by the word ‘New’. That is, at least according to the training data, there is a 100% probability that ‘New’ will be in the vicinity of ‘York’. However, if we take the 10 words in the vicinity of ‘York’ and randomly pick one of them, the probability of it being ‘New’ <em>is not</em> 100%; you may have picked one of the other words in the vicinity.</p><h1 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h1><p>Ok, are you ready for an exciting bit of insight into this network?</p><p>If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if <em>the word vectors are similar</em>. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!</p><p>And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well.</p><p>This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts.</p><h1 id="More-Math-Details"><a href="#More-Math-Details" class="headerlink" title="More Math Details"></a>More Math Details</h1><p>For each word $t=1\cdots T$, predict surrounding words in a window of “radius” $m$ of every word.</p><h2 id="Objective-function"><a href="#Objective-function" class="headerlink" title="Objective function:"></a>Objective function:</h2><p>Maximize the probability of any context word given the current center word:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-14-Image-8.png" alt="Page-14-Image-8"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-14-Image-7.png" alt="Page-14-Image-7"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-16-Image-9.png" alt="Page-16-Image-9"></p><h2 id="The-Skip-Gram-Algorithm"><a href="#The-Skip-Gram-Algorithm" class="headerlink" title="The Skip-Gram Algorithm:"></a>The Skip-Gram Algorithm:</h2><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-19-Image-17.png" alt="Page-19-Image-17"></p><h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-24-Image-31.png" alt="Page-24-Image-31"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-25-Image-32.png" alt="Page-25-Image-32"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-26-Image-33.png" alt="Page-26-Image-33"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/cs224n-lec2-201703/Page-27-Image-34.png" alt="Page-27-Image-34"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual i
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="natural language process" scheme="http://yoursite.com/tags/natural-language-process/"/>
    
  </entry>
  
  <entry>
    <title>Prioritized Experience Replay</title>
    <link href="http://yoursite.com/2017/10/30/Prioritized-Experience-Replay/"/>
    <id>http://yoursite.com/2017/10/30/Prioritized-Experience-Replay/</id>
    <published>2017-10-30T06:18:36.000Z</published>
    <updated>2017-10-30T06:25:34.813Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Prioritized-Experience-Replay"><a href="#Prioritized-Experience-Replay" class="headerlink" title="Prioritized Experience Replay"></a>Prioritized Experience Replay</h3><p>One of the possible improvements already acknowledged in the original research<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#fn-444-1" target="_blank" rel="external">2</a> lays in the way experience is used. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#fn-444-4" target="_blank" rel="external">3</a>(PER) is one strategy that tries to leverage this fact by changing the sampling distribution.</p><p>The main idea is that we prefer transitions that does not fit well to our current estimate of the Q function, because these are the transitions that we can learn most from. This reflects a simple intuition from our real world – if we encounter a situation that really differs from our expectation, we think about it over and over and change our model until it fits.</p><p>We can define an error of a sample $S = (s, a, r, s’)$ as a distance between the $Q(s, a)$ and its target $T(S)$:<br>$$<br>error = |Q(s, a) - T(S)|<br>$$<br>For DDQN described above, $T$ it would be:<br>$$<br>T(S) = r + \gamma \tilde{Q}(s’, argmax_a Q(s’, a))<br>$$<br>We will store this error in the agent’s memory along with every sample and update it with each learning step.</p><p>One of the possible approaches to PER is proportional prioritization. The error is first converted to priority using this formula:<br>$$<br>p = (error + \epsilon)^\alpha<br>$$<br>Epsilon $\epsilon$ is a small positive constant that ensures that no transition has zero priority.<br>Alpha, $0 \leq \alpha \leq 1$, controls the difference between high and low error. It determines how much prioritization is used. With $\alpha$ we would get the uniform case.</p><p>Priority is translated to probability of being chosen for replay. A sample $i$ has a probability of being picked during the experience replay determined by a formula:<br>$$<br>P_i = \frac{p_i}{\sum_k p_k}<br>$$<br>The algorithm is simple – during each learning step we will get a batch of samples with this probability distribution and train our network on it. We only need an effective way of storing these priorities and sampling from them.</p><h5 id="Initialization-and-new-transitions"><a href="#Initialization-and-new-transitions" class="headerlink" title="Initialization and new transitions"></a>Initialization and new transitions</h5><p>The original paper says that new transitions come without a known error<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/#fn-444-4" target="_blank" rel="external">3</a>, but I argue that with definition given above, we can compute it with a simple forward pass as it arrives. It’s also effective, because high value transitions are discovered immediately.</p><p>Another thing is the initialization. Remember that before the learning itself, we fill the memory using random agent. But this agent does not use any neural network, so how could we estimate any error? We can use a fact that untrained neural network is likely to return a value around zero for every input. In this case the error formula becomes very simple:<br>$$<br>error = |Q(s, a) - T(S)| = |Q(s, a) - r - \gamma \tilde{Q}(s’, argmax_a Q(s’, a))| = | r |<br>$$<br>The error in this case is simply the reward experienced in a given sample. Indeed, the transitions where the agent experienced any reward intuitively seem to be very promising.</p><h5 id="Efficient-implementation"><a href="#Efficient-implementation" class="headerlink" title="Efficient implementation"></a>Efficient implementation</h5><p>So how do we store the experience and effectively sample from it?</p><p>A naive implementation would be to have all samples in an array sorted according to their priorities. A random number <em>s</em>, $0 \leq s \leq \sum_k p_k$, would be picked and the array would be walked left to right, summing up a priority of the current element until the sum is exceeded and that element is chosen. This will select a sample with the desired probability distribution.</p><p><img src="https://jaromiru.files.wordpress.com/2016/11/per_bar_1.png?w=700" alt="Sorted experience"></p><p>But this would have a terrible efficiency: $O(n log n)$ for insertion and update and O$(n) $for sampling.</p><p>A first important observation is that we don’t have to actually store the array sorted. Unsorted array would do just as well. Elements with higher priorities are still picked with higher probability.</p><p><img src="https://jaromiru.files.wordpress.com/2016/11/per_bar_2.png?w=700" alt="Unsorted experience"></p><p>This releases the need for sorting, improving the algorithm to <em>O(1)</em> for insertion and update.</p><p>But the <em>O(n)</em> for sampling is still too high. We can further improve our algorithm by using a different data structure. We can store our samples in unsorted sum tree – a binary tree data structure where the parent’s value is the sum of its children. The samples themselves are stored in the leaf nodes.</p><p>Update of a leaf node involves propagating a value difference up the tree, obtaining <em>O(log n)</em>. Sampling follows the thought process of the array case, but achieves <em>O(log n)</em>. For a value <em>s</em>, $0 \leq s \leq \sum_k p_k$, we use the following algorithm (pseudo code):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieve</span><span class="params">(n, s)</span>:</span></div><div class="line">    <span class="keyword">if</span> n <span class="keyword">is</span> leaf_node: <span class="keyword">return</span> n</div><div class="line"> </div><div class="line">    <span class="keyword">if</span> n.left.val &gt;= s: <span class="keyword">return</span> retrieve(n.left, s)</div><div class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> retrieve(n.right, s - n.left.val)</div></pre></td></tr></table></figure><p>Following picture illustrates sampling from a tree with <em>s = 24</em>:</p><p><img src="https://jaromiru.files.wordpress.com/2016/11/sumtree.png?w=560" alt="Sampling from sum tree"></p><p>With this effective implementation we can use large memory sizes, with hundreds of thousands or millions of samples.</p><p>For known capacity, this sum tree data structure can be backed by an array. Its reference implementation containing 50 lines of code is available on <a href="https://github.com/jaara/AI-blog/blob/master/SumTree.py" target="_blank" rel="external">GitHub</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Prioritized-Experience-Replay&quot;&gt;&lt;a href=&quot;#Prioritized-Experience-Replay&quot; class=&quot;headerlink&quot; title=&quot;Prioritized Experience Replay&quot;&gt;&lt;/a
    
    </summary>
    
    
      <category term="Reinforcement Learning" scheme="http://yoursite.com/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu Server 16.04 Install Gnome and remote connect from Windows VNCViewer</title>
    <link href="http://yoursite.com/2017/10/26/Ubuntu-Server-16-04-Install-Gnome-and-remote-connect-from-Windows-VNCViewer/"/>
    <id>http://yoursite.com/2017/10/26/Ubuntu-Server-16-04-Install-Gnome-and-remote-connect-from-Windows-VNCViewer/</id>
    <published>2017-10-26T04:02:50.000Z</published>
    <updated>2017-10-30T06:04:58.307Z</updated>
    
    <content type="html"><![CDATA[<h3 id="第一步：装-Gnome-环境"><a href="#第一步：装-Gnome-环境" class="headerlink" title="第一步：装 Gnome 环境"></a>第一步：装 Gnome 环境</h3><p>首先按照如下命令安装 Gnome 环境。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get upgrade</div><div class="line">sudo apt-get install gnome</div><div class="line">sudo apt-get install ubuntu-gnome-desktop</div><div class="line">sudo apt-get install gnome-shell</div></pre></td></tr></table></figure><h3 id="第二步：安装-Gnome-界面管理工具"><a href="#第二步：安装-Gnome-界面管理工具" class="headerlink" title="第二步：安装 Gnome 界面管理工具"></a>第二步：安装 Gnome 界面管理工具</h3><p>安装 Gnome 桌面环境的配置工具。可以使用该工作对 Linux 进行很多配置，包括外观，工作台的数量等。后续安装的主题和图标都可以通过这个工具的 _外观（Appearance）_ 进行调整。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get gnome-tweak-tool</div></pre></td></tr></table></figure><h3 id="第三步：安装-Dash-to-Dock-工具条"><a href="#第三步：安装-Dash-to-Dock-工具条" class="headerlink" title="第三步：安装 Dash to Dock 工具条"></a>第三步：安装 Dash to Dock 工具条</h3><p>安装 Gnome 桌面环境下的 Dock 工具条，可提供 mac os 下dock类似的使用体验。</p><p>在任意浏览器打开 <a href="https://extensions.gnome.org/local/" target="_blank" rel="external">Gnome extensions</a>.</p><p>找到 _Dash to Dock_ 扩展栏，点开右面的 _[ON OFF]_ 选项。点击旁边的 _工具_ 选项，可进一步配置更多选项。</p><h3 id="第四步：安装-ARC-扁平化主题和图标"><a href="#第四步：安装-ARC-扁平化主题和图标" class="headerlink" title="第四步：安装 _ARC_ 扁平化主题和图标"></a>第四步：安装 _ARC_ 扁平化主题和图标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo add-apt-repository ppa:noobslab/themes</div><div class="line">sudo add-apt-repository ppa:noobslab/icons</div><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install arc-theme</div><div class="line">sudo apt-get install arc-icons</div></pre></td></tr></table></figure><h3 id="第五步：选装-Flat-Plat-扁平化主题"><a href="#第五步：选装-Flat-Plat-扁平化主题" class="headerlink" title="第五步：选装 _Flat Plat_ 扁平化主题"></a>第五步：选装 _Flat Plat_ 扁平化主题</h3><p>另一个扁平化主题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">curl -sL https://github.com/nana-4/Flat-Plat/archive/v20170323.tar.gz | tar xz</div><div class="line">cd Flat-Plat-20170323/</div><div class="line">sudo ./install.sh</div></pre></td></tr></table></figure><h3 id="第六步：安装vncserver"><a href="#第六步：安装vncserver" class="headerlink" title="第六步：安装vncserver"></a>第六步：安装vncserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install vnc4server</div><div class="line">sudo apt-get install gnome-panel gnome-settings-daemon metacity nautilus gnome-terminal</div><div class="line">cd ~/.vnc</div><div class="line">mv xstartup xstartup.bak</div><div class="line">vim xstartup</div></pre></td></tr></table></figure><p>使用以下配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">#!/bin/sh</div><div class="line"></div><div class="line">export XKL_XMODMAP_DISABLE=1</div><div class="line">unset SESSION_MANAGER</div><div class="line">unset DBUS_SESSION_BUS_ADDRESS</div><div class="line"></div><div class="line">[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup</div><div class="line">[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources</div><div class="line">xsetroot -solid grey</div><div class="line">vncconfig -iconic &amp;</div><div class="line"></div><div class="line">gnome-session &amp;</div><div class="line">gnome-panel &amp;</div><div class="line">gnome-settings-daemon &amp;</div><div class="line">metacity &amp;</div><div class="line">nautilus &amp;</div><div class="line">gnome-terminal &amp;</div></pre></td></tr></table></figure><h3 id="第七步：启动-vncserver"><a href="#第七步：启动-vncserver" class="headerlink" title="第七步：启动 vncserver"></a>第七步：启动 vncserver</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># ：1可以更改</div><div class="line">vncserver -geometry 1920x1080 -alwaysshared :1</div></pre></td></tr></table></figure><h3 id="第八步：在-Windows-上安装-VNCViewer"><a href="#第八步：在-Windows-上安装-VNCViewer" class="headerlink" title="第八步：在 Windows 上安装 VNCViewer"></a>第八步：在 Windows 上安装 <a href="https://www.realvnc.com/en/connect/download/viewer/" target="_blank" rel="external">VNCViewer</a></h3><p>启动只要 输入 ip:1 即可</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;第一步：装-Gnome-环境&quot;&gt;&lt;a href=&quot;#第一步：装-Gnome-环境&quot; class=&quot;headerlink&quot; title=&quot;第一步：装 Gnome 环境&quot;&gt;&lt;/a&gt;第一步：装 Gnome 环境&lt;/h3&gt;&lt;p&gt;首先按照如下命令安装 Gnome 环境。&lt;/
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Union Find</title>
    <link href="http://yoursite.com/2017/10/23/Union-Find/"/>
    <id>http://yoursite.com/2017/10/23/Union-Find/</id>
    <published>2017-10-23T05:41:42.000Z</published>
    <updated>2017-10-30T06:05:10.547Z</updated>
    
    <content type="html"><![CDATA[<h2 id="五分钟搞懂并查集"><a href="#五分钟搞懂并查集" class="headerlink" title="五分钟搞懂并查集"></a>五分钟搞懂并查集</h2><blockquote><p>转自：laserss<br><a href="http://blog.csdn.net/dellaserss/article/details/7724401/" target="_blank" rel="external">http://blog.csdn.net/dellaserss/article/details/7724401/</a></p></blockquote><p>并查集是我暑假从高手那里学到的一招，觉得真是太精妙的设计了。来看一个实例，杭电1232畅通工程。首先在地图上给你若干个城镇，这些城镇都可以看作点，然后告诉你哪些对城镇之间是有道路直接相连的。最后要解决的是整幅图的连通性问题。比如随意给你两个点，让你判断它们是否连通，或者问你整幅图一共有几个连通分支，也就是被分成了几个互相独立的块。</p><p>像畅通工程这题，问还需要修几条路，实质就是求有几个连通分支。如果是1个连通分支，说明整幅图上的点都连起来了，不用再修路了；如果是2个连通分支，则只要再修1条路，从两个分支中各选一个点，把它们连起来，那么所有的点都是连起来的了；如果是3个连通分支，则只要再修两条路……</p><p>以下面这组数据输入数据来说明</p><p>4 2 1 3 4 3</p><p>第一行告诉你，一共有4个点，2条路。下面两行告诉你，1、3之间有条路，4、3之间有条路。那么整幅图就被分成了1-3-4和2两部分。只要再加一条路，把2和其他任意一个点连起来，畅通工程就实现了，那么这个这组数据的输出结果就是1。好了，现在编程实现这个功能吧，城镇有几百个，路有不知道多少条，而且可能有回路。</p><p>这可如何是好？我以前也不会呀，自从用了并查集之后，嗨，效果还真好！我们全家都用它！并查集由一个整数型的数组和两个函数构成。数组pre[]记录了每个点的前导点是什么，函数find是查找，join是合并。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">int pre[1000 ];</div><div class="line">int find(int x)                                                                   //查找根节点</div><div class="line">&#123;  </div><div class="line">     int r=x; </div><div class="line">     while ( pre[r ] != r )                                                       //返回根节点 r </div><div class="line">           r=pre[r ];</div><div class="line"> </div><div class="line">     int i=x , j ;</div><div class="line">     while( i != r )                                                                 //路径压缩</div><div class="line">     &#123;</div><div class="line">          j = pre[ i ];     // 在改变上级之前用临时变量  j 记录下他的值 </div><div class="line">          pre[ i ]= r ;    //把上级改为根节点</div><div class="line">          i=j;</div><div class="line">     &#125;</div><div class="line">     return r ;</div><div class="line"> &#125;</div><div class="line"> </div><div class="line"> //判断x y是否连通，如果已经连通，就不用管了 //如果不连通，就把它们所在的连通分支合并起,</div><div class="line"> </div><div class="line"> void join(int x,int y)                     </div><div class="line"> &#123;</div><div class="line">     int fx=find(x),fy=find(y);</div><div class="line">     if(fx!=fy)</div><div class="line">         pre[fx ]=fy;</div><div class="line"> &#125;</div></pre></td></tr></table></figure><p>为了解释并查集的原理，我将举一个更有爱的例子。 话说江湖上散落着各式各样的大侠，有上千个之多。他们没有什么正当职业，整天背着剑在外面走来走去，碰到和自己不是一路人的，就免不了要打一架。但大侠们有一个优点就是讲义气，绝对不打自己的朋友。而且他们信奉“朋友的朋友就是我的朋友”，只要是能通过朋友关系串联起来的，不管拐了多少个弯，都认为是自己人。</p><p>这样一来，江湖上就形成了一个一个的群落，通过两两之间的朋友关系串联起来。而不在同一个群落的人，无论如何都无法通过朋友关系连起来，于是就可以放心往死了打。但是两个原本互不相识的人，如何判断是否属于一个朋友圈呢？</p><p>我们可以在每个朋友圈内推举出一个比较有名望的人，作为该圈子的代表人物，这样，每个圈子就可以这样命名“齐达内朋友之队”“罗纳尔多朋友之队”……两人只要互相对一下自己的队长是不是同一个人，就可以确定敌友关系了。</p><p>但是还有问题啊，大侠们只知道自己直接的朋友是谁，很多人压根就不认识队长，要判断自己的队长是谁，只能漫无目的的通过朋友的朋友关系问下去：“你是不是队长？你是不是队长？”这样一来，队长面子上挂不住了，而且效率太低，还有可能陷入无限循环中。</p><p>于是队长下令，重新组队。队内所有人实行分等级制度，形成树状结构，我队长就是根节点，下面分别是二级队员、三级队员。每个人只要记住自己的上级是谁就行了。遇到判断敌友的时候，只要一层层向上问，直到最高层，就可以在短时间内确定队长是谁了。</p><p>由于我们关心的只是两个人之间是否连通，至于他们是如何连通的，以及每个圈子内部的结构是怎样的，甚至队长是谁，并不重要。所以我们可以放任队长随意重新组队，只要不搞错敌友关系就好了。于是，门派产生了。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/QtPIxk7nOVdNRrpMREYRUAaor1hgv7kkWHTuGNGvAFRD5ibRSHjtSW8ZibABfibmYRrMWiaYJtrOnEQlL7UEhVt5PQ/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt=""></p><p>下面我们来看并查集的实现。 int pre[1000]; 这个数组，记录了每个大侠的上级是谁。大侠们从1或者0开始编号（依据题意而定），pre[15]=3就表示15号大侠的上级是3号大侠。如果一个人的上级就是他自己，那说明他就是掌门人了，查找到此为止。也有孤家寡人自成一派的，比如欧阳锋，那么他的上级就是他自己。</p><p>每个人都只认自己的上级。比如胡青牛同学只知道自己的上级是杨左使。张无忌是谁？不认识！要想知道自己的掌门是谁，只能一级级查上去。 find这个函数就是找掌门用的，意义再清楚不过了（路径压缩算法先不论，后面再说）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">int find(int x)                                          //查找我（x）的掌门</div><div class="line">&#123;</div><div class="line">    int r=x;                                               //委托 r 去找掌门</div><div class="line">    while (pre[r ]!=r)                                //如果r的上级不是r自己（也就是说找到的大侠他不是掌门 = =） </div><div class="line">    r=pre[r] ;                                           // r 就接着找他的上级，直到找到掌门为止。</div><div class="line">    return  r ;                                           //掌门驾到~~~</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>再来看看join函数，就是在两个点之间连一条线，这样一来，原先它们所在的两个板块的所有点就都可以互通了。这在图上很好办，画条线就行了。但我们现在是用并查集来描述武林中的状况的，一共只有一个pre[]数组，该如何实现呢？</p><p>还是举江湖的例子，假设现在武林中的形势如图所示。虚竹小和尚与周芷若MM是我非常喜欢的两个人物，他们的终极boss分别是玄慈方丈和灭绝师太，那明显就是两个阵营了。我不希望他们互相打架，就对他俩说：“你们两位拉拉勾，做好朋友吧。”他们看在我的面子上，同意了。这一同意可非同小可，整个少林和峨眉派的人就不能打架了。这么重大的变化，可如何实现呀，要改动多少地方？</p><p>其实非常简单，我对玄慈方丈说：“大师，麻烦你把你的上级改为灭绝师太吧。这样一来，两派原先的所有人员的终极boss都是师太，那还打个球啊！反正我们关心的只是连通性，门派内部的结构不要紧的。”玄慈一听肯定火大了：“我靠，凭什么是我变成她手下呀，怎么不反过来？我抗议！”抗议无效，上天安排的，最大。反正谁加入谁效果是一样的，我就随手指定了一个。这段函数的意思很明白了吧？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">void join(int x,int y)                                     //我想让虚竹和周芷若做朋友 </div><div class="line">&#123;</div><div class="line">    int fx=find(x),fy=find(y);                          //虚竹的老大是玄慈，芷若MM的老大是灭绝</div><div class="line">    if(fx!=fy)                                                  //玄慈和灭绝显然不是同一个人</div><div class="line">     pre[fx]=fy;                                              //方丈只好委委屈屈地当了师太的手下啦</div><div class="line">  </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>再来看看路径压缩算法。建立门派的过程是用join函数两个人两个人地连接起来的，谁当谁的手下完全随机。最后的树状结构会变成什么胎唇样，我也完全无法预计，一字排开也有可能。这样查找的效率就会比较低下。最理想的情况就是所有人的直接上级都是掌门，一共就两级结构，只要找一次就找到掌门了。哪怕不能完全做到，也最好尽量接近。这样就产生了路径压缩算法。</p><p>设想这样一个场景：两个互不相识的大侠碰面了，想知道能不能揍。 于是赶紧打电话问自己的上级：“你是不是掌门？” 上级说：“我不是呀，我的上级是谁谁谁，你问问他看看。” 一路问下去，原来两人的最终boss都是东厂曹公公。 “哎呀呀，原来是记己人，西礼西礼，在下三营六组白面葫芦娃!” “幸会幸会，在下九营十八组仙子狗尾巴花！” 两人高高兴兴地手拉手喝酒去了。 “等等等等，两位同学请留步，还有事情没完成呢！”我叫住他俩。 “哦，对了，还要做路径压缩。”两人醒悟。</p><p>白面葫芦娃打电话给他的上级六组长：“组长啊，我查过了，其习偶们的掌门是曹公公。不如偶们一起及接拜在曹公公手下吧，省得级别太低，以后查找掌门麻环。” “唔，有道理。” 白面葫芦娃接着打电话给刚才拜访过的三营长……仙子狗尾巴花也做了同样的事情。</p><p>这样，查询中所有涉及到的人物都聚集在曹公公的直接领导下。每次查询都做了优化处理，所以整个门派树的层数都会维持在比较低的水平上。路径压缩的代码，看得懂很好，看不懂也没关系，直接抄上用就行了。总之它所实现的功能就是这么个意思。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/QtPIxk7nOVdNRrpMREYRUAaor1hgv7kkRPXS7odI2JpZ10cHKV9dgGrsttLnxpSYwK6W8uYjqIffQKdP03IIhA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt=""></p><p>下面给出杭电1232畅通工程的解题代码，仅供大家参考，使用并查集来解决问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line">#include&lt;iostream  </div><div class="line">using namespace std;  </div><div class="line">int  pre[1050];  </div><div class="line">bool t[1050];               //t 用于标记独立块的根结点  </div><div class="line"></div><div class="line">int Find(int x)  </div><div class="line">&#123;  </div><div class="line">    int r=x;  </div><div class="line">    while(r!=pre[r])  </div><div class="line">        r=pre[r];   </div><div class="line"></div><div class="line">    int i=x,j;  </div><div class="line">    while(pre[i]!=r)  </div><div class="line">    &#123;  </div><div class="line">        j=pre[i];  </div><div class="line">        pre[i]=r;  </div><div class="line">        i=j;  </div><div class="line">    &#125;  </div><div class="line"></div><div class="line">    return r;  </div><div class="line">&#125;  </div><div class="line"></div><div class="line">void mix(int x,int y)  </div><div class="line">&#123;  </div><div class="line">    int fx=Find(x),fy=Find(y);  </div><div class="line">    if(fx!=fy)  </div><div class="line">    &#123;  </div><div class="line">        pre[fy]=fx;  </div><div class="line">    &#125;  </div><div class="line">&#125;   </div><div class="line"></div><div class="line">int main()  </div><div class="line">&#123; </div><div class="line">    int N,M,a,b,i,j,ans;  </div><div class="line">    while(scanf(&quot;%d%d&quot;,&amp;N,&amp;M)&amp;&amp;N)  </div><div class="line">    &#123;  </div><div class="line">        for(i=1;i&lt;=N;i++)          //初始化   </div><div class="line">            pre[i]=i;  </div><div class="line">        for(i=1;i&lt;=M;i++)          //吸收并整理数据   </div><div class="line">        &#123;  </div><div class="line">            scanf(&quot;%d%d&quot;,&amp;a,&amp;b);  </div><div class="line">            mix(a,b);  </div><div class="line">        &#125;         </div><div class="line">        memset(t,0,sizeof(t));  </div><div class="line">        for(i=1;i&lt;=N;i++)          //标记根结点  </div><div class="line">        &#123;  </div><div class="line">            t[Find(i)]=1;  </div><div class="line">        &#125;  </div><div class="line">        for(ans=0,i=1;i&lt;=N;i++)  </div><div class="line">            if(t[i])  </div><div class="line">                ans++;  </div><div class="line">        printf(&quot;%d\n&quot;,ans-1);  </div><div class="line">    &#125;  </div><div class="line"></div><div class="line">    return 0;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;五分钟搞懂并查集&quot;&gt;&lt;a href=&quot;#五分钟搞懂并查集&quot; class=&quot;headerlink&quot; title=&quot;五分钟搞懂并查集&quot;&gt;&lt;/a&gt;五分钟搞懂并查集&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;转自：laserss&lt;br&gt;&lt;a href=&quot;http://blog
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>37 Reasons why your Neural Network is not working [Repost]</title>
    <link href="http://yoursite.com/2017/07/28/37-Reasons-why-your-Neural-Network-is-not-working/"/>
    <id>http://yoursite.com/2017/07/28/37-Reasons-why-your-Neural-Network-is-not-working/</id>
    <published>2017-07-28T06:23:50.000Z</published>
    <updated>2017-07-28T06:38:10.114Z</updated>
    
    <content type="html"><![CDATA[<p>The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then came the predictions: all zeroes, all background, nothing detected. “What did I do wrong?” — I asked my computer, who didn’t answer.</p><p>Where do you start checking if your model is outputting garbage (for example predicting the mean of all outputs, or it has really poor accuracy)?</p><p>A network might not be training for a number of reasons. Over the course of many debugging sessions, I would often find myself doing the same checks. I’ve compiled my experience along with the best ideas around in this handy list. I hope they would be of use to you, too.</p><h3 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h3><blockquote><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#b6fb" target="_blank" rel="external">0. How to use this guide?</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#678a" target="_blank" rel="external">I. Dataset issues</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#86fe" target="_blank" rel="external">II. Data Normalization/Augmentation issues</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#95eb" target="_blank" rel="external">III. Implementation issues</a></p><p><a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#74de" target="_blank" rel="external">IV. Training issues</a></p></blockquote><h3 id="0-How-to-use-this-guide"><a href="#0-How-to-use-this-guide" class="headerlink" title="0. How to use this guide?"></a>0. How to use this guide?</h3><p>A lot of things can go wrong. But some of them are more likely to be broken than others. I usually start with this short list as an emergency first response:</p><ol><li>Start with a simple model that is known to work for this type of data (for example, VGG for images). Use a standard loss if possible.</li><li>Turn off all bells and whistles, e.g. regularization and data augmentation.</li><li>If finetuning a model, double check the preprocessing, for it should be the same as the original model’s training.</li><li>Verify that the input data is correct.</li><li>Start with a really small dataset (2–20 samples). Overfit on it and gradually add more data.</li><li>Start gradually adding back all the pieces that were omitted: augmentation/regularization, custom loss functions, try more complex models.</li></ol><p>If the steps above don’t do it, start going down the following big list and verify things one by one.</p><hr><h3 id="I-Dataset-issues"><a href="#I-Dataset-issues" class="headerlink" title="I. Dataset issues"></a>I. Dataset issues</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*xfIbyKKMDmjQF9JFuK2Ykg.png" alt="img">Source: <a href="http://dilbert.com/strip/2014-05-07" target="_blank" rel="external">http://dilbert.com/strip/2014-05-07</a></p><h4 id="1-Check-your-input-data"><a href="#1-Check-your-input-data" class="headerlink" title="1. Check your input data"></a>1. Check your input data</h4><p>Check if the input data you are feeding the network makes sense. For example, I’ve more than once mixed the width and the height of an image. Sometimes, I would feed all zeroes by mistake. Or I would use the same batch over and over. So print/display a couple of batches of input and target output and make sure they are OK.</p><h4 id="2-Try-random-input"><a href="#2-Try-random-input" class="headerlink" title="2. Try random input"></a>2. Try random input</h4><p>Try passing random numbers instead of actual data and see if the error behaves the same way. If it does, it’s a sure sign that your net is turning data into garbage at some point. Try debugging layer by layer /op by op/ and see where things go wrong.</p><h4 id="3-Check-the-data-loader"><a href="#3-Check-the-data-loader" class="headerlink" title="3. Check the data loader"></a>3. Check the data loader</h4><p>Your data might be fine but the code that passes the input to the net might be broken. Print the input of the first layer before any operations and check it.</p><h4 id="4-Make-sure-input-is-connected-to-output"><a href="#4-Make-sure-input-is-connected-to-output" class="headerlink" title="4. Make sure input is connected to output"></a>4. Make sure input is connected to output</h4><p>Check if a few input samples have the correct labels. Also make sure shuffling input samples works the same way for output labels.</p><h4 id="5-Is-the-relationship-between-input-and-output-too-random"><a href="#5-Is-the-relationship-between-input-and-output-too-random" class="headerlink" title="5. Is the relationship between input and output too random?"></a>5. Is the relationship between input and output too random?</h4><p>Maybe the non-random part of the relationship between the input and output is too small compared to the random part (one could argue that stock prices are like this). I.e. the input are not sufficiently related to the output. There isn’t an universal way to detect this as it depends on the nature of the data.</p><h4 id="6-Is-there-too-much-noise-in-the-dataset"><a href="#6-Is-there-too-much-noise-in-the-dataset" class="headerlink" title="6. Is there too much noise in the dataset?"></a>6. Is there too much noise in the dataset?</h4><p>This happened to me once when I scraped an image dataset off a food site. There were so many bad labels that the network couldn’t learn. Check a bunch of input samples manually and see if labels seem off.</p><p>The cutoff point is up for debate, as <a href="https://arxiv.org/pdf/1412.6596.pdf" target="_blank" rel="external">this paper</a> got above 50% accuracy on MNIST using 50% corrupted labels.</p><h4 id="7-Shuffle-the-dataset"><a href="#7-Shuffle-the-dataset" class="headerlink" title="7. Shuffle the dataset"></a>7. Shuffle the dataset</h4><p>If your dataset hasn’t been shuffled and has a particular order to it (ordered by label) this could negatively impact the learning. Shuffle your dataset to avoid this. Make sure you are shuffling input and labels together.</p><h4 id="8-Reduce-class-imbalance"><a href="#8-Reduce-class-imbalance" class="headerlink" title="8. Reduce class imbalance"></a>8. Reduce class imbalance</h4><p>Are there a 1000 class A images for every class B image? Then you might need to balance your loss function or <a href="http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/" target="_blank" rel="external">try other class imbalance approaches</a>.</p><h4 id="9-Do-you-have-enough-training-examples"><a href="#9-Do-you-have-enough-training-examples" class="headerlink" title="9. Do you have enough training examples?"></a>9. Do you have enough training examples?</h4><p>If you are training a net from scratch (i.e. not finetuning), you probably need lots of data. For image classification, <a href="https://stats.stackexchange.com/a/226693/30773" target="_blank" rel="external">people say</a> you need a 1000 images per class or more.</p><h4 id="10-Make-sure-your-batches-don’t-contain-a-single-label"><a href="#10-Make-sure-your-batches-don’t-contain-a-single-label" class="headerlink" title="10. Make sure your batches don’t contain a single label"></a>10. Make sure your batches don’t contain a single label</h4><p>This can happen in a sorted dataset (i.e. the first 10k samples contain the same class). Easily fixable by shuffling the dataset.</p><h4 id="11-Reduce-batch-size"><a href="#11-Reduce-batch-size" class="headerlink" title="11. Reduce batch size"></a>11. Reduce batch size</h4><p><a href="https://arxiv.org/abs/1609.04836" target="_blank" rel="external">This paper</a> points out that having a very large batch can reduce the generalization ability of the model.</p><h4 id="Addition-1-Use-standard-dataset-e-g-mnist-cifar10"><a href="#Addition-1-Use-standard-dataset-e-g-mnist-cifar10" class="headerlink" title="Addition 1. Use standard dataset (e.g. mnist, cifar10)"></a>Addition 1. Use standard dataset (e.g. mnist, cifar10)</h4><p>Thanks to @<a href="https://medium.com/@hengcherkeng" target="_blank" rel="external">hengcherkeng</a> for this one:</p><blockquote><p>When testing new network architecture or writing a new piece of code, use the standard datasets first, instead of your own data. This is because there are many reference results for these datasets and they are proved to be ‘solvable’. There will be no issues of label noise, train/test distribution difference , too much difficulty in dataset, etc.</p></blockquote><hr><h3 id="II-Data-Normalization-Augmentation"><a href="#II-Data-Normalization-Augmentation" class="headerlink" title="II. Data Normalization/Augmentation"></a>II. Data Normalization/Augmentation</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*UQLMfdKi5D4nNDN6Oxa5MA.png" alt="img"></p><h4 id="12-Standardize-the-features"><a href="#12-Standardize-the-features" class="headerlink" title="12. Standardize the features"></a><strong>12. Standardize</strong> the features</h4><p>Did you standardize your input to have zero mean and unit variance?</p><h4 id="13-Do-you-have-too-much-data-augmentation"><a href="#13-Do-you-have-too-much-data-augmentation" class="headerlink" title="13. Do you have too much data augmentation?"></a>13. Do you have too much data augmentation?</h4><p>Augmentation has a regularizing effect. Too much of this combined with other forms of regularization (weight L2, dropout, etc.) can cause the net to underfit.</p><h4 id="14-Check-the-preprocessing-of-your-pretrained-model"><a href="#14-Check-the-preprocessing-of-your-pretrained-model" class="headerlink" title="14. Check the preprocessing of your pretrained model"></a>14. Check the preprocessing of your pretrained model</h4><p>If you are using a pretrained model, make sure you are using the same normalization and preprocessing as the model was when training. For example, should an image pixel be in the range [0, 1], [-1, 1] or [0, 255]?</p><h4 id="15-Check-the-preprocessing-for-train-validation-test-set"><a href="#15-Check-the-preprocessing-for-train-validation-test-set" class="headerlink" title="15. Check the preprocessing for train/validation/test set"></a>15. Check the preprocessing for train/validation/test set</h4><p>CS231n points out a <a href="http://cs231n.github.io/neural-networks-2/#datapre" target="_blank" rel="external">common pitfall</a>:</p><blockquote><p>“… any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation/test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. “</p></blockquote><p>Also, check for different preprocessing in each sample or batch.</p><hr><h3 id="III-Implementation-issues"><a href="#III-Implementation-issues" class="headerlink" title="III. Implementation issues"></a>III. Implementation issues</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*EVy3hNSF4Nq7v7bNYOyNcQ.png" alt="img">Credit: <a href="https://xkcd.com/1838/" target="_blank" rel="external">https://xkcd.com/1838/</a></p><h4 id="16-Try-solving-a-simpler-version-of-the-problem"><a href="#16-Try-solving-a-simpler-version-of-the-problem" class="headerlink" title="16. Try solving a simpler version of the problem"></a>16. Try solving a simpler version of the problem</h4><p>This will help with finding where the issue is. For example, if the target output is an object class and coordinates, try limiting the prediction to object class only.</p><h4 id="17-Look-for-correct-loss-“at-chance”"><a href="#17-Look-for-correct-loss-“at-chance”" class="headerlink" title="17. Look for correct loss “at chance”"></a>17. Look for correct loss “at chance”</h4><p>Again from the excellent <a href="http://cs231n.github.io/neural-networks-3/#sanitycheck" target="_blank" rel="external">CS231n</a>: <em>Initialize with small parameters, without regularization. For example, if we have 10 classes, at chance means we will get the correct class 10% of the time, and the Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302.</em></p><p>After this, try increasing the regularization strength which should increase the loss.</p><h4 id="18-Check-your-loss-function"><a href="#18-Check-your-loss-function" class="headerlink" title="18. Check your loss function"></a>18. Check your loss function</h4><p>If you implemented your own loss function, check it for bugs and add unit tests. Often, my loss would be slightly incorrect and hurt the performance of the network in a subtle way.</p><h4 id="19-Verify-loss-input"><a href="#19-Verify-loss-input" class="headerlink" title="19. Verify loss input"></a>19. Verify loss input</h4><p>If you are using a loss function provided by your framework, make sure you are passing to it what it expects. For example, in PyTorch I would mix up the NLLLoss and CrossEntropyLoss as the former requires a softmax input and the latter doesn’t.</p><h4 id="20-Adjust-loss-weights"><a href="#20-Adjust-loss-weights" class="headerlink" title="20. Adjust loss weights"></a>20. Adjust loss weights</h4><p>If your loss is composed of several smaller loss functions, make sure their magnitude relative to each is correct. This might involve testing different combinations of loss weights.</p><h4 id="21-Monitor-other-metrics"><a href="#21-Monitor-other-metrics" class="headerlink" title="21. Monitor other metrics"></a>21. Monitor other metrics</h4><p>Sometimes the loss is not the best predictor of whether your network is training properly. If you can, use other metrics like accuracy.</p><h4 id="22-Test-any-custom-layers"><a href="#22-Test-any-custom-layers" class="headerlink" title="22. Test any custom layers"></a>22. Test any custom layers</h4><p>Did you implement any of the layers in the network yourself? Check and double-check to make sure they are working as intended.</p><h4 id="23-Check-for-“frozen”-layers-or-variables"><a href="#23-Check-for-“frozen”-layers-or-variables" class="headerlink" title="23. Check for “frozen” layers or variables"></a>23. Check for “frozen” layers or variables</h4><p>Check if you unintentionally disabled gradient updates for some layers/variables that should be learnable.</p><h4 id="24-Increase-network-size"><a href="#24-Increase-network-size" class="headerlink" title="24. Increase network size"></a>24. Increase network size</h4><p>Maybe the expressive power of your network is not enough to capture the target function. Try adding more layers or more hidden units in fully connected layers.</p><h4 id="25-Check-for-hidden-dimension-errors"><a href="#25-Check-for-hidden-dimension-errors" class="headerlink" title="25. Check for hidden dimension errors"></a>25. Check for hidden dimension errors</h4><p>If your input looks like (k, H, W) = (64, 64, 64) it’s easy to miss errors related to wrong dimensions. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network.</p><h4 id="26-Explore-Gradient-checking"><a href="#26-Explore-Gradient-checking" class="headerlink" title="26. Explore Gradient checking"></a>26. Explore Gradient checking</h4><p>If you implemented Gradient Descent by hand, gradient checking makes sure that your backpropagation works like it should. More info: <a href="http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/" target="_blank" rel="external">1</a> <a href="http://cs231n.github.io/neural-networks-3/#gradcheck" target="_blank" rel="external">2</a> <a href="https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking" target="_blank" rel="external">3</a>.</p><hr><h3 id="IV-Training-issues"><a href="#IV-Training-issues" class="headerlink" title="IV. Training issues"></a>IV. Training issues</h3><p><img src="https://cdn-images-1.medium.com/max/800/1*gfcJD0eymh5SGuquzuvpig.png" alt="img">Credit: <a href="http://carlvondrick.com/ihog/" target="_blank" rel="external">http://carlvondrick.com/ihog/</a></p><h4 id="27-Solve-for-a-really-small-dataset"><a href="#27-Solve-for-a-really-small-dataset" class="headerlink" title="27. Solve for a really small dataset"></a>27. Solve for a really small dataset</h4><p><strong>Overfit a small subset of the data and make sure it works. </strong>For example, train with just 1 or 2 examples and see if your network can learn to differentiate these. Move on to more samples per class.</p><h4 id="28-Check-weights-initialization"><a href="#28-Check-weights-initialization" class="headerlink" title="28. Check weights initialization"></a>28. Check weights initialization</h4><p>If unsure, use <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="external">Xavier</a> or <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf" target="_blank" rel="external">He</a> initialization. Also, your initialization might be leading you to a bad local minimum, so try a different initialization and see if it helps.</p><h4 id="29-Change-your-hyperparameters"><a href="#29-Change-your-hyperparameters" class="headerlink" title="29. Change your hyperparameters"></a>29. Change your hyperparameters</h4><p>Maybe you using a particularly bad set of hyperparameters. If feasible, try a <a href="http://scikit-learn.org/stable/modules/grid_search.html" target="_blank" rel="external">grid search</a>.</p><h4 id="30-Reduce-regularization"><a href="#30-Reduce-regularization" class="headerlink" title="30. Reduce regularization"></a>30. Reduce regularization</h4><p>Too much regularization can cause the network to underfit badly. Reduce regularization such as dropout, batch norm, weight/bias L2 regularization, etc. In the excellent “<a href="http://course.fast.ai/" target="_blank" rel="external">Practical Deep Learning for coders</a>” course, <a href="https://twitter.com/jeremyphoward" target="_blank" rel="external">Jeremy Howard</a> advises getting rid of underfitting first. This means you overfit the training data sufficiently, and only then addressing overfitting.</p><h4 id="31-Give-it-time"><a href="#31-Give-it-time" class="headerlink" title="31. Give it time"></a>31. Give it time</h4><p>Maybe your network needs more time to train before it starts making meaningful predictions. If your loss is steadily decreasing, let it train some more.</p><h4 id="32-Switch-from-Train-to-Test-mode"><a href="#32-Switch-from-Train-to-Test-mode" class="headerlink" title="32. Switch from Train to Test mode"></a>32. Switch from Train to Test mode</h4><p>Some frameworks have layers like Batch Norm, Dropout, and other layers behave differently during training and testing. Switching to the appropriate mode might help your network to predict properly.</p><h4 id="33-Visualize-the-training"><a href="#33-Visualize-the-training" class="headerlink" title="33. Visualize the training"></a>33. Visualize the training</h4><ul><li>Monitor the activations, weights, and updates of each layer. Make sure their magnitudes match. For example, the magnitude of the updates to the parameters (weights and biases) <a href="https://cs231n.github.io/neural-networks-3/#summary" target="_blank" rel="external">should be 1-e3</a>.</li><li>Consider a visualization library like <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard" target="_blank" rel="external">Tensorboard</a> and <a href="https://github.com/torrvision/crayon" target="_blank" rel="external">Crayon</a>. In a pinch, you can also print weights/biases/activations.</li><li>Be on the lookout for layer activations with a mean much larger than 0. Try Batch Norm or ELUs.</li><li><a href="https://deeplearning4j.org/visualization#usingui" target="_blank" rel="external">Deeplearning4j</a> points out what to expect in histograms of weights and biases:</li></ul><blockquote><p>“For weights, these histograms should have an <strong>approximately Gaussian (normal) </strong>distribution, after some time. For biases, these histograms will generally start at 0, and will usually end up being <strong>approximately Gaussian</strong>(One exception to this is for LSTM). Keep an eye out for parameters that are diverging to +/- infinity. Keep an eye out for biases that become very large. This can sometimes occur in the output layer for classification if the distribution of classes is very imbalanced.”</p></blockquote><ul><li>Check layer updates, they should have a Gaussian distribution.</li></ul><h4 id="34-Try-a-different-optimizer"><a href="#34-Try-a-different-optimizer" class="headerlink" title="34. Try a different optimizer"></a>34. Try a different optimizer</h4><p>Your choice of optimizer shouldn’t prevent your network from training unless you have selected particularly bad hyperparameters. However, the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time. The paper which describes the algorithm you are using should specify the optimizer. If not, I tend to use Adam or plain SGD with momentum.</p><p>Check this <a href="http://ruder.io/optimizing-gradient-descent/" target="_blank" rel="external">excellent post</a> by Sebastian Ruder to learn more about gradient descent optimizers.</p><h4 id="35-Exploding-Vanishing-gradients"><a href="#35-Exploding-Vanishing-gradients" class="headerlink" title="35. Exploding / Vanishing gradients"></a>35. Exploding / Vanishing gradients</h4><ul><li>Check layer updates, as very large values can indicate exploding gradients. Gradient clipping may help.</li><li>Check layer activations. From <a href="https://deeplearning4j.org/visualization#usingui" target="_blank" rel="external">Deeplearning4j</a> comes a great guideline: <em>“A good standard deviation for the activations is on the order of 0.5 to 2.0. Significantly outside of this range may indicate vanishing or exploding activations.”</em></li></ul><h4 id="36-Increase-Decrease-Learning-Rate"><a href="#36-Increase-Decrease-Learning-Rate" class="headerlink" title="36. Increase/Decrease Learning Rate"></a>36. Increase/Decrease Learning Rate</h4><p>A low learning rate will cause your model to converge very slowly.</p><p>A high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution.</p><p>Play around with your current learning rate by multiplying it by 0.1 or 10.</p><h4 id="37-Overcoming-NaNs"><a href="#37-Overcoming-NaNs" class="headerlink" title="37. Overcoming NaNs"></a>37. Overcoming NaNs</h4><p>Getting a NaN (Non-a-Number) is a much bigger issue when training RNNs (from what I hear). Some approaches to fix it:</p><ul><li>Decrease the learning rate, especially if you are getting NaNs in the first 100 iterations.</li><li>NaNs can arise from division by zero or natural log of zero or negative number.</li><li>Russell Stewart has great pointers on <a href="http://russellsstewart.com/notes/0.html" target="_blank" rel="external">how to deal with NaNs</a>.</li><li>Try evaluating your network layer by layer and see where the NaNs appear.</li></ul><hr><h4 id="Resources"><a href="#Resources" class="headerlink" title="Resources:"></a><strong>Resources:</strong></h4><blockquote><p><a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/</a><br><a href="http://russellsstewart.com/notes/0.html" target="_blank" rel="external">http://russellsstewart.com/notes/0.html</a><br><a href="https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class" target="_blank" rel="external">https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class</a><br><a href="https://deeplearning4j.org/visualization" target="_blank" rel="external">https://deeplearning4j.org/visualization</a><br><a href="https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/" target="_blank" rel="external">https://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/</a><br><a href="https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phase" target="_blank" rel="external">https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phase</a><br><a href="http://book.caltech.edu/bookforum/showthread.php?t=4113" target="_blank" rel="external">http://book.caltech.edu/bookforum/showthread.php?t=4113</a><br><a href="https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134" target="_blank" rel="external">https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134</a><br><a href="https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithm" target="_blank" rel="external">https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithm</a></p></blockquote><p><strong>Origin post is <a href="*https://medium.com/@slavivanov/4020854bd607*">here</a>.</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch Tutorial (fork from official website)</title>
    <link href="http://yoursite.com/2017/07/24/PyTorch-Totorial-fork-from-official-website/"/>
    <id>http://yoursite.com/2017/07/24/PyTorch-Totorial-fork-from-official-website/</id>
    <published>2017-07-24T13:41:03.000Z</published>
    <updated>2017-07-25T02:09:05.948Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/tensor_tutorial.ipynb" target="_blank" rel="external">Tensor tutorial</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/autograd_tutorial.ipynb" target="_blank" rel="external">Autograd_tutorial</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/neural_networks_tutorial.ipynb" target="_blank" rel="external">Neural_networks_tutorial</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/cifar10_tutorial.ipynb" target="_blank" rel="external">CIFAR10_tutorial</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/torch-tutorial/tensor_tutorial.ipynb&quot; target=&quot;_blank&quot; rel=&quot;extern
    
    </summary>
    
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
  </entry>
  
</feed>
