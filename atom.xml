<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Abracadabra</title>
  <subtitle>Do it yourself</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-04-14T06:12:23.300Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Ewan Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>策略梯度方法</title>
    <link href="http://yoursite.com/2019/04/14/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2019/04/14/策略梯度方法/</id>
    <published>2019-04-14T05:59:17.000Z</published>
    <updated>2019-04-14T06:12:23.300Z</updated>
    
    <content type="html"><![CDATA[<hr><p><strong>译者注：</strong></p><p><strong>本篇文章翻译自 <a href="https://lilianweng.github.io/lil-log/contact.html" target="_blank" rel="external">Dr. Weng</a> 的<a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#sac-with-automatically-adjusted-temperature" target="_blank" rel="external">博客</a>。</strong></p><p><strong>翻译行为已得到原作者授权，转载请注明原作者。</strong></p><hr><blockquote><p>摘要：在本文中，我们将深入探讨策略梯度算法的工作原理以及近年来提出的一些新的策略梯度算法：平凡策略梯度、演员评论家算法、离线策略演员评论家算法、A3C、A2C、DPG、DDPG、D4PG、MADDPG、TRPO、PPO、ACER、ACKTR、SAC以及TD3算法。</p></blockquote><h2 id="什么是策略梯度"><a href="#什么是策略梯度" class="headerlink" title="什么是策略梯度"></a>什么是策略梯度</h2><p>策略梯度算法是一类解决强化学习问题的方法。如果你对于强化学习领域还不熟悉，请首先阅读<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts" target="_blank" rel="external">这篇博客</a>来对强化学习的问题定义以及核心概念进行初步了解。</p><h3 id="符号"><a href="#符号" class="headerlink" title="符号"></a>符号</h3><p>下面是一个符号列表，可以帮助您更轻松地理解本文中的公式。</p><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>$s \in \mathcal{S}$</td><td>状态。</td></tr><tr><td>$a \in \mathcal{A}$</td><td>动作。</td></tr><tr><td>$r \in \mathcal{R}$</td><td>回报。</td></tr><tr><td>$S_{t}, A_{t}, R_{t}$</td><td>一个轨迹中第$t$个时间步对应的状态、动作以及回报。我可能会偶尔使用$s_{t}, a_{t}, r_{t}$来代替。</td></tr><tr><td>$\gamma$</td><td>折扣因子；用于惩罚未来回报中的不确定性；$0&lt;\gamma \leq 1$。</td></tr><tr><td>$G_{t}$</td><td>累积回报；或者说累积折扣回报；$G_{t}=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}$。</td></tr><tr><td>$P\left(s^{\prime}, r\vert s, a\right)$</td><td>在当前状态$s$下采取动作$a$后转移到下一个状态$s^{\prime}$并得到回报$r$的概率。</td></tr><tr><td>$\pi(a\vert s)$</td><td>随机策略（智能体行为逻辑）；$\pi_{\theta}( .)$代表由$\theta$参数化的策略。</td></tr><tr><td>$\mu(s)$</td><td>确定性策略；虽然也可以把确定性策略记为$\pi(s)$，但是采用一个不同的字母可以让我们更容易分辨一个策略到底是确定性的还是随机的。$\pi$或者$\mu$都是强化学习算法要学习的目标。</td></tr><tr><td>$V(s)$</td><td>状态-值函数衡量状态$s$的期望累积回报；$V_{w}( .)$代表由$w$参数化的状态-值函数。</td></tr><tr><td>$V^{\pi}(s)$</td><td>当智能体遵循策略$\pi$时状态$s$的期望累积回报；$V^{\pi}(s)=\mathbb{E}_{a \sim \pi}\left[G_{t}\vert S_{t}=s\right]$。</td></tr><tr><td>$Q(s, a)$</td><td>动作-值函数，与状态-值函数类似，但是它衡量在状态$s$下采取动作$a$后的期望累积回报；$Q_{w}( .)$代表由$w$参数化的动作-值函数。</td></tr><tr><td>$Q^{\pi}(s, a)$</td><td>与$V^{\pi}(s)$类似，当智能体遵循策略$\pi$时，在状态$s$下采取动作$a$后的期望累积回报；$Q^{\pi}(s, a)=\mathbb{E}_{a \sim \pi}\left[G_{t}\vert S_{t}=s,A_{t}=a\right]$。</td></tr><tr><td>$A(s, a)$</td><td>优势函数，$A(s, a)=Q(s, a)-V(s)$；可以认为优势函数是加强版本的动作-值函数，但是由于它采用状态-值函数作为基准使得它具有更小的方差。</td></tr></tbody></table><h3 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h3><p>强化学习的目标是为智能体找到一个最优的行为策略从而获取最大的回报。<strong>策略梯度</strong>方法主要特点在于直接对策略进行建模并优化。策略通常被建模为由$\theta$参数化的函数$\pi_{\theta}(a | s)$。回报（目标）函数的值受到该策略的直接影响，因而可以采用很多算法来对$\theta$进行优化来最大化回报（目标）函数。</p><p>回报（目标）函数定义如下：<br>$$<br>J(\theta)=\sum_{s \in \mathcal{S}} d^{\pi}(s) V^{\pi}(s)=\sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(a | s) Q^{\pi}(s, a)<br>$$<br>其中$d^{\pi}(s)$代表由$\pi_{\theta}$引出的马尔科夫链的平稳分布（$\pi$下的在线策略状态分布）。</p><hr><p><strong>译者注</strong>：上述目标函数是在连续环境（没有固定的终止状态）下的目标函数（被称为<strong>平均值</strong>），在连续环境下还有一种性质更好的目标函数，叫做<strong>平均回报</strong>：<br>$$<br>\begin{aligned} J(\theta) \doteq r(\pi) &amp; \doteq \lim _{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &amp;=\lim _{t \rightarrow \infty} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &amp;=\sum_{s} \mu(s) \sum_{a} \pi_{\theta}(a | s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right) r \end{aligned}<br>$$<br>在这种定义下，<strong>（差分）累积回报定义为回报与平均回报的差值的累加值</strong>：<br>$$<br>G_{t} \doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\cdots<br>$$<br>对应的还有差分状态-值函数以及差分-动作值函数：<br>$$<br>\begin{array}{l}<br>{V_{\pi}(s)=\sum_{a} \pi_{\theta}(a | s) \sum_{r, s^{\prime}} p\left(s^{\prime}, r | s, a\right)\left[r-r(\pi)+V_{\pi}\left(s^{\prime}\right)\right]} \\ {Q_{\pi}(s, a)=\sum_{r, s^{\prime}} p\left(s^{\prime}, r | s, a\right)\left[r-r(\pi)+\sum_{a^{\prime}} \pi_{\theta}\left(a^{\prime} | s^{\prime}\right) Q_{\pi}\left(s^{\prime}, a^{\prime}\right)\right]}\end{array}<br>$$<br>之所以说上述目标函数性质更好，是因为平均值目标函数其实只是它的另外一种形式，下面我们就来证明一下（<a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf" target="_blank" rel="external">Sutton&amp;Barto,2017</a>; Sec 10.4）：<br>$$<br>\begin{aligned}<br>J(\theta)=&amp;\sum_{s \in \mathcal{S}} d^{\pi}(s) V^{\pi}(s) \\<br>=&amp; \sum_{s} d_{\pi}(s) \sum_{a} \pi_{\theta}(a | s) \sum_{s^{\prime}} \sum_{r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma V_{\pi}\left(s^{\prime}\right)\right] \\<br>=&amp;\;r(\pi) + \sum_{s} d_{\pi}(s) \sum_{a} \pi_{\theta}(a | s) \sum_{s^{\prime}} \sum_{r} p\left(s^{\prime}, r | s, a\right)\gamma V_{\pi}\left(s^{\prime}\right) \\<br>=&amp;\;r(\pi) + \gamma \sum_{s^{\prime}} V_{\pi}\left(s^{\prime}\right) \sum_{s} d_{\pi}(s) \sum_{a} \pi_{\theta}(a | s) p\left(s^{\prime} | s, a\right) \\<br>=&amp;\;r(\pi) + \gamma\sum_{s^{\prime}}V_{\pi}(s^{\prime})d_{\pi}(s^{\prime}) \\<br>=&amp;\;r(\pi) + \gamma J(\theta) \\<br>=&amp;\;r(\pi) + \gamma r(\pi) + \gamma^{2}J(\theta) \\<br>=&amp;\;r(\pi) + \gamma r(\pi) + \gamma^{2}r(\pi) + \gamma^{3}J(\theta) + \cdots \\<br>=&amp;\;\frac{1}{1-\gamma}r(\pi)<br>\end{aligned}<br>$$<br><strong>因而在下面进行策略梯度定理证明时，我们仅考虑平均回报形式的目标函数。</strong></p><p>另一方面，在周期环境下的目标函数（<strong>不包含折扣因子</strong>）如下：<br>$$<br>J(\theta)=V^{\pi}(s_{0})<br>$$<br>之所以不在连续环境下也使用上述目标函数，是因为在连续环境下上述目标函数的值为无穷大，优化一个无穷大值是没有意义的。</p><hr><p>为了简化符号，当策略作为其他函数的上标或者下标出现时$\pi_{\theta}$中的参数$\theta$将省略，例如：$d^{\pi}$以及$Q^{\pi}$的完整形式为$d^{\pi_{\theta}}$以及$Q^{\pi_{\theta}}$。想象一下你一直在马尔科夫链构成的状态序列中游荡，随着时间不断向前推进，你经过某个状态的概率将保持不变——这个概率就是$\pi_{\theta}$下的平稳概率。$d^{\pi}(s)=\lim _{t \rightarrow \infty} P\left(s_{t}=s | s_{0}, \pi_{\theta}\right)$就表示你从状态$s_0$开始，在策略$\pi_{\theta}$下经过$t$个时间步后到达状态$s$的概率。实际上，PageRank正是利用了马尔科夫链的平稳分布。你可以阅读<a href="https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/" target="_blank" rel="external">这篇文章</a>以获取更多细节。</p><p>我们可以很自然的预见到基于策略的方法能够很好地处理连续空间中的强化学习问题。因为在连续空间中存在无限多的动作及（或）状态因而基于值函数的算法由于需要去估计所有的动作及（或）状态的值导致其所需的算力变得无法接受。例如，在<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy-iteration" target="_blank" rel="external">一般的策略迭代</a>过程中，策略提升步骤$\arg \max _{a \in \mathcal{A}} Q^{\pi}(s, a)$需要去遍历整个动作空间，因而会遭受<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="external">维度诅咒</a>。</p><p>使用<em>梯度上升</em>方法，我们可以将参数$\theta$往梯度$\nabla_{\theta} J(\theta)$给出的方向进行改变从而去找到最优的$\theta$使得其对应的策略$\pi_{\theta}$能够给智能体带来最大的期望累积回报。</p><h3 id="策略梯度定理"><a href="#策略梯度定理" class="headerlink" title="策略梯度定理"></a>策略梯度定理</h3><p>计算梯度$\nabla_{\theta} J(\theta)$可不是一件简单的事情。因为梯度值不仅依赖于动作的选择（由$\pi_{\theta}$直接决定），还依赖于由选择的动作而产生的状态的平稳分布（由$\pi_{\theta}$间接决定）。鉴于环境通常是未知的，很难去估计策略的更新对于状态分布造成的影响。</p><p>哇哦！这时候出现了策略梯度定理来拯救世界了！该定理对梯度的形式进行了变形使其不依赖于状态分布$d^{\pi}( .)$的导数，从而在很大程度上简化了梯度$\nabla_{\theta} J(\theta)$的计算：<br>$$<br>\begin{aligned}<br>\nabla_{\theta} J(\theta) &amp;=\nabla_{\theta} \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \pi_{\theta}(a | s) \\<br>&amp;\propto \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \nabla_{\theta} \pi_{\theta}(a \vert s) \end{aligned}<br>$$</p><hr><p><strong>译者注：</strong>这里策略梯度定理的形式不够完全，只考虑了目标函数为连续环境下的平均值形式，上述定理还同时适用于连续环境下的平均回报形式目标函数以及周期环境下的目标函数，即：<br>$$<br>\begin{aligned}<br>\nabla_{\theta} J(\theta) &amp;\stackrel{\text{def}}{=} \nabla_{\theta} \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \pi_{\theta}(a | s) &amp; \scriptstyle{\text{连续环境下的平均值形式目标函数}} \\<br>&amp;\stackrel{\text{def}}{=} \nabla_{\theta} \sum_{s\in\mathcal{S}} \mu(s) \sum_{a\in\mathcal{A}} \pi_{\theta}(a | s) \sum_{s^{\prime}\in\mathcal{S}, r} p\left(s^{\prime}, r | s, a\right) r &amp; \scriptstyle{\text{连续环境下的平均回报形式目标函数}}\\<br>&amp;\stackrel{\text{def}}{=} \nabla_{\theta} V^{\pi}(s_{0}) &amp; \scriptstyle{\text{周期环境下的目标函数}}\\<br>&amp; \propto \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \nabla_{\theta} \pi_{\theta}(a | s) &amp; \scriptstyle{}<br>\end{aligned}<br>$$</p><hr><h3 id="策略梯度定理的证明"><a href="#策略梯度定理的证明" class="headerlink" title="策略梯度定理的证明"></a>策略梯度定理的证明</h3><p>现在我们要深入上述定理的证明（<a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf" target="_blank" rel="external">Sutton&amp;Barto,2017</a>; Sec 13.1（<strong>译者注：这里应该更改为Sec 13.2</strong>））从而理解为什么该定理的正确的，因而这部分会包含很多的数学公式。</p><p>我们首先从计算状态-值函数的梯度开始：<br>$$<br>\begin{aligned}<br>&amp; \nabla_\theta V^\pi(s) \\<br>=&amp; \nabla_\theta \Big(\sum_{a \in \mathcal{A}} \pi_\theta(a \vert s)Q^\pi(s, a) \Big) &amp; \\<br>=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta Q^\pi(s, a)} \Big) &amp; \scriptstyle{\text{; 微分乘法法则}} \\<br>=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta \sum_{s’, r} P(s’,r \vert s,a)(r + V^\pi(s’))} \Big) &amp; \scriptstyle{\text{; 扩展} Q^\pi} \\<br>=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s’, r} P(s’,r \vert s,a) \nabla_\theta V^\pi(s’)} \Big) &amp; \scriptstyle{; P(s’,r \vert s,a) \text{或者} r \text{不是}\theta \text{的函数}}\\<br>=&amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s’} P(s’ \vert s,a) \nabla_\theta V^\pi(s’)} \Big) &amp; \scriptstyle{\text{; 因为 } P(s’ \vert s, a) = \sum_r P(s’, r \vert s, a)}<br>\end{aligned}<br>$$</p><p>现在我们有：<br>$$<br>{\color{red}{\nabla_\theta V^\pi(s)}}<br>= \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \sum_{s’} P(s’ \vert s,a) \color{red}{\nabla_\theta V^\pi(s’)} \Big)<br>$$<br>上述等式拥有很好的递归特性（红色部分）因而未来状态的状态-值函数$V^{\pi}\left(s^{\prime}\right)$可以根据上式来递归地展开。让我们考虑一个下面这样的状态访问序列并将从状态$s$开始在策略$\pi_{\theta}$下经过$k$个时间步到达状态$x$的概率记为$\rho^{\pi}(s \rightarrow x, k)$：<br>$$<br>s \xrightarrow[]{a \sim \pi_\theta(.\vert s)} s’ \xrightarrow[]{a \sim \pi_\theta(.\vert s’)} s’’ \xrightarrow[]{a \sim \pi_\theta(.\vert s’’)} \dots<br>$$</p><p>对于不同的$k$值，$\rho^{\pi}(s \rightarrow x, k) $值包含以下几种情况：</p><ul><li>当$k=0$时：$\rho^{\pi}(s \rightarrow s, k=0)=1$。</li><li>当$k=1$时，我们遍历在状态$s$下所有可能的动作$a$然后将所有从元组$(s,a)$转移到目标状态的概率累加：$\rho^{\pi}\left(s \rightarrow s^{\prime}, k=1\right)=\sum_{a} \pi_{\theta}(a | s) P\left(s^{\prime} | s, a\right)$。</li><li>设想以下我们的目标是从状态$s$开始依照策略$\pi_{\theta} $经过$k+1$个时间步最终达到目标状态$x$。为了实现这个目标，我们可以先从状态$s$开始经过$k$个时间步后达到某个中间状态$s^{\prime} $（任何一个状态$s\in\mathcal{S}$均可成为中间状态）然后经过最后一个时间步到达目标状态$x$。这样的话，我们就可以递归地计算访问概率：$\rho^{\pi}(s \rightarrow x, k+1)=\sum_{s^{\prime}} \rho^{\pi}\left(s \rightarrow s^{\prime}, k\right) \rho^{\pi}\left(s^{\prime} \rightarrow x, 1\right)$。</li></ul><p>有了以上的相关基础，我们就可以递归地展开$\nabla_\theta V^\pi(s)$！首先为了简化符号我们进行以下符号上的替换：$\phi(s)=\sum_{a \in \mathcal{A}} \nabla_{\theta} \pi_{\theta}(a | s) Q^{\pi}(s, a)$。如果我们不停地展开$\nabla_{\theta} V^{\pi}(\cdot)$，那么可以发现通过这个展开过程我们可以从状态$s$开始经过任意时间步后到达任意的状态，并且将上述过程中的访问概率累加起来就可以得到$\nabla_\theta V^\pi(s)$！<br>$$<br>\begin{aligned}<br>&amp; \color{red}{\nabla_\theta V^\pi(s)} \\<br>=&amp; \phi(s) + \sum_a \pi_\theta(a \vert s) \sum_{s’} P(s’ \vert s,a) \color{red}{\nabla_\theta V^\pi(s’)} \\<br>=&amp; \phi(s) + \sum_{s’} \sum_a \pi_\theta(a \vert s) P(s’ \vert s,a) \color{red}{\nabla_\theta V^\pi(s’)} \\<br>=&amp; \phi(s) + \sum_{s’} \rho^\pi(s \to s’, 1) \color{red}{\nabla_\theta V^\pi(s’)} \\<br>=&amp; \phi(s) + \sum_{s’} \rho^\pi(s \to s’, 1) \color{red}{[ \phi(s’) + \sum_{s’’} \rho^\pi(s’ \to s’’, 1) \nabla_\theta V^\pi(s’’)]} \\<br>=&amp; \phi(s) + \sum_{s’} \rho^\pi(s \to s’, 1) \phi(s’) + \sum_{s’’} \rho^\pi(s \to s’’, 2){\color{red}{\nabla_\theta V^\pi(s’’)}} \scriptstyle{\text{ ; 考虑将 }s’\text{ 作为 }s \to s’’}\text{的中间状态}\\<br>=&amp; \phi(s) + \sum_{s’} \rho^\pi(s \to s’, 1) \phi(s’) + \sum_{s’’} \rho^\pi(s \to s’’, 2)\phi(s’’) + \sum_{s’’’} \rho^\pi(s \to s’’’, 3)\color{red}{\nabla_\theta V^\pi(s’’’)} \\<br>=&amp; \dots \scriptstyle{\text{; 递归展开 }\nabla_\theta V^\pi(.)} \\<br>=&amp; \sum_{x\in\mathcal{S}}\sum_{k=0}^\infty \rho^\pi(s \to x, k) \phi(x)<br>\end{aligned}上述<br>$$</p><p>上述变形使得我们无需计算Q-值函数的梯度$\nabla_\theta Q^\pi(s, a)$。将其带入目标函数$J(\theta)$中，可得：<br>$$<br>\begin{aligned}<br>\nabla_\theta J(\theta)<br>&amp;= \nabla_\theta V^\pi(s_0) &amp; \scriptstyle{\text{; 从一个随机状态 } s_0 \text{开始}} \\<br>&amp;= \sum_{s}\color{blue}{\sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)} \phi(s) &amp;\scriptstyle{\text{; 令 }\color{blue}{\eta(s) = \sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)}} \\<br>&amp;= \sum_{s}\eta(s) \phi(s) &amp; \\<br>&amp;= \Big( {\sum_s \eta(s)} \Big)\sum_{s}\frac{\eta(s)}{\sum_s \eta(s)} \phi(s) &amp; \scriptstyle{\text{; 正则化 } \eta(s), s\in\mathcal{S} \text{ 使其成为一个概率分布}}\\<br>&amp;\propto \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) &amp; \scriptstyle{\sum_s \eta(s)\text{ 是一个常数}} \\<br>&amp;= \sum_s d^\pi(s) \sum_a \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) &amp; \scriptstyle{d^\pi(s) = \frac{\eta(s)}{\sum_s \eta(s)}\text{ 即为平稳分布}}<br>\end{aligned}<br>$$</p><hr><p>译者注：上述证明过程仅仅涉及周期环境，连续环境下的证明如下：<br>$$<br>\begin{aligned}<br>&amp; \nabla_\theta V^\pi(s) \\<br>=&amp; \nabla_\theta \Big(\sum_{a \in \mathcal{A}} \pi_\theta(a \vert s)Q^\pi(s, a) \Big) &amp; \\<br>=&amp; \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s) {\color{red}{\nabla_\theta Q^\pi(s, a)}} \Big) &amp; \scriptstyle{\text{; 微分乘法法则}} \\<br>=&amp; \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s) {\color{red}{\nabla_\theta \sum_{s’, r} P(s’,r \vert s,a)(r - r(\pi) + V^\pi(s’))}}\Big) &amp; \scriptstyle{\text{; 扩展} Q^\pi} \\<br>=&amp; \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s){\color{red}{\bigg[{\color{blue}{-\nabla_{\theta}r(\pi)}}+\sum_{s’, r} P(s’,r \vert s,a) \nabla_\theta V^\pi(s’)} \bigg]}\Big) &amp; \scriptstyle{; P(s’,r \vert s,a) \text{或者} r \text{不是}\theta \text{的函数}}\\<br>=&amp; \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s) {\color{red}{\bigg[{\color{blue}{-\nabla_{\theta}r(\pi)}}+\sum_{s’} P(s’ \vert s,a) \nabla_\theta V^\pi(s’)} \bigg]}\Big) &amp; \scriptstyle{\text{; 因为 } P(s’ \vert s, a) = \sum_r P(s’, r \vert s, a)} \\<br>=&amp; {\color{blue}{-\nabla_{\theta}r(\pi)}} + \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s) {\color{red}{\sum_{s’} P(s’ \vert s,a) \nabla_\theta V^\pi(s’)}}\Big)<br>\end{aligned}<br>$$</p><p>经过移项可得：<br>$$<br>\color{blue} {\nabla_{\theta}r(\pi)} =\phi(s) + \sum_{a}\left[\pi_{\theta}(a | s) \color{red} {\sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)} \right]-\nabla_{\theta} V^{\pi}(s)<br>$$<br>那么平均回报形式的目标函数的梯度有：<br>$$<br>\begin{aligned}<br>\nabla_{\theta}J(\theta) =&amp;\;\nabla_{\theta} r(\pi) \\<br>=&amp; \sum_{s} d^{\pi}(s)\left(\phi(s) + \sum_{a}\left[\pi_{\theta}(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)\right]-\nabla_{\theta} V^{\pi}(s)\right) \\<br>=&amp; \sum_{s} d^{\pi}(s) \phi(s) +\sum_{s} d^{\pi}(s) \sum_{a} \pi_{\theta}(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)-\sum_{s} d^{\pi}(s) \nabla_{\theta} V^{\pi}(s) \\<br>=&amp; \sum_{s} d^{\pi}(s) \phi(s) + \sum_{s^{\prime}} \underbrace{\sum_{s} d^{\pi}(s) \sum_{a} \pi_{\theta}(a|s) p\left(s^{\prime}|s,a\right)}_{d^{\pi}\left(s^{\prime}\right)} \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)-\sum_{s} d^{\pi}(s) \nabla_{\theta} V^{\pi}(s) \\<br>=&amp; \sum_{s} d^{\pi}(s) \phi(s) + \sum_{s^{\prime}} d^{\pi}(s^{\prime}) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)-\sum_{s} d^{\pi}(s) \nabla_{\theta} V^{\pi}(s) \\<br>=&amp; \sum_{s} d^{\pi}(s) \phi(s) \\<br>=&amp; \sum_s d^\pi(s) \sum_a Q^\pi(s, a)\nabla_\theta\pi_\theta(a \vert s) \\<br>\propto&amp; \sum_s d^\pi(s) \sum_a Q^\pi(s, a)\nabla_\theta \pi_\theta(a \vert s)<br>\end{aligned}<br>$$<br>其中第二个等式成立是因为$\nabla_{\theta}r(\pi)$不依赖$s$且$\sum_{s}d^{\pi}(s)=1$。</p><p>另外，平均值形式的目标函数的梯度有：<br>$$<br>\begin{aligned}<br>\nabla_{\theta}J(\theta) =&amp;\;\nabla_{\theta}\sum_{s \in \mathcal{S}} d^{\pi}(s) V^{\pi}(s) \\<br>=&amp;\;\nabla_{\theta} \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \pi_{\theta}(a | s)\\<br>=&amp;\;\frac{1}{1-\gamma}\nabla_{\theta} r(\pi) \\<br>=&amp;\;\frac{1}{1-\gamma}\left(\sum_s d^\pi(s) \sum_a Q^\pi(s, a)\nabla_\theta\pi_\theta(a \vert s)\right) \\<br>\propto&amp; \sum_s d^\pi(s) \sum_a Q^\pi(s, a)\nabla_\theta \pi_\theta(a \vert s)<br>\end{aligned}<br>$$</p><hr><p>梯度还可以改写为如下形式：<br>$$<br>\begin{aligned}<br>\nabla_\theta J(\theta)<br>&amp;\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) &amp;\\<br>&amp;= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} &amp;\\<br>&amp;= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] &amp; \scriptstyle{\text{; 因为 } (\ln x)’ = 1/x}<br>\end{aligned}<br>$$<br>$\mathbb{E}_{\pi}$代表$\mathbb{E}_{s \sim d_{\pi}, a \sim \pi_{\theta}}$，下标表示遵循策略$\pi_{\theta}$（在线策略）时状态以及动作的分布。</p><p>上述策略梯度定理是许多策略梯度算法的理论基础。平凡策略梯度算法由于直接使用采样得到的回报所以其估计的梯度不存在偏差（bias）但是有较大的方差（variance）（见下式）。因此后续的算法被相继提出用以在保持偏差有界的情况下减小方差。<br>$$<br>\nabla_{\theta} J(\theta)=\mathbb{E}_{\pi}\left[Q^{\pi}(s, a) \nabla_{\theta} \ln \pi_{\theta}(a | s)\right]<br>$$<br>这里有一个从GAE（泛化优势估计，genaral advantage estimate）论文 （<a href="https://arxiv.org/abs/1506.02438" target="_blank" rel="external">Schulman et al., 2016</a>）中引用的很好的策略梯度算法一般形式的归纳，并且<a href="https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/" target="_blank" rel="external">这篇博客</a>深入探讨了GAE的几大组成部分，值得一读。</p><p><img src="https://upload-images.jianshu.io/upload_images/13653853-593dfedad43dfce3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图1. 策略梯度方法的一般形式。图片来源：[Schulman et al., 2016](https://arxiv.org/abs/1506.02438)"></p><a id="more"></a><h2 id="策略梯度算法"><a href="#策略梯度算法" class="headerlink" title="策略梯度算法"></a>策略梯度算法</h2><p>今年来强化学习领域内提出了大量的策略梯度算法，我不可能罗列出所有的算法。因此在这里我仅仅列举出一些我恰巧了解的算法。</p><h3 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h3><p>REINFORCE（蒙特卡洛策略梯度）依靠使用<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#monte-carlo-methods" target="_blank" rel="external">蒙特卡洛</a>方法从采样出的轨迹样本得到的估计的累积回报来更新策略的参数$\theta $。REINFORCE算法能够起效的原因是因为采样的梯度的期望值是真实梯度的无偏估计：<br>$$<br>\begin{aligned}<br>\nabla_\theta J(\theta)<br>&amp;= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] &amp; \\<br>&amp;= \mathbb{E}_\pi [G_t \nabla_{\theta} \ln \pi_\theta(A_t \vert S_t)] &amp; \scriptstyle{\text{; 因为 } Q^\pi(S_t, A_t) = \mathbb{E}_\pi[G_t \vert S_t, A_t]}<br>\end{aligned}<br>$$<br>因此我们能够从真实的采样轨迹中计算$G_t$并且使用它来去更新我们的策略梯度。它依赖于一条完整的轨迹，这也是为什么被叫做蒙特卡洛方法的原因。</p><p>整个算法流程十分直接：</p><ol><li>随机初始化策略参数$\theta $</li><li>使用当前策略$\pi_{\theta} $产生一条完整的轨迹：$S_{1}, A_{1}, R_{2}, S_{2}, A_{2}, \dots, S_{T}$</li><li>对于每个时间步$\mathrm{t}=1,2, \ldots, \mathrm{T}$：<ol><li>估计累积回报$G_t$</li><li>更新参数：$\theta \leftarrow \theta+\alpha \gamma^{t} G_{t} \nabla_{\theta} \ln \pi_{\theta}\left(A_{t} | S_{t}\right)$</li></ol></li></ol><p>一个被广泛应用的REINFORCE算法的变种是从$G_t$中减去一个基准值用来<em>在保证偏差不变的情况下减小估计梯度时产生的方差</em>（我们总是希望尽可能这样做）。举个例子，一个被广泛使用的基准值是状态-值，如果我们应用状态-值作为基准，那么我们实际在估计梯度进行梯度上升来更新参数的过程中使用的是优势值：$A(s, a)=Q(s, a)-V(s)$。<a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/" target="_blank" rel="external">这篇博客</a>不仅仅很好地解释了为什么采用基准值会减小方差，而且还详细讲解了策略梯度的一些基础知识。</p><h3 id="演员-评论家算法（Actor-Critic）"><a href="#演员-评论家算法（Actor-Critic）" class="headerlink" title="演员-评论家算法（Actor-Critic）"></a>演员-评论家算法（Actor-Critic）</h3><p>策略梯度算法主要包括策略模型以及值函数两个部分。在学习策略的基础上额外学习值函数是有很大意义的，因为值函数可以辅助策略更新，例如在平凡策略梯度算法中利用值函数来进行方差缩减，而这也是演员-评论家算法在做的事情。</p><p>演员-评论家模型由两个模型组成，可以选择是否共享参数：</p><ul><li><strong>评论家</strong>更新值函数的参数$w$并且根据算法的不同值函数可以为动作-值$Q_{w}(a | s)$或者状态-值$V_{w}(s)$</li><li><strong>演员</strong>根据评论家建议的方向更新策略$\pi_{\theta}(a | s)$参数$\theta $</li></ul><p>下面让我们看看一个简单的动作-值演员-评论家算法：</p><ol><li>随机初始化$s, w, \theta$；从初始策略中采样$a \sim \pi_{\theta}(a | s)$</li><li>对于每个时间步$t=1 \ldots T :$<ol><li>采样回报$r_{t} \sim R(s, a)$以及下一个状态$s^{\prime} \sim P\left(s^{\prime} | s, a\right)$</li><li>采样下一个动作$a^{\prime} \sim \pi_{\theta}\left(a^{\prime} | s^{\prime}\right)$</li><li>更新策略参数：$\theta \leftarrow \theta+\alpha_{\theta} Q_{w}(s, a) \nabla_{\theta} \ln \pi_{\theta}(a | s)$</li><li>对于当前时间步的动作-值计算校正值（TD误差）：$\delta_{t}=r_{t}+\gamma Q_{w}\left(s^{\prime}, a^{\prime}\right)-Q_{w}(s, a)$并且使用它来更新动作-值函数的参数：$w \leftarrow w+\alpha_{w} \delta_{t} \nabla_{w} Q_{w}(s, a)$</li><li>更新当前动作$a \leftarrow a^{\prime}$以及状态$s \leftarrow s^{\prime}$</li></ol></li></ol><p>$\alpha_w$和$\alpha_{\theta}$是两个预先定义的分别用来更新策略以及值函数的学习率。</p><h3 id="离线策略梯度（Off-Policy-Policy-Gradient）"><a href="#离线策略梯度（Off-Policy-Policy-Gradient）" class="headerlink" title="离线策略梯度（Off-Policy Policy Gradient）"></a>离线策略梯度（Off-Policy Policy Gradient）</h3><p>REINFORCE算法以及上述简单版本的演员-评论家算法都是在线（on-policy）的：训练样本是通过目标策略（target policy）——我们想要去优化的策略——收集的。然而离线方法拥有以下额外的优势：</p><ol><li>离线方法不需要完整的轨迹样本并且可以复用任何历史轨迹的样本（<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deep-q-network" target="_blank" rel="external">“经验回放”</a>）从而具有更好的样本有效性。</li><li>训练样本根据行为策略（behavior policy）而不是目标策略收集而来，给算法带来更好的<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#exploration-exploitation-dilemma" target="_blank" rel="external">探索性</a>。</li></ol><p>现在让我们来看看离线策略梯度是如何计算的。用来收集训练样本的行为策略是一个已知的策略（类似一个预先定义好的超参数），我们把它记作$\beta(a | s)$。那么目标函数（这里我们采用的是平均值形式的目标函数，其他形式的目标函数可以导出相同的结果）为由此行为策略导出的平稳状态分布下的回报的加和：</p><p>$$<br>J(\theta)=\sum_{s \in \mathcal{S}} d^{\beta}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \pi_{\theta}(a | s)=\mathbb{E}_{s \sim d^{\beta}}\left[\sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \pi_{\theta}(a | s)\right],<br>$$<br>其中$d^{\beta}(s)$为行为策略$\beta$导出的平稳分布，$d^{\beta}(s)=\lim _{t \rightarrow \infty} P\left(S_{t}=s | S_{0}, \beta\right)$。$Q^{\pi}$为根据目标策略$\pi$（不是行为策略！）估计的动作-值函数。</p><p>给定根据行为策略采样得到的动作$a \sim \beta(a | s)$产生的训练样本，我们可以将策略梯度改写为如下形式：<br>$$<br>\begin{aligned}<br>\nabla_{\theta} J(\theta) &amp;= \nabla_{\theta} \mathbb{E}_{s \sim d^{\beta}} \Big[ \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \pi_{\theta}(a \vert s) \Big] &amp; \\<br>&amp;= \mathbb{E}_{s \sim d^{\beta}} \Big[ \sum_{a \in \mathcal{A}} \big( Q^{\pi}(s, a) \nabla_{\theta} \pi_{\theta}(a \vert s) + \color{red}{\pi_{\theta}(a \vert s) \nabla_{\theta} Q^{\pi}(s, a)} \big) \Big] &amp; \scriptstyle{\text{; 微分乘法法则.}}\\<br>&amp;\stackrel{(i)}{\approx} \mathbb{E}_{s \sim d^{\beta}} \Big[ \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \nabla_{\theta} \pi_{\theta}(a \vert s) \Big] &amp; \scriptstyle{\text{; 忽略红色部分: } \color{red}{\pi_{\theta}(a \vert s) \nabla_{\theta} Q^{\pi}(s, a)}}. \\<br>&amp;= \mathbb{E}_{s \sim d^{\beta}} \Big[ \sum_{a \in \mathcal{A}} \beta (a \vert s) \frac{\pi_{\theta}(a \vert s)}{\beta (a \vert s)} Q^{\pi}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(a \vert s)}{\pi_{\theta}(a \vert s)} \Big] &amp; \\<br>&amp;= \mathbb{E}_\beta \Big[\frac{\color{blue}{\pi_{\theta}(a \vert s)}}{\color{blue}{\beta (a \vert s)}} Q^{\pi}(s, a) \nabla_{\theta} \ln \pi_{\theta}(a \vert s) \Big], &amp; \scriptstyle{\text{; 蓝色部分成为重要性权重.}}<br>\end{aligned}<br>$$<br>其中$\frac{\pi_{\theta}(a | s)}{\beta(a | s)}$为<a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/" target="_blank" rel="external">重要性权重(importance weight)</a>。由于$Q^{\pi}$是目标策略的函数因而也就是策略参数$\theta$的参数，因而根据微分乘法法则我们需要计算梯度$\nabla_{\theta} Q^{\pi}(s, a)$。然而在现实问题中计算$\nabla_{\theta} Q^{\pi}(s, a)$是一件超级困难的事情。幸运的是如果我们直接忽略掉$Q$的梯度而去使用一个近似的策略梯度的话，依旧能够保证采用梯度上升算法能够使得策略性能提升并且最终收敛到一个真实的局部最优解。上述结论的具体证明过程请参阅<a href="https://arxiv.org/pdf/1205.4839.pdf" target="_blank" rel="external">这篇论文</a>（Degris, White &amp; Sutton, 2012）。</p><p>总而言之，当我们想在离线环境下应用策略梯度时，只需通过加权求和的方式对其进行简单的修改，权重为目标策略与行为策略的比值。</p><h3 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h3><p>[<a href="https://arxiv.org/abs/1602.01783" target="_blank" rel="external">论文</a>|<a href="https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient/a3c" target="_blank" rel="external">代码</a>]</p><p>异步优势演员-评论家方法（<strong>Asynchronous Advantage Actor-Critic</strong>）（<a href="https://arxiv.org/abs/1602.01783" target="_blank" rel="external">Mnih et al., 2016</a>），简写为<strong>A3C</strong>时一种经典的策略梯度方法，尤其注重并行训练。</p><p>在A3C中，评论家学习值函数，同时有多个演员并行训练并且不时与全局参数同步。因而，A3C旨在用于并行训练。</p><p>让我们用状态-值函数进行举例。状态值的损失函数是最小化均方误差 $J_{v}(w)=\left(G_{t}-V_{w}(s)\right)^{2}$，因而采用梯度下降方法可以找到最优参数$w$。状态-值函数用来在策略梯度更新中作为基准值。</p><p>下面是算法大纲：</p><ol><li>定义全局参数 $\theta$ 和 $w$ 以及特定线程参数 $\theta^{\prime}$ 和 $w^{\prime}$。</li><li>初始化时间步 $t=1$。</li><li><p>当 $T&lt;=T_{\mathrm{max}}$：</p><ol><li><p>重置梯度：$\mathrm{d} \theta=0$ 并且 $\mathrm{d} \mathrm{w}=0$。</p></li><li><p>将特定于线程的参数与全局参数同步：$\theta^{\prime}=\theta$ 以及 $w^{\prime}=w$。</p></li><li><p>令 $t_{\text {start }}=t$ 并且随机采样一个初始状态 $s_t$。</p></li><li><p>当 （$s_{t} !=$ 终止状态）并且 $t-t_{\text {start}}&lt;=t_{\text {max}}$：</p><ol><li>根据当前线程的策略选择当前执行的动作 $a_{t} \sim \pi_{\theta^{\prime}}\left(a_{t} | s_{t}\right)$，执行动作后接收回报$r_t$然后转移到下一个状态$s_{t+1}$。</li><li>更新 $t$ 以及 $T$：$t=t+1$ 并且 $T=T+1$。</li></ol></li><li><p>初始化保存累积回报估计值的变量：$r=\left\{\begin{array}{ll}{0} &amp; {\text { 如果 } s_{t} \text { 是终止状态}} \\ {V_{w^{\prime}}\left(s_{t}\right)} &amp; {\text { 否则}}\end{array}\right.$</p></li><li><p>对于 $i=t-1, \dots, t_{\text {start}}$：</p><ol><li><p>$r \leftarrow \gamma r+r_{i}$；这里 $r$ 是 $G_i$ 的蒙特卡洛估计。</p></li><li><p>累积关于参数 $\theta^{\prime}$ 的梯度：$d \theta \leftarrow d \theta+\nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(a_{i} | s_{i}\right)\left(r-V_{w^{\prime}}\left(s_{i}\right)\right)$；</p><p>累积关于参数 $w^{\prime}$ 的梯度：$d w \leftarrow d w+2\left(r-V_{w^{\prime}}\left(s_{i}\right)\right) \nabla_{w^{\prime}}\left(r-V_{w^{\prime}}\left(s_{i}\right)\right)$。</p></li></ol></li><li><p>分别使用 $d\theta$ 以及 $dw$异步更新 $\theta$ 以及 $w$。</p></li></ol></li></ol><p>A3C支持多智能体（<strong>译者注：这里的多智能体与多智能体强化学习中的多智能体不是一个概念</strong>）并行训练。梯度累积步骤（6.2）可以认为是基于小批量样本的随机梯度下降在并行环境下的变形：$w$ 或者 $\theta$ 的值在每个训练线程得出的更新方向上独立地校正一点点。</p><h3 id="A2C"><a href="#A2C" class="headerlink" title="A2C"></a>A2C</h3><p>[<a href="https://arxiv.org/abs/1602.01783" target="_blank" rel="external">论文</a>|<a href="https://github.com/openai/baselines/blob/master/baselines/a2c/a2c.py" target="_blank" rel="external">代码</a>]</p><p><strong>A2C</strong>是A3C的同步、确定版本；这也是为什么要把A3C的第一个”A“（”异步“）去掉。在A3C中，每个演员独立地与（保存）全局参数（的服务器）进行交互，因此有时不同线程中的演员将使用不同版本的策略，因此累积更新的方向将不是最优的。为了解决上述执行策略不一致问题，A2C中引入协调器，在更新全局参数之前等待所有并行的演员完成其工作，那么在下一次迭代中并行的演员将均执行同一策略。同步的梯度更新使得训练过程更加耦合因而有可能使得算法具有更快得收敛速度。</p><p>A2C已被<a href="https://blog.openai.com/baselines-acktr-a2c/" target="_blank" rel="external">证明</a>在能够实现与A3C相同或更好的性能得同时，更有效地利用GPU，并且能够适应更大的批量大小。</p><p><img src="https://upload-images.jianshu.io/upload_images/13653853-936d61bf7c84165f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图2. A3C与A2C架构对比。"></p><h3 id="确定性策略梯度（DPG）"><a href="#确定性策略梯度（DPG）" class="headerlink" title="确定性策略梯度（DPG）"></a>确定性策略梯度（DPG）</h3><p>[<a href="https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf" target="_blank" rel="external">论文</a>|代码]</p><p>在上述方法中，策略函数 $\pi( . | s)$ 总是被建模为给定当前状态下在动作空间 $\mathcal{A}$ 上的概率分布，因而策略是随机的。相反，确定性策略梯度（<strong>Deterministic Policy Gradient</strong>）将环境建模为一个确定性决策：$a=\mu(s)$。这可能看上去很奇怪——你如何计算一个只能输出单个动作的策略函数的梯度呢？让我们一步步往下看。</p><p>回忆一些符号以便进行下面的讨论：</p><ul><li>$\rho_{0}(s)$：初始状态分布</li><li>$\rho^{\mu}\left(s \rightarrow s^{\prime}, k\right)$：从状态 $s$ 开始，遵循策略 $\mu$ 的情况下经过 $k$ 个时间步后到达状态 $s^{\prime}$ 的访问概率密度</li><li>$\rho^{\mu}\left(s^{\prime}\right)$：折扣状态分布，定义为 $\rho^{\mu}\left(s^{\prime}\right)=\int_{\mathcal{S}} \sum_{k=1}^{\infty} \gamma^{k-1} \rho_{0}(s) \rho^{\mu}\left(s \rightarrow s^{\prime}, k\right) d s$</li></ul><p>（平均值形式的）目标函数定义如下：<br>$$<br>J(\theta)=\int_{\mathcal{S}} \rho^{\mu}(s) Q\left(s, \mu_{\theta}(s)\right) ds<br>$$<br><strong>确定性策略梯度定理</strong>：现在该计算梯度了！根据链式法则，我们首先计算 $Q$ 相对于动作 $a$ 的梯度然后计算确定性策略函数 $\mu$ 相对于其参数 $\theta$ 的参数：<br>$$<br>\begin{aligned}<br>\nabla_{\theta} J(\theta) &amp;=\int_{\mathcal{S}} \rho^{\mu}(s) \nabla_{a} Q^{\mu}(s, a) \nabla_{\theta} \mu_{\theta}\left.(s)\right|_{a=\mu_{\theta}(s)} d s \\ &amp;=\mathbb{E}_{s \sim \rho^{\mu}}\left[\nabla_{a} Q^{\mu}(s, a) \nabla_{\theta} \mu_{\theta}\left.(s)\right|_{a=\mu_{\theta}(s)}\right]<br>\end{aligned}<br>$$<br>我们可以将确定性策略看成是随即策略的一个特例，前者的在整个动作空间的概率分布只在一个动作上是非零值。实际上在DPG<a href="https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf" target="_blank" rel="external">论文</a>中作者表明，如果随机策略 $\pi_{\mu_{\theta}, \sigma}$ 被确定性策略 $\mu_{\theta}$ 以及一个方差变量 $\sigma$ 重参数化，则当 $\sigma = 0$ 时随机策略最终将等价于确定策略。与确定性策略相比，我们可以认为随机策略需要更多样本，因为它整合了整个状态和动作空间的数据。</p><p>确定性梯度定理可以整合到通用的策略梯度算法框架中。</p><p>让我们考虑一个在线演员-评论家算法的例子来表明这个过程。在在线演员-评论家算法的每次迭代过程中，两个时间步的动作通过确定策略来选择 $a=\mu_{\theta}(s)$，然后 <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control" target="_blank" rel="external">SARSA</a> 算法通过上面计算得到的新形式的梯度来更新策略参数：<br>$$<br>\begin{aligned}<br>\delta_t &amp;= R_t + \gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_t, a_t) &amp; \scriptstyle{\text{; SARSA算法中的 TD 误差}}\\<br>w_{t+1} &amp;= w_t + \alpha_w \delta_t \nabla_w Q_w(s_t, a_t) &amp; \\<br>\theta_{t+1} &amp;= \theta_t + \alpha_\theta \color{red}{\nabla_a Q_w(s_t, a_t) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}} &amp; \scriptstyle{\text{; 确定性策略梯度定理}}<br>\end{aligned}<br>$$<br>然而，除非环境本身具有足够多的噪声，否则由于策略的确定性很难保证在训练的过程中有进行足够多的<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#exploration-exploitation-dilemma" target="_blank" rel="external">探索</a>。我们可以在确定策略中添加噪音（讽刺的是，这使得它将不具有确定性！）或者通过遵循不同的随机<strong>行为</strong>策略来收集样本来离线地学习<strong>目标</strong>确定策略。</p><p>比方说，在离线方法中，训练轨迹样本是通过一个随机行为策略 $\beta(a | s)$ 产生的因而状态分布服从对应的折扣状态密度 $\rho^{\beta}$：<br>$$<br>\begin{aligned}<br>J_\beta(\theta) &amp;= \int_\mathcal{S} \rho^\beta Q^\mu(s, \mu_\theta(s)) ds \\<br>\nabla_\theta J_\beta(\theta) &amp;= \mathbb{E}_{s \sim \rho^\beta} [\nabla_a Q^\mu(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)} ]<br>\end{aligned}<br>$$<br>注意由于策略是确定性的，我们只需要通过 $Q^{\mu}\left(s, \mu_{\theta}(s)\right)$ 而不是 $\sum_{a} \pi(a | s) Q^{\pi}(s, a)$ 来估计给定状态 $s$ 的累积回报（<strong>译者注：这也是为什么上述第二个等式期望的下标只有状态的期望</strong>）。就像我们在<a href="#离线策略梯度">之前</a>提到的，在采用随机目标策略的离线方法中，会引入重要性采样来校正行为策略与目标策略之间的不匹配现象。然而，由于确定性策略梯度中移除了在动作空间中的积分项，我们就可以避免使用重要性采样。</p><h3 id="深度确定性策略梯度-（DDPG）"><a href="#深度确定性策略梯度-（DDPG）" class="headerlink" title="深度确定性策略梯度 （DDPG）"></a>深度确定性策略梯度 （DDPG）</h3><p>[<a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="external">论文</a>|<a href="https://github.com/openai/baselines/tree/master/baselines/ddpg" target="_blank" rel="external">代码</a>]</p><p><strong>DDPG</strong>（<a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="external">Lillicrap, et al., 2015</a>）是深度确定性策略梯度（<strong>Deep Deterministic Policy Gradient</strong>）的缩写，是一个结合了<a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#dpg" target="_blank" rel="external">DPG</a>以及<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deep-q-network" target="_blank" rel="external">DQN</a>的无模型离线演员-评论家算法。回忆一下，DQN（深度Q网络）通过经验回访以及冻结目标网络的方式来稳定Q函数的训练过程。原始的DQN算法只能在离散的动作空间上使用，DDPG算法在学习一个确定性策略的同时通过演员-评论家框架将其扩展到连续的动作空间中。</p><p>为了获得更好的探索度，DDPG通过添加噪声 $\mathcal{N}$ 的方式构建了一个探索策略 $\mu^{\prime}$：<br>$$<br>\mu^{\prime}(s)=\mu_{\theta}(s)+\mathcal{N}<br>$$<br>另外，DDPG对演员以及评论家的目标网络的参数实行的是软更新（”保守策略迭代“），其中 $\tau \ll 1$：$\theta^{\prime} \leftarrow \tau \theta+(1-\tau) \theta^{\prime}$。采用这种方式，目标网络的值被限制为缓慢变化，不同于在DQN的设计中目标网络在一段时间内直接被冻结。</p><p>论文中关于机器人领域特别有用的一个细节是如何正则化低维特征的不同物理单位。例如，我们设计一个模型旨在学习以机器人的位置和速度为输入的策略；这些物理统计数据本质上是不同的，甚至相同类型的统计数据在多个机器人中也可能会有很大差异。论文通过应用<a href="http://proceedings.mlr.press/v37/ioffe15.pdf" target="_blank" rel="external">批正则化</a>通过对一个小批量中的样本的每个维度进行正则化来解决上述问题。</p><p><img src="https://upload-images.jianshu.io/upload_images/13653853-c2801505c52ccbf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图3. DDPG算法。图片来源：[Lillicrap, et al., 2015](https://arxiv.org/pdf/1509.02971.pdf)"></p><h3 id="D4PG"><a href="#D4PG" class="headerlink" title="D4PG"></a>D4PG</h3><p>[<a href="https://openreview.net/forum?id=SyZipzbCb" target="_blank" rel="external">论文</a>|代码（在谷歌中搜索”github d4pg“可以找到一些非官方开源实现）]</p><p>分布地分布式DDPG（<strong>Distributed Distributional DDPG</strong>，<strong>D4PG</strong>）在DDPG算法上进行了一系列的改进使得其可以分布式地运行。</p><p>（1）<strong>分布式评论家</strong>：分布式评论家不再只估计Q值的期望值，而是去估计期望Q值的分布，即将期望Q值作为一个随机变量来进行估计——该变量服从一个由 $w$ 参数化的分布 $Z_w$ 因而 $Q_{w}(s, a)=\mathbb{E} Z_{w}(x, a)$。学习该分布的参数所对应的损失函数是去最小化两个分布之间的某种距离度量——分布式TD误差：$L(w)=\mathbb{E}\left[d\left(\mathcal{T}_{\mu_{\theta}}, Z_{w^{\prime}}(s, a), Z_{w}(s, a)\right]\right.$，其中 $T_{\mu_{\theta}}$ 表示贝尔曼算子。</p><p>相应的，确定性策略梯度更新转变为以下形式：<br>$$<br>\begin{aligned}<br>\nabla_\theta J(\theta)<br>&amp;\approx \mathbb{E}_{\rho^\mu} [\nabla_a Q_w(s, a) \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}] &amp; \scriptstyle{\text{; DPG中的梯度更新}} \\<br>&amp;= \mathbb{E}_{\rho^\mu} [\mathbb{E}[\nabla_a Q_w(s, a)] \nabla_\theta \mu_\theta(s) \rvert_{a=\mu_\theta(s)}] &amp; \scriptstyle{\text{; Q值分布的期望}}<br>\end{aligned}<br>$$<br>（2）<strong>N步累积回报</strong>：当计算TD误差时，D4PG计算的是N步的TD目标值而不仅仅只有一步，这样就可以考虑未来更多步骤的回报。因而新的TD目标值变为：<br>$$<br>r\left(s_{0}, a_{0}\right)+\mathbb{E}\left[\sum_{n=1}^{N-1} r\left(s_{n}, a_{n}\right)+\gamma^{N} Q\left(s_{N}, \mu_{\theta}\left(s_{N}\right)\right) | s_{0}, a_{0}\right]<br>$$<br>（3）<strong>多个分布式并行演员</strong>：D4PG使用$K$个独立的演员并行收集训练样本并存储到同一个回访缓冲中。</p><p>（4）<strong>优先经验回放</strong>（<strong>Prioritized Experience Replay</strong>，<a href="https://arxiv.org/abs/1511.05952" target="_blank" rel="external"><strong>PER</strong></a>）：最后一个改进是使用一个非均匀的概率 $p_i$ 从一个大小为 $R$ 的回放缓冲中进行采样。在这种采样方式下，一个样本 $i$ 将以概率 $\left(R p_{i}\right)^{-1}$ 被采样到因而重要性权重为 $\left(R p_{i}\right)^{-1}$。</p><p><img src="https://upload-images.jianshu.io/upload_images/13653853-73d004524ff39cc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图4. D4PG算法。图片来源：[Barth-Maron, et al. 2018。注意在原始论文中变量符号的选择与本文有些微的区别；例如，我使用$\mu(.)$而不是$\pi(.)$来表示一个确定性策略](https://openreview.net/forum?id=SyZipzbCb)。"></p><h3 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a>MADDPG</h3><p>[<a href="https://arxiv.org/pdf/1706.02275.pdf" target="_blank" rel="external">论文</a>|<a href="https://github.com/openai/maddpg" target="_blank" rel="external">代码</a>]</p><p>多智能体DDPG（<strong>Multi-agent DDPG</strong>，<strong>MADDPG</strong>，<a href="https://arxiv.org/pdf/1706.02275.pdf" target="_blank" rel="external">Lowe et al., 2017</a>）将DDPG扩展到多智能体环境中，其中包含多个智能体在仅依靠局部信息的情况下协作完成任务。从单个智能体的角度来看，环境是非平稳的，因为其他智能体的策略在很快地更新并且一直是未知的。MADDPG是一个经过重新设计的演员评论家算法，专门用于处理这种不断变化的环境以及智能体之间的互动。</p><p>这个问题可以被建模为多智能体版本的MDP，也被称为<em>马尔科夫游戏</em>。其中，共有 $N$ 个智能体，公共的状态空间为 $\mathcal{S}$。每个智能体拥有自己的动作空间 $\mathcal{A}_{1}, \dots, \mathcal{A}_{N}$ 以及观察空间 $\mathcal{O}_{1}, \dots, \mathcal{O}_{N}$。状态转移函数包括所有的状态、动作以及观察空间$\mathcal{T} : \mathcal{S} \times \mathcal{A}_{1} \times \ldots \mathcal{A}_{N} \mapsto \mathcal{S}$。每个智能体自己的随机策略仅仅用到属于自己的观察以及动作：$\pi_{\theta_{i}} : \mathcal{O}_{i} \times \mathcal{A}_{i} \mapsto[0,1]$，一个给定其自身观察下关于动作的概率分布；或者一个确定性策略：$\mu_{\theta_{i}} : \mathcal{O}_{i} \mapsto \mathcal{A}_{i}$。</p><p>令 $\vec{o}=o_{1}, \ldots, o_{N}, \vec{\mu}=\mu_{1}, \ldots, \mu_{N}$ 并且策略是由 $\vec{\theta}=\theta_{1}, \dots, \theta_{N}$ 参数化的。</p><p>MADDPG中的评论家为第 $i$ 个智能体（每个智能体）学习一个中心化的动作-值函数 $Q_{i}^{\vec{\mu}}\left(\vec{o}, a_{1}, \ldots, a_{N}\right)$，其中 $a_{1} \in \mathcal{A}_{1}, \ldots, a_{N} \in \mathcal{A}_{N}$ 是所有智能体的动作。每一个 $Q_{i}^{\vec{\mu}},\;i=1, \dots, N$ 都是独立学习的，因而每个智能体可以拥有任意形式的回报函数，包括竞争环境中相互冲突的回报函数。同时，每个智能体各自的演员，也是独立探索以及独立更新策略参数 $\theta_{i}$。</p><p><strong>演员更新：</strong><br>$$<br>\nabla_{\theta_{i}} J\left(\theta_{i}\right)=\mathbb{E}_{\vec{o}, a \sim D}\left[\nabla_{a_{i}} Q_{i}^{\vec{\mu}}\left(\vec{o}, a_{1}, \ldots, a_{N}\right) \nabla_{\theta_{i}} \mu_{\theta_{i}}\left.\left(o_{i}\right)\right|_{a_{i}=\mu_{\theta_{i}}\left(o_{i}\right)}\right]<br>$$<br>其中 $\mathcal{D}$ 表示经验回放缓冲，包含大量轨迹样本 $\left(\vec{o}, a_{1}, \ldots, a_{N}, r_{1}, \ldots, r_{N}, \vec{o}^{\prime}\right)$ —— 给定当前联合观察 $\vec{o}$，每个智能体分别执行动作 $a_{1}, \dots, a_{N}$ 后获取各自的回报 $r_{1}, \dots, r_{N}$，并转移到下一个联合观察 $\vec{o}^{\prime}$。</p><p><strong>评论家更新：</strong><br>$$<br>\begin{aligned}<br>\mathcal{L}(\theta_i) &amp;= \mathbb{E}_{\vec{o}, a_1, \dots, a_N, r_1, \dots, r_N, \vec{o}’}[ (Q^{\vec{\mu}}_i(\vec{o}, a_1, \dots, a_N) - y)^2 ] &amp; \\<br>\text{其中 } y &amp;= r_i + \gamma Q^{\vec{\mu}’}_i (\vec{o}’, a’_1, \dots, a’_N) \rvert_{a’_j = \mu’_{\theta_j}} &amp; \scriptstyle{\text{; TD目标值!}}<br>\end{aligned}<br>$$<br>其中 $\vec{\mu}^{\prime}$ 是延迟软更新参数的目标策略。</p><p>如果在评论家更新的过程中策略 $\vec{\mu}$ 是未知的，我们可以让每个智能体学习并更新自己对其他智能体策略的近似。当使用近似的策略时，尽管推断出的策略可能不够精确但是MADDPG仍然能够有效地学习。</p><p>为了缓解环境中竞争或协作关系的智能体之间由于相互作用所带来的高方差，MADDPG提出了另外一个技术——策略集成：</p><ol><li>为单个智能体训练 $K$ 个策略；</li><li>随机选取一个策略用以轨迹采样；</li><li>使用 $K$ 个策略的集成梯度来进行参数更新。</li></ol><p>总之，MADDPG在DDPG之上添加了三个额外部分，使其适应多智能体环境：</p><ul><li>中心化评论家+去中心化演员；</li><li>智能体能够使用估计的其他智能体的策略来进行学习；</li><li>策略集成能够很好的减小方差。</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/13653853-6620a93556a2d0e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图5. MADDPG的算法框架. 图片来源：[Lowe et al., 2017](https://arxiv.org/pdf/1706.02275.pdf)"></p><h3 id="TRPO"><a href="#TRPO" class="headerlink" title="TRPO"></a>TRPO</h3><p>[<a href="https://arxiv.org/pdf/1502.05477.pdf" target="_blank" rel="external">论文</a>|<a href="https://github.com/openai/baselines/tree/master/baselines/trpo_mpi" target="_blank" rel="external">代码</a>]</p><p>为了提升训练的稳定性，我们应该避免更新一步就使得策略发生剧烈变化的参数更新。置信区间策略优化（<strong>Trust region policy optimization，TRPO</strong> ，<a href="https://arxiv.org/pdf/1502.05477.pdf" target="_blank" rel="external">Schulman, et al., 2015</a>）通过在每次迭代时对策略更新的幅度强制施加KL散度约束来实现上述理念。</p><p>对于离线情况，目标函数衡量的是在遵循一个不同的行为策略 $\beta(a | s)$ 进行采样的同时在状态访问分布以及动作空间上所有的优势：<br>$$<br>\begin{aligned}<br>J(\theta)<br>&amp;= \sum_{s \in \mathcal{S}} \rho^{\pi_{\theta_\text{old}}} \sum_{a \in \mathcal{A}} \big( \pi_\theta(a \vert s) \hat{A}_{\theta_\text{old}}(s, a) \big) &amp; \\<br>&amp;= \sum_{s \in \mathcal{S}} \rho^{\pi_{\theta_\text{old}}} \sum_{a \in \mathcal{A}} \big( \beta(a \vert s) \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} \hat{A}_{\theta_\text{old}}(s, a) \big) &amp; \scriptstyle{\text{; 重要性采样}} \\<br>&amp;= \mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}}, a \sim \beta} \big[ \frac{\pi_\theta(a \vert s)}{\beta(a \vert s)} \hat{A}_{\theta_\text{old}}(s, a) \big] &amp;<br>\end{aligned}<br>$$<br>其中 $\theta_{\mathrm{old}}$ 是更新之前的策略参数因而对于我们来说是已知的；$\rho^{\pi_{\mathrm{old}}}$ 和<a href="#确定性策略梯度（DPG）">之前</a>的定义相同；$\beta(a | s)$是用来采样轨迹数据的行为策略。注意这里我们使用的是一个估计的优势 $\hat{A}(\cdot)$ 而不是真实的优势 $A(\cdot)$，因为真实的回报往往是未知的。</p><p>对于在线情况，行为策略是 $\pi_{\theta_{\text {old}}}(a | s) :$<br>$$<br>J(\theta) = \mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}}, a \sim \pi_{\theta_\text{old}}} \big[ \frac{\pi_\theta(a \vert s)}{\pi_{\theta_\text{old}}(a \vert s)} \hat{A}_{\theta_\text{old}}(s, a) \big]<br>$$<br>TRPO算法旨在满足<em>置信区间约束</em>下最大化目标函数 $J(\theta)$。该约束强制旧策略与新策略之间的<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="external">KL散度</a>小于某个参数 $\delta$：<br>$$<br>\mathbb{E}_{s \sim \rho^{\pi_{\theta_\text{old}}}} [D_\text{KL}(\pi_{\theta_\text{old}}(.\vert s) | \pi_\theta(.\vert s)] \leq \delta<br>$$<br>通过这种方式，当满足这种硬约束时，旧策略和新策略不会差距太大。虽然如此，TRPO可以保证策略一直在往好的方向迭代（很厉害，对吧？）。如果大家感兴趣的话，可以去看看<a href="https://arxiv.org/pdf/1502.05477.pdf" target="_blank" rel="external">论文</a>中的证明:)</p><h3 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h3><p>[<a href="https://arxiv.org/pdf/1707.06347.pdf" target="_blank" rel="external">论文</a>|<a href="https://github.com/openai/baselines/tree/master/baselines/ppo1" target="_blank" rel="external">代码</a>]</p><p>鉴于TRPO相对复杂但是我们仍然想要去实现一个类似的约束，近端策略优化（<strong>proximal policy optimization ，PPO</strong>）在实现相似性能的同时通过使用一个截断的替代目标函数来简化TRPO。</p><p>首先，让我们将旧策略和新策略之间的概率比值表示为：<br>$$<br>r(\theta)=\frac{\pi_{\theta}(a | s)}{\pi_{\theta_{\text {old}}}(a | s)}<br>$$<br>接着，TRPO的目标函数（在线）变为：<br>$$<br>J^{\mathrm{TRPO}}(\theta)=\mathbb{E}\left[r(\theta) \hat{A}_{\theta_{\mathrm{old}}}(s, a)\right]<br>$$<br>如果在最大化 $J^{\mathrm{TRPO}}(\theta)$ 时不对 $\theta_{\mathrm{old}}$ 和 $\theta$ 之间的距离加以限制的话，将会因为过大的参数更新幅度以及过大的策略比值而使得更新过程不稳定。PPO通过强行使得 $r(\theta)$ 保持在 $1$ 附近的邻域中，即 $[1-\varepsilon, 1+\varepsilon]$，来施加这一约束。其中 $\varepsilon$ 为超参数。<br>$$<br>J^{\mathrm{CLIP}}(\theta)=\mathbb{E}\left[\min \left(r(\theta) \hat{A}_{\theta_{\mathrm{dd}}}(s, a), \operatorname{clip}(r(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_{\theta_{\mathrm{dd}}}(s, a)\right)\right]<br>$$<br>函数 $\operatorname{clip}(r(\theta), 1-\epsilon, 1+\epsilon)$ 将策略比值约束在 $[1-\varepsilon, 1+\varepsilon]$ 范围内。PPO的目标函数是去取原始值与截断版本的之间的较小值因此我们违背了TRPO最开始的一个理念，即尽可能最大化策略的更新幅度从而得到更好的回报。</p><p>当将PPO算法应用在策略（演员）和值函数（评论家）共享参数的网络结构上时，除了截断回报之外，目标函数上还加上了关于值估计的误差项（红色部分）以及一个熵正则项（蓝色部分）用以鼓励探索。<br>$$<br>J^\text{CLIP’} (\theta) = \mathbb{E} [ J^\text{CLIP} (\theta) - \color{red}{c_1 (V_\theta(s) - V_\text{target})^2} + \color{blue}{c_2 H(s, \pi_\theta(.))} ]<br>$$<br>其中 $c_1$ 和 $c_2$ 为两个常数超参数。</p><p>PPO已经在一系列基准任务上进行了测试并证明可以以更加简单的方式得到可喜的结果。</p><h3 id="ACER"><a href="#ACER" class="headerlink" title="ACER"></a>ACER</h3><p>[<a href="https://arxiv.org/pdf/1611.01224.pdf" target="_blank" rel="external">论文</a>|<a href="https://github.com/openai/baselines/tree/master/baselines/acer" target="_blank" rel="external">代码</a>]</p><p>ACER，带有经验回放的演员-评论家（<strong>Actor-Critic with Experience Replay</strong>，<a href="https://arxiv.org/pdf/1611.01224.pdf" target="_blank" rel="external">Wang, et al., 2017</a>）算法的缩写，是一个带有经验回放的离线演员-评论家算法，它极大地提升了采样有效性并且较低了训练数据间的关联性。A3C基于ACER构建，只不过它是在线方法；ACER是A3C的离线版本。使A3C成为离线方法的主要障碍是如何保证离线估计器的稳定性。ACER提出了三种改进来去克服这个障碍：</p><ul><li>使用 Retrace Q值估计</li><li>使用偏差校正截断重要性权重</li><li>使用更加高效的TRPO算法</li></ul><p><strong>Retrace Q值估计</strong></p><p><a href="http://papers.nips.cc/paper/6538-safe-and-efficient-off-policy-reinforcement-learning.pdf" target="_blank" rel="external">Retrace</a> 是一种离线的基于累积回报的Q值估计算法。它在任意的目标-策略网络对 $(\pi, \beta)$ 下都有一个比较好的收敛性保证并且拥有很好的数据有效性。</p><p>回忆一下TD学习是如何进行预测的：</p><ol><li>计算TD误差：$\delta_{t}=R_{t}+\gamma \mathbb{E}_{a \sim \pi} Q\left(S_{t+1}, a\right)-Q\left(S_{t}, A_{t}\right)$；其中 $r_{t}+\gamma \mathbb{E}_{a \sim \pi} Q\left(s_{t+1}, a\right)$ 被称为”TD目标“。使用期望值 $\mathbb{E}_{a \sim \pi}$ 是因为如果我们遵循当前策略 $\pi$ 的话对于未来时间步我们能做的最好的估计就是累积回报可能是多少。</li><li>通过校正误差往目标移动来更新Q值：$Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha \delta_{t}$。换句话说，Q的增量更新幅度与TD误差成正比：$\Delta Q\left(S_{t}, A_{t}\right)=\alpha \delta_{t}$。</li></ol><p>当进行离线采样时，我们需要在Q值更新过程中应用重要性采样：<br>$$<br>\Delta Q^{\mathrm{imp}}\left(S_{t}, A_{t}\right)=\gamma^{t} \prod_{1 \leq \tau \leq t} \frac{\pi\left(A_{\tau} | S_{\tau}\right)}{\beta\left(A_{\tau} | S_{\tau}\right)} \delta_{t}<br>$$<br>当我们想象一下重要性权重的连乘会带来多大的方差时就会感觉到这个连乘项有多么可怕了。Retrace Q值估计方法通过截断重要性权重使其不超过某个常数 $c$ 的方式对 $\Delta Q$ 进行修改：<br>$$<br>\Delta Q^{\mathrm{ret}}\left(S_{t}, A_{t}\right)=\gamma^{t} \prod_{1 \leq \tau \leq t} \min \left(c, \frac{\pi\left(A_{\tau} | S_{\tau}\right)}{\beta\left(A_{\tau} | S_{\tau}\right)}\right) \delta_{t}<br>$$<br>ACER使用 $Q^{\mathrm{ret}}$ 作为TD目标通过最小化 $L2$ 误差项来训练评论家：$\left(Q^{\mathrm{ret}}(s, a)-Q(s, a)\right)^{2}$。</p><p><strong>重要性权重截断</strong></p><p>为了减少估计策略梯度 $\hat{g}$ 时产生的高方差，ACER使用一个常数 $c$ 加上一个校正项来截断重要性权重。$\hat{g}_{t}^{\text { acer }}$ 代表 $t$ 时刻的ACER策略梯度。<br>$$<br>\begin{aligned}<br>\hat{g}_t^\text{acer}<br>= &amp; \omega_t \big( Q^\text{ret}(S_t, A_t) - V_{\theta_v}(S_t) \big) \nabla_\theta \ln \pi_\theta(A_t \vert S_t)<br>&amp; \scriptstyle{\text{; 令 }\omega_t=\frac{\pi(A_t \vert S_t)}{\beta(A_t \vert S_t)}} \\<br>= &amp; \color{blue}{\min(c, \omega_t) \big( Q^\text{ret}(S_t, A_t) - V_w(S_t) \big) \nabla_\theta \ln \pi_\theta(A_t \vert S_t)} \\<br>&amp; + \color{red}{\mathbb{E}_{a \sim \pi} \big[ \max(0, \frac{\omega_t(a) - c}{\omega_t(a)}) \big( Q_w(S_t, a) - V_w(S_t) \big) \nabla_\theta \ln \pi_\theta(a \vert S_t) \big]}<br>&amp; \scriptstyle{\text{; 令 }\omega_t (a) =\frac{\pi(a \vert S_t)}{\beta(a \vert S_t)}}<br>\end{aligned}<br>$$<br>其中 $Q_{w}( .)$ 以及 $V_{w}( .)$ 由 $w$ 参数化的评论家预测的动作-值以及状态-值。第一项（蓝色部分）包含了截断重要性权重。截断操作促进方差缩减，减去状态-值 $V_{w}( .)$ 作为基准进一步加强了方差缩减的效果。第二项（红色部分）进行了校正使得上述方差估计为无偏估计。</p><p><strong>高效TRPO</strong></p><p>此外，ACER采用TRPO的思想，但通过一个小的调整使其具有更高的计算效率：ACER不再去计算当前策略与更新一步之后的新策略之间的KL散度，而是去维护一个历史策略的运行平均（running average）值并且强制新策略不会偏离这个平均策略太远。</p><p>ACER论文信息量很大，包含很多公式。但是在事先了解了TD学习，Q学习，重要性采样和TRPO之后，你会发现这篇论文会稍微变得容易理解一些:)</p><h3 id="ACKTR"><a href="#ACKTR" class="headerlink" title="ACKTR"></a>ACKTR</h3><p>[<a href="https://arxiv.org/pdf/1708.05144.pdf" target="_blank" rel="external">论文</a>|<a href="https://github.com/openai/baselines/tree/master/baselines/acktr" target="_blank" rel="external">代码</a>]</p><p>Kronecker因子化置信区间的演员-评论家算法（<strong>Actor-Critic using Kronecker-factored Trust Region，ACKTR</strong>，<a href="https://arxiv.org/pdf/1708.05144.pdf" target="_blank" rel="external">Yuhuai Wu, et al., 2017</a>）使用Kronecker因子化曲率估计（<a href="https://arxiv.org/pdf/1503.05671.pdf" target="_blank" rel="external">K-FAC</a>）同时进行演员以及评论家的梯度更新。K-FAC对自然梯度的计算进行了改进，这与我们的标准梯度有很大不同。<a href="http://kvfrans.com/a-intuitive-explanation-of-natural-gradient-descent/" target="_blank" rel="external">这里</a>有一个对于自然梯度很好很直观的解释。</p><p>如果要用一句话总结的话：</p><blockquote><p>“我们首先考虑所有参数组合，这些参数组合导致新网络与旧网络保持恒定的KL差异。该常数值可以视为步长或学习速率。在所有这些可能的组合中，我们选择最小化损失函数的组合。“</p></blockquote><p>我在这里列出了ACTKR主要是为了这篇文章的完整性，但我不会深入到细节部分，因为它涉及很多关于自然梯度和优化方法的理论知识。如果有兴趣，请在阅读ACKTR论文之前查看这些文章/帖子：</p><ul><li>Amari. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Natural Gradient Works Efficiently in Learning</a>. 1998</li><li>Kakade. <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="external">A Natural Policy Gradient</a>. 2002</li><li><a href="http://kvfrans.com/a-intuitive-explanation-of-natural-gradient-descent/" target="_blank" rel="external">A intuitive explanation of natural gradient descent</a></li><li><a href="https://en.wikipedia.org/wiki/Kronecker_product" target="_blank" rel="external">Wiki: Kronecker product</a></li><li>Martens &amp; Grosse. <a href="http://proceedings.mlr.press/v37/martens15.pdf" target="_blank" rel="external">Optimizing neural networks with kronecker-factored approximate curvature.</a> 2015.</li></ul><p>以下是K-FAC论文的高度概括（译者注：以下为论文原文，因此不做翻译）：</p><blockquote><p>“This approximation is built in two stages. In the first, the rows and columns of the Fisher are divided into groups, each of which corresponds to all the weights in a given layer, and this gives rise to a block-partitioning of the matrix. These blocks are then approximated as Kronecker products between much smaller matrices, which we show is equivalent to making certain approximating assumptions regarding the statistics of the network’s gradients.</p><p>In the second stage, this matrix is further approximated as having an inverse which is either block-diagonal or block-tridiagonal. We justify this approximation through a careful examination of the relationships between inverse covariances, tree-structured graphical models, and linear regression. Notably, this justification doesn’t apply to the Fisher itself, and our experiments confirm that while the inverse Fisher does indeed possess this structure (approximately), the Fisher itself does not.”</p></blockquote><h3 id="SAC"><a href="#SAC" class="headerlink" title="SAC"></a>SAC</h3><p>[<a href="https://arxiv.org/abs/1801.01290" target="_blank" rel="external">论文</a>|<a href="https://github.com/haarnoja/sac" target="_blank" rel="external">代码</a>]</p><p>软演员-评论家算法（<strong>Soft Actor-Critic，SAC</strong>，<a href="https://arxiv.org/abs/1801.01290" target="_blank" rel="external">Haarnoja et al. 2018</a>）将策略的熵度量纳入回报函数中用以鼓励探索：我们希望学习到一种尽可能随机行动的策略，同时仍然能够在任务中完成目标。它是一个遵循最大熵强化学习框架的离线演员-评论家模型。一个先例工作是<a href="https://arxiv.org/abs/1702.08165" target="_blank" rel="external">软Q学习</a>。</p><p>SAC算法中的三个关键部分如下：</p><ul><li>包含分离策略网络以及值函数网络的<a href="#演员-评论家算法">演员-评论家</a>架构；</li><li>离线形式使其能够复用历史收集的数据从而实现高采样有效性；</li><li>熵最大化以使得训练稳定并鼓励探索。</li></ul><p>策略的训练目标是同时最大化期望累积回报以及策略的熵度量：<br>$$<br>J(\theta)=\sum_{t=1}^{T} \mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \rho_{\pi_{\theta}}}\left[r\left(s_{t}, a_{t}\right)+\alpha \mathcal{H}\left(\pi_{\theta}\left( . | s_{t}\right)\right)\right]<br>$$</p><p>其中 $\mathcal{H}( .)$ 表示熵度量，$\alpha$ 被称为<em>热度（temperature）</em>参数用以控制熵正则项的重要度。熵最大化使得策略再训练过程中可以（1）进行更多的探索更多和（2）捕获近似最优策略的多种模式（例如，如果存在似乎同样好的多种选项，则策略应该为每个选项分配以相同的概率被选中）。</p><p>准确地说，SAC旨在学习三个函数：</p><ul><li>由 $\theta$ 参数化的策略 $\pi_{\theta}$。</li><li>由 $w$ 参数化的软Q值函数 $Q_w$。</li><li>由 $\psi$ 参数化的软状态-值函数 $V_{\psi}$ ；理论上来说我们可以通过 $Q$ 以及 $\pi$ 来推导出 $V$，但是在实际情况下，显式对状态-值函数建模可以使得训练过程更加稳定。</li></ul><p>软Q值以及软状态值分别定义如下：<br>$$<br>\begin{aligned}<br>Q(s_t, a_t) &amp;= r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_{\pi}(s)} [V(s_{t+1})] &amp; \text{; 根据贝尔曼方程}\\<br>\text{where }V(s_t) &amp;= \mathbb{E}_{a_t \sim \pi} [Q(s_t, a_t) - \alpha \log \pi(a_t \vert s_t)] &amp; \text{; 软状态值函数}<br>\end{aligned}<br>$$</p><p>$$<br>\text{Thus, } Q(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{(s_{t+1}, a_{t+1}) \sim \rho_{\pi}} [Q(s_{t+1}, a_{t+1}) - \alpha \log \pi(a_{t+1} \vert s_{t+1})]<br>$$</p><p>$\rho_{\pi}(s)$ 和 $\rho_{\pi}(s, a)$ 分别表示由策略 $\pi(a\vert s)$ 导出的状态分布的状态以及状态-动作边际分布；<a href="#离线策略梯度（Off-Policy Policy Gradient）">DPG算法</a>部分有类似的定义。</p><p>软状态值函数通过最小化均方误差来训练：<br>$$<br>\begin{aligned}<br>J_V(\psi) &amp;= \mathbb{E}_{s_t \sim \mathcal{D}} [\frac{1}{2} \big(V_\psi(s_t) - \mathbb{E}[Q_w(s_t, a_t) - \log \pi_\theta(a_t \vert s_t)] \big)^2] \\<br>\text{其中梯度为: }\nabla_\psi J_V(\psi) &amp;= \nabla_\psi V_\psi(s_t)\big( V_\psi(s_t) - Q_w(s_t, a_t) + \log \pi_\theta (a_t \vert s_t) \big)<br>\end{aligned}<br>$$<br>其中 $\mathcal{D}$ 代表经验回放缓冲。</p><p>软Q值函数通过最小化软贝尔曼残差来训练：<br>$$<br>\begin{aligned}<br>J_Q(w) &amp;= \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}} [\frac{1}{2}\big( Q_w(s_t, a_t) - (r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim \rho_\pi(s)}[V_{\bar{\psi}}(s_{t+1})]) \big)^2] \\<br>\text{其中梯度为: } \nabla_w J_Q(w) &amp;= \nabla_w Q_w(s_t, a_t) \big( Q_w(s_t, a_t) - r(s_t, a_t) - \gamma V_{\bar{\psi}}(s_{t+1})\big)<br>\end{aligned}<br>$$<br>其中 $\bar{\psi}$ 代表目标值函数，它是个指数移动平均值（exponential moving average）或者只是采用一种“硬”方式进行周期更新。就像<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deep-q-network" target="_blank" rel="external">DQN</a>中目标Q网络中的参数一样，为了使得训练过程更加稳定。</p><p>SAC通过最小化如下<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="external">KL散度</a>来去更新策略：<br>$$<br>\begin{aligned}<br>\pi_\text{new}<br>&amp;= \arg\min_{\pi’ \in \Pi} D_\text{KL} \Big( \pi’(.\vert s_t) | \frac{\exp(Q^{\pi_\text{old}}(s_t, .))}{Z^{\pi_\text{old}}(s_t)} \Big) \[6pt]<br>&amp;= \arg\min_{\pi’ \in \Pi} D_\text{KL} \big( \pi’(.\vert s_t) | \exp(Q^{\pi_\text{old}}(s_t, .) - \log Z^{\pi_\text{old}}(s_t)) \big) \[6pt]<br>\text{目标函数: } J_\pi(\theta) &amp;= \nabla_\theta D_\text{KL} \big( \pi_\theta(. \vert s_t) | \exp(Q_w(s_t, .) - \log Z_w(s_t)) \big) \[6pt]<br>&amp;= \mathbb{E}_{a_t\sim\pi} \Big[ - \log \big( \frac{\exp(Q_w(s_t, a_t) - \log Z_w(s_t))}{\pi_\theta(a_t \vert s_t)} \big) \Big] \[6pt]<br>&amp;= \mathbb{E}_{a_t\sim\pi} [ \log \pi_\theta(a_t \vert s_t) - Q_w(s_t, a_t) + \log Z_w(s_t) ]<br>\end{aligned}<br>$$<br>其中 $\prod$ 是潜在策略的集合，我们可以将这些策略建模为容易处理的形式；例如，$\prod$ 可以是高斯混合分布族，虽然建模时复杂度较高但是具有很强的表达能力并且易于处理。$Z^{\pi_\text{old}}(s_t)$ 是用于正则化分布的配分函数。它一般是很难处理的但所幸对于梯度值没有影响。最小化 $J_{\pi}(\theta)$ 的方式依赖于 $\prod$ 的选择。</p><p>一旦我们为软动作-值，软状态值和策略网络定义了目标函数和梯度，软演员-评论家算法就很简单了：</p><p><img src="https://upload-images.jianshu.io/upload_images/13653853-88808043234e89e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图6. 软演员-评论家算法. 图片来源：[原始论文](https://arxiv.org/abs/1801.01290)"></p><h3 id="带有自动热度调整的软演员-评论家算法"><a href="#带有自动热度调整的软演员-评论家算法" class="headerlink" title="带有自动热度调整的软演员-评论家算法"></a>带有自动热度调整的软演员-评论家算法</h3><p>[<a href="https://arxiv.org/abs/1812.05905" target="_blank" rel="external">论文</a>|<a href="https://github.com/rail-berkeley/softlearning" target="_blank" rel="external">代码</a>]</p><p>SAC算法对于热度参数十分敏感。不幸的是，调整热度参数是一件很困难的事情。因为在随着策略由于训练变得更优的过程中或者在不同任务中进行训练时熵都会出现不可预测的变化。在SAC算法针对上述问题的改进可以建模为如下带约束的优化问题：在最大化期望累积回报的同时，策略应满足最小熵约束：<br>$$<br>\max _{\pi_{0}, \ldots, \pi_{T}} \mathbb{E}\left[\sum_{t=0}^{T} r\left(s_{t}, a_{t}\right)\right] \text { s.t. } \forall t, \mathcal{H}\left(\pi_{t}\right) \geq \mathcal{H}_{0}<br>$$<br>其中 $\mathcal{H}_{0}$ 表示预定义的最小策略熵阈值。</p><p>其中期望收益 $\mathbb{E}\left[\sum_{t=0}^{T} r\left(s_{t}, a_{t}\right)\right]$ 可以分解为每一时间步回报的和。因为在时刻 $t$ 的策略 $\pi_t$ 不会影响到之前时刻的策略 $\pi_{t-1}$，我们可以从后往前逐步最大化收益——这其实就是<strong>动态规划</strong>了。<br>$$<br>\underbrace{\max_{\pi_0} \Big( \mathbb{E}[r(s_0, a_0)]+ \underbrace{\max_{\pi_1} \Big(\mathbb{E}[…] + \underbrace{\max_{\pi_T} \mathbb{E}[r(s_T, a_T)]}_\text{第一步最大化} \Big)}_\text{第二步但也是最后一步最大化} \Big)}_\text{最后一步最大化}<br>$$<br>其中 $\gamma=1$。</p><p>我们从最后一个时间步 $T$ 开始最大化：<br>$$<br>\mathbb{E}_{\left(s_{T}, a_{T}\right) \sim \rho_{\pi}}\left[r\left(s_{T}, a_{T}\right)\right] \text { s.t. } \mathcal{H}\left(\pi_{T}\right)-\mathcal{H}_{0} \geq 0<br>$$<br>首先，我们定义以下一些函数：<br>$$<br>\begin{aligned}<br>h(\pi_T) &amp;= \mathcal{H}(\pi_T) - \mathcal{H}_0 = \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [-\log \pi_T(a_T\vert s_T)] - \mathcal{H}_0\\<br>f(\pi_T) &amp;= \begin{cases}<br>\mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) ], &amp; \text{if }h(\pi_T) \geq 0 \\<br>-\infty, &amp; \text{otherwise}<br>\end{cases}<br>\end{aligned}<br>$$<br>因而优化问题就转变为如下形式：<br>$$<br>f\left(\pi_{T}\right) \text { s.t. } h\left(\pi_{T}\right) \geq 0<br>$$<br>为了解决带有不等式约束的最大化优化问题，我们可以构建一个带有拉格朗日乘子（也被称为“对偶变量”） $\alpha_{T}$ 的<a href="https://cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf" target="_blank" rel="external">拉格朗日表达式</a>：<br>$$<br>L\left(\pi_{T}, \alpha_{T}\right)=f\left(\pi_{T}\right)+\alpha_{T} h\left(\pi_{T}\right)<br>$$<br>考虑如下情况，在给定策略 $\pi_{T}$ 的情况下，我们尝试去找到最小化 $L\left(\pi_{T}, \alpha_{T}\right)$ 的 $\alpha_{T}$ 值，</p><ul><li>如果约束被满足，即 $h\left(\pi_{T}\right) \geq 0$ ，那么因为我们无法通过 $\alpha_T$ 控制 $f\left(\pi_{T}\right)$ 的值因此最好将其设置为0（<strong>译者注：这里我有些无法理解，如果要最小化，那么当 $h\left(\pi_{T}\right) \geq 0$ 时 $\alpha_T$ 为负数应该会使得目标函数更小？</strong>）。因而，$L\left(\pi_{T}, 0\right)=f\left(\pi_{T}\right)$。</li><li>如果约束被违背了，即 $h\left(\pi_{T}\right)&lt;0$，我们可以通过令 $\alpha_{T} \rightarrow \infty$ 来使得 $L\left(\pi_{T}, \alpha_{T}\right) \rightarrow-\infty$。因而 $L\left(\pi_{T}, \infty\right)=-\infty=f\left(\pi_{T}\right)$。</li></ul><p>无论哪种情况，我们都可以得到如下等式：<br>$$<br>f\left(\pi_{T}\right)=\min _{\alpha_{T} \geq 0} L\left(\pi_{T}, \alpha_{T}\right)<br>$$<br>同时，我们想要去最大化$ f\left(\pi_{T}\right)$：<br>$$<br>\max _{\pi_{T}} f\left(\pi_{T}\right)=\min _{\alpha_{T} \geq 0} \max _{\pi_{T}} L\left(\pi_{T}, \alpha_{T}\right)<br>$$<br>因此，为了最大化 $f\left(\pi_{T}\right)$，其对应的对偶问题罗列如下。注意为了保证 $\max _{\pi_{T}} f\left(\pi_{T}\right)$ 是一个良定的操作（即 $f\left(\pi_{T}\right)$ 不为 $-\infty$），约束必须要被满足。<br>$$<br>\begin{aligned}<br>\max_{\pi_T} \mathbb{E}[ r(s_T, a_T) ]<br>&amp;= \max_{\pi_T} f(\pi_T) \\<br>&amp;= \min_{\alpha_T \geq 0} \max_{\pi_T} L(\pi_T, \alpha_T) \\<br>&amp;= \min_{\alpha_T \geq 0} \max_{\pi_T} f(\pi_T) + \alpha_T h(\pi_T) \\<br>&amp;= \min_{\alpha_T \geq 0} \max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) ] + \alpha_T ( \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [-\log \pi_T(a_T\vert s_T)] - \mathcal{H}_0) \\<br>&amp;= \min_{\alpha_T \geq 0} \max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) - \alpha_T \log \pi_T(a_T\vert s_T)] - \alpha_T \mathcal{H}_0 \\<br>&amp;= \min_{\alpha_T \geq 0} \max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) + \alpha_T \mathcal{H}(\pi_T) - \alpha_T \mathcal{H}_0 ]<br>\end{aligned}<br>$$<br>我们可以迭代地计算最优的 $\pi_T$ 以及 $\alpha_T$。首先给定目前的 $\alpha_{T}$，通过最大化 $L\left(\pi_{T}^{\star}, \alpha_{T}\right)$ 来得到最优的策略 $\pi_{T}^{\star}$。然后将 $\pi_{T}^{\star}$ 代入去最小化 $L\left(\pi_{T}^{\star}, \alpha_{T}\right)$ 来计算 $\alpha_{T}^{\star}$。想象一下我们用一个神经网络来表示策略，用另一个网络来表示热度参数，这个迭代过程就与训练演员-评论家算法时更新两者的网络参数比较类似了。<br>$$<br>\begin{aligned}<br>\pi^{\star}_T<br>&amp;= \arg\max_{\pi_T} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi}} [ r(s_T, a_T) + \alpha_T \mathcal{H}(\pi_T) - \alpha_T \mathcal{H}_0 ] \\<br>\color{blue}{\alpha^{\star}_T}<br>&amp;\color{blue}{=} \color{blue}{\arg\min_{\alpha_T \geq 0} \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi^{\star}}} [\alpha_T \mathcal{H}(\pi^{\star}_T) - \alpha_T \mathcal{H}_0 ]}<br>\end{aligned}<br>$$</p><p>$$<br>\text{因而, }\max_{\pi_T} \mathbb{E} [ r(s_T, a_T) ]<br>= \mathbb{E}_{(s_T, a_T) \sim \rho_{\pi^{\star}}} [ r(s_T, a_T) + \alpha^{\star}_T \mathcal{H}(\pi^{\star}_T) - \alpha^{\star}_T \mathcal{H}_0 ]<br>$$</p><p>现在，让我们回到软Q值函数：<br>$$<br>\begin{aligned}<br>Q_{T-1}(s_{T-1}, a_{T-1})<br>&amp;= r(s_{T-1}, a_{T-1}) + \mathbb{E} [Q(s_T, a_T) - \alpha_T \log \pi(a_T \vert s_T)] \\<br>&amp;= r(s_{T-1}, a_{T-1}) + \mathbb{E} [r(s_T, a_T)] + \alpha_T \mathcal{H}(\pi_T) \\<br>Q_{T-1}^{\star}(s_{T-1}, a_{T-1})<br>&amp;= r(s_{T-1}, a_{T-1}) + \max_{\pi_T} \mathbb{E} [r(s_T, a_T)] + \alpha_T^{\star} \mathcal{H}(\pi^{\star}_T) &amp; \text{; 代入最优策略 }\pi_T^{\star}<br>\end{aligned}<br>$$<br>因此，当我们进一步退回到 $T-1$ 时间步时，期望收益如下：<br>$$<br>\begin{aligned}<br>&amp;\max_{\pi_{T-1}}\Big(\mathbb{E}[r(s_{T-1}, a_{T-1})] + \max_{\pi_T} \mathbb{E}[r(s_T, a_T] \Big) \\<br>&amp;= \max_{\pi_{T-1}} \Big( Q^{\star}_{T-1}(s_{T-1}, a_{T-1}) - \alpha^{\star}_T \mathcal{H}(\pi^{\star}_T) \Big) &amp; \text{; 需 } \mathcal{H}(\pi_{T-1}) - \mathcal{H}_0 \geq 0 \\<br>&amp;= \min_{\alpha_{T-1} \geq 0} \max_{\pi_{T-1}} \Big( Q^{\star}_{T-1}(s_{T-1}, a_{T-1}) - \alpha^{\star}_T \mathcal{H}(\pi^{\star}_T) + \alpha_{T-1} \big( \mathcal{H}(\pi_{T-1}) - \mathcal{H}_0 \big) \Big) &amp; \text{; 对偶问题} \\<br>&amp;= \min_{\alpha_{T-1} \geq 0} \max_{\pi_{T-1}} \Big( Q^{\star}_{T-1}(s_{T-1}, a_{T-1}) + \alpha_{T-1} \mathcal{H}(\pi_{T-1}) - \alpha_{T-1}\mathcal{H}_0 \Big) - \alpha^{\star}_T \mathcal{H}(\pi^{\star}_T)<br>\end{aligned}<br>$$<br>与之前类似，我们有：<br>$$<br>\begin{aligned}<br>\pi^{\star}_{T-1} &amp;= \arg\max_{\pi_{T-1}} \mathbb{E}_{(s_{T-1}, a_{T-1}) \sim \rho_\pi} [Q^{\star}_{T-1}(s_{T-1}, a_{T-1}) + \alpha_{T-1} \mathcal{H}(\pi_{T-1}) - \alpha_{T-1} \mathcal{H}_0 ] \\<br>\color{green}{\alpha^{\star}_{T-1}} &amp;\color{green}{=} \color{green}{\arg\min_{\alpha_{T-1} \geq 0} \mathbb{E}_{(s_{T-1}, a_{T-1}) \sim \rho_{\pi^{\star}}} [ \alpha_{T-1} \mathcal{H}(\pi^{\star}_{T-1}) - \alpha_{T-1}\mathcal{H}_0 ]}<br>\end{aligned}<br>$$<br>绿色部分更新 $\alpha_{T-1}^{\star}$ 的公式与上面蓝色部分更新 $\alpha_{T}^{\star}$ 的公式具有相同的形式。通过不断重复上述过程，我们可以通过最小化相同的目标函数（译者注：这里的目标函数中的策略不是最优的）来学习每个时间步的最优热度参数：<br>$$<br>J(\alpha)=\mathbb{E}_{a_{t} \sim \pi_{t}}\left[-\alpha \log \pi_{t}\left(a_{t} | \pi_{t}\right)-\alpha \mathcal{H}_{0}\right]<br>$$<br>除了根据最小化 $J(\alpha)$ 来显式地学习 $\alpha$ 之外本部分算法与SAC算法没有任何区别（见图7）。</p><p><img src="https://upload-images.jianshu.io/upload_images/13653853-b6a8d9fdca4d146d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图7. 带有自动热度调整的软演员-评论家算法. 图片来源：[原始论文](https://arxiv.org/abs/1812.05905)"></p><h3 id="TD3"><a href="#TD3" class="headerlink" title="TD3"></a>TD3</h3><p>[<a href="https://arxiv.org/abs/1802.09477" target="_blank" rel="external">论文</a>|<a href="https://github.com/sfujim/TD3" target="_blank" rel="external">代码</a>]</p><p>众所周知Q学习一直存在对值函数过估计的问题。过估计会随着训练过程不断传播最终会对策略学习造成负面影响。这个问题促使<a href="https://papers.nips.cc/paper/3964-double-q-learning" target="_blank" rel="external">双Q学习</a>以及<a href="https://arxiv.org/abs/1509.06461" target="_blank" rel="external">双DQN</a>的提出：通过使用两个值网络将动作选择和Q值更新进行解耦。</p><p>双延迟深度确定性策略梯度方法（<strong>Twin Delayed Deep Deterministic，TD3</strong>; <a href="https://arxiv.org/abs/1802.09477" target="_blank" rel="external">Fujimoto et al., 2018</a>）在<a href="#深度确定性策略梯度 （DDPG）">DDPG</a>算法的基础上应用了很多新的改进从而防止值函数的过估计现象：</p><p>（1）<strong>截断双Q学习</strong>：在双Q学习中，动作选择以及Q值估计是通过两个独立的网络完成的。在DDPG中，给定两个确定性演员 $\left(\mu_{\theta_{1}}, \mu_{\theta_{2}}\right)$ 以及两个对应的评论家 $\left(Q_{w_{1}}, Q_{w_{2}}\right)$，双Q学习的贝尔曼目标如下：<br>$$<br>\begin{aligned}<br>y_1 &amp;= r + \gamma Q_{w_2}(s’, \mu_{\theta_1}(s’))\\<br>y_2 &amp;= r + \gamma Q_{w_1}(s’, \mu_{\theta_2}(s’))<br>\end{aligned}<br>$$<br>然而，由于策略变化过于缓慢，使得两个演员网络会过于相似从而很难做出完全独立的决策。<em>截断双Q学习</em>使用两者中的最小估计，从而倾向于使用难以通过训练传播的欠估计偏差：<br>$$<br>\begin{aligned}<br>y_1 &amp;= r + \gamma \min_{i=1,2}Q_{w_i}(s’, \mu_{\theta_1}(s’))\\<br>y_2 &amp;= r + \gamma \min_{i=1,2} Q_{w_i}(s’, \mu_{\theta_2}(s’))<br>\end{aligned}<br>$$<br>（2）<strong>延迟更新目标和策略网络</strong>：在<a href="#演员-评论家算法（Actor-Critic）">演员-评论家</a>模型中，策略与值函数的更新深度耦合：当策略较差时值函数的估计将会由于过估计问题发散；相反如果值函数估计不准确又会使得策略变差。</p><p>为了减小训练过程中的方差，TD3以一个相对于Q值函数更低的更新频率来更新策略。策略网络的参数将会保持不变直至值函数误差经过多轮迭代后足够小。这个想法类似于定期更新的目标网络如何在<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#dqn" target="_blank" rel="external">DQN</a>中作为稳定的目标存在。</p><p>（3）<strong>目标策略平滑：</strong>考虑到确定性策略会过拟合到值函数的峰值上，TD3在值函数上引入了平滑正则化策略。在所选动作中添加少量经过截断的随机噪声，并对小批量数据进行平均。<br>$$<br>\begin{aligned}<br>y &amp;= r + \gamma Q_w (s’, \mu_{\theta}(s’) + \epsilon) &amp; \\<br>\epsilon &amp;\sim \text{clip}(\mathcal{N}(0, \sigma), -c, +c) &amp; \scriptstyle{\text{ ; 截断的随机噪声}}<br>\end{aligned}<br>$$<br>这种做法模仿了<a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#sarsa-on-policy-td-control" target="_blank" rel="external">SARSA</a>参数更新的思想，并强制相似的动作应具有相似的动作-值。</p><p>下面是最终的算法框架：</p><p><img src="https://upload-images.jianshu.io/upload_images/13653853-bb3945372d7d7578.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="图8. TD3算法. 图片来源：[原始论文](https://arxiv.org/abs/1802.09477)"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>研究完上面的所有算法后，我列出了一些似乎在它们中很常见的基础构件或原则：</p><ul><li>尽量减少方差并保持偏差不变以稳定训练过程。</li><li>离线方法可以带来更高的探索度以及更高的数据有效性。</li><li>经验回放（训练数据从一个回放缓存中采样）。</li><li>目标网络要么周期更新要么比正在学习的网络更慢地更新。</li><li>批标准化。</li><li>带有熵正则的回报函数</li><li>演员和评论家可以共享网络的低层参数然后两个输出头分别为策略和值函数。</li><li>可以学习一个确定性的策略而不是一个随即策略。</li><li>在策略更新上施加距离约束。</li><li>新的优化方法（例如K-FAC）。</li><li>最大化策略的熵度量从而鼓励探索。</li><li>避免对值函数过估计。</li><li>等等</li></ul><p>如果你在这篇文章中发现了一些错误或描述不当的地方，不要犹豫，马上通过邮件<em>[lilian dot wengweng at gmail dot com]</em>联系我，我很乐意及时改正！</p><p>下篇文章再见:D</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] jeremykun.com <a href="https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/" target="_blank" rel="external">Markov Chain Monte Carlo Without all the Bullshit</a></p><p>[2] Richard S. Sutton and Andrew G. Barto. <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf" target="_blank" rel="external">Reinforcement Learning: An Introduction; 2nd Edition</a>. 2017.</p><p>[3] John Schulman, et al. <a href="https://arxiv.org/pdf/1506.02438.pdf" target="_blank" rel="external">“High-dimensional continuous control using generalized advantage estimation.”</a> ICLR 2016.</p><p>[4] Thomas Degris, Martha White, and Richard S. Sutton. <a href="https://arxiv.org/pdf/1205.4839.pdf" target="_blank" rel="external">“Off-policy actor-critic.”</a> ICML 2012.</p><p>[5] timvieira.github.io <a href="http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/" target="_blank" rel="external">Importance sampling</a></p><p>[6] Mnih, Volodymyr, et al. <a href="https://arxiv.org/abs/1602.01783" target="_blank" rel="external">“Asynchronous methods for deep reinforcement learning.”</a> ICML. 2016.</p><p>[7] David Silver, et al. <a href="https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf" target="_blank" rel="external">“Deterministic policy gradient algorithms.”</a> ICML. 2014.</p><p>[8] Timothy P. Lillicrap, et al. <a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="external">“Continuous control with deep reinforcement learning.”</a> arXiv preprint arXiv:1509.02971 (2015).</p><p>[9] Ryan Lowe, et al. <a href="https://arxiv.org/pdf/1706.02275.pdf" target="_blank" rel="external">“Multi-agent actor-critic for mixed cooperative-competitive environments.”</a>NIPS. 2017.</p><p>[10] John Schulman, et al. <a href="https://arxiv.org/pdf/1502.05477.pdf" target="_blank" rel="external">“Trust region policy optimization.”</a> ICML. 2015.</p><p>[11] Ziyu Wang, et al. <a href="https://arxiv.org/pdf/1611.01224.pdf" target="_blank" rel="external">“Sample efficient actor-critic with experience replay.”</a> ICLR 2017.</p><p>[12] Rémi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. <a href="http://papers.nips.cc/paper/6538-safe-and-efficient-off-policy-reinforcement-learning.pdf" target="_blank" rel="external">“Safe and efficient off-policy reinforcement learning”</a> NIPS. 2016.</p><p>[13] Yuhuai Wu, et al. <a href="https://arxiv.org/pdf/1708.05144.pdf" target="_blank" rel="external">“Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.”</a> NIPS. 2017.</p><p>[14] kvfrans.com <a href="http://kvfrans.com/a-intuitive-explanation-of-natural-gradient-descent/" target="_blank" rel="external">A intuitive explanation of natural gradient descent</a></p><p>[15] Sham Kakade. <a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="external">“A Natural Policy Gradient.”</a>. NIPS. 2002.</p><p>[16] <a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/" target="_blank" rel="external">“Going Deeper Into Reinforcement Learning: Fundamentals of Policy Gradients.”</a> - Seita’s Place, Mar 2017.</p><p>[17] <a href="https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/" target="_blank" rel="external">“Notes on the Generalized Advantage Estimation Paper.”</a> - Seita’s Place, Apr, 2017.</p><p>[18] Gabriel Barth-Maron, et al. <a href="https://arxiv.org/pdf/1804.08617.pdf" target="_blank" rel="external">“Distributed Distributional Deterministic Policy Gradients.”</a> ICLR 2018 poster.</p><p>[19] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. <a href="https://arxiv.org/pdf/1801.01290.pdf" target="_blank" rel="external">“Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.”</a> arXiv preprint arXiv:1801.01290 (2018).</p><p>[20] Scott Fujimoto, Herke van Hoof, and Dave Meger. <a href="https://arxiv.org/abs/1802.09477" target="_blank" rel="external">“Addressing Function Approximation Error in Actor-Critic Methods.”</a> arXiv preprint arXiv:1802.09477 (2018).</p><p>[21] Tuomas Haarnoja, et al. <a href="https://arxiv.org/abs/1812.05905" target="_blank" rel="external">“Soft Actor-Critic Algorithms and Applications.”</a> arXiv preprint arXiv:1812.05905 (2018).</p><p>[22] David Knowles. <a href="https://cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf" target="_blank" rel="external">“Lagrangian Duality for Dummies”</a> Nov 13, 2010.</p>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;&lt;p&gt;&lt;strong&gt;译者注：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;本篇文章翻译自 &lt;a href=&quot;https://lilianweng.github.io/lil-log/contact.html&quot;&gt;Dr. Weng&lt;/a&gt; 的&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#sac-with-automatically-adjusted-temperature&quot;&gt;博客&lt;/a&gt;。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;翻译行为已得到原作者授权，转载请注明原作者。&lt;/strong&gt;&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt;&lt;p&gt;摘要：在本文中，我们将深入探讨策略梯度算法的工作原理以及近年来提出的一些新的策略梯度算法：平凡策略梯度、演员评论家算法、离线策略演员评论家算法、A3C、A2C、DPG、DDPG、D4PG、MADDPG、TRPO、PPO、ACER、ACKTR、SAC以及TD3算法。&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;什么是策略梯度&quot;&gt;&lt;a href=&quot;#什么是策略梯度&quot; class=&quot;headerlink&quot; title=&quot;什么是策略梯度&quot;&gt;&lt;/a&gt;什么是策略梯度&lt;/h2&gt;&lt;p&gt;策略梯度算法是一类解决强化学习问题的方法。如果你对于强化学习领域还不熟悉，请首先阅读&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts&quot;&gt;这篇博客&lt;/a&gt;来对强化学习的问题定义以及核心概念进行初步了解。&lt;/p&gt;&lt;h3 id=&quot;符号&quot;&gt;&lt;a href=&quot;#符号&quot; class=&quot;headerlink&quot; title=&quot;符号&quot;&gt;&lt;/a&gt;符号&lt;/h3&gt;&lt;p&gt;下面是一个符号列表，可以帮助您更轻松地理解本文中的公式。&lt;/p&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;符号&lt;/th&gt;&lt;th&gt;含义&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;$s \in \mathcal{S}$&lt;/td&gt;&lt;td&gt;状态。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$a \in \mathcal{A}$&lt;/td&gt;&lt;td&gt;动作。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$r \in \mathcal{R}$&lt;/td&gt;&lt;td&gt;回报。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$S_{t}, A_{t}, R_{t}$&lt;/td&gt;&lt;td&gt;一个轨迹中第$t$个时间步对应的状态、动作以及回报。我可能会偶尔使用$s_{t}, a_{t}, r_{t}$来代替。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$\gamma$&lt;/td&gt;&lt;td&gt;折扣因子；用于惩罚未来回报中的不确定性；$0&amp;lt;\gamma \leq 1$。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$G_{t}$&lt;/td&gt;&lt;td&gt;累积回报；或者说累积折扣回报；$G_{t}=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}$。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$P\left(s^{\prime}, r\vert s, a\right)$&lt;/td&gt;&lt;td&gt;在当前状态$s$下采取动作$a$后转移到下一个状态$s^{\prime}$并得到回报$r$的概率。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$\pi(a\vert s)$&lt;/td&gt;&lt;td&gt;随机策略（智能体行为逻辑）；$\pi_{\theta}( .)$代表由$\theta$参数化的策略。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$\mu(s)$&lt;/td&gt;&lt;td&gt;确定性策略；虽然也可以把确定性策略记为$\pi(s)$，但是采用一个不同的字母可以让我们更容易分辨一个策略到底是确定性的还是随机的。$\pi$或者$\mu$都是强化学习算法要学习的目标。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$V(s)$&lt;/td&gt;&lt;td&gt;状态-值函数衡量状态$s$的期望累积回报；$V_{w}( .)$代表由$w$参数化的状态-值函数。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$V^{\pi}(s)$&lt;/td&gt;&lt;td&gt;当智能体遵循策略$\pi$时状态$s$的期望累积回报；$V^{\pi}(s)=\mathbb{E}_{a \sim \pi}\left[G_{t}\vert S_{t}=s\right]$。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$Q(s, a)$&lt;/td&gt;&lt;td&gt;动作-值函数，与状态-值函数类似，但是它衡量在状态$s$下采取动作$a$后的期望累积回报；$Q_{w}( .)$代表由$w$参数化的动作-值函数。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$Q^{\pi}(s, a)$&lt;/td&gt;&lt;td&gt;与$V^{\pi}(s)$类似，当智能体遵循策略$\pi$时，在状态$s$下采取动作$a$后的期望累积回报；$Q^{\pi}(s, a)=\mathbb{E}_{a \sim \pi}\left[G_{t}\vert S_{t}=s,A_{t}=a\right]$。&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;$A(s, a)$&lt;/td&gt;&lt;td&gt;优势函数，$A(s, a)=Q(s, a)-V(s)$；可以认为优势函数是加强版本的动作-值函数，但是由于它采用状态-值函数作为基准使得它具有更小的方差。&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;策略梯度&quot;&gt;&lt;a href=&quot;#策略梯度&quot; class=&quot;headerlink&quot; title=&quot;策略梯度&quot;&gt;&lt;/a&gt;策略梯度&lt;/h3&gt;&lt;p&gt;强化学习的目标是为智能体找到一个最优的行为策略从而获取最大的回报。&lt;strong&gt;策略梯度&lt;/strong&gt;方法主要特点在于直接对策略进行建模并优化。策略通常被建模为由$\theta$参数化的函数$\pi_{\theta}(a | s)$。回报（目标）函数的值受到该策略的直接影响，因而可以采用很多算法来对$\theta$进行优化来最大化回报（目标）函数。&lt;/p&gt;&lt;p&gt;回报（目标）函数定义如下：&lt;br&gt;$$&lt;br&gt;J(\theta)=\sum_{s \in \mathcal{S}} d^{\pi}(s) V^{\pi}(s)=\sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(a | s) Q^{\pi}(s, a)&lt;br&gt;$$&lt;br&gt;其中$d^{\pi}(s)$代表由$\pi_{\theta}$引出的马尔科夫链的平稳分布（$\pi$下的在线策略状态分布）。&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;译者注&lt;/strong&gt;：上述目标函数是在连续环境（没有固定的终止状态）下的目标函数（被称为&lt;strong&gt;平均值&lt;/strong&gt;），在连续环境下还有一种性质更好的目标函数，叫做&lt;strong&gt;平均回报&lt;/strong&gt;：&lt;br&gt;$$&lt;br&gt;\begin{aligned} J(\theta) \doteq r(\pi) &amp;amp; \doteq \lim _{h \rightarrow \infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &amp;amp;=\lim _{t \rightarrow \infty} \mathbb{E}\left[R_{t} | S_{0}, A_{0 : t-1} \sim \pi\right] \\ &amp;amp;=\sum_{s} \mu(s) \sum_{a} \pi_{\theta}(a | s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r | s, a\right) r \end{aligned}&lt;br&gt;$$&lt;br&gt;在这种定义下，&lt;strong&gt;（差分）累积回报定义为回报与平均回报的差值的累加值&lt;/strong&gt;：&lt;br&gt;$$&lt;br&gt;G_{t} \doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\cdots&lt;br&gt;$$&lt;br&gt;对应的还有差分状态-值函数以及差分-动作值函数：&lt;br&gt;$$&lt;br&gt;\begin{array}{l}&lt;br&gt;{V_{\pi}(s)=\sum_{a} \pi_{\theta}(a | s) \sum_{r, s^{\prime}} p\left(s^{\prime}, r | s, a\right)\left[r-r(\pi)+V_{\pi}\left(s^{\prime}\right)\right]} \\ {Q_{\pi}(s, a)=\sum_{r, s^{\prime}} p\left(s^{\prime}, r | s, a\right)\left[r-r(\pi)+\sum_{a^{\prime}} \pi_{\theta}\left(a^{\prime} | s^{\prime}\right) Q_{\pi}\left(s^{\prime}, a^{\prime}\right)\right]}\end{array}&lt;br&gt;$$&lt;br&gt;之所以说上述目标函数性质更好，是因为平均值目标函数其实只是它的另外一种形式，下面我们就来证明一下（&lt;a href=&quot;http://incompleteideas.net/book/bookdraft2017nov5.pdf&quot;&gt;Sutton&amp;amp;Barto,2017&lt;/a&gt;; Sec 10.4）：&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;J(\theta)=&amp;amp;\sum_{s \in \mathcal{S}} d^{\pi}(s) V^{\pi}(s) \\&lt;br&gt;=&amp;amp; \sum_{s} d_{\pi}(s) \sum_{a} \pi_{\theta}(a | s) \sum_{s^{\prime}} \sum_{r} p\left(s^{\prime}, r | s, a\right)\left[r+\gamma V_{\pi}\left(s^{\prime}\right)\right] \\&lt;br&gt;=&amp;amp;\;r(\pi) + \sum_{s} d_{\pi}(s) \sum_{a} \pi_{\theta}(a | s) \sum_{s^{\prime}} \sum_{r} p\left(s^{\prime}, r | s, a\right)\gamma V_{\pi}\left(s^{\prime}\right) \\&lt;br&gt;=&amp;amp;\;r(\pi) + \gamma \sum_{s^{\prime}} V_{\pi}\left(s^{\prime}\right) \sum_{s} d_{\pi}(s) \sum_{a} \pi_{\theta}(a | s) p\left(s^{\prime} | s, a\right) \\&lt;br&gt;=&amp;amp;\;r(\pi) + \gamma\sum_{s^{\prime}}V_{\pi}(s^{\prime})d_{\pi}(s^{\prime}) \\&lt;br&gt;=&amp;amp;\;r(\pi) + \gamma J(\theta) \\&lt;br&gt;=&amp;amp;\;r(\pi) + \gamma r(\pi) + \gamma^{2}J(\theta) \\&lt;br&gt;=&amp;amp;\;r(\pi) + \gamma r(\pi) + \gamma^{2}r(\pi) + \gamma^{3}J(\theta) + \cdots \\&lt;br&gt;=&amp;amp;\;\frac{1}{1-\gamma}r(\pi)&lt;br&gt;\end{aligned}&lt;br&gt;$$&lt;br&gt;&lt;strong&gt;因而在下面进行策略梯度定理证明时，我们仅考虑平均回报形式的目标函数。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;另一方面，在周期环境下的目标函数（&lt;strong&gt;不包含折扣因子&lt;/strong&gt;）如下：&lt;br&gt;$$&lt;br&gt;J(\theta)=V^{\pi}(s_{0})&lt;br&gt;$$&lt;br&gt;之所以不在连续环境下也使用上述目标函数，是因为在连续环境下上述目标函数的值为无穷大，优化一个无穷大值是没有意义的。&lt;/p&gt;&lt;hr&gt;&lt;p&gt;为了简化符号，当策略作为其他函数的上标或者下标出现时$\pi_{\theta}$中的参数$\theta$将省略，例如：$d^{\pi}$以及$Q^{\pi}$的完整形式为$d^{\pi_{\theta}}$以及$Q^{\pi_{\theta}}$。想象一下你一直在马尔科夫链构成的状态序列中游荡，随着时间不断向前推进，你经过某个状态的概率将保持不变——这个概率就是$\pi_{\theta}$下的平稳概率。$d^{\pi}(s)=\lim _{t \rightarrow \infty} P\left(s_{t}=s | s_{0}, \pi_{\theta}\right)$就表示你从状态$s_0$开始，在策略$\pi_{\theta}$下经过$t$个时间步后到达状态$s$的概率。实际上，PageRank正是利用了马尔科夫链的平稳分布。你可以阅读&lt;a href=&quot;https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/&quot;&gt;这篇文章&lt;/a&gt;以获取更多细节。&lt;/p&gt;&lt;p&gt;我们可以很自然的预见到基于策略的方法能够很好地处理连续空间中的强化学习问题。因为在连续空间中存在无限多的动作及（或）状态因而基于值函数的算法由于需要去估计所有的动作及（或）状态的值导致其所需的算力变得无法接受。例如，在&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy-iteration&quot;&gt;一般的策略迭代&lt;/a&gt;过程中，策略提升步骤$\arg \max _{a \in \mathcal{A}} Q^{\pi}(s, a)$需要去遍历整个动作空间，因而会遭受&lt;a href=&quot;https://en.wikipedia.org/wiki/Curse_of_dimensionality&quot;&gt;维度诅咒&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;使用&lt;em&gt;梯度上升&lt;/em&gt;方法，我们可以将参数$\theta$往梯度$\nabla_{\theta} J(\theta)$给出的方向进行改变从而去找到最优的$\theta$使得其对应的策略$\pi_{\theta}$能够给智能体带来最大的期望累积回报。&lt;/p&gt;&lt;h3 id=&quot;策略梯度定理&quot;&gt;&lt;a href=&quot;#策略梯度定理&quot; class=&quot;headerlink&quot; title=&quot;策略梯度定理&quot;&gt;&lt;/a&gt;策略梯度定理&lt;/h3&gt;&lt;p&gt;计算梯度$\nabla_{\theta} J(\theta)$可不是一件简单的事情。因为梯度值不仅依赖于动作的选择（由$\pi_{\theta}$直接决定），还依赖于由选择的动作而产生的状态的平稳分布（由$\pi_{\theta}$间接决定）。鉴于环境通常是未知的，很难去估计策略的更新对于状态分布造成的影响。&lt;/p&gt;&lt;p&gt;哇哦！这时候出现了策略梯度定理来拯救世界了！该定理对梯度的形式进行了变形使其不依赖于状态分布$d^{\pi}( .)$的导数，从而在很大程度上简化了梯度$\nabla_{\theta} J(\theta)$的计算：&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;\nabla_{\theta} J(\theta) &amp;amp;=\nabla_{\theta} \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \pi_{\theta}(a | s) \\&lt;br&gt;&amp;amp;\propto \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \nabla_{\theta} \pi_{\theta}(a \vert s) \end{aligned}&lt;br&gt;$$&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;译者注：&lt;/strong&gt;这里策略梯度定理的形式不够完全，只考虑了目标函数为连续环境下的平均值形式，上述定理还同时适用于连续环境下的平均回报形式目标函数以及周期环境下的目标函数，即：&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;\nabla_{\theta} J(\theta) &amp;amp;\stackrel{\text{def}}{=} \nabla_{\theta} \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \pi_{\theta}(a | s) &amp;amp; \scriptstyle{\text{连续环境下的平均值形式目标函数}} \\&lt;br&gt;&amp;amp;\stackrel{\text{def}}{=} \nabla_{\theta} \sum_{s\in\mathcal{S}} \mu(s) \sum_{a\in\mathcal{A}} \pi_{\theta}(a | s) \sum_{s^{\prime}\in\mathcal{S}, r} p\left(s^{\prime}, r | s, a\right) r &amp;amp; \scriptstyle{\text{连续环境下的平均回报形式目标函数}}\\&lt;br&gt;&amp;amp;\stackrel{\text{def}}{=} \nabla_{\theta} V^{\pi}(s_{0}) &amp;amp; \scriptstyle{\text{周期环境下的目标函数}}\\&lt;br&gt;&amp;amp; \propto \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \nabla_{\theta} \pi_{\theta}(a | s) &amp;amp; \scriptstyle{}&lt;br&gt;\end{aligned}&lt;br&gt;$$&lt;/p&gt;&lt;hr&gt;&lt;h3 id=&quot;策略梯度定理的证明&quot;&gt;&lt;a href=&quot;#策略梯度定理的证明&quot; class=&quot;headerlink&quot; title=&quot;策略梯度定理的证明&quot;&gt;&lt;/a&gt;策略梯度定理的证明&lt;/h3&gt;&lt;p&gt;现在我们要深入上述定理的证明（&lt;a href=&quot;http://incompleteideas.net/book/bookdraft2017nov5.pdf&quot;&gt;Sutton&amp;amp;Barto,2017&lt;/a&gt;; Sec 13.1（&lt;strong&gt;译者注：这里应该更改为Sec 13.2&lt;/strong&gt;））从而理解为什么该定理的正确的，因而这部分会包含很多的数学公式。&lt;/p&gt;&lt;p&gt;我们首先从计算状态-值函数的梯度开始：&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;&amp;amp; \nabla_\theta V^\pi(s) \\&lt;br&gt;=&amp;amp; \nabla_\theta \Big(\sum_{a \in \mathcal{A}} \pi_\theta(a \vert s)Q^\pi(s, a) \Big) &amp;amp; \\&lt;br&gt;=&amp;amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta Q^\pi(s, a)} \Big) &amp;amp; \scriptstyle{\text{; 微分乘法法则}} \\&lt;br&gt;=&amp;amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta \sum_{s’, r} P(s’,r \vert s,a)(r + V^\pi(s’))} \Big) &amp;amp; \scriptstyle{\text{; 扩展} Q^\pi} \\&lt;br&gt;=&amp;amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s’, r} P(s’,r \vert s,a) \nabla_\theta V^\pi(s’)} \Big) &amp;amp; \scriptstyle{; P(s’,r \vert s,a) \text{或者} r \text{不是}\theta \text{的函数}}\\&lt;br&gt;=&amp;amp; \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s’} P(s’ \vert s,a) \nabla_\theta V^\pi(s’)} \Big) &amp;amp; \scriptstyle{\text{; 因为 } P(s’ \vert s, a) = \sum_r P(s’, r \vert s, a)}&lt;br&gt;\end{aligned}&lt;br&gt;$$&lt;/p&gt;&lt;p&gt;现在我们有：&lt;br&gt;$$&lt;br&gt;{\color{red}{\nabla_\theta V^\pi(s)}}&lt;br&gt;= \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \sum_{s’} P(s’ \vert s,a) \color{red}{\nabla_\theta V^\pi(s’)} \Big)&lt;br&gt;$$&lt;br&gt;上述等式拥有很好的递归特性（红色部分）因而未来状态的状态-值函数$V^{\pi}\left(s^{\prime}\right)$可以根据上式来递归地展开。让我们考虑一个下面这样的状态访问序列并将从状态$s$开始在策略$\pi_{\theta}$下经过$k$个时间步到达状态$x$的概率记为$\rho^{\pi}(s \rightarrow x, k)$：&lt;br&gt;$$&lt;br&gt;s \xrightarrow[]{a \sim \pi_\theta(.\vert s)} s’ \xrightarrow[]{a \sim \pi_\theta(.\vert s’)} s’’ \xrightarrow[]{a \sim \pi_\theta(.\vert s’’)} \dots&lt;br&gt;$$&lt;/p&gt;&lt;p&gt;对于不同的$k$值，$\rho^{\pi}(s \rightarrow x, k) $值包含以下几种情况：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当$k=0$时：$\rho^{\pi}(s \rightarrow s, k=0)=1$。&lt;/li&gt;&lt;li&gt;当$k=1$时，我们遍历在状态$s$下所有可能的动作$a$然后将所有从元组$(s,a)$转移到目标状态的概率累加：$\rho^{\pi}\left(s \rightarrow s^{\prime}, k=1\right)=\sum_{a} \pi_{\theta}(a | s) P\left(s^{\prime} | s, a\right)$。&lt;/li&gt;&lt;li&gt;设想以下我们的目标是从状态$s$开始依照策略$\pi_{\theta} $经过$k+1$个时间步最终达到目标状态$x$。为了实现这个目标，我们可以先从状态$s$开始经过$k$个时间步后达到某个中间状态$s^{\prime} $（任何一个状态$s\in\mathcal{S}$均可成为中间状态）然后经过最后一个时间步到达目标状态$x$。这样的话，我们就可以递归地计算访问概率：$\rho^{\pi}(s \rightarrow x, k+1)=\sum_{s^{\prime}} \rho^{\pi}\left(s \rightarrow s^{\prime}, k\right) \rho^{\pi}\left(s^{\prime} \rightarrow x, 1\right)$。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;有了以上的相关基础，我们就可以递归地展开$\nabla_\theta V^\pi(s)$！首先为了简化符号我们进行以下符号上的替换：$\phi(s)=\sum_{a \in \mathcal{A}} \nabla_{\theta} \pi_{\theta}(a | s) Q^{\pi}(s, a)$。如果我们不停地展开$\nabla_{\theta} V^{\pi}(\cdot)$，那么可以发现通过这个展开过程我们可以从状态$s$开始经过任意时间步后到达任意的状态，并且将上述过程中的访问概率累加起来就可以得到$\nabla_\theta V^\pi(s)$！&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;&amp;amp; \color{red}{\nabla_\theta V^\pi(s)} \\&lt;br&gt;=&amp;amp; \phi(s) + \sum_a \pi_\theta(a \vert s) \sum_{s’} P(s’ \vert s,a) \color{red}{\nabla_\theta V^\pi(s’)} \\&lt;br&gt;=&amp;amp; \phi(s) + \sum_{s’} \sum_a \pi_\theta(a \vert s) P(s’ \vert s,a) \color{red}{\nabla_\theta V^\pi(s’)} \\&lt;br&gt;=&amp;amp; \phi(s) + \sum_{s’} \rho^\pi(s \to s’, 1) \color{red}{\nabla_\theta V^\pi(s’)} \\&lt;br&gt;=&amp;amp; \phi(s) + \sum_{s’} \rho^\pi(s \to s’, 1) \color{red}{[ \phi(s’) + \sum_{s’’} \rho^\pi(s’ \to s’’, 1) \nabla_\theta V^\pi(s’’)]} \\&lt;br&gt;=&amp;amp; \phi(s) + \sum_{s’} \rho^\pi(s \to s’, 1) \phi(s’) + \sum_{s’’} \rho^\pi(s \to s’’, 2){\color{red}{\nabla_\theta V^\pi(s’’)}} \scriptstyle{\text{ ; 考虑将 }s’\text{ 作为 }s \to s’’}\text{的中间状态}\\&lt;br&gt;=&amp;amp; \phi(s) + \sum_{s’} \rho^\pi(s \to s’, 1) \phi(s’) + \sum_{s’’} \rho^\pi(s \to s’’, 2)\phi(s’’) + \sum_{s’’’} \rho^\pi(s \to s’’’, 3)\color{red}{\nabla_\theta V^\pi(s’’’)} \\&lt;br&gt;=&amp;amp; \dots \scriptstyle{\text{; 递归展开 }\nabla_\theta V^\pi(.)} \\&lt;br&gt;=&amp;amp; \sum_{x\in\mathcal{S}}\sum_{k=0}^\infty \rho^\pi(s \to x, k) \phi(x)&lt;br&gt;\end{aligned}上述&lt;br&gt;$$&lt;/p&gt;&lt;p&gt;上述变形使得我们无需计算Q-值函数的梯度$\nabla_\theta Q^\pi(s, a)$。将其带入目标函数$J(\theta)$中，可得：&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;\nabla_\theta J(\theta)&lt;br&gt;&amp;amp;= \nabla_\theta V^\pi(s_0) &amp;amp; \scriptstyle{\text{; 从一个随机状态 } s_0 \text{开始}} \\&lt;br&gt;&amp;amp;= \sum_{s}\color{blue}{\sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)} \phi(s) &amp;amp;\scriptstyle{\text{; 令 }\color{blue}{\eta(s) = \sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)}} \\&lt;br&gt;&amp;amp;= \sum_{s}\eta(s) \phi(s) &amp;amp; \\&lt;br&gt;&amp;amp;= \Big( {\sum_s \eta(s)} \Big)\sum_{s}\frac{\eta(s)}{\sum_s \eta(s)} \phi(s) &amp;amp; \scriptstyle{\text{; 正则化 } \eta(s), s\in\mathcal{S} \text{ 使其成为一个概率分布}}\\&lt;br&gt;&amp;amp;\propto \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) &amp;amp; \scriptstyle{\sum_s \eta(s)\text{ 是一个常数}} \\&lt;br&gt;&amp;amp;= \sum_s d^\pi(s) \sum_a \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) &amp;amp; \scriptstyle{d^\pi(s) = \frac{\eta(s)}{\sum_s \eta(s)}\text{ 即为平稳分布}}&lt;br&gt;\end{aligned}&lt;br&gt;$$&lt;/p&gt;&lt;hr&gt;&lt;p&gt;译者注：上述证明过程仅仅涉及周期环境，连续环境下的证明如下：&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;&amp;amp; \nabla_\theta V^\pi(s) \\&lt;br&gt;=&amp;amp; \nabla_\theta \Big(\sum_{a \in \mathcal{A}} \pi_\theta(a \vert s)Q^\pi(s, a) \Big) &amp;amp; \\&lt;br&gt;=&amp;amp; \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s) {\color{red}{\nabla_\theta Q^\pi(s, a)}} \Big) &amp;amp; \scriptstyle{\text{; 微分乘法法则}} \\&lt;br&gt;=&amp;amp; \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s) {\color{red}{\nabla_\theta \sum_{s’, r} P(s’,r \vert s,a)(r - r(\pi) + V^\pi(s’))}}\Big) &amp;amp; \scriptstyle{\text{; 扩展} Q^\pi} \\&lt;br&gt;=&amp;amp; \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s){\color{red}{\bigg[{\color{blue}{-\nabla_{\theta}r(\pi)}}+\sum_{s’, r} P(s’,r \vert s,a) \nabla_\theta V^\pi(s’)} \bigg]}\Big) &amp;amp; \scriptstyle{; P(s’,r \vert s,a) \text{或者} r \text{不是}\theta \text{的函数}}\\&lt;br&gt;=&amp;amp; \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s) {\color{red}{\bigg[{\color{blue}{-\nabla_{\theta}r(\pi)}}+\sum_{s’} P(s’ \vert s,a) \nabla_\theta V^\pi(s’)} \bigg]}\Big) &amp;amp; \scriptstyle{\text{; 因为 } P(s’ \vert s, a) = \sum_r P(s’, r \vert s, a)} \\&lt;br&gt;=&amp;amp; {\color{blue}{-\nabla_{\theta}r(\pi)}} + \phi(s) + \sum_{a \in \mathcal{A}} \Big( \pi_\theta(a \vert s) {\color{red}{\sum_{s’} P(s’ \vert s,a) \nabla_\theta V^\pi(s’)}}\Big)&lt;br&gt;\end{aligned}&lt;br&gt;$$&lt;/p&gt;&lt;p&gt;经过移项可得：&lt;br&gt;$$&lt;br&gt;\color{blue} {\nabla_{\theta}r(\pi)} =\phi(s) + \sum_{a}\left[\pi_{\theta}(a | s) \color{red} {\sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)} \right]-\nabla_{\theta} V^{\pi}(s)&lt;br&gt;$$&lt;br&gt;那么平均回报形式的目标函数的梯度有：&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;\nabla_{\theta}J(\theta) =&amp;amp;\;\nabla_{\theta} r(\pi) \\&lt;br&gt;=&amp;amp; \sum_{s} d^{\pi}(s)\left(\phi(s) + \sum_{a}\left[\pi_{\theta}(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)\right]-\nabla_{\theta} V^{\pi}(s)\right) \\&lt;br&gt;=&amp;amp; \sum_{s} d^{\pi}(s) \phi(s) +\sum_{s} d^{\pi}(s) \sum_{a} \pi_{\theta}(a | s) \sum_{s^{\prime}} p\left(s^{\prime} | s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)-\sum_{s} d^{\pi}(s) \nabla_{\theta} V^{\pi}(s) \\&lt;br&gt;=&amp;amp; \sum_{s} d^{\pi}(s) \phi(s) + \sum_{s^{\prime}} \underbrace{\sum_{s} d^{\pi}(s) \sum_{a} \pi_{\theta}(a|s) p\left(s^{\prime}|s,a\right)}_{d^{\pi}\left(s^{\prime}\right)} \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)-\sum_{s} d^{\pi}(s) \nabla_{\theta} V^{\pi}(s) \\&lt;br&gt;=&amp;amp; \sum_{s} d^{\pi}(s) \phi(s) + \sum_{s^{\prime}} d^{\pi}(s^{\prime}) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)-\sum_{s} d^{\pi}(s) \nabla_{\theta} V^{\pi}(s) \\&lt;br&gt;=&amp;amp; \sum_{s} d^{\pi}(s) \phi(s) \\&lt;br&gt;=&amp;amp; \sum_s d^\pi(s) \sum_a Q^\pi(s, a)\nabla_\theta\pi_\theta(a \vert s) \\&lt;br&gt;\propto&amp;amp; \sum_s d^\pi(s) \sum_a Q^\pi(s, a)\nabla_\theta \pi_\theta(a \vert s)&lt;br&gt;\end{aligned}&lt;br&gt;$$&lt;br&gt;其中第二个等式成立是因为$\nabla_{\theta}r(\pi)$不依赖$s$且$\sum_{s}d^{\pi}(s)=1$。&lt;/p&gt;&lt;p&gt;另外，平均值形式的目标函数的梯度有：&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;\nabla_{\theta}J(\theta) =&amp;amp;\;\nabla_{\theta}\sum_{s \in \mathcal{S}} d^{\pi}(s) V^{\pi}(s) \\&lt;br&gt;=&amp;amp;\;\nabla_{\theta} \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} Q^{\pi}(s, a) \pi_{\theta}(a | s)\\&lt;br&gt;=&amp;amp;\;\frac{1}{1-\gamma}\nabla_{\theta} r(\pi) \\&lt;br&gt;=&amp;amp;\;\frac{1}{1-\gamma}\left(\sum_s d^\pi(s) \sum_a Q^\pi(s, a)\nabla_\theta\pi_\theta(a \vert s)\right) \\&lt;br&gt;\propto&amp;amp; \sum_s d^\pi(s) \sum_a Q^\pi(s, a)\nabla_\theta \pi_\theta(a \vert s)&lt;br&gt;\end{aligned}&lt;br&gt;$$&lt;/p&gt;&lt;hr&gt;&lt;p&gt;梯度还可以改写为如下形式：&lt;br&gt;$$&lt;br&gt;\begin{aligned}&lt;br&gt;\nabla_\theta J(\theta)&lt;br&gt;&amp;amp;\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s) &amp;amp;\\&lt;br&gt;&amp;amp;= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} &amp;amp;\\&lt;br&gt;&amp;amp;= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] &amp;amp; \scriptstyle{\text{; 因为 } (\ln x)’ = 1/x}&lt;br&gt;\end{aligned}&lt;br&gt;$$&lt;br&gt;$\mathbb{E}_{\pi}$代表$\mathbb{E}_{s \sim d_{\pi}, a \sim \pi_{\theta}}$，下标表示遵循策略$\pi_{\theta}$（在线策略）时状态以及动作的分布。&lt;/p&gt;&lt;p&gt;上述策略梯度定理是许多策略梯度算法的理论基础。平凡策略梯度算法由于直接使用采样得到的回报所以其估计的梯度不存在偏差（bias）但是有较大的方差（variance）（见下式）。因此后续的算法被相继提出用以在保持偏差有界的情况下减小方差。&lt;br&gt;$$&lt;br&gt;\nabla_{\theta} J(\theta)=\mathbb{E}_{\pi}\left[Q^{\pi}(s, a) \nabla_{\theta} \ln \pi_{\theta}(a | s)\right]&lt;br&gt;$$&lt;br&gt;这里有一个从GAE（泛化优势估计，genaral advantage estimate）论文 （&lt;a href=&quot;https://arxiv.org/abs/1506.02438&quot;&gt;Schulman et al., 2016&lt;/a&gt;）中引用的很好的策略梯度算法一般形式的归纳，并且&lt;a href=&quot;https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/&quot;&gt;这篇博客&lt;/a&gt;深入探讨了GAE的几大组成部分，值得一读。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/13653853-593dfedad43dfce3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;图1. 策略梯度方法的一般形式。图片来源：[Schulman et al., 2016](https://arxiv.org/abs/1506.02438)&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Guided Policy Search(GPS)</title>
    <link href="http://yoursite.com/2019/04/06/Guided-Policy-Search-GPS/"/>
    <id>http://yoursite.com/2019/04/06/Guided-Policy-Search-GPS/</id>
    <published>2019-04-05T19:18:32.000Z</published>
    <updated>2019-04-05T19:29:00.387Z</updated>
    
    <content type="html"><![CDATA[<p><em>一篇因为各种突发状况断断续续写了将近两周的文章 = =</em></p><hr><p>这篇博客将介绍GPS方法，GPS方法是由强化学习大牛Sergey Levine（在最近的ICLR 2019发表了13篇论文）于2013年提出的，目前被作为基础算法广泛应用于各种强化学习任务中。其出发点在于纯粹的策略梯度方法在更新参数时不会用到环境模型因而属于一种无模型强化学习算法，所谓成也萧何败也萧何，虽然这使得策略梯度方法通用性很好，但是由于没有利用到任何环境的内在属性，使得其训练只能完全依靠试错，效率较低。</p><p>基于模型的路径优化算法（例如前面博客里提到的iLQR）方法，能够充分利用环境模型，从而在利用较少训练样本的情况下即可使得算法收敛到局部最优解。但是路径优化算法是一个开环方法，在随机环境下效果较差，虽然能够通过使用MPC方法（基本思想是每次只执行路径优化算法输出的第一个时间步的动作）来增加算法的稳定性，但是执行时耗时较长无法适用于实时任务。但是策略梯度方法是一个闭环方法，因而其对于随即环境的适应能力以及执行耗时上都能达到很好的效果。因而一个直观的想法是，能不能将两者结合起来，用路径优化算法的输出结果来指导策略梯度方法的训练过程，从而提升策略梯度方法的效率呢？GPS方法正是基于这种思想提出的。</p><p>本文主要对早期GPS的三篇论文进行了总结（还包括了一些其他论文的相关结论），具体请参阅文末的参考文献。文章的结构如下：第一部分将会对最原始的GPS方法进行介绍，第二部分将会介绍一个改进版本。注意，这两种版本的GPS算法都必须事先已知环境模型。第三部分将介绍一个在未知环境模型（需要在算法训练的过程中对环境模型进行局部估计）的情况下也能够使用的GPS算法。以上三种GPS算法均属于基于模型的强化学习算法（以后我将专门写一篇文章来介绍基于模型的强化学习算法）。为了方便起见，我将最原始的GPS算法记为GPS-V1(ICML2013)[^2]，改进版记为GPS-V2(ICML2014)[^3]，最后一个版本记为GPS-V3(NIPS2014)[^4]。</p><h2 id="GPS-V1"><a href="#GPS-V1" class="headerlink" title="GPS-V1"></a>GPS-V1</h2><p>原始版本的GPS算法基本思想是首先使用路径优化算法产生一些训练数据并加入训练集中用以指导后续策略梯度方法的训练。但是策略梯度方法是在线策略算法，只能使用当前策略采样得到的数据来估计梯度从而更新参数。为了能够使用其他策略采样的数据，这里必须要使用一种技术：重要性采样。在这里我首先跑一下题来介绍一下重要性采样。</p><h3 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h3><p>对于一个函数$f$以及一个概率分布$P$，我们想要计算如下统计量：<br>$$<br>\mathbf{E}_{P(X)}[f(X)]=\int_{x}P(x)f(x)dx.<br>$$<br>我们知道，一般估计一个期望值的方法是从变量从属的概率分布中进行采样，然后计算均值。但是实际上概率分布$P$可能非常复杂，我们没有办法从其中进行采样。重要性采样方法通过从另外一个较为简单的分布$Q$中采样出的样本对以上期望值进行估计：<br>$$<br>\begin{align}<br>\mathbf{E}_{P(X)}[f(X)]&amp;=\mathbf{E}_{Q(X)}\left[\frac{P(X)}{Q(X)}f(X)\right] \\<br>&amp;\approx \frac{1}{m}\sum_{i=1}^m \frac{P(x^{(i)})}{Q(x^{(i)})}f(x^{(i)})\;\;\text{with}\;\;x^{(i)}\sim Q.<br>\end{align}<br>$$</p><h3 id="基于重要性采样的策略梯度方法"><a href="#基于重要性采样的策略梯度方法" class="headerlink" title="基于重要性采样的策略梯度方法"></a>基于重要性采样的策略梯度方法</h3><p>让我们回到正题，利用这种方法就可以在估计当前正在学习的策略的梯度时采用其他策略采样出的样本：<br>$$<br>\mathbf{E}[J(\theta)]\approx\sum_{t=1}^T \frac{1}{Z_t(\theta)}\sum_{i=1}^m \frac{\pi_{\theta}(\tau_{i,1:t)}}{q(\tau_{i,1:t})}r(x_t^i,u^i_t),<br>$$<br>其中$Z_t(\theta)=\sum_{i=1}^m\frac{\pi_{\theta}(\tau_{i,1:t)}}{q(\tau_{i,1:t})}$。从理论上来说$Z_t(\theta)=m$才是期望的无偏估计，这里为了减小训练时的方差采用了这个特殊值。但是我们是在其他策略采样出的样本分布的基础上进行新策略的搜索，一旦新策略的样本分布与采样样本分布相距较远时，无法保证估计梯度的准确性。前面有工作是通过计算重要性权重的方差来判断新策略的准确性的[^6]，但是对于很长的路径，重要性权重在大部分地方都为0，方差也很小，但是并不能说明什么问题。V1版本的GPS算法通过在优化目标上额外加入重要性权重的对数值的方式，来“软最大化”重要性权重值，毕竟重要性权重越大，代表新策略分布与采样分布更为接近（<strong>但其实在采样分布概率较小的地方新策略分配一个较大的概率也会使得这个值比较大，所以感觉这种方法还是有很大缺陷的</strong>）：<br>$$<br>\Phi(\theta)=\sum_{t=1}^{T}\left[\frac{1}{Z_{t}(\theta)} \sum_{i=1}^{m} \frac{\pi_{\theta}\left(\zeta_{i, 1 : t}\right)}{q\left(\zeta_{i, 1 : t}\right)} r\left(\mathbf{x}_{t}^{i}, \mathbf{u}_{t}^{i}\right)+w_{r} \log Z_{t}(\theta)\right].<br>$$</p><h3 id="指导样本的生成"><a href="#指导样本的生成" class="headerlink" title="指导样本的生成"></a>指导样本的生成</h3><p>GPS系列算法希望使用路径优化算法生成的指导样本来引导策略梯度算法往高回报的区域搜索（而非暴力试错）。在之前的文章中我们讲过iLQR算法，但是只展开讲了确定性情况下的相关知识。而策略梯度算法的应用场景大部分都是非确定性场景，即使是确定性场景，也会因为噪声的存在使其实际上同样是非确定性的。因而，下面我们主要关注非确定性场景下的指导样本生成。</p><p>在非确定性条件下，指导样本将服从某个概率分布，我们希望这个概率分布满足以下两个性质：</p><ol><li>各区域的概率密度不要过大（否则会使得重要性权重较小，使得指导样本对于梯度的贡献很小）</li><li>该分布要尽可能覆盖高回报区域</li></ol><p>GPS-V1的作者发现，如果指导样本的分布$q$是分布$\rho(\zeta) \propto \exp (r(\zeta))$的<a href="https://en.wikipedia.org/wiki/Information_projection" target="_blank" rel="external">I-投影</a>，即最小化如下KL散度:<br>$$<br>D_{\mathrm{KL}}(q | \rho)=E_{q}[-r(\zeta)]-\mathcal{H}(q),<br>$$<br>得到的分布$q$即可满足以上两个性质。具体来说，上式右边第一部分保证了性质2，右边第二部分保证了性质1。那么剩余的问题是分布$q$的具体形式是什么呢？GPS-V1假设分布$q$是一个<strong>高斯分布</strong>，这是一个很自然的也是最容易想到的假设。<strong>而且如果我们希望能够用只能解决非随机环境的路径优化算法例如iLQR来解决随即环境下的规划问题的话，只能假设$q$为高斯分布。</strong></p><p>为了能够直接使用类似iLQR算法的路径优化算法，我们这里再引入另外一个概念或者一个框架，叫做线性可解马尔科夫决策过程（LMDP）[^5]。该框架的核心思想在于，摒弃动作（action）的概念。智能体在不断优化其策略的过程中，会通过动作来改变其自身的状态转移概率（即在一个状态下转移到下一个状态的概率，策略在不断优化，那么每个状态下采取的动作也会变化，因而状态转移概率也会发生变化）。那么为何不放弃这一间接的做法，将策略直接定义为状态转移概率呢？即:</p><p>$$<br>p\left(x^{\prime} | x, u\right)=u\left(x^{\prime} | x\right)<br>$$<br>这就是LMDP的核心思想（<strong>这部分牵涉的知识较广，我同样会之后单独写一篇文章来详细介绍</strong>）。在LMDP的框架下，其回报函数变成如下形式：<br>$$<br>\tilde{r}\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=r\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)-D_{\mathrm{KL}}\left(\pi_{\mathcal{G}}\left(\cdot | \mathbf{x}_{t}\right) | p\left(\cdot | \mathbf{x}_{t}\right)\right),<br>$$<br>其中$\pi_{\mathcal{G}}$代表学习到的策略（即学习到的状态转移概率），$p$代表智能体在没有任何算法控制的情况下的状态转移概率。当$p$表示一个均匀分布时，上式可转化为：<br>$$<br>E_{\pi_{\mathcal{G}}}[\tilde{r}(\zeta)]=E_{\pi_{\mathcal{G}}}[r(\zeta)]+\mathcal{H}\left(\pi_{\mathcal{G}}\right).<br>$$<br>因而求解一个LMDP得到的指导样本的概率分布是满足前面提到的两个性质的。另外，可以证明，当状态转移是线性以及回报函数是二次函数的情况下，最优策略可以直接通过iLQR算法求解并且求解出的最优策略服从以下高斯分布：<br>$$<br>\pi_{\mathcal{G}}\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right)=\mathcal{G}\left(\mathbf{u}_{t} ; g\left(\mathbf{x}_{t}\right),-Q_{\mathbf{u u} t}^{-1}\right).<br>$$<br>$g(\mathbf{x}_t)$代表确定环境下运用iLQR算法得出的最优策略，$Q_{\mathbf{u u} t}^{-1}$代表确定环境下运行iLQR算法中的参数，具体参见iLQR算法。值得注意的是，由于LMDP并没有动作这一概念，之所以可以得出以上结论，是采用LMDP估计MDP得出的结论，具体细节我会单独写一篇文章细讲。这里得出的结论是什么意思呢？其实就是说在运行iLQR算法时，将前向循环中计算最优动作的过程转化为从以上高斯分布中采样，然后将优化目标从仅仅最小化（或最大化）损失函数（或回报函数）转变为同时最大化最优策略的熵即可即可。这样导出的指导样本就满足我们的要求。</p><p>还有一个问题是$\pi_{\mathcal{G}}$只有在状态转移是线性的情况下才是一个高斯分布，在状态转移是非线性的情况下，$\pi_{\mathcal{G}}$是指导样本分布的一个局部的高斯估计。由于GPS算法需要通过指导样本来将策略搜索的方向引导向高回报区域（这个高回报区域就是采样出指导样本的高斯分布的均值区域），但是这个分布只在指导样本附近是准确的，离得比较远就会出现较大误差，但是策略搜索的区域是任意的，就会出现梯度估计不准确的现象。以上问题其实已经缓解了，前面提到的改进版的梯度公式中的正则项保证了策略搜索的区域会靠近指导样本的区域：<br>$$<br>\Phi(\theta)=\sum_{t=1}^{T}\left[\frac{1}{Z_{t}(\theta)} \sum_{i=1}^{m} \frac{\pi_{\theta}\left(\zeta_{i, 1 : t}\right)}{q\left(\zeta_{i, 1 : t}\right)} r\left(\mathbf{x}_{t}^{i}, \mathbf{u}_{t}^{i}\right)+w_{r} \log Z_{t}(\theta)\right].<br>$$</p><h3 id="适应性指导样本分布"><a href="#适应性指导样本分布" class="headerlink" title="适应性指导样本分布"></a>适应性指导样本分布</h3><p>我们的策略是用某个特定的函数来进行估计的，而函数的表示能力是有限的（即使是神经网络，不同网络层数以及神经元个数都会对网络的表示能力产生影响。而所谓的通用函数估计器是在神经元个数无限的情况下才成立），那么强行让学习策略的分布与知道样本的分布尽可能一致可能会导致一些问题。考虑下面这种情况：指导样本是在了解模型的情况下进行决策的，而策略梯度算法是在不知道模型的情况下进行决策的。因而前者除了观察到的状态外还利用了环境模型的信息，在相似的观察下可能会进行差异较大的决策。换句话讲，策略函数要尝试去拟合一些相似输入产生不同输出的数据点，这就会使得算法训练起来十分困难。</p><p>目前的算法流程是在算法初始化时就使用iLQR算法产生大量的指导样本，之后就不再产生新的指导样本了。为了解决以上问题，在策略不断更新的过程中，重新根据以下这个新的回报函数来运行iLQR算法产生新的指导样本：<br>$$<br>\overline{r}\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=r\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)+\log \pi_{\theta}\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right).<br>$$<br>通过以上回报函数产生的指导样本会尝试产生策略函数能够产生的样本分布。</p><h3 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h3><p>GPS-V1的整体流程如下图所示：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/GPS/GPS-V1.png" alt="GPS-V1"></p><p>这里有几点细节需要说明。</p><ol><li>算法第6行选取训练样本，其实主要包括以下两种样本。第一种，全部的指导样本；第二种，重要性权重较大的新样本，权重较大代表与指导样本更为相似。</li><li>第7行进行参数更新时，是从上一步最优的参数作为初始点进行更新的。但是有时算法会陷入到局部最优解中，使得在该局部最优解下指导样本的重要性权重较小，那么指导样本就对梯度的估计无法产生影响，这样就会使得算法进一步往更差的方向更新。为了防止以上问题，这一步的参数更新从两个不同的初始点开始：第一个初始点就是上一步更新的结果；而第二个则是求得一个使得当前采集到的回报较高的样本重要性权重最大的参数当作初始化参数。</li><li>关于第11-17行，具体解释以下为什么要采取这些操作。当新搜索到的策略比原先的策略要好时，适当减小约束项的权重，这样可以增大下一步的策略搜索范围，换句话说就是可以步子迈得大一些；反之，如果新搜索到的策略比上一步的要差，那么可能目前处于一个局部最优解较多的区域内，或者搜索区域过大使得梯度估计得不准确，这时候应该适当缩小搜索范围，在更接近指导样本的区域内搜索。再者，如果这样还是不能搜索到更好的策略，那么可能就是采样出的训练样本不好，这时候可以尝试重新采样。</li></ol><p>以上就是GPS-V1算法的全部内容。其实在理解GPS-V1算法之后，后面两个版本就很简单了，因此我下面的内容相对也会少很多。</p><h2 id="GPS-V2"><a href="#GPS-V2" class="headerlink" title="GPS-V2"></a>GPS-V2</h2><p>作者在GPS-V1算法里发现了一个问题，其实V1算法也尝试去解决这个问题但是效果不好，这个问题就是1.4节中所描述的问题。基于模型的路径优化算法产生的指导样本，不基于模型的策略梯度方法有时候并不能拟合出来，就像在上一节中讲到的那样，策略梯度算法观察不到一些通过模型才能反应出来的因素。这样会使得对于复杂问题学习出来的样本分布并不能与指导样本分布符合的很好。GPS-V2从另外一个角度解决了上述问题，即完全抛弃了策略梯度步骤，直接让策略通过对指导样本进行监督学习得出，并且在路径优化算法更新时考虑到与当前策略的距离并且尝试使得这个距离尽可能小。通过这样一种迭代更新的流程来使得两者最终匹配。</p><p>上述思想其实GPS-V1部分的1.4节已经考虑到了，但是是通过改变回报函数的方式达到的。GPS-V2通过一种更直接的方式来建模，并抛弃了策略梯度部分，通过更加鲁棒的监督学习来学习策略，使得算法更新更为稳定。具体来说，GPS-V2求解以下优化问题：<br>$$<br>\begin{align} \min _{\theta, q(\tau)} &amp; D_{\mathbf{KL}}(q(\tau) | \rho(\tau)) \\ \text { s.t. } &amp; q\left(\mathbf{x}_{1}\right)=p\left(\mathbf{x}_{1}\right) \nonumber \\ &amp; q\left(\mathbf{x}_{t+1} | \mathbf{x}_{t}, \mathbf{u}_{t}\right)=p\left(\mathbf{x}_{t+1} | \mathbf{x}_{t}, \mathbf{u}_{t}\right) \nonumber \\ &amp; D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t}\right) \pi_{\theta}\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right) | q\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)\right)=0. \nonumber \end{align}<br>$$<br>其中第一个和第二个约束在路径优化算法运行的过程中已经默认保证了，实际我们只需要考虑第三个约束<strong>。注意优化目标就是GPS-V1中提到的I-projection，只不过这里没有展开</strong>。我们采用拉格朗日乘子法（或者扩展拉格朗日乘子法）将上述问题转化为一个无约束优化问题：<br>$$<br>\begin{align} \mathcal{L}(\theta, q, \lambda)=&amp; D_{\mathrm{KL}}(q(\tau) | \rho(\tau))+\nonumber\\ &amp; \sum_{t=1}^{T} \lambda_{t} D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t}\right) \pi_{\theta}\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right) | q\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)\right). \end{align}<br>$$<br>而对于上述优化问题，我们可以采用对偶梯度下降法（DGD，或者交替方向乘子法，ADMM）分别更新三部分的参数。而对偶变量通过以下公式更新：<br>$$<br>\lambda_{t} \leftarrow \lambda_{t}+\eta D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t}\right) \pi_{\theta}\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right) | q\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)\right).<br>$$<br>可以看出更新$q$其实就是在采用路径优化算法，更新$\theta$时就是在做监督学习。最后给出算法流程：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/GPS/GPS-V2.png" alt="GPS-V2"></p><h2 id="GPS-V3"><a href="#GPS-V3" class="headerlink" title="GPS-V3"></a>GPS-V3</h2><p>其实我觉得最实用的还是V3版本的GPS算法，因为对于大部分现实问题，环境模型对与算法设计者来说都是未知的。但是前两个版本的GPS算法都假设环境模型是已知的（这样才能运行路径优化算法）。GPS-V3算法尝试解决事先未知环境模型场景下的相关问题，其实其基本思想也很直接，未知模型那我就估计模型，只不过全局模型肯定是很难估计准确的，而且要采用类似iLQR这种路径优化算法要对模型进行线性近似，因而更加不可能采用全局模型了。GPS-V3通过估计局部模型来缓解上述问题，但是采用局部模型又会引入类似GPS-V1这样的问题，在搜索区域距离当前样本过远时，算法误差就会较大，因而GPS-V3在GPS-V2的基础上再加了一个约束来解决这个问题。</p><p>具体来说，GPS-V1是解决如下优化问题来产生指导样本的：<br>$$<br>p(\tau)=\arg \min _{p(\tau) \in \mathcal{N}(\tau)} E_{p}[\ell(\tau)]-\mathcal{H}(p(\tau)) \text { s.t. } p\left(\mathbf{x}_{t+1} | \mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathcal{N}\left(\mathbf{x}_{t+1} ; f_{\mathbf{x} t} \mathbf{x}_{t}+f_{\mathbf{u} t} \mathbf{u}_{t}, \mathbf{F}_{t}\right).<br>$$<br>然后我们先转回到最原始iLQR算法的优化目标，不过因为这个时候我们是用估计的局部模型来运行该算法的，我们加上如下KL散度的约束来使得搜索范围不会离当前样本太远：<br>$$<br>\min _{p(\tau) \in \mathcal{N}(\tau)} E_{p}[\ell(\tau)] \text { s.t. } D_{\mathrm{KL}}(p(\tau) | \hat{p}(\tau)) \leq \epsilon.<br>$$<br>接下来同样采用拉格朗日乘子法（或者扩展拉格朗日乘子法）将其转变为无约束问题：<br>$$<br>\mathcal{L}_{\text { traj }}(p(\tau), \eta)=E_{p}[\ell(\tau)]+\eta\left[D_{\mathrm{KL}}(p(\tau) | \hat{p}(\tau))-\epsilon\right].<br>$$<br>将上式中的KL散度展开：<br>$$<br>\mathcal{L}_{\mathrm{traj}}(p(\tau), \eta)=\left[\sum_{t} E_{p\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)}\left[\ell\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)-\eta \log \hat{p}\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right)\right]\right]-\eta \mathcal{H}(p(\tau))-\eta \epsilon.<br>$$<br>和GPS-V1的优化问题对比，我们可以发现一个优秀的巧合，我们只需要对上式两边除以$\eta$，并将回报函数转变一下：$\tilde{\ell}\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=\frac{1}{\eta} \ell\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)-\log \hat{p}\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right)$，就可以直接采用GPS-V1一样的方法产生指导样本。再将GPS-V2算法引入进来：<br>$$<br>\begin{align} \mathcal{L}(\theta, q, \lambda)=&amp; \mathcal{L}_{\mathrm{traj}}(p(\tau), \eta) +\nonumber\\ &amp; \sum_{t=1}^{T} \lambda_{t} D_{\mathrm{KL}}\left(q\left(\mathbf{x}_{t}\right) \pi_{\theta}\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right) | q\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)\right). \end{align}<br>$$<br>对于上式同样采用GPS-V2一样的DGD方法（或者ADMM算法）求解即可。具体流程如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/GPS/GPS-V3.png" alt="GPS-V3"></p><p>这里需要再提一点，关于估计局部模型采用的方法。由于局部模型是一个高斯分布，我们其实只要去估计其均值即可（方差设定为$Q_{\mathbf{u u} t}^{-1}$）。而均值又是个线性函数，其实只要估计两个梯度即可：<br>$$<br>p\left(\mathbf{x}_{t+1} | \mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathcal{N}\left(\mathbf{A}_{t} \mathbf{x}_{t}+\mathbf{B}_{t} \mathbf{u}_{t}+\mathbf{c}, \mathbf{N}_{t}\right) \quad \mathbf{A}_{t} \approx \frac{d f}{d \mathbf{x}_{t}} \quad \mathbf{B}_{t} \approx \frac{d f}{d \mathbf{u}_{t}}<br>$$<br>因而可以用简单的线性回归方法。当然，对于较复杂的问题，一般会采用<strong>贝叶斯线性回归，选用高斯过程、深度网络或者高斯混合模型作为贝叶斯先验</strong>（这部分有机会我也会展开讲一讲）。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>最后，我引用[^1]文中的一段文字来总结GPS算法的核心思想：</p><blockquote><p>Since each trajectory-centric teacher only needs to solve the task from a single initial state, it is faced with a much easier problem. The final policy is trained with supervised learning, which allows us to use a nonlinear, high-dimensional representation for this final policy, such as a multilayer neural network, in order to learn complex behaviors with good generalization. A key component in guided policy search is adaptation between the trajectories produced by the teacher and the final policy. This adaptation ensures that, at convergence, the teacher does not take actions that the final policy cannot reproduce. This is realized by an alternating optimization procedure, which iteratively optimizes the policy to match each teacher, while the teachers adapt to gradually match the behavior of the final policy.</p></blockquote><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[^1]: Zhang, Marvin, et al. “Learning deep neural network policies with continuous memory states.” <em>2016 IEEE International Conference on Robotics and Automation (ICRA)</em>. IEEE, 2016.</p><p>[^2]: Levine, Sergey, and Vladlen Koltun. “Guided policy search.” <em>International Conference on Machine Learning</em>. 2013.</p><p>[^3]: Levine, Sergey, and Vladlen Koltun. “Learning complex neural network policies with trajectory optimization.” <em>International Conference on Machine Learning</em>. 2014.</p><p>[^4]: Levine, Sergey, and Pieter Abbeel. “Learning neural network policies with guided policy search under unknown dynamics.” <em>Advances in Neural Information Processing Systems</em>. 2014.</p><p>[^5]: Dvijotham, Krishnamurthy, and Emanuel Todorov. “Inverse optimal control with linearly-solvable MDPs.” <em>Proceedings of the 27th International Conference on Machine Learning (ICML-10)</em>. 2010.</p><p>[^6]: Jie, Tang, and Pieter Abbeel. “On a connection between importance sampling and the likelihood ratio policy gradient.” <em>Advances in Neural Information Processing Systems</em>. 2010.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;em&gt;一篇因为各种突发状况断断续续写了将近两周的文章 = =&lt;/em&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;这篇博客将介绍GPS方法，GPS方法是由强化学习大牛Sergey Levine（在最近的ICLR 2019发表了13篇论文）于2013年提出的，目前被作为基础算法广泛应用于各种强化
    
    </summary>
    
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>How to make an optimal decision in the case of knowing the environment model(CS294 lecture notes) ?</title>
    <link href="http://yoursite.com/2019/03/25/How-to-make-an-optimal-decision-in-the-case-of-knowing-the-environment-model-CS294-lecture-notes/"/>
    <id>http://yoursite.com/2019/03/25/How-to-make-an-optimal-decision-in-the-case-of-knowing-the-environment-model-CS294-lecture-notes/</id>
    <published>2019-03-24T17:31:40.000Z</published>
    <updated>2019-03-24T17:33:10.679Z</updated>
    
    <content type="html"><![CDATA[<p>首先我们假设环境是确定性的，即在某个状态执行某个动作之后，转移到的下一个状态是确定的，不存在任何随机性。而在这种情况下，我们想做的是在环境给了我们一个初始状态的条件下，根据我们需要完成的任务以及环境模型，直接得出从初始状态到任务完成状态中间最优的动作序列。因为环境是确定的，而我们又已知环境模型，因而以上想法是自然且可行的。下图展示了我们想做的事情：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/2.png" alt="目标任务"></p><p>现在我们将以上问题抽象成一个正式的优化问题：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/1.png" alt="优化目标"></p><p>其中$f$就代表环境模型。但是一旦环境不再是确定的，即正在某个状态执行某个动作之后，转移到的下一个状态是从一个状态分布中随机采样的。对于这种情况，上述的优化问题就会转变为最大化如下形式的期望值：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/3.png" alt="期望优化目标"></p><p>但是在随机环境下，解决与确定环境的类似的如上优化问题并不能得到与确定环境一样的最优解。原因在于我们只接受环境反馈回来的初始状态，接着便凭借着我们掌握的环境模型在脑海中进行规划。这种方法在确定环境下没有任何问题，但在随机环境下，智能体实际会转移到的状态可能并不符合我们的预期，因为它是从一个条件状态分布中随机采样的。而一旦从某一个状态开始与我们的预期产生偏差，那么后续的所有状态都会产生偏差，而我们设想的最优动作序列便不是最优了。从优化函数的角度来看，我们优化的只是一个期望值，而不是某一次随机采样的值。</p><p>我们把上述解决问题的方法叫做开环方法，与之对应的叫做闭环方法。那么这个“环”具体是指什么呢？具体示意图如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/4.png" alt="环"></p><p>开环方法是只在最开始时接收环境反馈的初始状态，然后开始规划从开始到任务完成的过程中所经历的所有状态对应的最优动作，并不需要一个基于状态产生动作的<strong>策略</strong>；反之，闭环方法在每一个时间步都会接收环境反馈的状态，然后利用一个根据状态输出动作的<strong>策略</strong>来产生一个动作。我们可以看出，对于一个随机环境，闭环方法显然比开环方法更具优势，因为其可以根据所处的状态随时调整自己的动作。</p><p>但接下来我们还是首先假定一个确定性的环境，因而采用开环方法来解决上述问题。下面将介绍三种优化方法：随机优化方法、蒙特卡洛树搜索法以及轨迹优化方法。</p><h2 id="随机优化方法"><a href="#随机优化方法" class="headerlink" title="随机优化方法"></a>随机优化方法</h2><p>对于随机优化方法来讲，上述优化问题可以简化为如下等价优化问题：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/5.png" alt="随机优化目标"></p><p>随机优化方法完全不关系优化目标的特殊结构等信息，而是把任何优化问题都当作上图右半边这样的一般优化问题。</p><h3 id="随即发射方法"><a href="#随即发射方法" class="headerlink" title="随即发射方法"></a>随即发射方法</h3><p>最简单的随机优化方法就是随机瞎猜，即随机选择一个动作序列，然后评估其累积的代价。如上过程不断进行，最后选择一个累积代价最小的动作序列作为上述优化问题的最优解，因而这类方法也叫做“随机发射方法”：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/6.png" alt="随机发射方法"></p><h3 id="CMA算法"><a href="#CMA算法" class="headerlink" title="CMA算法"></a>CMA算法</h3><p>但是这种方法在相对高维的情况下效率会很低，因为搜索空间太大但是目标区域比较小。回顾一下上述方法，我们可以在采样分布上做些文章。假设第一次从一个均匀分布采样一些动作序列之后，得到的累积代价分别为如下情况：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/7.png" alt="随机采样"></p><p>那么下一次我们可以不再继续从一个均匀分布中采样了，我们可以聚焦于累积代价较小（累积回报较大）的区域，然后估计那个区域的分布，在这里我们假设分布是一个高斯分布：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/8.png" alt="估计分布"></p><p>接下来的采样我们就从这个新的分布中进行采样：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/9.png" alt="新的采样结果"></p><p>然后在下一次采样之前，我们再次聚焦于性能更好的区域然后估计其分布：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/10.png" alt="再次估计分布"></p><p>就这样不断迭代，直到满足停止条件。以上方法就是Cross-Entropy Method(CEM)算法，其伪代码如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/11.png" alt="CEM算法"></p><p>该方法还有个进阶版的算法叫做CMA-ES算法，后者可以看作是CMA算法带动量的版本。CMA算法会直接舍弃之前采样的数据点，但是CMA-ES算法会保留部分之前采样的数据点的相关信息，用来指导后续的采样。可以类比一下梯度下降法以及带动量的梯度下降法。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>随机优化方法具有以下优点：</p><ul><li>并行化后效率极高</li><li>实现起来十分简单</li></ul><p>但是也存在如下不可避免地缺点：</p><ul><li>极易受到维度灾难的影响</li><li>只适用于开环情形</li></ul><p>随即优化方法虽然可以同时适用于连续变量以及离散变量的情况，但不是专门为离散情况设计的。下面我们将介绍一种专门为离散动作空间设计的强大的优化方法（严格来讲叫做启发式搜索算法）：蒙特卡洛树搜索MCTS方法。</p><h2 id="蒙特卡洛树搜索算法"><a href="#蒙特卡洛树搜索算法" class="headerlink" title="蒙特卡洛树搜索算法"></a>蒙特卡洛树搜索算法</h2><p>MCTS方法本质是一个搜索算法：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/12.png" alt="搜索算法"></p><p>假设我们想要训练一个智能体能够自动去玩上面这个游戏（击沉敌方潜水艇将会获得分数，但是潜水艇自身的氧气储存量是逐渐减少的，需要不时地去浮出水面补充氧气。被敌方潜水艇撞上会损失生命值，游戏目标就是获得尽可能多的分数）。一个简单的暴力搜索算法可能会包含上图右边的过程，假设一段最优动作序列仅仅包含十个时间步，每个状态下仅仅包含两个可能动作，那么最后一个时间步就包含1024个可能性。但对于大多数问题来说，十个时间步远远不足以完成目标，因而暴力搜索算法是不可行的。</p><p>那么蒙特卡洛算法是如何在不穷举所有可能性直到到达终点的情况下对一个动作序列进行评估的呢？考虑潜水艇游戏，在潜水艇做出攻击指令后，由于炮弹的运行需要时间，因而几个时间步之后敌方潜水艇才会被击沉从而受到奖励，在潜水艇做出攻击指令那个时间步是没有任何奖励的，因而智能体可能认为这个动作并不是一个优秀的动作。对于以上情况，其实我们只需要在做出攻击指令后，如果要评估这个动作的优劣，“等待”几个时间步 即可。蒙特卡洛树搜索算法正是采用这种思想，同样用上图右边的过程举例，当动作执行到第三层时，如何评估这四个动作序列的性能好坏呢？算法进行了某种“等待”，即从第三层开始，不再把树进行完全的扩展了，而是采用一个随机策略随机执行动作直到游戏结束或者到达某个设定的时间步。这就类似于在潜水艇游戏中，潜水艇在发出炮弹后，随机执行一些动作，直到炮弹击中敌方潜水艇。</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/13.png" alt="评估过程"></p><p>而蒙特卡洛算法正是通过这种评估方法来避免暴力搜索，具体来说，蒙特卡洛树搜索算法会根据评估结果的好坏以及访问次数来决定下一步应该搜索哪一条路径：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/14.png" alt="搜索策略"></p><p>可能以上描述有点难以理解，那么下面我们过一遍蒙特卡洛树搜索方法的搜索过程。我们首先给出算法的执行步骤：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/28.png" alt="蒙特卡洛树搜索算法框架"></p><p>首先我们处于一个初始状态：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/15.png" alt="初始状态"></p><p>然后我们进行算法第一步，根据一个“树策略”找到一个叶子节点，注意这里找到一个叶节点的意思是找到一个<em>*新的</em>叶节点。树策略的具体形式如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/29.png" alt="树策略"></p><p>根据以上策略，由于初始状态没有被完全扩展，因而随机选择一个动作，并执行第二步使用<strong>默认策略</strong>来评估执行这个动作的好坏，这里默认策略使用的是随机采样策略：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/16.png" alt="随机选择"></p><p>假设评估结果如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/17.png" alt="评估结果"></p><p>这里Q代表环境定义的回报，N代表访问这个状态的次数。<strong>这里值得注意的是，N记录的并不是某个具体的状态的访问次数，而是执行某个动作的次数，执行这个动作后在随机环境下可能转移到很多个不同的状态，但在树中均显示为一个节点。</strong>评估完之后，我们需要更新根节点到这个新加入的叶节点之间所有节点的Q值以及N值。由于这里两者之间并没有其他的节点，因而跳过这一步。然后以上过程开始循环，<strong>我们再将状态跳回到初始状态</strong>，遵循树策略，找到下一个新的叶节点。由于初始状态还是没有扩展完毕，因此这一次执行下一个未被执行过的状态：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/18.png" alt="第二轮随机选择"></p><p>再采用默认策略对其进行评估，假设我们得到了以下结果：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/19.png" alt="初始状态"></p><p>由于根节点与新的叶节点之间的路径并没有其他节点，因而更新步骤略过。再次重复以上过程，将状态跳回到初始状态，执行树策略找到一个新的叶节点。首先根据树策略，初始状态已经被完全扩展开了（即所有可能的动作均已经执行过），这个时候我们根据树策略中的公式计算每一条路径的一个<strong>分数</strong>。从分数计算公式可以看出，这个分数是同时考虑动作的回报以及动作的执行次数，更加倾向于执行被执行次数少的回报高的动作。在这里，由于两个动作被执行次数均为1，因而我们选择回报更高的第二个动作执行，然后再根据树策略（<strong>在没有找到新的叶节点之前，循环执行树策略</strong>），第二层的状态没有被完全扩展，因而随机选择一个动作执行：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/20.png" alt="再次找到新的叶节点"></p><p>依据默认策略进行评估：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/21.png" alt="采用默认策略评估"></p><p><strong>注意，到了这一步，根节点到新的叶节点之间的路径存在其他节点了，我们就要用最新的叶节点的评估值以及访问次数加到这些中间节点的评估值以及访问次数上：</strong></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/22.png" alt="更新中间节点"></p><p>再次重复上述过程，将状态跳回到初始状态，调用树策略，这时候根据分数计算公式，在假设一些超参数的情况下，我们假定这个时候更加侧重于执行被执行次数更小的动作并评估：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/23.png" alt="侧重被访问次数更少的动作"></p><p>然后再更新再跳回……</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/24.png" alt="循环"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/25.png" alt="循环"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/26.png" alt="循环"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/27.png" alt="循环"></p><p>如果想详细了解蒙特卡洛树搜索算法的扩展以及应用，可以参考下面这篇综述：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/30.png" alt="综述"></p><p>这里讲一个比较有意思的案例：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/31.png" alt="利用蒙特卡洛树搜索算法进行模仿学习"></p><p>其思想其实是将DAgger算法与MCTS算法进行结合。由于DAgger算法需要人工的不断参与进行新数据的标注，以上案例将专家标注的过程用MCTS算法进行替代，学习一个MCTS的策略估计器：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/32.png" alt="DAgger"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/33.png" alt="DAgger with MCTS"></p><p>那么为什么不直接使用MCTS算法呢？其实是基于以下两点考虑的：</p><ul><li>实时性要求较高的任务中MCTS太慢了</li><li>采用类似神经网络的策略估计器具有更好的泛化性</li></ul><h2 id="路径优化算法"><a href="#路径优化算法" class="headerlink" title="路径优化算法"></a>路径优化算法</h2><p>让我们再次回顾以下优化问题：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/1.png" alt="优化目标"></p><p>直接丢弃掉以上优化问题中的特殊结构显然不是十分恰当的，接下来让我们回到一般解决以上优化问题的思路。我们一看到以上问题，就会首先想到能不能利用类似梯度下降的方法呢？为了与最优控制中路径优化算法的一般符号记法一致，我们将以上问题重写为以下形式：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/34.png" alt="路径优化问题的优化目标"></p><p>我们可以将约束部分放进优化函数中从而将以上问题转变为一个无约束问题：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/35.png" alt="无约束形式"></p><p>对于以上问题，只要我们知晓以下四项，即可根据链式法则得出其最优解：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/36.png" alt="需要知道的梯度"></p><h3 id="LQR算法"><a href="#LQR算法" class="headerlink" title="LQR算法"></a>LQR算法</h3><h4 id="确定性环境"><a href="#确定性环境" class="headerlink" title="确定性环境"></a>确定性环境</h4><p>为了解决以上优化问题，我们接下来将介绍一种路径优化算法<strong>LQR</strong>，此算法<strong>假设环境模型是线性的，并且代价函数是二次的</strong>：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/37.png" alt="LQR优化目标"></p><p>为了解决这种特殊形式的以上优化问题，我们采用<strong>动态规划</strong>的思想，<strong>先找出最优的最后一个时间步的动作</strong>。之所以这样做，是因为我们可以发现，以上连加项中只有最后一项是与最后一个时间步的动作相关的。如果我们首先解决第一个时间步的最优动作，那么连加项的所有项都与第一个时间步的动作相关。接下来，我们把最后一项中连续的函数求值简写为$x_{T}$，<strong>注意这个值是未知的</strong>。进行了以上的准备工作后，求解最后一个时间步的最优动作对应的优化目标如下，我们把其记为$Q(x_T,u_T)$：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/38.png" alt="Q值函数"></p><p>然后我们将线性项系数以及二次项系数展开：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/39.png" alt="系数展开"></p><p>然后，为了得出最优动作，我们令这个优化目标关于最后一个时间步动作的梯度等于0：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/40.png" alt="最小化Q值函数"></p><p>求解以上线性方程，可以得出最后一个时间步的最优动作为：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/41.png" alt="最后一个时间步的最优动作"></p><p>将其进行简单的转化，我们可以看出，<strong>最后一个时间步的最优动作是最后一个时间步状态（现在还是未知项）的线性函数（以上关系适用于所有时间步）</strong>：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/42.png" alt="最后一个时间步的动作是状态的线性函数"></p><p>其中：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/43.png" alt="系数的具体形式"></p><p>由于最后一步的最优动作完全可以用最后一步的状态表示，我们可以得出最后一个时间步的最优的Q值，这里我们将其记为V：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/44.png" alt="最后一个时间步的最优Q值"></p><p><strong>这里的Q值以及V值其实是和强化学习中的定义是一致的。</strong>接下来，我们将上式展开：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/45.png" alt="V值展开"></p><p>将上式合并同类项可得：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/46.png" alt="合并同类项"></p><p>其中：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/47.png" alt="系数具体形式"></p><p>因而我们可以得到另一个关系，<strong>最后一个时间步的V值（最优Q值）是最后一个时间步状态的二次函数（以上关系适用于所有时间步）。</strong>进行到这里，我们已经解出最后一个时间步的最优动作了。接下来，我们要在此基础上解出倒数第二个时间步的最优动作。首先我们注意到，倒数第二个时间步的Q值函数可以记为：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/49.png" alt="倒数第二个时间步的Q值函数"></p><p>将环境模型引入可将V值展开：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/48.png" alt="环境模型"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/50.png" alt="展开后的V值"></p><p>我们将展开后的V值代入倒数第二时间步的Q值函数中：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/51.png" alt="带入展开后V值的倒数第二个时间步的Q值函数"></p><p>其中：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/52.png" alt="系数具体形式"></p><p>同样，为了求出倒数第二个时间步的最优动作，我们令相关梯度为零：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/53.png" alt="最小化Q值函数"></p><p>解得倒数第二个时间步的最优动作为：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/54.png" alt="倒数第二个时间步的最优动作"></p><p>其中：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/55.png" alt="系数具体形式"></p><p>让我们不断地重复以上过程，直到第一个时间步。<strong>值得注意的是，由于每一时间步的最优动作与那个时间步的状态有关，但是状态是未知的。</strong></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/56.png" alt="循环以上过程"></p><p>当整个过程回溯到初始时间步时，情况发生了变化，<strong>初始状态我们是已知的！</strong>因而，我们就可以算法初始时间步的最优动作。利用环境模型，我们就可以得知第二个时间步的状态，如此循环下去，我们就可以得知所有时间步的最优动作：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/57.png" alt="前向循环"></p><p>以上就是整个LQR算法的执行过程。</p><h4 id="非确定性环境-（未完成）"><a href="#非确定性环境-（未完成）" class="headerlink" title="非确定性环境 （未完成）"></a>非确定性环境 （未完成）</h4><p>对于非确定性环境，假设我们的环境模型如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/58.png" alt="随机环境模型"></p><p><strong>那么LQR算法依旧是完全可行的。</strong></p><h3 id="iLQR算法"><a href="#iLQR算法" class="headerlink" title="iLQR算法"></a>iLQR算法</h3><p>LQR算法由于假设环境模型以及代价函数是线性以及二次的，表达能力有限，对于更加复杂的任务显然不能很好的估计。因而，解决这个问题的iLQR算法应运而生。其基本思想很简单，既然线性以及二次函数不足以估计全局的真实函数，那么估计局部的总是足够的。因而我们可以<strong>对环境模型以及代价函数分别做一阶以及二阶的泰勒展开！</strong>：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/59.png" alt="泰勒展开"></p><p>那么我们的问题其实又转变回了原始的LQR设定：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/60.png" alt="原始设定"></p><p>其中：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/61.png" alt="参数具体形式"></p><p>iLQR算法的具体框架如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/62.png" alt="iLQR算法"></p><p><strong>该算法之所以采用迭代的形式，是因为其需要不断地用真实样本来去”矫正“其对于环境模型以及代价函数的估计。</strong>更严格来讲，该算法之所以能够达到很好的效果，是因为它和牛顿方法的本质是一样的（通过泰勒展示来估计一个复杂的非线性函数的局部特性）：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/63.png" alt="牛顿方法"></p><p>而如果我们对环境模型估计时也进行二阶泰勒展开：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/64.png" alt="参数具体形式"></p><p>那么我们的算法就变为微分动态规划算法（DDP）。但是在实际情况中，代价函数的形式一般比较简单，因而进行二阶泰勒展开代价不大。但是环境模型一般是十分复杂的，一阶展开还好，一旦进行二阶展开其复杂性将会大大增加。事实表明一阶展开其实是足够的。</p><p>但是以上算法还存在一个问题，考虑以下估计误差：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/65.png" alt="局部估计误差"></p><p>对于这种情况，其实我们只要简单的在原始iLQR算法中加一个line search过程即可：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/66.png" alt="Line Search"></p><p>最后我们看一个iLQR算法在实际情况应用的实例：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/67.png" alt="实例"></p><p>为了保证iLQR更加稳定，这个工作采用了如下形式的改进：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/CS294-112-Fa19-9-26-18/68.png" alt="改进"></p><p><strong>即在每一步都进行一个完整的规划，但是考虑到iLQR的估计误差随着时间会产生累积，因而每次只执行规划的第一步。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先我们假设环境是确定性的，即在某个状态执行某个动作之后，转移到的下一个状态是确定的，不存在任何随机性。而在这种情况下，我们想做的是在环境给了我们一个初始状态的条件下，根据我们需要完成的任务以及环境模型，直接得出从初始状态到任务完成状态中间最优的动作序列。因为环境是确定的，
    
    </summary>
    
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="optimal control" scheme="http://yoursite.com/tags/optimal-control/"/>
    
  </entry>
  
  <entry>
    <title>TD-VAE [ICLR 2019]</title>
    <link href="http://yoursite.com/2019/03/20/TD-VAE/"/>
    <id>http://yoursite.com/2019/03/20/TD-VAE/</id>
    <published>2019-03-20T13:51:58.000Z</published>
    <updated>2019-03-20T13:52:43.904Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>【笔记版】</p><p>今天要讲的是ICLR2019中DeepMind的一个工作，TD-VAE，一个<strong>序列生成模型</strong>。通过引入强化学习中<strong>时序差分</strong>以及<strong>变分自动编码器</strong>，来实现从当前时间步到未来时间步的预测。这里值得注意的是，TD-VAE并不是一个固定时间步的序列生成模型（当然如果训练时喂的训练数据是一个时间间隔固定的序列数据，那么训练出的模型就是固定时间步的序列生成模型），即其生成的数据时间间隔不是一个固定的时间步，而是随机的。如果想生成数据的时间间隔可控，那么可以在前向模型的建模中显式地将时间步作为变量即可。</p><p>这篇论文的作者认为，一个序列生成模型需要具备以下三点属性：</p><ul><li>这个模型应该学习一个数据的抽象<em>状态表示</em>并且在状态空间中进行预测，而不是在观察空间进行预测。</li><li>这个模型应该学习一个<em>置信状态</em>，这个状态需要包含目前为止智能体对于周围环境的所有感知信息。置信状态相当于状态表示的隐变量。</li><li>这个模型应该表现出<em>时序抽象</em>，既能够直接预测多个时间步之后的状态，也能够只通过两个独立的时间点进行训练而不需要中间所有时间点的信息。</li></ul><h1 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h1><p>TD-VAE的目标便是优化以下对数条件似然：<br>$$<br>\log p(x_t|x_{&lt;t})<br>$$<br>这里假设$x_t$可以通过该时间步以及上一个时间步的状态表示$z_t$和$z_{t-1}$推断得出，类似于VAE中损失函数的推导过程，这里同样引入ELBO，具体推导过程如下图：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/2.jpg" alt="推导过程"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/3.jpg" alt="推导过程"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/4.jpg" alt="推导过程"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/5.jpg" alt="推导过程"></p><p>最后的损失函数包含以下几个部分：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/9.png" alt="损失函数1"></p><p>然后我们把两个连续时间步的状态表示换为两个任意时刻的状态表示：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/10.png" alt="损失函数2"></p><p>这实质上是如下VAE的损失函数：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/8.jpg" alt="VAE"></p><p>其中$t2&gt;t1$。整个损失函数可以直观地解释为以下四个部分组成：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/15.png" alt="直观解释1"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/16.png" alt="直观解释2"></p><p>训练时的计算图如下所示：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/11.png" alt="计算图"></p><p>最后在三个不同任务上的实验结果：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/12.png" alt="直观解释1"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/13.png" alt="直观解释1"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/TD-VAE/14.png" alt="直观解释1"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;【笔记版】&lt;/p&gt;&lt;p&gt;今天要讲的是ICLR2019中DeepMind的一个工作，TD-VAE，一个&lt;strong&gt;序列生成模型&lt;/stro
    
    </summary>
    
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="VAE" scheme="http://yoursite.com/tags/VAE/"/>
    
  </entry>
  
  <entry>
    <title>Survey of Sim2Real: Part I</title>
    <link href="http://yoursite.com/2019/03/20/Survey-of-Sim2Real-Part-I/"/>
    <id>http://yoursite.com/2019/03/20/Survey-of-Sim2Real-Part-I/</id>
    <published>2019-03-20T13:50:53.000Z</published>
    <updated>2019-03-20T13:51:30.849Z</updated>
    
    <content type="html"><![CDATA[<p>最近survey了一下sim2real领域最近的相关工作，先整理个第一版（共有七篇论文）的总结。</p><p>整篇总结分为以下四个部分：</p><ul><li>问题的定义以及工作的出发点</li><li>方法的分类</li><li>具体算法</li><li>一个实例</li></ul><h1 id="问题的定义以及工作的出发点"><a href="#问题的定义以及工作的出发点" class="headerlink" title="问题的定义以及工作的出发点"></a>问题的定义以及工作的出发点</h1><p>sim2real的全称是simulation to reality，是强化学习的一个分支，同时也属于transfer learning的一种。主要解决的问题是机器人领域中，直接让机器人或者机械臂在现实环境中与环境进行交互、采样时，会出现以下两个比较严重的问题：</p><ul><li>采样效率太低（在用强化学习算法解决机器人相关问题时，所需要的样本量一般会达到上千万，在现实环境中采集如此数量级的样本要耗费几个月的时间）</li><li>安全问题 （由于强化学习需要通过智能体在环境中进行大范围的随机采样来进行试错，因而在某些时刻其做出的行为可能会损伤机器人自身，例如手臂转动角度过大或者避障任务中由于碰撞造成的不可逆损伤等等；也可能会损害周围的环境甚至生物）</li></ul><p>但是如果我们在模拟器中进行强化学习算法的训练，以上两个问题均可迎刃而解。但是，这里同样会存在一个问题，由于模拟器对于物理环境的建模都是存在误差的，因而在模拟环境中学习到的最优策略是否可以直接在现实环境中应用呢？答案往往是否定的，我们把这个问题称为 “reality gap”。而sim2real的工作就是去尝试解决这个问题。</p><p>这里值得注意的一点是，虽然这个方向叫做sim2real，其实其中的所有的算法都可以直接应用在sim2sim，real2real等的任务中。</p><h1 id="方法的分类"><a href="#方法的分类" class="headerlink" title="方法的分类"></a>方法的分类</h1><p>sim2real中的典型工作大致可以分为以下五类：</p><ul><li><strong>Domain Adaption</strong> 主要是通过学习一个模拟环境以及现实环境共同的状态到隐变量空间的映射，在模拟环境中，使用映射后的状态空间进行算法的训练；因而在迁移到现实环境中时，同样将状态映射到隐含空间后，就可以直接应用在模拟环境训练好的模型了。</li><li><strong>Progressive Network</strong> 利用一类特殊的Progressive Neural Network来进行sim2real。其主要思想类似于cumulative learning，从简单任务逐步过渡到复杂任务（这里可以认为模拟器中的任务总是要比现实任务简单的）。</li><li><strong>Inverse Dynamic Model</strong> 通过在现实环境中学习一个逆转移概率矩阵来直接在现实环境中应用模拟环境中训练好的模型。</li><li><strong>Domain Randomization</strong> 对模拟环境中的视觉信息或者物理参数进行随机化，例如对于避障任务，智能体在一个墙壁颜色、地板颜色等等或者摩擦力、大气压强会随机变化的模拟环境中进行学习。</li></ul><h1 id="具体算法"><a href="#具体算法" class="headerlink" title="具体算法"></a>具体算法</h1><p>这一部分将对以下六篇论文进行详细的说明：</p><ul><li><a href="https://pdfs.semanticscholar.org/a74a/420189a44c82a21f0ae79d0415bc9964116d.pdf?_ga=2.240723076.62241912.1552892977-1147771314.1552892977" target="_blank" rel="external">Towards Adapting Deep Visuomotor Representations from Simulated to Real Environments</a>[arXiv 2015] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao Peng, Sergey Levine, Kate Saenko, Trevor Darrell</li><li><a href="https://arxiv.org/abs/1703.02949" target="_blank" rel="external">Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning</a> [arXiv 2017] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, Sergey Levine</li><li><a href="https://arxiv.org/abs/1610.04286" target="_blank" rel="external">Sim-to-Real Robot Learning from Pixels with Progressive Nets</a> [arXiv 2016] Andrei A. Rusu Deepmind.</li><li><a href="https://arxiv.org/abs/1610.03518" target="_blank" rel="external">Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model</a>[arXiv 2016] Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba</li><li><a href="https://arxiv.org/abs/1710.06537" target="_blank" rel="external">Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</a> [ICRA 2018] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel</li><li><a href="https://arxiv.org/abs/1703.06907" target="_blank" rel="external">Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</a> [IROS 2017] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel</li></ul><h2 id="Towards-Adapting-Deep-Visuomotor-Representations-from-Simulated-to-Real-Environments"><a href="#Towards-Adapting-Deep-Visuomotor-Representations-from-Simulated-to-Real-Environments" class="headerlink" title="Towards Adapting Deep Visuomotor Representations from Simulated to Real Environments"></a>Towards Adapting Deep Visuomotor Representations from Simulated to Real Environments</h2><p>该论文属于 Domain Adaption 类别。</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image32.png" alt="虚拟环境以及现实环境收集到的图像对比"></p><p>如上图，本文的基本思想是，无论是在模拟环境还是在现实环境智能体收集的图像中，对于任务比较重要的便是一些可控制物体或者目标的位置。因而希望学到的隐含表示能够保留这部分物体的位置信息。</p><p>以上是针对图像局部信息的约束。而对于整体图像来说，本文希望模拟环境以及现实环境在这个公共的隐含表示空间中的隐含表示无法被一个二分类器所分辨出来。另外，对于一对图片，例如上图，本文希望这一对图片的隐含表示的欧氏距离能够尽可能接近。</p><p>根据以上三个约束，可以得到以下三个损失函数：</p><p>Pose Estimation Loss:</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image33.png" alt="Pose Estimation Loss"></p><p>Domain Confusion Loss:</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image34.png" alt="Domain Confusion Loss"></p><p>其中</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image35.png" alt="q function"></p><p>Contrastive Loss:</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image36.png" alt="Contrastive Loss"></p><p>其中：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image37.png" alt="D"></p><p>而求解整个问题的最终优化目标即以上三个损失函数的加权求和：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image38.png" alt="Objective Function"></p><p>给出一个更加容易理解的框架图：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image39.png" alt="Architecture"></p><p>但是这种方法存在一个问题，在计算contrastive loss时需要使用一对在模拟环境以及现实环境中能对应上的图片。这种对应关系如果需要人工完成工作量很大而且如何去分辨两张图是否是对应关系也没有一个绝对的标准。因而本文提出了一种无监督方法来自动从数据集中找出这种对应关系，具体来说分为以下五个步骤：</p><ol><li>只使用虚拟环境中收集的图片（进行位置标记）并只是用pose estimation loss训练一个表示学习网络。</li><li>使用上一步训练好的表示学习网络抽取数据集中所有图片（包括仿真环境以及真实环境）的第一个卷积特征图。</li><li>对以上特征图采用5x5的最大池化。</li><li>为每一个仿真-现实图片对计算相似度，即计算其拉直后的特征图的内积。</li><li>每一张真实环境中的图片对应的虚拟环境的图片为相似度最高的那一张。</li></ol><h2 id="Learning-Invariant-Feature-Spaces-to-Transfer-Skills-with-Reinforcement-Learning"><a href="#Learning-Invariant-Feature-Spaces-to-Transfer-Skills-with-Reinforcement-Learning" class="headerlink" title="Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning"></a>Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning</h2><p>这篇论文同样属于 Domain Adaption 领域，即学习一个虚拟环境以及真实环境的状态（state）的公共的隐含表示空间。其整个学习过程分为两步，第一步进行表示学习，第二步采用学习到的表示在现实环境中进行强化学习。</p><p>首先本文对需要解决的问题有如下假设：</p><blockquote><p>Assume that the reward functions share some structural similarity, in that the state distribution of an optimal policy in the source domain will resemble the state distribution of an optimal policy in the target domain when projected into some common feature space.</p></blockquote><p>即当仿真环境以及真实环境的状态同时映射到一个公共的隐含表示空间中，这两个环境所需要解决的问题的回报函数具有一定的相似性。举个例子，我们在仿真环境中构建一个拥有两个关节的机械臂希望它能够将一个冰球推到指定位置，回报函数设计为冰球与目标位置的距离的负值；然后在现实环境中，我们拥有一个有三个关节的机械臂去完成同样的任务。在这个例子中，虽然从智能体获得的图像表示完全不同（一个两关节一个三关节），但是回报函数其实是一样的，与关节数目没有关系。当然这是一个比较极端的例子，回报函数可以不完全一样。</p><p>所以本文的目标是学习两个映射函数，能够将两个环境中的状态映射到一个共同的隐含表示空间，这与上一篇论文只有一个公共的映射函数不同：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image40.png" alt="Common Feature Space"></p><p>而要能通过这个目标来求出两个映射函数，还需要做出以下假设:</p><ul><li>仿真环境以及真实环境的智能体需要学会完成同一个任务</li><li>动作空间一致，状态空间的维度一致</li></ul><p>第一个假设必须存在是由于需要从这个共同的任务中去学习这两个映射函数。这两个假设其实不算很强烈的假设，对于第一个假设来说，这个共同任务可以是一些比较简单的任务，使得训练成本较小；另外对于第二个假设，如果仿真环境以及现实环境中使用的是同一款机器人或者机械臂，动作空间一致以及状态空间的维度一致是一个非常自然的假设。</p><h3 id="表示学习"><a href="#表示学习" class="headerlink" title="表示学习"></a>表示学习</h3><p>要进行如上公式所示的表示学习，我们首先需要对两个环境中的状态（学会的共同任务中的状态）进行对齐（与上一篇论文里的对齐意义是一样的），这里存在两种方法进行对齐：</p><ul><li>Time-bases Alignment</li><li>Dynamic Time Wrapping</li></ul><p>第一种方法非常简单，对于两个环境中的智能体都学会解决的共同任务，如果智能体在仿真环境以及现实环境中动作执行的时钟是大致相同的，那么只要让两个环境中的智能体同时开始执行这个共同的任务，其分别产生的状态序列一定是对齐的；但是时钟相等的假设过于强烈了，因而第二种方法是一个可行性更高的方法。它是一个迭代的方法，它需要一个计算两个序列相似度的距离函数，根据这个距离函数来找出使得两个序列距离最近的对齐方式；对齐后，再根据新的对齐方式更新距离函数，如此不断迭代直至收敛或者到达停止条件。这个方法主要在于如何去选择这个距离函数，本文的做法是首先用time-bases alignment方法得到一个初始的对齐方式，再使用下面要讲到的表示学习的方法学习两个映射函数，将整个序列每一对对齐状态映射后隐含表示向量的欧氏距离的和作为dynamic time warpping方法中序列相似度的距离函数。</p><p>以上就是状态对齐步骤，下面就要进行正式的表示学习了。我们注意到，对于以上公式，其实有个非常简单的解，即这两个映射函数的是个输出永远为0的常数函数。这样一个解显然不是我们需要的，因为我们可以加上一个约束，即学习到的隐含表示能够尽可能多的保留原表示的信息，即学习到的隐含表示是一个auto encoder的隐向量。根据以上假设以及我们的优化目标，可以得到如下表示学习损失函数：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image41.png" alt="Common Feature Spaces Loss"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image42.png" alt="Auto Encoder Losses"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image43.png" alt="Objective Function"></p><p>同样给出一个更容易理解的框架图：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image44.png" alt="Architecture"></p><h3 id="知识迁移"><a href="#知识迁移" class="headerlink" title="知识迁移"></a>知识迁移</h3><p>在进行了第一步的表示学习后，我们需要利用学习到的表示在现实环境中进行新任务的训练。但是注意，我们学习的表示是经过如下两个约束学到的，第一个约束可以认为是一个auto encoder的降维；第二个约束是能够与模拟环境最优策略产生的状态概率分布相同的一个隐含状态表示空间。因而我们不能单单只利用学习到隐含表示去在现实世界中训练，这样在模拟环境中训练好的策略没有办法对现实任务的训练造成任何影响，这个影响必须通过将现实任务的状态序列与模拟环境中最优策略产生的状态序列对齐后才能够实现。</p><p>本文通过对现实环境中智能体需要解决的任务的回报函数的基础上加上如下附加项来实现知识迁移：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image45.png" alt="Addition Reward Term"></p><p>这里的上标$t$表明，在进行现实环境中智能体的训练时，模拟环境必须同步运行。</p><h2 id="Sim-to-Real-Robot-Learning-from-Pixels-with-Progressive-Nets"><a href="#Sim-to-Real-Robot-Learning-from-Pixels-with-Progressive-Nets" class="headerlink" title="Sim-to-Real Robot Learning from Pixels with Progressive Nets"></a>Sim-to-Real Robot Learning from Pixels with Progressive Nets</h2><p>本方法属于 Progressive Network 类别方法，其使用的Progressive Nerual Network是迁移学习领域提出的一种网络结构，其具体形式如下图（左）所示：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image46.png" alt="Progressive Neural Network"></p><p>左图中每一列（column）代表一个独立的任务，任务训练顺序从左到右。虽然任务训练顺序从简单到复杂从直觉上来看是比较合理的，但是PNN并不一定要满足这个规律， 其任务训练顺序可以是任意的。由于我们以第三列（第三个任务）为中心来考虑，因而有实线与虚线的差异。可以看到PNN的思想非常简单，在后面任务的每一层计算时，输入端并上之前任务前一层的输出即可。但是PNN扩展到强化学习中进行了如下三个改变：</p><ol><li>现实环境中使用的神经网络要比模拟环境中要小。原因主要是由于原PNN论文发现，当列数越多时每一列网络的参数都很稀疏，完全可以进行网络压缩或者剪枝。</li><li>输出层不再接受前置任务的输入。由于模拟环境与现实环境在动作空间上可能存在差异，因而在输出层借鉴前面任务的知识反而容易产生误导。</li><li>为了让智能体在现实环境中训练所需的样本量更小，因而输出层的参数直接复制之前任务的参数用来初始化，用以提升算法训练初期的探索度。</li></ol><p>值得注意的是，论文的结果还表明使用LSTM进行策略网络的建模要比使用MLP效果更好。其实还有很多其他工作也同样发现了这一点，主要还是因为大部分现实中的强化学习问题都是部分观察的，不满足马尔可夫性质。</p><h2 id="Transfer-from-Simulation-to-Real-World-through-Learning-Deep-Inverse-Dynamics-Model"><a href="#Transfer-from-Simulation-to-Real-World-through-Learning-Deep-Inverse-Dynamics-Model" class="headerlink" title="Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model"></a>Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model</h2><p>本文属于 Inverse Dynamic Model 类别。其主要基于的假设是即使虚拟环境无法对现实世界进行完全准确的建模，但是其状态的变化还是合理的。例如，对于一个将物体推到指定目标位置的任务来说，一个机械臂将冰球往前推那么下一个状态就是冰球往推动方向前进一些，但是不会往相反的方向移动。基于这个假设，首先在虚拟环境训练好一个策略，其输入是前n个时间步的状态（这里同样考虑到部分观察的问题），将输出的动作输入到虚拟环境模型中，就会转移到虚拟环境中的下一个状态。将这个状态与现实环境中的前n个时间步的状态输入到真实环境中学习到的逆动态模型中，就会得出能够输出这下一个状态所需要采取的动作。具体见下图：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image49.png" alt="训练流程图"></p><p>以上过程唯一需要详细说明的便是如何在现实环境中学习一个逆动态模型，其实非常简单：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image51.png" alt="Inverse Dynamic Model"></p><p>但是这个模型的好坏取决于在现实环境中收集的样本的质量，即样本是否具有足够的多样性从而覆盖足够大的状态空间。一个简单但有效的做法是在探索时的动作上增加一定的噪声，但是加入噪声的频率等需要仔细考量否则就会使得收集到的数据质量下降，论文经过实际的实验得出以下两点经验：</p><ol><li>不需要每个时间步都加入噪声。</li><li>当现实环境中智能体执行动作发生状态转移时转移到一个与虚拟环境差别很大的状态时，就应当即时停止这一轮的采样。</li></ol><h2 id="Sim-to-Real-Transfer-of-Robotic-Control-with-Dynamics-Randomization"><a href="#Sim-to-Real-Transfer-of-Robotic-Control-with-Dynamics-Randomization" class="headerlink" title="Sim-to-Real Transfer of Robotic Control with Dynamics Randomization"></a>Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</h2><p>本文属于 Domain Randomization 类别。本文出发点在于深度强化学习算法具有以下特性：</p><blockquote><p>DeepRL policies are prone to exploiting idiosyncrasies of the simulator to realize behaviours that are infeasible in the real world.</p></blockquote><p>即强化学习算法在一个特定环境中进行学习时，会尝试去挖掘某些专属于这个环境的特性从而使得算法的泛化能力很差。为此，本文将强化学习的优化目标更改如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image52.png" alt="Dynamic Randomization Problem"></p><p>这里的$\mu$代表决定环境的物理参数。如果智能体优化的是在大量不同物理参数确定的虚拟环境中累积回报的期望值的话，训练出的策略就会更加鲁棒。对于一个特定的环境，本文采用HER+RDPG算法进行最优策略的训练：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image53.png" alt="Dynamic Randomization RL Algorithm"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image54.png" alt="54"></p><p>其中的策略网络以及值函数网络采用如下方式进行建模：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image55.png" alt="Netowrk Archtectures"></p><h2 id="Domain-Randomization-for-Transferring-Deep-Neural-Networks-from-Simulation-to-the-Real-World"><a href="#Domain-Randomization-for-Transferring-Deep-Neural-Networks-from-Simulation-to-the-Real-World" class="headerlink" title="Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"></a>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World</h2><p>本文同样属于 Domain Randomization 类别，只不过不同于上一篇论文是随机化物理参数，本文是随机化环境的视觉表示。具体来说，本文是想学习一个定位器，通过输入一张图片来定位其中所有目标物体的三维坐标：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image59.png" alt="Domain Randomization"></p><p>具体需要随机化的视觉信息包括：</p><ul><li>桌子上所有目标物体的位置以及纹理</li><li>桌子、地板、背景以及机械臂的纹理</li><li>摄像机的位置、朝向以及可视范围</li><li>场景中光源的数量</li><li>场景中光源的位置、朝向以及光谱特征</li><li>加入到图像中噪声的类型以及数量</li></ul><h1 id="一个实例"><a href="#一个实例" class="headerlink" title="一个实例"></a>一个实例</h1><p>在这一部分我将介绍一下OpenAI在sim2real领域做出的一个工作，其地位类似于多智能体强化学习领域的OpenAI Five。</p><h2 id="Learning-Dexterous-In-Hand-Manipulation"><a href="#Learning-Dexterous-In-Hand-Manipulation" class="headerlink" title="Learning Dexterous In-Hand Manipulation"></a>Learning Dexterous In-Hand Manipulation</h2><p>这个例子主要用到的技术包括以下几点：虚拟环境的随机化、大规模分布式采样以及精确的虚拟环境搭建。其所需要完成的任务是：使用一个具有20个自由度的机械手，将其手掌中的立方体从初始朝向利用手指翻转到目标朝向：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image68.png" alt="Task"></p><p>为了建立一个足够精细（但是依旧存在无法建模的物理量）的虚拟环境，OpenAI以机械手为球心半径为80厘米的球面上均匀分布了16个精度为20微米的追踪器，能够定位机械手任意位置的微小位移。之所以采用如此高精度的追踪器是为了尽可能准确地对机械手的相关物理参数，例如手指关节处的阻尼等等，这样的物理参数有将近500个。我认为这个工作之所以能够直接将虚拟环境中学习到的最优策略直接应用到现实环境中，这个高精度的虚拟环境功不可没：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image69.png" alt="真实环境 vs 虚拟环境"></p><p>整个系统的训练步骤大致可分为以下三个部分（最后是训练完毕的执行部分）：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image70.png" alt="系统流程图"></p><p>下面将详细对每一个部分进行说明。首先是第一个部分，包括模拟环境中数据的并行采样以及整个强化学习的参数更新框架：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image72.png" alt="分布式数据采集以及参数更新"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image71.png" alt="分布式架构"></p><p>上图中采用的分布式数据收集以及模型训练框架同样也是OpenAI Five所采用的。从左下方开始，多个并行采样的worker会将自己根据当前策略采集的样本发送给与自己相关联的Redis服务器上，模型更新模块中的Puller将会定期异步地从Redis服务器中拉取一个batch的数据并放到RAM中，之后Stager从RAM中拉取一个mini-batch放到GPU上，与其他采用MPI协议联系的GPU一起对参数进行更新。更新后的参数将每个Optimizer都保存一份。之后Optimizer沿着之前相反的路径将更新后的参数存储到Redis服务器上，workers将定期异步地从Redis服务器上拉取最新的策略参数进行采样。整个训练过程就是以上过程的迭代。下图表示了不同规模的并行对于最终算法性能的影响：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image73.png" alt="不同并行度对于最终算法性能的影响"></p><p>第二部分，具体的强化算法选用的是PPO算法，其策略网络以及值函数网络建模如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image75.png" alt="强化学习算法架构"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image74.png" alt="策略网络以及值函数网络结构"></p><p>其中输入部分左边代表机械手的状态，右边代表物体的朝向，具体的维度如下所示：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image76.png" alt="状态空间"></p><p>由于PPO算法的值函数网络只会在训练时使用到，因而采用完整信息对其进行训练。在训练时算法采用了如下三种随机注入方法：</p><ul><li>Dynamic Randomization</li><li>Domain Randomization</li><li>Unmodeled Effects Randomization</li></ul><p>具体见下面三张图：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image77.png" alt="Dynamic Randomization"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image79.png" alt="Domain Randomization"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image78.png" alt="Unmodeled Effects Randomization"></p><p>最后一部分，由于机械手的任务不应该局限于转动方块，还应该包括操纵其他物体。而且由第二部分可知策略网络以及值函数网络的输入可知需要立方体的朝向以及位置信息，目前是通过16个高精度追踪器确定的。OpenAI为了提高整个系统的通用性，因而在方块的周围相隔一定高度120度角均匀放置了三个摄像头，尝试学习一个模型，输入是三张不同角度的图片，输出是立方体的位置以及朝向。这样就可以把立方体换为任意的物体。其具体的网络结构如下：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image81.png" alt="Object Pose Prediction"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/sim2real/v1/image80.png" alt="Pose Prediction Network Architecture"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近survey了一下sim2real领域最近的相关工作，先整理个第一版（共有七篇论文）的总结。&lt;/p&gt;&lt;p&gt;整篇总结分为以下四个部分：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;问题的定义以及工作的出发点&lt;/li&gt;&lt;li&gt;方法的分类&lt;/li&gt;&lt;li&gt;具体算法&lt;/li&gt;&lt;li&gt;一个实例&lt;/li
    
    </summary>
    
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="sim2real" scheme="http://yoursite.com/tags/sim2real/"/>
    
  </entry>
  
  <entry>
    <title>Variational Discreminator Bottlenect [ICLR 2019 Peng et al.]</title>
    <link href="http://yoursite.com/2019/03/20/Variational-Discreminator-Bottlenect/"/>
    <id>http://yoursite.com/2019/03/20/Variational-Discreminator-Bottlenect/</id>
    <published>2019-03-20T13:47:32.000Z</published>
    <updated>2019-03-20T13:49:32.851Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>对抗学习方法今年来被广泛应用于各领域中，但其训练过程极不稳定。由于判别器过于准确将会使得其产生的梯度包含的信息过少从而不能有效地对生成器进行训练，因而有效地平衡判别器以及生成器的性能至关重要。在这篇文章中，作者提出了一个简单但通用的方式来对判别器接收到的信息流采用信息瓶颈进行约束。通过<strong>对判别器的内部状态以及输入的原始数据之间的互信息施加约束</strong>可以有效地控制判别器的准确度从而使得其产生的梯度能够包含对生成器训练更加具有指导意义的信息。作者提出的变分判别器瓶颈能够显著提升<strong>模仿学习</strong>以及<strong>逆强化学习</strong>算法的特性，当然由于其通用性，任何对抗生成模型均可从中受益。</p><h1 id="变分信息瓶颈"><a href="#变分信息瓶颈" class="headerlink" title="变分信息瓶颈"></a>变分信息瓶颈</h1><p>我们从监督学习中的变分信息瓶颈出发。对于监督学习中一个分类任务，存在以下优化目标：<br>$$<br>\min_q \mathbb{E}_{x,y\sim p(x,y)}\left[ -\log q(y|x) \right].<br>$$<br>然而，优化上述目标容易使得训练出的模型过拟合。引入信息瓶颈可以使得模型只关注于输入数据中更加具有判别性的特征。首先我们一如一个编码器$\mathbb{E}(z|x)$将输入数据$x$映射到一个隐含分布中，然后对于编码后的数据以及原数据之间的互信息$I(X,Z)$的上界施加约束，即可得到下面的优化目标：<br>$$<br>\begin{align}<br>J(q,E)=&amp;\min_{q,E} \;\;\mathbb{E}_{x,y \sim p(x,y)} \left[ \mathbb{E}_{z \sim E(z|x)} \left[ -\log q(y|z) \right] \right] \nonumber \\<br>&amp;\text{s.t.}\;\;\;\;I(X,Z) \leq I_c.<br>\end{align}<br>$$<br>我们可以通过变分方法引入互信息的上界，从而推导出上述优化目标的上界，最后通过拉格朗日乘子法将上述带约束的优化问题转变为一个无约束的优化问题，具体推导过程见下图：</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/VDB/1.jpg" alt="推导过程"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/VDB/2.jpg" alt="推导过程"></p><h1 id="变分判别器瓶颈"><a href="#变分判别器瓶颈" class="headerlink" title="变分判别器瓶颈"></a>变分判别器瓶颈</h1><p>接着我们将上述变分信息瓶颈引入到一个标准的生成对抗网络的判别器损失函数上:</p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/VDB/3.jpg" alt="变分判别器瓶颈"></p><p>由于一些生成对抗模仿学习以及对抗逆强化学习算法均采用以上的生成对抗框架，因而可以引入以上变分判别器瓶颈来增强性能。</p><h1 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h1><p>为何引入变分判别器瓶颈可以提高生成对抗模型的性能呢？在生成对抗学习中，如果真实数据分布与生成数据分布具有不相交的支撑集时，一个最优的判别器能够完美分辨两个分布并且其梯度几乎处处为零。因而，当判别器收敛到最优性能时，用以训练生成器的梯度会因此消失。目前一种解决此问题的方法是对判别器的输入数据增加一些连续的噪声，因而使得两个分布在任何地方都拥有连续的支撑集。但是实际上，如果两个分布的距离很大时，增加噪声几乎没有影响。而引入变分判别器瓶颈时，首先编码器将输入映射到一个嵌入空间中并对嵌入表示施加信息瓶颈约束，使得两个分布不仅具有共享的支撑集而且分布之间存在明显的重合（距离不大），同时由于引入信息瓶颈与引入噪声部分等同，使得上述问题得以解决。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;对抗学习方法今年来被广泛应用于各领域中，但其训练过程极不稳定。由于判别器过于准确将会使得其产生的梯度包含的信息过少从而不能有效地对生成器进行
    
    </summary>
    
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="imitation learning" scheme="http://yoursite.com/tags/imitation-learning/"/>
    
      <category term="inverse reinforcement learning" scheme="http://yoursite.com/tags/inverse-reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Accelerate your pandas workflows by changing one line of code</title>
    <link href="http://yoursite.com/2018/10/26/Accelerate-your-pandas-workflows-by-changing-one-line-of-code/"/>
    <id>http://yoursite.com/2018/10/26/Accelerate-your-pandas-workflows-by-changing-one-line-of-code/</id>
    <published>2018-10-26T15:38:34.000Z</published>
    <updated>2018-10-26T15:48:18.748Z</updated>
    
    <content type="html"><![CDATA[<script src="https://gist.github.com/devin-petersohn/2384d06e536df1f14519e18b3ce46ecd.js"></script>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://gist.github.com/devin-petersohn/2384d06e536df1f14519e18b3ce46ecd.js&quot;&gt;&lt;/script&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Curiosity-Driven Learning made easy Part I (Repost)</title>
    <link href="http://yoursite.com/2018/10/17/Curiosity-Driven-Learning-made-easy-Part-I-Repost/"/>
    <id>http://yoursite.com/2018/10/17/Curiosity-Driven-Learning-made-easy-Part-I-Repost/</id>
    <published>2018-10-17T09:13:19.000Z</published>
    <updated>2018-10-17T09:15:07.256Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Curiosity-Driven-Learning-made-easy-Part-I"><a href="#Curiosity-Driven-Learning-made-easy-Part-I" class="headerlink" title="Curiosity-Driven Learning made easy Part I"></a>Curiosity-Driven Learning made easy Part I</h1><blockquote><p>This article is part of Deep Reinforcement Learning Course with Tensorflow 🕹️. Check the syllabus <a href="https://simoninithomas.github.io/Deep_reinforcement_learning_Course/" target="_blank" rel="external">here</a>.</p></blockquote><p><img src="https://cdn-images-1.medium.com/max/2000/0*qvMOs9XAWBAGfCgU.jpg" alt="img"></p><p>OpenAI Five contest</p><p>In the recent years, we’ve seen a lot of innovations in Deep Reinforcement Learning. From <a href="https://deepmind.com/research/dqn/" target="_blank" rel="external">DeepMind and the Deep Q learning architecture</a> in 2014 to <a href="https://blog.openai.com/openai-five/" target="_blank" rel="external">OpenAI playing Dota2 with OpenAI five in 2018</a>, we live in an exciting and promising moment.</p><p>And today we’ll learn about Curiosity-Driven Learning, <strong>one of the most exciting and promising strategy in Deep Reinforcement Learning.</strong></p><p>Reinforcement Learning is based on the <a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419" target="_blank" rel="external">reward hypothesis</a>, which is the idea that each goal can be described as the maximization of the rewards. However, the current problem of extrinsic rewards (aka rewards given by the environment) is that <strong>this function is hard coded by a human, which is not scalable.</strong></p><p>The idea of Curiosity-Driven learning, is to build a reward function that is <strong>intrinsic to the agent</strong> (generated by the agent itself). It means that the agent will be a self-learner since he will be the student but also the feedback master.</p><a id="more"></a><p><img src="https://cdn-images-1.medium.com/max/1600/1*SI8itmr1PZPkXCBtgIh2Sw.png" alt="img"></p><p>Sounds crazy? Yes but that’s a genius idea that was introduced in the 2017 paper <a href="https://pathak22.github.io/noreward-rl/" target="_blank" rel="external">Curiosity-driven Exploration by Self-supervised Prediction</a>. The results were then improved with the second paper <a href="https://pathak22.github.io/large-scale-curiosity/" target="_blank" rel="external">Large-Scale Study of Curiosity-Driven Learning.</a></p><p>They discovered that curiosity driven learning agents perform as good as if they had extrinsic rewards, <strong>and were able to generalize better with unexplored environments.</strong></p><p>In this first article we’ll talk about the theory and explain how works Curiosity Driven Learning in theory.</p><p>Then, in a second article, we’ll implement a Curiosity driven PPO agent playing Super Mario Bros.</p><p>Sounds fun? Let’s dive on in !</p><h3 id="Two-main-problems-in-Reinforcement-Learning"><a href="#Two-main-problems-in-Reinforcement-Learning" class="headerlink" title="Two main problems in Reinforcement Learning"></a>Two main problems in Reinforcement Learning</h3><p>First, the problem of <em>sparse rewards</em>, which is the time difference between an action and its feedback (its reward). An agent learns fast if each of its action has a reward, so that he gets a rapid feedback.</p><p>For instance, if you play Space Invaders, you shoot and kill an enemy, you get a reward. Consequently you’ll understand that this action at that state was good.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*jhrhZm8G1rQfpxEF8l5XoQ.png" alt="img"></p><p>Thanks the reward our agent knows that this action at that state was good</p><p>However, in complex games such as real time strategy games, you will not have a direct reward for each of your actions. Therefore, a bad decision will not have a feedback until hours later.</p><p>For example, if we take Age Of Empires II, we can see on the first image that agent decided to build one barrack and focus on collecting resources. Thus in the second picture (some hours after) the enemies destroyed our barrack consequently we have ton of resources but we can’t create an army so we’re dead.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*c6DjPzsDc9Qd-Iv-_Y8fxg.png" alt="img"></p><p>Enemies destroyed our barrack</p><p>The second big problem is that <em>extrinsic rewards are not scalable</em>. Since in each environment, a human implemented a reward function. But how we can scale that in big and complex environments?</p><p>The solution is to develop a reward function that is intrinsic to the agent (generated by the agent itself). <strong>This reward function will be called curiosity.</strong></p><h3 id="A-new-reward-function-curiosity"><a href="#A-new-reward-function-curiosity" class="headerlink" title="A new reward function: curiosity"></a>A new reward function: curiosity</h3><p>Curiosity is an intrinsic reward that is equal to the <strong>error of our agent to predict the consequence of its own actions given its current state (aka to predict the next state given current state and action taken).</strong></p><p>Why? Because the idea of curiosity is <strong>to encourage our agent to perform actions that reduce the uncertainty in the agent’s ability to predict the consequence of its own action</strong> (uncertainty will be higher <strong>in areas where the agent has spent less time,</strong> or in areas with complex dynamics).</p><p>Consequently measuring error requires <strong>building a model of environmental dynamics that predicts the next state given the current state and the action a.</strong></p><blockquote><p>The question that we can ask here is how we can calculate this error?</p></blockquote><p>To calculate curiosity, we will use a module introduced in the first paper called Intrinsic Curiosity module.</p><h3 id="Introducing-the-Intrinsic-Curiosity-Module"><a href="#Introducing-the-Intrinsic-Curiosity-Module" class="headerlink" title="Introducing the Intrinsic Curiosity Module"></a>Introducing the Intrinsic Curiosity Module</h3><h4 id="The-need-of-a-good-feature-space"><a href="#The-need-of-a-good-feature-space" class="headerlink" title="The need of a good feature space"></a>The need of a good feature space</h4><p>Before diving into the description of the module, we must ask ourselves <strong>how our agent can predict the next state given our current state and our action?</strong></p><p>We know that we can define the curiosity as the error between the predicted new state (st+1) given our state st and action at and the real new state.</p><p>But, remember that most of the time, our state is a stack of 4 frames (pixels). It means that we need to find a way to predict the next stack of frames which is really hard for two reasons:</p><p>First of all, it’s hard to predict the pixels directly, imagine you’re in Doom you move left, you need to predict 248*248 = 61504 pixels!</p><p>Second, the researchers think that’s not the right thing to do and take a good example to prove it.</p><p>Imagine you need to study the movement of the tree leaves in a breeze. First of all, it’s already hard to model breeze, consequently it is much harder to predict the pixel location of each leaves at each time step.</p><p>The problem, is that because you’ll always have a big pixel prediction error, the agent will always be curious even if the movement of the leaves is not the consequence of the agent actions <strong>therefore its continued curiosity is undesirable.</strong></p><p>Trying to predict the movement of each pixel at each timeframe is really hard</p><p>So instead of making prediction in the raw sensory space (pixels), we <strong>need to transform the raw sensory input (array of pixels) into a feature space with only relevant information.</strong></p><p>We need to define what rules must respect a good feature space, there are 3:</p><ul><li>Needs to model things that can be <strong>controlled by the agent.</strong></li><li>Needs also to model things that can’t be controlled by the agent but that <strong>can affect an agent.</strong></li><li>Needs to not model (and consequently be unaffected) by things that are not in agent’s control and have no effect on him.</li></ul><p>Let’s take this example, your agent is a car, if we want to create a good feature representation we need to model:</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*r5i0ZxqEWNE5nvY5thMdYg.png" alt="img"></p><p>The yellow boxes are the important elements</p><p>Our car (controlled by our agent), the other cars (we can’t control it but that can affect the agent) but we don’t need to model the leaves (not affect the agent and we can’t control it). This way we will have a feature representation with less noise.</p><p>The desired embedding space should:</p><ul><li>Be compact in terms of dimensional (remove irrelevant parts of the observation space).</li><li>Preserve sufficient information about the observation.</li><li>Stable: <strong>because non-stationary rewards make it difficult for reinforcement agents to learn.</strong></li></ul><h4 id="Intrinsic-Curiosity-Module-ICM"><a href="#Intrinsic-Curiosity-Module-ICM" class="headerlink" title="Intrinsic Curiosity Module (ICM)"></a>Intrinsic Curiosity Module (ICM)</h4><p><img src="https://cdn-images-1.medium.com/max/1600/1*JHhacgi6jzpzKtReLgNE2w.png" alt="img"></p><p>ICM Taken from the<a href="https://pathak22.github.io/noreward-rl/resources/icml17.pdf" target="_blank" rel="external"> Paper</a></p><p>The Intrinsic Curiosity Module is the system <strong>that helps us to generate curiosity.</strong> It is composed of two neural networks.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*pLDg3MIz5Q6TRsGesVmRwA.png" alt="img"></p><p>Remember, we want to only predict changes in the environment <strong>that could possibly be due to the actions of our agent or affect the agent and ignore the rest.</strong> It means, we need instead of making predictions from a raw sensory space (pixels), transform the sensory input <strong>into a feature vector where only the information relevant to the action performed by the agent is represented.</strong></p><p>To learn this feature space: we <strong>use self-supervision</strong>, training a neural network on a proxy inverse dynamics task of predicting the agent action (ât) given its current and next states (st and st+1).</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*xJehwVNbkI6SdrShiSyCbQ.png" alt="img"></p><p>Inverse Model Part</p><p>Since the neural network is only required to predict the action, <strong>it has no incentive to represent within its feature embedding space, the factors of variation in the environment that does not affect the agent itself.</strong></p><p><img src="https://cdn-images-1.medium.com/max/1600/1*k8gMwh8_ZVgE2bVCKZo_gA.png" alt="img"></p><p>Forward Model Part</p><p>Then we use this feature space to train a forward dynamics model that predicts the future representation of the next state phi(st+1), <strong>given the feature representation of the current state phi(st) and the action at.</strong></p><p>And we provide the prediction error of the forward dynamics model to the agent <strong>as an intrinsic reward to encourage its curiosity.</strong></p><p>Curiosity = predicted_phi(st+1) — phi(st+1)</p><p>So, we have two models in ICM:</p><ul><li><em>Inverse Model</em> (Blue): Encode the states st and st+1 into the feature vectors phi(st) and phi(st+1) that are trained to predict action ât.</li></ul><p><img src="https://cdn-images-1.medium.com/max/1600/1*G7O492AyEu-jlOHHQvTRug.png" alt="img"></p><p><img src="https://cdn-images-1.medium.com/max/1600/1*hw9WW9_DqI2DLiK5GjOSig.png" alt="img"></p><p>Inverse Loss function that measures the difference between the real action and our predicted action</p><ul><li><em>Forward Model</em> (Red): Takes as input phi(st) and at and predict the feature representation phi(st+1) of st+1.</li></ul><p><img src="https://cdn-images-1.medium.com/max/1600/1*EsMzj_wLYR_kx1UZ2fC1dQ.png" alt="img"></p><p><img src="https://cdn-images-1.medium.com/max/1600/1*cmKEatcnl83GRZ8kJriBiQ.png" alt="img"></p><p>Forward Model Loss function</p><p>Then mathematically speaking, curiosity will be the difference between our predicted feature vector of the next state and the real feature vector of the next state.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*-hRqX-e4OEcJlc6jp8rgRw.png" alt="img"></p><p>Finally the overall optimization problem of this module is a composition of Inverse Loss, Forward Loss.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*4BiRJ-_jGRF8N1HRFInBtQ.png" alt="img"></p><p>That’s was a lot of information and mathematics!</p><p>To recap:</p><ul><li>Because of extrinsic rewards implementation and sparse rewards problems, <strong>we want to create a reward that is intrinsic to the agent.</strong></li><li>To do that we created curiosity, <strong>which is the agent’s error in predicting the consequence of its action given its current state.</strong></li><li>Using curiosity will push our agent to <strong>favor transitions with high prediction error</strong> (which will be higher <strong>in areas where the agent has spent less time,</strong> or in areas with complex dynamics) and consequently better explore our environment.</li><li>But because we can’t predict the next state by predicting the next frame (too much complicated), we use a <strong>better feature representation that will keep only elements that can be controlled by our agent or affect our agent.</strong></li><li>To generate curiosity, we use Intrinsic Curiosity module that is composed of two models: <strong>Inverse Model</strong> that is used to learn the feature representation of state and next state and <strong>Forward Dynamics</strong> model used to generate the predicted feature representation of the next state.</li><li>Curiosity will be equal <strong>to the difference between predicted_phi(st+1) (Forward Dynamics model) and phi(st+1) (Inverse Dynamics model)</strong></li></ul><p>That’s all for today! Now that you understood the theory, you should read the two papers experiments results <a href="https://pathak22.github.io/noreward-rl/" target="_blank" rel="external">Curiosity-driven Exploration by Self-supervised Prediction</a> <a href="https://pathak22.github.io/large-scale-curiosity/" target="_blank" rel="external">and Large-Scale Study of Curiosity-Driven Learning.</a></p><p>Next time, we’ll implement a PPO agent using curiosity as intrinsic reward to play Super Mario Bros.</p><hr><p>Source address is <a href="https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359" target="_blank" rel="external">here</a>,</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Curiosity-Driven-Learning-made-easy-Part-I&quot;&gt;&lt;a href=&quot;#Curiosity-Driven-Learning-made-easy-Part-I&quot; class=&quot;headerlink&quot; title=&quot;Curiosity-Driven Learning made easy Part I&quot;&gt;&lt;/a&gt;Curiosity-Driven Learning made easy Part I&lt;/h1&gt;&lt;blockquote&gt;&lt;p&gt;This article is part of Deep Reinforcement Learning Course with Tensorflow 🕹️. Check the syllabus &lt;a href=&quot;https://simoninithomas.github.io/Deep_reinforcement_learning_Course/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2000/0*qvMOs9XAWBAGfCgU.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;p&gt;OpenAI Five contest&lt;/p&gt;&lt;p&gt;In the recent years, we’ve seen a lot of innovations in Deep Reinforcement Learning. From &lt;a href=&quot;https://deepmind.com/research/dqn/&quot;&gt;DeepMind and the Deep Q learning architecture&lt;/a&gt; in 2014 to &lt;a href=&quot;https://blog.openai.com/openai-five/&quot;&gt;OpenAI playing Dota2 with OpenAI five in 2018&lt;/a&gt;, we live in an exciting and promising moment.&lt;/p&gt;&lt;p&gt;And today we’ll learn about Curiosity-Driven Learning, &lt;strong&gt;one of the most exciting and promising strategy in Deep Reinforcement Learning.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Reinforcement Learning is based on the &lt;a href=&quot;https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419&quot;&gt;reward hypothesis&lt;/a&gt;, which is the idea that each goal can be described as the maximization of the rewards. However, the current problem of extrinsic rewards (aka rewards given by the environment) is that &lt;strong&gt;this function is hard coded by a human, which is not scalable.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The idea of Curiosity-Driven learning, is to build a reward function that is &lt;strong&gt;intrinsic to the agent&lt;/strong&gt; (generated by the agent itself). It means that the agent will be a self-learner since he will be the student but also the feedback master.&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Validation Checklist in Kaggle Competition</title>
    <link href="http://yoursite.com/2018/10/16/Validation-Checklist-in-Kaggle-Competition/"/>
    <id>http://yoursite.com/2018/10/16/Validation-Checklist-in-Kaggle-Competition/</id>
    <published>2018-10-16T13:47:42.000Z</published>
    <updated>2018-10-16T13:52:22.383Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Data Splitting Strategies</strong></p><ul><li>Random</li><li>Timewise</li><li>By id (maybe hidden)</li><li>Combined</li></ul><p><strong>Notices</strong></p><ul><li><strong>Make sure the strategy used by train/val splitting is same as train/test splitting.</strong></li><li><strong>Different models trained from different data splitting strategies have much performance gap.</strong></li><li><strong>Logic of feature generation depends on the data splitting strategy.</strong></li></ul><p><strong>Validation problems</strong></p><ul><li>Validation stage<ul><li>Causes of different scores and optimal parameters<ul><li>Too little data</li><li>Too diverse and inconsistent data</li></ul></li><li>Solutions<ul><li>Average scores from different K-Fold splits</li><li>Tune model on one split and evaluate score on the other</li></ul></li></ul></li><li>Submission stage<ul><li>We can observe that<ul><li>LB score is consistently higher/lower than validation score</li><li>LB score is not correlated with validation score at all</li></ul></li><li>Causes<ul><li>We may already have quite different scores in K-Fold<ul><li>make sure split train/validation correct</li></ul></li><li>too litter data in public LB<ul><li>Just trust your validation scores</li></ul></li><li>train and test data are from different distributions<ul><li>classes show in the test set not show in the train set<ul><li>make a shift to your prediction (mean of train minus mean of test) – LB probing</li></ul></li><li>classes ratio is not same<ul><li>make the validation classes ratio is same as test classes ratio</li></ul></li></ul></li></ul></li></ul></li></ul><p><strong>Expect LB shuffle because of</strong></p><ul><li>Randomness</li><li>Litter amount of data</li><li>Different public/private distributions</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Data Splitting Strategies&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Random&lt;/li&gt;&lt;li&gt;Timewise&lt;/li&gt;&lt;li&gt;By id (maybe hidden)&lt;/li&gt;&lt;li&gt;Combined&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="kaggle" scheme="http://yoursite.com/tags/kaggle/"/>
    
  </entry>
  
  <entry>
    <title>Coursera-dl plugin issues on Windows 10</title>
    <link href="http://yoursite.com/2018/10/16/Coursera-dl-plugin-issues-on-Windows-10/"/>
    <id>http://yoursite.com/2018/10/16/Coursera-dl-plugin-issues-on-Windows-10/</id>
    <published>2018-10-16T12:28:11.000Z</published>
    <updated>2018-10-16T12:29:41.525Z</updated>
    
    <content type="html"><![CDATA[<h1 id="I-can-x27-t-download-the-video"><a href="#I-can-x27-t-download-the-video" class="headerlink" title="I can&#x27;t download the video ~ "></a><a href="https://github.com/coursera-dl/coursera-dl/issues/606" target="_blank" rel="external">I can&#x27;t download the video ~</a></h1><blockquote><p>state: <strong>closed</strong> opened by: <strong>jenkey2011</strong> on: <strong>2017-05-04</strong></p></blockquote><h1 id="I-can-x27-t-download-the-video-1"><a href="#I-can-x27-t-download-the-video-1" class="headerlink" title="I can&#x27;t download the video ~"></a>I can&#x27;t download the video ~</h1><h3 id="Your-environment"><a href="#Your-environment" class="headerlink" title="Your environment"></a>Your environment</h3><ul><li>win10</li><li>Python version : 3.6</li><li>coursera-dl version:0.8</li><li>PS：I&#x27;m from China……<h3 id="Steps-to-reproduce"><a href="#Steps-to-reproduce" class="headerlink" title="Steps to reproduce"></a>Steps to reproduce</h3>&gt; coursera-dl -u xxx -p xxxx -b html-css-javascript</li></ul><p>Then it works , but only the subtitles were downloaded ; And the cmd shows &quot;The following URLs (64) could not be downloaded:&quot; , they&#x27;re all video links;</p><h3 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h3><hr><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-301991709" target="_blank" rel="external"><strong>wanghoppe</strong></a> on: <strong>2017-05-17</strong></p></blockquote><h2 id="Me-too-have-the-issue…-Could-someone-help"><a href="#Me-too-have-the-issue…-Could-someone-help" class="headerlink" title="Me too have the issue…. Could someone help??"></a>Me too have the issue…. Could someone help??</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-302359248" target="_blank" rel="external"><strong>lvhuiyang</strong></a> on: <strong>2017-05-18</strong></p></blockquote><p>I met the same problem yesterday.</p><p>I use the command &#x60;coursera-dl -u xx@xxx.com -p xxx course_name –wget&#x60; to solve it.</p><h2 id="env-Mac-OS-wget-python3-6"><a href="#env-Mac-OS-wget-python3-6" class="headerlink" title=" (env: Mac OS, wget, python3.6)"></a> (env: Mac OS, wget, python3.6)</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-302361973" target="_blank" rel="external"><strong>balta2ar</strong></a> on: <strong>2017-05-18</strong></p></blockquote><h2 id="Are-you-guys-all-from-China-Did-you-try-downloading-over-a-VPN-or-Tor-If-it-x27-s-your-government-x27-s-firewall-we-can-x27-t-do-anything-about-it-use-proxies-VPNs-and-Tor"><a href="#Are-you-guys-all-from-China-Did-you-try-downloading-over-a-VPN-or-Tor-If-it-x27-s-your-government-x27-s-firewall-we-can-x27-t-do-anything-about-it-use-proxies-VPNs-and-Tor" class="headerlink" title="Are you guys all from China? Did you try downloading over a VPN or Tor? If it&#x27;s your government&#x27;s firewall, we can&#x27;t do anything about it, use proxies, VPNs and Tor."></a>Are you guys all from China? Did you try downloading over a VPN or Tor? If it&#x27;s your government&#x27;s firewall, we can&#x27;t do anything about it, use proxies, VPNs and Tor.</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-302579356" target="_blank" rel="external"><strong>jenkey2011</strong></a> on: <strong>2017-05-19</strong></p></blockquote><h2 id="I-guess-so……-What-a-pity"><a href="#I-guess-so……-What-a-pity" class="headerlink" title="I guess so…… What a pity."></a>I guess so…… What a pity.</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-305698809" target="_blank" rel="external"><strong>FBryce</strong></a> on: <strong>2017-06-02</strong></p></blockquote><h2 id="Hi-If-you-are-from-China-adding-quot-52-84-246-72-d3c33hcgiwev3-cloudfront-net-quot-in-the-host-file-and-fresh-dns-with-quot-ipconfig-flushdns-quot-may-work"><a href="#Hi-If-you-are-from-China-adding-quot-52-84-246-72-d3c33hcgiwev3-cloudfront-net-quot-in-the-host-file-and-fresh-dns-with-quot-ipconfig-flushdns-quot-may-work" class="headerlink" title="Hi, If you are from China, adding  &quot;52.84.246.72  d3c33hcgiwev3.cloudfront.net&quot; in the host file and fresh dns with &quot; ipconfig/flushdns&quot;  may work "></a>Hi, If you are from China, adding &quot;52.84.246.72 d3c33hcgiwev3.cloudfront.net&quot; in the host file and fresh dns with &quot; ipconfig/flushdns&quot; may work</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-305716556" target="_blank" rel="external"><strong>wanghoppe</strong></a> on: <strong>2017-06-02</strong></p></blockquote><a id="more"></a><h2 id="FBryce-It-worked-Thank-you-so-much"><a href="#FBryce-It-worked-Thank-you-so-much" class="headerlink" title="@FBryce It worked! Thank you so much."></a>@FBryce It worked! Thank you so much.</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-318615319" target="_blank" rel="external"><strong>XiangBicheng</strong></a> on: <strong>2017-07-28</strong></p></blockquote><h2 id="FBryce-Nice-Thank-you-for-useful-information"><a href="#FBryce-Nice-Thank-you-for-useful-information" class="headerlink" title="@FBryce Nice! Thank you for useful information."></a>@FBryce Nice! Thank you for useful information.</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-324089509" target="_blank" rel="external"><strong>techlarry</strong></a> on: <strong>2017-08-23</strong></p></blockquote><h2 id="Good-job-FBryce-I-downloaded-several-courses-manually-because-of-the-issue-It-wasted-me-at-least-half-day-to-download-files…"><a href="#Good-job-FBryce-I-downloaded-several-courses-manually-because-of-the-issue-It-wasted-me-at-least-half-day-to-download-files…" class="headerlink" title="Good job @FBryce, I downloaded several courses manually because of the issue. It wasted me at least half day to download files…."></a>Good job @FBryce, I downloaded several courses manually because of the issue. It wasted me at least half day to download files….</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-325155491" target="_blank" rel="external"><strong>balta2ar</strong></a> on: <strong>2017-08-27</strong></p></blockquote><p>&gt; Hi, If you are from China, adding &quot;52.84.246.72 d3c33hcgiwev3.cloudfront.net&quot; in the host file and fresh dns with &quot; ipconfig/flushdns&quot; may work</p><h2 id="I-x27-ve-added-your-comment-to-the-readme-Thanks-Closing"><a href="#I-x27-ve-added-your-comment-to-the-readme-Thanks-Closing" class="headerlink" title="I&#x27;ve added your comment to the readme. Thanks! Closing."></a>I&#x27;ve added your comment to the readme. Thanks! Closing.</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-353938932" target="_blank" rel="external"><strong>wenxingxing</strong></a> on: <strong>2017-12-26</strong></p></blockquote><h2 id="It-works-thanks"><a href="#It-works-thanks" class="headerlink" title="It works, thanks~~"></a>It works, thanks~~</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-423740448" target="_blank" rel="external"><strong>1c7</strong></a> on: <strong>2018-09-22</strong></p></blockquote><p><img src="https://user-images.githubusercontent.com/1804755/45917239-53d2b800-bea5-11e8-91b0-4c421b5b9cb8.png" alt="image"></p><h2 id="Still-work-in-2018-Thanks"><a href="#Still-work-in-2018-Thanks" class="headerlink" title="Still work in 2018, Thanks!"></a>Still work in 2018, Thanks!</h2><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-427588031" target="_blank" rel="external"><strong>shuoooo</strong></a> on: <strong>2018-10-07</strong></p></blockquote><p>&gt; Hi, If you are from China, adding &quot;52.84.246.72 d3c33hcgiwev3.cloudfront.net&quot; in the host file and fresh dns with &quot; ipconfig/flushdns&quot; may work</p><p>Works beautifully, thanks a lot</p><h1 id="Could-not-authenticate-Cannot-login-on-coursera-org-400-Client-Error-Bad-Request-for-url-https-api-coursera-org-api-login-v3"><a href="#Could-not-authenticate-Cannot-login-on-coursera-org-400-Client-Error-Bad-Request-for-url-https-api-coursera-org-api-login-v3" class="headerlink" title="Could not authenticate: Cannot login on coursera.org: 400 Client Error: Bad Request for url: https://api.coursera.org/api/login/v3"></a><a href="https://github.com/coursera-dl/coursera-dl/issues/670" target="_blank" rel="external">Could not authenticate: Cannot login on coursera.org: 400 Client Error: Bad Request for url: https://api.coursera.org/api/login/v3</a></h1><blockquote><p>state: <strong>closed</strong> opened by: <strong>jeet-parekh</strong> on: <strong>2018-06-15</strong></p></blockquote><h3 id="Subject-of-the-issue"><a href="#Subject-of-the-issue" class="headerlink" title="Subject of the issue"></a>Subject of the issue</h3><p>Running coursera-dl gives an error 400.</p><h3 id="Your-environment-1"><a href="#Your-environment-1" class="headerlink" title="Your environment"></a>Your environment</h3><ul><li>Operating System (name/version): Windows 8.1</li><li>Python version: 3.6.5</li><li>coursera-dl version: 0.11.2</li></ul><h3 id="Steps-to-reproduce-1"><a href="#Steps-to-reproduce-1" class="headerlink" title="Steps to reproduce"></a>Steps to reproduce</h3><p>I ran the command &#x60;coursera-dl -u myusername -p mypassword machine-learning&#x60;.</p><ul><li>Is the problem happening with the latest version of the script?<br>Yes.</li><li>Do you have all the recommended versions of the modules? See them in the<br>file &#x60;requirements.txt&#x60;.<br>Yes. I did a &#x60;pip install&#x60;.</li><li>What is the course that you are trying to access?<br>machine-learning</li><li>What is the precise command line that you are using (don&#x27;t forget to obfuscate<br>your username and password, but leave all other information untouched).<br>&#x60;coursera-dl -u myusername -p mypassword machine-learning&#x60;</li><li>What are the precise messages that you get? Please, use the &#x60;–debug&#x60;<br>option before posting the messages as a bug report. Please, copy and paste<br>them. Don&#x27;t reword/paraphrase the messages.</li></ul><p>&#x60;&#x60;&#x60;<br>root[main] coursera_dl version 0.11.2<br>root[main] Downloading class: machine-learning (1 / 1)<br>root[download_class] Downloading new style (on demand) class machine-learning<br>root[login] Initiating login.<br>root[login] There were no .coursera.org cookies to be cleared.<br>root[prepape_auth_headers] Forging cookie header: csrftoken&#x3D;rgrpC7s9fPPIdLTaWeGA; csrf2_token_doUFgKoj&#x3D;G39Y5Rvw4XFBwlb8W8cN5SEM.<br>urllib3.connectionpool[_new_conn] Starting new HTTPS connection (1): api.coursera.org<br>urllib3.connectionpool[_make_request] <a href="https://api.coursera.org:443" target="_blank" rel="external">https://api.coursera.org:443</a> &quot;POST /api/login/v3 HTTP/1.1&quot; 400 None<br>root[main] Could not authenticate: Cannot login on coursera.org: 400 Client Error: Bad Request for url: <a href="https://api.coursera.org/api/login/v3" target="_blank" rel="external">https://api.coursera.org/api/login/v3</a><br>&#x60;&#x60;&#x60;</p><h3 id="Expected-behaviour"><a href="#Expected-behaviour" class="headerlink" title="Expected behaviour"></a>Expected behaviour</h3><p>There would be no errors and the course would download.</p><h3 id="Actual-behaviour"><a href="#Actual-behaviour" class="headerlink" title="Actual behaviour"></a>Actual behaviour</h3><p>The output given above.</p><h3 id="Comments-1"><a href="#Comments-1" class="headerlink" title="Comments"></a>Comments</h3><hr><blockquote><p>from: <a href="https://github.com/coursera-dl/coursera-dl/issues/670#issuecomment-397850484" target="_blank" rel="external"><strong>jeet-parekh</strong></a> on: <strong>2018-06-17</strong></p></blockquote><p>It worked when I used my email id as the user name.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;I-can-x27-t-download-the-video&quot;&gt;&lt;a href=&quot;#I-can-x27-t-download-the-video&quot; class=&quot;headerlink&quot; title=&quot;I can&amp;#x27;t download the video ~ &quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/coursera-dl/coursera-dl/issues/606&quot;&gt;I can&amp;#x27;t download the video ~&lt;/a&gt;&lt;/h1&gt;&lt;blockquote&gt;&lt;p&gt;state: &lt;strong&gt;closed&lt;/strong&gt; opened by: &lt;strong&gt;jenkey2011&lt;/strong&gt; on: &lt;strong&gt;2017-05-04&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h1 id=&quot;I-can-x27-t-download-the-video-1&quot;&gt;&lt;a href=&quot;#I-can-x27-t-download-the-video-1&quot; class=&quot;headerlink&quot; title=&quot;I can&amp;#x27;t download the video ~&quot;&gt;&lt;/a&gt;I can&amp;#x27;t download the video ~&lt;/h1&gt;&lt;h3 id=&quot;Your-environment&quot;&gt;&lt;a href=&quot;#Your-environment&quot; class=&quot;headerlink&quot; title=&quot;Your environment&quot;&gt;&lt;/a&gt;Your environment&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;win10&lt;/li&gt;&lt;li&gt;Python version : 3.6&lt;/li&gt;&lt;li&gt;coursera-dl version:0.8&lt;/li&gt;&lt;li&gt;PS：I&amp;#x27;m from China……&lt;h3 id=&quot;Steps-to-reproduce&quot;&gt;&lt;a href=&quot;#Steps-to-reproduce&quot; class=&quot;headerlink&quot; title=&quot;Steps to reproduce&quot;&gt;&lt;/a&gt;Steps to reproduce&lt;/h3&gt;&amp;gt; coursera-dl -u xxx -p xxxx -b html-css-javascript&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Then it works , but only the subtitles were downloaded ; And the cmd shows &amp;quot;The following URLs (64) could not be downloaded:&amp;quot; , they&amp;#x27;re all video links;&lt;/p&gt;&lt;h3 id=&quot;Comments&quot;&gt;&lt;a href=&quot;#Comments&quot; class=&quot;headerlink&quot; title=&quot;Comments&quot;&gt;&lt;/a&gt;Comments&lt;/h3&gt;&lt;hr&gt;&lt;blockquote&gt;&lt;p&gt;from: &lt;a href=&quot;https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-301991709&quot;&gt;&lt;strong&gt;wanghoppe&lt;/strong&gt;&lt;/a&gt; on: &lt;strong&gt;2017-05-17&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;Me-too-have-the-issue…-Could-someone-help&quot;&gt;&lt;a href=&quot;#Me-too-have-the-issue…-Could-someone-help&quot; class=&quot;headerlink&quot; title=&quot;Me too have the issue…. Could someone help??&quot;&gt;&lt;/a&gt;Me too have the issue…. Could someone help??&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;from: &lt;a href=&quot;https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-302359248&quot;&gt;&lt;strong&gt;lvhuiyang&lt;/strong&gt;&lt;/a&gt; on: &lt;strong&gt;2017-05-18&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;I met the same problem yesterday.&lt;/p&gt;&lt;p&gt;I use the command &amp;#x60;coursera-dl -u xx@xxx.com -p xxx course_name –wget&amp;#x60; to solve it.&lt;/p&gt;&lt;h2 id=&quot;env-Mac-OS-wget-python3-6&quot;&gt;&lt;a href=&quot;#env-Mac-OS-wget-python3-6&quot; class=&quot;headerlink&quot; title=&quot; (env: Mac OS, wget, python3.6)&quot;&gt;&lt;/a&gt; (env: Mac OS, wget, python3.6)&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;from: &lt;a href=&quot;https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-302361973&quot;&gt;&lt;strong&gt;balta2ar&lt;/strong&gt;&lt;/a&gt; on: &lt;strong&gt;2017-05-18&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;Are-you-guys-all-from-China-Did-you-try-downloading-over-a-VPN-or-Tor-If-it-x27-s-your-government-x27-s-firewall-we-can-x27-t-do-anything-about-it-use-proxies-VPNs-and-Tor&quot;&gt;&lt;a href=&quot;#Are-you-guys-all-from-China-Did-you-try-downloading-over-a-VPN-or-Tor-If-it-x27-s-your-government-x27-s-firewall-we-can-x27-t-do-anything-about-it-use-proxies-VPNs-and-Tor&quot; class=&quot;headerlink&quot; title=&quot;Are you guys all from China? Did you try downloading over a VPN or Tor? If it&amp;#x27;s your government&amp;#x27;s firewall, we can&amp;#x27;t do anything about it, use proxies, VPNs and Tor.&quot;&gt;&lt;/a&gt;Are you guys all from China? Did you try downloading over a VPN or Tor? If it&amp;#x27;s your government&amp;#x27;s firewall, we can&amp;#x27;t do anything about it, use proxies, VPNs and Tor.&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;from: &lt;a href=&quot;https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-302579356&quot;&gt;&lt;strong&gt;jenkey2011&lt;/strong&gt;&lt;/a&gt; on: &lt;strong&gt;2017-05-19&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;I-guess-so……-What-a-pity&quot;&gt;&lt;a href=&quot;#I-guess-so……-What-a-pity&quot; class=&quot;headerlink&quot; title=&quot;I guess so…… What a pity.&quot;&gt;&lt;/a&gt;I guess so…… What a pity.&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;from: &lt;a href=&quot;https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-305698809&quot;&gt;&lt;strong&gt;FBryce&lt;/strong&gt;&lt;/a&gt; on: &lt;strong&gt;2017-06-02&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h2 id=&quot;Hi-If-you-are-from-China-adding-quot-52-84-246-72-d3c33hcgiwev3-cloudfront-net-quot-in-the-host-file-and-fresh-dns-with-quot-ipconfig-flushdns-quot-may-work&quot;&gt;&lt;a href=&quot;#Hi-If-you-are-from-China-adding-quot-52-84-246-72-d3c33hcgiwev3-cloudfront-net-quot-in-the-host-file-and-fresh-dns-with-quot-ipconfig-flushdns-quot-may-work&quot; class=&quot;headerlink&quot; title=&quot;Hi, If you are from China, adding  &amp;quot;52.84.246.72  d3c33hcgiwev3.cloudfront.net&amp;quot; in the host file and fresh dns with &amp;quot; ipconfig/flushdns&amp;quot;  may work &quot;&gt;&lt;/a&gt;Hi, If you are from China, adding &amp;quot;52.84.246.72 d3c33hcgiwev3.cloudfront.net&amp;quot; in the host file and fresh dns with &amp;quot; ipconfig/flushdns&amp;quot; may work&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;from: &lt;a href=&quot;https://github.com/coursera-dl/coursera-dl/issues/606#issuecomment-305716556&quot;&gt;&lt;strong&gt;wanghoppe&lt;/strong&gt;&lt;/a&gt; on: &lt;strong&gt;2017-06-02&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>EDA Example I (Springleaf competition)</title>
    <link href="http://yoursite.com/2018/10/16/EDA-Example-I-Springleaf-competition/"/>
    <id>http://yoursite.com/2018/10/16/EDA-Example-I-Springleaf-competition/</id>
    <published>2018-10-16T10:19:20.000Z</published>
    <updated>2018-10-16T10:28:20.153Z</updated>
    
    <content type="html"><![CDATA[<p>This is a notebook, used in the screencast video. Note, that the data files are not present here in Jupyter hub and you will not be able to run it. But you can always download the notebook to your local machine as well as the competition data and make it interactive.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </div><div class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm_notebook</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">%matplotlib inline</div><div class="line"></div><div class="line"><span class="keyword">import</span> warnings</div><div class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</div><div class="line"></div><div class="line"><span class="keyword">import</span> seaborn</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">autolabel</span><span class="params">(arrayA)</span>:</span></div><div class="line">    <span class="string">''' label each colored square with the corresponding data value. </span></div><div class="line">    If value &gt; 20, the text is in black, else in white.</div><div class="line">    '''</div><div class="line">    arrayA = np.array(arrayA)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(arrayA.shape[<span class="number">0</span>]):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(arrayA.shape[<span class="number">1</span>]):</div><div class="line">                plt.text(j,i, <span class="string">"%.2f"</span>%arrayA[i,j], ha=<span class="string">'center'</span>, va=<span class="string">'bottom'</span>,color=<span class="string">'w'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">hist_it</span><span class="params">(feat)</span>:</span></div><div class="line">    plt.figure(figsize=(<span class="number">16</span>,<span class="number">4</span>))</div><div class="line">    feat[Y==<span class="number">0</span>].hist(bins=range(int(feat.min()),int(feat.max()+<span class="number">2</span>)),normed=<span class="keyword">True</span>,alpha=<span class="number">0.8</span>)</div><div class="line">    feat[Y==<span class="number">1</span>].hist(bins=range(int(feat.min()),int(feat.max()+<span class="number">2</span>)),normed=<span class="keyword">True</span>,alpha=<span class="number">0.5</span>)</div><div class="line">    plt.ylim((<span class="number">0</span>,<span class="number">1</span>))</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gt_matrix</span><span class="params">(feats,sz=<span class="number">16</span>)</span>:</span></div><div class="line">    a = []</div><div class="line">    <span class="keyword">for</span> i,c1 <span class="keyword">in</span> enumerate(feats):</div><div class="line">        b = [] </div><div class="line">        <span class="keyword">for</span> j,c2 <span class="keyword">in</span> enumerate(feats):</div><div class="line">            mask = (~train[c1].isnull()) &amp; (~train[c2].isnull())</div><div class="line">            <span class="keyword">if</span> i&gt;=j:</div><div class="line">                b.append((train.loc[mask,c1].values&gt;=train.loc[mask,c2].values).mean())</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                b.append((train.loc[mask,c1].values&gt;train.loc[mask,c2].values).mean())</div><div class="line"></div><div class="line">        a.append(b)</div><div class="line"></div><div class="line">    plt.figure(figsize = (sz,sz))</div><div class="line">    plt.imshow(a, interpolation = <span class="string">'None'</span>)</div><div class="line">    _ = plt.xticks(range(len(feats)),feats,rotation = <span class="number">90</span>)</div><div class="line">    _ = plt.yticks(range(len(feats)),feats,rotation = <span class="number">0</span>)</div><div class="line">    autolabel(a)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">hist_it1</span><span class="params">(feat)</span>:</span></div><div class="line">    plt.figure(figsize=(<span class="number">16</span>,<span class="number">4</span>))</div><div class="line">    feat[Y==<span class="number">0</span>].hist(bins=<span class="number">100</span>,range=(feat.min(),feat.max()),normed=<span class="keyword">True</span>,alpha=<span class="number">0.5</span>)</div><div class="line">    feat[Y==<span class="number">1</span>].hist(bins=<span class="number">100</span>,range=(feat.min(),feat.max()),normed=<span class="keyword">True</span>,alpha=<span class="number">0.5</span>)</div><div class="line">    plt.ylim((<span class="number">0</span>,<span class="number">1</span>))</div></pre></td></tr></table></figure><h1 id="Read-the-data"><a href="#Read-the-data" class="headerlink" title="Read the data"></a>Read the data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train = pd.read_csv(<span class="string">'train.csv.zip'</span>)</div><div class="line">Y = train.target</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">test = pd.read_csv(<span class="string">'test.csv.zip'</span>)</div><div class="line">test_ID = test.ID</div></pre></td></tr></table></figure><a id="more"></a><h1 id="Data-overview"><a href="#Data-overview" class="headerlink" title="Data overview"></a>Data overview</h1><p>Probably the first thing you check is the shapes of the train and test matrices and look inside them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> <span class="string">'Train shape'</span>, train.shape</div><div class="line"><span class="keyword">print</span> <span class="string">'Test shape'</span>,  test.shape</div></pre></td></tr></table></figure><pre><code>Train shape (145231, 1934)
Test shape (145232, 1933)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.head()</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>ID</th><br><th>VAR_0001</th><br><th>VAR_0002</th><br><th>VAR_0003</th><br><th>VAR_0004</th><br><th>VAR_0005</th><br><th>VAR_0006</th><br><th>VAR_0007</th><br><th>VAR_0008</th><br><th>VAR_0009</th><br><th>…</th><br><th>VAR_1926</th><br><th>VAR_1927</th><br><th>VAR_1928</th><br><th>VAR_1929</th><br><th>VAR_1930</th><br><th>VAR_1931</th><br><th>VAR_1932</th><br><th>VAR_1933</th><br><th>VAR_1934</th><br><th>target</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>2</td><br><td>H</td><br><td>224</td><br><td>0</td><br><td>4300</td><br><td>C</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br><td>0</td><br></tr><br><tr><br><th>1</th><br><td>4</td><br><td>H</td><br><td>7</td><br><td>53</td><br><td>4448</td><br><td>B</td><br><td>1.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br><td>0</td><br></tr><br><tr><br><th>2</th><br><td>5</td><br><td>H</td><br><td>116</td><br><td>3</td><br><td>3464</td><br><td>C</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br><td>0</td><br></tr><br><tr><br><th>3</th><br><td>7</td><br><td>H</td><br><td>240</td><br><td>300</td><br><td>3200</td><br><td>C</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>RCC</td><br><td>0</td><br></tr><br><tr><br><th>4</th><br><td>8</td><br><td>R</td><br><td>72</td><br><td>261</td><br><td>2000</td><br><td>N</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>BRANCH</td><br><td>1</td><br></tr><br></tbody><br></table><br><p>5 rows × 1934 columns</p><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.head()</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>ID</th><br><th>VAR_0001</th><br><th>VAR_0002</th><br><th>VAR_0003</th><br><th>VAR_0004</th><br><th>VAR_0005</th><br><th>VAR_0006</th><br><th>VAR_0007</th><br><th>VAR_0008</th><br><th>VAR_0009</th><br><th>…</th><br><th>VAR_1925</th><br><th>VAR_1926</th><br><th>VAR_1927</th><br><th>VAR_1928</th><br><th>VAR_1929</th><br><th>VAR_1930</th><br><th>VAR_1931</th><br><th>VAR_1932</th><br><th>VAR_1933</th><br><th>VAR_1934</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>1</td><br><td>R</td><br><td>360</td><br><td>25</td><br><td>2251</td><br><td>B</td><br><td>2.0</td><br><td>2.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br><tr><br><th>1</th><br><td>3</td><br><td>R</td><br><td>74</td><br><td>192</td><br><td>3274</td><br><td>C</td><br><td>2.0</td><br><td>3.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br><tr><br><th>2</th><br><td>6</td><br><td>R</td><br><td>21</td><br><td>36</td><br><td>3500</td><br><td>C</td><br><td>1.0</td><br><td>1.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br><tr><br><th>3</th><br><td>9</td><br><td>R</td><br><td>8</td><br><td>2</td><br><td>1500</td><br><td>B</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br><tr><br><th>4</th><br><td>10</td><br><td>H</td><br><td>91</td><br><td>39</td><br><td>84500</td><br><td>C</td><br><td>8.0</td><br><td>3.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br></tbody><br></table><br><p>5 rows × 1933 columns</p><br></div><p>There are almost 2000 anonymized variables! It’s clear, some of them are categorical, some look like numeric. Some numeric feateures are integer typed, so probably they are event conters or dates. And others are of float type, but from the first few rows they look like integer-typed too, since fractional part is zero, but pandas treats them as <code>float</code> since there are NaN values in that features.</p><p>From the first glance we see train has one more column <code>target</code> which we should not forget to drop before fitting a classifier. We also see <code>ID</code> column is shared between train and test, which sometimes can be succesfully used to improve the score.</p><p>It is also useful to know if there are any NaNs in the data. You should pay attention to columns with NaNs and the number of NaNs for each row can serve as a nice feature later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Number of NaNs for each object</span></div><div class="line">train.isnull().sum(axis=<span class="number">1</span>).head(<span class="number">15</span>)</div></pre></td></tr></table></figure><pre><code>0     25
1     19
2     24
3     24
4     24
5     24
6     24
7     24
8     16
9     24
10    22
11    24
12    17
13    24
14    24
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Number of NaNs for each column</span></div><div class="line">train.isnull().sum(axis=<span class="number">0</span>).head(<span class="number">15</span>)</div></pre></td></tr></table></figure><pre><code>ID           0
VAR_0001     0
VAR_0002     0
VAR_0003     0
VAR_0004     0
VAR_0005     0
VAR_0006    56
VAR_0007    56
VAR_0008    56
VAR_0009    56
VAR_0010    56
VAR_0011    56
VAR_0012    56
VAR_0013    56
VAR_0014    56
dtype: int64
</code></pre><p>Just by reviewing the head of the lists we immediately see the patterns, exactly 56 NaNs for a set of variables, and 24 NaNs for objects.</p><h1 id="Dataset-cleaning"><a href="#Dataset-cleaning" class="headerlink" title="Dataset cleaning"></a>Dataset cleaning</h1><h3 id="Remove-constant-features"><a href="#Remove-constant-features" class="headerlink" title="Remove constant features"></a>Remove constant features</h3><p>All 1932 columns are anonimized which makes us to deduce the meaning of the features ourselves. We will now try to clean the dataset.</p><p>It is usually convenient to concatenate train and test into one dataframe and do all feature engineering using it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">traintest = pd.concat([train, test], axis = <span class="number">0</span>)</div></pre></td></tr></table></figure><p>First we schould look for a constant features, such features do not provide any information and only make our dataset larger.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># `dropna = False` makes nunique treat NaNs as a distinct value</span></div><div class="line">feats_counts = train.nunique(dropna = <span class="keyword">False</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">feats_counts.sort_values()[:<span class="number">10</span>]</div></pre></td></tr></table></figure><pre><code>VAR_0213    1
VAR_0207    1
VAR_0840    1
VAR_0847    1
VAR_1428    1
VAR_1165    2
VAR_0438    2
VAR_1164    2
VAR_1163    2
VAR_1162    2
dtype: int64
</code></pre><p>We found 5 constant features. Let’s remove them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">constant_features = feats_counts.loc[feats_counts==<span class="number">1</span>].index.tolist()</div><div class="line"><span class="keyword">print</span> (constant_features)</div><div class="line"></div><div class="line"></div><div class="line">traintest.drop(constant_features,axis = <span class="number">1</span>,inplace=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><pre><code>[&apos;VAR_0207&apos;, &apos;VAR_0213&apos;, &apos;VAR_0840&apos;, &apos;VAR_0847&apos;, &apos;VAR_1428&apos;]
</code></pre><h3 id="Remove-duplicated-features"><a href="#Remove-duplicated-features" class="headerlink" title="Remove duplicated features"></a>Remove duplicated features</h3><p>Fill NaNs with something we can find later if needed.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">traintest.fillna(<span class="string">'NaN'</span>, inplace=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>Now let’s encode each feature, as we discussed.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">train_enc =  pd.DataFrame(index = train.index)</div><div class="line"></div><div class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tqdm_notebook(traintest.columns):</div><div class="line">    train_enc[col] = train[col].factorize()[<span class="number">0</span>]</div></pre></td></tr></table></figure><p>​</p><p>We could also do something like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># train_enc[col] = train[col].map(train[col].value_counts())</span></div></pre></td></tr></table></figure><p>The resulting data frame is very very large, so we cannot just transpose it and use .duplicated. That is why we will use a simple loop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">dup_cols = &#123;&#125;</div><div class="line"></div><div class="line"><span class="keyword">for</span> i, c1 <span class="keyword">in</span> enumerate(tqdm_notebook(train_enc.columns)):</div><div class="line">    <span class="keyword">for</span> c2 <span class="keyword">in</span> train_enc.columns[i + <span class="number">1</span>:]:</div><div class="line">        <span class="keyword">if</span> c2 <span class="keyword">not</span> <span class="keyword">in</span> dup_cols <span class="keyword">and</span> np.all(train_enc[c1] == train_enc[c2]):</div><div class="line">            dup_cols[c2] = c1</div></pre></td></tr></table></figure><p>​</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dup_cols</div></pre></td></tr></table></figure><pre><code>{&apos;VAR_0009&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0010&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0011&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0012&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0013&apos;: &apos;VAR_0006&apos;,
 &apos;VAR_0018&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0019&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0020&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0021&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0022&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0023&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0024&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0025&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0026&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0027&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0028&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0029&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0030&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0031&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0032&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0038&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0039&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0040&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0041&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0042&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0043&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0044&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0181&apos;: &apos;VAR_0180&apos;,
 &apos;VAR_0182&apos;: &apos;VAR_0180&apos;,
 &apos;VAR_0189&apos;: &apos;VAR_0188&apos;,
 &apos;VAR_0190&apos;: &apos;VAR_0188&apos;,
 &apos;VAR_0196&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0197&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0199&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0201&apos;: &apos;VAR_0051&apos;,
 &apos;VAR_0202&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0203&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0210&apos;: &apos;VAR_0208&apos;,
 &apos;VAR_0211&apos;: &apos;VAR_0208&apos;,
 &apos;VAR_0215&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0216&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0221&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0222&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0223&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0228&apos;: &apos;VAR_0227&apos;,
 &apos;VAR_0229&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0238&apos;: &apos;VAR_0089&apos;,
 &apos;VAR_0239&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0357&apos;: &apos;VAR_0260&apos;,
 &apos;VAR_0394&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0438&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0446&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0512&apos;: &apos;VAR_0506&apos;,
 &apos;VAR_0527&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0528&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0529&apos;: &apos;VAR_0526&apos;,
 &apos;VAR_0530&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0672&apos;: &apos;VAR_0670&apos;,
 &apos;VAR_1036&apos;: &apos;VAR_0916&apos;}
</code></pre><p>Don’t forget to save them, as it takes long time to find these.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</div><div class="line">pickle.dump(dup_cols, open(<span class="string">'dup_cols.p'</span>, <span class="string">'w'</span>), protocol=pickle.HIGHEST_PROTOCOL)</div></pre></td></tr></table></figure><p>Drop from traintest.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">traintest.drop(dup_cols.keys(), axis = <span class="number">1</span>,inplace=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><h1 id="Determine-types"><a href="#Determine-types" class="headerlink" title="Determine types"></a>Determine types</h1><p>Let’s examine the number of unique values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">nunique = train.nunique(dropna=<span class="keyword">False</span>)</div><div class="line">nunique</div></pre></td></tr></table></figure><pre><code>ID                145231
VAR_0001               3
VAR_0002             820
VAR_0003             588
VAR_0004            7935
VAR_0005               4
VAR_0006              38
VAR_0007              36
VAR_0008               2
VAR_0009               2
VAR_0010               2
VAR_0011               2
VAR_0012               2
VAR_0013              38
VAR_0014              38
VAR_0015              27
VAR_0016              30
VAR_0017              26
VAR_0018               2
VAR_0019               2
VAR_0020               2
VAR_0021               2
VAR_0022               2
VAR_0023               2
VAR_0024               2
VAR_0025               2
VAR_0026               2
VAR_0027               2
VAR_0028               2
VAR_0029               2
                   ...  
VAR_1907              41
VAR_1908              37
VAR_1909              41
VAR_1910              37
VAR_1911             107
VAR_1912           16370
VAR_1913           25426
VAR_1914           14226
VAR_1915            1148
VAR_1916               8
VAR_1917              10
VAR_1918              86
VAR_1919             383
VAR_1920              22
VAR_1921              18
VAR_1922            6798
VAR_1923            2445
VAR_1924             573
VAR_1925              11
VAR_1926               6
VAR_1927              10
VAR_1928              30
VAR_1929             591
VAR_1930               8
VAR_1931              10
VAR_1932              74
VAR_1933             363
VAR_1934               5
target                 2
VAR_0004_mod50        50
Length: 1935, dtype: int64
</code></pre><p>and build a histogram of those values</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</div><div class="line">_ = plt.hist(nunique.astype(float)/train.shape[<span class="number">0</span>], bins=<span class="number">100</span>)</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_44_0.png" alt="png"></p><p>Let’s take a looks at the features with a huge number of unique values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mask = (nunique.astype(float)/train.shape[<span class="number">0</span>] &gt; <span class="number">0.8</span>)</div><div class="line">train.loc[:, mask]</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>ID</th><br><th>VAR_0212</th><br><th>VAR_0227</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>2</td><br><td>NaN</td><br><td>311951</td><br></tr><br><tr><br><th>1</th><br><td>4</td><br><td>9.20713e+10</td><br><td>2.76949e+06</td><br></tr><br><tr><br><th>2</th><br><td>5</td><br><td>2.65477e+10</td><br><td>654127</td><br></tr><br><tr><br><th>3</th><br><td>7</td><br><td>7.75753e+10</td><br><td>3.01509e+06</td><br></tr><br><tr><br><th>4</th><br><td>8</td><br><td>6.04238e+10</td><br><td>118678</td><br></tr><br><tr><br><th>5</th><br><td>14</td><br><td>7.73796e+10</td><br><td>1.76557e+06</td><br></tr><br><tr><br><th>6</th><br><td>16</td><br><td>9.70303e+10</td><br><td>80151</td><br></tr><br><tr><br><th>7</th><br><td>20</td><br><td>3.10981e+10</td><br><td>853641</td><br></tr><br><tr><br><th>8</th><br><td>21</td><br><td>7.82124e+10</td><br><td>1.40254e+06</td><br></tr><br><tr><br><th>9</th><br><td>22</td><br><td>1.94014e+10</td><br><td>2.2187e+06</td><br></tr><br><tr><br><th>10</th><br><td>23</td><br><td>3.71295e+10</td><br><td>2.77679e+06</td><br></tr><br><tr><br><th>11</th><br><td>24</td><br><td>3.01203e+10</td><br><td>434300</td><br></tr><br><tr><br><th>12</th><br><td>25</td><br><td>1.80185e+10</td><br><td>1.48914e+06</td><br></tr><br><tr><br><th>13</th><br><td>26</td><br><td>9.83358e+10</td><br><td>686666</td><br></tr><br><tr><br><th>14</th><br><td>28</td><br><td>9.33087e+10</td><br><td>1.4847e+06</td><br></tr><br><tr><br><th>15</th><br><td>30</td><br><td>2.01715e+10</td><br><td>883714</td><br></tr><br><tr><br><th>16</th><br><td>31</td><br><td>4.15638e+10</td><br><td>2.6707e+06</td><br></tr><br><tr><br><th>17</th><br><td>32</td><br><td>9.17617e+10</td><br><td>2.65485e+06</td><br></tr><br><tr><br><th>18</th><br><td>35</td><br><td>3.81344e+10</td><br><td>487721</td><br></tr><br><tr><br><th>19</th><br><td>36</td><br><td>NaN</td><br><td>2.54705e+06</td><br></tr><br><tr><br><th>20</th><br><td>37</td><br><td>3.27144e+10</td><br><td>1.74684e+06</td><br></tr><br><tr><br><th>21</th><br><td>38</td><br><td>1.82142e+10</td><br><td>2.5813e+06</td><br></tr><br><tr><br><th>22</th><br><td>40</td><br><td>7.70153e+10</td><br><td>2.59396e+06</td><br></tr><br><tr><br><th>23</th><br><td>42</td><br><td>4.69701e+10</td><br><td>1.02977e+06</td><br></tr><br><tr><br><th>24</th><br><td>43</td><br><td>9.84442e+10</td><br><td>1.45101e+06</td><br></tr><br><tr><br><th>25</th><br><td>46</td><br><td>NaN</td><br><td>2.37136e+06</td><br></tr><br><tr><br><th>26</th><br><td>50</td><br><td>9.25094e+10</td><br><td>665930</td><br></tr><br><tr><br><th>27</th><br><td>51</td><br><td>3.09094e+10</td><br><td>497686</td><br></tr><br><tr><br><th>28</th><br><td>52</td><br><td>6.06105e+10</td><br><td>1.95816e+06</td><br></tr><br><tr><br><th>29</th><br><td>54</td><br><td>3.78768e+10</td><br><td>1.62591e+06</td><br></tr><br><tr><br><th>…</th><br><td>…</td><br><td>…</td><br><td>…</td><br></tr><br><tr><br><th>145201</th><br><td>290409</td><br><td>8.80126e+10</td><br><td>1.83053e+06</td><br></tr><br><tr><br><th>145202</th><br><td>290412</td><br><td>4.6152e+10</td><br><td>1.02024e+06</td><br></tr><br><tr><br><th>145203</th><br><td>290414</td><br><td>9.33055e+10</td><br><td>1.88151e+06</td><br></tr><br><tr><br><th>145204</th><br><td>290415</td><br><td>4.63509e+10</td><br><td>669351</td><br></tr><br><tr><br><th>145205</th><br><td>290417</td><br><td>2.36028e+10</td><br><td>655797</td><br></tr><br><tr><br><th>145206</th><br><td>290424</td><br><td>3.73293e+10</td><br><td>1.45626e+06</td><br></tr><br><tr><br><th>145207</th><br><td>290426</td><br><td>2.38892e+10</td><br><td>1.9503e+06</td><br></tr><br><tr><br><th>145208</th><br><td>290427</td><br><td>6.38632e+10</td><br><td>596365</td><br></tr><br><tr><br><th>145209</th><br><td>290429</td><br><td>3.00602e+10</td><br><td>572119</td><br></tr><br><tr><br><th>145210</th><br><td>290431</td><br><td>4.33429e+10</td><br><td>16120</td><br></tr><br><tr><br><th>145211</th><br><td>290432</td><br><td>3.86543e+10</td><br><td>2.08375e+06</td><br></tr><br><tr><br><th>145212</th><br><td>290434</td><br><td>9.21391e+10</td><br><td>1.89779e+06</td><br></tr><br><tr><br><th>145213</th><br><td>290436</td><br><td>3.07472e+10</td><br><td>2.94532e+06</td><br></tr><br><tr><br><th>145214</th><br><td>290439</td><br><td>7.83326e+10</td><br><td>2.54726e+06</td><br></tr><br><tr><br><th>145215</th><br><td>290440</td><br><td>NaN</td><br><td>600318</td><br></tr><br><tr><br><th>145216</th><br><td>290441</td><br><td>2.78561e+10</td><br><td>602505</td><br></tr><br><tr><br><th>145217</th><br><td>290443</td><br><td>1.90952e+10</td><br><td>2.44184e+06</td><br></tr><br><tr><br><th>145218</th><br><td>290445</td><br><td>4.62035e+10</td><br><td>2.87349e+06</td><br></tr><br><tr><br><th>145219</th><br><td>290447</td><br><td>NaN</td><br><td>1.53493e+06</td><br></tr><br><tr><br><th>145220</th><br><td>290448</td><br><td>7.54282e+10</td><br><td>1.60102e+06</td><br></tr><br><tr><br><th>145221</th><br><td>290449</td><br><td>4.30768e+10</td><br><td>2.08415e+06</td><br></tr><br><tr><br><th>145222</th><br><td>290450</td><br><td>7.81325e+10</td><br><td>2.85367e+06</td><br></tr><br><tr><br><th>145223</th><br><td>290452</td><br><td>4.51061e+10</td><br><td>1.56506e+06</td><br></tr><br><tr><br><th>145224</th><br><td>290453</td><br><td>4.62223e+10</td><br><td>1.46815e+06</td><br></tr><br><tr><br><th>145225</th><br><td>290454</td><br><td>7.74507e+10</td><br><td>2.92811e+06</td><br></tr><br><tr><br><th>145226</th><br><td>290457</td><br><td>7.05088e+10</td><br><td>2.03657e+06</td><br></tr><br><tr><br><th>145227</th><br><td>290458</td><br><td>9.02492e+10</td><br><td>1.68013e+06</td><br></tr><br><tr><br><th>145228</th><br><td>290459</td><br><td>9.17224e+10</td><br><td>2.41922e+06</td><br></tr><br><tr><br><th>145229</th><br><td>290461</td><br><td>4.51033e+10</td><br><td>1.53960e+06</td><br></tr><br><tr><br><th>145230</th><br><td>290463</td><br><td>9.14114e+10</td><br><td>2.6609e+06</td><br></tr><br></tbody><br></table><br><p>145231 rows × 3 columns</p><br></div><p>The values are not float, they are integer, so these features are likely to be even counts. Let’s look at another pack of features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mask = (nunique.astype(float)/train.shape[<span class="number">0</span>] &lt; <span class="number">0.8</span>) &amp; (nunique.astype(float)/train.shape[<span class="number">0</span>] &gt; <span class="number">0.4</span>)</div><div class="line">train.loc[:<span class="number">25</span>, mask]</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>VAR_0541</th><br><th>VAR_0543</th><br><th>VAR_0899</th><br><th>VAR_1081</th><br><th>VAR_1082</th><br><th>VAR_1087</th><br><th>VAR_1179</th><br><th>VAR_1180</th><br><th>VAR_1181</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>49463</td><br><td>116783</td><br><td>112871</td><br><td>76857</td><br><td>76857</td><br><td>116783</td><br><td>76857</td><br><td>76857</td><br><td>76857</td><br></tr><br><tr><br><th>1</th><br><td>303472</td><br><td>346196</td><br><td>346375</td><br><td>341365</td><br><td>341365</td><br><td>346196</td><br><td>341365</td><br><td>341365</td><br><td>176604</td><br></tr><br><tr><br><th>2</th><br><td>94990</td><br><td>122601</td><br><td>121501</td><br><td>107267</td><br><td>107267</td><br><td>121501</td><br><td>107267</td><br><td>107267</td><br><td>58714</td><br></tr><br><tr><br><th>3</th><br><td>20593</td><br><td>59490</td><br><td>61890</td><br><td>45794</td><br><td>47568</td><br><td>59490</td><br><td>45794</td><br><td>47568</td><br><td>47568</td><br></tr><br><tr><br><th>4</th><br><td>10071</td><br><td>35708</td><br><td>34787</td><br><td>20475</td><br><td>23647</td><br><td>34708</td><br><td>20475</td><br><td>23647</td><br><td>23647</td><br></tr><br><tr><br><th>5</th><br><td>18877</td><br><td>28055</td><br><td>28455</td><br><td>21139</td><br><td>21139</td><br><td>28055</td><br><td>21139</td><br><td>21139</td><br><td>20627</td><br></tr><br><tr><br><th>6</th><br><td>321783</td><br><td>333565</td><br><td>886886</td><br><td>327744</td><br><td>327744</td><br><td>333565</td><br><td>327744</td><br><td>327744</td><br><td>163944</td><br></tr><br><tr><br><th>7</th><br><td>2961</td><br><td>5181</td><br><td>11084</td><br><td>4326</td><br><td>4326</td><br><td>5181</td><br><td>4326</td><br><td>4326</td><br><td>4326</td><br></tr><br><tr><br><th>8</th><br><td>20359</td><br><td>30114</td><br><td>33434</td><br><td>24969</td><br><td>27128</td><br><td>30114</td><br><td>24969</td><br><td>27128</td><br><td>27128</td><br></tr><br><tr><br><th>9</th><br><td>815</td><br><td>1300</td><br><td>7677</td><br><td>1197</td><br><td>1197</td><br><td>1300</td><br><td>1197</td><br><td>1197</td><br><td>1197</td><br></tr><br><tr><br><th>10</th><br><td>6088</td><br><td>15233</td><br><td>15483</td><br><td>7077</td><br><td>7077</td><br><td>15233</td><br><td>7077</td><br><td>7077</td><br><td>4033</td><br></tr><br><tr><br><th>11</th><br><td>432</td><br><td>1457</td><br><td>2000</td><br><td>621</td><br><td>621</td><br><td>757</td><br><td>621</td><br><td>621</td><br><td>621</td><br></tr><br><tr><br><th>12</th><br><td>383</td><br><td>539</td><br><td>860</td><br><td>752</td><br><td>1158</td><br><td>539</td><br><td>752</td><br><td>1158</td><br><td>1158</td><br></tr><br><tr><br><th>13</th><br><td>14359</td><br><td>47562</td><br><td>47562</td><br><td>17706</td><br><td>17706</td><br><td>47562</td><br><td>17706</td><br><td>17706</td><br><td>17706</td><br></tr><br><tr><br><th>14</th><br><td>145391</td><br><td>218067</td><br><td>214836</td><br><td>176627</td><br><td>176627</td><br><td>216307</td><br><td>175273</td><br><td>175273</td><br><td>91019</td><br></tr><br><tr><br><th>15</th><br><td>10040</td><br><td>12119</td><br><td>17263</td><br><td>10399</td><br><td>10399</td><br><td>12119</td><br><td>10399</td><br><td>10399</td><br><td>5379</td><br></tr><br><tr><br><th>16</th><br><td>4880</td><br><td>9607</td><br><td>9607</td><br><td>9165</td><br><td>9165</td><br><td>9607</td><br><td>9165</td><br><td>9165</td><br><td>9165</td><br></tr><br><tr><br><th>17</th><br><td>12900</td><br><td>35590</td><br><td>35781</td><br><td>26096</td><br><td>26096</td><br><td>35590</td><br><td>26096</td><br><td>26096</td><br><td>19646</td><br></tr><br><tr><br><th>18</th><br><td>104442</td><br><td>139605</td><br><td>150505</td><br><td>136419</td><br><td>142218</td><br><td>139605</td><br><td>136419</td><br><td>142218</td><br><td>142218</td><br></tr><br><tr><br><th>19</th><br><td>13898</td><br><td>25566</td><br><td>26685</td><br><td>20122</td><br><td>20122</td><br><td>25566</td><br><td>20122</td><br><td>20122</td><br><td>20122</td><br></tr><br><tr><br><th>20</th><br><td>3524</td><br><td>10033</td><br><td>10133</td><br><td>5838</td><br><td>5838</td><br><td>10033</td><br><td>5838</td><br><td>5838</td><br><td>5838</td><br></tr><br><tr><br><th>21</th><br><td>129873</td><br><td>204072</td><br><td>206946</td><br><td>183049</td><br><td>183049</td><br><td>204072</td><br><td>183049</td><br><td>183049</td><br><td>96736</td><br></tr><br><tr><br><th>22</th><br><td>3591</td><br><td>11400</td><br><td>17680</td><br><td>5565</td><br><td>5565</td><br><td>11400</td><br><td>5565</td><br><td>5565</td><br><td>5565</td><br></tr><br><tr><br><th>23</th><br><td>999999999</td><br><td>999999999</td><br><td>-99999</td><br><td>999999999</td><br><td>999999999</td><br><td>999999999</td><br><td>999999999</td><br><td>999999999</td><br><td>999999999</td><br></tr><br><tr><br><th>24</th><br><td>1270</td><br><td>4955</td><br><td>12201</td><br><td>2490</td><br><td>2490</td><br><td>4955</td><br><td>2490</td><br><td>2490</td><br><td>2490</td><br></tr><br><tr><br><th>25</th><br><td>2015</td><br><td>2458</td><br><td>2458</td><br><td>2015</td><br><td>2015</td><br><td>2458</td><br><td>2015</td><br><td>2015</td><br><td>1008</td><br></tr><br></tbody><br></table><br></div><p>These look like counts too. First thing to notice is the 23th line: 99999.., -99999 values look like NaNs so we should probably built a related feature. Second: the columns are sometimes placed next to each other, so the columns are probably grouped together and we can disentangle that.</p><p>Our conclusion: there are no floating point variables, there are some counts variables, which we will treat as numeric.</p><p>And finally, let’s pick one variable (in this case ‘VAR_0015’) from the third group of features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train[<span class="string">'VAR_0015'</span>].value_counts()</div></pre></td></tr></table></figure><pre><code> 0.0      102382
 1.0       28045
 2.0        8981
 3.0        3199
 4.0        1274
 5.0         588
 6.0         275
 7.0         166
 8.0          97
-999.0        56
 9.0          51
 10.0         39
 11.0         18
 12.0         16
 13.0          9
 14.0          8
 15.0          8
 16.0          6
 22.0          3
 21.0          3
 19.0          1
 35.0          1
 17.0          1
 29.0          1
 18.0          1
 32.0          1
 23.0          1
Name: VAR_0015, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cat_cols = list(train.select_dtypes(include=[<span class="string">'object'</span>]).columns)</div><div class="line">num_cols = list(train.select_dtypes(exclude=[<span class="string">'object'</span>]).columns)</div></pre></td></tr></table></figure><h1 id="Go-through"><a href="#Go-through" class="headerlink" title="Go through"></a>Go through</h1><p>Let’s replace NaNs with something first.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.replace(<span class="string">'NaN'</span>, <span class="number">-999</span>, inplace=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>Let’s calculate how many times one feature is greater than the other and create cross tabel out of it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># select first 42 numeric features</span></div><div class="line">feats = num_cols[:<span class="number">42</span>]</div><div class="line"></div><div class="line"><span class="comment"># build 'mean(feat1 &gt; feat2)' plot</span></div><div class="line">gt_matrix(feats,<span class="number">16</span>)</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_57_0.png" alt="png"></p><p>Indeed, we see interesting patterns here. There are blocks of geatures where one is strictly greater than the other. So we can hypothesize, that each column correspondes to cumulative counts, e.g. feature number one is counts in first month, second – total count number in first two month and so on. So we immediately understand what features we should generate to make tree-based models more efficient: the differences between consecutive values.</p><h2 id="VAR-0002-VAR-0003"><a href="#VAR-0002-VAR-0003" class="headerlink" title="VAR_0002, VAR_0003"></a>VAR_0002, VAR_0003</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hist_it(train[<span class="string">'VAR_0002'</span>])</div><div class="line">plt.ylim((<span class="number">0</span>,<span class="number">0.05</span>))</div><div class="line">plt.xlim((<span class="number">-10</span>,<span class="number">1010</span>))</div><div class="line"></div><div class="line">hist_it(train[<span class="string">'VAR_0003'</span>])</div><div class="line">plt.ylim((<span class="number">0</span>,<span class="number">0.03</span>))</div><div class="line">plt.xlim((<span class="number">-10</span>,<span class="number">1010</span>))</div></pre></td></tr></table></figure><pre><code>(-10, 1010)
</code></pre><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_60_1.png" alt="png"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_60_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train[<span class="string">'VAR_0002'</span>].value_counts()</div></pre></td></tr></table></figure><pre><code>12     5264
24     4763
36     3499
60     2899
6      2657
13     2478
72     2243
48     2222
3      2171
4      1917
2      1835
84     1801
120    1786
1      1724
7      1671
26     1637
5      1624
14     1572
18     1555
8      1513
999    1510
25     1504
96     1445
30     1438
9      1306
144    1283
15     1221
27     1186
38     1146
37     1078
       ... 
877       1
785       1
750       1
653       1
784       1
764       1
751       1
797       1
926       1
691       1
808       1
774       1
902       1
755       1
656       1
814       1
813       1
685       1
739       1
935       1
906       1
807       1
550       1
933       1
804       1
675       1
674       1
745       1
778       1
851       1
Name: VAR_0002, Length: 820, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train[<span class="string">'VAR_0003'</span>].value_counts()</div></pre></td></tr></table></figure><pre><code>0      17436
24      3469
12      3271
60      3054
36      2498
72      2081
48      2048
6       1993
1       1797
3       1679
84      1553
2       1459
999     1428
4       1419
120     1411
7       1356
13      1297
18      1296
96      1253
14      1228
8       1216
5       1189
9       1182
30      1100
25      1100
144     1090
15      1047
61      1008
26       929
42       921
       ...  
560        1
552        1
550        1
804        1
543        1
668        1
794        1
537        1
531        1
664        1
632        1
709        1
597        1
965        1
852        1
648        1
596        1
466        1
592        1
521        1
533        1
636        1
975        1
973        1
587        1
523        1
584        1
759        1
583        1
570        1
Name: VAR_0003, Length: 588, dtype: int64
</code></pre><p>We see there is something special about 12, 24 and so on, sowe can create another feature x mod 12.</p><h2 id="VAR-0004"><a href="#VAR-0004" class="headerlink" title="VAR_0004"></a>VAR_0004</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">train[<span class="string">'VAR_0004_mod50'</span>] = train[<span class="string">'VAR_0004'</span>] % <span class="number">50</span></div><div class="line">hist_it(train[<span class="string">'VAR_0004_mod50'</span>])</div><div class="line">plt.ylim((<span class="number">0</span>,<span class="number">0.6</span>))</div></pre></td></tr></table></figure><pre><code>(0, 0.6)
</code></pre><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_65_1.png" alt="png"></p><h1 id="Categorical-features"><a href="#Categorical-features" class="headerlink" title="Categorical features"></a>Categorical features</h1><p>Let’s take a look at categorical features we have.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.loc[:,cat_cols].head().T</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>0</th><br><th>1</th><br><th>2</th><br><th>3</th><br><th>4</th><br></tr><br></thead><br><tbody><br><tr><br><th>VAR_0001</th><br><td>H</td><br><td>H</td><br><td>H</td><br><td>H</td><br><td>R</td><br></tr><br><tr><br><th>VAR_0005</th><br><td>C</td><br><td>B</td><br><td>C</td><br><td>C</td><br><td>N</td><br></tr><br><tr><br><th>VAR_0008</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0009</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0010</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0011</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0012</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0043</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0044</th><br><td>[]</td><br><td>[]</td><br><td>[]</td><br><td>[]</td><br><td>[]</td><br></tr><br><tr><br><th>VAR_0073</th><br><td>NaT</td><br><td>2012-09-04 00:00:00</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0075</th><br><td>2011-11-08 00:00:00</td><br><td>2011-11-10 00:00:00</td><br><td>2011-12-13 00:00:00</td><br><td>2010-09-23 00:00:00</td><br><td>2011-10-15 00:00:00</td><br></tr><br><tr><br><th>VAR_0156</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0157</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0158</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0159</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0166</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0167</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0168</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0169</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0176</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0177</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0178</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0179</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0196</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0200</th><br><td>FT LAUDERDALE</td><br><td>SANTEE</td><br><td>REEDSVILLE</td><br><td>LIBERTY</td><br><td>FRANKFORT</td><br></tr><br><tr><br><th>VAR_0202</th><br><td>BatchInquiry</td><br><td>BatchInquiry</td><br><td>BatchInquiry</td><br><td>BatchInquiry</td><br><td>BatchInquiry</td><br></tr><br><tr><br><th>VAR_0204</th><br><td>2014-01-29 21:16:00</td><br><td>2014-02-01 00:11:00</td><br><td>2014-01-30 15:11:00</td><br><td>2014-02-01 00:07:00</td><br><td>2014-01-29 19:31:00</td><br></tr><br><tr><br><th>VAR_0214</th><br><td>NaN</td><br><td>NaN</td><br><td>NaN</td><br><td>NaN</td><br><td>NaN</td><br></tr><br><tr><br><th>VAR_0216</th><br><td>DS</td><br><td>DS</td><br><td>DS</td><br><td>DS</td><br><td>DS</td><br></tr><br><tr><br><th>VAR_0217</th><br><td>2011-11-08 02:00:00</td><br><td>2012-10-02 02:00:00</td><br><td>2011-12-13 02:00:00</td><br><td>2012-11-01 02:00:00</td><br><td>2011-10-15 02:00:00</td><br></tr><br><tr><br><th>VAR_0222</th><br><td>C6</td><br><td>C6</td><br><td>C6</td><br><td>C6</td><br><td>C6</td><br></tr><br><tr><br><th>VAR_0226</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0229</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0230</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0232</th><br><td>True</td><br><td>False</td><br><td>True</td><br><td>False</td><br><td>True</td><br></tr><br><tr><br><th>VAR_0236</th><br><td>True</td><br><td>True</td><br><td>True</td><br><td>True</td><br><td>True</td><br></tr><br><tr><br><th>VAR_0237</th><br><td>FL</td><br><td>CA</td><br><td>WV</td><br><td>TX</td><br><td>IL</td><br></tr><br><tr><br><th>VAR_0239</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0274</th><br><td>FL</td><br><td>MI</td><br><td>WV</td><br><td>TX</td><br><td>IL</td><br></tr><br><tr><br><th>VAR_0283</th><br><td>S</td><br><td>S</td><br><td>S</td><br><td>S</td><br><td>S</td><br></tr><br><tr><br><th>VAR_0305</th><br><td>S</td><br><td>S</td><br><td>P</td><br><td>P</td><br><td>P</td><br></tr><br><tr><br><th>VAR_0325</th><br><td>-1</td><br><td>H</td><br><td>R</td><br><td>H</td><br><td>S</td><br></tr><br><tr><br><th>VAR_0342</th><br><td>CF</td><br><td>EC</td><br><td>UU</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_0352</th><br><td>O</td><br><td>O</td><br><td>R</td><br><td>R</td><br><td>R</td><br></tr><br><tr><br><th>VAR_0353</th><br><td>U</td><br><td>R</td><br><td>R</td><br><td>R</td><br><td>U</td><br></tr><br><tr><br><th>VAR_0354</th><br><td>O</td><br><td>R</td><br><td>-1</td><br><td>-1</td><br><td>O</td><br></tr><br><tr><br><th>VAR_0404</th><br><td>CHIEF EXECUTIVE OFFICER</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_0466</th><br><td>-1</td><br><td>I</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_0467</th><br><td>-1</td><br><td>Discharged</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_0493</th><br><td>COMMUNITY ASSOCIATION MANAGER</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_1934</th><br><td>IAPS</td><br><td>IAPS</td><br><td>IAPS</td><br><td>RCC</td><br><td>BRANCH</td><br></tr><br></tbody><br></table><br></div><p><code>VAR_0200</code>, <code>VAR_0237</code>, <code>VAR_0274</code> look like some georgraphical data thus one could generate geography related features, we will talk later in the course.</p><p>There are some features, that are hard to identify, but look, there a date columns <code>VAR_0073</code> – <code>VAR_0179</code>, <code>VAR_0204</code>, <code>VAR_0217</code>. It is useful to plot one date against another to find relationships.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">date_cols = [<span class="string">u'VAR_0073'</span>,<span class="string">'VAR_0075'</span>,</div><div class="line">             <span class="string">u'VAR_0156'</span>,<span class="string">u'VAR_0157'</span>,<span class="string">u'VAR_0158'</span>,<span class="string">'VAR_0159'</span>,</div><div class="line">             <span class="string">u'VAR_0166'</span>, <span class="string">u'VAR_0167'</span>,<span class="string">u'VAR_0168'</span>,<span class="string">u'VAR_0169'</span>,</div><div class="line">             <span class="string">u'VAR_0176'</span>,<span class="string">u'VAR_0177'</span>,<span class="string">u'VAR_0178'</span>,<span class="string">u'VAR_0179'</span>,</div><div class="line">             <span class="string">u'VAR_0204'</span>,</div><div class="line">             <span class="string">u'VAR_0217'</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> c <span class="keyword">in</span> date_cols:</div><div class="line">    train[c] = pd.to_datetime(train[c],format = <span class="string">'%d%b%y:%H:%M:%S'</span>)</div><div class="line">    test[c] = pd.to_datetime(test[c],  format = <span class="string">'%d%b%y:%H:%M:%S'</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">c1 = <span class="string">'VAR_0217'</span></div><div class="line">c2 = <span class="string">'VAR_0073'</span></div><div class="line"></div><div class="line"><span class="comment"># mask = (~test[c1].isnull()) &amp; (~test[c2].isnull())</span></div><div class="line"><span class="comment"># sc2(test.ix[mask,c1].values,test.ix[mask,c2].values,alpha=0.7,c = 'black')</span></div><div class="line"></div><div class="line">mask = (~train[c1].isnull()) &amp; (~train[c2].isnull())</div><div class="line">sc2(train.loc[mask,c1].values,train.loc[mask,c2].values,c=train.loc[mask,<span class="string">'target'</span>].values)</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_71_0.png" alt="png"></p><p>We see that one date is strictly greater than the other, so the difference between them can be a good feature. Also look at horizontal line there – it also looks like NaN, so I would rather create a new binary feature which will serve as an idicator that our time feature is NaN.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is a notebook, used in the screencast video. Note, that the data files are not present here in Jupyter hub and you will not be able to run it. But you can always download the notebook to your local machine as well as the competition data and make it interactive.&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; os&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; tqdm &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tqdm_notebook&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;%matplotlib inline&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; warnings&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;warnings.filterwarnings(&lt;span class=&quot;string&quot;&gt;&#39;ignore&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; seaborn&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;autolabel&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(arrayA)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&#39;&#39;&#39; label each colored square with the corresponding data value. &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    If value &amp;gt; 20, the text is in black, else in white.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &#39;&#39;&#39;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    arrayA = np.array(arrayA)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(arrayA.shape[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; j &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(arrayA.shape[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;                plt.text(j,i, &lt;span class=&quot;string&quot;&gt;&quot;%.2f&quot;&lt;/span&gt;%arrayA[i,j], ha=&lt;span class=&quot;string&quot;&gt;&#39;center&#39;&lt;/span&gt;, va=&lt;span class=&quot;string&quot;&gt;&#39;bottom&#39;&lt;/span&gt;,color=&lt;span class=&quot;string&quot;&gt;&#39;w&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;hist_it&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(feat)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.figure(figsize=(&lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    feat[Y==&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;].hist(bins=range(int(feat.min()),int(feat.max()+&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)),normed=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,alpha=&lt;span class=&quot;number&quot;&gt;0.8&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    feat[Y==&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;].hist(bins=range(int(feat.min()),int(feat.max()+&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)),normed=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,alpha=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.ylim((&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;gt_matrix&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(feats,sz=&lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    a = []&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i,c1 &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; enumerate(feats):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        b = [] &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; j,c2 &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; enumerate(feats):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            mask = (~train[c1].isnull()) &amp;amp; (~train[c2].isnull())&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; i&amp;gt;=j:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;                b.append((train.loc[mask,c1].values&amp;gt;=train.loc[mask,c2].values).mean())&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;                b.append((train.loc[mask,c1].values&amp;gt;train.loc[mask,c2].values).mean())&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        a.append(b)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.figure(figsize = (sz,sz))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.imshow(a, interpolation = &lt;span class=&quot;string&quot;&gt;&#39;None&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    _ = plt.xticks(range(len(feats)),feats,rotation = &lt;span class=&quot;number&quot;&gt;90&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    _ = plt.yticks(range(len(feats)),feats,rotation = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    autolabel(a)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;hist_it1&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(feat)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.figure(figsize=(&lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    feat[Y==&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;].hist(bins=&lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;,range=(feat.min(),feat.max()),normed=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,alpha=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    feat[Y==&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;].hist(bins=&lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;,range=(feat.min(),feat.max()),normed=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,alpha=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.ylim((&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;))&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&quot;Read-the-data&quot;&gt;&lt;a href=&quot;#Read-the-data&quot; class=&quot;headerlink&quot; title=&quot;Read the data&quot;&gt;&lt;/a&gt;Read the data&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;train = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;train.csv.zip&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Y = train.target&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;test = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;test.csv.zip&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;test_ID = test.ID&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>EDA check list</title>
    <link href="http://yoursite.com/2018/10/16/EDA-check-list/"/>
    <id>http://yoursite.com/2018/10/16/EDA-check-list/</id>
    <published>2018-10-16T08:05:28.000Z</published>
    <updated>2018-10-16T08:06:51.636Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Get domain knowledge</li><li>Check if the data is intuitive (abnormal detection)<ul><li>add a feature <code>is_incorrect</code></li></ul></li><li><a href="https://tomaxent.com/2018/10/16/Exploratory-Data-Analysis/" target="_blank" rel="external">Understand how the data was generated</a><ul><li>It is crucial to understand the generation process to set up a proper validation scheme</li></ul></li><li>Two things to do with anonymized features<ul><li>Try to decode the features<ul><li><a href="https://tomaxent.com/2018/10/16/Processing-anonymized-features/" target="_blank" rel="external">Guess the true meaning of the feature</a></li></ul></li><li>Guess the feature types<ul><li>Each type need its own preprocessing</li></ul></li></ul></li><li>Visualization<ul><li>Tools for individual features exploration<ul><li>Histograms <code>plt.hist(x)</code></li><li>Plot (index versus value) <code>plt.plot(x, something)</code></li><li>Statistics <code>df.describe() or x.mean() or x.var()</code></li><li>Other tools <code>x.value_counts() or x.isnull()</code></li></ul></li><li>Tools for feature relationships<ul><li>Pairs<ul><li><code>plt.scatter(x1, x2)</code></li><li><code>pd.scatter_matrix(df)</code></li><li><code>df.corr() or plt.matshow()</code></li></ul></li><li>Groups:<ul><li>Clustering</li><li>Plot (index vs feature statistics) <code>df.mean().sort_values().plot()</code></li></ul></li></ul></li></ul></li><li>Data Clean<ul><li>remove duplicated and constant features<ul><li><code>traintest.nunique(axis=1) == 1</code></li><li><code>traintest.T.drop_duplicates()</code></li><li><code>for f in categorical_feats: traintest[f] = traintest[f].factorize then traintest.T.drop_duplicates()</code></li></ul></li><li>check if same rows have same label</li><li>check if dataset is shuffled</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;&lt;li&gt;Get domain knowledge&lt;/li&gt;&lt;li&gt;Check if the data is intuitive (abnormal detection)&lt;ul&gt;&lt;li&gt;add a feature &lt;code&gt;is_incorrect&lt;/code&gt;&lt;/li&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Processing Anonymized Features</title>
    <link href="http://yoursite.com/2018/10/16/Processing-anonymized-features/"/>
    <id>http://yoursite.com/2018/10/16/Processing-anonymized-features/</id>
    <published>2018-10-16T07:22:45.000Z</published>
    <updated>2018-10-16T07:23:19.410Z</updated>
    
    <content type="html"><![CDATA[<p><strong>IMPORTANT:</strong> You will not be able to run this notebook at coursera platform, as the dataset is not there. The notebook is in read-only mode.</p><p>But you can run the notebook locally and download the dataset using <a href="https://habrastorage.org/storage/stuff/special/beeline/00.beeline_bigdata.zip" target="_blank" rel="external">this link</a> to explore the data interactively.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pd.set_option(<span class="string">'max_columns'</span>, <span class="number">100</span>)</div></pre></td></tr></table></figure><h1 id="Load-the-data"><a href="#Load-the-data" class="headerlink" title="Load the data"></a>Load the data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train = pd.read_csv(<span class="string">'./train.csv'</span>)</div><div class="line">train.head()</div></pre></td></tr></table></figure><a id="more"></a><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>x0</th><br><th>x1</th><br><th>x2</th><br><th>x3</th><br><th>x4</th><br><th>x5</th><br><th>x6</th><br><th>x7</th><br><th>x8</th><br><th>x9</th><br><th>x10</th><br><th>x11</th><br><th>x12</th><br><th>x13</th><br><th>x14</th><br><th>x15</th><br><th>x16</th><br><th>x17</th><br><th>x18</th><br><th>x19</th><br><th>x20</th><br><th>x21</th><br><th>x22</th><br><th>x23</th><br><th>x24</th><br><th>x25</th><br><th>x26</th><br><th>x27</th><br><th>x28</th><br><th>x29</th><br><th>x30</th><br><th>x31</th><br><th>x32</th><br><th>x33</th><br><th>x34</th><br><th>x35</th><br><th>x36</th><br><th>x37</th><br><th>x38</th><br><th>x39</th><br><th>x40</th><br><th>x41</th><br><th>x42</th><br><th>x43</th><br><th>x44</th><br><th>x45</th><br><th>x46</th><br><th>x47</th><br><th>x48</th><br><th>x49</th><br><th>x50</th><br><th>x51</th><br><th>x52</th><br><th>x53</th><br><th>x54</th><br><th>x55</th><br><th>x56</th><br><th>x57</th><br><th>x58</th><br><th>x59</th><br><th>x60</th><br><th>x61</th><br><th>y</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>b4d8a653ea</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>a62168d626</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>-0.688706</td><br><td>7e5c97705a</td><br><td>e5df3eff9b</td><br><td>91bb549494</td><br><td>e33c63cf35</td><br><td>3694.0</td><br><td>6e40247e69</td><br><td>617a4ad3f9</td><br><td>718c61545b</td><br><td>c26d08129a</td><br><td>634e3cf3ac</td><br><td>dd9c9e0da2</td><br><td>17c99905b6</td><br><td>513a3e3f36</td><br><td>9aba4d7f51</td><br><td>40.579612</td><br><td>-0.112693</td><br><td>-0.172191</td><br><td>1.166667</td><br><td>1.674538</td><br><td>0.630889</td><br><td>37.000000</td><br><td>1.294922</td><br><td>55.0</td><br><td>0.166667</td><br><td>10.0</td><br><td>0.0</td><br><td>0.000000</td><br><td>1.0</td><br><td>9.0</td><br><td>0.0</td><br><td>1.0</td><br><td>23.0</td><br><td>3.67</td><br><td>0.12</td><br><td>1.935</td><br><td>2.2</td><br><td>0.625</td><br><td>0.250</td><br><td>0.125</td><br><td>0.000</td><br><td>0.813</td><br><td>0.074</td><br><td>0.634</td><br><td>0.548</td><br><td>0.235333</td><br><td>0.264952</td><br><td>0.000000</td><br><td>0.333333</td><br><td>0.333333</td><br><td>0.333333</td><br><td>0.000000</td><br><td>0.000000</td><br><td>9.0</td><br><td>2</td><br></tr><br><tr><br><th>1</th><br><td>467f9617a3</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>b7584c2d52</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>0.870871</td><br><td>5624b8f759</td><br><td>fa0b797a92</td><br><td>669ea3d319</td><br><td>f178803074</td><br><td>18156.0</td><br><td>01ede04b4b</td><br><td>617a4ad3f9</td><br><td>718c61545b</td><br><td>d342e2765f</td><br><td>bb20e1ca06</td><br><td>8a6c8cef83</td><br><td>1b02793146</td><br><td>992153ed65</td><br><td>9aba4d7f51</td><br><td>28.765503</td><br><td>2.612285</td><br><td>2.159091</td><br><td>4.000000</td><br><td>1.710714</td><br><td>1.713538</td><br><td>0.166667</td><br><td>0.027669</td><br><td>109.0</td><br><td>0.000000</td><br><td>31.0</td><br><td>0.0</td><br><td>0.000000</td><br><td>1.0</td><br><td>244.0</td><br><td>1.0</td><br><td>1.0</td><br><td>68.0</td><br><td>17.25</td><br><td>0.57</td><br><td>3.452</td><br><td>4.0</td><br><td>0.409</td><br><td>0.619</td><br><td>0.579</td><br><td>0.248</td><br><td>0.346</td><br><td>0.541</td><br><td>0.522</td><br><td>0.000</td><br><td>1.782346</td><br><td>1.322409</td><br><td>0.011647</td><br><td>0.397671</td><br><td>0.239601</td><br><td>0.249584</td><br><td>0.068220</td><br><td>0.033278</td><br><td>601.0</td><br><td>4</td><br></tr><br><tr><br><th>2</th><br><td>190436e528</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>b7584c2d52</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>0.437655</td><br><td>5624b8f759</td><br><td>152af2cb2f</td><br><td>91bb549494</td><br><td>e33c63cf35</td><br><td>1178.0</td><br><td>cc69cbe29a</td><br><td>617a4ad3f9</td><br><td>e8a040423a</td><br><td>c82c3dbd33</td><br><td>ee3501282b</td><br><td>199ce7c484</td><br><td>5f17dedd5c</td><br><td>5c5025bd0a</td><br><td>9aba4d7f51</td><br><td>24.943933</td><br><td>-0.814660</td><br><td>-0.708308</td><br><td>1.500000</td><br><td>-0.512422</td><br><td>-0.733967</td><br><td>0.333333</td><br><td>14.837728</td><br><td>11.0</td><br><td>0.000000</td><br><td>24.0</td><br><td>0.0</td><br><td>0.000000</td><br><td>1.0</td><br><td>29.0</td><br><td>0.0</td><br><td>3.0</td><br><td>11.0</td><br><td>4.42</td><br><td>0.15</td><br><td>0.161</td><br><td>0.2</td><br><td>1.000</td><br><td>1.000</td><br><td>1.000</td><br><td>1.000</td><br><td>1.000</td><br><td>0.520</td><br><td>0.533</td><br><td>0.835</td><br><td>-0.586540</td><br><td>0.672436</td><br><td>0.000000</td><br><td>0.606061</td><br><td>0.121212</td><br><td>0.212121</td><br><td>0.060606</td><br><td>0.000000</td><br><td>33.0</td><br><td>3</td><br></tr><br><tr><br><th>3</th><br><td>43859085bc</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>a62168d626</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>0.004439</td><br><td>f67f142e40</td><br><td>c4dd2197c3</td><br><td>91bb549494</td><br><td>e33c63cf35</td><br><td>14559.0</td><br><td>6e40247e69</td><br><td>617a4ad3f9</td><br><td>718c61545b</td><br><td>c26d08129a</td><br><td>9e166b965d</td><br><td>466f8951b0</td><br><td>fde72a6d5c</td><br><td>acfadc5c01</td><br><td>9aba4d7f51</td><br><td>41.576860</td><br><td>-0.907833</td><br><td>-0.761736</td><br><td>0.500000</td><br><td>-0.627525</td><br><td>-0.805801</td><br><td>1.166667</td><br><td>0.004395</td><br><td>0.0</td><br><td>0.500000</td><br><td>0.0</td><br><td>0.0</td><br><td>0.000000</td><br><td>7.0</td><br><td>7.0</td><br><td>0.0</td><br><td>3.0</td><br><td>15.0</td><br><td>8.92</td><br><td>0.29</td><br><td>0.226</td><br><td>0.8</td><br><td>0.000</td><br><td>0.000</td><br><td>0.000</td><br><td>0.000</td><br><td>0.000</td><br><td>1.000</td><br><td>0.000</td><br><td>0.000</td><br><td>-1.600326</td><br><td>-1.838680</td><br><td>0.000000</td><br><td>1.000000</td><br><td>0.000000</td><br><td>0.000000</td><br><td>0.000000</td><br><td>0.000000</td><br><td>1.0</td><br><td>4</td><br></tr><br><tr><br><th>4</th><br><td>a4c3095b75</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>b7584c2d52</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>0.480977</td><br><td>7e5c97705a</td><br><td>e071d01df5</td><br><td>91bb549494</td><br><td>e33c63cf35</td><br><td>5777.0</td><br><td>6e40247e69</td><br><td>617a4ad3f9</td><br><td>4b9480aa42</td><br><td>e84655292c</td><br><td>527b6ca8cc</td><br><td>dd9c9e0da2</td><br><td>17c99905b6</td><br><td>0fc56ea1f0</td><br><td>9aba4d7f51</td><br><td>31.080282</td><br><td>-0.371787</td><br><td>-0.367616</td><br><td>1.666667</td><br><td>0.271307</td><br><td>0.013112</td><br><td>17.333333</td><br><td>1713.439128</td><br><td>33.0</td><br><td>0.000000</td><br><td>6.0</td><br><td>1.0</td><br><td>0.666667</td><br><td>8.0</td><br><td>108.0</td><br><td>1.0</td><br><td>4.0</td><br><td>86.0</td><br><td>1.58</td><br><td>0.05</td><br><td>2.032</td><br><td>2.4</td><br><td>0.348</td><br><td>0.762</td><br><td>0.550</td><br><td>0.392</td><br><td>0.489</td><br><td>0.517</td><br><td>1.000</td><br><td>0.642</td><br><td>0.960991</td><br><td>0.790990</td><br><td>0.020161</td><br><td>0.645161</td><br><td>0.258065</td><br><td>0.036290</td><br><td>0.040323</td><br><td>0.000000</td><br><td>248.0</td><br><td>3</td><br></tr><br></tbody><br></table><br></div><h1 id="Build-a-quick-baseline"><a href="#Build-a-quick-baseline" class="headerlink" title="Build a quick baseline"></a>Build a quick baseline</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</div><div class="line"></div><div class="line"><span class="comment"># Create a copy to work with</span></div><div class="line">X = train.copy()</div><div class="line"></div><div class="line"><span class="comment"># Save and drop labels</span></div><div class="line">y = train.y</div><div class="line">X = X.drop(<span class="string">'y'</span>, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># fill NANs </span></div><div class="line">X = X.fillna(<span class="number">-999</span>)</div><div class="line"></div><div class="line"><span class="comment"># Label encoder</span></div><div class="line"><span class="keyword">for</span> c <span class="keyword">in</span> train.columns[train.dtypes == <span class="string">'object'</span>]:</div><div class="line">    X[c] = X[c].factorize()[<span class="number">0</span>]</div><div class="line">    </div><div class="line">rf = RandomForestClassifier()</div><div class="line">rf.fit(X,y)</div></pre></td></tr></table></figure><pre><code>RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&apos;gini&apos;,
            max_depth=None, max_features=&apos;auto&apos;, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,
            verbose=0, warm_start=False)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plt.plot(rf.feature_importances_)</div><div class="line">plt.xticks(np.arange(X.shape[<span class="number">1</span>]), X.columns.tolist(), rotation=<span class="number">90</span>);</div></pre></td></tr></table></figure><pre><code>/home/dulyanov/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family [u&apos;serif&apos;] not found. Falling back to DejaVu Sans
  (prop.get_family(), self.defaultFamily[fontext]))
</code></pre><p><img src="https://github.com/ewanlee/blog-image-hosting/blob/master/output_6_1.png?raw=true" alt="png"></p><p>There is something interesting about <code>x8</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># we see it was standard scaled, most likely, if we concat train and test, we will get exact mean=1, and std 1 </span></div><div class="line"><span class="keyword">print</span> <span class="string">'Mean:'</span>, train.x8.mean()</div><div class="line"><span class="keyword">print</span> <span class="string">'std:'</span>, train.x8.std()</div></pre></td></tr></table></figure><pre><code>Mean: -0.000252352028622
std: 1.02328163601
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># And we see that it has a lot of repeated values</span></div><div class="line">train.x8.value_counts().head(<span class="number">15</span>)</div></pre></td></tr></table></figure><pre><code>-2.984750    2770
 0.480977    2569
 0.610941    1828
 0.654263    1759
 0.567620    1746
 0.697585    1691
 0.524298    1639
 0.740906    1628
 0.394333    1610
 0.437655    1513
 0.351012    1450
 0.264369    1429
 0.307690    1401
 0.221047    1372
 0.784228    1293
Name: x8, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># It's very hard to work with scaled feature, so let's try to scale them back</span></div><div class="line"><span class="comment"># Let's first take a look at difference between neighbouring values in x8</span></div><div class="line"></div><div class="line">x8_unique = train.x8.unique()</div><div class="line">x8_unique_sorted = np.sort(x8_unique)</div><div class="line">                           </div><div class="line">np.diff(x8_unique_sorted)</div></pre></td></tr></table></figure><pre><code>array([ 43.27826527,  38.98942817,   0.21660793,   0.04332159,
         0.17328635,   0.21660793,   0.08664317,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.12996476,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.21660793,   1.16968285,
         0.04332159,   0.38989428,          nan])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># The most of the diffs are 0.04332159! </span></div><div class="line"><span class="comment"># The data is scaled, so we don't know what was the diff value for the original feature</span></div><div class="line"><span class="comment"># But let's assume it was 1.0</span></div><div class="line"><span class="comment"># Let's devide all the numbers by 0.04332159 to get the right scaling</span></div><div class="line"><span class="comment"># note, that feature will still have zero mean</span></div><div class="line"></div><div class="line">np.diff(x8_unique_sorted/<span class="number">0.04332159</span>)</div></pre></td></tr></table></figure><pre><code>array([ 998.99992752,  899.9999347 ,    4.99999964,    0.99999993,
          3.99999971,    4.99999964,    1.99999985,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    2.99999978,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    4.99999964,   26.99999804,
          0.99999993,    8.99999935,           nan])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(train.x8/<span class="number">0.04332159</span>).head(<span class="number">10</span>)</div></pre></td></tr></table></figure><pre><code>0   -15.897530
1    20.102468
2    10.102468
3     0.102469
4    11.102468
5   -68.897526
6    10.102468
7    15.102468
8     9.102468
9   -68.897526
Name: x8, dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Ok, now we see .102468 in every value</span></div><div class="line"><span class="comment"># this looks like a part of a mean that was subtracted during standard scaling</span></div><div class="line"><span class="comment"># If we subtract it, the values become almost integers</span></div><div class="line">(train.x8/<span class="number">0.04332159</span> - <span class="number">.102468</span>).head(<span class="number">10</span>)</div></pre></td></tr></table></figure><pre><code>0   -15.999998
1    20.000000
2    10.000000
3     0.000001
4    11.000000
5   -68.999994
6    10.000000
7    15.000000
8     9.000000
9   -68.999994
Name: x8, dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># let's round them </span></div><div class="line">x8_int = (train.x8/<span class="number">0.04332159</span> - <span class="number">.102468</span>).round()</div><div class="line">x8_int.head(<span class="number">10</span>)</div></pre></td></tr></table></figure><pre><code>0   -16.0
1    20.0
2    10.0
3     0.0
4    11.0
5   -69.0
6    10.0
7    15.0
8     9.0
9   -69.0
Name: x8, dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Ok, what's next? In fact it is not obvious how to find shift parameter, </span></div><div class="line"><span class="comment"># and how to understand what the data this feature actually store</span></div><div class="line"><span class="comment"># But ...</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x8_int.value_counts()</div></pre></td></tr></table></figure><pre><code>-69.0      2770
 11.0      2569
 14.0      1828
 15.0      1759
 13.0      1746
 16.0      1691
 12.0      1639
 17.0      1628
 9.0       1610
 10.0      1513
 8.0       1450
 6.0       1429
 7.0       1401
 5.0       1372
 18.0      1293
 1.0       1290
 4.0       1276
 2.0       1250
 3.0       1213
-1.0       1085
 0.0       1080
-2.0       1006
-4.0        995
-3.0        976
-5.0        954
-8.0        923
-9.0        921
-6.0        906
 19.0       893
-7.0        881
           ... 
 26.0         3
-40.0         3
-41.0         3
 25.0         2
-59.0         2
 31.0         2
 34.0         2
-46.0         2
-49.0         2
 33.0         2
-42.0         2
 32.0         2
 37.0         2
 30.0         2
-45.0         2
-54.0         1
 36.0         1
-51.0         1
 27.0         1
 79.0         1
-47.0         1
 69.0         1
 70.0         1
-50.0         1
-1968.0       1
 42.0         1
-63.0         1
-48.0         1
-64.0         1
 35.0         1
Name: x8, Length: 99, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># do you see this -1968? Doesn't it look like a year? ... So my hypothesis is that this feature is a year of birth! </span></div><div class="line"><span class="comment"># Maybe it was a textbox where users enter their year of birth, and someone entered 0000 instead</span></div><div class="line"><span class="comment"># The hypothesis looks plausible, isn't it?</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(x8_int + <span class="number">1968.0</span>).value_counts().sort_index()</div></pre></td></tr></table></figure><pre><code>0.0          1
999.0        4
1899.0    2770
1904.0       1
1905.0       1
1909.0       2
1914.0       1
1916.0       3
1917.0       1
1918.0       1
1919.0       2
1920.0       1
1921.0       1
1922.0       2
1923.0       2
1924.0       4
1925.0       4
1926.0       2
1927.0       3
1928.0       3
1929.0       4
1930.0       4
1931.0      12
1932.0      10
1933.0       7
1934.0      13
1935.0      28
1936.0      35
1937.0      35
1938.0      45
          ... 
1978.0    1513
1979.0    2569
1980.0    1639
1981.0    1746
1982.0    1828
1983.0    1759
1984.0    1691
1985.0    1628
1986.0    1293
1987.0     893
1988.0     624
1989.0     434
1990.0     233
1991.0     110
1992.0      31
1993.0       2
1994.0       3
1995.0       1
1998.0       2
1999.0       2
2000.0       2
2001.0       2
2002.0       2
2003.0       1
2004.0       1
2005.0       2
2010.0       1
2037.0       1
2038.0       1
2047.0       1
Name: x8, Length: 99, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># After the competition ended the organisers told it was really a year of birth</span></div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt; You will not be able to run this notebook at coursera platform, as the dataset is not there. The notebook is in read-only mode.&lt;/p&gt;&lt;p&gt;But you can run the notebook locally and download the dataset using &lt;a href=&quot;https://habrastorage.org/storage/stuff/special/beeline/00.beeline_bigdata.zip&quot;&gt;this link&lt;/a&gt; to explore the data interactively.&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;pd.set_option(&lt;span class=&quot;string&quot;&gt;&#39;max_columns&#39;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&quot;Load-the-data&quot;&gt;&lt;a href=&quot;#Load-the-data&quot; class=&quot;headerlink&quot; title=&quot;Load the data&quot;&gt;&lt;/a&gt;Load the data&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;train = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;./train.csv&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;train.head()&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Exploratory Data Analysis</title>
    <link href="http://yoursite.com/2018/10/16/Exploratory-Data-Analysis/"/>
    <id>http://yoursite.com/2018/10/16/Exploratory-Data-Analysis/</id>
    <published>2018-10-16T06:39:40.000Z</published>
    <updated>2018-10-16T07:15:25.325Z</updated>
    
    <content type="html"><![CDATA[<p>This is a detailed EDA of the data, shown in the second video of “Exploratory data analysis” lecture (week 2).</p><p><strong>PLEASE NOTE</strong>: the dataset cannot be published, so this notebook is read-only.</p><h2 id="Load-data"><a href="#Load-data" class="headerlink" title="Load data"></a>Load data</h2><p>In this competition hosted by <em>solutions.se</em>, the task was to predict the advertisement cost for a particular ad.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">%matplotlib inline</div><div class="line"></div><div class="line">data_path = <span class="string">'./data'</span></div><div class="line">train = pd.read_csv(<span class="string">'%s/train.csv.gz'</span> % data_path, parse_dates=[<span class="string">'Date'</span>])</div><div class="line">test  = pd.read_csv(<span class="string">'%s/test.csv.gz'</span> % data_path,  parse_dates=[<span class="string">'Date'</span>])</div></pre></td></tr></table></figure><p>Let’s look at the data (notice that the table is transposed, so we can see all feature names).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.head().T</div></pre></td></tr></table></figure><a id="more"></a><table border="1" class="dataframe"><br><br><br><thead><br><tr style="text-align:right"><br><th></th><br><th>0</th><br><th>1</th><br><th>2</th><br><th>3</th><br><th>4</th><br></tr><br></thead><br><tbody><br><tr><br><th>AdGroupId</th><br><td>78db034136</td><br><td>68a0110c69</td><br><td>21af1035af</td><br><td>f63fda0c33</td><br><td>cd868ebdcc</td><br></tr><br><tr><br><th>AdGroupName</th><br><td>6d91d 25866 9c594</td><br><td>2657d cb2d0 6d91d</td><br><td>6d91d e33a0 9a99b</td><br><td>59991 9c594</td><br><td>6d91d 25866 9a99b</td><br></tr><br><tr><br><th>AdNetworkType2</th><br><td>s</td><br><td>s</td><br><td>s</td><br><td>s</td><br><td>s</td><br></tr><br><tr><br><th>AveragePosition</th><br><td>1.2</td><br><td>2</td><br><td>1</td><br><td>1</td><br><td>1.1</td><br></tr><br><tr><br><th>CampaignId</th><br><td>273823cb71</td><br><td>273823cb71</td><br><td>273823cb71</td><br><td>273823cb71</td><br><td>273823cb71</td><br></tr><br><tr><br><th>CampaignName</th><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br></tr><br><tr><br><th>Clicks</th><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br><td>3</td><br></tr><br><tr><br><th>Conversions</th><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br></tr><br><tr><br><th>ConversionsManyPerClick</th><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br></tr><br><tr><br><th>Cost</th><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0.94</td><br></tr><br><tr><br><th>Date</th><br><td>2014-01-01 00:00:00</td><br><td>2014-01-01 00:00:00</td><br><td>2014-01-01 00:00:00</td><br><td>2014-01-01 00:00:00</td><br><td>2014-01-01 00:00:00</td><br></tr><br><tr><br><th>DestinationUrl</th><br><td>98035d60fc</td><br><td>c25f23cd08</td><br><td>01f87f7639</td><br><td>5c0e89f532</td><br><td>8888b55dde</td><br></tr><br><tr><br><th>Device</th><br><td>t</td><br><td>t</td><br><td>t</td><br><td>d</td><br><td>d</td><br></tr><br><tr><br><th>FirstPageCpc</th><br><td>1.06</td><br><td>2.94</td><br><td>0.42</td><br><td>1.75</td><br><td>0.17</td><br></tr><br><tr><br><th>Impressions</th><br><td>32</td><br><td>1</td><br><td>4</td><br><td>1</td><br><td>22</td><br></tr><br><tr><br><th>KeywordMatchType</th><br><td>b</td><br><td>b</td><br><td>b</td><br><td>b</td><br><td>b</td><br></tr><br><tr><br><th>KeywordText</th><br><td>jze 10 +uxsgk</td><br><td>+jze +dznvgyhjclr</td><br><td>jze 100 +gzpxyk</td><br><td>jze 10 +uxsgk 1950k</td><br><td>jze 10 mykj +gzpxyk</td><br></tr><br><tr><br><th>MaxCpc</th><br><td>0.28</td><br><td>1</td><br><td>0.22</td><br><td>0.54</td><br><td>0.12</td><br></tr><br><tr><br><th>QualityScore</th><br><td>1</td><br><td>1</td><br><td>1</td><br><td>1</td><br><td>1</td><br></tr><br><tr><br><th>Slot</th><br><td>s_2</td><br><td>s_2</td><br><td>s_1</td><br><td>s_2</td><br><td>s_1</td><br></tr><br><tr><br><th>TopOfPageCpc</th><br><td>1.07</td><br><td>5.02</td><br><td>0.42</td><br><td>4</td><br><td>0.25</td><br></tr><br><tr><br><th>KeywordId</th><br><td>7d20d63df9</td><br><td>a617d4f037</td><br><td>6e0b7024d2</td><br><td>9c2ea0cdf8</td><br><td>4c8ba7affd</td><br></tr><br></tbody><br></table><p>We see a lot of features with not obvious names. If you search for the <em>CampaignId</em>, <em>AdGroupName</em>, <em>AdNetworkType2</em> using any web search engine, you will find this dataset was exported from Google AdWords. So what is the required domain knowledge here? The knowledge of how web advertisement and Google AdWords work! After you have learned it, the features will make sense to you and you can proceed.</p><p>For the sake of the story I will briefly describe Google AdWords system now. Basically every time a user queries a search engine, Google AdWords decides what ad will be shown along with the actual search results. On the other side of AdWords, the advertisers manage the ads – they can set a multiple keywords, that a user should query in order to their ad to be shown. If the keywords are set properly and are relevant to the ad, then the ad will be shown to relevant users and the ad will get clicked. Advertisers pay to Google for some type of events, happened with their ad: for example for a click event, i.e. the user saw this ad and clicked it. AdWords uses complex algorithms to decide which ad to show to a particular user with a particular search query. The advertisers can only indirectly influence AdWords decesion process by changing keywords and several other parameters. So at a high level, the task is to predict what will be the costs for the advertiser (how much he will pay to Google, column <em>Cost</em>) when the parameters (e.g. keywords) are changed.</p><p>The ads are grouped in groups, there are features <em>AdGroupId</em> <em>AdGroupName</em> describing them. A campaign corresponds to some specific parameters that an advertiser sets. Similarly, there are ID and name features <em>CampaignId</em>, <em>CampaignName</em>. And finally there is some information about keywords: <em>KeywordId</em> and <em>KeywordText</em>. Slot is $1$ when ad is shown on top of the page, and $2$ when on the side. Device is a categorical variable and can be either “tablet”, “mobile” or “pc”. And finally the <em>Date</em> is just the date, for which clicks were aggregated.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.head().T</div></pre></td></tr></table></figure><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>0</th><br><th>1</th><br><th>2</th><br><th>3</th><br><th>4</th><br></tr><br></thead><br><tbody><br><tr><br><th>Id</th><br><td>0</td><br><td>1</td><br><td>2</td><br><td>3</td><br><td>4</td><br></tr><br><tr><br><th>AdGroupId</th><br><td>00096e7611</td><br><td>00096e7611</td><br><td>00096e7611</td><br><td>00096e7611</td><br><td>00096e7611</td><br></tr><br><tr><br><th>AdGroupName</th><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br></tr><br><tr><br><th>AdNetworkType2</th><br><td>s</td><br><td>s</td><br><td>s</td><br><td>s</td><br><td>s</td><br></tr><br><tr><br><th>AveragePosition</th><br><td>1</td><br><td>1</td><br><td>1</td><br><td>1</td><br><td>1</td><br></tr><br><tr><br><th>CampaignId</th><br><td>e62b4bc4c3</td><br><td>e62b4bc4c3</td><br><td>e62b4bc4c3</td><br><td>e62b4bc4c3</td><br><td>e62b4bc4c3</td><br></tr><br><tr><br><th>CampaignName</th><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br></tr><br><tr><br><th>Date</th><br><td>2014-06-01 00:00:00</td><br><td>2014-06-01 00:00:00</td><br><td>2014-06-01 00:00:00</td><br><td>2014-06-01 00:00:00</td><br><td>2014-06-01 00:00:00</td><br></tr><br><tr><br><th>DestinationUrl</th><br><td>f5aad09031</td><br><td>f5aad09031</td><br><td>f5aad09031</td><br><td>f5aad09031</td><br><td>f5aad09031</td><br></tr><br><tr><br><th>Device</th><br><td>t</td><br><td>d</td><br><td>m</td><br><td>t</td><br><td>d</td><br></tr><br><tr><br><th>KeywordId</th><br><td>539778bb80</td><br><td>539778bb80</td><br><td>539778bb80</td><br><td>539778bb80</td><br><td>539778bb80</td><br></tr><br><tr><br><th>KeywordMatchType</th><br><td>e</td><br><td>e</td><br><td>e</td><br><td>e</td><br><td>e</td><br></tr><br><tr><br><th>KeywordText</th><br><td>tcjnw gzpxyk nyss ewzhy</td><br><td>tcjnw gzpxyk nyss ewzhy</td><br><td>tcjnw gzpxyk nyss ewzhy</td><br><td>tcjnw gzpxyk nyss ewzhy</td><br><td>tcjnw gzpxyk nyss ewzhy</td><br></tr><br><tr><br><th>Slot</th><br><td>s_1</td><br><td>s_1</td><br><td>s_1</td><br><td>s_2</td><br><td>s_2</td><br></tr><br></tbody><br></table><p>Notice there is diffrent number of columns in test and train – our target is <em>Cost</em> column, but it is closly related to several other features, e.g. <em>Clicks</em>, <em>Conversions</em>. All of the related columns were deleted from the test set to avoid data leakages.</p><h1 id="Let’s-analyze"><a href="#Let’s-analyze" class="headerlink" title="Let’s analyze"></a>Let’s analyze</h1><p>Are we ready to modeling? Not yet. Take a look at this statistic:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> <span class="string">'Train min/max date: %s / %s'</span> % (train.Date.min().date(), train.Date.max().date())</div><div class="line"><span class="keyword">print</span> <span class="string">'Test  min/max date: %s / %s'</span> % ( test.Date.min().date(),  test.Date.max().date())</div><div class="line"><span class="keyword">print</span> <span class="string">''</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Number of days in train: %d'</span> % ((train.Date.max() - train.Date.min()).days + <span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">'Number of days in test:  %d'</span> % (( test.Date.max() -  test.Date.min()).days + <span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">''</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Train shape: %d rows'</span> % train.shape[<span class="number">0</span>]</div><div class="line"><span class="keyword">print</span> <span class="string">'Test shape: %d rows'</span>  % test.shape[<span class="number">0</span>]</div></pre></td></tr></table></figure><pre><code>Train min/max date: 2014-01-01 / 2014-05-31
Test  min/max date: 2014-06-01 / 2014-06-14

Number of days in train: 151
Number of days in test:  14

Train shape: 3493820 rows
Test shape: 8951040 rows
</code></pre><p>Train period is more than 10 times larger than the test period, but train set has fewer rows, how could that happen?</p><p>At this point I suggest you to stop and think yourself, what could be a reason, why this did happen. Unfortunately we cannot share the data for this competition, but the information from above should be enough to get a right idea.</p><p>Alternatively, you can go along for the explanation, if you want.</p><h1 id="Investigation"><a href="#Investigation" class="headerlink" title="Investigation"></a>Investigation</h1><p>Let’s take a look how many rows with each date we have in train and test.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.Date.value_counts()</div></pre></td></tr></table></figure><pre><code>2014-06-02    639360
2014-06-12    639360
2014-06-09    639360
2014-06-14    639360
2014-06-01    639360
2014-06-11    639360
2014-06-08    639360
2014-06-05    639360
2014-06-10    639360
2014-06-07    639360
2014-06-04    639360
2014-06-06    639360
2014-06-03    639360
2014-06-13    639360
Name: Date, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># print only first 10</span></div><div class="line">train.Date.value_counts().head(<span class="number">10</span>)</div></pre></td></tr></table></figure><pre><code>2014-01-01    36869
2014-01-04    36427
2014-01-05    36137
2014-01-02    34755
2014-01-03    34693
2014-01-06    31349
2014-04-07    30950
2014-02-09    30101
2014-01-26    29830
2014-02-08    29187
Name: Date, dtype: int64
</code></pre><p>Interesting, for the test set we have the same number of rows for every date, while in train set the number of rows is different for each day. It looks like that for each day in the test set a loop through some kind of IDs had been run. But what about train set? So far we don’t know, but let’s find the test IDs first.</p><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>So now we know, that there is $639360$ different IDs. It should be easy to find the columns, that form ID, because if the ID is [‘col1’, ‘col2’], then to compute the number of combinations we should just multiply the number of unique elements in each.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">test_nunique = test.nunique()</div><div class="line">test_nunique</div></pre></td></tr></table></figure><pre><code>Id                  8951040
AdGroupId             13548
AdGroupName            2281
AdNetworkType2            2
AveragePosition         131
CampaignId              252
CampaignName            252
Date                     14
DestinationUrl        52675
Device                    3
KeywordId             12285
KeywordMatchType          3
KeywordText           11349
Slot                      4
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"><span class="comment"># This function looks for a combination of elements </span></div><div class="line"><span class="comment"># with product of 639360 </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_prod</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="comment"># combinations of not more than 5 features</span></div><div class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>):</div><div class="line">        <span class="comment"># iterate through all combinations</span></div><div class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> itertools.combinations(range(len(data)), n):</div><div class="line">            <span class="keyword">if</span> data[list(c)].prod() == <span class="number">639360</span>:</div><div class="line">                <span class="keyword">print</span> test_nunique.index[c]</div><div class="line">                <span class="keyword">return</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Nothing found'</span></div><div class="line"></div><div class="line">    </div><div class="line">find_prod(test_nunique.values)</div></pre></td></tr></table></figure><pre><code>Nothing found
</code></pre><p>Hmm, nothing found! The problem is that some features are tied, and the number of their combinations does not equal to product of individual unique number of elements. For example it does not make sense to create all possible combinations of <em>DestinationUrl</em> and <em>AdGroupId</em> as <em>DestinationUrl</em> belong to exactly one <em>AdGroupId</em>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.groupby(<span class="string">'DestinationUrl'</span>).AdGroupId.nunique()</div></pre></td></tr></table></figure><pre><code>DestinationUrl
00010d62df    1
000249f717    1
00054cf3f8    1
000684bf0b    1
00072a9fa7    1
00077a6729    1
0007cc191f    1
0009388900    1
001144cae4    1
00115f6477    1
00141a299f    1
00169dc49b    1
0018b27e06    1
001b0b3d06    1
001ef8368e    1
00205e056a    1
002082ab8b    1
0020c585ea    1
0021419f7e    1
00225519cc    1
002498dc88    1
0026171436    1
00265dc4bb    1
0026833e5c    1
0027ffbad9    1
002b1deb25    1
002c55ccef    1
002e44290f    1
0030ca870e    1
0032b64beb    1
             ..
ffda377018    1
ffda3c412a    1
ffda5b53d6    1
ffda8c0d8c    1
ffdbf5d179    1
ffdc872fcf    1
ffde114af5    1
ffde41a800    1
ffe2fb7007    1
ffe4a040d4    1
ffe685e937    1
ffe8c3da53    1
ffe8f82e08    1
ffeb9fda9d    1
ffebd1d253    1
ffebea724f    1
ffecf398b1    1
ffecf3e7d4    1
ffed185438    1
fff02d7269    1
fff10adcb0    1
fff12e5f19    1
fff132d5bd    1
fff19836a0    1
fff3539204    1
fff4c5d255    1
fff55db78a    1
fff8c11ad9    1
fff90ea351    1
fffb248bf0    1
Name: AdGroupId, Length: 52675, dtype: int64
</code></pre><p>So, now let’s try to find ID differently. Let’s try to find a list of columns, such that threre is exazctly $639360$ unique combinations of their values <strong>in the test set</strong> (not overall). So, we want to find <code>columns</code>, such that:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test[columns].drop_duplicates().shape[<span class="number">0</span>]  == <span class="number">639360</span></div></pre></td></tr></table></figure><p>We could do it with a similar loop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_ncombinations</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="comment"># combinations of not more than 5 features</span></div><div class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>):</div><div class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> itertools.combinations(range(data.shape[<span class="number">1</span>]), n):</div><div class="line">            <span class="keyword">print</span> c</div><div class="line">            columns = test.columns[list(c)]</div><div class="line">            <span class="keyword">if</span> test[columns].drop_duplicates().shape[<span class="number">0</span>] == <span class="number">639360</span>:</div><div class="line">                <span class="keyword">print</span> columns</div><div class="line">                <span class="keyword">return</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Nothing found'</span></div><div class="line"></div><div class="line">    </div><div class="line">find_ncombinations(test)</div></pre></td></tr></table></figure><p>But it will take forever to compute. So it is easier to find the combination manually.</p><p>So after some time of trials and errors I figured out, that the four features <em>KeywordId, AdGroupId, Device, Slot</em> form the index. The number of unique rows is exactly <em>639360</em> as we wanted to find.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">columns = [<span class="string">'KeywordId'</span>, <span class="string">'AdGroupId'</span>, <span class="string">'Device'</span>, <span class="string">'Slot'</span>]</div><div class="line">test[columns].drop_duplicates().shape</div></pre></td></tr></table></figure><pre><code>(639360, 4)
</code></pre><p>Looks reasonable. For each <em>AdGroupId</em> there is a <strong>distinct set</strong> of possible <em>KeywordId’s</em>, but <em>Device</em> and <em>Slot</em> variants are the same for each ad. And the target is to predict what will be the daily cost for using different <em>KeywordId’s</em>, <em>Device</em> type, <em>Slot</em> type to advertise ads from <em>AdGroups</em>.</p><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><p>To this end, we found how test set was constructed, but what about the train set? Let us plot something, probably we will find it out.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line">sns.set(palette=<span class="string">'pastel'</span>)</div><div class="line">sns.set(font_scale=<span class="number">2</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># from absolute dates to relative</span></div><div class="line">train[<span class="string">'date_diff'</span>] =  (train.Date - train.Date.min()).dt.days</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># group by the index, that we've found</span></div><div class="line">g= train.groupby([<span class="string">'KeywordId'</span>, <span class="string">'AdGroupId'</span>, <span class="string">'Device'</span>, <span class="string">'Slot'</span>])</div><div class="line"></div><div class="line"><span class="comment"># and for each index show average relative date versus </span></div><div class="line"><span class="comment"># the number of rows with that index</span></div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>))</div><div class="line">plt.scatter(g.date_diff.mean(),g.size(),edgecolor = <span class="string">'none'</span>,alpha = <span class="number">0.2</span>, s=<span class="number">20</span>, c=<span class="string">'b'</span>)</div><div class="line">plt.xlabel(<span class="string">'Group mean relative date'</span>)</div><div class="line">plt.ylabel(<span class="string">'Group size'</span>)</div><div class="line">plt.title(<span class="string">'Train'</span>);</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_36_0.png" alt="png"></p><p>Looks interesting, isn’t it? That is something we need to explain! How the same plot looks for the test set?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># from absolute dates to relative</span></div><div class="line">test[<span class="string">'date_diff'</span>] =  (test.Date - test.Date.min()).dt.days</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># group by the index, that we've found</span></div><div class="line">g= test.groupby([<span class="string">'KeywordId'</span>, <span class="string">'AdGroupId'</span>, <span class="string">'Device'</span>, <span class="string">'Slot'</span>])</div><div class="line"></div><div class="line"><span class="comment"># and for each index show average relative date versus </span></div><div class="line"><span class="comment"># the number of rows with that index</span></div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>))</div><div class="line">plt.scatter(g.date_diff.mean(),g.size(),edgecolor = <span class="string">'none'</span>,alpha = <span class="number">0.2</span>, s=<span class="number">20</span>, c=<span class="string">'b'</span>)</div><div class="line">plt.xlabel(<span class="string">'Group mean relative date'</span>)</div><div class="line">plt.ylabel(<span class="string">'Group size'</span>)</div><div class="line">plt.ylim(<span class="number">-2</span>, <span class="number">30</span>)</div><div class="line">plt.title(<span class="string">'Test'</span>);</div></pre></td></tr></table></figure><p><img src="https://github.com/ewanlee/blog-image-hosting/blob/master/output_39_0.png?raw=true" alt="png"></p><p>Just a dot!</p><p>Now let’s think, what we actually plotted? We grouped the data by the ID that we’ve found previously and we plotted average <em>Date</em> in the group versus the size of each group. We found that ID is an aggregation index – so for each date the <em>Cost</em> is aggreagated for each possible index. So group size shows for how many days we have <em>Const</em> information for each ID and mean relative date shows some information about these days.</p><p>For test set it is expectable that both average date and the size of the groups are the same for each group: the size of each group is $14$ (as we have $14$ test days) and mean date is $6.5$, because for each group (index) we have $14$ different days, and $\frac{0 + 1 + \dots + 13}{14} = 6.5$.</p><p>And now we can explain everything for the train set. Look at the top of the triangle: for those points (groups) we have <em>Cost</em> information for all the days in the train period, while on the sides we see groups, for which we have very few rows.</p><p>But why for some groups we have smaller number of rows, than number of days? Let’s look at the <em>Impressions</em> column.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.Impressions.value_counts()</div></pre></td></tr></table></figure><pre><code>1         1602929
2          565896
3          287128
4          175197
5          119092
6           86651
7           66443
8           53007
9           42984
10          35731
11          30248
12          25950
13          22629
14          20126
15          17503
16          15682
17          14100
18          12848
19          11597
20          10724
21           9864
22           8931
23           8316
24           7953
25           7168
26           6684
27           6196
28           5863
29           5556
30           5223
           ...   
4978            1
15210           1
9076            1
13174           1
116535          1
4979            1
17273           1
90974           1
4976            1
5906            1
7023            1
60282           1
7955            1
13881           1
2921            1
4970            1
7019            1
17249           1
23394           1
28210           1
11116           1
15929           1
7017            1
95761           1
2923            1
15213           1
9070            1
5692            1
13162           1
13922           1
Name: Impressions, Length: 8135, dtype: int64
</code></pre><p>We never have $0$ value in <em>Imressions</em> column. But in reality, of course, some ads with some combination of keyword, slot, device were never shown. So this looks like a nice explanation for the data: in the train set we <strong>only</strong> have information about ads (IDs, groups) which were shown at least once. And for the test set, we, of course, want to predict <em>Cost</em> <strong>for every</strong> possible ID.</p><p>What it means for competitors, is that if one would just fit a model on the train set as is, the predictions for the test set will be biased by a lot. The predictions will be much higher than they should be, as we are only given a specific subset of rows as <code>train.csv</code> file.</p><p>So, before modeling we should first extend the trainset and inject rows with <code>0</code> impressions. Such change will make train set very similar to the test set and the models will generalize nicely.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is a detailed EDA of the data, shown in the second video of “Exploratory data analysis” lecture (week 2).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;PLEASE NOTE&lt;/strong&gt;: the dataset cannot be published, so this notebook is read-only.&lt;/p&gt;&lt;h2 id=&quot;Load-data&quot;&gt;&lt;a href=&quot;#Load-data&quot; class=&quot;headerlink&quot; title=&quot;Load data&quot;&gt;&lt;/a&gt;Load data&lt;/h2&gt;&lt;p&gt;In this competition hosted by &lt;em&gt;solutions.se&lt;/em&gt;, the task was to predict the advertisement cost for a particular ad.&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;%matplotlib inline&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;data_path = &lt;span class=&quot;string&quot;&gt;&#39;./data&#39;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;train = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;%s/train.csv.gz&#39;&lt;/span&gt; % data_path, parse_dates=[&lt;span class=&quot;string&quot;&gt;&#39;Date&#39;&lt;/span&gt;])&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;test  = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;%s/test.csv.gz&#39;&lt;/span&gt; % data_path,  parse_dates=[&lt;span class=&quot;string&quot;&gt;&#39;Date&#39;&lt;/span&gt;])&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;Let’s look at the data (notice that the table is transposed, so we can see all feature names).&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;train.head().T&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理相关技术</title>
    <link href="http://yoursite.com/2018/10/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/"/>
    <id>http://yoursite.com/2018/10/15/数据预处理相关技术/</id>
    <published>2018-10-15T14:07:35.000Z</published>
    <updated>2018-10-15T14:35:02.055Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数值型数据-non-tree-based-model"><a href="#数值型数据-non-tree-based-model" class="headerlink" title="数值型数据 (non-tree based model)"></a>数值型数据 (non-tree based model)</h1><ul><li>特征预处理<ul><li>MinMaxScalar 不会改变数据分布</li><li>StandardScalar</li><li>scipy.stats.rankdata</li><li>log transform <code>np.log(1+x)</code></li><li>raising to the power &lt; 1 <code>np.sqrt(x + 2/3)</code></li><li><strong>drop outlier（winsorization，specify upper and lower bound）</strong></li></ul></li></ul><p>融合不同预处理方法得到的特征训练一个模型或者每一种特征训练出一个模型最后做模型融合</p><ul><li>特征生成<ul><li><strong>主要依据先验经验以及对数据的深刻理解</strong></li><li>例如，浮点数的小数部分单独提取出来作为特征</li></ul></li></ul><h1 id="类别数据以及有序类别数据"><a href="#类别数据以及有序类别数据" class="headerlink" title="类别数据以及有序类别数据"></a>类别数据以及有序类别数据</h1><ul><li>特征预处理<ul><li>Label encoding (tree(or non-tree)-based model)<ul><li>alphabetical sorted <code>sklearn.preprocessing.LabelEncoder</code></li><li>order of appearance <code>Pandas.factorize</code></li><li><strong>frequency encoding</strong> （非常适用于测试数据中包含训练数据未包含的类别）</li></ul></li><li>Label encoding (non-tree(or tree)-based model)<ul><li>one-hot encoding (sparse matrix)</li></ul></li></ul></li><li>特征生成<ul><li>枚举不同的类别特征的组合形成新的类别特征 (linear models and KNN)</li></ul></li></ul><h1 id="日期数据以及坐标数据"><a href="#日期数据以及坐标数据" class="headerlink" title="日期数据以及坐标数据"></a>日期数据以及坐标数据</h1><h2 id="日期数据"><a href="#日期数据" class="headerlink" title="日期数据"></a>日期数据</h2><ul><li>特征生成<ul><li>周期性数据<ul><li>Day number in week, month, season, year</li><li>second, minute, second</li></ul></li><li>自什么时候以来<ul><li>问题无关， 比如自1970年1月1日以来</li><li>问题相关，比如距离下一个节假日还有多少天等等</li></ul></li><li>两个日期特征之间的差值</li></ul></li></ul><h2 id="坐标数据"><a href="#坐标数据" class="headerlink" title="坐标数据"></a>坐标数据</h2><ul><li><p>特征生成</p><ul><li>距离某些关键坐标的距离等等（需要外部数据支持）</li><li>对坐标进行网格化或者聚类，然后计算每个网格中的点距离选定点的距离或者每个簇中的点距离聚类中心的距离</li><li>点的密度（某一限定范围之内）</li><li>区域价值，例如物价房价等（某一限定范围之内）</li></ul></li><li><p>特征预处理</p><ul><li>坐标旋转（例如45°）</li></ul></li></ul><h1 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h1><ul><li>找出隐含的NaN，通过可视化数据分布</li><li>填充方法<ul><li><code>-999, -1</code>等</li><li>中值，均值等</li><li>尝试恢复缺失数据（线性回归）</li></ul></li><li>特征生成<ul><li>增加一个特征，是否有缺失值</li><li><strong>采用填充的缺失值进行特征生成要特别小心，一般来说若要进行特征生成，则最好不要在之前进行缺失值填充</strong></li></ul></li><li>xgboost对于缺失值不敏感</li></ul><h1 id="文本数据"><a href="#文本数据" class="headerlink" title="文本数据"></a>文本数据</h1><ul><li>特征生成<ul><li>词袋 <code>skearn.feature_extraction.text.CountVectorizer</code></li><li>TF-IDF <code>skearn.feature_extraction.text.TfidfVectorizer</code></li><li>N-grams <code>ngram</code></li></ul></li><li>特征预处理<ul><li>lowercase</li><li>lemmatization (单词最原始的形式)</li><li>stemming</li><li>stopwords <code>nltk</code></li></ul></li><li><strong>Word2Vec, Doc2vec, Glove, FastText, etc</strong></li><li>Pipeline<ol><li>预处理</li><li>Ngrams then TF-IDF</li><li>or Word2Vec, etc</li></ol></li></ul><h1 id="图像数据"><a href="#图像数据" class="headerlink" title="图像数据"></a>图像数据</h1><ul><li>可以结合不同层的特征图</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;数值型数据-non-tree-based-model&quot;&gt;&lt;a href=&quot;#数值型数据-non-tree-based-model&quot; class=&quot;headerlink&quot; title=&quot;数值型数据 (non-tree based model)&quot;&gt;&lt;/a&gt;数值型数据 
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>About Feature Scaling and Normalization</title>
    <link href="http://yoursite.com/2018/10/15/About-Feature-Scaling-and-Normalization/"/>
    <id>http://yoursite.com/2018/10/15/About-Feature-Scaling-and-Normalization/</id>
    <published>2018-10-15T13:48:20.000Z</published>
    <updated>2018-10-15T14:05:31.310Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About-standardization"><a href="#About-standardization" class="headerlink" title="About standardization"></a>About standardization</h2><p>The result of <strong>standardization</strong> (or <strong>Z-score normalization</strong>) is that the features will be rescaled so that they’ll have the properties of a standard normal distribution with $\mu=0$ and $\sigma=1$</p><p>where $\mu$ is the mean (average) and $\sigma$ is the standard deviation from the mean; standard scores (also called <strong>z</strong> scores) of the samples are calculated as follows:<br>$$<br>z = \frac{x - \mu}{\sigma}<br>$$<br>Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. Intuitively, we can think of gradient descent as a prominent example (an optimization algorithm often used in logistic regression, SVMs, perceptrons, neural networks etc.); with features being on different scales, certain weights may update faster than others since the feature values $x_j$ play a role in the weight updates<br>$$<br>\Delta w_j = - \eta \frac{\partial J}{\partial w_j} = \eta \sum_i (t^{(i)} - o^{(i)}) x_j^{(i)},<br>$$<br>so that</p><p>$w_j := w_j + \Delta w_j$, where $\eta$ is the learning rate, $t$ the target class label, and $o$ the actual output. Other intuitive examples include K-Nearest Neighbor algorithms and clustering algorithms that use, for example, Euclidean distance measures – in fact, tree-based classifier are probably the only classifiers where feature scaling doesn’t make a difference.</p><p>In fact, the only family of algorithms that I could think of being scale-invariant are tree-based methods. Let’s take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as “is feature x_i &gt;= some_val?” Intuitively, we can see that it really doesn’t matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale – it really doesn’t matter).</p><p>Some examples of algorithms where feature scaling matters are:</p><ul><li>k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally</li><li>k-means (see k-nearest neighbors)</li><li>logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others</li><li>linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not.</li></ul><p>In addition, we’d also want to think about whether we want to “standardize” or “normalize” (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights “equally.” As a rule of thumb I’d say: When in doubt, just standardize the data, it shouldn’t hurt.</p><h2 id="About-Min-Max-scaling"><a href="#About-Min-Max-scaling" class="headerlink" title="About Min-Max scaling"></a>About Min-Max scaling</h2><p>An alternative approach to Z-score normalization (or standardization) is the so-called <strong>Min-Max scaling</strong>(often also simply called “normalization” - a common cause for ambiguities).<br>In this approach, the data is scaled to a fixed range - usually 0 to 1.<br>The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.</p><p>A Min-Max scaling is typically done via the following equation:<br>$$<br>X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}<br>$$</p><h2 id="Z-score-standardization-or-Min-Max-scaling"><a href="#Z-score-standardization-or-Min-Max-scaling" class="headerlink" title="Z-score standardization or Min-Max scaling?"></a>Z-score standardization or Min-Max scaling?</h2><p><em>“Standardization or Min-Max scaling?”</em> - There is no obvious answer to this question: it really depends on the application.</p><p>For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; <a href="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html" target="_blank" rel="external">but more about PCA in my previous article</a>).</p><p>However, this doesn’t mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale.</p><h2 id="Standardizing-and-normalizing-how-it-can-be-done-using-scikit-learn"><a href="#Standardizing-and-normalizing-how-it-can-be-done-using-scikit-learn" class="headerlink" title="Standardizing and normalizing - how it can be done using scikit-learn"></a>Standardizing and normalizing - how it can be done using scikit-learn</h2><p>Of course, we could make use of NumPy’s vectorization capabilities to calculate the z-scores for standardization and to normalize the data using the equations that were mentioned in the previous sections. However, there is an even more convenient approach using the preprocessing module from one of Python’s open-source machine learning library <a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn</a>.</p><p>For the following examples and discussion, we will have a look at the free “Wine” Dataset that is deposited on the UCI machine learning repository<br>(<a href="http://archive.ics.uci.edu/ml/datasets/Wine" target="_blank" rel="external">http://archive.ics.uci.edu/ml/datasets/Wine</a>).</p><blockquote><p>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</p><p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml" target="_blank" rel="external">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p></blockquote><p>The Wine dataset consists of 3 different classes where each row correspond to a particular wine sample.</p><p>The class labels (1, 2, 3) are listed in the first column, and the columns 2-14 correspond to 13 different attributes (features):</p><p>1) Alcohol<br>2) Malic acid<br>…</p><h4 id="Loading-the-wine-dataset"><a href="#Loading-the-wine-dataset" class="headerlink" title="Loading the wine dataset"></a>Loading the wine dataset</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.io.parsers.read_csv(</div><div class="line">    <span class="string">'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv'</span>,</div><div class="line">     header=<span class="keyword">None</span>,</div><div class="line">     usecols=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</div><div class="line">    )</div><div class="line"></div><div class="line">df.columns=[<span class="string">'Class label'</span>, <span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]</div><div class="line"></div><div class="line">df.head()</div></pre></td></tr></table></figure><table><thead><tr><th></th><th>Class label</th><th>Alcohol</th><th>Malic acid</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>14.23</td><td>1.71</td></tr><tr><td>1</td><td>1</td><td>13.20</td><td>1.78</td></tr><tr><td>2</td><td>1</td><td>13.16</td><td>2.36</td></tr><tr><td>3</td><td>1</td><td>14.37</td><td>1.95</td></tr><tr><td>4</td><td>1</td><td>13.24</td><td>2.59</td></tr></tbody></table><p>As we can see in the table above, the features <strong>Alcohol</strong> (percent/volumne) and <strong>Malic acid</strong> (g/l) are measured on different scales, so that <strong>Feature Scaling</strong> is necessary important prior to any comparison or combination of these data.</p><h4 id="Standardization-and-Min-Max-scaling"><a href="#Standardization-and-Min-Max-scaling" class="headerlink" title="Standardization and Min-Max scaling"></a>Standardization and Min-Max scaling</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line">std_scale = preprocessing.StandardScaler().fit(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line">df_std = std_scale.transform(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line"></div><div class="line">minmax_scale = preprocessing.MinMaxScaler().fit(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line">df_minmax = minmax_scale.transform(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'Mean after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_std[:,<span class="number">0</span>].mean(), df_std[:,<span class="number">1</span>].mean()))</div><div class="line">print(<span class="string">'\nStandard deviation after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_std[:,<span class="number">0</span>].std(), df_std[:,<span class="number">1</span>].std()))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Mean after standardization:</div><div class="line">Alcohol=<span class="number">0.00</span>, Malic acid=<span class="number">0.00</span></div><div class="line"></div><div class="line">Standard deviation after standardization:</div><div class="line">Alcohol=<span class="number">1.00</span>, Malic acid=<span class="number">1.00</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'Min-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_minmax[:,<span class="number">0</span>].min(), df_minmax[:,<span class="number">1</span>].min()))</div><div class="line">print(<span class="string">'\nMax-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_minmax[:,<span class="number">0</span>].max(), df_minmax[:,<span class="number">1</span>].max()))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Min-value after min-max scaling:</div><div class="line">Alcohol=<span class="number">0.00</span>, Malic acid=<span class="number">0.00</span></div><div class="line"></div><div class="line">Max-value after min-max scaling:</div><div class="line">Alcohol=<span class="number">1.00</span>, Malic acid=<span class="number">1.00</span></div></pre></td></tr></table></figure><h4 id="Plotting"><a href="#Plotting" class="headerlink" title="Plotting"></a>Plotting</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">()</span>:</span></div><div class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</div><div class="line"></div><div class="line">    plt.scatter(df[<span class="string">'Alcohol'</span>], df[<span class="string">'Malic acid'</span>],</div><div class="line">            color=<span class="string">'green'</span>, label=<span class="string">'input scale'</span>, alpha=<span class="number">0.5</span>)</div><div class="line"></div><div class="line">    plt.scatter(df_std[:,<span class="number">0</span>], df_std[:,<span class="number">1</span>], color=<span class="string">'red'</span>,</div><div class="line">            label=<span class="string">'Standardized [$$N  (\mu=0, \; \sigma=1)$$]'</span>, alpha=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">    plt.scatter(df_minmax[:,<span class="number">0</span>], df_minmax[:,<span class="number">1</span>],</div><div class="line">            color=<span class="string">'blue'</span>, label=<span class="string">'min-max scaled [min=0, max=1]'</span>, alpha=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">    plt.title(<span class="string">'Alcohol and Malic Acid content of the wine dataset'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Alcohol'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Malic Acid'</span>)</div><div class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</div><div class="line">    plt.grid()</div><div class="line"></div><div class="line">    plt.tight_layout()</div><div class="line"></div><div class="line">plot()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_44_0.png" alt="png"></p><p>The plot above includes the wine datapoints on all three different scales: the input scale where the alcohol content was measured in volume-percent (green), the standardized features (red), and the normalized features (blue). In the following plot, we will zoom in into the three different axis-scales.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">fig, ax = plt.subplots(<span class="number">3</span>, figsize=(<span class="number">6</span>,<span class="number">14</span>))</div><div class="line"></div><div class="line"><span class="keyword">for</span> a,d,l <span class="keyword">in</span> zip(range(len(ax)),</div><div class="line">               (df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]].values, df_std, df_minmax),</div><div class="line">               (<span class="string">'Input scale'</span>,</div><div class="line">                <span class="string">'Standardized [$$N  (\mu=0, \; \sigma=1)$$]'</span>,</div><div class="line">                <span class="string">'min-max scaled [min=0, max=1]'</span>)</div><div class="line">                ):</div><div class="line">    <span class="keyword">for</span> i,c <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>)):</div><div class="line">        ax[a].scatter(d[df[<span class="string">'Class label'</span>].values == i, <span class="number">0</span>],</div><div class="line">                  d[df[<span class="string">'Class label'</span>].values == i, <span class="number">1</span>],</div><div class="line">                  alpha=<span class="number">0.5</span>,</div><div class="line">                  color=c,</div><div class="line">                  label=<span class="string">'Class %s'</span> %i</div><div class="line">                  )</div><div class="line">    ax[a].set_title(l)</div><div class="line">    ax[a].set_xlabel(<span class="string">'Alcohol'</span>)</div><div class="line">    ax[a].set_ylabel(<span class="string">'Malic Acid'</span>)</div><div class="line">    ax[a].legend(loc=<span class="string">'upper left'</span>)</div><div class="line">    ax[a].grid()</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_48_0.png" alt="png"></p><h2 id="Bottom-up-approaches"><a href="#Bottom-up-approaches" class="headerlink" title="Bottom-up approaches"></a>Bottom-up approaches</h2><p>Of course, we can also code the equations for standardization and 0-1 Min-Max scaling “manually”. However, the scikit-learn methods are still useful if you are working with test and training data sets and want to scale them equally.</p><p>E.g.,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">std_scale = preprocessing.StandardScaler().fit(X_train)</div><div class="line">X_train = std_scale.transform(X_train)</div><div class="line">X_test = std_scale.transform(X_test)</div></pre></td></tr></table></figure><p>Below, we will perform the calculations using “pure” Python code, and an more convenient NumPy solution, which is especially useful if we attempt to transform a whole matrix.</p><h3 id="Vanilla-Python"><a href="#Vanilla-Python" class="headerlink" title="Vanilla Python"></a>Vanilla Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Standardization</span></div><div class="line"></div><div class="line">x = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">mean = sum(x)/len(x)</div><div class="line">std_dev = (<span class="number">1</span>/len(x) * sum([ (x_i - mean)**<span class="number">2</span> <span class="keyword">for</span> x_i <span class="keyword">in</span> x]))**<span class="number">0.5</span></div><div class="line"></div><div class="line">z_scores = [(x_i - mean)/std_dev <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</div><div class="line"></div><div class="line"><span class="comment"># Min-Max scaling</span></div><div class="line"></div><div class="line">minmax = [(x_i - min(x)) / (max(x) - min(x)) <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</div></pre></td></tr></table></figure><h3 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Standardization</span></div><div class="line"></div><div class="line">x_np = np.asarray(x)</div><div class="line">z_scores_np = (x_np - x_np.mean()) / x_np.std()</div><div class="line"></div><div class="line"><span class="comment"># Min-Max scaling</span></div><div class="line"></div><div class="line">np_minmax = (x_np - x_np.min()) / (x_np.max() - x_np.min())</div></pre></td></tr></table></figure><h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><p>Just to make sure that our code works correctly, let us plot the results via matplotlib.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=<span class="number">2</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line"></div><div class="line">y_pos = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x))]</div><div class="line"></div><div class="line">ax1.scatter(z_scores, y_pos, color=<span class="string">'g'</span>)</div><div class="line">ax1.set_title(<span class="string">'Python standardization'</span>, color=<span class="string">'g'</span>)</div><div class="line"></div><div class="line">ax2.scatter(minmax, y_pos, color=<span class="string">'g'</span>)</div><div class="line">ax2.set_title(<span class="string">'Python Min-Max scaling'</span>, color=<span class="string">'g'</span>)</div><div class="line"></div><div class="line">ax3.scatter(z_scores_np, y_pos, color=<span class="string">'b'</span>)</div><div class="line">ax3.set_title(<span class="string">'Python NumPy standardization'</span>, color=<span class="string">'b'</span>)</div><div class="line"></div><div class="line">ax4.scatter(np_minmax, y_pos, color=<span class="string">'b'</span>)</div><div class="line">ax4.set_title(<span class="string">'Python NumPy Min-Max scaling'</span>, color=<span class="string">'b'</span>)</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> (ax1, ax2, ax3, ax4):</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.grid()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_64_0.png" alt="png"></p><h2 id="The-effect-of-standardization-on-PCA-in-a-pattern-classification-task"><a href="#The-effect-of-standardization-on-PCA-in-a-pattern-classification-task" class="headerlink" title="The effect of standardization on PCA in a pattern classification task"></a>The effect of standardization on PCA in a pattern classification task</h2><p>Earlier, I mentioned the Principal Component Analysis (PCA) as an example where standardization is crucial, since it is “analyzing” the variances of the different features.<br>Now, let us see how the standardization affects PCA and a following supervised classification on the whole wine dataset.</p><p>In the following section, we will go through the following steps:</p><ul><li>Reading in the dataset</li><li>Dividing the dataset into a separate training and test dataset</li><li>Standardization of the features</li><li>Principal Component Analysis (PCA) to reduce the dimensionality</li><li>Training a naive Bayes classifier</li><li>Evaluating the classification accuracy with and without standardization</li></ul><h3 id="Reading-in-the-dataset"><a href="#Reading-in-the-dataset" class="headerlink" title="Reading in the dataset"></a>Reading in the dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">df = pd.io.parsers.read_csv(</div><div class="line">    <span class="string">'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv'</span>,</div><div class="line">    header=<span class="keyword">None</span>,</div><div class="line">    )</div></pre></td></tr></table></figure><h3 id="Dividing-the-dataset-into-a-separate-training-and-test-dataset"><a href="#Dividing-the-dataset-into-a-separate-training-and-test-dataset" class="headerlink" title="Dividing the dataset into a separate training and test dataset"></a>Dividing the dataset into a separate training and test dataset</h3><p>In this step, we will randomly divide the wine dataset into a training dataset and a test dataset where the training dataset will contain 70% of the samples and the test dataset will contain 30%, respectively.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line">X_wine = df.values[:,<span class="number">1</span>:]</div><div class="line">y_wine = df.values[:,<span class="number">0</span>]</div><div class="line"></div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine,</div><div class="line">    test_size=<span class="number">0.30</span>, random_state=<span class="number">12345</span>)</div></pre></td></tr></table></figure><h3 id="Feature-Scaling-Standardization"><a href="#Feature-Scaling-Standardization" class="headerlink" title="Feature Scaling - Standardization"></a>Feature Scaling - Standardization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line">std_scale = preprocessing.StandardScaler().fit(X_train)</div><div class="line">X_train_std = std_scale.transform(X_train)</div><div class="line">X_test_std = std_scale.transform(X_test)</div></pre></td></tr></table></figure><h3 id="Dimensionality-reduction-via-Principal-Component-Analysis-PCA"><a href="#Dimensionality-reduction-via-Principal-Component-Analysis-PCA" class="headerlink" title="Dimensionality reduction via Principal Component Analysis (PCA)"></a>Dimensionality reduction via Principal Component Analysis (PCA)</h3><p>Now, we perform a PCA on the standardized and the non-standardized datasets to transform the dataset onto a 2-dimensional feature subspace.<br>In a real application, a procedure like cross-validation would be done in order to find out what choice of features would yield a optimal balance between “preserving information” and “overfitting” for different classifiers. However, we will omit this step since we don’t want to train a perfect classifier here, but merely compare the effects of standardization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"></div><div class="line"><span class="comment"># on non-standardized data</span></div><div class="line">pca = PCA(n_components=<span class="number">2</span>).fit(X_train)</div><div class="line">X_train = pca.transform(X_train)</div><div class="line">X_test = pca.transform(X_test)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># om standardized data</span></div><div class="line">pca_std = PCA(n_components=<span class="number">2</span>).fit(X_train_std)</div><div class="line">X_train_std = pca_std.transform(X_train_std)</div><div class="line">X_test_std = pca_std.transform(X_test_std)</div></pre></td></tr></table></figure><p>Let us quickly visualize how our new feature subspace looks like (note that class labels are not considered in a PCA - in contrast to a Linear Discriminant Analysis - but I will add them in the plot for clarity).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">fig, (ax1, ax2) = plt.subplots(ncols=<span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">4</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">for</span> l,c,m <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'blue'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>), (<span class="string">'^'</span>, <span class="string">'s'</span>, <span class="string">'o'</span>)):</div><div class="line">    ax1.scatter(X_train[y_train==l, <span class="number">0</span>], X_train[y_train==l, <span class="number">1</span>],</div><div class="line">        color=c,</div><div class="line">        label=<span class="string">'class %s'</span> %l,</div><div class="line">        alpha=<span class="number">0.5</span>,</div><div class="line">        marker=m</div><div class="line">        )</div><div class="line"></div><div class="line"><span class="keyword">for</span> l,c,m <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'blue'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>), (<span class="string">'^'</span>, <span class="string">'s'</span>, <span class="string">'o'</span>)):</div><div class="line">    ax2.scatter(X_train_std[y_train==l, <span class="number">0</span>], X_train_std[y_train==l, <span class="number">1</span>],</div><div class="line">        color=c,</div><div class="line">        label=<span class="string">'class %s'</span> %l,</div><div class="line">        alpha=<span class="number">0.5</span>,</div><div class="line">        marker=m</div><div class="line">        )</div><div class="line"></div><div class="line">ax1.set_title(<span class="string">'Transformed NON-standardized training dataset after PCA'</span>)    </div><div class="line">ax2.set_title(<span class="string">'Transformed standardized training dataset after PCA'</span>)    </div><div class="line"></div><div class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> (ax1, ax2):</div><div class="line"></div><div class="line">    ax.set_xlabel(<span class="string">'1st principal component'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'2nd principal component'</span>)</div><div class="line">    ax.legend(loc=<span class="string">'upper right'</span>)</div><div class="line">    ax.grid()</div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_89_0.png" alt="png"></p><h3 id="Training-a-naive-Bayes-classifier"><a href="#Training-a-naive-Bayes-classifier" class="headerlink" title="Training a naive Bayes classifier"></a>Training a naive Bayes classifier</h3><p>We will use a naive Bayes classifier for the classification task. If you are not familiar with it, the term “naive” comes from the assumption that all features are “independent”.<br>All in all, it is a simple but robust classifier based on Bayes’ rule</p><p>I don’t want to get into more detail about Bayes’ rule in this article, but if you are interested in a more detailed collection of examples, please have a look at the <a href="https://github.com/rasbt/pattern_classification#statistical-pattern-recognition-examples" target="_blank" rel="external">Statistical Patter Classification</a> in my pattern classification repository.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"></div><div class="line"><span class="comment"># on non-standardized data</span></div><div class="line">gnb = GaussianNB()</div><div class="line">fit = gnb.fit(X_train, y_train)</div><div class="line"></div><div class="line"><span class="comment"># on standardized data</span></div><div class="line">gnb_std = GaussianNB()</div><div class="line">fit_std = gnb_std.fit(X_train_std, y_train)</div></pre></td></tr></table></figure><h3 id="Evaluating-the-classification-accuracy-with-and-without-standardization"><a href="#Evaluating-the-classification-accuracy-with-and-without-standardization" class="headerlink" title="Evaluating the classification accuracy with and without standardization"></a>Evaluating the classification accuracy with and without standardization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"></div><div class="line">pred_train = gnb.predict(X_train)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the training dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;'</span>.format(metrics.accuracy_score(y_train, pred_train)))</div><div class="line"></div><div class="line">pred_test = gnb.predict(X_test)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the test dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;\n'</span>.format(metrics.accuracy_score(y_test, pred_test)))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Prediction accuracy <span class="keyword">for</span> the training dataset</div><div class="line"><span class="number">81.45</span>%</div><div class="line"></div><div class="line">Prediction accuracy <span class="keyword">for</span> the test dataset</div><div class="line"><span class="number">64.81</span>%</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">pred_train_std = gnb_std.predict(X_train_std)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the training dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;'</span>.format(metrics.accuracy_score(y_train, pred_train_std)))</div><div class="line"></div><div class="line">pred_test_std = gnb_std.predict(X_test_std)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the test dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;\n'</span>.format(metrics.accuracy_score(y_test, pred_test_std)))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Prediction accuracy <span class="keyword">for</span> the training dataset</div><div class="line"><span class="number">96.77</span>%</div><div class="line"></div><div class="line">Prediction accuracy <span class="keyword">for</span> the test dataset</div><div class="line"><span class="number">98.15</span>%</div></pre></td></tr></table></figure><p>As we can see, the standardization prior to the PCA definitely led to an decrease in the empirical error rate on classifying samples from test dataset.</p><h2 id="Appendix-A-The-effect-of-scaling-and-mean-centering-of-variables-prior-to-PCA"><a href="#Appendix-A-The-effect-of-scaling-and-mean-centering-of-variables-prior-to-PCA" class="headerlink" title="Appendix A: The effect of scaling and mean centering of variables prior to PCA"></a>Appendix A: The effect of scaling and mean centering of variables prior to PCA</h2><p>Let us think about whether it matters or not if the variables are centered for applications such as Principal Component Analysis (PCA) if the PCA is calculated from the covariance matrix (i.e., the kkprincipal components are the eigenvectors of the covariance matrix that correspond to the kk largest eigenvalues.</p><h3 id="1-Mean-centering-does-not-affect-the-covariance-matrix"><a href="#1-Mean-centering-does-not-affect-the-covariance-matrix" class="headerlink" title="1. Mean centering does not affect the covariance matrix"></a>1. Mean centering does not affect the covariance matrix</h3><p>Here, the rational is: If the covariance is the same whether the variables are centered or not, the result of the PCA will be the same.</p><p>Let’s assume we have the 2 variables $x$ and $y$ Then the covariance between the attributes is calculated as<br>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>Let us write the centered variables as<br>$$<br>x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}<br>$$<br>The centered covariance would then be calculated as follows:<br>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i’ - \bar{y}’)<br>$$<br>But since after centering, $\bar{x}’ = 0$ and $\bar{y}’ = 0$ we have</p><p>$\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} x_i’ y_i’$ which is our original covariance matrix if we resubstitute back the terms $x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}$.</p><p>Even centering only one variable, e.g., xx wouldn’t affect the covariance:</p><p>$$<br>\sigma_{\text{xy}} = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i - \bar{y})<br>$$</p><h3 id="2-Scaling-of-variables-does-affect-the-covariance-matrix"><a href="#2-Scaling-of-variables-does-affect-the-covariance-matrix" class="headerlink" title="2. Scaling of variables does affect the covariance matrix"></a>2. Scaling of variables does affect the covariance matrix</h3><p>If one variable is scaled, e.g, from pounds into kilogram (1 pound = 0.453592 kg), it does affect the covariance and therefore influences the results of a PCA.</p><p>Let cc be the scaling factor for $x$</p><p>Given that the “original” covariance is calculated as</p><p>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>the covariance after scaling would be calculated as:</p><p>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (c \cdot x_i - c \cdot \bar{x})(y_i - \bar{y}) = \frac{c}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y}) \Rightarrow \sigma_{xy}’ = c \cdot \sigma_{xy}<br>$$<br>Therefore, the covariance after scaling one attribute by the constant $c$ will result in a rescaled covariance $c \sigma_{xy}$ So if we’d scaled $x$ from pounds to kilograms, the covariance between $x$ and $y$ will be 0.453592 times smaller.</p><h3 id="3-Standardizing-affects-the-covariance"><a href="#3-Standardizing-affects-the-covariance" class="headerlink" title="3. Standardizing affects the covariance"></a>3. Standardizing affects the covariance</h3><p>Standardization of features will have an effect on the outcome of a PCA (assuming that the variables are originally not standardized). This is because we are scaling the covariance between every pair of variables by the product of the standard deviations of each pair of variables.</p><p>The equation for standardization of a variable is written as</p><p>$$<br>z = \frac{x_i - \bar{x}}{\sigma}<br>$$<br>The “original” covariance matrix:</p><p>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>And after standardizing both variables:</p><p>$$<br>x’ = \frac{x - \bar{x}}{\sigma_x} \text{ and } y’ =\frac{y - \bar{y}}{\sigma_y}<br>$$</p><p>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - 0)(y_i’ - 0) = \frac{1}{n-1} \sum_{i}^{n} \bigg(\frac{x - \bar{x}}{\sigma_x}\bigg)\bigg(\frac{y - \bar{y}}{\sigma_y}\bigg) = \frac{1}{(n-1) \cdot \sigma_x \sigma_y} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$</p><p>$$<br>\Rightarrow \sigma_{xy}’ = \frac{\sigma_{xy}}{\sigma_x \sigma_y}<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About-standardization&quot;&gt;&lt;a href=&quot;#About-standardization&quot; class=&quot;headerlink&quot; title=&quot;About standardization&quot;&gt;&lt;/a&gt;About standardization&lt;/
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Install SerpentAI on Windows 10</title>
    <link href="http://yoursite.com/2018/06/07/Install-SerpentAI-on-Windows-10/"/>
    <id>http://yoursite.com/2018/06/07/Install-SerpentAI-on-Windows-10/</id>
    <published>2018-06-07T13:50:00.000Z</published>
    <updated>2018-06-08T06:45:10.253Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Python-Environment"><a href="#Python-Environment" class="headerlink" title="Python Environment"></a>Python Environment</h2><h3 id="Python-3-6-with-Anaconda"><a href="#Python-3-6-with-Anaconda" class="headerlink" title="Python 3.6+ (with Anaconda)"></a>Python 3.6+ (with Anaconda)</h3><p>Serpent.AI was developed taking full advantage of Python 3.6 so it is only natural that the Python requirement be for versions 3.6 and up.</p><p>Installing regular Python 3.6+ isn’t exactly difficult but Serpent.AI relies on a good amount of scientific computing libraries that are extremely difficult / impossible to compile on your own on Windows. Thankfully, the <a href="https://www.anaconda.com/distribution" target="_blank" rel="external">Anaconda Distribution</a> exists and takes this huge weight off our collective shoulders.</p><h4 id="Installing-Anaconda-5-2-0-Python-3-6"><a href="#Installing-Anaconda-5-2-0-Python-3-6" class="headerlink" title="Installing Anaconda 5.2.0 (Python 3.6)"></a>Installing Anaconda 5.2.0 (Python 3.6)</h4><p><a href="https://www.anaconda.com/download/" target="_blank" rel="external">Download</a> the Python 3.6 version of Anaconda 5.2.0 and run the graphical installer.</p><p>The following commands are to be performed in an <em>Anaconda Prompt</em> with elevated privileges (Right click and <strong>Run as Administrator</strong>). It is recommended to create a shortcut to this prompt because every Python and Serpent command will have to be performed from there starting now.</p><h4 id="Creating-a-Conda-Env-for-Serpent-AI"><a href="#Creating-a-Conda-Env-for-Serpent-AI" class="headerlink" title="Creating a Conda Env for Serpent.AI"></a>Creating a Conda Env for Serpent.AI</h4><p><code>conda create --name serpent python=3.6</code> (‘serpent’ can be replaced with another name)</p><h4 id="Creating-a-directory-for-your-Serpent-AI-projects"><a href="#Creating-a-directory-for-your-Serpent-AI-projects" class="headerlink" title="Creating a directory for your Serpent.AI projects"></a>Creating a directory for your Serpent.AI projects</h4><p><code>mkdir SerpentAI &amp;&amp; cd SerpentAI</code></p><h4 id="Activating-the-Conda-Env"><a href="#Activating-the-Conda-Env" class="headerlink" title="Activating the Conda Env"></a>Activating the Conda Env</h4><p><code>conda activate serpent</code></p><h2 id="3rd-Party-Dependencies"><a href="#3rd-Party-Dependencies" class="headerlink" title="3rd-Party Dependencies"></a>3rd-Party Dependencies</h2><h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><p>Redis is used in the framework as the in-memory store for the captured frame buffers as well as the temporary storage of analytics events. It is not meant to be compatible with Windows! Microsoft used to <a href="https://github.com/MicrosoftArchive/redis" target="_blank" rel="external">maintain a port</a> but it’s been abandoned since. This being said, that Redis version is sufficient and it outperforms stuff like running it in WSL on Windows 10. It will install as a Windows Service. Make sure you set it to start automatically.</p><h4 id="Install-Windows-Subsystem-for-Linux-WSL"><a href="#Install-Windows-Subsystem-for-Linux-WSL" class="headerlink" title="Install Windows Subsystem for Linux (WSL)"></a><a href="https://msdn.microsoft.com/en-us/commandline/wsl/install_guide" target="_blank" rel="external">Install Windows Subsystem for Linux (WSL)</a></h4><ol><li>From Start, search for <strong>Turn Windows features on or off</strong> (type <code>turn</code>)</li><li><strong>Select Windows Subsystem for Linux (beta)</strong></li></ol><p><a href="https://raw.githubusercontent.com/ServiceStack/Assets/master/img/redis/install-wsl.png" target="_blank" rel="external"><img src="https://raw.githubusercontent.com/ServiceStack/Assets/master/img/redis/install-wsl.png" alt="img"></a></p><p>Once installed you can run bash on Ubuntu by typing <strong>bash</strong> from a Windows Command Prompt. To install the latest version of Redis we’ll need to use a repository that maintains up-to-date packages for Ubuntu and Debian servers like <a href="https://www.dotdeb.org/" target="_blank" rel="external">https://www.dotdeb.org</a> which you can add to Ubuntu’s apt-get sources with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ echo deb http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list</div><div class="line">$ echo deb-src http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list</div><div class="line">$ sudo mv dotdeb.org.list /etc/apt/sources.list.d</div><div class="line">$ wget -q -O - http://www.dotdeb.org/dotdeb.gpg | sudo apt-key add -</div></pre></td></tr></table></figure><p>Then after updating our APT cache we can install Redis with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install redis-server</div></pre></td></tr></table></figure><p>You’ll then be able to launch redis with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ redis-server --daemonize yes</div></pre></td></tr></table></figure><p>Which will run redis in the background freeing your shell so you can play with it using the redis client:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ redis-cli</div><div class="line">$ 127.0.0.1:6379&gt; SET foo bar</div><div class="line">OK</div><div class="line">$ 127.0.0.1:6379&gt; GET foo</div><div class="line">&quot;bar&quot;</div></pre></td></tr></table></figure><p>Which you can connect to from within bash or from your Windows desktop using the <a href="https://github.com/ServiceStack/redis-windows#option-3-running-microsofts-native-port-of-redis" target="_blank" rel="external">redis-cli native Windows binary from MSOpenTech</a>.</p><h3 id="Build-Tools-for-Visual-Studio-2017"><a href="#Build-Tools-for-Visual-Studio-2017" class="headerlink" title="Build Tools for Visual Studio 2017"></a>Build Tools for Visual Studio 2017</h3><p>Some of the packages that will be installed alongside Serpent.AI are not pre-compiled binaries and will be need to be built from source. This is a little more problematic for Windows but with the correct C++ Build Tools for Visual Studio it all goes down smoothly.</p><p>You can get the proper installer by visiting <a href="https://www.visualstudio.com/downloads/" target="_blank" rel="external">https://www.visualstudio.com/downloads/</a> and scrolling down to the <em>Build Tools for Visual Studio 2017</em> download. Download, run, select the <em>Visual C++ build tools</em> section and make sure the following components are checked (VSs are not installed):</p><ul><li>Visual C++ Build Tools core features</li><li>VC++ 2017 version 15.7 v14.14 latest v141 tools</li><li>Visual C++ 2017 Redistributable Update</li><li>VC++ 2015.3 v14.00 (v140) toolset for desktop</li><li>Windows 10 SDK (10.0.17134.0)</li><li>Windows Universal CRT SDK</li></ul><h2 id="Installing-Serpent-AI"><a href="#Installing-Serpent-AI" class="headerlink" title="Installing Serpent.AI"></a>Installing Serpent.AI</h2><p>Once all of the above had been installed and set up, you are ready to install the framework. Remember that PATH changes in Windows are not reflected in your command prompts that were opened while you made the changes. Open a fresh Anaconda prompt before continuing to avoid installation issues.</p><p>Go back to the directory you created earlier for your Serpent.AI projects. Make sure you are scoped in your Conda Env.</p><p>Run <code>pip install SerpentAI</code></p><p>Then run <code>serpent setup</code> to install the remaining dependencies automatically.</p><h2 id="Installing-Optional-Modules"><a href="#Installing-Optional-Modules" class="headerlink" title="Installing Optional Modules"></a>Installing Optional Modules</h2><p>In the spirit of keeping the initial installation on the light side, some specialized / niche components with extra dependencies have been isolated from the core. It is recommended to only focus on installing them once you reach a point where you actually need them. The framework will provide a warning when a feature you are trying to use requires one of those modules.</p><h3 id="OCR"><a href="#OCR" class="headerlink" title="OCR"></a>OCR</h3><p>A module to provide OCR functionality in your game agents.</p><h4 id="Tesseract"><a href="#Tesseract" class="headerlink" title="Tesseract"></a>Tesseract</h4><p>Serpent.AI leverages Tesseract for its OCR functionality. You can install Tesseract for Windows by following these steps:</p><ol><li>Visit <a href="https://github.com/UB-Mannheim/tesseract/wiki" target="_blank" rel="external">https://github.com/UB-Mannheim/tesseract/wiki</a></li><li>Download the .exe for version 3</li><li>Run the graphical installer (Remember the install path!)</li><li>Add the path to <em>tesseract.exe</em> to your %PATH% environment variable</li></ol><p>You can test your Tesseract installation by opening an Anaconda Prompt and executing <code>tesseract --list-langs</code>.</p><h4 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h4><p>Once you’ve validated that Tesseract has been properly set up, you can install the module with <code>serpent setup ocr</code></p><h3 id="GUI"><a href="#GUI" class="headerlink" title="GUI"></a>GUI</h3><p>A module to allow Serpent.AI desktop app to run.</p><h4 id="Kivy"><a href="#Kivy" class="headerlink" title="Kivy"></a>Kivy</h4><p>Kivy is the GUI framework used in the framework.</p><p>Once you are ready to test your Kivy, you can install the module with <code>serpent setup gui</code> and try to run <code>serpent visual_debugger</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Python-Environment&quot;&gt;&lt;a href=&quot;#Python-Environment&quot; class=&quot;headerlink&quot; title=&quot;Python Environment&quot;&gt;&lt;/a&gt;Python Environment&lt;/h2&gt;&lt;h3 id=&quot;P
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Matching Networks for One Shot Learning</title>
    <link href="http://yoursite.com/2018/06/02/Matching-Networks-for-One-Shot-Learning/"/>
    <id>http://yoursite.com/2018/06/02/Matching-Networks-for-One-Shot-Learning/</id>
    <published>2018-06-02T14:35:08.000Z</published>
    <updated>2018-06-02T14:39:51.593Z</updated>
    
    <content type="html"><![CDATA[<p>By DeepMind crew: <strong>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra</strong></p><p>This is a paper on <strong>one-shot</strong> learning, where we’d like to learn a class based on very few (or indeed, 1) training examples. E.g. it suffices to show a child a single giraffe, not a few hundred thousands before it can recognize more giraffes.</p><p>This paper falls into a category of <em>“duh of course”</em> kind of paper, something very interesting, powerful, but somehow obvious only in retrospect. I like it.</p><p>Suppose you’re given a single example of some class and would like to label it in test images.</p><ul><li><strong>Observation 1</strong>: a standard approach might be to train an Exemplar SVM for this one (or few) examples vs. all the other training examples - i.e. a linear classifier. But this requires optimization.</li><li><strong>Observation 2:</strong> known non-parameteric alternatives (e.g. k-Nearest Neighbor) don’t suffer from this problem. E.g. I could immediately use a Nearest Neighbor to classify the new class without having to do any optimization whatsoever. However, NN is gross because it depends on an (arbitrarily-chosen) metric, e.g. L2 distance. Ew.</li><li><strong>Core idea</strong>: lets train a fully end-to-end nearest neighbor classifer!<img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2010.08.44%20PM.png" alt="Screen Shot 2016-08-07 at 10.08.44 PM"></li></ul><h2 id="The-training-protocol"><a href="#The-training-protocol" class="headerlink" title="The training protocol"></a>The training protocol</h2><p>As the authors amusingly point out in the conclusion (and this is the <em>duh of course</em> part), <em>“one-shot learning is much easier if you train the network to do one-shot learning”</em>. Therefore, we want the test-time protocol (given N novel classes with only k examples each (e.g. k = 1 or 5), predict new instances to one of N classes) to exactly match the training time protocol.</p><p>To create each “episode” of training from a dataset of examples then:</p><ol><li>Sample a task T from the training data, e.g. select 5 labels, and up to 5 examples per label (i.e. 5-25 examples).</li><li>To form one episode sample a label set L (e.g. {cats, dogs}) and then use L to sample the support set S and a batch B of examples to evaluate loss on.</li></ol><p>The idea on high level is clear but the writing here is a bit unclear on details, of exactly how the sampling is done.</p><h2 id="The-model"><a href="#The-model" class="headerlink" title="The model"></a>The model</h2><p>I find the paper’s model description slightly wordy and unclear, but basically we’re building a <strong>differentiable nearest neighbor++</strong>. The output \hat{y} for a test example \hat{x} is computed very similar to what you might see in Nearest Neighbors:<img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.14.26%20PM.png" alt="Screen Shot 2016-08-07 at 11.14.26 PM"><br>where <strong>a</strong> acts as a kernel, computing the extent to which \hat{x} is similar to a training example x_i, and then the labels from the training examples (y_i) are weight-blended together accordingly. The paper doesn’t mention this but I assume for classification y_i would presumbly be one-hot vectors.</p><p>Now, we’re going to embed both the training examples x_i and the test example \hat{x}, and we’ll interpret their inner products (or here a cosine similarity) as the “match”, and pass that through a softmax to get normalized mixing weights so they add up to 1. No surprises here, this is quite natural:</p><p><img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.20.29%20PM.png" alt="Screen Shot 2016-08-07 at 11.20.29 PM"><br>Here <strong>c()</strong> is cosine distance, which I presume is implemented by normalizing the two input vectors to have unit L2 norm and taking a dot product. I assume the authors tried skipping the normalization too and it did worse? Anyway, now all that’s left to define is the function <strong>f</strong> (i.e. how do we embed the test example into a vector) and the function <strong>g</strong> (i.e. how do we embed each training example into a vector?).</p><p><strong>Embedding the training examples.</strong> This (the function <strong>g</strong>) is a bidirectional LSTM over the examples:</p><p><img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.57.10%20PM.png" alt="Screen Shot 2016-08-07 at 11.57.10 PM"></p><p>i.e. encoding of i’th example x_i is a function of its “raw” embedding g’(x_i) and the embedding of its friends, communicated through the bidirectional network’s hidden states. i.e. each training example is a function of not just itself but all of its friends in the set. This is part of the ++ above, because in a normal nearest neighbor you wouldn’t change the representation of an example as a function of the other data points in the training set.</p><p>It’s odd that the <strong>order</strong> is not mentioned, I assume it’s random? This is a bit gross because order matters to a bidirectional LSTM; you’d get different embeddings if you permute the examples.</p><p><strong>Embedding the test example.</strong> This (the function <strong>f</strong>) is a an LSTM that processes for a fixed amount (K time steps) and at each point also <em>attends</em> over the examples in the training set. The encoding is the last hidden state of the LSTM. Again, this way we’re allowing the network to change its encoding of the test example as a function of the training examples. Nifty: <img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2012.11.15%20AM.png" alt="Screen Shot 2016-08-08 at 12.11.15 AM"></p><p>That looks scary at first but it’s really just a vanilla LSTM with attention where the input at each time step is constant (f’(\hat{x}), an encoding of the test example all by itself) and the hidden state is a function of previous hidden state but also a concatenated readout vector <strong>r</strong>, which we obtain by attending over the encoded training examples (encoded with <strong>g</strong> from above).</p><p>Oh and I assume there is a typo in equation (5), it should say r_k = … without the -1 on LHS.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>Task</strong>: N-way k-shot learning task. i.e. we’re given k (e.g. 1 or 5) labelled examples for N classes that we have not previously trained on and asked to classify new instances into he N classes.</p><p><strong>Baselines:</strong> an “obvious” strategy of using a pretrained ConvNet and doing nearest neighbor based on the codes. An option of finetuning the network on the new examples as well (requires training and careful and strong regularization!).</p><p><strong>MANN</strong> of Santoro et al. [21]: Also a DeepMind paper, a fun NTM-like Meta-Learning approach that is fed a sequence of examples and asked to predict their labels.</p><p><strong>Siamese network</strong> of Koch et al. [11]: A siamese network that takes two examples and predicts whether they are from the same class or not with logistic regression. A test example is labeled with a nearest neighbor: with the class it matches best according to the siamese net (requires iteration over all training examples one by one). Also, this approach is less end-to-end than the one here because it requires the ad-hoc nearest neighbor matching, while here the <em>exact</em> end task is optimized for. It’s beautiful.</p><h3 id="Omniglot-experiments"><a href="#Omniglot-experiments" class="headerlink" title="Omniglot experiments"></a>Omniglot experiments</h3><h3><a href="#" class="headerlink"></a><img src="https://github.com/karpathy/paper-notes/raw/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2010.21.45%20AM.png" alt="Screen Shot 2016-08-08 at 10.21.45 AM"></h3><p>Omniglot of <a href="http://www.cs.toronto.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf" target="_blank" rel="external">Lake et al. [14]</a> is a MNIST-like scribbles dataset with 1623 characters with 20 examples each.</p><p>Image encoder is a CNN with 4 modules of [3x3 CONV 64 filters, batchnorm, ReLU, 2x2 max pool]. The original image is claimed to be so resized from original 28x28 to 1x1x64, which doesn’t make sense because factor of 2 downsampling 4 times is reduction of 16, and 28/16 is a non-integer &gt;1. I’m assuming they use VALID convs?</p><p>Results: <img src="https://github.com/karpathy/paper-notes/raw/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2010.27.46%20AM.png" alt="Screen Shot 2016-08-08 at 10.27.46 AM"></p><p>Matching nets do best. Fully Conditional Embeddings (FCE) by which I mean they the “Full Context Embeddings” of Section 2.1.2 instead are not used here, mentioned to not work much better. Finetuning helps a bit on baselines but not with Matching nets (weird).</p><p>The comparisons in this table are somewhat confusing:</p><ul><li>I can’t find the MANN numbers of 82.8% and 94.9% in their paper [21]; not clear where they come from. E.g. for 5 classes and 5-shot they seem to report 88.4% not 94.9% as seen here. I must be missing something.</li><li>I also can’t find the numbers reported here in the Siamese Net [11] paper. As far as I can tell in their Table 2 they report one-shot accuracy, 20-way classification to be 92.0, while here it is listed as 88.1%?</li><li>The results of Lake et al. [14] who proposed Omniglot are also missing from the table. If I’m understanding this correctly they report 95.2% on 1-shot 20-way, while matching nets here show 93.8%, and humans are estimated at 95.5%. That is, the results here appear weaker than those of Lake et al., but one should keep in mind that the method here is significantly more generic and does not make any assumptions about the existence of strokes, etc., and it’s a simple, single fully-differentiable blob of neural stuff.</li></ul><p>(skipping ImageNet/LM experiments as there are few surprises)</p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Good paper, effectively develops a differentiable nearest neighbor trained end-to-end. It’s something new, I like it!</p><p>A few concerns:</p><ul><li><p>A bidirectional LSTMs (not order-invariant compute) is applied over sets of training examples to encode them. The authors don’t talk about the order actually used, which presumably is random, or mention this potentially unsatisfying feature. This can be solved by using a recurrent attentional mechanism instead, as the authors are certainly aware of and as has been discussed at length in <a href="https://arxiv.org/abs/1511.06391" target="_blank" rel="external">ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS</a>, where Oriol is also the first author. I wish there was a comment on this point in the paper somewhere.</p></li><li><p>The approach also gets quite a bit slower as the number of training examples grow, but once this number is large one would presumable switch over to a parameteric approach.</p></li><li><p>It’s also potentially concerning that during training the method uses a specific number of examples, e.g. 5-25, so this is the number of that must also be used at test time. What happens if we want the size of our training set to grow online? It appears that we need to retrain the network because the encoder LSTM for the training data is not “used to” seeing inputs of more examples? That is unless you fall back to iteratively subsampling the training data, doing multiple inference passes and averaging, or something like that. If we don’t use FCE it can still be that the attention mechanism LSTM can still not be “used to” attending over many more examples, but it’s not clear how much this matters. An interesting experiment would be to not use FCE and try to use 100 or 1000 training examples, while only training on up to 25 (with and fithout FCE). Discussion surrounding this point would be interesting.</p></li><li><p>Not clear what happened with the Omniglot experiments, with incorrect numbers for [11], [21], and the exclusion of Lake et al. [14] comparison.</p></li><li><p>A baseline that is missing would in my opinion also include training of an <a href="https://www.cs.cmu.edu/~tmalisie/projects/iccv11/" target="_blank" rel="external">Exemplar SVM</a>, which is a much more powerful approach than encode-with-a-cnn-and-nearest-neighbor.</p><p>​</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;By DeepMind crew: &lt;strong&gt;Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This is a p
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="one-shot learning" scheme="http://yoursite.com/tags/one-shot-learning/"/>
    
  </entry>
  
  <entry>
    <title>One Shot Learning and Siamese Networks in Keras</title>
    <link href="http://yoursite.com/2018/06/02/One-Shot-Learning-and-Siamese-Networks-in-Keras/"/>
    <id>http://yoursite.com/2018/06/02/One-Shot-Learning-and-Siamese-Networks-in-Keras/</id>
    <published>2018-06-02T14:21:29.000Z</published>
    <updated>2018-06-02T14:32:48.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background:"></a>Background:</h2><p>Conventional wisdom says that deep neural networks are really good at learning from high dimensional data like images or spoken language, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of <em>one-shot learning</em> - if you take a human who’s never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with astoundingly high precision.</p><p><a href="https://sorenbouma.github.io/images/spatula.jpg" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/spatula.jpg" alt="image"></a></p><p>Never been inside a kitchen before? Now’s your chance to test your one shot learning ability! which of the images on the right is of the same type as the big image? Email me for the correct answer.</p><p>..Yet another one of the <a href="https://dspace.mit.edu/handle/1721.1/6125" target="_blank" rel="external">things</a> humans can do that seemed trivial to us right up until we tried to make an algorithm do it.</p><p>This ability to rapidly learn from very little data seems like it’s obviously desirable for machine learning systems to have because collecting and labelling data is expensive. I also think this is an important step on the long road towards general intelligence.</p><p>Recently there have been <a href="https://arxiv.org/abs/1703.07326" target="_blank" rel="external">many</a> <a href="https://sorenbouma.github.io/blog/oneshot/%22%22" target="_blank" rel="external">interesting</a> <a href="https://sorenbouma.github.io/blog/oneshot/%22%22" target="_blank" rel="external">papers</a> about one-shot learning with neural nets and they’ve gotten some good results. This is a new area that really excites me, so I wanted to make a gentle introduction to make it more accessible to fellow newcomers to deep learning.</p><p>In this post, I want to:</p><ul><li>Introduce and formulate the problem of one-shot learning</li><li>Describe benchmarks for one-shot classification and give a baseline for performance</li><li>Give an example of deep one-shot learning by partially reimplementing the model in <a href="http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" target="_blank" rel="external">this paper</a> with keras.</li><li>Hopefully point out some small insights that aren’t obvious to everyone</li></ul><h2 id="Formulating-the-Problem-N-way-One-Shot-Learning"><a href="#Formulating-the-Problem-N-way-One-Shot-Learning" class="headerlink" title="Formulating the Problem - N-way One-Shot Learning"></a>Formulating the Problem - N-way One-Shot Learning</h2><p>Before we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:</p><p>Our model is given a tiny labelled training set SS, which has N examples, each vectors of the same dimension with a distinct label yy.<br>$$<br>S={(x_1,y_1),…,(x_N,y_N)}<br>$$<br>It is also given $\hat{x}$, the test example it has to classify. Since exactly one example in the support set has the right class, the aim is to correctly predict which $y \in S$ is the same as $\hat{x}$ ‘s label, $\hat{y}$.</p><p>There are fancier ways of defining the problem, but this one is ours. Here are some things to make note of:</p><ul><li>Real world problems might not always have the constraint that exactly one image has the correct class</li><li>It’s easy to generalize this to k-shot learning by having there be k examples for each yiyirather than just one.</li><li>When N is higher, there are more possible classes that $\hat{x}$ can belong to, so it’s harder to predict the correct one.</li><li>Random guessing will average $\frac{100}{n}\%$ accuracy.</li></ul><p>Here are some examples of one-shot learning tasks on the Omniglot dataset, which I’ll describe in the next section.</p><p><a href="https://sorenbouma.github.io/images/task_9.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/task_9.png" alt="image"></a><a href="https://sorenbouma.github.io/images/task_25.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/task_25.png" alt="image"></a><a href="hhttps://sorenbouma.github.io/images/task_36.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/task_36.png" alt="image"></a>9, 25 and 36 way one-shot learnng tasks.</p><h2 id="About-the-data-Omniglot"><a href="#About-the-data-Omniglot" class="headerlink" title="About the data - Omniglot! :"></a>About the data - Omniglot! :</h2><p>The <a href="https://github.com/brendenlake/omniglot" target="_blank" rel="external">Omniglot dataset</a> is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are just 20 examples, each drawn by a different person at resolution 105x105.</p><p><a href="https://sorenbouma.github.io/images/alphabets/Braille.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Braille.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Bengali.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Bengali.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Sanskrit.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Sanskrit.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Greek.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Greek.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Futurama.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Futurama.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Hebrew.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Hebrew.png" alt="image"></a>A few of the alphabets from the omniglot dataset. As you can see, there’s a huge variety of different symbols.</p><p>If you like machine learning, you’ve probably heard of the <a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank" rel="external">MNIST dataset</a>. Omniglot is sometimes referred to as the <em>transpose</em> of mnist, since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits. There is also data about the strokes used to create each character, but we won’t be using that. Usually, it’s split into 30 training alphabets and 20 evaluation alphabets. All those different characters make for lots of possible one-shot tasks, so it’s a really good benchmark for one-shot learning algorithms.</p><h4 id="A-One-Shot-Learning-Baseline-1-Nearest-Neighbour"><a href="#A-One-Shot-Learning-Baseline-1-Nearest-Neighbour" class="headerlink" title="A One-Shot Learning Baseline / 1 Nearest Neighbour"></a>A One-Shot Learning Baseline / 1 Nearest Neighbour</h4><p>The simplest way of doing classification is with k-nearest neighbours, but since there is only one example per class we have to do 1 nearest neighbour. This is very simple, just calculate the Euclidean distance of the test example from each training example and pick the closest one:<br>$$<br>C(\hat{x})={\arg \min}_{c\in S}||\hat{x} − x_c||<br>$$<br>According to Koch et al, 1-nn gets ~28% accuracy in 20 way one shot classification on omniglot. 28% doesn’t sound great, but it’s nearly six times more accurate than random guessing(5%). This is a good baseline or “sanity check” to compare future one-shot algorithms with.</p><p><a href="http://cims.nyu.edu/~brenden/LakeEtAlNips2013.pdf" target="_blank" rel="external">Hierarchical Bayesian Program Learning</a> from Lake et al gets 95.2% - very impressive! The ~30% of this paper which I understood was very interesting. Comparing it with deep learning results that train on raw pixels is kind of “apples and oranges” though, because:</p><ol><li>HBPL used data about the strokes, not just the raw pixels</li><li>HBPL on omniglot involved learning a generative model for strokes. The algorithm requires data with more complicated annotation, so unlike deep learning it can’t easily be tweaked to one-shot learn from raw pixels of dogs/trucks/brain scans/spatulas and other objects that aren’t made up of brushstrokes.</li></ol><p>Lake et al also says that humans get 95.5% accuracy in 20 way classification on omniglot, only beating HBPL by a tiny margin. In the spirit of nullius in verba, I tried testing myself on the 20 way tasks and managed to average 97.2%. I wasn’t always doing true one-shot learning though - I saw several symbols I recognised, since I’m familiar with the greek alphabet, hiragana and katakana. I removed those alphabets and tried again but still managed 96.7%. My hypothesis is that having to read my own terrible handwriting has endowed me with superhuman symbol recognition ability.</p><h4 id="Ways-to-use-deep-networks-for-one-shot-learning"><a href="#Ways-to-use-deep-networks-for-one-shot-learning" class="headerlink" title="Ways to use deep networks for one shot learning?!"></a>Ways to use deep networks for one shot learning?!</h4><p>If we naively train a neural network on a one-shot as a vanilla cross-entropy-loss softmax classifier, it will <em>severely</em> overfit. Heck, even if it was a <em>hundred</em> shot learning a modern neural net would still probably overfit. Big neural networks have millions of parameters to adjust to their data and so they can learn a huge space of possible functions. (More formally, they have a high <a href="https://en.wikipedia.org/wiki/VC_dimension" target="_blank" rel="external">VC dimension</a>, which is part of why they do so well at learning from complex data with high dimensionality.) Unfortunately this strength also appears to be their undoing for one-shot learning. When there are millions of parameters to gradient descend upon, and a staggeringly huge number of possible mappings that can be learned, how can we make a network learn one that generalizes when there’s just a single example to learn from?</p><p>It’s easier for humans to one-shot learn the concept of a spatula or the letter ΘΘ because they have spent a lifetime observing and learning from similar objects. It’s not really fair to compare the performance of a human who’s spent a lifetime having to classify objects and symbols with that of a randomly initialized neural net, which imposes a very weak prior about the structure of the mapping to be learned from the data. This is why most of the one-shot learning papers I’ve seen take the approach of <em>knowledge transfer</em> from other tasks.</p><p>Neural nets are really good at extracting useful features from structurally complex/high dimensional data, such as images. If a neural network is given training data that is similar to (but not the same as) that in the one-shot task, it might be able to learn useful features which can be used in a simple learning algorithm that doesn’t require adjusting these parameters. It still counts as one-shot learning as long as the training examples are of different classes to the examples used for one-shot testing.</p><p>(NOTE: Here a <em>feature</em> means a “transformation of the data that is useful for learning”.)</p><p>So now an interesting problem is <em>how do we get a neural network to learn the features?</em> The most obvious way of doing this (if there’s labelled data) is just vanilla transfer learning - train a softmax classifier on the training set, then fine-tune the weights of the last layer on the support set of the one-shot task. In practice, neural net classifiers don’t work too well for data like omniglot where there are few examples per class, and even fine tuning only the weights in the last layer is enough to overfit the support set. Still works quite a lot better than L2 distance nearest neighbour though! (See <a href="https://arxiv.org/pdf/1606.04080" target="_blank" rel="external">Matching Networks for One Shot learning</a> for a comparison table of various deep one-shot learning methods and their accuracy.)</p><p>There’s a better way of doing it though! Remember 1 nearest neighbour? This simple, non-parametric one-shot learner just classifies the test example with the same class of whatever support example is the closest in L2 distance. This works ok, but L2 Distance suffers from the ominous sounding <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="external">curse of dimensionality</a> and so won’t work well for data with thousands of dimensions like omniglot. Also, if you have two nearly identical images and move one over a few pixels to the right the L2 distance can go from being almost zero to being really high. L2 distance is a metric that is just woefully inadequate for this task. Deep learning to the rescue? We can use a deep convolutional network to learn some kind of similarity function that a non-parametric classifer like nearest neighbor can use.</p><h3 id="Siamese-networks"><a href="#Siamese-networks" class="headerlink" title="Siamese networks"></a>Siamese networks</h3><p><a href="https://sorenbouma.github.io/images/cats.jpg" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/cats.jpg" alt="image"></a>I originally planned to have craniopagus conjoined twins as the accompanying image for this section but ultimately decided that siamese cats would go over better..</p><p><a href="http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" target="_blank" rel="external">This wonderful paper</a> is what I will be implementing in this tutorial. Koch et al’s approach to getting a neural net to do one-shot classification is to give it two images and train it to guess whether they have the same category. Then when doing a one-shot classification task described above, the network can compare the test image to each image in the support set, and pick which one it thinks is most likely to be of the same category. So we want a neural net architecture that takes two images as input and outputs the probability they share the same class.</p><p>Say x1 and x2 are two images in our dataset, and let x1∘x2 mean “x1 and x2 are images with the same class”. Note that x1∘x2 is the same as x2∘x1 - this means that if we reverse the order of the inputs to the neural network, the output should be the same - p(x1∘x2) should equal p(x2∘x1). This property is called <em>symmetry</em> and siamese nets are designed around having it.</p><p>Symmetry is important because it’s required for learning a distance metric - the distance from x1 to x2 should equal the distance x2 to x1.</p><p>If we just concatenate two examples together and use them as a single input to a neural net, each example will be matrix multiplied(or convolved) with a different set of weights, which breaks symmetry. Sure it’s possible it will eventually manage to learn the exact same weights for each input, but it would be much easier to learn a single set of weights applied to both inputs. So we could propagate both inputs through identical twin neural nets with shared parameters, then use the absolute difference as the input to a linear classifier - this is essentially what a siamese net is. Two identical twins, joined at the head, hence the name.</p><h4 id="Network-architecture"><a href="#Network-architecture" class="headerlink" title="Network architecture"></a>Network architecture</h4><p><em>Unfortunately, properly explaining how and why a convolutional neural net work would make this post twice as long. If you want to understand convnets work, I suggest checking out cs231n and then colah. For any non-dl people who are reading this, the best summary I can give of a CNN is this: An image is a 3D array of pixels. A convolutional layer is where you have a neuron connected to a tiny subgrid of pixels or neurons, and use copies of that neuron across all parts of the image/block to make another 3d array of neuron activations. A max pooling layer makes a block of activations spatially smaller. Lots of these stacked on top of one another can be trained with gradient descent and are really good at learning from images.</em></p><p>I’m going to describe the architecture pretty briefly because it’s not the important part of the paper. Koch et al uses a <em>convolutional</em> siamese network to classify pairs of omniglot images, so the twin networks are both convolutional neural nets(CNNs). The twins each have the following architecture: convolution with 64 10x10 filters, relu -&gt; max pool -&gt; convolution with 128 7x7 filters, relu -&gt; max pool -&gt; convolution with 128 4x4 filters, relu -&gt; max pool -&gt; convolution with 256 4x4 filters. The twin networks reduce their inputs down to smaller and smaller 3d tensors, finally their is a fully connected layer with 4096 units. The absolute difference between the two vectors is used as input to a linear classifier. All up, the network has 38,951,745 parameters - 96% of which belong to the fully connected layer. This is quite a lot, so the network has high capacity to overfit, but as I show below, pairwse training means the dataset size is huge so this won’t be a problem.</p><p><a href="https://sorenbouma.github.io/images/Siamese_diagram_2.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/Siamese_diagram_2.png" alt="image"></a>Hastily made architecture diagram.</p><p>The output is squashed into [0,1] with a sigmoid function to make it a probability. We use the target t=1t=1 when the images have the same class and t=0t=0 for a different class. It’s trained with logistic regression. This means the loss function should be binary cross entropy between the predictions and targets. There is also a L2 weight decay term in the loss to encourage the network to learn smaller/less noisy weights and possibly improve generalization:<br>$$<br>L(x_1,x_2,t)=t⋅\log(p(x_1∘x_2))+(1−t)⋅\log(1−p(x_1∘x_2))+λ⋅||w||^2<br>$$<br>When it does a one-shot task, the siamese net simply classifies the test image as whatever image in the support set it thinks is most similar to the test image:<br>$$<br>C(\hat{x},S) = {\arg\max}_c P(\hat{x}∘x_c),x_c∈S<br>$$<br>This uses an argmax unlike nearest neighbour which uses an argmin, because a <em>metric</em> like L2 is higher the more “different” the examples are, but this models outputs p(x1∘x2), so we want the highest. This approach has one flaw that’s obvious to me: for any xaxa in the support set,the probability $\hat{x}∘x_a$ is independent of every other example in the support set! This means the probabilities won’t sum to 1, ignores important information, namely that the test image will be the same type as exactly <em>one</em> x∈S…</p><h4 id="Observation-effective-dataset-size-in-pairwise-training"><a href="#Observation-effective-dataset-size-in-pairwise-training" class="headerlink" title="Observation: effective dataset size in pairwise training"></a>Observation: effective dataset size in pairwise training</h4><p>EDIT: After discussing this with a PhD student at UoA, I think this bit might be overstated or even just wrong. Emperically, my implementation <em>did</em> overfit, even though it wasn’t trained for enough iterations to sample every possible pair, which kind of contradicts this section. I’m leaving it up in the spirit of being wrong loudly.</p><p>One cool thing I noticed about training on pairs is that there are quadratically many possible pairs of images to train the model on, making it hard to overfit. Say we have CC examples each of EE classes. Since there are C⋅EC⋅E images total, the total number of possible pairs is given by<br>$$<br>Npairs=(C⋅E 2)=(C⋅E)!/2!(C⋅E−2)!<br>$$<br>For omniglot with its 20 examples of 964 training classes, this leads to 185,849,560 possible pairs, which is huge! However, the siamese network needs examples of both same and different class pairs. There are E examples per class, so there will be (E 2) pairs for every class, which means there are Nsame=(E 2)⋅C possible pairs with the same class - 183,160 pairs for omniglot. Even though 183,160 example pairs is plenty, it’s only a thousandth of the possible pairs, and the number of same-class pairs increases quadratically with E but only linearly with C. This is important because the siamese network should be given a 1:1 ratio of same-class and different-class pairs to train on - perhaps it implies that pairwise training is easier on datasets with lots of examples per class.</p><h3 id="The-Code"><a href="#The-Code" class="headerlink" title="The Code:"></a>The Code:</h3><p><a href="https://github.com/sorenbouma/keras-oneshot" target="_blank" rel="external">Prefer to just play with a jupyter notebook? I got you fam</a></p><p>Here is the model definition, it should be pretty easy to follow if you’ve seen keras before. I only define the twin network’s architecture once as a Sequential() model and then call it with respect to each of two input layers, this way the same parameters are used for both inputs. Then merge them together with absolute distance and add an output layer, and compile the model with binary cross entropy loss.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, Sequential</div><div class="line"><span class="keyword">from</span> keras.regularizers <span class="keyword">import</span> l2</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD,Adam</div><div class="line"><span class="keyword">from</span> keras.losses <span class="keyword">import</span> binary_crossentropy</div><div class="line"><span class="keyword">import</span> numpy.random <span class="keyword">as</span> rng</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> dill <span class="keyword">as</span> pickle</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">W_init</span><span class="params">(shape,name=None)</span>:</span></div><div class="line">    <span class="string">"""Initialize weights as in paper"""</span></div><div class="line">    values = rng.normal(loc=<span class="number">0</span>,scale=<span class="number">1e-2</span>,size=shape)</div><div class="line">    <span class="keyword">return</span> K.variable(values,name=name)</div><div class="line"><span class="comment">#//<span class="doctag">TODO:</span> figure out how to initialize layer biases in keras.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">b_init</span><span class="params">(shape,name=None)</span>:</span></div><div class="line">    <span class="string">"""Initialize bias as in paper"""</span></div><div class="line">    values=rng.normal(loc=<span class="number">0.5</span>,scale=<span class="number">1e-2</span>,size=shape)</div><div class="line">    <span class="keyword">return</span> K.variable(values,name=name)</div><div class="line"></div><div class="line">input_shape = (<span class="number">105</span>, <span class="number">105</span>, <span class="number">1</span>)</div><div class="line">left_input = Input(input_shape)</div><div class="line">right_input = Input(input_shape)</div><div class="line"><span class="comment">#build convnet to use in each siamese 'leg'</span></div><div class="line">convnet = Sequential()</div><div class="line">convnet.add(Conv2D(<span class="number">64</span>,(<span class="number">10</span>,<span class="number">10</span>),activation=<span class="string">'relu'</span>,input_shape=input_shape,</div><div class="line">                   kernel_initializer=W_init,kernel_regularizer=l2(<span class="number">2e-4</span>)))</div><div class="line">convnet.add(MaxPooling2D())</div><div class="line">convnet.add(Conv2D(<span class="number">128</span>,(<span class="number">7</span>,<span class="number">7</span>),activation=<span class="string">'relu'</span>,</div><div class="line">                   kernel_regularizer=l2(<span class="number">2e-4</span>),kernel_initializer=W_init,bias_initializer=b_init))</div><div class="line">convnet.add(MaxPooling2D())</div><div class="line">convnet.add(Conv2D(<span class="number">128</span>,(<span class="number">4</span>,<span class="number">4</span>),activation=<span class="string">'relu'</span>,kernel_initializer=W_init,kernel_regularizer=l2(<span class="number">2e-4</span>),bias_initializer=b_init))</div><div class="line">convnet.add(MaxPooling2D())</div><div class="line">convnet.add(Conv2D(<span class="number">256</span>,(<span class="number">4</span>,<span class="number">4</span>),activation=<span class="string">'relu'</span>,kernel_initializer=W_init,kernel_regularizer=l2(<span class="number">2e-4</span>),bias_initializer=b_init))</div><div class="line">convnet.add(Flatten())</div><div class="line">convnet.add(Dense(<span class="number">4096</span>,activation=<span class="string">"sigmoid"</span>,kernel_regularizer=l2(<span class="number">1e-3</span>),kernel_initializer=W_init,bias_initializer=b_init))</div><div class="line"><span class="comment">#encode each of the two inputs into a vector with the convnet</span></div><div class="line">encoded_l = convnet(left_input)</div><div class="line">encoded_r = convnet(right_input)</div><div class="line"><span class="comment">#merge two encoded inputs with the l1 distance between them</span></div><div class="line">L1_distance = <span class="keyword">lambda</span> x: K.abs(x[<span class="number">0</span>]-x[<span class="number">1</span>])</div><div class="line">both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</div><div class="line">prediction = Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>,bias_initializer=b_init)(both)</div><div class="line">siamese_net = Model(input=[left_input,right_input],output=prediction)</div><div class="line"><span class="comment">#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)</span></div><div class="line"></div><div class="line">optimizer = Adam(<span class="number">0.00006</span>)</div><div class="line"><span class="comment">#//<span class="doctag">TODO:</span> get layerwise learning rates and momentum annealing scheme described in paperworking</span></div><div class="line">siamese_net.compile(loss=<span class="string">"binary_crossentropy"</span>,optimizer=optimizer)</div><div class="line"></div><div class="line">siamese_net.count_params()</div></pre></td></tr></table></figure><p>The original paper used layerwise learning rates and momentum - I skipped this because it; was kind of messy to implement in keras and the hyperparameters aren’t the interesting part of the paper. Koch et al adds examples to the dataset by distorting the images and runs experiments with a fixed training set of up to 150,000 pairs. Since that won’t fit in my computers memory, I decided to just randomly sample pairs. Loading image pairs was probably the hardest part of this to implement. Since there were 20 examples for every class, I reshaped the data into N_classes x 20 x 105 x 105 arrays, to make it easier to index by category.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Siamese_Loader</span>:</span></div><div class="line">    <span class="string">"""For loading batches and testing tasks to a siamese net"""</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,Xtrain,Xval)</span>:</span></div><div class="line">        self.Xval = Xval</div><div class="line">        self.Xtrain = Xtrain</div><div class="line">        self.n_classes,self.n_examples,self.w,self.h = Xtrain.shape</div><div class="line">        self.n_val,self.n_ex_val,_,_ = Xval.shape</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(self,n)</span>:</span></div><div class="line">        <span class="string">"""Create batch of n pairs, half same class, half different class"""</span></div><div class="line">        categories = rng.choice(self.n_classes,size=(n,),replace=<span class="keyword">False</span>)</div><div class="line">        pairs=[np.zeros((n, self.h, self.w,<span class="number">1</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>)]</div><div class="line">        targets=np.zeros((n,))</div><div class="line">        targets[n//<span class="number">2</span>:] = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">            category = categories[i]</div><div class="line">            idx_1 = rng.randint(<span class="number">0</span>,self.n_examples)</div><div class="line">            pairs[<span class="number">0</span>][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,<span class="number">1</span>)</div><div class="line">            idx_2 = rng.randint(<span class="number">0</span>,self.n_examples)</div><div class="line">            <span class="comment">#pick images of same class for 1st half, different for 2nd</span></div><div class="line">            category_2 = category <span class="keyword">if</span> i &gt;= n//<span class="number">2</span> <span class="keyword">else</span> (category + rng.randint(<span class="number">1</span>,self.n_classes)) % self.n_classes</div><div class="line">            pairs[<span class="number">1</span>][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> pairs, targets</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_oneshot_task</span><span class="params">(self,N)</span>:</span></div><div class="line">        <span class="string">"""Create pairs of test image, support set for testing N way one-shot learning. """</span></div><div class="line">        categories = rng.choice(self.n_val,size=(N,),replace=<span class="keyword">False</span>)</div><div class="line">        indices = rng.randint(<span class="number">0</span>,self.n_ex_val,size=(N,))</div><div class="line">        true_category = categories[<span class="number">0</span>]</div><div class="line">        ex1, ex2 = rng.choice(self.n_examples,replace=<span class="keyword">False</span>,size=(<span class="number">2</span>,))</div><div class="line">        test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,<span class="number">1</span>)</div><div class="line">        support_set = self.Xval[categories,indices,:,:]</div><div class="line">        support_set[<span class="number">0</span>,:,:] = self.Xval[true_category,ex2]</div><div class="line">        support_set = support_set.reshape(N,self.w,self.h,<span class="number">1</span>)</div><div class="line">        pairs = [test_image,support_set]</div><div class="line">        targets = np.zeros((N,))</div><div class="line">        targets[<span class="number">0</span>] = <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> pairs, targets</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_oneshot</span><span class="params">(self,model,N,k,verbose=<span class="number">0</span>)</span>:</span></div><div class="line">        <span class="string">"""Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks"""</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line">        n_correct = <span class="number">0</span></div><div class="line">        <span class="keyword">if</span> verbose:</div><div class="line">            print(<span class="string">"Evaluating model on &#123;&#125; unique &#123;&#125; way one-shot learning tasks ..."</span>.format(k,N))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">            inputs, targets = self.make_oneshot_task(N)</div><div class="line">            probs = model.predict(inputs)</div><div class="line">            <span class="keyword">if</span> np.argmax(probs) == <span class="number">0</span>:</div><div class="line">                n_correct+=<span class="number">1</span></div><div class="line">        percent_correct = (<span class="number">100.0</span>*n_correct / k)</div><div class="line">        <span class="keyword">if</span> verbose:</div><div class="line">            print(<span class="string">"Got an average of &#123;&#125;% &#123;&#125; way one-shot learning accuracy"</span>.format(percent_correct,N))</div><div class="line">        <span class="keyword">return</span> percent_correct</div></pre></td></tr></table></figure><p>..And now the training loop. Nothing unusual here, except for that I monitor one-shot tasks validation accuracy to test performance, rather than loss on the validation set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">evaluate_every = <span class="number">7000</span></div><div class="line">loss_every=<span class="number">300</span></div><div class="line">batch_size = <span class="number">32</span></div><div class="line">N_way = <span class="number">20</span></div><div class="line">n_val = <span class="number">550</span></div><div class="line">siamese_net.load_weights(<span class="string">"PATH"</span>)</div><div class="line">best = <span class="number">76.0</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">900000</span>):</div><div class="line">    (inputs,targets)=loader.get_batch(batch_size)</div><div class="line">    loss=siamese_net.train_on_batch(inputs,targets)</div><div class="line">    <span class="keyword">if</span> i % evaluate_every == <span class="number">0</span>:</div><div class="line">        val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">if</span> val_acc &gt;= best:</div><div class="line">            print(<span class="string">"saving"</span>)</div><div class="line">            siamese_net.save(<span class="string">'PATH'</span>)</div><div class="line">            best=val_acc</div><div class="line"></div><div class="line">    <span class="keyword">if</span> i % loss_every == <span class="number">0</span>:</div><div class="line">        print(<span class="string">"iteration &#123;&#125;, training loss: &#123;:.2f&#125;,"</span>.format(i,loss))</div></pre></td></tr></table></figure><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>Once the learning curve flattened out, I used the weights which got the best validation 20 way accuracy for testing. My network averaged ~83% accuracy for tasks from the evaluation set, compared to 93% in the original paper. Probably this difference is because I didn’t implement many of the performance enhancing tricks from the original paper, like layerwise learning rates/momentum, data augmentation with distortions, bayesian hyperparemeter optimization and I also probably trained for less epochs. I’m not too worried about this because this tutorial was more about introducing one-shot learning in general, than squeezing the last few % performance out of a classifier. There is no shortage of resources on that!</p><p>I was curious to see how accuracy varied over different values of “N” in N way one shot learning, so I plotted it, with comparisons to 1 nearest neighbours, random guessing and training set performance.</p><p><a href="https://sorenbouma.github.io/images/results1.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/results1.png" alt="image"></a>results.</p><p>As you can see, it performs worse on tasks from the validaiton set than the train set, especially for high values of N, so there must be overfitting. It would be interesting to see how well traditional regularization methods like dropout work when the validation set is made of completely different classes to the training set. It works better than I expected for large N, still averaging above 65% accuracy for 50-60 way tasks.</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>We’ve just trained a neural network trained to do same-different pairwise classification on symbols. More importantly, we’ve shown that it can then get reasonable accuracy in 20 way one-shot learning on symbols from unseen alphabets. Of course, this is not the only way to use deep networks for one-shot learning.</p><p>As I touched on earlier, I think a major flaw of this siamese approach is that it only compares the test image to every support image individualy, when it should be comparing it to the support set as a whole. When the network compares the test image to any image x1, p(x^∘x1) is the same no matter what else is the support set. This is silly. Say you’re doing a one-shot task and you see an image that looks similar to the test image. You should be much less confident they have the same class if there is another image in the support set that also looks similar to the test image. The training objective is different to the test objective. It might work better to have a model that can compare the test image to the support set as a whole and use the constraint that only one support image has the same class.</p><p><a href="https://arxiv.org/pdf/1606.04080.pdf" target="_blank" rel="external">Matching Networks for One Shot learning</a> does exactly that. Rather than learning a similarity function, they have a deep model learn a full nearest neighbour classifier end to end, training directly on oneshot tasks rather than on image pairs. <a href="https://github.com/karpathy/paper-notes/blob/master/matching_networks.md" target="_blank" rel="external">Andrej Karpathy’s notes</a> explain it much better than I can. Since you are learning a machine classifier, this can be seen as a kind of <em>meta-learning</em>. <a href="https://arxiv.org/pdf/1605.06065.pdf" target="_blank" rel="external">One-shot Learning with Memory-Augmented Neural Networks</a> explores the connection between one-shot learning and meta learning and trains a memory augmented network on omniglot, though I confess I had trouble understanding this paper.</p><h3 id="What-next"><a href="#What-next" class="headerlink" title="What next?"></a>What next?</h3><p>The omniglot dataset has been around since 2015, and already there are scalable ML algorithms getting within the ballpark of human level performance on certain one-shot learning tasks. Hopefully one day it will be seen as a mere “sanity check” for one-shot classification algorithms much like MNIST is for supervised learning now.</p><p>Image classification is cool but I don’t think it’s the most interesting problem in machine learning. Now that we know deep one-shot learning can work pretty good, I think it would be cool to see attempts at one-shot learning for other, more exotic tasks.</p><p>Ideas from one-shot learning could be used for more sample efficient reinforcement learning, especially for problems like OpenAI’s Universe, where there are lots of MDPs/environments that have similar visual features and dynamics. - It would be cool to have an RL agent that could efficiently explore a new environment after learning in similar MDPs.</p><p><a href="https://sorenbouma.github.io/images/worldofbits.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/worldofbits.jpg" alt="image"></a>OpenAI’s world of bits environments.</p><p><a href="https://arxiv.org/abs/1703.07326" target="_blank" rel="external">One-shot Imitation learning</a> is one of my favourite one-shot learning papers. The goal is to have an agent learn a robust policy for solving a task from a single human demonstration of that task.This is done by:</p><ol><li>Having a neural net map from the current state and a sequence of states(the human demonstration) to an action</li><li>Training it on pairs of human demonstrations on slightly different variants of the same task, with the goal of reproducing the second demonstration based on the first.</li></ol><p>This strikes me as a really promising path to one day having broadly applicable, learning based robots!</p><p>Bringing one-shot learning to NLP tasks is a cool idea too. <em>Matching Networks for One-Shot learning</em> has an attempt at one-shot language modeling, filling a missing word in a test sentence given a small set of support sentences, and it seems to work pretty well. Exciting!</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Anyway, thanks for reading! I hope you’ve managed to one-shot learn the concept of one-shot learning :) If not, I’d love to hear feedback or answer any questions you have!</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background:&quot;&gt;&lt;/a&gt;Background:&lt;/h2&gt;&lt;p&gt;Conventional wisdom says that deep n
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="one-shot learning" scheme="http://yoursite.com/tags/one-shot-learning/"/>
    
  </entry>
  
  <entry>
    <title>Anaconda uses socket proxy on Windows 10</title>
    <link href="http://yoursite.com/2018/05/17/Anaconda-uses-socket-proxy-on-Windows-10/"/>
    <id>http://yoursite.com/2018/05/17/Anaconda-uses-socket-proxy-on-Windows-10/</id>
    <published>2018-05-17T09:54:53.000Z</published>
    <updated>2018-05-17T09:58:52.027Z</updated>
    
    <content type="html"><![CDATA[<p>you need to create a <strong>.condarc</strong> file in you Windows user area:</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">C:\Users\&lt;username&gt;\</div></pre></td></tr></table></figure><p>The file should contain (if you are using shadowsocks):</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">channels:</div><div class="line">- defaults</div><div class="line"></div><div class="line"><span class="comment"># Show channel URLs when displaying what is going to be downloaded and</span></div><div class="line"><span class="comment"># in 'conda list'. The default is False.</span></div><div class="line">show_channel_urls: True</div><div class="line">allow_other_channels: True</div><div class="line"></div><div class="line">proxy_servers:</div><div class="line">    http: socks5://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">1080</span></div><div class="line">    https: socks5://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">1080</span></div><div class="line"></div><div class="line">ssl_verify: False</div></pre></td></tr></table></figure><p>Noticed that you cannot create a file that begins with a dot in Windows directly.</p><p>To <strong>create/rename on windows explorer</strong>, just rename to <code>.name.</code> - The additional dot at the end is necessary, and will be removed by Windows Explorer.</p><p>To create a new file begins with a dot, on command prompt:</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo testing &gt; .name</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;you need to create a &lt;strong&gt;.condarc&lt;/strong&gt; file in you Windows user area:&lt;/p&gt;&lt;figure class=&quot;highlight powershell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td clas
    
    </summary>
    
    
  </entry>
  
</feed>
