<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Abracadabra</title>
  <subtitle>Do it yourself</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-10-16T10:28:20.153Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Ewan Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>EDA Example I (Springleaf competition)</title>
    <link href="http://yoursite.com/2018/10/16/EDA-Example-I-Springleaf-competition/"/>
    <id>http://yoursite.com/2018/10/16/EDA-Example-I-Springleaf-competition/</id>
    <published>2018-10-16T10:19:20.000Z</published>
    <updated>2018-10-16T10:28:20.153Z</updated>
    
    <content type="html"><![CDATA[<p>This is a notebook, used in the screencast video. Note, that the data files are not present here in Jupyter hub and you will not be able to run it. But you can always download the notebook to your local machine as well as the competition data and make it interactive.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </div><div class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm_notebook</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">%matplotlib inline</div><div class="line"></div><div class="line"><span class="keyword">import</span> warnings</div><div class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</div><div class="line"></div><div class="line"><span class="keyword">import</span> seaborn</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">autolabel</span><span class="params">(arrayA)</span>:</span></div><div class="line">    <span class="string">''' label each colored square with the corresponding data value. </span></div><div class="line">    If value &gt; 20, the text is in black, else in white.</div><div class="line">    '''</div><div class="line">    arrayA = np.array(arrayA)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(arrayA.shape[<span class="number">0</span>]):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(arrayA.shape[<span class="number">1</span>]):</div><div class="line">                plt.text(j,i, <span class="string">"%.2f"</span>%arrayA[i,j], ha=<span class="string">'center'</span>, va=<span class="string">'bottom'</span>,color=<span class="string">'w'</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">hist_it</span><span class="params">(feat)</span>:</span></div><div class="line">    plt.figure(figsize=(<span class="number">16</span>,<span class="number">4</span>))</div><div class="line">    feat[Y==<span class="number">0</span>].hist(bins=range(int(feat.min()),int(feat.max()+<span class="number">2</span>)),normed=<span class="keyword">True</span>,alpha=<span class="number">0.8</span>)</div><div class="line">    feat[Y==<span class="number">1</span>].hist(bins=range(int(feat.min()),int(feat.max()+<span class="number">2</span>)),normed=<span class="keyword">True</span>,alpha=<span class="number">0.5</span>)</div><div class="line">    plt.ylim((<span class="number">0</span>,<span class="number">1</span>))</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gt_matrix</span><span class="params">(feats,sz=<span class="number">16</span>)</span>:</span></div><div class="line">    a = []</div><div class="line">    <span class="keyword">for</span> i,c1 <span class="keyword">in</span> enumerate(feats):</div><div class="line">        b = [] </div><div class="line">        <span class="keyword">for</span> j,c2 <span class="keyword">in</span> enumerate(feats):</div><div class="line">            mask = (~train[c1].isnull()) &amp; (~train[c2].isnull())</div><div class="line">            <span class="keyword">if</span> i&gt;=j:</div><div class="line">                b.append((train.loc[mask,c1].values&gt;=train.loc[mask,c2].values).mean())</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                b.append((train.loc[mask,c1].values&gt;train.loc[mask,c2].values).mean())</div><div class="line"></div><div class="line">        a.append(b)</div><div class="line"></div><div class="line">    plt.figure(figsize = (sz,sz))</div><div class="line">    plt.imshow(a, interpolation = <span class="string">'None'</span>)</div><div class="line">    _ = plt.xticks(range(len(feats)),feats,rotation = <span class="number">90</span>)</div><div class="line">    _ = plt.yticks(range(len(feats)),feats,rotation = <span class="number">0</span>)</div><div class="line">    autolabel(a)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">hist_it1</span><span class="params">(feat)</span>:</span></div><div class="line">    plt.figure(figsize=(<span class="number">16</span>,<span class="number">4</span>))</div><div class="line">    feat[Y==<span class="number">0</span>].hist(bins=<span class="number">100</span>,range=(feat.min(),feat.max()),normed=<span class="keyword">True</span>,alpha=<span class="number">0.5</span>)</div><div class="line">    feat[Y==<span class="number">1</span>].hist(bins=<span class="number">100</span>,range=(feat.min(),feat.max()),normed=<span class="keyword">True</span>,alpha=<span class="number">0.5</span>)</div><div class="line">    plt.ylim((<span class="number">0</span>,<span class="number">1</span>))</div></pre></td></tr></table></figure><h1 id="Read-the-data"><a href="#Read-the-data" class="headerlink" title="Read the data"></a>Read the data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train = pd.read_csv(<span class="string">'train.csv.zip'</span>)</div><div class="line">Y = train.target</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">test = pd.read_csv(<span class="string">'test.csv.zip'</span>)</div><div class="line">test_ID = test.ID</div></pre></td></tr></table></figure><a id="more"></a><h1 id="Data-overview"><a href="#Data-overview" class="headerlink" title="Data overview"></a>Data overview</h1><p>Probably the first thing you check is the shapes of the train and test matrices and look inside them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> <span class="string">'Train shape'</span>, train.shape</div><div class="line"><span class="keyword">print</span> <span class="string">'Test shape'</span>,  test.shape</div></pre></td></tr></table></figure><pre><code>Train shape (145231, 1934)
Test shape (145232, 1933)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.head()</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>ID</th><br><th>VAR_0001</th><br><th>VAR_0002</th><br><th>VAR_0003</th><br><th>VAR_0004</th><br><th>VAR_0005</th><br><th>VAR_0006</th><br><th>VAR_0007</th><br><th>VAR_0008</th><br><th>VAR_0009</th><br><th>…</th><br><th>VAR_1926</th><br><th>VAR_1927</th><br><th>VAR_1928</th><br><th>VAR_1929</th><br><th>VAR_1930</th><br><th>VAR_1931</th><br><th>VAR_1932</th><br><th>VAR_1933</th><br><th>VAR_1934</th><br><th>target</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>2</td><br><td>H</td><br><td>224</td><br><td>0</td><br><td>4300</td><br><td>C</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br><td>0</td><br></tr><br><tr><br><th>1</th><br><td>4</td><br><td>H</td><br><td>7</td><br><td>53</td><br><td>4448</td><br><td>B</td><br><td>1.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br><td>0</td><br></tr><br><tr><br><th>2</th><br><td>5</td><br><td>H</td><br><td>116</td><br><td>3</td><br><td>3464</td><br><td>C</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br><td>0</td><br></tr><br><tr><br><th>3</th><br><td>7</td><br><td>H</td><br><td>240</td><br><td>300</td><br><td>3200</td><br><td>C</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>RCC</td><br><td>0</td><br></tr><br><tr><br><th>4</th><br><td>8</td><br><td>R</td><br><td>72</td><br><td>261</td><br><td>2000</td><br><td>N</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>BRANCH</td><br><td>1</td><br></tr><br></tbody><br></table><br><p>5 rows × 1934 columns</p><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.head()</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>ID</th><br><th>VAR_0001</th><br><th>VAR_0002</th><br><th>VAR_0003</th><br><th>VAR_0004</th><br><th>VAR_0005</th><br><th>VAR_0006</th><br><th>VAR_0007</th><br><th>VAR_0008</th><br><th>VAR_0009</th><br><th>…</th><br><th>VAR_1925</th><br><th>VAR_1926</th><br><th>VAR_1927</th><br><th>VAR_1928</th><br><th>VAR_1929</th><br><th>VAR_1930</th><br><th>VAR_1931</th><br><th>VAR_1932</th><br><th>VAR_1933</th><br><th>VAR_1934</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>1</td><br><td>R</td><br><td>360</td><br><td>25</td><br><td>2251</td><br><td>B</td><br><td>2.0</td><br><td>2.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br><tr><br><th>1</th><br><td>3</td><br><td>R</td><br><td>74</td><br><td>192</td><br><td>3274</td><br><td>C</td><br><td>2.0</td><br><td>3.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br><tr><br><th>2</th><br><td>6</td><br><td>R</td><br><td>21</td><br><td>36</td><br><td>3500</td><br><td>C</td><br><td>1.0</td><br><td>1.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br><tr><br><th>3</th><br><td>9</td><br><td>R</td><br><td>8</td><br><td>2</td><br><td>1500</td><br><td>B</td><br><td>0.0</td><br><td>0.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br><tr><br><th>4</th><br><td>10</td><br><td>H</td><br><td>91</td><br><td>39</td><br><td>84500</td><br><td>C</td><br><td>8.0</td><br><td>3.0</td><br><td>False</td><br><td>False</td><br><td>…</td><br><td>0</td><br><td>98</td><br><td>98</td><br><td>998</td><br><td>999999998</td><br><td>998</td><br><td>998</td><br><td>9998</td><br><td>9998</td><br><td>IAPS</td><br></tr><br></tbody><br></table><br><p>5 rows × 1933 columns</p><br></div><p>There are almost 2000 anonymized variables! It’s clear, some of them are categorical, some look like numeric. Some numeric feateures are integer typed, so probably they are event conters or dates. And others are of float type, but from the first few rows they look like integer-typed too, since fractional part is zero, but pandas treats them as <code>float</code> since there are NaN values in that features.</p><p>From the first glance we see train has one more column <code>target</code> which we should not forget to drop before fitting a classifier. We also see <code>ID</code> column is shared between train and test, which sometimes can be succesfully used to improve the score.</p><p>It is also useful to know if there are any NaNs in the data. You should pay attention to columns with NaNs and the number of NaNs for each row can serve as a nice feature later.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Number of NaNs for each object</span></div><div class="line">train.isnull().sum(axis=<span class="number">1</span>).head(<span class="number">15</span>)</div></pre></td></tr></table></figure><pre><code>0     25
1     19
2     24
3     24
4     24
5     24
6     24
7     24
8     16
9     24
10    22
11    24
12    17
13    24
14    24
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Number of NaNs for each column</span></div><div class="line">train.isnull().sum(axis=<span class="number">0</span>).head(<span class="number">15</span>)</div></pre></td></tr></table></figure><pre><code>ID           0
VAR_0001     0
VAR_0002     0
VAR_0003     0
VAR_0004     0
VAR_0005     0
VAR_0006    56
VAR_0007    56
VAR_0008    56
VAR_0009    56
VAR_0010    56
VAR_0011    56
VAR_0012    56
VAR_0013    56
VAR_0014    56
dtype: int64
</code></pre><p>Just by reviewing the head of the lists we immediately see the patterns, exactly 56 NaNs for a set of variables, and 24 NaNs for objects.</p><h1 id="Dataset-cleaning"><a href="#Dataset-cleaning" class="headerlink" title="Dataset cleaning"></a>Dataset cleaning</h1><h3 id="Remove-constant-features"><a href="#Remove-constant-features" class="headerlink" title="Remove constant features"></a>Remove constant features</h3><p>All 1932 columns are anonimized which makes us to deduce the meaning of the features ourselves. We will now try to clean the dataset.</p><p>It is usually convenient to concatenate train and test into one dataframe and do all feature engineering using it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">traintest = pd.concat([train, test], axis = <span class="number">0</span>)</div></pre></td></tr></table></figure><p>First we schould look for a constant features, such features do not provide any information and only make our dataset larger.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># `dropna = False` makes nunique treat NaNs as a distinct value</span></div><div class="line">feats_counts = train.nunique(dropna = <span class="keyword">False</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">feats_counts.sort_values()[:<span class="number">10</span>]</div></pre></td></tr></table></figure><pre><code>VAR_0213    1
VAR_0207    1
VAR_0840    1
VAR_0847    1
VAR_1428    1
VAR_1165    2
VAR_0438    2
VAR_1164    2
VAR_1163    2
VAR_1162    2
dtype: int64
</code></pre><p>We found 5 constant features. Let’s remove them.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">constant_features = feats_counts.loc[feats_counts==<span class="number">1</span>].index.tolist()</div><div class="line"><span class="keyword">print</span> (constant_features)</div><div class="line"></div><div class="line"></div><div class="line">traintest.drop(constant_features,axis = <span class="number">1</span>,inplace=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><pre><code>[&apos;VAR_0207&apos;, &apos;VAR_0213&apos;, &apos;VAR_0840&apos;, &apos;VAR_0847&apos;, &apos;VAR_1428&apos;]
</code></pre><h3 id="Remove-duplicated-features"><a href="#Remove-duplicated-features" class="headerlink" title="Remove duplicated features"></a>Remove duplicated features</h3><p>Fill NaNs with something we can find later if needed.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">traintest.fillna(<span class="string">'NaN'</span>, inplace=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>Now let’s encode each feature, as we discussed.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">train_enc =  pd.DataFrame(index = train.index)</div><div class="line"></div><div class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tqdm_notebook(traintest.columns):</div><div class="line">    train_enc[col] = train[col].factorize()[<span class="number">0</span>]</div></pre></td></tr></table></figure><p>​</p><p>We could also do something like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># train_enc[col] = train[col].map(train[col].value_counts())</span></div></pre></td></tr></table></figure><p>The resulting data frame is very very large, so we cannot just transpose it and use .duplicated. That is why we will use a simple loop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">dup_cols = &#123;&#125;</div><div class="line"></div><div class="line"><span class="keyword">for</span> i, c1 <span class="keyword">in</span> enumerate(tqdm_notebook(train_enc.columns)):</div><div class="line">    <span class="keyword">for</span> c2 <span class="keyword">in</span> train_enc.columns[i + <span class="number">1</span>:]:</div><div class="line">        <span class="keyword">if</span> c2 <span class="keyword">not</span> <span class="keyword">in</span> dup_cols <span class="keyword">and</span> np.all(train_enc[c1] == train_enc[c2]):</div><div class="line">            dup_cols[c2] = c1</div></pre></td></tr></table></figure><p>​</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dup_cols</div></pre></td></tr></table></figure><pre><code>{&apos;VAR_0009&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0010&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0011&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0012&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0013&apos;: &apos;VAR_0006&apos;,
 &apos;VAR_0018&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0019&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0020&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0021&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0022&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0023&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0024&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0025&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0026&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0027&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0028&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0029&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0030&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0031&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0032&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0038&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0039&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0040&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0041&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0042&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0043&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0044&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0181&apos;: &apos;VAR_0180&apos;,
 &apos;VAR_0182&apos;: &apos;VAR_0180&apos;,
 &apos;VAR_0189&apos;: &apos;VAR_0188&apos;,
 &apos;VAR_0190&apos;: &apos;VAR_0188&apos;,
 &apos;VAR_0196&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0197&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0199&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0201&apos;: &apos;VAR_0051&apos;,
 &apos;VAR_0202&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0203&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0210&apos;: &apos;VAR_0208&apos;,
 &apos;VAR_0211&apos;: &apos;VAR_0208&apos;,
 &apos;VAR_0215&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0216&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0221&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0222&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0223&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0228&apos;: &apos;VAR_0227&apos;,
 &apos;VAR_0229&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0238&apos;: &apos;VAR_0089&apos;,
 &apos;VAR_0239&apos;: &apos;VAR_0008&apos;,
 &apos;VAR_0357&apos;: &apos;VAR_0260&apos;,
 &apos;VAR_0394&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0438&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0446&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0512&apos;: &apos;VAR_0506&apos;,
 &apos;VAR_0527&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0528&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0529&apos;: &apos;VAR_0526&apos;,
 &apos;VAR_0530&apos;: &apos;VAR_0246&apos;,
 &apos;VAR_0672&apos;: &apos;VAR_0670&apos;,
 &apos;VAR_1036&apos;: &apos;VAR_0916&apos;}
</code></pre><p>Don’t forget to save them, as it takes long time to find these.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</div><div class="line">pickle.dump(dup_cols, open(<span class="string">'dup_cols.p'</span>, <span class="string">'w'</span>), protocol=pickle.HIGHEST_PROTOCOL)</div></pre></td></tr></table></figure><p>Drop from traintest.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">traintest.drop(dup_cols.keys(), axis = <span class="number">1</span>,inplace=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><h1 id="Determine-types"><a href="#Determine-types" class="headerlink" title="Determine types"></a>Determine types</h1><p>Let’s examine the number of unique values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">nunique = train.nunique(dropna=<span class="keyword">False</span>)</div><div class="line">nunique</div></pre></td></tr></table></figure><pre><code>ID                145231
VAR_0001               3
VAR_0002             820
VAR_0003             588
VAR_0004            7935
VAR_0005               4
VAR_0006              38
VAR_0007              36
VAR_0008               2
VAR_0009               2
VAR_0010               2
VAR_0011               2
VAR_0012               2
VAR_0013              38
VAR_0014              38
VAR_0015              27
VAR_0016              30
VAR_0017              26
VAR_0018               2
VAR_0019               2
VAR_0020               2
VAR_0021               2
VAR_0022               2
VAR_0023               2
VAR_0024               2
VAR_0025               2
VAR_0026               2
VAR_0027               2
VAR_0028               2
VAR_0029               2
                   ...  
VAR_1907              41
VAR_1908              37
VAR_1909              41
VAR_1910              37
VAR_1911             107
VAR_1912           16370
VAR_1913           25426
VAR_1914           14226
VAR_1915            1148
VAR_1916               8
VAR_1917              10
VAR_1918              86
VAR_1919             383
VAR_1920              22
VAR_1921              18
VAR_1922            6798
VAR_1923            2445
VAR_1924             573
VAR_1925              11
VAR_1926               6
VAR_1927              10
VAR_1928              30
VAR_1929             591
VAR_1930               8
VAR_1931              10
VAR_1932              74
VAR_1933             363
VAR_1934               5
target                 2
VAR_0004_mod50        50
Length: 1935, dtype: int64
</code></pre><p>and build a histogram of those values</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</div><div class="line">_ = plt.hist(nunique.astype(float)/train.shape[<span class="number">0</span>], bins=<span class="number">100</span>)</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_44_0.png" alt="png"></p><p>Let’s take a looks at the features with a huge number of unique values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mask = (nunique.astype(float)/train.shape[<span class="number">0</span>] &gt; <span class="number">0.8</span>)</div><div class="line">train.loc[:, mask]</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>ID</th><br><th>VAR_0212</th><br><th>VAR_0227</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>2</td><br><td>NaN</td><br><td>311951</td><br></tr><br><tr><br><th>1</th><br><td>4</td><br><td>9.20713e+10</td><br><td>2.76949e+06</td><br></tr><br><tr><br><th>2</th><br><td>5</td><br><td>2.65477e+10</td><br><td>654127</td><br></tr><br><tr><br><th>3</th><br><td>7</td><br><td>7.75753e+10</td><br><td>3.01509e+06</td><br></tr><br><tr><br><th>4</th><br><td>8</td><br><td>6.04238e+10</td><br><td>118678</td><br></tr><br><tr><br><th>5</th><br><td>14</td><br><td>7.73796e+10</td><br><td>1.76557e+06</td><br></tr><br><tr><br><th>6</th><br><td>16</td><br><td>9.70303e+10</td><br><td>80151</td><br></tr><br><tr><br><th>7</th><br><td>20</td><br><td>3.10981e+10</td><br><td>853641</td><br></tr><br><tr><br><th>8</th><br><td>21</td><br><td>7.82124e+10</td><br><td>1.40254e+06</td><br></tr><br><tr><br><th>9</th><br><td>22</td><br><td>1.94014e+10</td><br><td>2.2187e+06</td><br></tr><br><tr><br><th>10</th><br><td>23</td><br><td>3.71295e+10</td><br><td>2.77679e+06</td><br></tr><br><tr><br><th>11</th><br><td>24</td><br><td>3.01203e+10</td><br><td>434300</td><br></tr><br><tr><br><th>12</th><br><td>25</td><br><td>1.80185e+10</td><br><td>1.48914e+06</td><br></tr><br><tr><br><th>13</th><br><td>26</td><br><td>9.83358e+10</td><br><td>686666</td><br></tr><br><tr><br><th>14</th><br><td>28</td><br><td>9.33087e+10</td><br><td>1.4847e+06</td><br></tr><br><tr><br><th>15</th><br><td>30</td><br><td>2.01715e+10</td><br><td>883714</td><br></tr><br><tr><br><th>16</th><br><td>31</td><br><td>4.15638e+10</td><br><td>2.6707e+06</td><br></tr><br><tr><br><th>17</th><br><td>32</td><br><td>9.17617e+10</td><br><td>2.65485e+06</td><br></tr><br><tr><br><th>18</th><br><td>35</td><br><td>3.81344e+10</td><br><td>487721</td><br></tr><br><tr><br><th>19</th><br><td>36</td><br><td>NaN</td><br><td>2.54705e+06</td><br></tr><br><tr><br><th>20</th><br><td>37</td><br><td>3.27144e+10</td><br><td>1.74684e+06</td><br></tr><br><tr><br><th>21</th><br><td>38</td><br><td>1.82142e+10</td><br><td>2.5813e+06</td><br></tr><br><tr><br><th>22</th><br><td>40</td><br><td>7.70153e+10</td><br><td>2.59396e+06</td><br></tr><br><tr><br><th>23</th><br><td>42</td><br><td>4.69701e+10</td><br><td>1.02977e+06</td><br></tr><br><tr><br><th>24</th><br><td>43</td><br><td>9.84442e+10</td><br><td>1.45101e+06</td><br></tr><br><tr><br><th>25</th><br><td>46</td><br><td>NaN</td><br><td>2.37136e+06</td><br></tr><br><tr><br><th>26</th><br><td>50</td><br><td>9.25094e+10</td><br><td>665930</td><br></tr><br><tr><br><th>27</th><br><td>51</td><br><td>3.09094e+10</td><br><td>497686</td><br></tr><br><tr><br><th>28</th><br><td>52</td><br><td>6.06105e+10</td><br><td>1.95816e+06</td><br></tr><br><tr><br><th>29</th><br><td>54</td><br><td>3.78768e+10</td><br><td>1.62591e+06</td><br></tr><br><tr><br><th>…</th><br><td>…</td><br><td>…</td><br><td>…</td><br></tr><br><tr><br><th>145201</th><br><td>290409</td><br><td>8.80126e+10</td><br><td>1.83053e+06</td><br></tr><br><tr><br><th>145202</th><br><td>290412</td><br><td>4.6152e+10</td><br><td>1.02024e+06</td><br></tr><br><tr><br><th>145203</th><br><td>290414</td><br><td>9.33055e+10</td><br><td>1.88151e+06</td><br></tr><br><tr><br><th>145204</th><br><td>290415</td><br><td>4.63509e+10</td><br><td>669351</td><br></tr><br><tr><br><th>145205</th><br><td>290417</td><br><td>2.36028e+10</td><br><td>655797</td><br></tr><br><tr><br><th>145206</th><br><td>290424</td><br><td>3.73293e+10</td><br><td>1.45626e+06</td><br></tr><br><tr><br><th>145207</th><br><td>290426</td><br><td>2.38892e+10</td><br><td>1.9503e+06</td><br></tr><br><tr><br><th>145208</th><br><td>290427</td><br><td>6.38632e+10</td><br><td>596365</td><br></tr><br><tr><br><th>145209</th><br><td>290429</td><br><td>3.00602e+10</td><br><td>572119</td><br></tr><br><tr><br><th>145210</th><br><td>290431</td><br><td>4.33429e+10</td><br><td>16120</td><br></tr><br><tr><br><th>145211</th><br><td>290432</td><br><td>3.86543e+10</td><br><td>2.08375e+06</td><br></tr><br><tr><br><th>145212</th><br><td>290434</td><br><td>9.21391e+10</td><br><td>1.89779e+06</td><br></tr><br><tr><br><th>145213</th><br><td>290436</td><br><td>3.07472e+10</td><br><td>2.94532e+06</td><br></tr><br><tr><br><th>145214</th><br><td>290439</td><br><td>7.83326e+10</td><br><td>2.54726e+06</td><br></tr><br><tr><br><th>145215</th><br><td>290440</td><br><td>NaN</td><br><td>600318</td><br></tr><br><tr><br><th>145216</th><br><td>290441</td><br><td>2.78561e+10</td><br><td>602505</td><br></tr><br><tr><br><th>145217</th><br><td>290443</td><br><td>1.90952e+10</td><br><td>2.44184e+06</td><br></tr><br><tr><br><th>145218</th><br><td>290445</td><br><td>4.62035e+10</td><br><td>2.87349e+06</td><br></tr><br><tr><br><th>145219</th><br><td>290447</td><br><td>NaN</td><br><td>1.53493e+06</td><br></tr><br><tr><br><th>145220</th><br><td>290448</td><br><td>7.54282e+10</td><br><td>1.60102e+06</td><br></tr><br><tr><br><th>145221</th><br><td>290449</td><br><td>4.30768e+10</td><br><td>2.08415e+06</td><br></tr><br><tr><br><th>145222</th><br><td>290450</td><br><td>7.81325e+10</td><br><td>2.85367e+06</td><br></tr><br><tr><br><th>145223</th><br><td>290452</td><br><td>4.51061e+10</td><br><td>1.56506e+06</td><br></tr><br><tr><br><th>145224</th><br><td>290453</td><br><td>4.62223e+10</td><br><td>1.46815e+06</td><br></tr><br><tr><br><th>145225</th><br><td>290454</td><br><td>7.74507e+10</td><br><td>2.92811e+06</td><br></tr><br><tr><br><th>145226</th><br><td>290457</td><br><td>7.05088e+10</td><br><td>2.03657e+06</td><br></tr><br><tr><br><th>145227</th><br><td>290458</td><br><td>9.02492e+10</td><br><td>1.68013e+06</td><br></tr><br><tr><br><th>145228</th><br><td>290459</td><br><td>9.17224e+10</td><br><td>2.41922e+06</td><br></tr><br><tr><br><th>145229</th><br><td>290461</td><br><td>4.51033e+10</td><br><td>1.53960e+06</td><br></tr><br><tr><br><th>145230</th><br><td>290463</td><br><td>9.14114e+10</td><br><td>2.6609e+06</td><br></tr><br></tbody><br></table><br><p>145231 rows × 3 columns</p><br></div><p>The values are not float, they are integer, so these features are likely to be even counts. Let’s look at another pack of features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mask = (nunique.astype(float)/train.shape[<span class="number">0</span>] &lt; <span class="number">0.8</span>) &amp; (nunique.astype(float)/train.shape[<span class="number">0</span>] &gt; <span class="number">0.4</span>)</div><div class="line">train.loc[:<span class="number">25</span>, mask]</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>VAR_0541</th><br><th>VAR_0543</th><br><th>VAR_0899</th><br><th>VAR_1081</th><br><th>VAR_1082</th><br><th>VAR_1087</th><br><th>VAR_1179</th><br><th>VAR_1180</th><br><th>VAR_1181</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>49463</td><br><td>116783</td><br><td>112871</td><br><td>76857</td><br><td>76857</td><br><td>116783</td><br><td>76857</td><br><td>76857</td><br><td>76857</td><br></tr><br><tr><br><th>1</th><br><td>303472</td><br><td>346196</td><br><td>346375</td><br><td>341365</td><br><td>341365</td><br><td>346196</td><br><td>341365</td><br><td>341365</td><br><td>176604</td><br></tr><br><tr><br><th>2</th><br><td>94990</td><br><td>122601</td><br><td>121501</td><br><td>107267</td><br><td>107267</td><br><td>121501</td><br><td>107267</td><br><td>107267</td><br><td>58714</td><br></tr><br><tr><br><th>3</th><br><td>20593</td><br><td>59490</td><br><td>61890</td><br><td>45794</td><br><td>47568</td><br><td>59490</td><br><td>45794</td><br><td>47568</td><br><td>47568</td><br></tr><br><tr><br><th>4</th><br><td>10071</td><br><td>35708</td><br><td>34787</td><br><td>20475</td><br><td>23647</td><br><td>34708</td><br><td>20475</td><br><td>23647</td><br><td>23647</td><br></tr><br><tr><br><th>5</th><br><td>18877</td><br><td>28055</td><br><td>28455</td><br><td>21139</td><br><td>21139</td><br><td>28055</td><br><td>21139</td><br><td>21139</td><br><td>20627</td><br></tr><br><tr><br><th>6</th><br><td>321783</td><br><td>333565</td><br><td>886886</td><br><td>327744</td><br><td>327744</td><br><td>333565</td><br><td>327744</td><br><td>327744</td><br><td>163944</td><br></tr><br><tr><br><th>7</th><br><td>2961</td><br><td>5181</td><br><td>11084</td><br><td>4326</td><br><td>4326</td><br><td>5181</td><br><td>4326</td><br><td>4326</td><br><td>4326</td><br></tr><br><tr><br><th>8</th><br><td>20359</td><br><td>30114</td><br><td>33434</td><br><td>24969</td><br><td>27128</td><br><td>30114</td><br><td>24969</td><br><td>27128</td><br><td>27128</td><br></tr><br><tr><br><th>9</th><br><td>815</td><br><td>1300</td><br><td>7677</td><br><td>1197</td><br><td>1197</td><br><td>1300</td><br><td>1197</td><br><td>1197</td><br><td>1197</td><br></tr><br><tr><br><th>10</th><br><td>6088</td><br><td>15233</td><br><td>15483</td><br><td>7077</td><br><td>7077</td><br><td>15233</td><br><td>7077</td><br><td>7077</td><br><td>4033</td><br></tr><br><tr><br><th>11</th><br><td>432</td><br><td>1457</td><br><td>2000</td><br><td>621</td><br><td>621</td><br><td>757</td><br><td>621</td><br><td>621</td><br><td>621</td><br></tr><br><tr><br><th>12</th><br><td>383</td><br><td>539</td><br><td>860</td><br><td>752</td><br><td>1158</td><br><td>539</td><br><td>752</td><br><td>1158</td><br><td>1158</td><br></tr><br><tr><br><th>13</th><br><td>14359</td><br><td>47562</td><br><td>47562</td><br><td>17706</td><br><td>17706</td><br><td>47562</td><br><td>17706</td><br><td>17706</td><br><td>17706</td><br></tr><br><tr><br><th>14</th><br><td>145391</td><br><td>218067</td><br><td>214836</td><br><td>176627</td><br><td>176627</td><br><td>216307</td><br><td>175273</td><br><td>175273</td><br><td>91019</td><br></tr><br><tr><br><th>15</th><br><td>10040</td><br><td>12119</td><br><td>17263</td><br><td>10399</td><br><td>10399</td><br><td>12119</td><br><td>10399</td><br><td>10399</td><br><td>5379</td><br></tr><br><tr><br><th>16</th><br><td>4880</td><br><td>9607</td><br><td>9607</td><br><td>9165</td><br><td>9165</td><br><td>9607</td><br><td>9165</td><br><td>9165</td><br><td>9165</td><br></tr><br><tr><br><th>17</th><br><td>12900</td><br><td>35590</td><br><td>35781</td><br><td>26096</td><br><td>26096</td><br><td>35590</td><br><td>26096</td><br><td>26096</td><br><td>19646</td><br></tr><br><tr><br><th>18</th><br><td>104442</td><br><td>139605</td><br><td>150505</td><br><td>136419</td><br><td>142218</td><br><td>139605</td><br><td>136419</td><br><td>142218</td><br><td>142218</td><br></tr><br><tr><br><th>19</th><br><td>13898</td><br><td>25566</td><br><td>26685</td><br><td>20122</td><br><td>20122</td><br><td>25566</td><br><td>20122</td><br><td>20122</td><br><td>20122</td><br></tr><br><tr><br><th>20</th><br><td>3524</td><br><td>10033</td><br><td>10133</td><br><td>5838</td><br><td>5838</td><br><td>10033</td><br><td>5838</td><br><td>5838</td><br><td>5838</td><br></tr><br><tr><br><th>21</th><br><td>129873</td><br><td>204072</td><br><td>206946</td><br><td>183049</td><br><td>183049</td><br><td>204072</td><br><td>183049</td><br><td>183049</td><br><td>96736</td><br></tr><br><tr><br><th>22</th><br><td>3591</td><br><td>11400</td><br><td>17680</td><br><td>5565</td><br><td>5565</td><br><td>11400</td><br><td>5565</td><br><td>5565</td><br><td>5565</td><br></tr><br><tr><br><th>23</th><br><td>999999999</td><br><td>999999999</td><br><td>-99999</td><br><td>999999999</td><br><td>999999999</td><br><td>999999999</td><br><td>999999999</td><br><td>999999999</td><br><td>999999999</td><br></tr><br><tr><br><th>24</th><br><td>1270</td><br><td>4955</td><br><td>12201</td><br><td>2490</td><br><td>2490</td><br><td>4955</td><br><td>2490</td><br><td>2490</td><br><td>2490</td><br></tr><br><tr><br><th>25</th><br><td>2015</td><br><td>2458</td><br><td>2458</td><br><td>2015</td><br><td>2015</td><br><td>2458</td><br><td>2015</td><br><td>2015</td><br><td>1008</td><br></tr><br></tbody><br></table><br></div><p>These look like counts too. First thing to notice is the 23th line: 99999.., -99999 values look like NaNs so we should probably built a related feature. Second: the columns are sometimes placed next to each other, so the columns are probably grouped together and we can disentangle that.</p><p>Our conclusion: there are no floating point variables, there are some counts variables, which we will treat as numeric.</p><p>And finally, let’s pick one variable (in this case ‘VAR_0015’) from the third group of features.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train[<span class="string">'VAR_0015'</span>].value_counts()</div></pre></td></tr></table></figure><pre><code> 0.0      102382
 1.0       28045
 2.0        8981
 3.0        3199
 4.0        1274
 5.0         588
 6.0         275
 7.0         166
 8.0          97
-999.0        56
 9.0          51
 10.0         39
 11.0         18
 12.0         16
 13.0          9
 14.0          8
 15.0          8
 16.0          6
 22.0          3
 21.0          3
 19.0          1
 35.0          1
 17.0          1
 29.0          1
 18.0          1
 32.0          1
 23.0          1
Name: VAR_0015, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cat_cols = list(train.select_dtypes(include=[<span class="string">'object'</span>]).columns)</div><div class="line">num_cols = list(train.select_dtypes(exclude=[<span class="string">'object'</span>]).columns)</div></pre></td></tr></table></figure><h1 id="Go-through"><a href="#Go-through" class="headerlink" title="Go through"></a>Go through</h1><p>Let’s replace NaNs with something first.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.replace(<span class="string">'NaN'</span>, <span class="number">-999</span>, inplace=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>Let’s calculate how many times one feature is greater than the other and create cross tabel out of it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># select first 42 numeric features</span></div><div class="line">feats = num_cols[:<span class="number">42</span>]</div><div class="line"></div><div class="line"><span class="comment"># build 'mean(feat1 &gt; feat2)' plot</span></div><div class="line">gt_matrix(feats,<span class="number">16</span>)</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_57_0.png" alt="png"></p><p>Indeed, we see interesting patterns here. There are blocks of geatures where one is strictly greater than the other. So we can hypothesize, that each column correspondes to cumulative counts, e.g. feature number one is counts in first month, second – total count number in first two month and so on. So we immediately understand what features we should generate to make tree-based models more efficient: the differences between consecutive values.</p><h2 id="VAR-0002-VAR-0003"><a href="#VAR-0002-VAR-0003" class="headerlink" title="VAR_0002, VAR_0003"></a>VAR_0002, VAR_0003</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hist_it(train[<span class="string">'VAR_0002'</span>])</div><div class="line">plt.ylim((<span class="number">0</span>,<span class="number">0.05</span>))</div><div class="line">plt.xlim((<span class="number">-10</span>,<span class="number">1010</span>))</div><div class="line"></div><div class="line">hist_it(train[<span class="string">'VAR_0003'</span>])</div><div class="line">plt.ylim((<span class="number">0</span>,<span class="number">0.03</span>))</div><div class="line">plt.xlim((<span class="number">-10</span>,<span class="number">1010</span>))</div></pre></td></tr></table></figure><pre><code>(-10, 1010)
</code></pre><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_60_1.png" alt="png"></p><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_60_2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train[<span class="string">'VAR_0002'</span>].value_counts()</div></pre></td></tr></table></figure><pre><code>12     5264
24     4763
36     3499
60     2899
6      2657
13     2478
72     2243
48     2222
3      2171
4      1917
2      1835
84     1801
120    1786
1      1724
7      1671
26     1637
5      1624
14     1572
18     1555
8      1513
999    1510
25     1504
96     1445
30     1438
9      1306
144    1283
15     1221
27     1186
38     1146
37     1078
       ... 
877       1
785       1
750       1
653       1
784       1
764       1
751       1
797       1
926       1
691       1
808       1
774       1
902       1
755       1
656       1
814       1
813       1
685       1
739       1
935       1
906       1
807       1
550       1
933       1
804       1
675       1
674       1
745       1
778       1
851       1
Name: VAR_0002, Length: 820, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train[<span class="string">'VAR_0003'</span>].value_counts()</div></pre></td></tr></table></figure><pre><code>0      17436
24      3469
12      3271
60      3054
36      2498
72      2081
48      2048
6       1993
1       1797
3       1679
84      1553
2       1459
999     1428
4       1419
120     1411
7       1356
13      1297
18      1296
96      1253
14      1228
8       1216
5       1189
9       1182
30      1100
25      1100
144     1090
15      1047
61      1008
26       929
42       921
       ...  
560        1
552        1
550        1
804        1
543        1
668        1
794        1
537        1
531        1
664        1
632        1
709        1
597        1
965        1
852        1
648        1
596        1
466        1
592        1
521        1
533        1
636        1
975        1
973        1
587        1
523        1
584        1
759        1
583        1
570        1
Name: VAR_0003, Length: 588, dtype: int64
</code></pre><p>We see there is something special about 12, 24 and so on, sowe can create another feature x mod 12.</p><h2 id="VAR-0004"><a href="#VAR-0004" class="headerlink" title="VAR_0004"></a>VAR_0004</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">train[<span class="string">'VAR_0004_mod50'</span>] = train[<span class="string">'VAR_0004'</span>] % <span class="number">50</span></div><div class="line">hist_it(train[<span class="string">'VAR_0004_mod50'</span>])</div><div class="line">plt.ylim((<span class="number">0</span>,<span class="number">0.6</span>))</div></pre></td></tr></table></figure><pre><code>(0, 0.6)
</code></pre><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_65_1.png" alt="png"></p><h1 id="Categorical-features"><a href="#Categorical-features" class="headerlink" title="Categorical features"></a>Categorical features</h1><p>Let’s take a look at categorical features we have.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.loc[:,cat_cols].head().T</div></pre></td></tr></table></figure><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>0</th><br><th>1</th><br><th>2</th><br><th>3</th><br><th>4</th><br></tr><br></thead><br><tbody><br><tr><br><th>VAR_0001</th><br><td>H</td><br><td>H</td><br><td>H</td><br><td>H</td><br><td>R</td><br></tr><br><tr><br><th>VAR_0005</th><br><td>C</td><br><td>B</td><br><td>C</td><br><td>C</td><br><td>N</td><br></tr><br><tr><br><th>VAR_0008</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0009</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0010</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0011</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0012</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0043</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0044</th><br><td>[]</td><br><td>[]</td><br><td>[]</td><br><td>[]</td><br><td>[]</td><br></tr><br><tr><br><th>VAR_0073</th><br><td>NaT</td><br><td>2012-09-04 00:00:00</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0075</th><br><td>2011-11-08 00:00:00</td><br><td>2011-11-10 00:00:00</td><br><td>2011-12-13 00:00:00</td><br><td>2010-09-23 00:00:00</td><br><td>2011-10-15 00:00:00</td><br></tr><br><tr><br><th>VAR_0156</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0157</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0158</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0159</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0166</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0167</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0168</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0169</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0176</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0177</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0178</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0179</th><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br><td>NaT</td><br></tr><br><tr><br><th>VAR_0196</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0200</th><br><td>FT LAUDERDALE</td><br><td>SANTEE</td><br><td>REEDSVILLE</td><br><td>LIBERTY</td><br><td>FRANKFORT</td><br></tr><br><tr><br><th>VAR_0202</th><br><td>BatchInquiry</td><br><td>BatchInquiry</td><br><td>BatchInquiry</td><br><td>BatchInquiry</td><br><td>BatchInquiry</td><br></tr><br><tr><br><th>VAR_0204</th><br><td>2014-01-29 21:16:00</td><br><td>2014-02-01 00:11:00</td><br><td>2014-01-30 15:11:00</td><br><td>2014-02-01 00:07:00</td><br><td>2014-01-29 19:31:00</td><br></tr><br><tr><br><th>VAR_0214</th><br><td>NaN</td><br><td>NaN</td><br><td>NaN</td><br><td>NaN</td><br><td>NaN</td><br></tr><br><tr><br><th>VAR_0216</th><br><td>DS</td><br><td>DS</td><br><td>DS</td><br><td>DS</td><br><td>DS</td><br></tr><br><tr><br><th>VAR_0217</th><br><td>2011-11-08 02:00:00</td><br><td>2012-10-02 02:00:00</td><br><td>2011-12-13 02:00:00</td><br><td>2012-11-01 02:00:00</td><br><td>2011-10-15 02:00:00</td><br></tr><br><tr><br><th>VAR_0222</th><br><td>C6</td><br><td>C6</td><br><td>C6</td><br><td>C6</td><br><td>C6</td><br></tr><br><tr><br><th>VAR_0226</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0229</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0230</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0232</th><br><td>True</td><br><td>False</td><br><td>True</td><br><td>False</td><br><td>True</td><br></tr><br><tr><br><th>VAR_0236</th><br><td>True</td><br><td>True</td><br><td>True</td><br><td>True</td><br><td>True</td><br></tr><br><tr><br><th>VAR_0237</th><br><td>FL</td><br><td>CA</td><br><td>WV</td><br><td>TX</td><br><td>IL</td><br></tr><br><tr><br><th>VAR_0239</th><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br><td>False</td><br></tr><br><tr><br><th>VAR_0274</th><br><td>FL</td><br><td>MI</td><br><td>WV</td><br><td>TX</td><br><td>IL</td><br></tr><br><tr><br><th>VAR_0283</th><br><td>S</td><br><td>S</td><br><td>S</td><br><td>S</td><br><td>S</td><br></tr><br><tr><br><th>VAR_0305</th><br><td>S</td><br><td>S</td><br><td>P</td><br><td>P</td><br><td>P</td><br></tr><br><tr><br><th>VAR_0325</th><br><td>-1</td><br><td>H</td><br><td>R</td><br><td>H</td><br><td>S</td><br></tr><br><tr><br><th>VAR_0342</th><br><td>CF</td><br><td>EC</td><br><td>UU</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_0352</th><br><td>O</td><br><td>O</td><br><td>R</td><br><td>R</td><br><td>R</td><br></tr><br><tr><br><th>VAR_0353</th><br><td>U</td><br><td>R</td><br><td>R</td><br><td>R</td><br><td>U</td><br></tr><br><tr><br><th>VAR_0354</th><br><td>O</td><br><td>R</td><br><td>-1</td><br><td>-1</td><br><td>O</td><br></tr><br><tr><br><th>VAR_0404</th><br><td>CHIEF EXECUTIVE OFFICER</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_0466</th><br><td>-1</td><br><td>I</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_0467</th><br><td>-1</td><br><td>Discharged</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_0493</th><br><td>COMMUNITY ASSOCIATION MANAGER</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br><td>-1</td><br></tr><br><tr><br><th>VAR_1934</th><br><td>IAPS</td><br><td>IAPS</td><br><td>IAPS</td><br><td>RCC</td><br><td>BRANCH</td><br></tr><br></tbody><br></table><br></div><p><code>VAR_0200</code>, <code>VAR_0237</code>, <code>VAR_0274</code> look like some georgraphical data thus one could generate geography related features, we will talk later in the course.</p><p>There are some features, that are hard to identify, but look, there a date columns <code>VAR_0073</code> – <code>VAR_0179</code>, <code>VAR_0204</code>, <code>VAR_0217</code>. It is useful to plot one date against another to find relationships.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">date_cols = [<span class="string">u'VAR_0073'</span>,<span class="string">'VAR_0075'</span>,</div><div class="line">             <span class="string">u'VAR_0156'</span>,<span class="string">u'VAR_0157'</span>,<span class="string">u'VAR_0158'</span>,<span class="string">'VAR_0159'</span>,</div><div class="line">             <span class="string">u'VAR_0166'</span>, <span class="string">u'VAR_0167'</span>,<span class="string">u'VAR_0168'</span>,<span class="string">u'VAR_0169'</span>,</div><div class="line">             <span class="string">u'VAR_0176'</span>,<span class="string">u'VAR_0177'</span>,<span class="string">u'VAR_0178'</span>,<span class="string">u'VAR_0179'</span>,</div><div class="line">             <span class="string">u'VAR_0204'</span>,</div><div class="line">             <span class="string">u'VAR_0217'</span>]</div><div class="line"></div><div class="line"><span class="keyword">for</span> c <span class="keyword">in</span> date_cols:</div><div class="line">    train[c] = pd.to_datetime(train[c],format = <span class="string">'%d%b%y:%H:%M:%S'</span>)</div><div class="line">    test[c] = pd.to_datetime(test[c],  format = <span class="string">'%d%b%y:%H:%M:%S'</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">c1 = <span class="string">'VAR_0217'</span></div><div class="line">c2 = <span class="string">'VAR_0073'</span></div><div class="line"></div><div class="line"><span class="comment"># mask = (~test[c1].isnull()) &amp; (~test[c2].isnull())</span></div><div class="line"><span class="comment"># sc2(test.ix[mask,c1].values,test.ix[mask,c2].values,alpha=0.7,c = 'black')</span></div><div class="line"></div><div class="line">mask = (~train[c1].isnull()) &amp; (~train[c2].isnull())</div><div class="line">sc2(train.loc[mask,c1].values,train.loc[mask,c2].values,c=train.loc[mask,<span class="string">'target'</span>].values)</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_71_0.png" alt="png"></p><p>We see that one date is strictly greater than the other, so the difference between them can be a good feature. Also look at horizontal line there – it also looks like NaN, so I would rather create a new binary feature which will serve as an idicator that our time feature is NaN.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is a notebook, used in the screencast video. Note, that the data files are not present here in Jupyter hub and you will not be able to run it. But you can always download the notebook to your local machine as well as the competition data and make it interactive.&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; os&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; tqdm &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; tqdm_notebook&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;%matplotlib inline&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; warnings&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;warnings.filterwarnings(&lt;span class=&quot;string&quot;&gt;&#39;ignore&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; seaborn&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;19&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;20&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;21&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;22&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;23&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;24&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;25&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;26&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;27&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;28&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;29&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;30&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;31&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;32&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;33&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;autolabel&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(arrayA)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&#39;&#39;&#39; label each colored square with the corresponding data value. &lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    If value &amp;gt; 20, the text is in black, else in white.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &#39;&#39;&#39;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    arrayA = np.array(arrayA)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(arrayA.shape[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; j &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(arrayA.shape[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;                plt.text(j,i, &lt;span class=&quot;string&quot;&gt;&quot;%.2f&quot;&lt;/span&gt;%arrayA[i,j], ha=&lt;span class=&quot;string&quot;&gt;&#39;center&#39;&lt;/span&gt;, va=&lt;span class=&quot;string&quot;&gt;&#39;bottom&#39;&lt;/span&gt;,color=&lt;span class=&quot;string&quot;&gt;&#39;w&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;hist_it&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(feat)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.figure(figsize=(&lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    feat[Y==&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;].hist(bins=range(int(feat.min()),int(feat.max()+&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)),normed=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,alpha=&lt;span class=&quot;number&quot;&gt;0.8&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    feat[Y==&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;].hist(bins=range(int(feat.min()),int(feat.max()+&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)),normed=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,alpha=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.ylim((&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;gt_matrix&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(feats,sz=&lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    a = []&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i,c1 &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; enumerate(feats):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        b = [] &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; j,c2 &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; enumerate(feats):&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            mask = (~train[c1].isnull()) &amp;amp; (~train[c2].isnull())&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; i&amp;gt;=j:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;                b.append((train.loc[mask,c1].values&amp;gt;=train.loc[mask,c2].values).mean())&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;                b.append((train.loc[mask,c1].values&amp;gt;train.loc[mask,c2].values).mean())&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        a.append(b)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.figure(figsize = (sz,sz))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.imshow(a, interpolation = &lt;span class=&quot;string&quot;&gt;&#39;None&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    _ = plt.xticks(range(len(feats)),feats,rotation = &lt;span class=&quot;number&quot;&gt;90&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    _ = plt.yticks(range(len(feats)),feats,rotation = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    autolabel(a)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;hist_it1&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(feat)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.figure(figsize=(&lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    feat[Y==&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;].hist(bins=&lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;,range=(feat.min(),feat.max()),normed=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,alpha=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    feat[Y==&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;].hist(bins=&lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;,range=(feat.min(),feat.max()),normed=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;,alpha=&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    plt.ylim((&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;))&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&quot;Read-the-data&quot;&gt;&lt;a href=&quot;#Read-the-data&quot; class=&quot;headerlink&quot; title=&quot;Read the data&quot;&gt;&lt;/a&gt;Read the data&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;train = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;train.csv.zip&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;Y = train.target&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;test = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;test.csv.zip&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;test_ID = test.ID&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>EDA check list</title>
    <link href="http://yoursite.com/2018/10/16/EDA-check-list/"/>
    <id>http://yoursite.com/2018/10/16/EDA-check-list/</id>
    <published>2018-10-16T08:05:28.000Z</published>
    <updated>2018-10-16T08:06:51.636Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Get domain knowledge</li><li>Check if the data is intuitive (abnormal detection)<ul><li>add a feature <code>is_incorrect</code></li></ul></li><li><a href="https://tomaxent.com/2018/10/16/Exploratory-Data-Analysis/" target="_blank" rel="external">Understand how the data was generated</a><ul><li>It is crucial to understand the generation process to set up a proper validation scheme</li></ul></li><li>Two things to do with anonymized features<ul><li>Try to decode the features<ul><li><a href="https://tomaxent.com/2018/10/16/Processing-anonymized-features/" target="_blank" rel="external">Guess the true meaning of the feature</a></li></ul></li><li>Guess the feature types<ul><li>Each type need its own preprocessing</li></ul></li></ul></li><li>Visualization<ul><li>Tools for individual features exploration<ul><li>Histograms <code>plt.hist(x)</code></li><li>Plot (index versus value) <code>plt.plot(x, something)</code></li><li>Statistics <code>df.describe() or x.mean() or x.var()</code></li><li>Other tools <code>x.value_counts() or x.isnull()</code></li></ul></li><li>Tools for feature relationships<ul><li>Pairs<ul><li><code>plt.scatter(x1, x2)</code></li><li><code>pd.scatter_matrix(df)</code></li><li><code>df.corr() or plt.matshow()</code></li></ul></li><li>Groups:<ul><li>Clustering</li><li>Plot (index vs feature statistics) <code>df.mean().sort_values().plot()</code></li></ul></li></ul></li></ul></li><li>Data Clean<ul><li>remove duplicated and constant features<ul><li><code>traintest.nunique(axis=1) == 1</code></li><li><code>traintest.T.drop_duplicates()</code></li><li><code>for f in categorical_feats: traintest[f] = traintest[f].factorize then traintest.T.drop_duplicates()</code></li></ul></li><li>check if same rows have same label</li><li>check if dataset is shuffled</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;&lt;li&gt;Get domain knowledge&lt;/li&gt;&lt;li&gt;Check if the data is intuitive (abnormal detection)&lt;ul&gt;&lt;li&gt;add a feature &lt;code&gt;is_incorrect&lt;/code&gt;&lt;/li&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Processing Anonymized Features</title>
    <link href="http://yoursite.com/2018/10/16/Processing-anonymized-features/"/>
    <id>http://yoursite.com/2018/10/16/Processing-anonymized-features/</id>
    <published>2018-10-16T07:22:45.000Z</published>
    <updated>2018-10-16T07:23:19.410Z</updated>
    
    <content type="html"><![CDATA[<p><strong>IMPORTANT:</strong> You will not be able to run this notebook at coursera platform, as the dataset is not there. The notebook is in read-only mode.</p><p>But you can run the notebook locally and download the dataset using <a href="https://habrastorage.org/storage/stuff/special/beeline/00.beeline_bigdata.zip" target="_blank" rel="external">this link</a> to explore the data interactively.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pd.set_option(<span class="string">'max_columns'</span>, <span class="number">100</span>)</div></pre></td></tr></table></figure><h1 id="Load-the-data"><a href="#Load-the-data" class="headerlink" title="Load the data"></a>Load the data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train = pd.read_csv(<span class="string">'./train.csv'</span>)</div><div class="line">train.head()</div></pre></td></tr></table></figure><a id="more"></a><div><br><style><br>.dataframe thead tr:only-child th{<br>        text-align:right}<br><br>.dataframe thead th{<br>        text-align:left}<br><br>.dataframe tbody tr th{<br>        vertical-align:top}<br></style><br><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>x0</th><br><th>x1</th><br><th>x2</th><br><th>x3</th><br><th>x4</th><br><th>x5</th><br><th>x6</th><br><th>x7</th><br><th>x8</th><br><th>x9</th><br><th>x10</th><br><th>x11</th><br><th>x12</th><br><th>x13</th><br><th>x14</th><br><th>x15</th><br><th>x16</th><br><th>x17</th><br><th>x18</th><br><th>x19</th><br><th>x20</th><br><th>x21</th><br><th>x22</th><br><th>x23</th><br><th>x24</th><br><th>x25</th><br><th>x26</th><br><th>x27</th><br><th>x28</th><br><th>x29</th><br><th>x30</th><br><th>x31</th><br><th>x32</th><br><th>x33</th><br><th>x34</th><br><th>x35</th><br><th>x36</th><br><th>x37</th><br><th>x38</th><br><th>x39</th><br><th>x40</th><br><th>x41</th><br><th>x42</th><br><th>x43</th><br><th>x44</th><br><th>x45</th><br><th>x46</th><br><th>x47</th><br><th>x48</th><br><th>x49</th><br><th>x50</th><br><th>x51</th><br><th>x52</th><br><th>x53</th><br><th>x54</th><br><th>x55</th><br><th>x56</th><br><th>x57</th><br><th>x58</th><br><th>x59</th><br><th>x60</th><br><th>x61</th><br><th>y</th><br></tr><br></thead><br><tbody><br><tr><br><th>0</th><br><td>b4d8a653ea</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>a62168d626</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>-0.688706</td><br><td>7e5c97705a</td><br><td>e5df3eff9b</td><br><td>91bb549494</td><br><td>e33c63cf35</td><br><td>3694.0</td><br><td>6e40247e69</td><br><td>617a4ad3f9</td><br><td>718c61545b</td><br><td>c26d08129a</td><br><td>634e3cf3ac</td><br><td>dd9c9e0da2</td><br><td>17c99905b6</td><br><td>513a3e3f36</td><br><td>9aba4d7f51</td><br><td>40.579612</td><br><td>-0.112693</td><br><td>-0.172191</td><br><td>1.166667</td><br><td>1.674538</td><br><td>0.630889</td><br><td>37.000000</td><br><td>1.294922</td><br><td>55.0</td><br><td>0.166667</td><br><td>10.0</td><br><td>0.0</td><br><td>0.000000</td><br><td>1.0</td><br><td>9.0</td><br><td>0.0</td><br><td>1.0</td><br><td>23.0</td><br><td>3.67</td><br><td>0.12</td><br><td>1.935</td><br><td>2.2</td><br><td>0.625</td><br><td>0.250</td><br><td>0.125</td><br><td>0.000</td><br><td>0.813</td><br><td>0.074</td><br><td>0.634</td><br><td>0.548</td><br><td>0.235333</td><br><td>0.264952</td><br><td>0.000000</td><br><td>0.333333</td><br><td>0.333333</td><br><td>0.333333</td><br><td>0.000000</td><br><td>0.000000</td><br><td>9.0</td><br><td>2</td><br></tr><br><tr><br><th>1</th><br><td>467f9617a3</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>b7584c2d52</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>0.870871</td><br><td>5624b8f759</td><br><td>fa0b797a92</td><br><td>669ea3d319</td><br><td>f178803074</td><br><td>18156.0</td><br><td>01ede04b4b</td><br><td>617a4ad3f9</td><br><td>718c61545b</td><br><td>d342e2765f</td><br><td>bb20e1ca06</td><br><td>8a6c8cef83</td><br><td>1b02793146</td><br><td>992153ed65</td><br><td>9aba4d7f51</td><br><td>28.765503</td><br><td>2.612285</td><br><td>2.159091</td><br><td>4.000000</td><br><td>1.710714</td><br><td>1.713538</td><br><td>0.166667</td><br><td>0.027669</td><br><td>109.0</td><br><td>0.000000</td><br><td>31.0</td><br><td>0.0</td><br><td>0.000000</td><br><td>1.0</td><br><td>244.0</td><br><td>1.0</td><br><td>1.0</td><br><td>68.0</td><br><td>17.25</td><br><td>0.57</td><br><td>3.452</td><br><td>4.0</td><br><td>0.409</td><br><td>0.619</td><br><td>0.579</td><br><td>0.248</td><br><td>0.346</td><br><td>0.541</td><br><td>0.522</td><br><td>0.000</td><br><td>1.782346</td><br><td>1.322409</td><br><td>0.011647</td><br><td>0.397671</td><br><td>0.239601</td><br><td>0.249584</td><br><td>0.068220</td><br><td>0.033278</td><br><td>601.0</td><br><td>4</td><br></tr><br><tr><br><th>2</th><br><td>190436e528</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>b7584c2d52</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>0.437655</td><br><td>5624b8f759</td><br><td>152af2cb2f</td><br><td>91bb549494</td><br><td>e33c63cf35</td><br><td>1178.0</td><br><td>cc69cbe29a</td><br><td>617a4ad3f9</td><br><td>e8a040423a</td><br><td>c82c3dbd33</td><br><td>ee3501282b</td><br><td>199ce7c484</td><br><td>5f17dedd5c</td><br><td>5c5025bd0a</td><br><td>9aba4d7f51</td><br><td>24.943933</td><br><td>-0.814660</td><br><td>-0.708308</td><br><td>1.500000</td><br><td>-0.512422</td><br><td>-0.733967</td><br><td>0.333333</td><br><td>14.837728</td><br><td>11.0</td><br><td>0.000000</td><br><td>24.0</td><br><td>0.0</td><br><td>0.000000</td><br><td>1.0</td><br><td>29.0</td><br><td>0.0</td><br><td>3.0</td><br><td>11.0</td><br><td>4.42</td><br><td>0.15</td><br><td>0.161</td><br><td>0.2</td><br><td>1.000</td><br><td>1.000</td><br><td>1.000</td><br><td>1.000</td><br><td>1.000</td><br><td>0.520</td><br><td>0.533</td><br><td>0.835</td><br><td>-0.586540</td><br><td>0.672436</td><br><td>0.000000</td><br><td>0.606061</td><br><td>0.121212</td><br><td>0.212121</td><br><td>0.060606</td><br><td>0.000000</td><br><td>33.0</td><br><td>3</td><br></tr><br><tr><br><th>3</th><br><td>43859085bc</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>a62168d626</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>0.004439</td><br><td>f67f142e40</td><br><td>c4dd2197c3</td><br><td>91bb549494</td><br><td>e33c63cf35</td><br><td>14559.0</td><br><td>6e40247e69</td><br><td>617a4ad3f9</td><br><td>718c61545b</td><br><td>c26d08129a</td><br><td>9e166b965d</td><br><td>466f8951b0</td><br><td>fde72a6d5c</td><br><td>acfadc5c01</td><br><td>9aba4d7f51</td><br><td>41.576860</td><br><td>-0.907833</td><br><td>-0.761736</td><br><td>0.500000</td><br><td>-0.627525</td><br><td>-0.805801</td><br><td>1.166667</td><br><td>0.004395</td><br><td>0.0</td><br><td>0.500000</td><br><td>0.0</td><br><td>0.0</td><br><td>0.000000</td><br><td>7.0</td><br><td>7.0</td><br><td>0.0</td><br><td>3.0</td><br><td>15.0</td><br><td>8.92</td><br><td>0.29</td><br><td>0.226</td><br><td>0.8</td><br><td>0.000</td><br><td>0.000</td><br><td>0.000</td><br><td>0.000</td><br><td>0.000</td><br><td>1.000</td><br><td>0.000</td><br><td>0.000</td><br><td>-1.600326</td><br><td>-1.838680</td><br><td>0.000000</td><br><td>1.000000</td><br><td>0.000000</td><br><td>0.000000</td><br><td>0.000000</td><br><td>0.000000</td><br><td>1.0</td><br><td>4</td><br></tr><br><tr><br><th>4</th><br><td>a4c3095b75</td><br><td>16a14a2d17</td><br><td>06330986ed</td><br><td>ca63304de0</td><br><td>b7584c2d52</td><br><td>1746600cb0</td><br><td>1</td><br><td>1</td><br><td>0.480977</td><br><td>7e5c97705a</td><br><td>e071d01df5</td><br><td>91bb549494</td><br><td>e33c63cf35</td><br><td>5777.0</td><br><td>6e40247e69</td><br><td>617a4ad3f9</td><br><td>4b9480aa42</td><br><td>e84655292c</td><br><td>527b6ca8cc</td><br><td>dd9c9e0da2</td><br><td>17c99905b6</td><br><td>0fc56ea1f0</td><br><td>9aba4d7f51</td><br><td>31.080282</td><br><td>-0.371787</td><br><td>-0.367616</td><br><td>1.666667</td><br><td>0.271307</td><br><td>0.013112</td><br><td>17.333333</td><br><td>1713.439128</td><br><td>33.0</td><br><td>0.000000</td><br><td>6.0</td><br><td>1.0</td><br><td>0.666667</td><br><td>8.0</td><br><td>108.0</td><br><td>1.0</td><br><td>4.0</td><br><td>86.0</td><br><td>1.58</td><br><td>0.05</td><br><td>2.032</td><br><td>2.4</td><br><td>0.348</td><br><td>0.762</td><br><td>0.550</td><br><td>0.392</td><br><td>0.489</td><br><td>0.517</td><br><td>1.000</td><br><td>0.642</td><br><td>0.960991</td><br><td>0.790990</td><br><td>0.020161</td><br><td>0.645161</td><br><td>0.258065</td><br><td>0.036290</td><br><td>0.040323</td><br><td>0.000000</td><br><td>248.0</td><br><td>3</td><br></tr><br></tbody><br></table><br></div><h1 id="Build-a-quick-baseline"><a href="#Build-a-quick-baseline" class="headerlink" title="Build a quick baseline"></a>Build a quick baseline</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</div><div class="line"></div><div class="line"><span class="comment"># Create a copy to work with</span></div><div class="line">X = train.copy()</div><div class="line"></div><div class="line"><span class="comment"># Save and drop labels</span></div><div class="line">y = train.y</div><div class="line">X = X.drop(<span class="string">'y'</span>, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># fill NANs </span></div><div class="line">X = X.fillna(<span class="number">-999</span>)</div><div class="line"></div><div class="line"><span class="comment"># Label encoder</span></div><div class="line"><span class="keyword">for</span> c <span class="keyword">in</span> train.columns[train.dtypes == <span class="string">'object'</span>]:</div><div class="line">    X[c] = X[c].factorize()[<span class="number">0</span>]</div><div class="line">    </div><div class="line">rf = RandomForestClassifier()</div><div class="line">rf.fit(X,y)</div></pre></td></tr></table></figure><pre><code>RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&apos;gini&apos;,
            max_depth=None, max_features=&apos;auto&apos;, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,
            verbose=0, warm_start=False)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plt.plot(rf.feature_importances_)</div><div class="line">plt.xticks(np.arange(X.shape[<span class="number">1</span>]), X.columns.tolist(), rotation=<span class="number">90</span>);</div></pre></td></tr></table></figure><pre><code>/home/dulyanov/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family [u&apos;serif&apos;] not found. Falling back to DejaVu Sans
  (prop.get_family(), self.defaultFamily[fontext]))
</code></pre><p><img src="https://github.com/ewanlee/blog-image-hosting/blob/master/output_6_1.png?raw=true" alt="png"></p><p>There is something interesting about <code>x8</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># we see it was standard scaled, most likely, if we concat train and test, we will get exact mean=1, and std 1 </span></div><div class="line"><span class="keyword">print</span> <span class="string">'Mean:'</span>, train.x8.mean()</div><div class="line"><span class="keyword">print</span> <span class="string">'std:'</span>, train.x8.std()</div></pre></td></tr></table></figure><pre><code>Mean: -0.000252352028622
std: 1.02328163601
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># And we see that it has a lot of repeated values</span></div><div class="line">train.x8.value_counts().head(<span class="number">15</span>)</div></pre></td></tr></table></figure><pre><code>-2.984750    2770
 0.480977    2569
 0.610941    1828
 0.654263    1759
 0.567620    1746
 0.697585    1691
 0.524298    1639
 0.740906    1628
 0.394333    1610
 0.437655    1513
 0.351012    1450
 0.264369    1429
 0.307690    1401
 0.221047    1372
 0.784228    1293
Name: x8, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># It's very hard to work with scaled feature, so let's try to scale them back</span></div><div class="line"><span class="comment"># Let's first take a look at difference between neighbouring values in x8</span></div><div class="line"></div><div class="line">x8_unique = train.x8.unique()</div><div class="line">x8_unique_sorted = np.sort(x8_unique)</div><div class="line">                           </div><div class="line">np.diff(x8_unique_sorted)</div></pre></td></tr></table></figure><pre><code>array([ 43.27826527,  38.98942817,   0.21660793,   0.04332159,
         0.17328635,   0.21660793,   0.08664317,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.12996476,   0.04332159,
         0.04332159,   0.04332159,   0.04332159,   0.04332159,
         0.04332159,   0.04332159,   0.21660793,   1.16968285,
         0.04332159,   0.38989428,          nan])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># The most of the diffs are 0.04332159! </span></div><div class="line"><span class="comment"># The data is scaled, so we don't know what was the diff value for the original feature</span></div><div class="line"><span class="comment"># But let's assume it was 1.0</span></div><div class="line"><span class="comment"># Let's devide all the numbers by 0.04332159 to get the right scaling</span></div><div class="line"><span class="comment"># note, that feature will still have zero mean</span></div><div class="line"></div><div class="line">np.diff(x8_unique_sorted/<span class="number">0.04332159</span>)</div></pre></td></tr></table></figure><pre><code>array([ 998.99992752,  899.9999347 ,    4.99999964,    0.99999993,
          3.99999971,    4.99999964,    1.99999985,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    2.99999978,    0.99999993,
          0.99999993,    0.99999993,    0.99999993,    0.99999993,
          0.99999993,    0.99999993,    4.99999964,   26.99999804,
          0.99999993,    8.99999935,           nan])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(train.x8/<span class="number">0.04332159</span>).head(<span class="number">10</span>)</div></pre></td></tr></table></figure><pre><code>0   -15.897530
1    20.102468
2    10.102468
3     0.102469
4    11.102468
5   -68.897526
6    10.102468
7    15.102468
8     9.102468
9   -68.897526
Name: x8, dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Ok, now we see .102468 in every value</span></div><div class="line"><span class="comment"># this looks like a part of a mean that was subtracted during standard scaling</span></div><div class="line"><span class="comment"># If we subtract it, the values become almost integers</span></div><div class="line">(train.x8/<span class="number">0.04332159</span> - <span class="number">.102468</span>).head(<span class="number">10</span>)</div></pre></td></tr></table></figure><pre><code>0   -15.999998
1    20.000000
2    10.000000
3     0.000001
4    11.000000
5   -68.999994
6    10.000000
7    15.000000
8     9.000000
9   -68.999994
Name: x8, dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># let's round them </span></div><div class="line">x8_int = (train.x8/<span class="number">0.04332159</span> - <span class="number">.102468</span>).round()</div><div class="line">x8_int.head(<span class="number">10</span>)</div></pre></td></tr></table></figure><pre><code>0   -16.0
1    20.0
2    10.0
3     0.0
4    11.0
5   -69.0
6    10.0
7    15.0
8     9.0
9   -69.0
Name: x8, dtype: float64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Ok, what's next? In fact it is not obvious how to find shift parameter, </span></div><div class="line"><span class="comment"># and how to understand what the data this feature actually store</span></div><div class="line"><span class="comment"># But ...</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">x8_int.value_counts()</div></pre></td></tr></table></figure><pre><code>-69.0      2770
 11.0      2569
 14.0      1828
 15.0      1759
 13.0      1746
 16.0      1691
 12.0      1639
 17.0      1628
 9.0       1610
 10.0      1513
 8.0       1450
 6.0       1429
 7.0       1401
 5.0       1372
 18.0      1293
 1.0       1290
 4.0       1276
 2.0       1250
 3.0       1213
-1.0       1085
 0.0       1080
-2.0       1006
-4.0        995
-3.0        976
-5.0        954
-8.0        923
-9.0        921
-6.0        906
 19.0       893
-7.0        881
           ... 
 26.0         3
-40.0         3
-41.0         3
 25.0         2
-59.0         2
 31.0         2
 34.0         2
-46.0         2
-49.0         2
 33.0         2
-42.0         2
 32.0         2
 37.0         2
 30.0         2
-45.0         2
-54.0         1
 36.0         1
-51.0         1
 27.0         1
 79.0         1
-47.0         1
 69.0         1
 70.0         1
-50.0         1
-1968.0       1
 42.0         1
-63.0         1
-48.0         1
-64.0         1
 35.0         1
Name: x8, Length: 99, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># do you see this -1968? Doesn't it look like a year? ... So my hypothesis is that this feature is a year of birth! </span></div><div class="line"><span class="comment"># Maybe it was a textbox where users enter their year of birth, and someone entered 0000 instead</span></div><div class="line"><span class="comment"># The hypothesis looks plausible, isn't it?</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(x8_int + <span class="number">1968.0</span>).value_counts().sort_index()</div></pre></td></tr></table></figure><pre><code>0.0          1
999.0        4
1899.0    2770
1904.0       1
1905.0       1
1909.0       2
1914.0       1
1916.0       3
1917.0       1
1918.0       1
1919.0       2
1920.0       1
1921.0       1
1922.0       2
1923.0       2
1924.0       4
1925.0       4
1926.0       2
1927.0       3
1928.0       3
1929.0       4
1930.0       4
1931.0      12
1932.0      10
1933.0       7
1934.0      13
1935.0      28
1936.0      35
1937.0      35
1938.0      45
          ... 
1978.0    1513
1979.0    2569
1980.0    1639
1981.0    1746
1982.0    1828
1983.0    1759
1984.0    1691
1985.0    1628
1986.0    1293
1987.0     893
1988.0     624
1989.0     434
1990.0     233
1991.0     110
1992.0      31
1993.0       2
1994.0       3
1995.0       1
1998.0       2
1999.0       2
2000.0       2
2001.0       2
2002.0       2
2003.0       1
2004.0       1
2005.0       2
2010.0       1
2037.0       1
2038.0       1
2047.0       1
Name: x8, Length: 99, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># After the competition ended the organisers told it was really a year of birth</span></div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt; You will not be able to run this notebook at coursera platform, as the dataset is not there. The notebook is in read-only mode.&lt;/p&gt;&lt;p&gt;But you can run the notebook locally and download the dataset using &lt;a href=&quot;https://habrastorage.org/storage/stuff/special/beeline/00.beeline_bigdata.zip&quot;&gt;this link&lt;/a&gt; to explore the data interactively.&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;pd.set_option(&lt;span class=&quot;string&quot;&gt;&#39;max_columns&#39;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&quot;Load-the-data&quot;&gt;&lt;a href=&quot;#Load-the-data&quot; class=&quot;headerlink&quot; title=&quot;Load the data&quot;&gt;&lt;/a&gt;Load the data&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;train = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;./train.csv&#39;&lt;/span&gt;)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;train.head()&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Exploratory Data Analysis</title>
    <link href="http://yoursite.com/2018/10/16/Exploratory-Data-Analysis/"/>
    <id>http://yoursite.com/2018/10/16/Exploratory-Data-Analysis/</id>
    <published>2018-10-16T06:39:40.000Z</published>
    <updated>2018-10-16T07:15:25.325Z</updated>
    
    <content type="html"><![CDATA[<p>This is a detailed EDA of the data, shown in the second video of “Exploratory data analysis” lecture (week 2).</p><p><strong>PLEASE NOTE</strong>: the dataset cannot be published, so this notebook is read-only.</p><h2 id="Load-data"><a href="#Load-data" class="headerlink" title="Load data"></a>Load data</h2><p>In this competition hosted by <em>solutions.se</em>, the task was to predict the advertisement cost for a particular ad.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line">%matplotlib inline</div><div class="line"></div><div class="line">data_path = <span class="string">'./data'</span></div><div class="line">train = pd.read_csv(<span class="string">'%s/train.csv.gz'</span> % data_path, parse_dates=[<span class="string">'Date'</span>])</div><div class="line">test  = pd.read_csv(<span class="string">'%s/test.csv.gz'</span> % data_path,  parse_dates=[<span class="string">'Date'</span>])</div></pre></td></tr></table></figure><p>Let’s look at the data (notice that the table is transposed, so we can see all feature names).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.head().T</div></pre></td></tr></table></figure><a id="more"></a><table border="1" class="dataframe"><br><br><br><thead><br><tr style="text-align:right"><br><th></th><br><th>0</th><br><th>1</th><br><th>2</th><br><th>3</th><br><th>4</th><br></tr><br></thead><br><tbody><br><tr><br><th>AdGroupId</th><br><td>78db034136</td><br><td>68a0110c69</td><br><td>21af1035af</td><br><td>f63fda0c33</td><br><td>cd868ebdcc</td><br></tr><br><tr><br><th>AdGroupName</th><br><td>6d91d 25866 9c594</td><br><td>2657d cb2d0 6d91d</td><br><td>6d91d e33a0 9a99b</td><br><td>59991 9c594</td><br><td>6d91d 25866 9a99b</td><br></tr><br><tr><br><th>AdNetworkType2</th><br><td>s</td><br><td>s</td><br><td>s</td><br><td>s</td><br><td>s</td><br></tr><br><tr><br><th>AveragePosition</th><br><td>1.2</td><br><td>2</td><br><td>1</td><br><td>1</td><br><td>1.1</td><br></tr><br><tr><br><th>CampaignId</th><br><td>273823cb71</td><br><td>273823cb71</td><br><td>273823cb71</td><br><td>273823cb71</td><br><td>273823cb71</td><br></tr><br><tr><br><th>CampaignName</th><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br><td>2657d 16cb2 74532 b4842 0136e 35aca f140d</td><br></tr><br><tr><br><th>Clicks</th><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br><td>3</td><br></tr><br><tr><br><th>Conversions</th><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br></tr><br><tr><br><th>ConversionsManyPerClick</th><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br></tr><br><tr><br><th>Cost</th><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0</td><br><td>0.94</td><br></tr><br><tr><br><th>Date</th><br><td>2014-01-01 00:00:00</td><br><td>2014-01-01 00:00:00</td><br><td>2014-01-01 00:00:00</td><br><td>2014-01-01 00:00:00</td><br><td>2014-01-01 00:00:00</td><br></tr><br><tr><br><th>DestinationUrl</th><br><td>98035d60fc</td><br><td>c25f23cd08</td><br><td>01f87f7639</td><br><td>5c0e89f532</td><br><td>8888b55dde</td><br></tr><br><tr><br><th>Device</th><br><td>t</td><br><td>t</td><br><td>t</td><br><td>d</td><br><td>d</td><br></tr><br><tr><br><th>FirstPageCpc</th><br><td>1.06</td><br><td>2.94</td><br><td>0.42</td><br><td>1.75</td><br><td>0.17</td><br></tr><br><tr><br><th>Impressions</th><br><td>32</td><br><td>1</td><br><td>4</td><br><td>1</td><br><td>22</td><br></tr><br><tr><br><th>KeywordMatchType</th><br><td>b</td><br><td>b</td><br><td>b</td><br><td>b</td><br><td>b</td><br></tr><br><tr><br><th>KeywordText</th><br><td>jze 10 +uxsgk</td><br><td>+jze +dznvgyhjclr</td><br><td>jze 100 +gzpxyk</td><br><td>jze 10 +uxsgk 1950k</td><br><td>jze 10 mykj +gzpxyk</td><br></tr><br><tr><br><th>MaxCpc</th><br><td>0.28</td><br><td>1</td><br><td>0.22</td><br><td>0.54</td><br><td>0.12</td><br></tr><br><tr><br><th>QualityScore</th><br><td>1</td><br><td>1</td><br><td>1</td><br><td>1</td><br><td>1</td><br></tr><br><tr><br><th>Slot</th><br><td>s_2</td><br><td>s_2</td><br><td>s_1</td><br><td>s_2</td><br><td>s_1</td><br></tr><br><tr><br><th>TopOfPageCpc</th><br><td>1.07</td><br><td>5.02</td><br><td>0.42</td><br><td>4</td><br><td>0.25</td><br></tr><br><tr><br><th>KeywordId</th><br><td>7d20d63df9</td><br><td>a617d4f037</td><br><td>6e0b7024d2</td><br><td>9c2ea0cdf8</td><br><td>4c8ba7affd</td><br></tr><br></tbody><br></table><p>We see a lot of features with not obvious names. If you search for the <em>CampaignId</em>, <em>AdGroupName</em>, <em>AdNetworkType2</em> using any web search engine, you will find this dataset was exported from Google AdWords. So what is the required domain knowledge here? The knowledge of how web advertisement and Google AdWords work! After you have learned it, the features will make sense to you and you can proceed.</p><p>For the sake of the story I will briefly describe Google AdWords system now. Basically every time a user queries a search engine, Google AdWords decides what ad will be shown along with the actual search results. On the other side of AdWords, the advertisers manage the ads – they can set a multiple keywords, that a user should query in order to their ad to be shown. If the keywords are set properly and are relevant to the ad, then the ad will be shown to relevant users and the ad will get clicked. Advertisers pay to Google for some type of events, happened with their ad: for example for a click event, i.e. the user saw this ad and clicked it. AdWords uses complex algorithms to decide which ad to show to a particular user with a particular search query. The advertisers can only indirectly influence AdWords decesion process by changing keywords and several other parameters. So at a high level, the task is to predict what will be the costs for the advertiser (how much he will pay to Google, column <em>Cost</em>) when the parameters (e.g. keywords) are changed.</p><p>The ads are grouped in groups, there are features <em>AdGroupId</em> <em>AdGroupName</em> describing them. A campaign corresponds to some specific parameters that an advertiser sets. Similarly, there are ID and name features <em>CampaignId</em>, <em>CampaignName</em>. And finally there is some information about keywords: <em>KeywordId</em> and <em>KeywordText</em>. Slot is $1$ when ad is shown on top of the page, and $2$ when on the side. Device is a categorical variable and can be either “tablet”, “mobile” or “pc”. And finally the <em>Date</em> is just the date, for which clicks were aggregated.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.head().T</div></pre></td></tr></table></figure><table border="1" class="dataframe"><br><thead><br><tr style="text-align:right"><br><th></th><br><th>0</th><br><th>1</th><br><th>2</th><br><th>3</th><br><th>4</th><br></tr><br></thead><br><tbody><br><tr><br><th>Id</th><br><td>0</td><br><td>1</td><br><td>2</td><br><td>3</td><br><td>4</td><br></tr><br><tr><br><th>AdGroupId</th><br><td>00096e7611</td><br><td>00096e7611</td><br><td>00096e7611</td><br><td>00096e7611</td><br><td>00096e7611</td><br></tr><br><tr><br><th>AdGroupName</th><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br><td>c8037 75b01 9a99b 3b678 52ba4 2657d</td><br></tr><br><tr><br><th>AdNetworkType2</th><br><td>s</td><br><td>s</td><br><td>s</td><br><td>s</td><br><td>s</td><br></tr><br><tr><br><th>AveragePosition</th><br><td>1</td><br><td>1</td><br><td>1</td><br><td>1</td><br><td>1</td><br></tr><br><tr><br><th>CampaignId</th><br><td>e62b4bc4c3</td><br><td>e62b4bc4c3</td><br><td>e62b4bc4c3</td><br><td>e62b4bc4c3</td><br><td>e62b4bc4c3</td><br></tr><br><tr><br><th>CampaignName</th><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br><td>2657d 16cb2 74532 06feb 0136e 3a15d</td><br></tr><br><tr><br><th>Date</th><br><td>2014-06-01 00:00:00</td><br><td>2014-06-01 00:00:00</td><br><td>2014-06-01 00:00:00</td><br><td>2014-06-01 00:00:00</td><br><td>2014-06-01 00:00:00</td><br></tr><br><tr><br><th>DestinationUrl</th><br><td>f5aad09031</td><br><td>f5aad09031</td><br><td>f5aad09031</td><br><td>f5aad09031</td><br><td>f5aad09031</td><br></tr><br><tr><br><th>Device</th><br><td>t</td><br><td>d</td><br><td>m</td><br><td>t</td><br><td>d</td><br></tr><br><tr><br><th>KeywordId</th><br><td>539778bb80</td><br><td>539778bb80</td><br><td>539778bb80</td><br><td>539778bb80</td><br><td>539778bb80</td><br></tr><br><tr><br><th>KeywordMatchType</th><br><td>e</td><br><td>e</td><br><td>e</td><br><td>e</td><br><td>e</td><br></tr><br><tr><br><th>KeywordText</th><br><td>tcjnw gzpxyk nyss ewzhy</td><br><td>tcjnw gzpxyk nyss ewzhy</td><br><td>tcjnw gzpxyk nyss ewzhy</td><br><td>tcjnw gzpxyk nyss ewzhy</td><br><td>tcjnw gzpxyk nyss ewzhy</td><br></tr><br><tr><br><th>Slot</th><br><td>s_1</td><br><td>s_1</td><br><td>s_1</td><br><td>s_2</td><br><td>s_2</td><br></tr><br></tbody><br></table><p>Notice there is diffrent number of columns in test and train – our target is <em>Cost</em> column, but it is closly related to several other features, e.g. <em>Clicks</em>, <em>Conversions</em>. All of the related columns were deleted from the test set to avoid data leakages.</p><h1 id="Let’s-analyze"><a href="#Let’s-analyze" class="headerlink" title="Let’s analyze"></a>Let’s analyze</h1><p>Are we ready to modeling? Not yet. Take a look at this statistic:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> <span class="string">'Train min/max date: %s / %s'</span> % (train.Date.min().date(), train.Date.max().date())</div><div class="line"><span class="keyword">print</span> <span class="string">'Test  min/max date: %s / %s'</span> % ( test.Date.min().date(),  test.Date.max().date())</div><div class="line"><span class="keyword">print</span> <span class="string">''</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Number of days in train: %d'</span> % ((train.Date.max() - train.Date.min()).days + <span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">'Number of days in test:  %d'</span> % (( test.Date.max() -  test.Date.min()).days + <span class="number">1</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">''</span></div><div class="line"><span class="keyword">print</span> <span class="string">'Train shape: %d rows'</span> % train.shape[<span class="number">0</span>]</div><div class="line"><span class="keyword">print</span> <span class="string">'Test shape: %d rows'</span>  % test.shape[<span class="number">0</span>]</div></pre></td></tr></table></figure><pre><code>Train min/max date: 2014-01-01 / 2014-05-31
Test  min/max date: 2014-06-01 / 2014-06-14

Number of days in train: 151
Number of days in test:  14

Train shape: 3493820 rows
Test shape: 8951040 rows
</code></pre><p>Train period is more than 10 times larger than the test period, but train set has fewer rows, how could that happen?</p><p>At this point I suggest you to stop and think yourself, what could be a reason, why this did happen. Unfortunately we cannot share the data for this competition, but the information from above should be enough to get a right idea.</p><p>Alternatively, you can go along for the explanation, if you want.</p><h1 id="Investigation"><a href="#Investigation" class="headerlink" title="Investigation"></a>Investigation</h1><p>Let’s take a look how many rows with each date we have in train and test.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.Date.value_counts()</div></pre></td></tr></table></figure><pre><code>2014-06-02    639360
2014-06-12    639360
2014-06-09    639360
2014-06-14    639360
2014-06-01    639360
2014-06-11    639360
2014-06-08    639360
2014-06-05    639360
2014-06-10    639360
2014-06-07    639360
2014-06-04    639360
2014-06-06    639360
2014-06-03    639360
2014-06-13    639360
Name: Date, dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># print only first 10</span></div><div class="line">train.Date.value_counts().head(<span class="number">10</span>)</div></pre></td></tr></table></figure><pre><code>2014-01-01    36869
2014-01-04    36427
2014-01-05    36137
2014-01-02    34755
2014-01-03    34693
2014-01-06    31349
2014-04-07    30950
2014-02-09    30101
2014-01-26    29830
2014-02-08    29187
Name: Date, dtype: int64
</code></pre><p>Interesting, for the test set we have the same number of rows for every date, while in train set the number of rows is different for each day. It looks like that for each day in the test set a loop through some kind of IDs had been run. But what about train set? So far we don’t know, but let’s find the test IDs first.</p><h3 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h3><p>So now we know, that there is $639360$ different IDs. It should be easy to find the columns, that form ID, because if the ID is [‘col1’, ‘col2’], then to compute the number of combinations we should just multiply the number of unique elements in each.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">test_nunique = test.nunique()</div><div class="line">test_nunique</div></pre></td></tr></table></figure><pre><code>Id                  8951040
AdGroupId             13548
AdGroupName            2281
AdNetworkType2            2
AveragePosition         131
CampaignId              252
CampaignName            252
Date                     14
DestinationUrl        52675
Device                    3
KeywordId             12285
KeywordMatchType          3
KeywordText           11349
Slot                      4
dtype: int64
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"><span class="comment"># This function looks for a combination of elements </span></div><div class="line"><span class="comment"># with product of 639360 </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_prod</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="comment"># combinations of not more than 5 features</span></div><div class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>):</div><div class="line">        <span class="comment"># iterate through all combinations</span></div><div class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> itertools.combinations(range(len(data)), n):</div><div class="line">            <span class="keyword">if</span> data[list(c)].prod() == <span class="number">639360</span>:</div><div class="line">                <span class="keyword">print</span> test_nunique.index[c]</div><div class="line">                <span class="keyword">return</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Nothing found'</span></div><div class="line"></div><div class="line">    </div><div class="line">find_prod(test_nunique.values)</div></pre></td></tr></table></figure><pre><code>Nothing found
</code></pre><p>Hmm, nothing found! The problem is that some features are tied, and the number of their combinations does not equal to product of individual unique number of elements. For example it does not make sense to create all possible combinations of <em>DestinationUrl</em> and <em>AdGroupId</em> as <em>DestinationUrl</em> belong to exactly one <em>AdGroupId</em>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test.groupby(<span class="string">'DestinationUrl'</span>).AdGroupId.nunique()</div></pre></td></tr></table></figure><pre><code>DestinationUrl
00010d62df    1
000249f717    1
00054cf3f8    1
000684bf0b    1
00072a9fa7    1
00077a6729    1
0007cc191f    1
0009388900    1
001144cae4    1
00115f6477    1
00141a299f    1
00169dc49b    1
0018b27e06    1
001b0b3d06    1
001ef8368e    1
00205e056a    1
002082ab8b    1
0020c585ea    1
0021419f7e    1
00225519cc    1
002498dc88    1
0026171436    1
00265dc4bb    1
0026833e5c    1
0027ffbad9    1
002b1deb25    1
002c55ccef    1
002e44290f    1
0030ca870e    1
0032b64beb    1
             ..
ffda377018    1
ffda3c412a    1
ffda5b53d6    1
ffda8c0d8c    1
ffdbf5d179    1
ffdc872fcf    1
ffde114af5    1
ffde41a800    1
ffe2fb7007    1
ffe4a040d4    1
ffe685e937    1
ffe8c3da53    1
ffe8f82e08    1
ffeb9fda9d    1
ffebd1d253    1
ffebea724f    1
ffecf398b1    1
ffecf3e7d4    1
ffed185438    1
fff02d7269    1
fff10adcb0    1
fff12e5f19    1
fff132d5bd    1
fff19836a0    1
fff3539204    1
fff4c5d255    1
fff55db78a    1
fff8c11ad9    1
fff90ea351    1
fffb248bf0    1
Name: AdGroupId, Length: 52675, dtype: int64
</code></pre><p>So, now let’s try to find ID differently. Let’s try to find a list of columns, such that threre is exazctly $639360$ unique combinations of their values <strong>in the test set</strong> (not overall). So, we want to find <code>columns</code>, such that:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">test[columns].drop_duplicates().shape[<span class="number">0</span>]  == <span class="number">639360</span></div></pre></td></tr></table></figure><p>We could do it with a similar loop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> itertools</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_ncombinations</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="comment"># combinations of not more than 5 features</span></div><div class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">5</span>):</div><div class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> itertools.combinations(range(data.shape[<span class="number">1</span>]), n):</div><div class="line">            <span class="keyword">print</span> c</div><div class="line">            columns = test.columns[list(c)]</div><div class="line">            <span class="keyword">if</span> test[columns].drop_duplicates().shape[<span class="number">0</span>] == <span class="number">639360</span>:</div><div class="line">                <span class="keyword">print</span> columns</div><div class="line">                <span class="keyword">return</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'Nothing found'</span></div><div class="line"></div><div class="line">    </div><div class="line">find_ncombinations(test)</div></pre></td></tr></table></figure><p>But it will take forever to compute. So it is easier to find the combination manually.</p><p>So after some time of trials and errors I figured out, that the four features <em>KeywordId, AdGroupId, Device, Slot</em> form the index. The number of unique rows is exactly <em>639360</em> as we wanted to find.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">columns = [<span class="string">'KeywordId'</span>, <span class="string">'AdGroupId'</span>, <span class="string">'Device'</span>, <span class="string">'Slot'</span>]</div><div class="line">test[columns].drop_duplicates().shape</div></pre></td></tr></table></figure><pre><code>(639360, 4)
</code></pre><p>Looks reasonable. For each <em>AdGroupId</em> there is a <strong>distinct set</strong> of possible <em>KeywordId’s</em>, but <em>Device</em> and <em>Slot</em> variants are the same for each ad. And the target is to predict what will be the daily cost for using different <em>KeywordId’s</em>, <em>Device</em> type, <em>Slot</em> type to advertise ads from <em>AdGroups</em>.</p><h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><p>To this end, we found how test set was constructed, but what about the train set? Let us plot something, probably we will find it out.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line">sns.set(palette=<span class="string">'pastel'</span>)</div><div class="line">sns.set(font_scale=<span class="number">2</span>)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># from absolute dates to relative</span></div><div class="line">train[<span class="string">'date_diff'</span>] =  (train.Date - train.Date.min()).dt.days</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># group by the index, that we've found</span></div><div class="line">g= train.groupby([<span class="string">'KeywordId'</span>, <span class="string">'AdGroupId'</span>, <span class="string">'Device'</span>, <span class="string">'Slot'</span>])</div><div class="line"></div><div class="line"><span class="comment"># and for each index show average relative date versus </span></div><div class="line"><span class="comment"># the number of rows with that index</span></div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>))</div><div class="line">plt.scatter(g.date_diff.mean(),g.size(),edgecolor = <span class="string">'none'</span>,alpha = <span class="number">0.2</span>, s=<span class="number">20</span>, c=<span class="string">'b'</span>)</div><div class="line">plt.xlabel(<span class="string">'Group mean relative date'</span>)</div><div class="line">plt.ylabel(<span class="string">'Group size'</span>)</div><div class="line">plt.title(<span class="string">'Train'</span>);</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/ewanlee/blog-image-hosting/master/output_36_0.png" alt="png"></p><p>Looks interesting, isn’t it? That is something we need to explain! How the same plot looks for the test set?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># from absolute dates to relative</span></div><div class="line">test[<span class="string">'date_diff'</span>] =  (test.Date - test.Date.min()).dt.days</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># group by the index, that we've found</span></div><div class="line">g= test.groupby([<span class="string">'KeywordId'</span>, <span class="string">'AdGroupId'</span>, <span class="string">'Device'</span>, <span class="string">'Slot'</span>])</div><div class="line"></div><div class="line"><span class="comment"># and for each index show average relative date versus </span></div><div class="line"><span class="comment"># the number of rows with that index</span></div><div class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>))</div><div class="line">plt.scatter(g.date_diff.mean(),g.size(),edgecolor = <span class="string">'none'</span>,alpha = <span class="number">0.2</span>, s=<span class="number">20</span>, c=<span class="string">'b'</span>)</div><div class="line">plt.xlabel(<span class="string">'Group mean relative date'</span>)</div><div class="line">plt.ylabel(<span class="string">'Group size'</span>)</div><div class="line">plt.ylim(<span class="number">-2</span>, <span class="number">30</span>)</div><div class="line">plt.title(<span class="string">'Test'</span>);</div></pre></td></tr></table></figure><p><img src="https://github.com/ewanlee/blog-image-hosting/blob/master/output_39_0.png?raw=true" alt="png"></p><p>Just a dot!</p><p>Now let’s think, what we actually plotted? We grouped the data by the ID that we’ve found previously and we plotted average <em>Date</em> in the group versus the size of each group. We found that ID is an aggregation index – so for each date the <em>Cost</em> is aggreagated for each possible index. So group size shows for how many days we have <em>Const</em> information for each ID and mean relative date shows some information about these days.</p><p>For test set it is expectable that both average date and the size of the groups are the same for each group: the size of each group is $14$ (as we have $14$ test days) and mean date is $6.5$, because for each group (index) we have $14$ different days, and $\frac{0 + 1 + \dots + 13}{14} = 6.5$.</p><p>And now we can explain everything for the train set. Look at the top of the triangle: for those points (groups) we have <em>Cost</em> information for all the days in the train period, while on the sides we see groups, for which we have very few rows.</p><p>But why for some groups we have smaller number of rows, than number of days? Let’s look at the <em>Impressions</em> column.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train.Impressions.value_counts()</div></pre></td></tr></table></figure><pre><code>1         1602929
2          565896
3          287128
4          175197
5          119092
6           86651
7           66443
8           53007
9           42984
10          35731
11          30248
12          25950
13          22629
14          20126
15          17503
16          15682
17          14100
18          12848
19          11597
20          10724
21           9864
22           8931
23           8316
24           7953
25           7168
26           6684
27           6196
28           5863
29           5556
30           5223
           ...   
4978            1
15210           1
9076            1
13174           1
116535          1
4979            1
17273           1
90974           1
4976            1
5906            1
7023            1
60282           1
7955            1
13881           1
2921            1
4970            1
7019            1
17249           1
23394           1
28210           1
11116           1
15929           1
7017            1
95761           1
2923            1
15213           1
9070            1
5692            1
13162           1
13922           1
Name: Impressions, Length: 8135, dtype: int64
</code></pre><p>We never have $0$ value in <em>Imressions</em> column. But in reality, of course, some ads with some combination of keyword, slot, device were never shown. So this looks like a nice explanation for the data: in the train set we <strong>only</strong> have information about ads (IDs, groups) which were shown at least once. And for the test set, we, of course, want to predict <em>Cost</em> <strong>for every</strong> possible ID.</p><p>What it means for competitors, is that if one would just fit a model on the train set as is, the predictions for the test set will be biased by a lot. The predictions will be much higher than they should be, as we are only given a specific subset of rows as <code>train.csv</code> file.</p><p>So, before modeling we should first extend the trainset and inject rows with <code>0</code> impressions. Such change will make train set very similar to the test set and the models will generalize nicely.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is a detailed EDA of the data, shown in the second video of “Exploratory data analysis” lecture (week 2).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;PLEASE NOTE&lt;/strong&gt;: the dataset cannot be published, so this notebook is read-only.&lt;/p&gt;&lt;h2 id=&quot;Load-data&quot;&gt;&lt;a href=&quot;#Load-data&quot; class=&quot;headerlink&quot; title=&quot;Load data&quot;&gt;&lt;/a&gt;Load data&lt;/h2&gt;&lt;p&gt;In this competition hosted by &lt;em&gt;solutions.se&lt;/em&gt;, the task was to predict the advertisement cost for a particular ad.&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; pd&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; numpy &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; np&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;%matplotlib inline&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;data_path = &lt;span class=&quot;string&quot;&gt;&#39;./data&#39;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;train = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;%s/train.csv.gz&#39;&lt;/span&gt; % data_path, parse_dates=[&lt;span class=&quot;string&quot;&gt;&#39;Date&#39;&lt;/span&gt;])&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;test  = pd.read_csv(&lt;span class=&quot;string&quot;&gt;&#39;%s/test.csv.gz&#39;&lt;/span&gt; % data_path,  parse_dates=[&lt;span class=&quot;string&quot;&gt;&#39;Date&#39;&lt;/span&gt;])&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;Let’s look at the data (notice that the table is transposed, so we can see all feature names).&lt;/p&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;train.head().T&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>数据预处理相关技术</title>
    <link href="http://yoursite.com/2018/10/15/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/"/>
    <id>http://yoursite.com/2018/10/15/数据预处理相关技术/</id>
    <published>2018-10-15T14:07:35.000Z</published>
    <updated>2018-10-15T14:35:02.055Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数值型数据-non-tree-based-model"><a href="#数值型数据-non-tree-based-model" class="headerlink" title="数值型数据 (non-tree based model)"></a>数值型数据 (non-tree based model)</h1><ul><li>特征预处理<ul><li>MinMaxScalar 不会改变数据分布</li><li>StandardScalar</li><li>scipy.stats.rankdata</li><li>log transform <code>np.log(1+x)</code></li><li>raising to the power &lt; 1 <code>np.sqrt(x + 2/3)</code></li><li><strong>drop outlier（winsorization，specify upper and lower bound）</strong></li></ul></li></ul><p>融合不同预处理方法得到的特征训练一个模型或者每一种特征训练出一个模型最后做模型融合</p><ul><li>特征生成<ul><li><strong>主要依据先验经验以及对数据的深刻理解</strong></li><li>例如，浮点数的小数部分单独提取出来作为特征</li></ul></li></ul><h1 id="类别数据以及有序类别数据"><a href="#类别数据以及有序类别数据" class="headerlink" title="类别数据以及有序类别数据"></a>类别数据以及有序类别数据</h1><ul><li>特征预处理<ul><li>Label encoding (tree(or non-tree)-based model)<ul><li>alphabetical sorted <code>sklearn.preprocessing.LabelEncoder</code></li><li>order of appearance <code>Pandas.factorize</code></li><li><strong>frequency encoding</strong> （非常适用于测试数据中包含训练数据未包含的类别）</li></ul></li><li>Label encoding (non-tree(or tree)-based model)<ul><li>one-hot encoding (sparse matrix)</li></ul></li></ul></li><li>特征生成<ul><li>枚举不同的类别特征的组合形成新的类别特征 (linear models and KNN)</li></ul></li></ul><h1 id="日期数据以及坐标数据"><a href="#日期数据以及坐标数据" class="headerlink" title="日期数据以及坐标数据"></a>日期数据以及坐标数据</h1><h2 id="日期数据"><a href="#日期数据" class="headerlink" title="日期数据"></a>日期数据</h2><ul><li>特征生成<ul><li>周期性数据<ul><li>Day number in week, month, season, year</li><li>second, minute, second</li></ul></li><li>自什么时候以来<ul><li>问题无关， 比如自1970年1月1日以来</li><li>问题相关，比如距离下一个节假日还有多少天等等</li></ul></li><li>两个日期特征之间的差值</li></ul></li></ul><h2 id="坐标数据"><a href="#坐标数据" class="headerlink" title="坐标数据"></a>坐标数据</h2><ul><li><p>特征生成</p><ul><li>距离某些关键坐标的距离等等（需要外部数据支持）</li><li>对坐标进行网格化或者聚类，然后计算每个网格中的点距离选定点的距离或者每个簇中的点距离聚类中心的距离</li><li>点的密度（某一限定范围之内）</li><li>区域价值，例如物价房价等（某一限定范围之内）</li></ul></li><li><p>特征预处理</p><ul><li>坐标旋转（例如45°）</li></ul></li></ul><h1 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h1><ul><li>找出隐含的NaN，通过可视化数据分布</li><li>填充方法<ul><li><code>-999, -1</code>等</li><li>中值，均值等</li><li>尝试恢复缺失数据（线性回归）</li></ul></li><li>特征生成<ul><li>增加一个特征，是否有缺失值</li><li><strong>采用填充的缺失值进行特征生成要特别小心，一般来说若要进行特征生成，则最好不要在之前进行缺失值填充</strong></li></ul></li><li>xgboost对于缺失值不敏感</li></ul><h1 id="文本数据"><a href="#文本数据" class="headerlink" title="文本数据"></a>文本数据</h1><ul><li>特征生成<ul><li>词袋 <code>skearn.feature_extraction.text.CountVectorizer</code></li><li>TF-IDF <code>skearn.feature_extraction.text.TfidfVectorizer</code></li><li>N-grams <code>ngram</code></li></ul></li><li>特征预处理<ul><li>lowercase</li><li>lemmatization (单词最原始的形式)</li><li>stemming</li><li>stopwords <code>nltk</code></li></ul></li><li><strong>Word2Vec, Doc2vec, Glove, FastText, etc</strong></li><li>Pipeline<ol><li>预处理</li><li>Ngrams then TF-IDF</li><li>or Word2Vec, etc</li></ol></li></ul><h1 id="图像数据"><a href="#图像数据" class="headerlink" title="图像数据"></a>图像数据</h1><ul><li>可以结合不同层的特征图</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;数值型数据-non-tree-based-model&quot;&gt;&lt;a href=&quot;#数值型数据-non-tree-based-model&quot; class=&quot;headerlink&quot; title=&quot;数值型数据 (non-tree based model)&quot;&gt;&lt;/a&gt;数值型数据 
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>About Feature Scaling and Normalization</title>
    <link href="http://yoursite.com/2018/10/15/About-Feature-Scaling-and-Normalization/"/>
    <id>http://yoursite.com/2018/10/15/About-Feature-Scaling-and-Normalization/</id>
    <published>2018-10-15T13:48:20.000Z</published>
    <updated>2018-10-15T14:05:31.310Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About-standardization"><a href="#About-standardization" class="headerlink" title="About standardization"></a>About standardization</h2><p>The result of <strong>standardization</strong> (or <strong>Z-score normalization</strong>) is that the features will be rescaled so that they’ll have the properties of a standard normal distribution with $\mu=0$ and $\sigma=1$</p><p>where $\mu$ is the mean (average) and $\sigma$ is the standard deviation from the mean; standard scores (also called <strong>z</strong> scores) of the samples are calculated as follows:<br>$$<br>z = \frac{x - \mu}{\sigma}<br>$$<br>Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. Intuitively, we can think of gradient descent as a prominent example (an optimization algorithm often used in logistic regression, SVMs, perceptrons, neural networks etc.); with features being on different scales, certain weights may update faster than others since the feature values $x_j$ play a role in the weight updates<br>$$<br>\Delta w_j = - \eta \frac{\partial J}{\partial w_j} = \eta \sum_i (t^{(i)} - o^{(i)}) x_j^{(i)},<br>$$<br>so that</p><p>$w_j := w_j + \Delta w_j$, where $\eta$ is the learning rate, $t$ the target class label, and $o$ the actual output. Other intuitive examples include K-Nearest Neighbor algorithms and clustering algorithms that use, for example, Euclidean distance measures – in fact, tree-based classifier are probably the only classifiers where feature scaling doesn’t make a difference.</p><p>In fact, the only family of algorithms that I could think of being scale-invariant are tree-based methods. Let’s take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as “is feature x_i &gt;= some_val?” Intuitively, we can see that it really doesn’t matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale – it really doesn’t matter).</p><p>Some examples of algorithms where feature scaling matters are:</p><ul><li>k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally</li><li>k-means (see k-nearest neighbors)</li><li>logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others</li><li>linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not.</li></ul><p>In addition, we’d also want to think about whether we want to “standardize” or “normalize” (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights “equally.” As a rule of thumb I’d say: When in doubt, just standardize the data, it shouldn’t hurt.</p><h2 id="About-Min-Max-scaling"><a href="#About-Min-Max-scaling" class="headerlink" title="About Min-Max scaling"></a>About Min-Max scaling</h2><p>An alternative approach to Z-score normalization (or standardization) is the so-called <strong>Min-Max scaling</strong>(often also simply called “normalization” - a common cause for ambiguities).<br>In this approach, the data is scaled to a fixed range - usually 0 to 1.<br>The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.</p><p>A Min-Max scaling is typically done via the following equation:<br>$$<br>X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}<br>$$</p><h2 id="Z-score-standardization-or-Min-Max-scaling"><a href="#Z-score-standardization-or-Min-Max-scaling" class="headerlink" title="Z-score standardization or Min-Max scaling?"></a>Z-score standardization or Min-Max scaling?</h2><p><em>“Standardization or Min-Max scaling?”</em> - There is no obvious answer to this question: it really depends on the application.</p><p>For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; <a href="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html" target="_blank" rel="external">but more about PCA in my previous article</a>).</p><p>However, this doesn’t mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale.</p><h2 id="Standardizing-and-normalizing-how-it-can-be-done-using-scikit-learn"><a href="#Standardizing-and-normalizing-how-it-can-be-done-using-scikit-learn" class="headerlink" title="Standardizing and normalizing - how it can be done using scikit-learn"></a>Standardizing and normalizing - how it can be done using scikit-learn</h2><p>Of course, we could make use of NumPy’s vectorization capabilities to calculate the z-scores for standardization and to normalize the data using the equations that were mentioned in the previous sections. However, there is an even more convenient approach using the preprocessing module from one of Python’s open-source machine learning library <a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn</a>.</p><p>For the following examples and discussion, we will have a look at the free “Wine” Dataset that is deposited on the UCI machine learning repository<br>(<a href="http://archive.ics.uci.edu/ml/datasets/Wine" target="_blank" rel="external">http://archive.ics.uci.edu/ml/datasets/Wine</a>).</p><blockquote><p>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</p><p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml" target="_blank" rel="external">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p></blockquote><p>The Wine dataset consists of 3 different classes where each row correspond to a particular wine sample.</p><p>The class labels (1, 2, 3) are listed in the first column, and the columns 2-14 correspond to 13 different attributes (features):</p><p>1) Alcohol<br>2) Malic acid<br>…</p><h4 id="Loading-the-wine-dataset"><a href="#Loading-the-wine-dataset" class="headerlink" title="Loading the wine dataset"></a>Loading the wine dataset</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.io.parsers.read_csv(</div><div class="line">    <span class="string">'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv'</span>,</div><div class="line">     header=<span class="keyword">None</span>,</div><div class="line">     usecols=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</div><div class="line">    )</div><div class="line"></div><div class="line">df.columns=[<span class="string">'Class label'</span>, <span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]</div><div class="line"></div><div class="line">df.head()</div></pre></td></tr></table></figure><table><thead><tr><th></th><th>Class label</th><th>Alcohol</th><th>Malic acid</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>14.23</td><td>1.71</td></tr><tr><td>1</td><td>1</td><td>13.20</td><td>1.78</td></tr><tr><td>2</td><td>1</td><td>13.16</td><td>2.36</td></tr><tr><td>3</td><td>1</td><td>14.37</td><td>1.95</td></tr><tr><td>4</td><td>1</td><td>13.24</td><td>2.59</td></tr></tbody></table><p>As we can see in the table above, the features <strong>Alcohol</strong> (percent/volumne) and <strong>Malic acid</strong> (g/l) are measured on different scales, so that <strong>Feature Scaling</strong> is necessary important prior to any comparison or combination of these data.</p><h4 id="Standardization-and-Min-Max-scaling"><a href="#Standardization-and-Min-Max-scaling" class="headerlink" title="Standardization and Min-Max scaling"></a>Standardization and Min-Max scaling</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line">std_scale = preprocessing.StandardScaler().fit(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line">df_std = std_scale.transform(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line"></div><div class="line">minmax_scale = preprocessing.MinMaxScaler().fit(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line">df_minmax = minmax_scale.transform(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'Mean after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_std[:,<span class="number">0</span>].mean(), df_std[:,<span class="number">1</span>].mean()))</div><div class="line">print(<span class="string">'\nStandard deviation after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_std[:,<span class="number">0</span>].std(), df_std[:,<span class="number">1</span>].std()))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Mean after standardization:</div><div class="line">Alcohol=<span class="number">0.00</span>, Malic acid=<span class="number">0.00</span></div><div class="line"></div><div class="line">Standard deviation after standardization:</div><div class="line">Alcohol=<span class="number">1.00</span>, Malic acid=<span class="number">1.00</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'Min-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_minmax[:,<span class="number">0</span>].min(), df_minmax[:,<span class="number">1</span>].min()))</div><div class="line">print(<span class="string">'\nMax-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_minmax[:,<span class="number">0</span>].max(), df_minmax[:,<span class="number">1</span>].max()))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Min-value after min-max scaling:</div><div class="line">Alcohol=<span class="number">0.00</span>, Malic acid=<span class="number">0.00</span></div><div class="line"></div><div class="line">Max-value after min-max scaling:</div><div class="line">Alcohol=<span class="number">1.00</span>, Malic acid=<span class="number">1.00</span></div></pre></td></tr></table></figure><h4 id="Plotting"><a href="#Plotting" class="headerlink" title="Plotting"></a>Plotting</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">()</span>:</span></div><div class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</div><div class="line"></div><div class="line">    plt.scatter(df[<span class="string">'Alcohol'</span>], df[<span class="string">'Malic acid'</span>],</div><div class="line">            color=<span class="string">'green'</span>, label=<span class="string">'input scale'</span>, alpha=<span class="number">0.5</span>)</div><div class="line"></div><div class="line">    plt.scatter(df_std[:,<span class="number">0</span>], df_std[:,<span class="number">1</span>], color=<span class="string">'red'</span>,</div><div class="line">            label=<span class="string">'Standardized [$$N  (\mu=0, \; \sigma=1)$$]'</span>, alpha=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">    plt.scatter(df_minmax[:,<span class="number">0</span>], df_minmax[:,<span class="number">1</span>],</div><div class="line">            color=<span class="string">'blue'</span>, label=<span class="string">'min-max scaled [min=0, max=1]'</span>, alpha=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">    plt.title(<span class="string">'Alcohol and Malic Acid content of the wine dataset'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Alcohol'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Malic Acid'</span>)</div><div class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</div><div class="line">    plt.grid()</div><div class="line"></div><div class="line">    plt.tight_layout()</div><div class="line"></div><div class="line">plot()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_44_0.png" alt="png"></p><p>The plot above includes the wine datapoints on all three different scales: the input scale where the alcohol content was measured in volume-percent (green), the standardized features (red), and the normalized features (blue). In the following plot, we will zoom in into the three different axis-scales.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">fig, ax = plt.subplots(<span class="number">3</span>, figsize=(<span class="number">6</span>,<span class="number">14</span>))</div><div class="line"></div><div class="line"><span class="keyword">for</span> a,d,l <span class="keyword">in</span> zip(range(len(ax)),</div><div class="line">               (df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]].values, df_std, df_minmax),</div><div class="line">               (<span class="string">'Input scale'</span>,</div><div class="line">                <span class="string">'Standardized [$$N  (\mu=0, \; \sigma=1)$$]'</span>,</div><div class="line">                <span class="string">'min-max scaled [min=0, max=1]'</span>)</div><div class="line">                ):</div><div class="line">    <span class="keyword">for</span> i,c <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>)):</div><div class="line">        ax[a].scatter(d[df[<span class="string">'Class label'</span>].values == i, <span class="number">0</span>],</div><div class="line">                  d[df[<span class="string">'Class label'</span>].values == i, <span class="number">1</span>],</div><div class="line">                  alpha=<span class="number">0.5</span>,</div><div class="line">                  color=c,</div><div class="line">                  label=<span class="string">'Class %s'</span> %i</div><div class="line">                  )</div><div class="line">    ax[a].set_title(l)</div><div class="line">    ax[a].set_xlabel(<span class="string">'Alcohol'</span>)</div><div class="line">    ax[a].set_ylabel(<span class="string">'Malic Acid'</span>)</div><div class="line">    ax[a].legend(loc=<span class="string">'upper left'</span>)</div><div class="line">    ax[a].grid()</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_48_0.png" alt="png"></p><h2 id="Bottom-up-approaches"><a href="#Bottom-up-approaches" class="headerlink" title="Bottom-up approaches"></a>Bottom-up approaches</h2><p>Of course, we can also code the equations for standardization and 0-1 Min-Max scaling “manually”. However, the scikit-learn methods are still useful if you are working with test and training data sets and want to scale them equally.</p><p>E.g.,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">std_scale = preprocessing.StandardScaler().fit(X_train)</div><div class="line">X_train = std_scale.transform(X_train)</div><div class="line">X_test = std_scale.transform(X_test)</div></pre></td></tr></table></figure><p>Below, we will perform the calculations using “pure” Python code, and an more convenient NumPy solution, which is especially useful if we attempt to transform a whole matrix.</p><h3 id="Vanilla-Python"><a href="#Vanilla-Python" class="headerlink" title="Vanilla Python"></a>Vanilla Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Standardization</span></div><div class="line"></div><div class="line">x = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">mean = sum(x)/len(x)</div><div class="line">std_dev = (<span class="number">1</span>/len(x) * sum([ (x_i - mean)**<span class="number">2</span> <span class="keyword">for</span> x_i <span class="keyword">in</span> x]))**<span class="number">0.5</span></div><div class="line"></div><div class="line">z_scores = [(x_i - mean)/std_dev <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</div><div class="line"></div><div class="line"><span class="comment"># Min-Max scaling</span></div><div class="line"></div><div class="line">minmax = [(x_i - min(x)) / (max(x) - min(x)) <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</div></pre></td></tr></table></figure><h3 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Standardization</span></div><div class="line"></div><div class="line">x_np = np.asarray(x)</div><div class="line">z_scores_np = (x_np - x_np.mean()) / x_np.std()</div><div class="line"></div><div class="line"><span class="comment"># Min-Max scaling</span></div><div class="line"></div><div class="line">np_minmax = (x_np - x_np.min()) / (x_np.max() - x_np.min())</div></pre></td></tr></table></figure><h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><p>Just to make sure that our code works correctly, let us plot the results via matplotlib.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=<span class="number">2</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line"></div><div class="line">y_pos = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x))]</div><div class="line"></div><div class="line">ax1.scatter(z_scores, y_pos, color=<span class="string">'g'</span>)</div><div class="line">ax1.set_title(<span class="string">'Python standardization'</span>, color=<span class="string">'g'</span>)</div><div class="line"></div><div class="line">ax2.scatter(minmax, y_pos, color=<span class="string">'g'</span>)</div><div class="line">ax2.set_title(<span class="string">'Python Min-Max scaling'</span>, color=<span class="string">'g'</span>)</div><div class="line"></div><div class="line">ax3.scatter(z_scores_np, y_pos, color=<span class="string">'b'</span>)</div><div class="line">ax3.set_title(<span class="string">'Python NumPy standardization'</span>, color=<span class="string">'b'</span>)</div><div class="line"></div><div class="line">ax4.scatter(np_minmax, y_pos, color=<span class="string">'b'</span>)</div><div class="line">ax4.set_title(<span class="string">'Python NumPy Min-Max scaling'</span>, color=<span class="string">'b'</span>)</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> (ax1, ax2, ax3, ax4):</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.grid()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_64_0.png" alt="png"></p><h2 id="The-effect-of-standardization-on-PCA-in-a-pattern-classification-task"><a href="#The-effect-of-standardization-on-PCA-in-a-pattern-classification-task" class="headerlink" title="The effect of standardization on PCA in a pattern classification task"></a>The effect of standardization on PCA in a pattern classification task</h2><p>Earlier, I mentioned the Principal Component Analysis (PCA) as an example where standardization is crucial, since it is “analyzing” the variances of the different features.<br>Now, let us see how the standardization affects PCA and a following supervised classification on the whole wine dataset.</p><p>In the following section, we will go through the following steps:</p><ul><li>Reading in the dataset</li><li>Dividing the dataset into a separate training and test dataset</li><li>Standardization of the features</li><li>Principal Component Analysis (PCA) to reduce the dimensionality</li><li>Training a naive Bayes classifier</li><li>Evaluating the classification accuracy with and without standardization</li></ul><h3 id="Reading-in-the-dataset"><a href="#Reading-in-the-dataset" class="headerlink" title="Reading in the dataset"></a>Reading in the dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">df = pd.io.parsers.read_csv(</div><div class="line">    <span class="string">'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv'</span>,</div><div class="line">    header=<span class="keyword">None</span>,</div><div class="line">    )</div></pre></td></tr></table></figure><h3 id="Dividing-the-dataset-into-a-separate-training-and-test-dataset"><a href="#Dividing-the-dataset-into-a-separate-training-and-test-dataset" class="headerlink" title="Dividing the dataset into a separate training and test dataset"></a>Dividing the dataset into a separate training and test dataset</h3><p>In this step, we will randomly divide the wine dataset into a training dataset and a test dataset where the training dataset will contain 70% of the samples and the test dataset will contain 30%, respectively.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line">X_wine = df.values[:,<span class="number">1</span>:]</div><div class="line">y_wine = df.values[:,<span class="number">0</span>]</div><div class="line"></div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine,</div><div class="line">    test_size=<span class="number">0.30</span>, random_state=<span class="number">12345</span>)</div></pre></td></tr></table></figure><h3 id="Feature-Scaling-Standardization"><a href="#Feature-Scaling-Standardization" class="headerlink" title="Feature Scaling - Standardization"></a>Feature Scaling - Standardization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line">std_scale = preprocessing.StandardScaler().fit(X_train)</div><div class="line">X_train_std = std_scale.transform(X_train)</div><div class="line">X_test_std = std_scale.transform(X_test)</div></pre></td></tr></table></figure><h3 id="Dimensionality-reduction-via-Principal-Component-Analysis-PCA"><a href="#Dimensionality-reduction-via-Principal-Component-Analysis-PCA" class="headerlink" title="Dimensionality reduction via Principal Component Analysis (PCA)"></a>Dimensionality reduction via Principal Component Analysis (PCA)</h3><p>Now, we perform a PCA on the standardized and the non-standardized datasets to transform the dataset onto a 2-dimensional feature subspace.<br>In a real application, a procedure like cross-validation would be done in order to find out what choice of features would yield a optimal balance between “preserving information” and “overfitting” for different classifiers. However, we will omit this step since we don’t want to train a perfect classifier here, but merely compare the effects of standardization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"></div><div class="line"><span class="comment"># on non-standardized data</span></div><div class="line">pca = PCA(n_components=<span class="number">2</span>).fit(X_train)</div><div class="line">X_train = pca.transform(X_train)</div><div class="line">X_test = pca.transform(X_test)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># om standardized data</span></div><div class="line">pca_std = PCA(n_components=<span class="number">2</span>).fit(X_train_std)</div><div class="line">X_train_std = pca_std.transform(X_train_std)</div><div class="line">X_test_std = pca_std.transform(X_test_std)</div></pre></td></tr></table></figure><p>Let us quickly visualize how our new feature subspace looks like (note that class labels are not considered in a PCA - in contrast to a Linear Discriminant Analysis - but I will add them in the plot for clarity).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">fig, (ax1, ax2) = plt.subplots(ncols=<span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">4</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">for</span> l,c,m <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'blue'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>), (<span class="string">'^'</span>, <span class="string">'s'</span>, <span class="string">'o'</span>)):</div><div class="line">    ax1.scatter(X_train[y_train==l, <span class="number">0</span>], X_train[y_train==l, <span class="number">1</span>],</div><div class="line">        color=c,</div><div class="line">        label=<span class="string">'class %s'</span> %l,</div><div class="line">        alpha=<span class="number">0.5</span>,</div><div class="line">        marker=m</div><div class="line">        )</div><div class="line"></div><div class="line"><span class="keyword">for</span> l,c,m <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'blue'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>), (<span class="string">'^'</span>, <span class="string">'s'</span>, <span class="string">'o'</span>)):</div><div class="line">    ax2.scatter(X_train_std[y_train==l, <span class="number">0</span>], X_train_std[y_train==l, <span class="number">1</span>],</div><div class="line">        color=c,</div><div class="line">        label=<span class="string">'class %s'</span> %l,</div><div class="line">        alpha=<span class="number">0.5</span>,</div><div class="line">        marker=m</div><div class="line">        )</div><div class="line"></div><div class="line">ax1.set_title(<span class="string">'Transformed NON-standardized training dataset after PCA'</span>)    </div><div class="line">ax2.set_title(<span class="string">'Transformed standardized training dataset after PCA'</span>)    </div><div class="line"></div><div class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> (ax1, ax2):</div><div class="line"></div><div class="line">    ax.set_xlabel(<span class="string">'1st principal component'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'2nd principal component'</span>)</div><div class="line">    ax.legend(loc=<span class="string">'upper right'</span>)</div><div class="line">    ax.grid()</div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_89_0.png" alt="png"></p><h3 id="Training-a-naive-Bayes-classifier"><a href="#Training-a-naive-Bayes-classifier" class="headerlink" title="Training a naive Bayes classifier"></a>Training a naive Bayes classifier</h3><p>We will use a naive Bayes classifier for the classification task. If you are not familiar with it, the term “naive” comes from the assumption that all features are “independent”.<br>All in all, it is a simple but robust classifier based on Bayes’ rule</p><p>I don’t want to get into more detail about Bayes’ rule in this article, but if you are interested in a more detailed collection of examples, please have a look at the <a href="https://github.com/rasbt/pattern_classification#statistical-pattern-recognition-examples" target="_blank" rel="external">Statistical Patter Classification</a> in my pattern classification repository.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"></div><div class="line"><span class="comment"># on non-standardized data</span></div><div class="line">gnb = GaussianNB()</div><div class="line">fit = gnb.fit(X_train, y_train)</div><div class="line"></div><div class="line"><span class="comment"># on standardized data</span></div><div class="line">gnb_std = GaussianNB()</div><div class="line">fit_std = gnb_std.fit(X_train_std, y_train)</div></pre></td></tr></table></figure><h3 id="Evaluating-the-classification-accuracy-with-and-without-standardization"><a href="#Evaluating-the-classification-accuracy-with-and-without-standardization" class="headerlink" title="Evaluating the classification accuracy with and without standardization"></a>Evaluating the classification accuracy with and without standardization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"></div><div class="line">pred_train = gnb.predict(X_train)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the training dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;'</span>.format(metrics.accuracy_score(y_train, pred_train)))</div><div class="line"></div><div class="line">pred_test = gnb.predict(X_test)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the test dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;\n'</span>.format(metrics.accuracy_score(y_test, pred_test)))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Prediction accuracy <span class="keyword">for</span> the training dataset</div><div class="line"><span class="number">81.45</span>%</div><div class="line"></div><div class="line">Prediction accuracy <span class="keyword">for</span> the test dataset</div><div class="line"><span class="number">64.81</span>%</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">pred_train_std = gnb_std.predict(X_train_std)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the training dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;'</span>.format(metrics.accuracy_score(y_train, pred_train_std)))</div><div class="line"></div><div class="line">pred_test_std = gnb_std.predict(X_test_std)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the test dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;\n'</span>.format(metrics.accuracy_score(y_test, pred_test_std)))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Prediction accuracy <span class="keyword">for</span> the training dataset</div><div class="line"><span class="number">96.77</span>%</div><div class="line"></div><div class="line">Prediction accuracy <span class="keyword">for</span> the test dataset</div><div class="line"><span class="number">98.15</span>%</div></pre></td></tr></table></figure><p>As we can see, the standardization prior to the PCA definitely led to an decrease in the empirical error rate on classifying samples from test dataset.</p><h2 id="Appendix-A-The-effect-of-scaling-and-mean-centering-of-variables-prior-to-PCA"><a href="#Appendix-A-The-effect-of-scaling-and-mean-centering-of-variables-prior-to-PCA" class="headerlink" title="Appendix A: The effect of scaling and mean centering of variables prior to PCA"></a>Appendix A: The effect of scaling and mean centering of variables prior to PCA</h2><p>Let us think about whether it matters or not if the variables are centered for applications such as Principal Component Analysis (PCA) if the PCA is calculated from the covariance matrix (i.e., the kkprincipal components are the eigenvectors of the covariance matrix that correspond to the kk largest eigenvalues.</p><h3 id="1-Mean-centering-does-not-affect-the-covariance-matrix"><a href="#1-Mean-centering-does-not-affect-the-covariance-matrix" class="headerlink" title="1. Mean centering does not affect the covariance matrix"></a>1. Mean centering does not affect the covariance matrix</h3><p>Here, the rational is: If the covariance is the same whether the variables are centered or not, the result of the PCA will be the same.</p><p>Let’s assume we have the 2 variables $x$ and $y$ Then the covariance between the attributes is calculated as<br>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>Let us write the centered variables as<br>$$<br>x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}<br>$$<br>The centered covariance would then be calculated as follows:<br>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i’ - \bar{y}’)<br>$$<br>But since after centering, $\bar{x}’ = 0$ and $\bar{y}’ = 0$ we have</p><p>$\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} x_i’ y_i’$ which is our original covariance matrix if we resubstitute back the terms $x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}$.</p><p>Even centering only one variable, e.g., xx wouldn’t affect the covariance:</p><p>$$<br>\sigma_{\text{xy}} = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i - \bar{y})<br>$$</p><h3 id="2-Scaling-of-variables-does-affect-the-covariance-matrix"><a href="#2-Scaling-of-variables-does-affect-the-covariance-matrix" class="headerlink" title="2. Scaling of variables does affect the covariance matrix"></a>2. Scaling of variables does affect the covariance matrix</h3><p>If one variable is scaled, e.g, from pounds into kilogram (1 pound = 0.453592 kg), it does affect the covariance and therefore influences the results of a PCA.</p><p>Let cc be the scaling factor for $x$</p><p>Given that the “original” covariance is calculated as</p><p>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>the covariance after scaling would be calculated as:</p><p>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (c \cdot x_i - c \cdot \bar{x})(y_i - \bar{y}) = \frac{c}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y}) \Rightarrow \sigma_{xy}’ = c \cdot \sigma_{xy}<br>$$<br>Therefore, the covariance after scaling one attribute by the constant $c$ will result in a rescaled covariance $c \sigma_{xy}$ So if we’d scaled $x$ from pounds to kilograms, the covariance between $x$ and $y$ will be 0.453592 times smaller.</p><h3 id="3-Standardizing-affects-the-covariance"><a href="#3-Standardizing-affects-the-covariance" class="headerlink" title="3. Standardizing affects the covariance"></a>3. Standardizing affects the covariance</h3><p>Standardization of features will have an effect on the outcome of a PCA (assuming that the variables are originally not standardized). This is because we are scaling the covariance between every pair of variables by the product of the standard deviations of each pair of variables.</p><p>The equation for standardization of a variable is written as</p><p>$$<br>z = \frac{x_i - \bar{x}}{\sigma}<br>$$<br>The “original” covariance matrix:</p><p>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>And after standardizing both variables:</p><p>$$<br>x’ = \frac{x - \bar{x}}{\sigma_x} \text{ and } y’ =\frac{y - \bar{y}}{\sigma_y}<br>$$</p><p>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - 0)(y_i’ - 0) = \frac{1}{n-1} \sum_{i}^{n} \bigg(\frac{x - \bar{x}}{\sigma_x}\bigg)\bigg(\frac{y - \bar{y}}{\sigma_y}\bigg) = \frac{1}{(n-1) \cdot \sigma_x \sigma_y} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$</p><p>$$<br>\Rightarrow \sigma_{xy}’ = \frac{\sigma_{xy}}{\sigma_x \sigma_y}<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;About-standardization&quot;&gt;&lt;a href=&quot;#About-standardization&quot; class=&quot;headerlink&quot; title=&quot;About standardization&quot;&gt;&lt;/a&gt;About standardization&lt;/
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Install SerpentAI on Windows 10</title>
    <link href="http://yoursite.com/2018/06/07/Install-SerpentAI-on-Windows-10/"/>
    <id>http://yoursite.com/2018/06/07/Install-SerpentAI-on-Windows-10/</id>
    <published>2018-06-07T13:50:00.000Z</published>
    <updated>2018-06-08T06:45:10.253Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Python-Environment"><a href="#Python-Environment" class="headerlink" title="Python Environment"></a>Python Environment</h2><h3 id="Python-3-6-with-Anaconda"><a href="#Python-3-6-with-Anaconda" class="headerlink" title="Python 3.6+ (with Anaconda)"></a>Python 3.6+ (with Anaconda)</h3><p>Serpent.AI was developed taking full advantage of Python 3.6 so it is only natural that the Python requirement be for versions 3.6 and up.</p><p>Installing regular Python 3.6+ isn’t exactly difficult but Serpent.AI relies on a good amount of scientific computing libraries that are extremely difficult / impossible to compile on your own on Windows. Thankfully, the <a href="https://www.anaconda.com/distribution" target="_blank" rel="external">Anaconda Distribution</a> exists and takes this huge weight off our collective shoulders.</p><h4 id="Installing-Anaconda-5-2-0-Python-3-6"><a href="#Installing-Anaconda-5-2-0-Python-3-6" class="headerlink" title="Installing Anaconda 5.2.0 (Python 3.6)"></a>Installing Anaconda 5.2.0 (Python 3.6)</h4><p><a href="https://www.anaconda.com/download/" target="_blank" rel="external">Download</a> the Python 3.6 version of Anaconda 5.2.0 and run the graphical installer.</p><p>The following commands are to be performed in an <em>Anaconda Prompt</em> with elevated privileges (Right click and <strong>Run as Administrator</strong>). It is recommended to create a shortcut to this prompt because every Python and Serpent command will have to be performed from there starting now.</p><h4 id="Creating-a-Conda-Env-for-Serpent-AI"><a href="#Creating-a-Conda-Env-for-Serpent-AI" class="headerlink" title="Creating a Conda Env for Serpent.AI"></a>Creating a Conda Env for Serpent.AI</h4><p><code>conda create --name serpent python=3.6</code> (‘serpent’ can be replaced with another name)</p><h4 id="Creating-a-directory-for-your-Serpent-AI-projects"><a href="#Creating-a-directory-for-your-Serpent-AI-projects" class="headerlink" title="Creating a directory for your Serpent.AI projects"></a>Creating a directory for your Serpent.AI projects</h4><p><code>mkdir SerpentAI &amp;&amp; cd SerpentAI</code></p><h4 id="Activating-the-Conda-Env"><a href="#Activating-the-Conda-Env" class="headerlink" title="Activating the Conda Env"></a>Activating the Conda Env</h4><p><code>conda activate serpent</code></p><h2 id="3rd-Party-Dependencies"><a href="#3rd-Party-Dependencies" class="headerlink" title="3rd-Party Dependencies"></a>3rd-Party Dependencies</h2><h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><p>Redis is used in the framework as the in-memory store for the captured frame buffers as well as the temporary storage of analytics events. It is not meant to be compatible with Windows! Microsoft used to <a href="https://github.com/MicrosoftArchive/redis" target="_blank" rel="external">maintain a port</a> but it’s been abandoned since. This being said, that Redis version is sufficient and it outperforms stuff like running it in WSL on Windows 10. It will install as a Windows Service. Make sure you set it to start automatically.</p><h4 id="Install-Windows-Subsystem-for-Linux-WSL"><a href="#Install-Windows-Subsystem-for-Linux-WSL" class="headerlink" title="Install Windows Subsystem for Linux (WSL)"></a><a href="https://msdn.microsoft.com/en-us/commandline/wsl/install_guide" target="_blank" rel="external">Install Windows Subsystem for Linux (WSL)</a></h4><ol><li>From Start, search for <strong>Turn Windows features on or off</strong> (type <code>turn</code>)</li><li><strong>Select Windows Subsystem for Linux (beta)</strong></li></ol><p><a href="https://raw.githubusercontent.com/ServiceStack/Assets/master/img/redis/install-wsl.png" target="_blank" rel="external"><img src="https://raw.githubusercontent.com/ServiceStack/Assets/master/img/redis/install-wsl.png" alt="img"></a></p><p>Once installed you can run bash on Ubuntu by typing <strong>bash</strong> from a Windows Command Prompt. To install the latest version of Redis we’ll need to use a repository that maintains up-to-date packages for Ubuntu and Debian servers like <a href="https://www.dotdeb.org/" target="_blank" rel="external">https://www.dotdeb.org</a> which you can add to Ubuntu’s apt-get sources with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ echo deb http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list</div><div class="line">$ echo deb-src http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list</div><div class="line">$ sudo mv dotdeb.org.list /etc/apt/sources.list.d</div><div class="line">$ wget -q -O - http://www.dotdeb.org/dotdeb.gpg | sudo apt-key add -</div></pre></td></tr></table></figure><p>Then after updating our APT cache we can install Redis with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install redis-server</div></pre></td></tr></table></figure><p>You’ll then be able to launch redis with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ redis-server --daemonize yes</div></pre></td></tr></table></figure><p>Which will run redis in the background freeing your shell so you can play with it using the redis client:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ redis-cli</div><div class="line">$ 127.0.0.1:6379&gt; SET foo bar</div><div class="line">OK</div><div class="line">$ 127.0.0.1:6379&gt; GET foo</div><div class="line">&quot;bar&quot;</div></pre></td></tr></table></figure><p>Which you can connect to from within bash or from your Windows desktop using the <a href="https://github.com/ServiceStack/redis-windows#option-3-running-microsofts-native-port-of-redis" target="_blank" rel="external">redis-cli native Windows binary from MSOpenTech</a>.</p><h3 id="Build-Tools-for-Visual-Studio-2017"><a href="#Build-Tools-for-Visual-Studio-2017" class="headerlink" title="Build Tools for Visual Studio 2017"></a>Build Tools for Visual Studio 2017</h3><p>Some of the packages that will be installed alongside Serpent.AI are not pre-compiled binaries and will be need to be built from source. This is a little more problematic for Windows but with the correct C++ Build Tools for Visual Studio it all goes down smoothly.</p><p>You can get the proper installer by visiting <a href="https://www.visualstudio.com/downloads/" target="_blank" rel="external">https://www.visualstudio.com/downloads/</a> and scrolling down to the <em>Build Tools for Visual Studio 2017</em> download. Download, run, select the <em>Visual C++ build tools</em> section and make sure the following components are checked (VSs are not installed):</p><ul><li>Visual C++ Build Tools core features</li><li>VC++ 2017 version 15.7 v14.14 latest v141 tools</li><li>Visual C++ 2017 Redistributable Update</li><li>VC++ 2015.3 v14.00 (v140) toolset for desktop</li><li>Windows 10 SDK (10.0.17134.0)</li><li>Windows Universal CRT SDK</li></ul><h2 id="Installing-Serpent-AI"><a href="#Installing-Serpent-AI" class="headerlink" title="Installing Serpent.AI"></a>Installing Serpent.AI</h2><p>Once all of the above had been installed and set up, you are ready to install the framework. Remember that PATH changes in Windows are not reflected in your command prompts that were opened while you made the changes. Open a fresh Anaconda prompt before continuing to avoid installation issues.</p><p>Go back to the directory you created earlier for your Serpent.AI projects. Make sure you are scoped in your Conda Env.</p><p>Run <code>pip install SerpentAI</code></p><p>Then run <code>serpent setup</code> to install the remaining dependencies automatically.</p><h2 id="Installing-Optional-Modules"><a href="#Installing-Optional-Modules" class="headerlink" title="Installing Optional Modules"></a>Installing Optional Modules</h2><p>In the spirit of keeping the initial installation on the light side, some specialized / niche components with extra dependencies have been isolated from the core. It is recommended to only focus on installing them once you reach a point where you actually need them. The framework will provide a warning when a feature you are trying to use requires one of those modules.</p><h3 id="OCR"><a href="#OCR" class="headerlink" title="OCR"></a>OCR</h3><p>A module to provide OCR functionality in your game agents.</p><h4 id="Tesseract"><a href="#Tesseract" class="headerlink" title="Tesseract"></a>Tesseract</h4><p>Serpent.AI leverages Tesseract for its OCR functionality. You can install Tesseract for Windows by following these steps:</p><ol><li>Visit <a href="https://github.com/UB-Mannheim/tesseract/wiki" target="_blank" rel="external">https://github.com/UB-Mannheim/tesseract/wiki</a></li><li>Download the .exe for version 3</li><li>Run the graphical installer (Remember the install path!)</li><li>Add the path to <em>tesseract.exe</em> to your %PATH% environment variable</li></ol><p>You can test your Tesseract installation by opening an Anaconda Prompt and executing <code>tesseract --list-langs</code>.</p><h4 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h4><p>Once you’ve validated that Tesseract has been properly set up, you can install the module with <code>serpent setup ocr</code></p><h3 id="GUI"><a href="#GUI" class="headerlink" title="GUI"></a>GUI</h3><p>A module to allow Serpent.AI desktop app to run.</p><h4 id="Kivy"><a href="#Kivy" class="headerlink" title="Kivy"></a>Kivy</h4><p>Kivy is the GUI framework used in the framework.</p><p>Once you are ready to test your Kivy, you can install the module with <code>serpent setup gui</code> and try to run <code>serpent visual_debugger</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Python-Environment&quot;&gt;&lt;a href=&quot;#Python-Environment&quot; class=&quot;headerlink&quot; title=&quot;Python Environment&quot;&gt;&lt;/a&gt;Python Environment&lt;/h2&gt;&lt;h3 id=&quot;P
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Matching Networks for One Shot Learning</title>
    <link href="http://yoursite.com/2018/06/02/Matching-Networks-for-One-Shot-Learning/"/>
    <id>http://yoursite.com/2018/06/02/Matching-Networks-for-One-Shot-Learning/</id>
    <published>2018-06-02T14:35:08.000Z</published>
    <updated>2018-06-02T14:39:51.593Z</updated>
    
    <content type="html"><![CDATA[<p>By DeepMind crew: <strong>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra</strong></p><p>This is a paper on <strong>one-shot</strong> learning, where we’d like to learn a class based on very few (or indeed, 1) training examples. E.g. it suffices to show a child a single giraffe, not a few hundred thousands before it can recognize more giraffes.</p><p>This paper falls into a category of <em>“duh of course”</em> kind of paper, something very interesting, powerful, but somehow obvious only in retrospect. I like it.</p><p>Suppose you’re given a single example of some class and would like to label it in test images.</p><ul><li><strong>Observation 1</strong>: a standard approach might be to train an Exemplar SVM for this one (or few) examples vs. all the other training examples - i.e. a linear classifier. But this requires optimization.</li><li><strong>Observation 2:</strong> known non-parameteric alternatives (e.g. k-Nearest Neighbor) don’t suffer from this problem. E.g. I could immediately use a Nearest Neighbor to classify the new class without having to do any optimization whatsoever. However, NN is gross because it depends on an (arbitrarily-chosen) metric, e.g. L2 distance. Ew.</li><li><strong>Core idea</strong>: lets train a fully end-to-end nearest neighbor classifer!<img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2010.08.44%20PM.png" alt="Screen Shot 2016-08-07 at 10.08.44 PM"></li></ul><h2 id="The-training-protocol"><a href="#The-training-protocol" class="headerlink" title="The training protocol"></a>The training protocol</h2><p>As the authors amusingly point out in the conclusion (and this is the <em>duh of course</em> part), <em>“one-shot learning is much easier if you train the network to do one-shot learning”</em>. Therefore, we want the test-time protocol (given N novel classes with only k examples each (e.g. k = 1 or 5), predict new instances to one of N classes) to exactly match the training time protocol.</p><p>To create each “episode” of training from a dataset of examples then:</p><ol><li>Sample a task T from the training data, e.g. select 5 labels, and up to 5 examples per label (i.e. 5-25 examples).</li><li>To form one episode sample a label set L (e.g. {cats, dogs}) and then use L to sample the support set S and a batch B of examples to evaluate loss on.</li></ol><p>The idea on high level is clear but the writing here is a bit unclear on details, of exactly how the sampling is done.</p><h2 id="The-model"><a href="#The-model" class="headerlink" title="The model"></a>The model</h2><p>I find the paper’s model description slightly wordy and unclear, but basically we’re building a <strong>differentiable nearest neighbor++</strong>. The output \hat{y} for a test example \hat{x} is computed very similar to what you might see in Nearest Neighbors:<img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.14.26%20PM.png" alt="Screen Shot 2016-08-07 at 11.14.26 PM"><br>where <strong>a</strong> acts as a kernel, computing the extent to which \hat{x} is similar to a training example x_i, and then the labels from the training examples (y_i) are weight-blended together accordingly. The paper doesn’t mention this but I assume for classification y_i would presumbly be one-hot vectors.</p><p>Now, we’re going to embed both the training examples x_i and the test example \hat{x}, and we’ll interpret their inner products (or here a cosine similarity) as the “match”, and pass that through a softmax to get normalized mixing weights so they add up to 1. No surprises here, this is quite natural:</p><p><img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.20.29%20PM.png" alt="Screen Shot 2016-08-07 at 11.20.29 PM"><br>Here <strong>c()</strong> is cosine distance, which I presume is implemented by normalizing the two input vectors to have unit L2 norm and taking a dot product. I assume the authors tried skipping the normalization too and it did worse? Anyway, now all that’s left to define is the function <strong>f</strong> (i.e. how do we embed the test example into a vector) and the function <strong>g</strong> (i.e. how do we embed each training example into a vector?).</p><p><strong>Embedding the training examples.</strong> This (the function <strong>g</strong>) is a bidirectional LSTM over the examples:</p><p><img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.57.10%20PM.png" alt="Screen Shot 2016-08-07 at 11.57.10 PM"></p><p>i.e. encoding of i’th example x_i is a function of its “raw” embedding g’(x_i) and the embedding of its friends, communicated through the bidirectional network’s hidden states. i.e. each training example is a function of not just itself but all of its friends in the set. This is part of the ++ above, because in a normal nearest neighbor you wouldn’t change the representation of an example as a function of the other data points in the training set.</p><p>It’s odd that the <strong>order</strong> is not mentioned, I assume it’s random? This is a bit gross because order matters to a bidirectional LSTM; you’d get different embeddings if you permute the examples.</p><p><strong>Embedding the test example.</strong> This (the function <strong>f</strong>) is a an LSTM that processes for a fixed amount (K time steps) and at each point also <em>attends</em> over the examples in the training set. The encoding is the last hidden state of the LSTM. Again, this way we’re allowing the network to change its encoding of the test example as a function of the training examples. Nifty: <img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2012.11.15%20AM.png" alt="Screen Shot 2016-08-08 at 12.11.15 AM"></p><p>That looks scary at first but it’s really just a vanilla LSTM with attention where the input at each time step is constant (f’(\hat{x}), an encoding of the test example all by itself) and the hidden state is a function of previous hidden state but also a concatenated readout vector <strong>r</strong>, which we obtain by attending over the encoded training examples (encoded with <strong>g</strong> from above).</p><p>Oh and I assume there is a typo in equation (5), it should say r_k = … without the -1 on LHS.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>Task</strong>: N-way k-shot learning task. i.e. we’re given k (e.g. 1 or 5) labelled examples for N classes that we have not previously trained on and asked to classify new instances into he N classes.</p><p><strong>Baselines:</strong> an “obvious” strategy of using a pretrained ConvNet and doing nearest neighbor based on the codes. An option of finetuning the network on the new examples as well (requires training and careful and strong regularization!).</p><p><strong>MANN</strong> of Santoro et al. [21]: Also a DeepMind paper, a fun NTM-like Meta-Learning approach that is fed a sequence of examples and asked to predict their labels.</p><p><strong>Siamese network</strong> of Koch et al. [11]: A siamese network that takes two examples and predicts whether they are from the same class or not with logistic regression. A test example is labeled with a nearest neighbor: with the class it matches best according to the siamese net (requires iteration over all training examples one by one). Also, this approach is less end-to-end than the one here because it requires the ad-hoc nearest neighbor matching, while here the <em>exact</em> end task is optimized for. It’s beautiful.</p><h3 id="Omniglot-experiments"><a href="#Omniglot-experiments" class="headerlink" title="Omniglot experiments"></a>Omniglot experiments</h3><h3><a href="#" class="headerlink"></a><img src="https://github.com/karpathy/paper-notes/raw/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2010.21.45%20AM.png" alt="Screen Shot 2016-08-08 at 10.21.45 AM"></h3><p>Omniglot of <a href="http://www.cs.toronto.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf" target="_blank" rel="external">Lake et al. [14]</a> is a MNIST-like scribbles dataset with 1623 characters with 20 examples each.</p><p>Image encoder is a CNN with 4 modules of [3x3 CONV 64 filters, batchnorm, ReLU, 2x2 max pool]. The original image is claimed to be so resized from original 28x28 to 1x1x64, which doesn’t make sense because factor of 2 downsampling 4 times is reduction of 16, and 28/16 is a non-integer &gt;1. I’m assuming they use VALID convs?</p><p>Results: <img src="https://github.com/karpathy/paper-notes/raw/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2010.27.46%20AM.png" alt="Screen Shot 2016-08-08 at 10.27.46 AM"></p><p>Matching nets do best. Fully Conditional Embeddings (FCE) by which I mean they the “Full Context Embeddings” of Section 2.1.2 instead are not used here, mentioned to not work much better. Finetuning helps a bit on baselines but not with Matching nets (weird).</p><p>The comparisons in this table are somewhat confusing:</p><ul><li>I can’t find the MANN numbers of 82.8% and 94.9% in their paper [21]; not clear where they come from. E.g. for 5 classes and 5-shot they seem to report 88.4% not 94.9% as seen here. I must be missing something.</li><li>I also can’t find the numbers reported here in the Siamese Net [11] paper. As far as I can tell in their Table 2 they report one-shot accuracy, 20-way classification to be 92.0, while here it is listed as 88.1%?</li><li>The results of Lake et al. [14] who proposed Omniglot are also missing from the table. If I’m understanding this correctly they report 95.2% on 1-shot 20-way, while matching nets here show 93.8%, and humans are estimated at 95.5%. That is, the results here appear weaker than those of Lake et al., but one should keep in mind that the method here is significantly more generic and does not make any assumptions about the existence of strokes, etc., and it’s a simple, single fully-differentiable blob of neural stuff.</li></ul><p>(skipping ImageNet/LM experiments as there are few surprises)</p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Good paper, effectively develops a differentiable nearest neighbor trained end-to-end. It’s something new, I like it!</p><p>A few concerns:</p><ul><li><p>A bidirectional LSTMs (not order-invariant compute) is applied over sets of training examples to encode them. The authors don’t talk about the order actually used, which presumably is random, or mention this potentially unsatisfying feature. This can be solved by using a recurrent attentional mechanism instead, as the authors are certainly aware of and as has been discussed at length in <a href="https://arxiv.org/abs/1511.06391" target="_blank" rel="external">ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS</a>, where Oriol is also the first author. I wish there was a comment on this point in the paper somewhere.</p></li><li><p>The approach also gets quite a bit slower as the number of training examples grow, but once this number is large one would presumable switch over to a parameteric approach.</p></li><li><p>It’s also potentially concerning that during training the method uses a specific number of examples, e.g. 5-25, so this is the number of that must also be used at test time. What happens if we want the size of our training set to grow online? It appears that we need to retrain the network because the encoder LSTM for the training data is not “used to” seeing inputs of more examples? That is unless you fall back to iteratively subsampling the training data, doing multiple inference passes and averaging, or something like that. If we don’t use FCE it can still be that the attention mechanism LSTM can still not be “used to” attending over many more examples, but it’s not clear how much this matters. An interesting experiment would be to not use FCE and try to use 100 or 1000 training examples, while only training on up to 25 (with and fithout FCE). Discussion surrounding this point would be interesting.</p></li><li><p>Not clear what happened with the Omniglot experiments, with incorrect numbers for [11], [21], and the exclusion of Lake et al. [14] comparison.</p></li><li><p>A baseline that is missing would in my opinion also include training of an <a href="https://www.cs.cmu.edu/~tmalisie/projects/iccv11/" target="_blank" rel="external">Exemplar SVM</a>, which is a much more powerful approach than encode-with-a-cnn-and-nearest-neighbor.</p><p>​</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;By DeepMind crew: &lt;strong&gt;Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This is a p
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="one-shot learning" scheme="http://yoursite.com/tags/one-shot-learning/"/>
    
  </entry>
  
  <entry>
    <title>One Shot Learning and Siamese Networks in Keras</title>
    <link href="http://yoursite.com/2018/06/02/One-Shot-Learning-and-Siamese-Networks-in-Keras/"/>
    <id>http://yoursite.com/2018/06/02/One-Shot-Learning-and-Siamese-Networks-in-Keras/</id>
    <published>2018-06-02T14:21:29.000Z</published>
    <updated>2018-06-02T14:32:48.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Background"><a href="#Background" class="headerlink" title="Background:"></a>Background:</h2><p>Conventional wisdom says that deep neural networks are really good at learning from high dimensional data like images or spoken language, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of <em>one-shot learning</em> - if you take a human who’s never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with astoundingly high precision.</p><p><a href="https://sorenbouma.github.io/images/spatula.jpg" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/spatula.jpg" alt="image"></a></p><p>Never been inside a kitchen before? Now’s your chance to test your one shot learning ability! which of the images on the right is of the same type as the big image? Email me for the correct answer.</p><p>..Yet another one of the <a href="https://dspace.mit.edu/handle/1721.1/6125" target="_blank" rel="external">things</a> humans can do that seemed trivial to us right up until we tried to make an algorithm do it.</p><p>This ability to rapidly learn from very little data seems like it’s obviously desirable for machine learning systems to have because collecting and labelling data is expensive. I also think this is an important step on the long road towards general intelligence.</p><p>Recently there have been <a href="https://arxiv.org/abs/1703.07326" target="_blank" rel="external">many</a> <a href="https://sorenbouma.github.io/blog/oneshot/%22%22" target="_blank" rel="external">interesting</a> <a href="https://sorenbouma.github.io/blog/oneshot/%22%22" target="_blank" rel="external">papers</a> about one-shot learning with neural nets and they’ve gotten some good results. This is a new area that really excites me, so I wanted to make a gentle introduction to make it more accessible to fellow newcomers to deep learning.</p><p>In this post, I want to:</p><ul><li>Introduce and formulate the problem of one-shot learning</li><li>Describe benchmarks for one-shot classification and give a baseline for performance</li><li>Give an example of deep one-shot learning by partially reimplementing the model in <a href="http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" target="_blank" rel="external">this paper</a> with keras.</li><li>Hopefully point out some small insights that aren’t obvious to everyone</li></ul><h2 id="Formulating-the-Problem-N-way-One-Shot-Learning"><a href="#Formulating-the-Problem-N-way-One-Shot-Learning" class="headerlink" title="Formulating the Problem - N-way One-Shot Learning"></a>Formulating the Problem - N-way One-Shot Learning</h2><p>Before we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:</p><p>Our model is given a tiny labelled training set SS, which has N examples, each vectors of the same dimension with a distinct label yy.<br>$$<br>S={(x_1,y_1),…,(x_N,y_N)}<br>$$<br>It is also given $\hat{x}$, the test example it has to classify. Since exactly one example in the support set has the right class, the aim is to correctly predict which $y \in S$ is the same as $\hat{x}$ ‘s label, $\hat{y}$.</p><p>There are fancier ways of defining the problem, but this one is ours. Here are some things to make note of:</p><ul><li>Real world problems might not always have the constraint that exactly one image has the correct class</li><li>It’s easy to generalize this to k-shot learning by having there be k examples for each yiyirather than just one.</li><li>When N is higher, there are more possible classes that $\hat{x}$ can belong to, so it’s harder to predict the correct one.</li><li>Random guessing will average $\frac{100}{n}\%$ accuracy.</li></ul><p>Here are some examples of one-shot learning tasks on the Omniglot dataset, which I’ll describe in the next section.</p><p><a href="https://sorenbouma.github.io/images/task_9.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/task_9.png" alt="image"></a><a href="https://sorenbouma.github.io/images/task_25.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/task_25.png" alt="image"></a><a href="hhttps://sorenbouma.github.io/images/task_36.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/task_36.png" alt="image"></a>9, 25 and 36 way one-shot learnng tasks.</p><h2 id="About-the-data-Omniglot"><a href="#About-the-data-Omniglot" class="headerlink" title="About the data - Omniglot! :"></a>About the data - Omniglot! :</h2><p>The <a href="https://github.com/brendenlake/omniglot" target="_blank" rel="external">Omniglot dataset</a> is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are just 20 examples, each drawn by a different person at resolution 105x105.</p><p><a href="https://sorenbouma.github.io/images/alphabets/Braille.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Braille.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Bengali.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Bengali.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Sanskrit.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Sanskrit.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Greek.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Greek.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Futurama.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Futurama.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Hebrew.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Hebrew.png" alt="image"></a>A few of the alphabets from the omniglot dataset. As you can see, there’s a huge variety of different symbols.</p><p>If you like machine learning, you’ve probably heard of the <a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank" rel="external">MNIST dataset</a>. Omniglot is sometimes referred to as the <em>transpose</em> of mnist, since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits. There is also data about the strokes used to create each character, but we won’t be using that. Usually, it’s split into 30 training alphabets and 20 evaluation alphabets. All those different characters make for lots of possible one-shot tasks, so it’s a really good benchmark for one-shot learning algorithms.</p><h4 id="A-One-Shot-Learning-Baseline-1-Nearest-Neighbour"><a href="#A-One-Shot-Learning-Baseline-1-Nearest-Neighbour" class="headerlink" title="A One-Shot Learning Baseline / 1 Nearest Neighbour"></a>A One-Shot Learning Baseline / 1 Nearest Neighbour</h4><p>The simplest way of doing classification is with k-nearest neighbours, but since there is only one example per class we have to do 1 nearest neighbour. This is very simple, just calculate the Euclidean distance of the test example from each training example and pick the closest one:<br>$$<br>C(\hat{x})={\arg \min}_{c\in S}||\hat{x} − x_c||<br>$$<br>According to Koch et al, 1-nn gets ~28% accuracy in 20 way one shot classification on omniglot. 28% doesn’t sound great, but it’s nearly six times more accurate than random guessing(5%). This is a good baseline or “sanity check” to compare future one-shot algorithms with.</p><p><a href="http://cims.nyu.edu/~brenden/LakeEtAlNips2013.pdf" target="_blank" rel="external">Hierarchical Bayesian Program Learning</a> from Lake et al gets 95.2% - very impressive! The ~30% of this paper which I understood was very interesting. Comparing it with deep learning results that train on raw pixels is kind of “apples and oranges” though, because:</p><ol><li>HBPL used data about the strokes, not just the raw pixels</li><li>HBPL on omniglot involved learning a generative model for strokes. The algorithm requires data with more complicated annotation, so unlike deep learning it can’t easily be tweaked to one-shot learn from raw pixels of dogs/trucks/brain scans/spatulas and other objects that aren’t made up of brushstrokes.</li></ol><p>Lake et al also says that humans get 95.5% accuracy in 20 way classification on omniglot, only beating HBPL by a tiny margin. In the spirit of nullius in verba, I tried testing myself on the 20 way tasks and managed to average 97.2%. I wasn’t always doing true one-shot learning though - I saw several symbols I recognised, since I’m familiar with the greek alphabet, hiragana and katakana. I removed those alphabets and tried again but still managed 96.7%. My hypothesis is that having to read my own terrible handwriting has endowed me with superhuman symbol recognition ability.</p><h4 id="Ways-to-use-deep-networks-for-one-shot-learning"><a href="#Ways-to-use-deep-networks-for-one-shot-learning" class="headerlink" title="Ways to use deep networks for one shot learning?!"></a>Ways to use deep networks for one shot learning?!</h4><p>If we naively train a neural network on a one-shot as a vanilla cross-entropy-loss softmax classifier, it will <em>severely</em> overfit. Heck, even if it was a <em>hundred</em> shot learning a modern neural net would still probably overfit. Big neural networks have millions of parameters to adjust to their data and so they can learn a huge space of possible functions. (More formally, they have a high <a href="https://en.wikipedia.org/wiki/VC_dimension" target="_blank" rel="external">VC dimension</a>, which is part of why they do so well at learning from complex data with high dimensionality.) Unfortunately this strength also appears to be their undoing for one-shot learning. When there are millions of parameters to gradient descend upon, and a staggeringly huge number of possible mappings that can be learned, how can we make a network learn one that generalizes when there’s just a single example to learn from?</p><p>It’s easier for humans to one-shot learn the concept of a spatula or the letter ΘΘ because they have spent a lifetime observing and learning from similar objects. It’s not really fair to compare the performance of a human who’s spent a lifetime having to classify objects and symbols with that of a randomly initialized neural net, which imposes a very weak prior about the structure of the mapping to be learned from the data. This is why most of the one-shot learning papers I’ve seen take the approach of <em>knowledge transfer</em> from other tasks.</p><p>Neural nets are really good at extracting useful features from structurally complex/high dimensional data, such as images. If a neural network is given training data that is similar to (but not the same as) that in the one-shot task, it might be able to learn useful features which can be used in a simple learning algorithm that doesn’t require adjusting these parameters. It still counts as one-shot learning as long as the training examples are of different classes to the examples used for one-shot testing.</p><p>(NOTE: Here a <em>feature</em> means a “transformation of the data that is useful for learning”.)</p><p>So now an interesting problem is <em>how do we get a neural network to learn the features?</em> The most obvious way of doing this (if there’s labelled data) is just vanilla transfer learning - train a softmax classifier on the training set, then fine-tune the weights of the last layer on the support set of the one-shot task. In practice, neural net classifiers don’t work too well for data like omniglot where there are few examples per class, and even fine tuning only the weights in the last layer is enough to overfit the support set. Still works quite a lot better than L2 distance nearest neighbour though! (See <a href="https://arxiv.org/pdf/1606.04080" target="_blank" rel="external">Matching Networks for One Shot learning</a> for a comparison table of various deep one-shot learning methods and their accuracy.)</p><p>There’s a better way of doing it though! Remember 1 nearest neighbour? This simple, non-parametric one-shot learner just classifies the test example with the same class of whatever support example is the closest in L2 distance. This works ok, but L2 Distance suffers from the ominous sounding <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="external">curse of dimensionality</a> and so won’t work well for data with thousands of dimensions like omniglot. Also, if you have two nearly identical images and move one over a few pixels to the right the L2 distance can go from being almost zero to being really high. L2 distance is a metric that is just woefully inadequate for this task. Deep learning to the rescue? We can use a deep convolutional network to learn some kind of similarity function that a non-parametric classifer like nearest neighbor can use.</p><h3 id="Siamese-networks"><a href="#Siamese-networks" class="headerlink" title="Siamese networks"></a>Siamese networks</h3><p><a href="https://sorenbouma.github.io/images/cats.jpg" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/cats.jpg" alt="image"></a>I originally planned to have craniopagus conjoined twins as the accompanying image for this section but ultimately decided that siamese cats would go over better..</p><p><a href="http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" target="_blank" rel="external">This wonderful paper</a> is what I will be implementing in this tutorial. Koch et al’s approach to getting a neural net to do one-shot classification is to give it two images and train it to guess whether they have the same category. Then when doing a one-shot classification task described above, the network can compare the test image to each image in the support set, and pick which one it thinks is most likely to be of the same category. So we want a neural net architecture that takes two images as input and outputs the probability they share the same class.</p><p>Say x1 and x2 are two images in our dataset, and let x1∘x2 mean “x1 and x2 are images with the same class”. Note that x1∘x2 is the same as x2∘x1 - this means that if we reverse the order of the inputs to the neural network, the output should be the same - p(x1∘x2) should equal p(x2∘x1). This property is called <em>symmetry</em> and siamese nets are designed around having it.</p><p>Symmetry is important because it’s required for learning a distance metric - the distance from x1 to x2 should equal the distance x2 to x1.</p><p>If we just concatenate two examples together and use them as a single input to a neural net, each example will be matrix multiplied(or convolved) with a different set of weights, which breaks symmetry. Sure it’s possible it will eventually manage to learn the exact same weights for each input, but it would be much easier to learn a single set of weights applied to both inputs. So we could propagate both inputs through identical twin neural nets with shared parameters, then use the absolute difference as the input to a linear classifier - this is essentially what a siamese net is. Two identical twins, joined at the head, hence the name.</p><h4 id="Network-architecture"><a href="#Network-architecture" class="headerlink" title="Network architecture"></a>Network architecture</h4><p><em>Unfortunately, properly explaining how and why a convolutional neural net work would make this post twice as long. If you want to understand convnets work, I suggest checking out cs231n and then colah. For any non-dl people who are reading this, the best summary I can give of a CNN is this: An image is a 3D array of pixels. A convolutional layer is where you have a neuron connected to a tiny subgrid of pixels or neurons, and use copies of that neuron across all parts of the image/block to make another 3d array of neuron activations. A max pooling layer makes a block of activations spatially smaller. Lots of these stacked on top of one another can be trained with gradient descent and are really good at learning from images.</em></p><p>I’m going to describe the architecture pretty briefly because it’s not the important part of the paper. Koch et al uses a <em>convolutional</em> siamese network to classify pairs of omniglot images, so the twin networks are both convolutional neural nets(CNNs). The twins each have the following architecture: convolution with 64 10x10 filters, relu -&gt; max pool -&gt; convolution with 128 7x7 filters, relu -&gt; max pool -&gt; convolution with 128 4x4 filters, relu -&gt; max pool -&gt; convolution with 256 4x4 filters. The twin networks reduce their inputs down to smaller and smaller 3d tensors, finally their is a fully connected layer with 4096 units. The absolute difference between the two vectors is used as input to a linear classifier. All up, the network has 38,951,745 parameters - 96% of which belong to the fully connected layer. This is quite a lot, so the network has high capacity to overfit, but as I show below, pairwse training means the dataset size is huge so this won’t be a problem.</p><p><a href="https://sorenbouma.github.io/images/Siamese_diagram_2.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/Siamese_diagram_2.png" alt="image"></a>Hastily made architecture diagram.</p><p>The output is squashed into [0,1] with a sigmoid function to make it a probability. We use the target t=1t=1 when the images have the same class and t=0t=0 for a different class. It’s trained with logistic regression. This means the loss function should be binary cross entropy between the predictions and targets. There is also a L2 weight decay term in the loss to encourage the network to learn smaller/less noisy weights and possibly improve generalization:<br>$$<br>L(x_1,x_2,t)=t⋅\log(p(x_1∘x_2))+(1−t)⋅\log(1−p(x_1∘x_2))+λ⋅||w||^2<br>$$<br>When it does a one-shot task, the siamese net simply classifies the test image as whatever image in the support set it thinks is most similar to the test image:<br>$$<br>C(\hat{x},S) = {\arg\max}_c P(\hat{x}∘x_c),x_c∈S<br>$$<br>This uses an argmax unlike nearest neighbour which uses an argmin, because a <em>metric</em> like L2 is higher the more “different” the examples are, but this models outputs p(x1∘x2), so we want the highest. This approach has one flaw that’s obvious to me: for any xaxa in the support set,the probability $\hat{x}∘x_a$ is independent of every other example in the support set! This means the probabilities won’t sum to 1, ignores important information, namely that the test image will be the same type as exactly <em>one</em> x∈S…</p><h4 id="Observation-effective-dataset-size-in-pairwise-training"><a href="#Observation-effective-dataset-size-in-pairwise-training" class="headerlink" title="Observation: effective dataset size in pairwise training"></a>Observation: effective dataset size in pairwise training</h4><p>EDIT: After discussing this with a PhD student at UoA, I think this bit might be overstated or even just wrong. Emperically, my implementation <em>did</em> overfit, even though it wasn’t trained for enough iterations to sample every possible pair, which kind of contradicts this section. I’m leaving it up in the spirit of being wrong loudly.</p><p>One cool thing I noticed about training on pairs is that there are quadratically many possible pairs of images to train the model on, making it hard to overfit. Say we have CC examples each of EE classes. Since there are C⋅EC⋅E images total, the total number of possible pairs is given by<br>$$<br>Npairs=(C⋅E 2)=(C⋅E)!/2!(C⋅E−2)!<br>$$<br>For omniglot with its 20 examples of 964 training classes, this leads to 185,849,560 possible pairs, which is huge! However, the siamese network needs examples of both same and different class pairs. There are E examples per class, so there will be (E 2) pairs for every class, which means there are Nsame=(E 2)⋅C possible pairs with the same class - 183,160 pairs for omniglot. Even though 183,160 example pairs is plenty, it’s only a thousandth of the possible pairs, and the number of same-class pairs increases quadratically with E but only linearly with C. This is important because the siamese network should be given a 1:1 ratio of same-class and different-class pairs to train on - perhaps it implies that pairwise training is easier on datasets with lots of examples per class.</p><h3 id="The-Code"><a href="#The-Code" class="headerlink" title="The Code:"></a>The Code:</h3><p><a href="https://github.com/sorenbouma/keras-oneshot" target="_blank" rel="external">Prefer to just play with a jupyter notebook? I got you fam</a></p><p>Here is the model definition, it should be pretty easy to follow if you’ve seen keras before. I only define the twin network’s architecture once as a Sequential() model and then call it with respect to each of two input layers, this way the same parameters are used for both inputs. Then merge them together with absolute distance and add an output layer, and compile the model with binary cross entropy loss.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, Sequential</div><div class="line"><span class="keyword">from</span> keras.regularizers <span class="keyword">import</span> l2</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD,Adam</div><div class="line"><span class="keyword">from</span> keras.losses <span class="keyword">import</span> binary_crossentropy</div><div class="line"><span class="keyword">import</span> numpy.random <span class="keyword">as</span> rng</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> dill <span class="keyword">as</span> pickle</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">W_init</span><span class="params">(shape,name=None)</span>:</span></div><div class="line">    <span class="string">"""Initialize weights as in paper"""</span></div><div class="line">    values = rng.normal(loc=<span class="number">0</span>,scale=<span class="number">1e-2</span>,size=shape)</div><div class="line">    <span class="keyword">return</span> K.variable(values,name=name)</div><div class="line"><span class="comment">#//<span class="doctag">TODO:</span> figure out how to initialize layer biases in keras.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">b_init</span><span class="params">(shape,name=None)</span>:</span></div><div class="line">    <span class="string">"""Initialize bias as in paper"""</span></div><div class="line">    values=rng.normal(loc=<span class="number">0.5</span>,scale=<span class="number">1e-2</span>,size=shape)</div><div class="line">    <span class="keyword">return</span> K.variable(values,name=name)</div><div class="line"></div><div class="line">input_shape = (<span class="number">105</span>, <span class="number">105</span>, <span class="number">1</span>)</div><div class="line">left_input = Input(input_shape)</div><div class="line">right_input = Input(input_shape)</div><div class="line"><span class="comment">#build convnet to use in each siamese 'leg'</span></div><div class="line">convnet = Sequential()</div><div class="line">convnet.add(Conv2D(<span class="number">64</span>,(<span class="number">10</span>,<span class="number">10</span>),activation=<span class="string">'relu'</span>,input_shape=input_shape,</div><div class="line">                   kernel_initializer=W_init,kernel_regularizer=l2(<span class="number">2e-4</span>)))</div><div class="line">convnet.add(MaxPooling2D())</div><div class="line">convnet.add(Conv2D(<span class="number">128</span>,(<span class="number">7</span>,<span class="number">7</span>),activation=<span class="string">'relu'</span>,</div><div class="line">                   kernel_regularizer=l2(<span class="number">2e-4</span>),kernel_initializer=W_init,bias_initializer=b_init))</div><div class="line">convnet.add(MaxPooling2D())</div><div class="line">convnet.add(Conv2D(<span class="number">128</span>,(<span class="number">4</span>,<span class="number">4</span>),activation=<span class="string">'relu'</span>,kernel_initializer=W_init,kernel_regularizer=l2(<span class="number">2e-4</span>),bias_initializer=b_init))</div><div class="line">convnet.add(MaxPooling2D())</div><div class="line">convnet.add(Conv2D(<span class="number">256</span>,(<span class="number">4</span>,<span class="number">4</span>),activation=<span class="string">'relu'</span>,kernel_initializer=W_init,kernel_regularizer=l2(<span class="number">2e-4</span>),bias_initializer=b_init))</div><div class="line">convnet.add(Flatten())</div><div class="line">convnet.add(Dense(<span class="number">4096</span>,activation=<span class="string">"sigmoid"</span>,kernel_regularizer=l2(<span class="number">1e-3</span>),kernel_initializer=W_init,bias_initializer=b_init))</div><div class="line"><span class="comment">#encode each of the two inputs into a vector with the convnet</span></div><div class="line">encoded_l = convnet(left_input)</div><div class="line">encoded_r = convnet(right_input)</div><div class="line"><span class="comment">#merge two encoded inputs with the l1 distance between them</span></div><div class="line">L1_distance = <span class="keyword">lambda</span> x: K.abs(x[<span class="number">0</span>]-x[<span class="number">1</span>])</div><div class="line">both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</div><div class="line">prediction = Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>,bias_initializer=b_init)(both)</div><div class="line">siamese_net = Model(input=[left_input,right_input],output=prediction)</div><div class="line"><span class="comment">#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)</span></div><div class="line"></div><div class="line">optimizer = Adam(<span class="number">0.00006</span>)</div><div class="line"><span class="comment">#//<span class="doctag">TODO:</span> get layerwise learning rates and momentum annealing scheme described in paperworking</span></div><div class="line">siamese_net.compile(loss=<span class="string">"binary_crossentropy"</span>,optimizer=optimizer)</div><div class="line"></div><div class="line">siamese_net.count_params()</div></pre></td></tr></table></figure><p>The original paper used layerwise learning rates and momentum - I skipped this because it; was kind of messy to implement in keras and the hyperparameters aren’t the interesting part of the paper. Koch et al adds examples to the dataset by distorting the images and runs experiments with a fixed training set of up to 150,000 pairs. Since that won’t fit in my computers memory, I decided to just randomly sample pairs. Loading image pairs was probably the hardest part of this to implement. Since there were 20 examples for every class, I reshaped the data into N_classes x 20 x 105 x 105 arrays, to make it easier to index by category.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Siamese_Loader</span>:</span></div><div class="line">    <span class="string">"""For loading batches and testing tasks to a siamese net"""</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,Xtrain,Xval)</span>:</span></div><div class="line">        self.Xval = Xval</div><div class="line">        self.Xtrain = Xtrain</div><div class="line">        self.n_classes,self.n_examples,self.w,self.h = Xtrain.shape</div><div class="line">        self.n_val,self.n_ex_val,_,_ = Xval.shape</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(self,n)</span>:</span></div><div class="line">        <span class="string">"""Create batch of n pairs, half same class, half different class"""</span></div><div class="line">        categories = rng.choice(self.n_classes,size=(n,),replace=<span class="keyword">False</span>)</div><div class="line">        pairs=[np.zeros((n, self.h, self.w,<span class="number">1</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>)]</div><div class="line">        targets=np.zeros((n,))</div><div class="line">        targets[n//<span class="number">2</span>:] = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">            category = categories[i]</div><div class="line">            idx_1 = rng.randint(<span class="number">0</span>,self.n_examples)</div><div class="line">            pairs[<span class="number">0</span>][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,<span class="number">1</span>)</div><div class="line">            idx_2 = rng.randint(<span class="number">0</span>,self.n_examples)</div><div class="line">            <span class="comment">#pick images of same class for 1st half, different for 2nd</span></div><div class="line">            category_2 = category <span class="keyword">if</span> i &gt;= n//<span class="number">2</span> <span class="keyword">else</span> (category + rng.randint(<span class="number">1</span>,self.n_classes)) % self.n_classes</div><div class="line">            pairs[<span class="number">1</span>][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> pairs, targets</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_oneshot_task</span><span class="params">(self,N)</span>:</span></div><div class="line">        <span class="string">"""Create pairs of test image, support set for testing N way one-shot learning. """</span></div><div class="line">        categories = rng.choice(self.n_val,size=(N,),replace=<span class="keyword">False</span>)</div><div class="line">        indices = rng.randint(<span class="number">0</span>,self.n_ex_val,size=(N,))</div><div class="line">        true_category = categories[<span class="number">0</span>]</div><div class="line">        ex1, ex2 = rng.choice(self.n_examples,replace=<span class="keyword">False</span>,size=(<span class="number">2</span>,))</div><div class="line">        test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,<span class="number">1</span>)</div><div class="line">        support_set = self.Xval[categories,indices,:,:]</div><div class="line">        support_set[<span class="number">0</span>,:,:] = self.Xval[true_category,ex2]</div><div class="line">        support_set = support_set.reshape(N,self.w,self.h,<span class="number">1</span>)</div><div class="line">        pairs = [test_image,support_set]</div><div class="line">        targets = np.zeros((N,))</div><div class="line">        targets[<span class="number">0</span>] = <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> pairs, targets</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_oneshot</span><span class="params">(self,model,N,k,verbose=<span class="number">0</span>)</span>:</span></div><div class="line">        <span class="string">"""Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks"""</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line">        n_correct = <span class="number">0</span></div><div class="line">        <span class="keyword">if</span> verbose:</div><div class="line">            print(<span class="string">"Evaluating model on &#123;&#125; unique &#123;&#125; way one-shot learning tasks ..."</span>.format(k,N))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">            inputs, targets = self.make_oneshot_task(N)</div><div class="line">            probs = model.predict(inputs)</div><div class="line">            <span class="keyword">if</span> np.argmax(probs) == <span class="number">0</span>:</div><div class="line">                n_correct+=<span class="number">1</span></div><div class="line">        percent_correct = (<span class="number">100.0</span>*n_correct / k)</div><div class="line">        <span class="keyword">if</span> verbose:</div><div class="line">            print(<span class="string">"Got an average of &#123;&#125;% &#123;&#125; way one-shot learning accuracy"</span>.format(percent_correct,N))</div><div class="line">        <span class="keyword">return</span> percent_correct</div></pre></td></tr></table></figure><p>..And now the training loop. Nothing unusual here, except for that I monitor one-shot tasks validation accuracy to test performance, rather than loss on the validation set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">evaluate_every = <span class="number">7000</span></div><div class="line">loss_every=<span class="number">300</span></div><div class="line">batch_size = <span class="number">32</span></div><div class="line">N_way = <span class="number">20</span></div><div class="line">n_val = <span class="number">550</span></div><div class="line">siamese_net.load_weights(<span class="string">"PATH"</span>)</div><div class="line">best = <span class="number">76.0</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">900000</span>):</div><div class="line">    (inputs,targets)=loader.get_batch(batch_size)</div><div class="line">    loss=siamese_net.train_on_batch(inputs,targets)</div><div class="line">    <span class="keyword">if</span> i % evaluate_every == <span class="number">0</span>:</div><div class="line">        val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">if</span> val_acc &gt;= best:</div><div class="line">            print(<span class="string">"saving"</span>)</div><div class="line">            siamese_net.save(<span class="string">'PATH'</span>)</div><div class="line">            best=val_acc</div><div class="line"></div><div class="line">    <span class="keyword">if</span> i % loss_every == <span class="number">0</span>:</div><div class="line">        print(<span class="string">"iteration &#123;&#125;, training loss: &#123;:.2f&#125;,"</span>.format(i,loss))</div></pre></td></tr></table></figure><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>Once the learning curve flattened out, I used the weights which got the best validation 20 way accuracy for testing. My network averaged ~83% accuracy for tasks from the evaluation set, compared to 93% in the original paper. Probably this difference is because I didn’t implement many of the performance enhancing tricks from the original paper, like layerwise learning rates/momentum, data augmentation with distortions, bayesian hyperparemeter optimization and I also probably trained for less epochs. I’m not too worried about this because this tutorial was more about introducing one-shot learning in general, than squeezing the last few % performance out of a classifier. There is no shortage of resources on that!</p><p>I was curious to see how accuracy varied over different values of “N” in N way one shot learning, so I plotted it, with comparisons to 1 nearest neighbours, random guessing and training set performance.</p><p><a href="https://sorenbouma.github.io/images/results1.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/results1.png" alt="image"></a>results.</p><p>As you can see, it performs worse on tasks from the validaiton set than the train set, especially for high values of N, so there must be overfitting. It would be interesting to see how well traditional regularization methods like dropout work when the validation set is made of completely different classes to the training set. It works better than I expected for large N, still averaging above 65% accuracy for 50-60 way tasks.</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>We’ve just trained a neural network trained to do same-different pairwise classification on symbols. More importantly, we’ve shown that it can then get reasonable accuracy in 20 way one-shot learning on symbols from unseen alphabets. Of course, this is not the only way to use deep networks for one-shot learning.</p><p>As I touched on earlier, I think a major flaw of this siamese approach is that it only compares the test image to every support image individualy, when it should be comparing it to the support set as a whole. When the network compares the test image to any image x1, p(x^∘x1) is the same no matter what else is the support set. This is silly. Say you’re doing a one-shot task and you see an image that looks similar to the test image. You should be much less confident they have the same class if there is another image in the support set that also looks similar to the test image. The training objective is different to the test objective. It might work better to have a model that can compare the test image to the support set as a whole and use the constraint that only one support image has the same class.</p><p><a href="https://arxiv.org/pdf/1606.04080.pdf" target="_blank" rel="external">Matching Networks for One Shot learning</a> does exactly that. Rather than learning a similarity function, they have a deep model learn a full nearest neighbour classifier end to end, training directly on oneshot tasks rather than on image pairs. <a href="https://github.com/karpathy/paper-notes/blob/master/matching_networks.md" target="_blank" rel="external">Andrej Karpathy’s notes</a> explain it much better than I can. Since you are learning a machine classifier, this can be seen as a kind of <em>meta-learning</em>. <a href="https://arxiv.org/pdf/1605.06065.pdf" target="_blank" rel="external">One-shot Learning with Memory-Augmented Neural Networks</a> explores the connection between one-shot learning and meta learning and trains a memory augmented network on omniglot, though I confess I had trouble understanding this paper.</p><h3 id="What-next"><a href="#What-next" class="headerlink" title="What next?"></a>What next?</h3><p>The omniglot dataset has been around since 2015, and already there are scalable ML algorithms getting within the ballpark of human level performance on certain one-shot learning tasks. Hopefully one day it will be seen as a mere “sanity check” for one-shot classification algorithms much like MNIST is for supervised learning now.</p><p>Image classification is cool but I don’t think it’s the most interesting problem in machine learning. Now that we know deep one-shot learning can work pretty good, I think it would be cool to see attempts at one-shot learning for other, more exotic tasks.</p><p>Ideas from one-shot learning could be used for more sample efficient reinforcement learning, especially for problems like OpenAI’s Universe, where there are lots of MDPs/environments that have similar visual features and dynamics. - It would be cool to have an RL agent that could efficiently explore a new environment after learning in similar MDPs.</p><p><a href="https://sorenbouma.github.io/images/worldofbits.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/worldofbits.jpg" alt="image"></a>OpenAI’s world of bits environments.</p><p><a href="https://arxiv.org/abs/1703.07326" target="_blank" rel="external">One-shot Imitation learning</a> is one of my favourite one-shot learning papers. The goal is to have an agent learn a robust policy for solving a task from a single human demonstration of that task.This is done by:</p><ol><li>Having a neural net map from the current state and a sequence of states(the human demonstration) to an action</li><li>Training it on pairs of human demonstrations on slightly different variants of the same task, with the goal of reproducing the second demonstration based on the first.</li></ol><p>This strikes me as a really promising path to one day having broadly applicable, learning based robots!</p><p>Bringing one-shot learning to NLP tasks is a cool idea too. <em>Matching Networks for One-Shot learning</em> has an attempt at one-shot language modeling, filling a missing word in a test sentence given a small set of support sentences, and it seems to work pretty well. Exciting!</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Anyway, thanks for reading! I hope you’ve managed to one-shot learn the concept of one-shot learning :) If not, I’d love to hear feedback or answer any questions you have!</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Background&quot;&gt;&lt;a href=&quot;#Background&quot; class=&quot;headerlink&quot; title=&quot;Background:&quot;&gt;&lt;/a&gt;Background:&lt;/h2&gt;&lt;p&gt;Conventional wisdom says that deep n
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="one-shot learning" scheme="http://yoursite.com/tags/one-shot-learning/"/>
    
  </entry>
  
  <entry>
    <title>Anaconda uses socket proxy on Windows 10</title>
    <link href="http://yoursite.com/2018/05/17/Anaconda-uses-socket-proxy-on-Windows-10/"/>
    <id>http://yoursite.com/2018/05/17/Anaconda-uses-socket-proxy-on-Windows-10/</id>
    <published>2018-05-17T09:54:53.000Z</published>
    <updated>2018-05-17T09:58:52.027Z</updated>
    
    <content type="html"><![CDATA[<p>you need to create a <strong>.condarc</strong> file in you Windows user area:</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">C:\Users\&lt;username&gt;\</div></pre></td></tr></table></figure><p>The file should contain (if you are using shadowsocks):</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">channels:</div><div class="line">- defaults</div><div class="line"></div><div class="line"><span class="comment"># Show channel URLs when displaying what is going to be downloaded and</span></div><div class="line"><span class="comment"># in 'conda list'. The default is False.</span></div><div class="line">show_channel_urls: True</div><div class="line">allow_other_channels: True</div><div class="line"></div><div class="line">proxy_servers:</div><div class="line">    http: socks5://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">1080</span></div><div class="line">    https: socks5://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">1080</span></div><div class="line"></div><div class="line">ssl_verify: False</div></pre></td></tr></table></figure><p>Noticed that you cannot create a file that begins with a dot in Windows directly.</p><p>To <strong>create/rename on windows explorer</strong>, just rename to <code>.name.</code> - The additional dot at the end is necessary, and will be removed by Windows Explorer.</p><p>To create a new file begins with a dot, on command prompt:</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo testing &gt; .name</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;you need to create a &lt;strong&gt;.condarc&lt;/strong&gt; file in you Windows user area:&lt;/p&gt;&lt;figure class=&quot;highlight powershell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td clas
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Paper Note: Selective Search for Object Recognition</title>
    <link href="http://yoursite.com/2018/05/03/Paper-Note-Selective-Search-for-Object-Recognition/"/>
    <id>http://yoursite.com/2018/05/03/Paper-Note-Selective-Search-for-Object-Recognition/</id>
    <published>2018-05-03T05:59:00.000Z</published>
    <updated>2018-05-03T06:08:07.390Z</updated>
    
    <content type="html"><![CDATA[<p>与 Selective Search 初次见面是在著名的物体检测论文 <code>Rich feature hierarchies for accurate object detection and semantic segmentation</code> ，因此，这篇论文算是阅读 R-CNN 的准备。</p><p>这篇论文的标题虽然也提到了 Object Recognition ，但就创新点而言，其实在 Selective Search 。所以，这里只简单介绍 Selective Search 的思想和算法过程，对于 Object Recognition 则不再赘述。</p><h3 id="什么是-Selective-Search"><a href="#什么是-Selective-Search" class="headerlink" title="什么是 Selective Search"></a>什么是 Selective Search</h3><p>Selective Search，说的简单点，就是从图片中找出物体可能存在的区域。</p><p><a href="http://jermmy.xyz/images/2017-5-4/result.png" target="_blank" rel="external"><img src="http://jermmy.xyz/images/2017-5-4/result.png" alt="result"></a>result</p><p>上面这幅宇航员的图片中，那些红色的框就是 Selective Search 找出来的可能存在物体的区域。</p><p>在进一步探讨它的原理之前，我们分析一下，如何判别哪些 region 属于一个物体？</p><p><a href="http://jermmy.xyz/images/2017-5-4/image%20seg.png" target="_blank" rel="external"><img src="http://jermmy.xyz/images/2017-5-4/image%20seg.png" alt="image seg"></a>image seg</p><p>作者在论文中用以上四幅图，分别描述了四种可能的情况：</p><ol><li>图 a ，物体之间可能存在层级关系，比如：碗里有个勺；</li><li>图 b，我们可以用颜色来分开两只猫，却没法用纹理来区分；</li><li>图 c，我们可以用纹理来区分变色龙，却没法用颜色来区分；</li><li>图 d，轮胎是车的一部分，不是因为它们颜色相近、纹理相近，而是因为轮胎包含在车上。</li></ol><p>所以，我们没法用单一的特征来定位物体，需要综合考虑多种策略，这一点是 Selective Search 精要所在。</p><h3 id="需要考虑的问题"><a href="#需要考虑的问题" class="headerlink" title="需要考虑的问题"></a>需要考虑的问题</h3><p>在学习 Selective Search 算法之前，我曾在计算机视觉课上学到过关于物体（主要是人脸）检测的方法。通常来说，最常规也是最简单粗暴的方法，就是用不同尺寸的矩形框，一行一行地扫描整张图像，通过提取矩形框内的特征判断是否是待检测物体。这种方法的复杂度极高，所以又被称为 <strong>exhaustive search</strong>。在人脸识别中，由于使用了 Haar 特征，因此可以借助 <strong>Paul Viola</strong> 和 <strong>Michael Jones</strong> 两位大牛提出的积分图，使检测在常规时间内完成。但并不是每种特征都适用于积分图，尤其在神经网络中，积分图这种动态规划的思路就没什么作用了。</p><p>针对传统方法的不足，Selective Search 从三个角度提出了改进：</p><ol><li>我们没法事先得知物体的大小，在传统方法中需要用不同尺寸的矩形框检测物体，防止遗漏。而 Selective Search 采用了一种具备层次结构的算法来解决这个问题；</li><li>检测的时间复杂度可能会很高。Selective Search 遵循简单即是美的原则，只负责快速地生成可能是物体的区域，而不做具体的检测；</li><li>另外，结合上一节提出的，采用多种先验知识来对各个区域进行简单的判别，避免一些无用的搜索，提高速度和精度。</li></ol><h3 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h3><p><a href="http://jermmy.xyz/images/2017-5-4/algorithm.png" target="_blank" rel="external"><img src="http://jermmy.xyz/images/2017-5-4/algorithm.png" alt="algorithm"></a>algorithm</p><p>论文中给出的这个算法框架还是很详细的，这里再简单翻译一下。</p><ul><li>输入：彩色图片。</li><li>输出：物体可能的位置，实际上是很多的矩形坐标。</li><li>首先，我们使用这篇<a href="http://cs.brown.edu/~pff/segment/" target="_blank" rel="external">论文</a>的方法将图片初始化为很多小区域 $R=r_i, \cdots, r_n$。由于我们的重点是 Selective Search，因此我直接将该论文的算法当成一个黑盒子。</li><li>初始化一个相似集合为空集： $S=∅$。</li><li>计算所有相邻区域之间的相似度（相似度函数之后会重点分析），放入集合 S 中，集合 S 保存的其实是一个<strong>区域对</strong>以及它们之间的相似度。</li><li>找出 S 中相似度最高的区域对，将它们合并，并从 S 中删除与它们相关的所有相似度和区域对。重新计算这个新区域与周围区域的相似度，放入集合 S 中，并将这个新合并的区域放入集合 R 中。重复这个步骤直到 S 为空。</li><li>从 R 中找出所有区域的 bounding box（即包围该区域的最小矩形框），这些 box 就是物体可能的区域。</li></ul><p>另外，为了提高速度，新合并区域的 feature 可以通过之前的两个区域获得，而不必重新遍历新区域的像素点进行计算。这个 feature 会被用于计算相似度。</p><h3 id="相似度计算方法"><a href="#相似度计算方法" class="headerlink" title="相似度计算方法"></a>相似度计算方法</h3><p>相似度计算方法将直接影响合并区域的顺序，进而影响到检测结果的好坏。</p><p>论文中比较了八种颜色空间的特点，在实际操作中，只选择一个颜色空间（比如：RGB 空间）进行计算。</p><p>正如一开始提出的那样，我们需要综合多种信息来判断。作者将相似度度量公式分为四个子公式，称为<strong>互补相似度测量(Complementary Similarity Measures)</strong> 。这四个子公式的值都被归一化到区间 [0, 1] 内。</p><h4 id="1-颜色相似度scolor-ri-rj-scolor-ri-rj"><a href="#1-颜色相似度scolor-ri-rj-scolor-ri-rj" class="headerlink" title="1. 颜色相似度scolor (ri,rj)scolor (ri,rj)"></a>1. 颜色相似度scolor (ri,rj)scolor (ri,rj)</h4><p>正如本文一开始提到的，颜色是一个很重要的区分物体的因素。论文中将每个 region 的像素按不同颜色通道统计成直方图，其中，每个颜色通道的直方图为 25 bins （比如，对于 0 ～ 255 的颜色通道来说，就每隔 9(255/25=9) 个数值统计像素数量）。这样，三个通道可以得到一个 75 维的直方图向量 $C_i={c_{i}^{1}, …, c_{i}^{n}}$，其中 n = 75。之后，我们用 <strong>L1 范数</strong>（绝对值之和）对直方图进行归一化。由直方图我们就可以计算两个区域的颜色相似度：<br>$$<br>s_{color}(r_i, r_j) =\sum_{k=1}^{n}{min(c_{i}^{k}, c_{j}^{k})}<br>$$<br>这个颜色直方图可以在合并区域的时候，很方便地传递给下一级区域。即它们合并后的区域的直方图向量为：<br>$$<br>C_t=\frac{size(r_i)<em>C_i+size(r_j)</em>C_j}{size(r_i)+size(r_j)}<br>$$<br>，其中$size(r_i)$ 表示区域 $r_i$ 的面积，合并后的区域为 $size(r_t)=size(r_i)+size(r_j)$。</p><h4 id="2-纹理相似度-s-texture-r-i-r-j"><a href="#2-纹理相似度-s-texture-r-i-r-j" class="headerlink" title="2. 纹理相似度$s_{texture}(r_i,r_j)$"></a>2. 纹理相似度$s_{texture}(r_i,r_j)$</h4><p>另一个需要考虑的因素是纹理，即图像的梯度信息。</p><p>论文中对纹理的计算采用了 SIFT-like 特征，该特征借鉴了 SIFT 的计算思路，对每个颜色通道的像素点，沿周围 8 个方向计算高斯一阶导数(σ=1σ=1)，每个方向统计一个直方图（bin = 10），这样，一个颜色通道统计得到的直方图向量为 80 维，三个通道就是 240 维：$T_i={t_i^{(1)}, …, t_i^{(n)}}$，其中 n = 240。注意这个直方图要用 <strong>L1 范数</strong>归一化。然后，我们按照颜色相似度的计算思路计算两个区域的纹理相似度：<br>$$<br>s_{texture}(r_i, r_j) =\sum_{k=1}^{n}{min(t_{i}^{k}, t_{j}^{k})}<br>$$</p><h4 id="3-尺寸相似度-s-size-r-i-r-j"><a href="#3-尺寸相似度-s-size-r-i-r-j" class="headerlink" title="3. 尺寸相似度$s_{size} (r_i,r_j)$"></a>3. 尺寸相似度$s_{size} (r_i,r_j)$</h4><p>在合并区域的时候，论文优先考虑小区域的合并，这种做法可以在一定程度上保证每次合并的区域面积都比较相似，防止大区域对小区域的逐步蚕食。这么做的理由也很简单，我们要均匀地在图片的每个角落生成不同尺寸的区域，作用相当于 <strong>exhaustive search</strong> 中用不同尺寸的矩形扫描图片。具体的相似度计算公式为：<br>$$<br>s_{size}(r_i, r_j)=1-\frac{size(r_i) + size(r_j)}{size(im)}<br>$$<br>其中，$size(im)$ 表示原图片的像素数量。</p><h4 id="4-填充相似度-s-fill-r-i-r-j"><a href="#4-填充相似度-s-fill-r-i-r-j" class="headerlink" title="4. 填充相似度$s_{fill}(r_i,r_j)$"></a>4. 填充相似度$s_{fill}(r_i,r_j)$</h4><p>填充相似度主要用来测量两个区域之间 fit 的程度，个人觉得这一点是要解决文章最开始提出的物体之间的包含关系（比如：轮胎包含在汽车上）。在给出填充相似度的公式前，我们需要定义一个矩形区域 $BB_{ij}$，它表示包含 $r_i$ 和 $r_j$ 的最小的 bounding box。基于此，我们给出相似度计算公式为：<br>$$<br>s_{fill}(r_i, r_j)=1-\frac{size(BB_{ij})-size(r_i)-size(r_j)}{size(im)}<br>$$<br>为了高效地计算 $BB_{ij}$，我们可以在计算每个 region 的时候，都保存它们的 bounding box 的位置，这样，$BB_{ij}$ 就可以很快地由两个区域的 bounding box 推出来。</p><h4 id="5-相似度计算公式"><a href="#5-相似度计算公式" class="headerlink" title="5. 相似度计算公式"></a>5. 相似度计算公式</h4><p>综合上面四个子公式，我们可以得到计算相似度的最终公式：<br>$$<br>s(r_i, r_j) = a_1 s_{color}(r_i, r_j) +a_2s_{texture}(r_i, r_j) \\\\ +a_3s_{size}(r_i, r_j)+a_4s_{fill}(r_i, r_j)<br>$$<br>其中，$a_i$的取值为 0 或 1，表示某个相似度是否被采纳。</p><h3 id="Combining-Locations"><a href="#Combining-Locations" class="headerlink" title="Combining Locations"></a>Combining Locations</h3><p>前面我们基本完成了 Selective Search 的流程，从图片中提取出了物体可能的位置。现在，我们想完善最后一个问题，那就是给这些位置排个序。因为提取出来的矩形框数量巨大，而用户可能只需要其中的几个，这个时候我们就很有必要对这些矩形框赋予优先级，按照优先级高低返回给用户。原文中作者称这一步为 <strong>Combining Locations</strong>，我找不出合适的翻译，就姑且保留英文原文。</p><p>这个排序的方法也很简单。作者先给各个 region 一个序号，前面说了，Selective Search 是一个逐步合并的层级结构，因此，我们将覆盖整个区域的 region 的序号标记为 1，合成这个区域的两个子区域的序号为 2，以此类推。但如果仅按序号排序，会存在一个漏洞，那就是区域面积大的会排在前面，为了避免这个漏洞，作者又在每个序号前乘上一个随机数 $RND∈[0,1]$，通过这个新计算出来的数值，按从小到大的顺序得出 region 最终的排序结果。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="http://blog.csdn.net/langb2014/article/details/52575507" target="_blank" rel="external">Selective Search for Object Recognition(阅读)</a></li><li><a href="http://cs.brown.edu/~pff/segment/" target="_blank" rel="external">Efficient Graph-Based Image Segmentation</a></li></ul><blockquote><p><strong>本文作者：</strong> Jermmy</p><p><strong>本文链接：</strong> <a href="https://jermmy.github.io/2017/05/04/2017-5-4-paper-notes-selective-search/" target="_blank" rel="external">https://jermmy.github.io/2017/05/04/2017-5-4-paper-notes-selective-search/</a></p><p><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank" rel="external">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;与 Selective Search 初次见面是在著名的物体检测论文 &lt;code&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/code&gt; ，因此，这篇论文
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Micro- and Macro-average of Precision, Recall and F-Score</title>
    <link href="http://yoursite.com/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/"/>
    <id>http://yoursite.com/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/</id>
    <published>2018-04-27T06:36:15.000Z</published>
    <updated>2018-04-27T06:37:49.011Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Micro-average-Method"><a href="#Micro-average-Method" class="headerlink" title="Micro-average Method"></a><strong>Micro-average Method</strong></h3><p>In Micro-average method, you sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics. For example, for a set of data, the system’s</p><p>True positive (TP1)= 12<br>False positive (FP1)=9<br>False negative (FN1)=3</p><p>Then precision (P1) and recall (R1) will be 57.14 and 80</p><p>and for a different set of data, the system’s</p><p>True positive (TP2)= 50<br>False positive (FP2)=23<br>False negative (FN2)=9</p><p>Then precision (P2) and recall (R2) will be 68.49 and 84.75</p><p>Now, the average precision and recall of the system using the Micro-average method is</p><p>Micro-average of precision = (TP1+TP2)/(TP1+TP2+FP1+FP2) = (12+50)/(12+50+9+23) = 65.96<br>Micro-average of recall = (TP1+TP2)/(TP1+TP2+FN1+FN2) = (12+50)/(12+50+3+9) = 83.78</p><p>The Micro-average F-Score will be simply the harmonic mean of these two figures.</p><h3 id="Macro-average-Method"><a href="#Macro-average-Method" class="headerlink" title="Macro-average Method"></a><strong>Macro-average Method</strong></h3><p>The method is straight forward. Just take the average of the precision and recall of the system on different sets. For example, the macro-average precision and recall of the system for the given example is</p><p>Macro-average precision = (P1+P2)/2 = (57.14+68.49)/2 = 62.82<br>Macro-average recall = (R1+R2)/2 = (80+84.75)/2 = 82.25</p><p>The Macro-average F-Score will be simply the harmonic mean of these two figures.</p><p>Suitability<br>Macro-average method can be used when you want to know how the system performs overall across the sets of data. You should not come up with any specific decision with this average.</p><p>On the other hand, micro-average can be a useful measure when your dataset varies in size.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Micro-average-Method&quot;&gt;&lt;a href=&quot;#Micro-average-Method&quot; class=&quot;headerlink&quot; title=&quot;Micro-average Method&quot;&gt;&lt;/a&gt;&lt;strong&gt;Micro-average Meth
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Get financial data from Tushare</title>
    <link href="http://yoursite.com/2018/04/11/Get-financial-data-from-Tushare/"/>
    <id>http://yoursite.com/2018/04/11/Get-financial-data-from-Tushare/</id>
    <published>2018-04-11T11:42:46.000Z</published>
    <updated>2018-04-11T12:12:04.409Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>TuShare is a famous free, open source python financial data interface package. Its official home page is: <a href="http://tushare.waditu.com/" target="_blank" rel="external">TuShare - financial data interface package</a>. The interface package now provides a large amount of financial data covering a wide range of data such as stocks, fundamentals, macros, news, etc. (Please check the official website for details) and keep updating. At present, the length of the stock’s data is three years. Although it is a bit short, it can basically meet the needs of quantitative beginners for testing.</p><h1 id="Tutorial"><a href="#Tutorial" class="headerlink" title="Tutorial"></a>Tutorial</h1><h2 id="Install-and-Import"><a href="#Install-and-Import" class="headerlink" title="Install and Import"></a>Install and Import</h2><p><strong>You need to install first:</strong></p><ul><li>Pandas</li><li>lxml</li></ul><p><strong>Two way to install tushare:</strong></p><ol><li><code>pip install tushare</code></li><li>visit <a href="https://pypi.python.org/pypi/Tushare/" target="_blank" rel="external">https://pypi.python.org/pypi/Tushare/</a>, download and install</li></ol><p><strong>How to update:</strong></p><p><code>pip install tushare --upgrade</code></p><p><strong>Import package and view package version:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tushare</div><div class="line"></div><div class="line">print(tushare.__version__)</div></pre></td></tr></table></figure><h2 id="Use-some-simple-function"><a href="#Use-some-simple-function" class="headerlink" title="Use some simple function"></a>Use some simple function</h2><h3 id="Stock-data"><a href="#Stock-data" class="headerlink" title="Stock data"></a>Stock data</h3><p><strong>update：Many of the quotes returned by the <code>get_hist_data</code> function are wrong, but both <code>get_h_data</code> and <code>get_k_data</code> can be used</strong></p><p>We should still master how to use <code>tushare</code> to obtain stock market data, using the <code>ts.get_hist_data()</code> function whose <strong>input parameters</strong> are:</p><ul><li><strong>code: </strong>Stock code, ie 6-digit code, or index code (sh = Shanghai index sz = Shenzhen index hs300 = CSI 300 index sz50 = SSE 50 zxb = small and medium board cyb = board)</li><li><strong>start: </strong>Start date, format YYYY-MM-DD</li><li><strong>end: </strong>End date, format YYYY-MM-DD</li><li><strong>ktype: </strong>Data type, D = day k line W = week M = month 5 = 5 minutes 15 = 15 minutes 30 = 30 minutes 60 = 60 minutes, the default is D</li><li><strong>retry_count: </strong>The number of retries after the network is abnormal. The default is 3</li><li><strong>pause: </strong>Pause seconds when retrying, default is 0</li></ul><p><strong>Return values:</strong></p><ul><li><strong>date</strong>：date</li><li><strong>open</strong>：Opening price</li><li><strong>high</strong>：Highest price</li><li><strong>close</strong>：Closing price</li><li><strong>low</strong>：Lowest price</li><li><strong>volume</strong>：Volume</li><li><strong>price_change</strong>：price fluncuation</li><li><strong>p_change</strong>：Quote change</li><li><strong>ma5</strong>：5-day average price</li><li><strong>ma10</strong>：10-day average price</li><li><strong>ma20</strong>: 20-day average price</li><li><strong>v_ma5</strong>: 5-day average volume</li><li><strong>v_ma10</strong>: 10-day average volume</li><li><strong>v_ma20</strong>: 20-day average volume</li><li><strong>turnover</strong>: Change in hand rate [Note: Index does not have this item]</li></ul><h4 id="Specific-examples"><a href="#Specific-examples" class="headerlink" title="Specific examples:"></a>Specific examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">ts.get_hist_data(<span class="string">'600848'</span>)</div><div class="line"></div><div class="line"> date       open    high   close     low     volume    p_change   ma5    </div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-11</span>   <span class="number">6.880</span>   <span class="number">7.380</span>   <span class="number">7.060</span>   <span class="number">6.880</span>   <span class="number">14129.96</span>     <span class="number">2.62</span>   <span class="number">7.060</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-12</span>   <span class="number">7.050</span>   <span class="number">7.100</span>   <span class="number">6.980</span>   <span class="number">6.900</span>    <span class="number">7895.19</span>    <span class="number">-1.13</span>   <span class="number">7.020</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-13</span>   <span class="number">6.950</span>   <span class="number">7.000</span>   <span class="number">6.700</span>   <span class="number">6.690</span>    <span class="number">6611.87</span>    <span class="number">-4.01</span>   <span class="number">6.913</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-16</span>   <span class="number">6.680</span>   <span class="number">6.750</span>   <span class="number">6.510</span>   <span class="number">6.480</span>    <span class="number">2941.63</span>    <span class="number">-2.84</span>   <span class="number">6.813</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-17</span>   <span class="number">6.660</span>   <span class="number">6.880</span>   <span class="number">6.860</span>   <span class="number">6.460</span>    <span class="number">8642.57</span>     <span class="number">5.38</span>   <span class="number">6.822</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-18</span>   <span class="number">7.000</span>   <span class="number">7.300</span>   <span class="number">6.890</span>   <span class="number">6.880</span>   <span class="number">13075.40</span>     <span class="number">0.44</span>   <span class="number">6.788</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-19</span>   <span class="number">6.690</span>   <span class="number">6.950</span>   <span class="number">6.890</span>   <span class="number">6.680</span>    <span class="number">6117.32</span>     <span class="number">0.00</span>   <span class="number">6.770</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-20</span>   <span class="number">6.870</span>   <span class="number">7.080</span>   <span class="number">7.010</span>   <span class="number">6.870</span>    <span class="number">6813.09</span>     <span class="number">1.74</span>   <span class="number">6.832</span></div><div class="line"></div><div class="line">date         ma10    ma20      v_ma5     v_ma10     v_ma20     turnover</div><div class="line"></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-11</span>   <span class="number">7.060</span>   <span class="number">7.060</span>   <span class="number">14129.96</span>   <span class="number">14129.96</span>   <span class="number">14129.96</span>     <span class="number">0.48</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-12</span>   <span class="number">7.020</span>   <span class="number">7.020</span>   <span class="number">11012.58</span>   <span class="number">11012.58</span>   <span class="number">11012.58</span>     <span class="number">0.27</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-13</span>   <span class="number">6.913</span>   <span class="number">6.913</span>    <span class="number">9545.67</span>    <span class="number">9545.67</span>    <span class="number">9545.67</span>     <span class="number">0.23</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-16</span>   <span class="number">6.813</span>   <span class="number">6.813</span>    <span class="number">7894.66</span>    <span class="number">7894.66</span>    <span class="number">7894.66</span>     <span class="number">0.10</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-17</span>   <span class="number">6.822</span>   <span class="number">6.822</span>    <span class="number">8044.24</span>    <span class="number">8044.24</span>    <span class="number">8044.24</span>     <span class="number">0.30</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-18</span>   <span class="number">6.833</span>   <span class="number">6.833</span>    <span class="number">7833.33</span>    <span class="number">8882.77</span>    <span class="number">8882.77</span>     <span class="number">0.45</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-19</span>   <span class="number">6.841</span>   <span class="number">6.841</span>    <span class="number">7477.76</span>    <span class="number">8487.71</span>    <span class="number">8487.71</span>     <span class="number">0.21</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-20</span>   <span class="number">6.863</span>   <span class="number">6.863</span>    <span class="number">7518.00</span>    <span class="number">8278.38</span>    <span class="number">8278.38</span>     <span class="number">0.23</span></div></pre></td></tr></table></figure><p>You can also set the start time and end time of historical data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">ts.get_hist_data(<span class="string">'600848'</span>,start=<span class="string">'2015-01-05'</span>,end=<span class="string">'2015-01-09'</span>)</div><div class="line"></div><div class="line"> date       open    high   close     low    volume   p_change   ma5    ma10</div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-05</span>  <span class="number">11.160</span>  <span class="number">11.390</span>  <span class="number">11.260</span>  <span class="number">10.890</span>  <span class="number">46383.57</span>     <span class="number">1.26</span>  <span class="number">11.156</span>  <span class="number">11.212</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-06</span>  <span class="number">11.130</span>  <span class="number">11.660</span>  <span class="number">11.610</span>  <span class="number">11.030</span>  <span class="number">59199.93</span>     <span class="number">3.11</span>  <span class="number">11.182</span>  <span class="number">11.155</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-07</span>  <span class="number">11.580</span>  <span class="number">11.990</span>  <span class="number">11.920</span>  <span class="number">11.480</span>  <span class="number">86681.38</span>     <span class="number">2.67</span>  <span class="number">11.366</span>  <span class="number">11.251</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-08</span>  <span class="number">11.700</span>  <span class="number">11.920</span>  <span class="number">11.670</span>  <span class="number">11.640</span>  <span class="number">56845.71</span>    <span class="number">-2.10</span>  <span class="number">11.516</span>  <span class="number">11.349</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-09</span>  <span class="number">11.680</span>  <span class="number">11.710</span>  <span class="number">11.230</span>  <span class="number">11.190</span>  <span class="number">44851.56</span>    <span class="number">-3.77</span>  <span class="number">11.538</span>  <span class="number">11.363</span></div><div class="line"> date        ma20     v_ma5    v_ma10     v_ma20      turnover</div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-05</span>  <span class="number">11.198</span>  <span class="number">58648.75</span>  <span class="number">68429.87</span>   <span class="number">97141.81</span>     <span class="number">1.59</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-06</span>  <span class="number">11.382</span>  <span class="number">54854.38</span>  <span class="number">63401.05</span>   <span class="number">98686.98</span>     <span class="number">2.03</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-07</span>  <span class="number">11.543</span>  <span class="number">55049.74</span>  <span class="number">61628.07</span>  <span class="number">103010.58</span>     <span class="number">2.97</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-08</span>  <span class="number">11.647</span>  <span class="number">57268.99</span>  <span class="number">61376.00</span>  <span class="number">105823.50</span>     <span class="number">1.95</span></div></pre></td></tr></table></figure><p>Others:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'W'</span>) <span class="comment"># Get weekly k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'M'</span>) <span class="comment"># Get monthly k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'5'</span>) <span class="comment"># Get 5 minutes k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'15'</span>) <span class="comment"># Get 15 minutes k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'30'</span>) <span class="comment"># Get 30 minutes k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'60'</span>) <span class="comment"># Get 60 minutes k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'sh'</span>）<span class="comment"># Get data on the Shanghai index k-line, other parameters consistent with the stocks, the same below</span></div><div class="line">ts.get_hist_data(<span class="string">'sz'</span>）<span class="comment"># Get Shenzhen Chengzhi k line data</span></div><div class="line">ts.get_hist_data(<span class="string">'hs300'</span>）<span class="comment"># Get the CSI 300 k line data</span></div><div class="line">ts.get_hist_data(<span class="string">'sz50'</span>）<span class="comment"># Get SSE 50 Index k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'zxb'</span>）<span class="comment"># Get the k-line data of small and medium board indices</span></div><div class="line">ts.get_hist_data(<span class="string">'cyb'</span>）<span class="comment"># Get GEM Index k-line data</span></div></pre></td></tr></table></figure><h3 id="Fundamental-data"><a href="#Fundamental-data" class="headerlink" title="Fundamental data"></a>Fundamental data</h3><p>With <code>tushare</code> we can also get fundamental data through <code>ts.get_stock_basics()</code> (shown in the results section):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">ts.get_stock_basics()</div><div class="line"></div><div class="line">code    name      industry  area     pe     outstanding    totals   totalAssets                                                                   </div><div class="line"><span class="number">300563</span>    N神宇  通信设备   江苏    <span class="number">26.73</span>      <span class="number">2000.00</span>     <span class="number">8000.00</span>  <span class="number">4.216000e+04</span>   </div><div class="line"><span class="number">601882</span>   海天精工     机床制造   浙江    <span class="number">26.83</span>      <span class="number">5220.00</span>    <span class="number">52200.00</span>  <span class="number">1.877284e+05</span> </div><div class="line"><span class="number">601880</span>    大连港       港口   辽宁    <span class="number">76.40</span>    <span class="number">773582.00</span>  <span class="number">1289453.63</span>  <span class="number">3.263012e+06</span>   </div><div class="line"><span class="number">300556</span>   丝路视觉     软件服务   深圳   <span class="number">101.38</span>      <span class="number">2780.00</span>    <span class="number">11113.33</span>  <span class="number">4.448248e+04</span> </div><div class="line"><span class="number">600528</span>   中铁二局     建筑施工   四川   <span class="number">149.34</span>    <span class="number">145920.00</span>   <span class="number">145920.00</span>  <span class="number">5.709568e+06</span> </div><div class="line"><span class="number">002495</span>   佳隆股份       食品   广东   <span class="number">202.12</span>     <span class="number">66611.13</span>    <span class="number">93562.56</span>  <span class="number">1.169174e+05</span> </div><div class="line"><span class="number">600917</span>   重庆燃气     供气供热   重庆    <span class="number">76.87</span>     <span class="number">15600.00</span>   <span class="number">155600.00</span>  <span class="number">8.444600e+05</span> </div><div class="line"><span class="number">002752</span>   昇兴股份     广告包装   福建    <span class="number">75.14</span>     <span class="number">12306.83</span>    <span class="number">63000.00</span>  <span class="number">2.387493e+05</span> </div><div class="line"><span class="number">002346</span>   柘中股份     电气设备   上海   <span class="number">643.97</span>      <span class="number">7980.00</span>    <span class="number">44157.53</span>  <span class="number">2.263010e+05</span> </div><div class="line"><span class="number">000680</span>   山推股份     工程机械   山东     <span class="number">0.00</span>    <span class="number">105694.97</span>   <span class="number">124078.75</span>  <span class="number">9.050701e+05</span></div><div class="line">...</div></pre></td></tr></table></figure><h3 id="Macro-data"><a href="#Macro-data" class="headerlink" title="Macro data"></a>Macro data</h3><p>We use the resident consumer index as an example, which can be obtained through the <code>ts.get_cpi()</code> function (it will get 322 items at a time, some of them will be displayed):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> ts.get_cpi()</div><div class="line"></div><div class="line">       month     cpi</div><div class="line"><span class="number">0</span>    <span class="number">2016.10</span>  <span class="number">102.10</span></div><div class="line"><span class="number">1</span>     <span class="number">2016.9</span>  <span class="number">101.90</span></div><div class="line"><span class="number">2</span>     <span class="number">2016.8</span>  <span class="number">101.34</span></div><div class="line"><span class="number">3</span>     <span class="number">2016.7</span>  <span class="number">101.77</span></div><div class="line"><span class="number">4</span>     <span class="number">2016.6</span>  <span class="number">101.88</span></div><div class="line"><span class="number">5</span>     <span class="number">2016.5</span>  <span class="number">102.04</span></div><div class="line"><span class="number">6</span>     <span class="number">2016.4</span>  <span class="number">102.33</span></div><div class="line"><span class="number">7</span>     <span class="number">2016.3</span>  <span class="number">102.30</span></div><div class="line"><span class="number">8</span>     <span class="number">2016.2</span>  <span class="number">102.28</span></div><div class="line"><span class="number">9</span>     <span class="number">2016.1</span>  <span class="number">101.75</span></div><div class="line"><span class="number">10</span>   <span class="number">2015.12</span>  <span class="number">101.64</span></div><div class="line">...</div></pre></td></tr></table></figure><h3 id="Recent-news"><a href="#Recent-news" class="headerlink" title="Recent news"></a>Recent news</h3><p>The <code>tushare</code> package can use the <code>ts.get_latest_news()</code> function to view the latest news, and it will return 80. For reasons of space, we only show the first 15 here. We can see that it is all Sina Finance’s news data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> ts.get_latest_news();</div><div class="line"></div><div class="line">   classify                         title         time  \</div><div class="line"><span class="number">0</span>        美股            “特朗普通胀”预期升温 美国国债下挫  <span class="number">11</span><span class="number">-14</span> <span class="number">23</span>:<span class="number">10</span>   </div><div class="line"><span class="number">1</span>        美股          特朗普：脸书、推特等社交媒体助我入主白宫  <span class="number">11</span><span class="number">-14</span> <span class="number">23</span>:<span class="number">10</span>   </div><div class="line"><span class="number">2</span>        证券                <span class="number">11</span>月<span class="number">14</span>日晚增减持每日速览  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">54</span>   </div><div class="line"><span class="number">3</span>        美股          财经观察：日本为何急于推动TPP批准程序  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">54</span>   </div><div class="line"><span class="number">4</span>        美股              新总统谜题：特朗普会连续加息吗？  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">52</span>   </div><div class="line"><span class="number">5</span>        证券      神州专车财报遭质疑 增发<span class="number">100</span>亿股东退出需<span class="number">50</span>年  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">41</span>   </div><div class="line"><span class="number">6</span>        证券           恒大闪电杀回马枪锁仓半年 戒短炒了吗？  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">38</span>   </div><div class="line"><span class="number">7</span>      国内财经         楼继伟力推改革做派 或加快国有资本划拨社保  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">36</span>   </div><div class="line"><span class="number">8</span>        美股            开盘：美股周一小幅高开 延续上周涨势  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">32</span>   </div><div class="line"><span class="number">9</span>        美股            喜达屋创始人：当好总统就要走中庸之道  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">24</span>   </div><div class="line"><span class="number">10</span>       证券              北京高华：将乐视网评级下调至中性  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">09</span>   </div><div class="line"><span class="number">11</span>       美股             <span class="number">11</span>月<span class="number">14</span>日<span class="number">22</span>点交易员正关注要闻  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">02</span>   </div><div class="line"><span class="number">12</span>       美股           摩根大通：新兴市场股市、货币的前景悲观  <span class="number">11</span><span class="number">-14</span> <span class="number">21</span>:<span class="number">55</span>   </div><div class="line"><span class="number">13</span>     国内财经        人民日报刊文谈全面深化改革这三年：啃下硬骨头  <span class="number">11</span><span class="number">-14</span> <span class="number">21</span>:<span class="number">46</span>   </div><div class="line"><span class="number">14</span>       证券       泽平宏观：经济L型延续 地产销量回落投资超预期  <span class="number">11</span><span class="number">-14</span> <span class="number">21</span>:<span class="number">43</span>   </div><div class="line"><span class="number">15</span>       证券       黄燕铭等五大券商大佬告诉你 <span class="number">2017</span>年买点啥？  <span class="number">11</span><span class="number">-14</span> <span class="number">21</span>:<span class="number">41</span>   </div><div class="line"></div><div class="line">url  </div><div class="line"><span class="number">0</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">1</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">2</span>   http://finance.sina.com.cn/stock/y/<span class="number">2016</span><span class="number">-11</span><span class="number">-14</span>/...  </div><div class="line"><span class="number">3</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">4</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">5</span>   http://finance.sina.com.cn/stock/marketresearc...  </div><div class="line"><span class="number">6</span>   http://finance.sina.com.cn/stock/marketresearc...  </div><div class="line"><span class="number">7</span>   http://finance.sina.com.cn/china/gncj/<span class="number">2016</span><span class="number">-11</span>-...  </div><div class="line"><span class="number">8</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">9</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">10</span>  http://finance.sina.com.cn/stock/s/<span class="number">2016</span><span class="number">-11</span><span class="number">-14</span>/...  </div><div class="line"><span class="number">11</span>  http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">12</span>  http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">13</span>  http://finance.sina.com.cn/china/gncj/<span class="number">2016</span><span class="number">-11</span>-...  </div><div class="line"><span class="number">14</span>  http://finance.sina.com.cn/stock/marketresearc...  </div><div class="line"><span class="number">15</span>  http://finance.sina.com.cn/stock/marketresearc...</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;TuShare is a famous free, open
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Some Paper Summaries of Semantic Segmentation with Deep Learning</title>
    <link href="http://yoursite.com/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/"/>
    <id>http://yoursite.com/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/</id>
    <published>2018-04-10T12:01:58.000Z</published>
    <updated>2018-04-10T12:04:49.436Z</updated>
    
    <content type="html"><![CDATA[<h3 id="What-exactly-is-semantic-segmentation"><a href="#What-exactly-is-semantic-segmentation" class="headerlink" title="What exactly is semantic segmentation?"></a>What exactly is semantic segmentation?</h3><p>Semantic segmentation is understanding an image at pixel level i.e, we want to assign each pixel in the image an object class. For example, check out the following images.</p><p><img src="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21.jpg" alt="biker"> <img src="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21_class.png" alt="biker"><br><em>Left</em>: Input image. <em>Right</em>: It’s semantic segmentation. <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html" target="_blank" rel="external">Source.</a></p><p>Apart from recognizing the bike and the person riding it, we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models.</p><p><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" target="_blank" rel="external">VOC2012</a> and <a href="http://mscoco.org/explore/" target="_blank" rel="external">MSCOCO</a> are the most important datasets for semantic segmentation.</p><h3 id="What-are-the-different-approaches"><a href="#What-are-the-different-approaches" class="headerlink" title="What are the different approaches?"></a>What are the different approaches?</h3><p>Before deep learning took over computer vision, people used approaches like <a href="http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2008-CVPR-semantic-texton-forests.pdf" target="_blank" rel="external">TextonForest</a> and <a href="http://www.cse.chalmers.se/edu/year/2011/course/TDA361/Advanced%20Computer%20Graphics/BodyPartRecognition.pdf" target="_blank" rel="external">Random Forest based classifiers</a> for semantic segmentation. As with image classification, convolutional neural networks (CNN) have had enormous success on segmentation problems.</p><p>One of the popular initial deep learning approaches was <a href="http://people.idsia.ch/~juergen/nips2012.pdf" target="_blank" rel="external">patch classification</a> where each pixel was separately classified into classes using a patch of image around it. Main reason to use patches was that classification networks usually have full connected layers and therefore required fixed size images.</p><p>In 2014, <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#fcn" target="_blank" rel="external">Fully Convolutional Networks (FCN)</a> by Long et al. from Berkeley, popularized CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for image of any size and was also much faster compared to the patch classification approach. Almost all the subsequent state of the art approaches on semantic segmentation adopted this paradigm.</p><p>Apart from fully connected layers, one of the main problems with using CNNs for segmentation is <em>pooling layers</em>. Pooling layers increase the field of view and are able to aggregate the context while discarding the ‘where’ information. However, semantic segmentation requires the exact alignment of class maps and thus, needs the ‘where’ information to be preserved. Two different classes of architectures evolved in the literature to tackle this issue.</p><p>First one is encoder-decoder architecture. Encoder gradually reduces the spatial dimension with pooling layers and decoder gradually recovers the object details and spatial dimension. There are usually shortcut connections from encoder to decoder to help decoder recover the object details better. <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="external">U-Net</a> is a popular architecture from this class.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/unet.png" alt="U-Net architecture"><br>U-Net: An encoder-decoder architecture. <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="external">Source</a>.</p><p>Architectures in the second class use what are called as <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated/atrous convolutions</a>and do away with pooling layers.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/dilated_conv.png" alt="Dilated/atrous convolutions"><br>Dilated/atrous convolutions. rate=1 is same as normal convolutions. <a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="external">Source</a>.</p><p><a href="https://arxiv.org/abs/1210.5644" target="_blank" rel="external">Conditional Random Field (CRF) postprocessing</a> are usually used to improve the segmentation. CRFs are graphical models which ‘smooth’ segmentation based on the underlying image intensities. They work based on the observation that similar intensity pixels tend to be labeled as the same class. CRFs can boost scores by 1-2%.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/crf.png" alt="CRF"><br>CRF illustration. (b) Unary classifiers is the segmentation input to the CRF. (c, d, e) are variants of CRF with (e) being the widely used one. <a href="https://arxiv.org/abs/1210.5644" target="_blank" rel="external">Source</a>.</p><p>In the next section, I’ll summarize a few papers that represent the evolution of segmentation architectures starting from FCN. All these architectures are benchmarked on <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php" target="_blank" rel="external">VOC2012 evaluation server</a>.</p><h3 id="Summaries"><a href="#Summaries" class="headerlink" title="Summaries"></a>Summaries</h3><p>Following papers are summarized (in chronological order):</p><ol><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#fcn" target="_blank" rel="external">FCN</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#segnet" target="_blank" rel="external">SegNet</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">Dilated Convolutions</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLab (v1 &amp; v2)</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#refinenet" target="_blank" rel="external">RefineNet</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#pspnet" target="_blank" rel="external">PSPNet</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#large-kernel" target="_blank" rel="external">Large Kernel Matters</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplabv3" target="_blank" rel="external">DeepLab v3</a></li></ol><p>For each of these papers, I list down their key contributions and explain them. I also show their benchmark scores (mean IOU) on VOC2012 test dataset.</p><h4 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h4><ul><li>Fully Convolutional Networks for Semantic Segmentation</li><li>Submitted on 14 Nov 2014</li><li><a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Popularize the use of end to end convolutional networks for semantic segmentation</li><li>Re-purpose imagenet pretrained networks for segmentation</li><li>Upsample using <em>deconvolutional</em> layers</li><li>Introduce skip connections to improve over the coarseness of upsampling</li></ul><p><em>Explanation</em>:</p><p>Key observation is that fully connected layers in classification networks can be viewed as convolutions with kernels that cover their entire input regions. This is equivalent to evaluating the original classification network on overlapping input patches but is much more efficient because computation is shared over the overlapping regions of patches. Although this observation is not unique to this paper (see <a href="https://arxiv.org/abs/1312.6229" target="_blank" rel="external">overfeat</a>, <a href="https://plus.google.com/+PierreSermanet/posts/VngsFR3tug9" target="_blank" rel="external">this post</a>), it improved the state of the art on VOC2012 significantly.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/FCN%20-%20illustration.png" alt="FCN architecture"><br>Fully connected layers as a convolution. <a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="external">Source</a>.</p><p>After convolutionalizing fully connected layers in a imagenet pretrained network like VGG, feature maps still need to be upsampled because of pooling operations in CNNs. Instead of using simple bilinear interpolation, <em>deconvolutional layers</em> can learn the interpolation. This layer is also known as upconvolution, full convolution, transposed convolution or fractionally-strided convolution.</p><p>However, upsampling (even with deconvolutional layers) produces coarse segmentation maps because of loss of information during pooling. Therefore, shortcut/skip connections are introduced from higher resolution feature maps.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>62.2</td><td>-</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6&amp;submid=6103#KEY_FCN-8s" target="_blank" rel="external">leaderboard</a></td></tr><tr><td>67.2</td><td>More momentum. Not described in paper</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6&amp;submid=6103#KEY_FCN-8s-heavy" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><p><em>My Comments</em>:</p><ul><li>This was an important contribution but state of the art has improved a lot by now though.</li></ul><h4 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h4><ul><li>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</li><li>Submitted on 2 Nov 2015</li><li><a href="https://arxiv.org/abs/1511.00561" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Maxpooling indices transferred to decoder to improve the segmentation resolution.</li></ul><p><em>Explanation</em>:</p><p>FCN, despite upconvolutional layers and a few shortcut connections produces coarse segmentation maps. Therefore, more shortcut connections are introduced. However, instead of copying the encoder features as in FCN, indices from maxpooling are copied. This makes SegNet more memory efficient than FCN.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/segnet_architecture.png" alt="SegNet Architecture"><br>Segnet Architecture. <a href="https://arxiv.org/abs/1511.00561" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>59.9</td><td>-</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_SegNet" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><p><em>My comments</em>:</p><ul><li>FCN and SegNet are one of the first encoder-decoder architectures.</li><li>Benchmarks for SegNet are not good enough to be used anymore.</li></ul><h4 id="Dilated-Convolutions"><a href="#Dilated-Convolutions" class="headerlink" title="Dilated Convolutions"></a>Dilated Convolutions</h4><ul><li>Multi-Scale Context Aggregation by Dilated Convolutions</li><li>Submitted on 23 Nov 2015</li><li><a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Use dilated convolutions, a convolutional layer for dense predictions.</li><li>Propose ‘context module’ which uses dilated convolutions for multi scale aggregation.</li></ul><p><em>Explanation</em>:</p><p>Pooling helps in classification networks because receptive field increases. But this is not the best thing to do for segmentation because pooling decreases the resolution. Therefore, authors use <em>dilated convolution</em> layer which works like this:</p><p><img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif" alt="Dilated/Atrous Convolutions"><br>Dilated/Atrous Convolutions. <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="external">Source</a></p><p>Dilated convolutional layer (also called as atrous convolution in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLab</a>) allows for exponential increase in field of view without decrease of spatial dimensions.</p><p>Last two pooling layers from pretrained classification network (here, VGG) are removed and subsequent convolutional layers are replaced with dilated convolutions. In particular, convolutions between the pool-3 and pool-4 have dilation 2 and convolutions after pool-4 have dilation 4. With this module (called <em>frontend module</em> in the paper), dense predictions are obtained without any increase in number of parameters.</p><p>A module (called <em>context module</em> in the paper) is trained separately with the outputs of frontend module as inputs. This module is a cascade of dilated convolutions of different dilations so that multi scale context is aggregated and predictions from frontend are improved.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>71.3</td><td>frontend</td><td>reported in the paper</td></tr><tr><td>73.5</td><td>frontend + context</td><td>reported in the paper</td></tr><tr><td>74.7</td><td>frontend + context + CRF</td><td>reported in the paper</td></tr><tr><td>75.3</td><td>frontend + context + CRF-RNN</td><td>reported in the paper</td></tr></tbody></table><p><em>My comments</em>:</p><ul><li>Note that predicted segmentation map’s size is 1/8th of that of the image. This is the case with almost all the approaches. They are interpolated to get the final segmentation map.</li></ul><h4 id="DeepLab-v1-amp-v2"><a href="#DeepLab-v1-amp-v2" class="headerlink" title="DeepLab (v1 &amp; v2)"></a>DeepLab (v1 &amp; v2)</h4><ul><li><strong>v1</strong> : Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</li><li>Submitted on 22 Dec 2014</li><li><a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="external">Arxiv Link</a></li><li></li><li><strong>v2</strong> : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</li><li>Submitted on 2 Jun 2016</li><li><a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Use atrous/dilated convolutions.</li><li>Propose atrous spatial pyramid pooling (ASPP)</li><li>Use Fully connected CRF</li></ul><p><em>Explanation</em>:</p><p>Atrous/Dilated convolutions increase the field of view without increasing the number of parameters. Net is modified like in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions paper</a>.</p><p>Multiscale processing is achieved either by passing multiple rescaled versions of original images to parallel CNN branches (Image pyramid) and/or by using multiple parallel atrous convolutional layers with different sampling rates (ASPP).</p><p>Structured prediction is done by fully connected CRF. CRF is trained/tuned separately as a post processing step.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/deeplabv2.png" alt="DeepLab2 Pipeline"><br>DeepLab2 Pipeline. <a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>79.7</td><td>ResNet-101 + atrous Convolutions + ASPP + CRF</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6&amp;submid=6103#KEY_DeepLabv2-CRF" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><h4 id="RefineNet"><a href="#RefineNet" class="headerlink" title="RefineNet"></a>RefineNet</h4><ul><li>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</li><li>Submitted on 20 Nov 2016</li><li><a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Encoder-Decoder architecture with well thought-out decoder blocks</li><li>All the components follow residual connection design</li></ul><p><em>Explanation</em>:</p><p>Approach of using dilated/atrous convolutions are not without downsides. Dilated convolutions are computationally expensive and take a lot of memory because they have to be applied on large number of high resolution feature maps. This hampers the computation of high-res predictions. <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLab’s</a> predictions, for example are 1/8th the size of original input.</p><p>So, the paper proposes to use encoder-decoder architecture. Encoder part is ResNet-101 blocks. Decoder has RefineNet blocks which concatenate/fuse high resolution features from encoder and low resolution features from previous RefineNet block.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/refinenet%20-%20architecture.png" alt="RefineNet Architecture"><br>RefineNet Architecture. <a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">Source</a>.</p><p>Each RefineNet block has a component to fuse the multi resolution features by upsampling the lower resolution features and a component to capture context based on repeated 5 x 5 <em>stride 1</em> pool layers. Each of these components employ the residual connection design following the identity map mindset.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/refinenet%20-%20block.png" alt="RefineNet Block"><br>RefineNet Block. <a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>84.2</td><td>Uses CRF, Multiscale inputs, COCO pretraining</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_Multipath-RefineNet" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><h4 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h4><ul><li>Pyramid Scene Parsing Network</li><li>Submitted on 4 Dec 2016</li><li><a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Propose pyramid pooling module to aggregate the context.</li><li>Use auxiliary loss</li></ul><p><em>Explanation</em>:</p><p>Global scene categories matter because it provides clues on the distribution of the segmentation classes. Pyramid pooling module captures this information by applying large kernel pooling layers.</p><p>Dilated convolutions are used as in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions paper</a> to modify Resnet and a pyramid pooling module is added to it. This module concatenates the feature maps from ResNet with upsampled output of parallel pooling layers with kernels covering whole, half of and small portions of image.</p><p>An auxiliary loss, additional to the loss on main branch, is applied after the fourth stage of ResNet (i.e input to pyramid pooling module). This idea was also called as intermediate supervision elsewhere.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/pspnet.png" alt="PSPNet Architecture"><br>PSPNet Architecture. <a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>85.4</td><td>MSCOCO pretraining, multi scale input, no CRF</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_PSPNet" target="_blank" rel="external">leaderboard</a></td></tr><tr><td>82.6</td><td>no MSCOCO pretraining, multi scale input, no CRF</td><td>reported in the paper</td></tr></tbody></table><h4 id="Large-Kernel-Matters"><a href="#Large-Kernel-Matters" class="headerlink" title="Large Kernel Matters"></a>Large Kernel Matters</h4><ul><li>Large Kernel Matters – Improve Semantic Segmentation by Global Convolutional Network</li><li>Submitted on 8 Mar 2017</li><li><a href="https://arxiv.org/abs/1703.02719" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Propose a encoder-decoder architecture with very large kernels convolutions</li></ul><p><em>Explanation</em>:</p><p>Semantic segmentation requires both segmentation and classification of the segmented objects. Since fully connected layers cannot be present in a segmentation architecture, convolutions with very large kernels are adopted instead.</p><p>Another reason to adopt large kernels is that although deeper networks like ResNet have very large receptive field, <a href="https://arxiv.org/abs/1412.6856" target="_blank" rel="external">studies</a> show that the network tends to gather information from a much smaller region (valid receptive filed).</p><p>Larger kernels are computationally expensive and have a lot of parameters. Therefore, k x k convolution is approximated with sum of 1 x k + k x 1 and k x 1 and 1 x k convolutions. This module is called as <em>Global Convolutional Network</em> (GCN) in the paper.</p><p>Coming to architecture, ResNet(without any dilated convolutions) forms encoder part of the architecture while GCNs and deconvolutions form decoder. A simple residual block called <em>Boundary Refinement</em> (BR) is also used.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/large_kernel_matter.png" alt="GCN Architecture"><br>GCN Architecture. <a href="https://arxiv.org/abs/1703.02719" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>82.2</td><td>-</td><td>reported in the paper</td></tr><tr><td>83.6</td><td>Improved training, not described in the paper</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_Large_Kernel_Matters" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><h4 id="DeepLab-v3"><a href="#DeepLab-v3" class="headerlink" title="DeepLab v3"></a>DeepLab v3</h4><ul><li>Rethinking Atrous Convolution for Semantic Image Segmentation</li><li>Submitted on 17 Jun 2017</li><li><a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Improved atrous spatial pyramid pooling (ASPP)</li><li>Module which employ atrous convolutions in cascade</li></ul><p><em>Explanation</em>:</p><p>ResNet model is modified to use dilated/atrous convolutions as in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLabv2</a> and <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions</a>. Improved ASPP involves concatenation of image-level features, a 1x1 convolution and three 3x3 atrous convolutions with different rates. Batch normalization is used after each of the parallel convolutional layers.</p><p>Cascaded module is a resnet block except that component convolution layers are made atrous with different rates. This module is similar to context module used in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions paper</a> but this is applied directly on intermediate feature maps instead of belief maps (belief maps are final CNN feature maps with channels equal to number of classes).</p><p>Both the proposed models are evaluated independently and attempt to combine the both did not improve the performance. Both of them performed very similarly on val set with ASPP performing slightly better. CRF is not used.</p><p>Both these models outperform the best model from <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLabv2</a>. Authors note that the improvement comes from the batch normalization and better way to encode multi scale context.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/deeplabv3.png" alt="DeepLabv3 ASPP"><br>DeepLabv3 ASPP (used for submission). <a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>85.7</td><td>used ASPP (no cascaded modules)</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_DeepLabv3" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><p>Reblog from <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#sec-2" target="_blank" rel="external">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;What-exactly-is-semantic-segmentation&quot;&gt;&lt;a href=&quot;#What-exactly-is-semantic-segmentation&quot; class=&quot;headerlink&quot; title=&quot;What exactly is se
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="semantic segmentation" scheme="http://yoursite.com/tags/semantic-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN</title>
    <link href="http://yoursite.com/2018/04/10/A-Brief-History-of-CNNs-in-Image-Segmentation-From-R-CNN-to-Mask-R-CNN/"/>
    <id>http://yoursite.com/2018/04/10/A-Brief-History-of-CNNs-in-Image-Segmentation-From-R-CNN-to-Mask-R-CNN/</id>
    <published>2018-04-10T08:34:47.000Z</published>
    <updated>2018-04-10T12:05:59.651Z</updated>
    
    <content type="html"><![CDATA[<p>Ever since <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">Alex Krizhevsky, Geoff Hinton, and Ilya Sutskever won ImageNet in 2012</a>, Convolutional Neural Networks(CNNs) have become the gold standard for image classification. In fact, since then, CNNs have improved to the point where they now outperform humans on the ImageNet challenge!</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*bGTawFxQwzc5yV1_szDrwQ.png" alt="img"></p><p>CNNs now outperform humans on the ImageNet challenge. The y-axis in the above graph is the error rate on ImageNet.</p><p>While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*8GVucX9yhnL21KCtcyFDRQ.png" alt="img"></p><p>An example of an image used in the classification challenge. Note how the image is well framed and has just one object.</p><p>In classification, there’s generally an image with a single object as the focus and the task is to say what that image is (see above). But when we look at the world around us, we carry out far more complex tasks.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*eJjj2TVUVZDiVSTcnzh7fA.png" alt="img"></p><p>Sights in real life are often composed of a multitude of different, overlapping objects, backgrounds, and actions.</p><p>We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*NdwfHMrW3rpj5SW_VQtWVw.png" alt="img"></p><p>In image segmentation, our goal is to classify the different objects in the image, and identify their boundaries. Source: Mask R-CNN paper.</p><p>Can CNNs help us with such complex tasks? Namely, given a more complicated image, can we use CNNs to identify the different objects in the image, and their boundaries? As has been shown by Ross Girshick and his peers over the last few years, the answer is conclusively yes.</p><h4 id="Goals-of-this-Post"><a href="#Goals-of-this-Post" class="headerlink" title="Goals of this Post"></a>Goals of this Post</h4><p>Through this post, we’ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they’ve evolved from one implementation to the next. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Finally, we’ll cover Mask R-CNN, a paper released recently by Facebook Research that extends such object detection techniques to provide pixel level segmentation. Here are the papers referenced in this post:</p><ol><li>R-CNN: <a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">https://arxiv.org/abs/1311.2524</a></li><li>Fast R-CNN: <a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="external">https://arxiv.org/abs/1504.08083</a></li><li>Faster R-CNN: <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a></li><li>Mask R-CNN: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a></li></ol><hr><h4 id="2014-R-CNN-An-Early-Application-of-CNNs-to-Object-Detection"><a href="#2014-R-CNN-An-Early-Application-of-CNNs-to-Object-Detection" class="headerlink" title="2014: R-CNN - An Early Application of CNNs to Object Detection"></a>2014: R-CNN - An Early Application of CNNs to Object Detection</h4><p><img src="https://cdn-images-1.medium.com/max/1000/1*r9ELExnk1B1zHnRReDW9Ow.png" alt="img"></p><p>Object detection algorithms such as R-CNN take in an image and identify the locations and classifications of the main objects in the image. Source: <a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">https://arxiv.org/abs/1311.2524</a>.</p><p>Inspired by the research of Hinton’s lab at the University of Toronto, a small team at UC Berkeley, led by Professor Jitendra Malik, asked themselves what today seems like an inevitable question:</p><blockquote><p>To what extent do [Krizhevsky et. al’s results] generalize to object detection?</p></blockquote><p>Object detection is the task of finding the different objects in an image and classifying them (as seen in the image above). The team, comprised of Ross Girshick (a name we’ll see again), Jeff Donahue, and Trevor Darrel found that this problem can be solved with Krizhevsky’s results by testing on the PASCAL VOC Challenge, a popular object detection challenge akin to ImageNet. They write,</p><blockquote><p>This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features.</p></blockquote><p>Let’s now take a moment to understand how their architecture, Regions With CNNs (R-CNN) works.</p><p><strong>Understanding R-CNN</strong></p><p>The goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image.</p><ul><li><strong>Inputs</strong>: Image</li><li><strong>Outputs</strong>: Bounding boxes + labels for each object in the image.</li></ul><p>But how do we find out where these bounding boxes are? R-CNN does what we might intuitively do as well - <strong>propose</strong> <strong>a bunch of boxes in the image and see if any of them actually correspond to an object</strong>.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*ZQ03Ib84bYioFKoho5HnKg.png" alt="img"></p><p>Selective Search looks through windows of multiple scales and looks for adjacent pixels that share textures, colors, or intensities. Image source: <a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="external">https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf</a></p><p>R-CNN creates these bounding boxes, or region proposals, using a process called Selective Search which you can read about <a href="http://www.cs.cornell.edu/courses/cs7670/2014sp/slides/VisionSeminar14.pdf" target="_blank" rel="external">here</a>. At a high level, Selective Search (shown in the image above) looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.</p><p><img src="https://cdn-images-1.medium.com/max/1000/0*Sdj6sKDRQyZpO6oH." alt="img"></p><p>After creating a set of region proposals, R-CNN passes the image through a modified version of AlexNet to determine whether or not it is a valid region. Source: <a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">https://arxiv.org/abs/1311.2524</a>.</p><p>Once the proposals are created, R-CNN warps the region to a standard square size and passes it through to a modified version of AlexNet (the winning submission to ImageNet 2012 that inspired R-CNN), as shown above.</p><p>On the final layer of the CNN, R-CNN adds a Support Vector Machine (SVM) that simply classifies whether this is an object, and if so what object. This is step 4 in the image above.</p><p><strong>Improving the Bounding Boxes</strong></p><p>Now, having found the object in the box, can we tighten the box to fit the true dimensions of the object? We can, and this is the final step of R-CNN. R-CNN runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result. Here are the inputs and outputs of this regression model:</p><ul><li><strong>Inputs</strong>: sub-regions of the image corresponding to objects.</li><li><strong>Outputs</strong>: New bounding box coordinates for the object in the sub-region.</li></ul><p>So, to summarize, R-CNN is just the following steps:</p><ol><li>Generate a set of proposals for bounding boxes.</li><li>Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is.</li><li>Run the box through a linear regression model to output tighter coordinates for the box once the object has been classified.</li></ol><hr><h4 id="2015-Fast-R-CNN-Speeding-up-and-Simplifying-R-CNN"><a href="#2015-Fast-R-CNN-Speeding-up-and-Simplifying-R-CNN" class="headerlink" title="2015: Fast R-CNN - Speeding up and Simplifying R-CNN"></a>2015: Fast R-CNN - Speeding up and Simplifying R-CNN</h4><p><img src="https://cdn-images-1.medium.com/max/1000/1*3xnXHBEAz6FGzb-EehXtkA.png" alt="img"></p><p>Ross Girshick wrote both R-CNN and Fast R-CNN. He continues to push the boundaries of Computer Vision at Facebook Research.</p><p>R-CNN works really well, but is really quite slow for a few simple reasons:</p><ol><li>It requires a forward pass of the CNN (AlexNet) for every single region proposal for every single image (that’s around 2000 forward passes per image!).</li><li>It has to train three different models separately - the CNN to generate image features, the classifier that predicts the class, and the regression model to tighten the bounding boxes. This makes the pipeline extremely hard to train.</li></ol><p>In 2015, Ross Girshick, the first author of R-CNN, solved both these problems, leading to the second algorithm in our short history - Fast R-CNN. Let’s now go over its main insights.</p><p><strong>Fast R-CNN Insight 1: RoI (Region of Interest) Pooling</strong></p><p>For the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). His insight was simple — <strong>Why not run the CNN just once per image and then find a way to share that computation across the ~2000 proposals?</strong></p><p><img src="https://cdn-images-1.medium.com/max/1000/1*4K_Bq1AhAsTe9vlT0wsdXQ.png" alt="img"></p><p>In RoIPool, a full forward pass of the image is created and the conv features for each region of interest are extracted from the resulting forward pass. Source: Stanford’s CS231N slides by Fei Fei Li, Andrei Karpathy, and Justin Johnson.</p><p>This is exactly what Fast R-CNN does using a technique known as RoIPool (Region of Interest Pooling). At its core, RoIPool shares the forward pass of a CNN for an image across its subregions. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. Then, the features in each region are pooled (usually using max pooling). So all it takes us is one pass of the original image as opposed to ~2000!</p><p><strong>Fast R-CNN Insight 2: Combine All Models into One Network</strong></p><p><img src="https://cdn-images-1.medium.com/max/1000/1*E_P1vAEbGT4HNYjqMtIz4g.png" alt="img"></p><p>Fast R-CNN combined the CNN, classifier, and bounding box regressor into one, single network. Source: <a href="https://www.slideshare.net/simplyinsimple/detection-52781995" target="_blank" rel="external">https://www.slideshare.net/simplyinsimple/detection-52781995</a>.</p><p>The second insight of Fast R-CNN is to jointly train the CNN, classifier, and bounding box regressor in a single model. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), <strong>Fast R-CNN instead used a single network to compute all three.</strong></p><p>You can see how this was done in the image above. Fast R-CNN replaced the SVM classifier with a softmax layer on top of the CNN to output a classification. It also added a linear regression layer parallel to the softmax layer to output bounding box coordinates. In this way, all the outputs needed came from one single network! Here are the inputs and outputs to this overall model:</p><ul><li><strong>Inputs</strong>: Images with region proposals.</li><li><strong>Outputs</strong>: Object classifications of each region along with tighter bounding boxes.</li></ul><hr><h4 id="2016-Faster-R-CNN-Speeding-Up-Region-Proposal"><a href="#2016-Faster-R-CNN-Speeding-Up-Region-Proposal" class="headerlink" title="2016: Faster R-CNN - Speeding Up Region Proposal"></a>2016: Faster R-CNN - Speeding Up Region Proposal</h4><p>Even with all these advancements, there was still one remaining bottleneck in the Fast R-CNN process — the region proposer. As we saw, the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test. In Fast R-CNN, these proposals were created using <strong>Selective Search</strong>, a fairly slow process that was found to be the bottleneck of the overall process.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*xY9rmw06KZWQlNIPk6ItqA.png" alt="img"></p><p>Jian Sun, a principal researcher at Microsoft Research, led the team behind Faster R-CNN. Source: <a href="https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdp" target="_blank" rel="external">https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdp</a></p><p>In the middle 2015, a team at Microsoft Research composed of Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, found a way to make the region proposal step almost cost free through an architecture they (creatively) named Faster R-CNN.</p><p>The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). <strong>So why not reuse those same CNN results for region proposals instead of running a separate selective search algorithm?</strong></p><p><img src="https://cdn-images-1.medium.com/max/1000/0*_nNI03ESXm2P6YXO." alt="img"></p><p>In Faster R-CNN, a single CNN is used for region proposals, and classifications. Source: <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a>.</p><p>Indeed, this is just what the Faster R-CNN team achieved. In the image above, you can see how a single CNN is used to both carry out region proposals and classification. This way, <strong>only one CNN needs to be trained</strong> and we get region proposals almost for free! The authors write:</p><blockquote><p>Our observation is that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region proposals [thus enabling nearly cost-free region proposals].</p></blockquote><p>Here are the inputs and outputs of their model:</p><ul><li><strong>Inputs</strong>: Images (Notice how region proposals are not needed).</li><li><strong>Outputs</strong>: Classifications and bounding box coordinates of objects in the images.</li></ul><p><strong>How the Regions are Generated</strong></p><p>Let’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Faster R-CNN adds a Fully Convolutional Network on top of the features of the CNN creating what’s known as the <strong>Region Proposal Network</strong>.</p><p><img src="https://cdn-images-1.medium.com/max/1000/0*n6pZEyvW47nlcdQz." alt="img"></p><p>The Region Proposal Network slides a window over the features of the CNN. At each window location, the network outputs a score and a bounding box per anchor (hence 4k box coordinates where k is the number of anchors). Source: <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a>.</p><p>The Region Proposal Network works by passing a sliding window over the CNN feature map and at each window, outputting <strong>k </strong>potential bounding boxes and scores for how good each of those boxes is expected to be. What do these <strong>k </strong>boxes represent?</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*pJ3OTVXjtp9vWfBOPsnWIw.png" alt="img"></p><p>We know that the bounding boxes for people tend to be rectangular and vertical. We can use this intuition to guide our Region Proposal networks through creating an anchor of such dimensions. Image Source: <a href="http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignments/assignment1/bbox0062.jpg" target="_blank" rel="external">http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignments/assignment1/bbox0062.jpg</a>.</p><p>Intuitively, we know that objects in an image should fit certain common aspect ratios and sizes. For instance, we know that we want some rectangular boxes that resemble the shapes of humans. Likewise, we know we won’t see many boxes that are very very thin. In such a way, we create <strong>k</strong> such common aspect ratios we call <strong>anchor boxes</strong>. For each such anchor box, we output one bounding box and score per position in the image.</p><p>With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:</p><ul><li><strong>Inputs</strong>: CNN Feature Map.</li><li><strong>Outputs</strong>: A bounding box per anchor. A score representing how likely the image in that bounding box will be an object.</li></ul><p>We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes.</p><hr><h4 id="2017-Mask-R-CNN-Extending-Faster-R-CNN-for-Pixel-Level-Segmentation"><a href="#2017-Mask-R-CNN-Extending-Faster-R-CNN-for-Pixel-Level-Segmentation" class="headerlink" title="2017: Mask R-CNN - Extending Faster R-CNN for Pixel Level Segmentation"></a>2017: Mask R-CNN - Extending Faster R-CNN for Pixel Level Segmentation</h4><p><img src="https://cdn-images-1.medium.com/max/1000/1*E_5qBTrotLzclyaxsekBmQ.png" alt="img"></p><p>The goal of image instance segmentation is to identify, at a pixel level, what the different objets in a scene are. Source: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a>.</p><p>So far, we’ve seen how we’ve been able to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes.</p><p>Can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes? This problem, known as image segmentation, is what Kaiming He and a team of researchers, including Girshick, explored at Facebook AI using an architecture known as <strong>Mask R-CNN</strong>.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*cYW3EdKx75Stl1EreATdfw.png" alt="img"></p><p>Kaiming He, a researcher at Facebook AI, is lead author of Mask R-CNN and also a coauthor of Faster R-CNN.</p><p>Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward. Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel level segmentation?</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*BiRpf-ogjxARQf5LxI17Jw.png" alt="img"></p><p>In Mask R-CNN, a Fully Convolutional Network (FCN) is added on top of the CNN features of Faster R-CNN to generate a mask (segmentation output). Notice how this is in parallel to the classification and bounding box regression network of Faster R-CNN. Source: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a>.</p><p>Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch (in white in the above image), as before, is just a Fully Convolutional Network on top of a CNN based feature map. Here are its inputs and outputs:</p><ul><li><strong>Inputs</strong>: CNN Feature Map.</li><li><strong>Outputs</strong>: Matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a <a href="https://en.wikipedia.org/wiki/Mask_%28computing%29" target="_blank" rel="external">binary mask</a>).</li></ul><p>But the Mask R-CNN authors had to make one small adjustment to make this pipeline work as expected.</p><p><strong>RoiAlign - Realigning RoIPool to be More Accurate</strong></p><p><img src="https://cdn-images-1.medium.com/max/1000/0*KtaZfpUErYqwH4RX." alt="img"></p><p>Instead of RoIPool, the image gets passed through RoIAlign so that the regions of the feature map selected by RoIPool correspond more precisely to the regions of the original image. This is needed because pixel level segmentation requires more fine-grained alignment than bounding boxes. Source: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a>.</p><p>When run without modifications on the original Faster R-CNN architecture, the Mask R-CNN authors realized that the regions of the feature map selected by RoIPool were slightly misaligned from the regions of the original image. Since image segmentation requires pixel level specificity, unlike bounding boxes, this naturally led to inaccuracies.</p><p>The authors were able to solve this problem by cleverly adjusting RoIPool to be more precisely aligned using a method known as RoIAlign.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg" alt="img"></p><p>How do we accurately map a region of interest from the original image onto the feature map?</p><p>Imagine we have an image of size <strong>128x128</strong> and a feature map of size <strong>25x25</strong>. Let’s imagine we want features the region corresponding to the top-left <strong>15x15</strong>pixels in the original image (see above). How might we select these pixels from the feature map?</p><p>We know each pixel in the original image corresponds to ~ 25/128 pixels in the feature map. To select 15 pixels from the original image, we just select 15 <em>25/128 ~= <em>*2.93</em></em> pixels.</p><p>In RoIPool, we would round this down and select 2 pixels causing a slight misalignment. However, in RoIAlign, <strong>we avoid such rounding.</strong> Instead, we use <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation" target="_blank" rel="external">bilinear interpolation</a> to get a precise idea of what would be at pixel 2.93. This, at a high level, is what allows us to avoid the misalignments caused by RoIPool.</p><p>Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:</p><p><img src="https://cdn-images-1.medium.com/max/1250/1*6CClgIKH8zhZjmcftfNoEQ.png" alt="img"></p><p>Mask R-CNN is able to segment as well as classify the objects in an image. Source: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a>.</p><hr><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p>If you’re interested in trying out these algorithms yourselves, here are relevant repositories:</p><p><strong>Faster R-CNN</strong></p><ul><li>Caffe: <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external">https://github.com/rbgirshick/py-faster-rcnn</a></li><li>PyTorch: <a href="https://github.com/longcw/faster_rcnn_pytorch" target="_blank" rel="external">https://github.com/longcw/faster_rcnn_pytorch</a></li><li>MatLab: <a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="external">https://github.com/ShaoqingRen/faster_rcnn</a></li></ul><p><strong>Mask R-CNN</strong></p><ul><li>PyTorch: <a href="https://github.com/felixgwu/mask_rcnn_pytorch" target="_blank" rel="external">https://github.com/felixgwu/mask_rcnn_pytorch</a></li><li>TensorFlow: <a href="https://github.com/CharlesShang/FastMaskRCNN" target="_blank" rel="external">https://github.com/CharlesShang/FastMaskRCNN</a></li></ul><p>Reblog from <a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" target="_blank" rel="external">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ever since &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&quot; target=&quot;_blank&quot; re
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="Deep learning" scheme="http://yoursite.com/tags/Deep-learning/"/>
    
      <category term="CNN" scheme="http://yoursite.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>Understanding nested list comprehension syntax in Python</title>
    <link href="http://yoursite.com/2018/03/27/Understanding-nested-list-comprehension-syntax-in-Python/"/>
    <id>http://yoursite.com/2018/03/27/Understanding-nested-list-comprehension-syntax-in-Python/</id>
    <published>2018-03-27T13:04:02.000Z</published>
    <updated>2018-03-27T13:06:48.955Z</updated>
    
    <content type="html"><![CDATA[<p>List comprehensions are one of the really nice and powerful features of Python. It is actually a smart way to introduce new users to functional programming concepts (after all a list comprehension is just a combination of map and filter) and compact statements.</p><p>However, one thing that always troubled me when using list comprehensions is their non intuitive syntax when nesting was needed. For example, let’s say that we just want to flatten a list of lists using a nested list comprehension:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">non_flat = [ [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>] ]</div></pre></td></tr></table></figure><p>To write that, somebody would think: For a simple list comprehension I need to write <code>[ x for x in non_flat ]</code> to get all its items - however I want to retrieve each element of the <code>x</code> list so I’ll write something like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>[y <span class="keyword">for</span> y <span class="keyword">in</span> x <span class="keyword">for</span> x <span class="keyword">in</span> non_flat]</div><div class="line">[<span class="number">7</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>]</div></pre></td></tr></table></figure><p>Well duh! At this time I’d need research google for a working list comprehension syntax and adjust it to my needs (or give up and write it as a double for loop).</p><p>Here’s the correct nested list comprehension people wondering:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>[y <span class="keyword">for</span> x <span class="keyword">in</span> non_flat <span class="keyword">for</span> y <span class="keyword">in</span> x]</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</div></pre></td></tr></table></figure><p>What if I wanted to add a third level of nesting or an if? Well I’d just bite the bullet and use for loops!</p><p>However, if you take a look at the document describing list comprehensions in python (PEP202) you’ll see the following phrase:</p><blockquote><p>It is proposed to allow conditional construction of list literals using for and if clauses. <strong>They would nest in the same way for loops and if statements nest now.</strong></p></blockquote><p>This statement explains everything! <em>Just think in for-loops syntax</em>. So, If I used for loops for the previous flattening, I’d do something like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> x <span class="keyword">in</span> non_flat:</div><div class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> x:</div><div class="line">        y</div></pre></td></tr></table></figure><p>which, if y is moved to the front and joined in one line would be the correct nested list comprehension!</p><p>So that’s the way… What If I wanted to include only lists with more than 2 elements in the flattening (so [7,8] should not be included)? I’ll write it with for loops first:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> x <span class="keyword">in</span> non_flat:</div><div class="line">    <span class="keyword">if</span> len(x) &gt; <span class="number">2</span></div><div class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> x:</div><div class="line">            y</div></pre></td></tr></table></figure><p>so by convering this to list comprehension we get:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>[ y <span class="keyword">for</span> x <span class="keyword">in</span> non_flat <span class="keyword">if</span> len(x) &gt; <span class="number">2</span> <span class="keyword">for</span> y <span class="keyword">in</span> x ]</div><div class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</div></pre></td></tr></table></figure><p>Success!</p><p>One final, more complex example: Let’s say that we have a list of lists of words and we want to get a list of all the letters of these words along with the index of the list they belong to but only for words with more than two characters. Using the same for-loop syntax for the nested list comprehensions we’ll get:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>strings = [ [<span class="string">'foo'</span>, <span class="string">'bar'</span>], [<span class="string">'baz'</span>, <span class="string">'taz'</span>], [<span class="string">'w'</span>, <span class="string">'koko'</span>] ]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>[ (letter, idx) <span class="keyword">for</span> idx, lst <span class="keyword">in</span> enumerate(strings) <span class="keyword">for</span> word <span class="keyword">in</span> lst <span class="keyword">if</span> len(word)&gt;<span class="number">2</span> <span class="keyword">for</span> letter <span class="keyword">in</span> word]</div><div class="line">[(<span class="string">'f'</span>, <span class="number">0</span>), (<span class="string">'o'</span>, <span class="number">0</span>), (<span class="string">'o'</span>, <span class="number">0</span>), (<span class="string">'b'</span>, <span class="number">0</span>), (<span class="string">'a'</span>, <span class="number">0</span>), (<span class="string">'r'</span>, <span class="number">0</span>), (<span class="string">'b'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'z'</span>, <span class="number">1</span>), (<span class="string">'t'</span>, <span class="number">1</span>), (<span class="string">'a'</span>, <span class="number">1</span>), (<span class="string">'z'</span>, <span class="number">1</span>), (<span class="string">'k'</span>, <span class="number">2</span>), (<span class="string">'o'</span>, <span class="number">2</span>), (<span class="string">'k'</span>, <span class="number">2</span>), (<span class="string">'o'</span>, <span class="number">2</span>)]</div></pre></td></tr></table></figure><hr><p>source blog is <a href="https://spapas.github.io/2016/04/27/python-nested-list-comprehensions/" target="_blank" rel="external">here</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;List comprehensions are one of the really nice and powerful features of Python. It is actually a smart way to introduce new users to func
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python多核编程mpi4py实践</title>
    <link href="http://yoursite.com/2018/03/19/Python%E5%A4%9A%E6%A0%B8%E7%BC%96%E7%A8%8Bmpi4py%E5%AE%9E%E8%B7%B5/"/>
    <id>http://yoursite.com/2018/03/19/Python多核编程mpi4py实践/</id>
    <published>2018-03-19T05:25:36.000Z</published>
    <updated>2018-03-19T05:39:28.570Z</updated>
    
    <content type="html"><![CDATA[<p>转载自<a href="http://blog.csdn.net/ztf312/article/details/74997939" target="_blank" rel="external">这篇博文</a>.</p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>​ CPU从三十多年前的8086，到十年前的奔腾，再到当下的多核i7。一开始，以单核cpu的主频为目标，架构的改良和集成电路工艺的进步使得cpu的性能高速上升，单核cpu的主频从老爷车的MHz阶段一度接近4GHz高地。然而，也因为工艺和功耗等的限制，单核cpu遇到了人生的天花板，急需转换思维，以满足无止境的性能需求。多核cpu在此登上历史舞台。给你的老爷车多加两个引擎，让你有法拉利的感觉。现时代，连手机都到处叫嚣自己有4核8核处理器的时代，PC就更不用说了。</p><p>​ 扯远了，anyway，对于俺们程序员来说，如何利用如此强大的引擎完成我们的任务才是我们要考虑的。随着大规模数据处理、大规模问题和复杂系统求解需求的增加，以前的单核编程已经有心无力了。如果程序一跑就得几个小时，甚至一天，想想都无法原谅自己。那如何让自己更快的过度到高大上的多核并行编程中去呢？哈哈，广大人民的力量！</p><p>​ 目前工作中我所接触到的并行处理框架主要有MPI、OpenMP和MapReduce(Hadoop)三个（CUDA属于GPU并行编程，这里不提及）。MPI和Hadoop都可以在集群中运行，而OpenMP因为共享存储结构的关系，不能在集群上运行，只能单机。另外，MPI可以让数据保留在内存中，可以为节点间的通信和数据交互保存上下文，所以能执行迭代算法，而Hadoop却不具有这个特性。因此，需要迭代的机器学习算法大多使用MPI来实现。当然了，部分机器学习算法也是可以通过设计使用Hadoop来完成的。（浅见，如果错误，希望各位不吝指出，谢谢）。</p><p>​ 本文主要介绍Python环境下MPI编程的实践基础。</p><h1 id="MPI与mpi4py"><a href="#MPI与mpi4py" class="headerlink" title="MPI与mpi4py"></a>MPI与mpi4py</h1><p>​ MPI是Message Passing Interface的简称，也就是消息传递。消息传递指的是并行执行的各个进程具有自己独立的堆栈和代码段，作为互不相关的多个程序独立执行，进程之间的信息交互完全通过显示地调用通信函数来完成。</p><p>​ Mpi4py是构建在mpi之上的python库，使得python的数据结构可以在进程（或者多个cpu）之间进行传递。</p><h2 id="MPI的工作方式"><a href="#MPI的工作方式" class="headerlink" title="MPI的工作方式"></a>MPI的工作方式</h2><p>​ 很简单，就是你启动了一组MPI进程，每个进程都是执行同样的代码！然后每个进程都有一个ID，也就是rank来标记我是谁。什么意思呢？假设一个CPU是你请的一个工人，共有10个工人。你有100块砖头要搬，然后很公平，让每个工人搬10块。这时候，你把任务写到一个任务卡里面，让10个工人都执行这个任务卡中的任务，也就是搬砖！这个任务卡中的“搬砖”就是你写的代码。然后10个CPU执行同一段代码。需要注意的是，代码里面的所有变量都是每个进程独有的，虽然名字相同。</p><p>​ 例如，一个脚本test.py，里面包含以下代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from mpi4py import MPI  </div><div class="line">print("hello world'')  </div><div class="line">print("my rank is: %d" %MPI.rank)</div></pre></td></tr></table></figure><p>​ 然后我们在命令行通过以下方式运行：</p><p>​ <code>mpirun –np 5 python test.py</code></p><p>​ <code>-np5</code> 指定启动5个mpi进程来执行后面的程序。相当于对脚本拷贝了5份，每个进程运行一份，互不干扰。在运行的时候代码里面唯一的不同，就是各自的rank也就是ID不一样。所以这个代码就会打印5个hello world和5个不同的rank值，从0到4.</p><h2 id="点对点通信"><a href="#点对点通信" class="headerlink" title="点对点通信"></a>点对点通信</h2><p>​ 点对点通信（Point-to-PointCommunication）的能力是信息传递系统最基本的要求。意思就是让两个进程直接可以传输数据，也就是一个发送数据，另一个接收数据。接口就两个，send和recv，来个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="comment"># point to point communication  </span></div><div class="line">data_send = [comm_rank]*<span class="number">5</span>  </div><div class="line">comm.send(data_send,dest=(comm_rank+<span class="number">1</span>)%comm_size)  </div><div class="line">data_recv =comm.recv(source=(comm_rank<span class="number">-1</span>)%comm_size)  </div><div class="line">print(<span class="string">"my rank is %d, and Ireceived:"</span> % comm_rank)  </div><div class="line"><span class="keyword">print</span> data_recv</div></pre></td></tr></table></figure><p>​ 启动5个进程运行以上代码，结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">my rank <span class="keyword">is</span> <span class="number">0</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]  </div><div class="line">my rank <span class="keyword">is</span> <span class="number">1</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]  </div><div class="line">my rank <span class="keyword">is</span> <span class="number">2</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]  </div><div class="line">my rank <span class="keyword">is</span> <span class="number">3</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]  </div><div class="line">my rank <span class="keyword">is</span> <span class="number">4</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]</div></pre></td></tr></table></figure><p>​ 可以看到，每个进程都创建了一个数组，然后把它传递给下一个进程，最后的那个进程传递给第一个进程。<code>comm_size</code>就是mpi的进程个数，也就是<code>-np</code>指定的那个数。<code>MPI.COMM_WORLD</code>表示进程所在的通信组。</p><p>​ 但这里面有个需要注意的问题，如果我们要发送的数据比较小的话，mpi会缓存我们的数据，也就是说执行到<code>send</code>这个代码的时候，会缓存被send的数据，然后继续执行后面的指令，而不会等待对方进程执行<code>recv</code>指令接收完这个数据。但是，如果要发送的数据很大，那么进程就是挂起等待，直到接收进程执行了<code>recv</code>指令接收了这个数据，进程才继续往下执行。所以上述的代码发送[rank]<em>5没啥问题，如果发送[rank]</em>500程序就会半死不活的样子了。因为所有的进程都会卡在发送这条指令，等待下一个进程发起接收的这个指令，但是进程是执行完发送的指令才能执行接收的指令，这就和死锁差不多了。所以一般，我们将其修改成以下的方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line">data_send = [comm_rank]*<span class="number">5</span>  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   comm.send(data_send, dest=(comm_rank+<span class="number">1</span>)%comm_size)  </div><div class="line"><span class="keyword">if</span> comm_rank &gt; <span class="number">0</span>:  </div><div class="line">   data_recv = comm.recv(source=(comm_rank<span class="number">-1</span>)%comm_size)  </div><div class="line">   comm.send(data_send, dest=(comm_rank+<span class="number">1</span>)%comm_size)  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data_recv = comm.recv(source=(comm_rank<span class="number">-1</span>)%comm_size)  </div><div class="line">print(<span class="string">"my rank is %d, and Ireceived:"</span> % comm_rank)  </div><div class="line"><span class="keyword">print</span> data_recv</div></pre></td></tr></table></figure><p>​ 第一个进程一开始就发送数据，其他进程一开始都是在等待接收数据，这时候进程1接收了进程0的数据，然后发送进程1的数据，进程2接收了，再发送进程2的数据……知道最后进程0接收最后一个进程的数据，从而避免了上述问题。</p><p>​ 一个比较常用的方法是封一个组长，也就是一个主进程，一般是进程0作为主进程leader。主进程将数据发送给其他的进程，其他的进程处理数据，然后返回结果给进程0。换句话说，就是进程0来控制整个数据处理流程。</p><h2 id="群体通信"><a href="#群体通信" class="headerlink" title="群体通信"></a>群体通信</h2><p>​ 点对点通信是A发送给B，一个人将自己的秘密告诉另一个人，群体通信（Collective Communications）像是拿个大喇叭，一次性告诉所有的人。前者是一对一，后者是一对多。但是，群体通信是以更有效的方式工作的。它的原则就一个：尽量把所有的进程在所有的时刻都使用上！我们在下面的bcast小节讲述。</p><p>​ 群体通信还是发送和接收两类，一个是一次性把数据发给所有人，另一个是一次性从所有人那里回收结果。</p><h3 id="广播bcast"><a href="#广播bcast" class="headerlink" title="广播bcast"></a>广播bcast</h3><p>​ 将一份数据发送给所有的进程。例如我有200份数据，有10个进程，那么每个进程都会得到这200份数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data = range(comm_size)  </div><div class="line">data = comm.bcast(data <span class="keyword">if</span> comm_rank == <span class="number">0</span><span class="keyword">else</span> <span class="keyword">None</span>, root=<span class="number">0</span>)  </div><div class="line"><span class="keyword">print</span> <span class="string">'rank %d, got:'</span> % (comm_rank)  </div><div class="line"><span class="keyword">print</span> data</div></pre></td></tr></table></figure><p>​ 结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">rank <span class="number">0</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">1</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">2</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">3</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">4</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</div></pre></td></tr></table></figure><p>​ Root进程自己建了一个列表，然后广播给所有的进程。这样所有的进程都拥有了这个列表。然后爱干嘛就干嘛了。</p><p>​ 对广播最直观的观点是某个特定进程将数据一一发送给每个进程。假设有n个进程，那么假设我们的数据在0进程，那么0进程就需要将数据发送给剩下的n-1个进程，这是非常低效的，复杂度是O(n)。那有没有高效的方式？一个最常用也是非常高效的手段是规约树广播：收到广播数据的所有进程都参与到数据广播的过程中。首先只有一个进程有数据，然后它把它发送给第一个进程，此时有两个进程有数据；然后这两个进程都参与到下一次的广播中，这时就会有4个进程有数据，……，以此类推，每次都会有2的次方个进程有数据。通过这种规约树的广播方法，广播的复杂度降为O(log n)。这就是上面说的群体通信的高效原则：充分利用所有的进程来实现数据的发送和接收。</p><h3 id="散播scatter"><a href="#散播scatter" class="headerlink" title="散播scatter"></a>散播scatter</h3><p>​ 将一份数据平分给所有的进程。例如我有200份数据，有10个进程，那么每个进程会分别得到20份数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data = range(comm_size)  </div><div class="line">   <span class="keyword">print</span> data  </div><div class="line"><span class="keyword">else</span>:  </div><div class="line">   data = <span class="keyword">None</span>  </div><div class="line">local_data = comm.scatter(data, root=<span class="number">0</span>)  </div><div class="line"><span class="keyword">print</span> <span class="string">'rank %d, got:'</span> % comm_rank  </div><div class="line"><span class="keyword">print</span> local_data</div></pre></td></tr></table></figure><p>​ 结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">0</span>, got:  </div><div class="line"><span class="number">0</span>  </div><div class="line">rank <span class="number">1</span>, got:  </div><div class="line"><span class="number">1</span>  </div><div class="line">rank <span class="number">2</span>, got:  </div><div class="line"><span class="number">2</span>  </div><div class="line">rank <span class="number">3</span>, got:  </div><div class="line"><span class="number">3</span>  </div><div class="line">rank <span class="number">4</span>, got:  </div><div class="line"><span class="number">4</span></div></pre></td></tr></table></figure><p>​ 这里root进程创建了一个list，然后将它散播给所有的进程，相当于对这个list做了划分，每个进程获得等分的数据，这里就是list的每一个数。（主要根据list的索引来划分，list索引为第i份的数据就发送给第i个进程）。如果是矩阵，那么就等分的划分行，每个进程获得相同的行数进行处理。</p><p>​ 需要注意的是，MPI的工作方式是每个进程都会执行所有的代码，所以每个进程都会执行scatter这个指令，但只有root执行它的时候，它才兼备发送者和接收者的身份（root也会得到属于自己的数据），对于其他进程来说，他们都只是接收者而已。</p><h3 id="收集gather"><a href="#收集gather" class="headerlink" title="收集gather"></a>收集gather</h3><p>​ 那有发送，就有一起回收的函数。Gather是将所有进程的数据收集回来，合并成一个列表。下面联合scatter和gather组成一个完成的分发和收回过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data = range(comm_size)  </div><div class="line">   <span class="keyword">print</span> data  </div><div class="line"><span class="keyword">else</span>:  </div><div class="line">   data = <span class="keyword">None</span>  </div><div class="line">local_data = comm.scatter(data, root=<span class="number">0</span>)  </div><div class="line">local_data = local_data * <span class="number">2</span>  </div><div class="line"><span class="keyword">print</span> <span class="string">'rank %d, got and do:'</span> % comm_rank  </div><div class="line"><span class="keyword">print</span> local_data  </div><div class="line">combine_data = comm.gather(local_data,root=<span class="number">0</span>)  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">printcombine_data</div></pre></td></tr></table></figure><p>​ 结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">0</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">0</span>  </div><div class="line">rank <span class="number">1</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">2</span>  </div><div class="line">rank <span class="number">2</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">4</span>  </div><div class="line">rank <span class="number">4</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">8</span>  </div><div class="line">rank <span class="number">3</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">6</span>  </div><div class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>]</div></pre></td></tr></table></figure><p>​ Root进程将数据通过scatter等分发给所有的进程，等待所有的进程都处理完后（这里只是简单的乘以2），root进程再通过gather回收他们的结果，和分发的原则一样，组成一个list。Gather还有一个变体就是allgather，可以理解为它在gather的基础上将gather的结果再bcast了一次。啥意思？意思是root进程将所有进程的结果都回收统计完后，再把整个统计结果告诉大家。这样，不仅root可以访问combine_data，所有的进程都可以访问combine_data了。</p><h3 id="规约reduce"><a href="#规约reduce" class="headerlink" title="规约reduce"></a>规约reduce</h3><p>​ 规约是指不但将所有的数据收集回来，收集回来的过程中还进行了简单的计算，例如求和，求最大值等等。为什么要有这个呢？我们不是可以直接用gather全部收集回来了，再对列表求个sum或者max就可以了吗？这样不是累死组长吗？为什么不充分使用每个工人呢？规约实际上是使用规约树来实现的。例如求max，完成可以让工人两两pk后，再返回两两pk的最大值，然后再对第二层的最大值两两pk，直到返回一个最终的max给组长。组长就非常聪明的将工作分配下工人高效的完成了。这是O(n)的复杂度，下降到O(log n)（底数为2）的复杂度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data = range(comm_size)  </div><div class="line">   <span class="keyword">print</span> data  </div><div class="line"><span class="keyword">else</span>:  </div><div class="line">   data = <span class="keyword">None</span>  </div><div class="line">local_data = comm.scatter(data, root=<span class="number">0</span>)  </div><div class="line">local_data = local_data * <span class="number">2</span>  </div><div class="line"><span class="keyword">print</span> <span class="string">'rank %d, got and do:'</span> % comm_rank  </div><div class="line"><span class="keyword">print</span> local_data  </div><div class="line">all_sum = comm.reduce(local_data, root=<span class="number">0</span>,op=MPI.SUM)  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line"><span class="keyword">print</span> <span class="string">'sumis:%d'</span> % all_sum</div></pre></td></tr></table></figure><p>​ 结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">0</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">0</span>  </div><div class="line">rank <span class="number">1</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">2</span>  </div><div class="line">rank <span class="number">2</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">4</span>  </div><div class="line">rank <span class="number">3</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">6</span>  </div><div class="line">rank <span class="number">4</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">8</span>  </div><div class="line">sum <span class="keyword">is</span>:<span class="number">20</span></div></pre></td></tr></table></figure><p>​ 可以看到，最后可以得到一个sum值。</p><h1 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a>常见用法</h1><h2 id="对一个文件的多个行并行处理"><a href="#对一个文件的多个行并行处理" class="headerlink" title="对一个文件的多个行并行处理"></a>对一个文件的多个行并行处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!usr/bin/env python  </span></div><div class="line"><span class="comment">#-*- coding: utf-8 -*-  </span></div><div class="line">   </div><div class="line"><span class="keyword">import</span> sys  </div><div class="line"><span class="keyword">import</span> os  </div><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </div><div class="line">   </div><div class="line">   </div><div class="line"><span class="comment">#  </span></div><div class="line"><span class="comment">#  Global variables for MPI  </span></div><div class="line"><span class="comment">#  </span></div><div class="line">   </div><div class="line"><span class="comment"># instance for invoking MPI relatedfunctions  </span></div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line"><span class="comment"># the node rank in the whole community  </span></div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line"><span class="comment"># the size of the whole community, i.e.,the total number of working nodes in the MPI cluster  </span></div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line">   </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </div><div class="line">   <span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">       sys.stderr.write(<span class="string">"processor root starts reading data...\n"</span>)  </div><div class="line">       all_lines = sys.stdin.readlines()  </div><div class="line">   all_lines = comm.bcast(all_lines <span class="keyword">if</span> comm_rank == <span class="number">0</span> <span class="keyword">else</span> <span class="keyword">None</span>, root = <span class="number">0</span>)  </div><div class="line">   num_lines = len(all_lines)  </div><div class="line">   local_lines_offset = np.linspace(<span class="number">0</span>, num_lines, comm_size +<span class="number">1</span>).astype(<span class="string">'int'</span>)  </div><div class="line">   local_lines = all_lines[local_lines_offset[comm_rank] :local_lines_offset[comm_rank + <span class="number">1</span>]]  </div><div class="line">   sys.stderr.write(<span class="string">"%d/%d processor gets %d/%d data \n"</span> %(comm_rank, comm_size, len(local_lines), num_lines))  </div><div class="line">   cnt = <span class="number">0</span>  </div><div class="line">   <span class="keyword">for</span> line <span class="keyword">in</span> local_lines:  </div><div class="line">       fields = line.strip().split(<span class="string">'\t'</span>)  </div><div class="line">       cnt += <span class="number">1</span>  </div><div class="line">       <span class="keyword">if</span> cnt % <span class="number">100</span> == <span class="number">0</span>:  </div><div class="line">           sys.stderr.write(<span class="string">"processor %d has processed %d/%d lines \n"</span> %(comm_rank, cnt, len(local_lines)))  </div><div class="line">       output = line.strip() + <span class="string">' process every line here'</span>  </div><div class="line">       <span class="keyword">print</span> output</div></pre></td></tr></table></figure><h2 id="对多个文件并行处理"><a href="#对多个文件并行处理" class="headerlink" title="对多个文件并行处理"></a>对多个文件并行处理</h2><p>​ 如果我们的文件太大，例如几千万行，那么mpi是没办法将这么大的数据bcast给所有的进程的，所以我们可以先把大的文件split成小的文件，再让每个进程处理少数的文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!usr/bin/env python  </span></div><div class="line"><span class="comment">#-*- coding: utf-8 -*-  </span></div><div class="line">   </div><div class="line"><span class="keyword">import</span> sys  </div><div class="line"><span class="keyword">import</span> os  </div><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </div><div class="line">   </div><div class="line"><span class="comment">#  </span></div><div class="line"><span class="comment">#  Global variables for MPI  </span></div><div class="line"><span class="comment">#  </span></div><div class="line">   </div><div class="line"><span class="comment"># instance for invoking MPI relatedfunctions  </span></div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line"><span class="comment"># the node rank in the whole community  </span></div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line"><span class="comment"># the size of the whole community, i.e.,the total number of working nodes in the MPI cluster  </span></div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line">   </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </div><div class="line">   <span class="keyword">if</span> len(sys.argv) != <span class="number">2</span>:  </div><div class="line">       sys.stderr.write(<span class="string">"Usage: python *.py directoty_with_files\n"</span>)  </div><div class="line">       sys.exit(<span class="number">1</span>)  </div><div class="line">   path = sys.argv[<span class="number">1</span>]  </div><div class="line">   <span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">       file_list = os.listdir(path)  </div><div class="line">       sys.stderr.write(<span class="string">"%d files\n"</span> % len(file_list))  </div><div class="line">   file_list = comm.bcast(file_list <span class="keyword">if</span> comm_rank == <span class="number">0</span> <span class="keyword">else</span> <span class="keyword">None</span>, root = <span class="number">0</span>)  </div><div class="line">   num_files = len(file_list)  </div><div class="line">   local_files_offset = np.linspace(<span class="number">0</span>, num_files, comm_size +<span class="number">1</span>).astype(<span class="string">'int'</span>)  </div><div class="line">   local_files = file_list[local_files_offset[comm_rank] :local_files_offset[comm_rank + <span class="number">1</span>]]  </div><div class="line">   sys.stderr.write(<span class="string">"%d/%d processor gets %d/%d data \n"</span> %(comm_rank, comm_size, len(local_files), num_files))  </div><div class="line">    cnt = <span class="number">0</span>  </div><div class="line">   <span class="keyword">for</span> file_name <span class="keyword">in</span> local_files:  </div><div class="line">       hd = open(os.path.join(path, file_name))  </div><div class="line">       <span class="keyword">for</span> line <span class="keyword">in</span> hd:  </div><div class="line">           output = line.strip() + <span class="string">' process every line here'</span>  </div><div class="line">           <span class="keyword">print</span> output  </div><div class="line">       cnt += <span class="number">1</span>  </div><div class="line">       sys.stderr.write(<span class="string">"processor %d has processed %d/%d files \n"</span> %(comm_rank, cnt, len(local_files)))  </div><div class="line">       hd.close()</div></pre></td></tr></table></figure><h2 id="联合numpy对矩阵的多个行或者多列并行处理"><a href="#联合numpy对矩阵的多个行或者多列并行处理" class="headerlink" title="联合numpy对矩阵的多个行或者多列并行处理"></a>联合numpy对矩阵的多个行或者多列并行处理</h2><p>​ Mpi4py一个非常优秀的特性是完美支持numpy！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os, sys, time  </div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </div><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">   </div><div class="line"><span class="comment">#  </span></div><div class="line"><span class="comment">#  Global variables for MPI  </span></div><div class="line"><span class="comment">#  </span></div><div class="line">   </div><div class="line">   </div><div class="line"><span class="comment"># instance for invoking MPI relatedfunctions  </span></div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line"><span class="comment"># the node rank in the whole community  </span></div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line"><span class="comment"># the size of the whole community, i.e.,the total number of working nodes in the MPI cluster  </span></div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="comment"># test MPI  </span></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:  </div><div class="line">    <span class="comment">#create a matrix  </span></div><div class="line">   <span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">       all_data = np.arange(<span class="number">20</span>).reshape(<span class="number">4</span>, <span class="number">5</span>)  </div><div class="line">       <span class="keyword">print</span> <span class="string">"************ data ******************"</span>  </div><div class="line">       <span class="keyword">print</span> all_data  </div><div class="line">     </div><div class="line">    <span class="comment">#broadcast the data to all processors  </span></div><div class="line">   all_data = comm.bcast(all_data <span class="keyword">if</span> comm_rank == <span class="number">0</span> <span class="keyword">else</span> <span class="keyword">None</span>, root = <span class="number">0</span>)  </div><div class="line">     </div><div class="line">    <span class="comment">#divide the data to each processor  </span></div><div class="line">   num_samples = all_data.shape[<span class="number">0</span>]  </div><div class="line">   local_data_offset = np.linspace(<span class="number">0</span>, num_samples, comm_size + <span class="number">1</span>).astype(<span class="string">'int'</span>)  </div><div class="line">     </div><div class="line">    <span class="comment">#get the local data which will be processed in this processor  </span></div><div class="line">   local_data = all_data[local_data_offset[comm_rank] :local_data_offset[comm_rank + <span class="number">1</span>]]  </div><div class="line">   <span class="keyword">print</span> <span class="string">"****** %d/%d processor gets local data ****"</span> %(comm_rank, comm_size)  </div><div class="line">   <span class="keyword">print</span> local_data  </div><div class="line">     </div><div class="line">    <span class="comment">#reduce to get sum of elements  </span></div><div class="line">   local_sum = local_data.sum()  </div><div class="line">   all_sum = comm.reduce(local_sum, root = <span class="number">0</span>, op = MPI.SUM)  </div><div class="line">     </div><div class="line">    <span class="comment">#process in local  </span></div><div class="line">   local_result = local_data ** <span class="number">2</span>  </div><div class="line">     </div><div class="line">    <span class="comment">#gather the result from all processors and broadcast it  </span></div><div class="line">   result = comm.allgather(local_result)  </div><div class="line">   result = np.vstack(result)  </div><div class="line">     </div><div class="line">   <span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">       <span class="keyword">print</span> <span class="string">"*** sum: "</span>, all_sum  </div><div class="line">       <span class="keyword">print</span> <span class="string">"************ result ******************"</span>  </div><div class="line">       <span class="keyword">print</span> result</div></pre></td></tr></table></figure><h1 id="MPI和mpi4py的环境搭建"><a href="#MPI和mpi4py的环境搭建" class="headerlink" title="MPI和mpi4py的环境搭建"></a>MPI和mpi4py的环境搭建</h1><p>​ 这章放到这里是作为一个附录。我们的环境是linux，需要安装的包有python、openmpi、numpy、cpython和mpi4py，过程如下：</p><h2 id="安装Python"><a href="#安装Python" class="headerlink" title="安装Python"></a>安装Python</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">tar xzvf Python-2.7.tgz  </div><div class="line">cd Python-2.7  </div><div class="line">./configure--prefix=/home/work/vis/zouxiaoyi/my_tools  </div><div class="line">make  </div><div class="line">make install</div></pre></td></tr></table></figure><p>​ 先将Python放到环境变量里面，还有Python的插件库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">exportPATH=/home/work/vis/zouxiaoyi/my_tools/bin:$PATH  </div><div class="line">exportLD_LIBRARY_PATH=/home/work/vis/zouxiaoyi/my_tools/lib:$LD_LIBRARY_PATH</div></pre></td></tr></table></figure><p>​ 执行<code>python</code>，如果看到可爱的&gt;&gt;&gt;出来，就表示成功了。按<code>crtl+d</code>退出</p><h2 id="安装openmpi"><a href="#安装openmpi" class="headerlink" title="安装openmpi"></a>安装openmpi</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">wget http://www.open-mpi.org/software/ompi/v1.4/downloads/openmpi-1.4.1.tar.gz  </div><div class="line">tar xzvf openmpi-1.4.1.tar.gz  </div><div class="line">cd openmpi-1.4.1  </div><div class="line">./configure--prefix=/home/work/vis/zouxiaoyi/my_tools  </div><div class="line">make -j 8  </div><div class="line">make install</div></pre></td></tr></table></figure><p>​ 然后把bin路径加到环境变量里面：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">exportPATH=/home/work/vis/zouxiaoyi/my_tools/bin:$PATH  </div><div class="line">exportLD_LIBRARY_PATH=/home/work/vis/zouxiaoyi/my_tools/lib:$LD_LIBRARY_PATH</div></pre></td></tr></table></figure><p>​ 执行<code>mpirun</code>，如果有帮助信息打印出来，就表示安装好了。需要注意的是，我安装了几个版本都没有成功，最后安装了1.4.1这个版本才能成功，因此就看你的人品了。</p><h2 id="安装numpy和Cython"><a href="#安装numpy和Cython" class="headerlink" title="安装numpy和Cython"></a>安装numpy和Cython</h2><p>​ 安装python库的方法可以参考<a href="http://blog.csdn.net/zouxy09/article/details/48903179" target="_blank" rel="external">之前的博客</a>。过程一般如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tar –xgvf Cython-0.20.2.tar.gz  </div><div class="line">cd Cython-0.20.2  </div><div class="line">python setup.py install</div></pre></td></tr></table></figure><p>​ 打开Python，import Cython，如果没有报错，就表示安装成功了</p><h2 id="安装mpi4py"><a href="#安装mpi4py" class="headerlink" title="安装mpi4py"></a>安装mpi4py</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tar –xgvf mpi4py_1.3.1.tar.gz  </div><div class="line">cd mpi4py  </div><div class="line">vi mpi.cfg</div></pre></td></tr></table></figure><p>​ 在68行，<code>[openmpi]</code>下面，将刚才已经安装好的openmpi的目录给改上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mpi_dir = /home/work/vis/zouxiaoyi/my_tools  </div><div class="line">python setup.py install</div></pre></td></tr></table></figure><p>​ 打开Python，<code>import mpi4py as MPI</code>，如果没有报错，就表示安装成功了</p><p>​ 下面就可以开始属于你的并行之旅了，勇敢探索多核的乐趣吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转载自&lt;a href=&quot;http://blog.csdn.net/ztf312/article/details/74997939&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;这篇博文&lt;/a&gt;.&lt;/p&gt;&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; c
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>A Visual Guide to Evolution Strategies</title>
    <link href="http://yoursite.com/2018/01/29/A-Visual-Guide-to-Evolution-Strategies/"/>
    <id>http://yoursite.com/2018/01/29/A-Visual-Guide-to-Evolution-Strategies/</id>
    <published>2018-01-29T13:23:43.000Z</published>
    <updated>2018-01-29T14:13:09.010Z</updated>
    
    <content type="html"><![CDATA[<p>Source post is <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/" target="_blank" rel="external">here</a>.</p><p><img src="http://blog.otoro.net/assets/20171031/es_bear.jpeg" alt="img"><br><em>Survival of the fittest.</em></p><p>In this post I explain how evolution strategies (ES) work with the aid of a few visual examples. I try to keep the equations light, and I provide links to original articles if the reader wishes to understand more details. This is the first post in a series of articles, where I plan to show how to apply these algorithms to a range of tasks from MNIST, OpenAI Gym, Roboschool to PyBullet environments.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Neural network models are highly expressive and flexible, and if we are able to find a suitable set of model parameters, we can use neural nets to solve many challenging problems. Deep learning’s success largely comes from the ability to use the backpropagation algorithm to efficiently calculate the gradient of an objective function over each model parameter. With these gradients, we can efficiently search over the parameter space to find a solution that is often good enough for our neural net to accomplish difficult tasks.</p><p>However, there are many problems where the backpropagation algorithm cannot be used. For example, in reinforcement learning (RL) problems, we can also train a neural network to make decisions to perform a sequence of actions to accomplish some task in an environment. However, it is not trivial to estimate the gradient of reward signals given to the agent in the future to an action performed by the agent right now, especially if the reward is realised many timesteps in the future. Even if we are able to calculate accurate gradients, there is also the issue of being stuck in a local optimum, which exists many for RL tasks.</p><p><img src="http://blog.otoro.net/assets/20171031/biped/biped_local_optima.gif" alt="img"></p><p><em>Stuck in a local optimum.</em></p><p>A whole area within RL is devoted to studying this credit-assignment problem, and great progress has been made in recent years. However, credit assignment is still difficult when the reward signals are sparse. In the real world, rewards can be sparse and noisy. Sometimes we are given just a single reward, like a bonus check at the end of the year, and depending on our employer, it may be difficult to figure out exactly why it is so low. For these problems, rather than rely on a very noisy and possibly meaningless gradient estimate of the future to our policy, we might as well just ignore any gradient information, and attempt to use black-box optimisation techniques such as genetic algorithms (GA) or ES.</p><p>OpenAI published a paper called <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a> where they showed that evolution strategies, while being less data efficient than RL, offer many benefits. The ability to abandon gradient calculation allows such algorithms to be evaluated more efficiently. It is also easy to distribute the computation for an ES algorithm to thousands of machines for parallel computation. By running the algorithm from scratch many times, they also showed that policies discovered using ES tend to be more diverse compared to policies discovered by RL algorithms.</p><p>I would like to point out that even for the problem of identifying a machine learning model, such as designing a neural net’s architecture, is one where we cannot directly compute gradients. While <a href="https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html" target="_blank" rel="external">RL</a>, <a href="https://arxiv.org/abs/1703.00548" target="_blank" rel="external">Evolution</a>, <a href="http://blog.otoro.net/2016/05/07/backprop-neat/" target="_blank" rel="external">GA</a> etc., can be applied to search in the space of model architectures, in this post, I will focus only on applying these algorithms to search for parameters of a pre-defined model.</p><h2 id="What-is-an-Evolution-Strategy"><a href="#What-is-an-Evolution-Strategy" class="headerlink" title="What is an Evolution Strategy?"></a>What is an Evolution Strategy?</h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/8/8b/Rastrigin_function.png" alt="img"><br><em>Two-dimensional Rastrigin function has many local optima (Source: Wikipedia</em>).</p><p>The diagrams below are top-down plots of <em>shifted</em> 2D <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization" target="_blank" rel="external">Schaffer and Rastrigin</a> functions, two of several simple toy problems used for testing continuous black-box optimisation algorithms. Lighter regions of the plots represent higher values of $F(x,y)$. As you can see, there are many local optimums in this function. Our job is to find a set of <em>model parameters</em> $(x, y)$, such that $F(x,y)$ is as close as possible to the global maximum.</p><p><em>Schaffer-2D Function</em><br><img src="http://blog.otoro.net/assets/20171031/schaffer/schaffer_label.png" alt="img"></p><p><em>Rastrigin-2D Function</em><br><img src="http://blog.otoro.net/assets/20171031/rastrigin/rastrigin_label.png" alt="img"></p><p>Although there are many definitions of evolution strategies, we can define an evolution strategy as an algorithm that provides the user a set of candidate solutions to evaluate a problem. The evaluation is based on an <em>objective function</em> that takes a given solution and returns a single <em>fitness</em> value. Based on the fitness results of the current solutions, the algorithm will then produce the next generation of candidate solutions that is more likely to produce even better results than the current generation. The iterative process will stop once the best known solution is satisfactory for the user.</p><p>Given an evolution strategy algorithm called <code>EvolutionStrategy</code>, we can use in the following way:</p><hr><p><code>solver = EvolutionStrategy()</code></p><p><code>while True:</code></p><p><code># ask the ES to give us a set of candidate solutions</code><br><code>solutions = solver.ask()</code></p><p><code># create an array to hold the fitness results.</code><br><code>fitness_list = np.zeros(solver.popsize)</code></p><p><code># evaluate the fitness for each given solution.</code><br><code>for i in range(solver.popsize):</code><br><code>fitness_list[i] = evaluate(solutions[i])</code></p><p><code># give list of fitness results back to ES</code><br><code>solver.tell(fitness_list)</code></p><p><code># get best parameter, fitness from ES</code><br><code>best_solution, best_fitness = solver.result()</code></p><p><code>if best_fitness &gt; MY_REQUIRED_FITNESS:</code><br><code>break</code></p><hr><p>Although the size of the population is usually held constant for each generation, they don’t need to be. The ES can generate as many candidate solutions as we want, because the solutions produced by an ES are <em>sampled</em> from a distribution whose parameters are being updated by the ES at each generation. I will explain this sampling process with an example of a simple evolution strategy.</p><h2 id="Simple-Evolution-Strategy"><a href="#Simple-Evolution-Strategy" class="headerlink" title="Simple Evolution Strategy"></a>Simple Evolution Strategy</h2><p>One of the simplest evolution strategy we can imagine will just sample a set of solutions from a Normal distribution, with a mean \muμand a fixed standard deviation \sigmaσ. In our 2D problem, \mu = (\mu_x, \mu_y)μ=(μx,μy) and \sigma = (\sigma_x, \sigma_y)σ=(σx,σy). Initially, \muμ is set at the origin. After the fitness results are evaluated, we set \muμ to the best solution in the population, and sample the next generation of solutions around this new mean. This is how the algorithm behaves over 20 generations on the two problems mentioned earlier:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/simplees.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/simplees.gif" alt="img"></p><p>In the visualisation above, the green dot indicates the mean of the distribution at each generation, the blue dots are the sampled solutions, and the red dot is the best solution found so far by our algorithm.</p><p>This simple algorithm will generally only work for simple problems. Given its greedy nature, it throws away all but the best solution, and can be prone to be stuck at a local optimum for more complicated problems. It would be beneficial to sample the next generation from a probability distribution that represents a more diverse set of ideas, rather than just from the best solution from the current generation.</p><h2 id="Simple-Genetic-Algorithm"><a href="#Simple-Genetic-Algorithm" class="headerlink" title="Simple Genetic Algorithm"></a>Simple Genetic Algorithm</h2><p>One of the oldest black-box optimisation algorithms is the genetic algorithm. There are many variations with many degrees of sophistication, but I will illustrate the simplest version here.</p><p>The idea is quite simple: keep only 10% of the best performing solutions in the current generation, and let the rest of the population die. In the next generation, to sample a new solution is to randomly select two solutions from the survivors of the previous generation, and recombine their parameters to form a new solution. This <em>crossover</em> recombination process uses a coin toss to determine which parent to take each parameter from. In the case of our 2D toy function, our new solution might inherit xx or yy from either parents with 50% chance. Gaussian noise with a fixed standard deviation will also be injected into each new solution after this recombination process.</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/simplega.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/simplega.gif" alt="img"></p><p>The figure above illustrates how the simple genetic algorithm works. The green dots represent members of the elite population from the previous generation, the blue dots are the offsprings to form the set of candidate solutions, and the red dot is the best solution.</p><p>Genetic algorithms help diversity by keeping track of a diverse set of candidate solutions to reproduce the next generation. However, in practice, most of the solutions in the elite surviving population tend to converge to a local optimum over time. There are more sophisticated variations of GA out there, such as <a href="http://people.idsia.ch/~juergen/gomez08a.pdf" target="_blank" rel="external">CoSyNe</a>, <a href="http://blog.otoro.net/2015/03/10/esp-algorithm-for-double-pendulum/" target="_blank" rel="external">ESP</a>, and <a href="http://blog.otoro.net/2016/05/07/backprop-neat/" target="_blank" rel="external">NEAT</a>, where the idea is to cluster similar solutions in the population together into different species, to maintain better diversity over time.</p><h2 id="Covariance-Matrix-Adaptation-Evolution-Strategy-CMA-ES"><a href="#Covariance-Matrix-Adaptation-Evolution-Strategy-CMA-ES" class="headerlink" title="Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)"></a>Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)</h2><p>A shortcoming of both the Simple ES and Simple GA is that our standard deviation noise parameter is fixed. There are times when we want to explore more and increase the standard deviation of our search space, and there are times when we are confident we are close to a good optima and just want to fine tune the solution. We basically want our search process to behave like this:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/cmaes.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes.gif" alt="img"></p><p>Amazing isn’it it? The search process shown in the figure above is produced by <a href="https://en.wikipedia.org/wiki/CMA-ES" target="_blank" rel="external">Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)</a>. CMA-ES an algorithm that can take the results of each generation, and adaptively increase or decrease the search space for the next generation. It will not only adapt for the mean $\mu$ and sigma $\sigma$ parameters, but will calculate the entire covariance matrix of the parameter space. At each generation, CMA-ES provides the parameters of a multi-variate normal distribution to sample solutions from. So how does it know how to increase or decrease the search space?</p><p>Before we discuss its methodology, let’s review how to estimate a <a href="https://en.wikipedia.org/wiki/Covariance_matrix" target="_blank" rel="external">covariance matrix</a>. This will be important to understand CMA-ES’s methodology later on. If we want to estimate the covariance matrix of our entire sampled population of size of $N$, we can do so using the set of equations below to calculate the maximum likelihood estimate of a covariance matrix $C$. We first calculate the means of each of the $x_i$ and $y_i$ in our population:<br>$$<br>\mu_x = \frac{1}{N} \sum_{i=1}^{N}x_i,<br>$$</p><p>$$<br>\mu_y = \frac{1}{N} \sum_{i=1}^{N}y_i.<br>$$</p><p>The terms of the 2x2 covariance matrix $C$ will be:<br>$$<br>\begin{align}<br>\sigma_x^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)^2, \\<br>\sigma_y^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(y_i - \mu_y)^2, \\<br>\sigma_{xy} &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y).<br>\end{align}<br>$$<br>Of course, these resulting mean estimates $\mu_x$ and $\mu_y$, and covariance terms $\sigma_x$, $\sigma_y$, $\sigma_{xy}$ will just be an estimate to the actual covariance matrix that we originally sampled from, and not particularly useful to us.</p><p>CMA-ES modifies the above covariance calculation formula in a clever way to make it adapt well to an optimisation problem. I will go over how it does this step-by-step. Firstly, it focuses on the best $N_{best}$ solutions in the current generation. For simplicity let’s set $N_{best}$ to be the best 25% of solutions. After sorting the solutions based on fitness, we calculate the mean $\mu^{(g+1)}$ of the next generation $(g+1)$ as the average of only the best 25% of the solutions in current population $(g)$, i.e.:<br>$$<br>\begin{align}<br>\mu_x^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}x_i, \\<br>\mu_y^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}y_i.<br>\end{align}<br>$$<br>Next, we use only the best 25% of the solutions to estimate the covariance matrix $C^{(g+1)}$ of the next generation, but the clever <em>hack</em> here is that it uses the <em>current</em> generation’s $\mu^{(g)}$, rather than the updated $\mu^{(g+1)}$ parameters that we had just calculated, in the calculation:<br>$$<br>\begin{align}<br>\sigma_x^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})^2, \\<br>\sigma_y^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(y_i - \mu_y^{(g)})^2, \\<br>\sigma_{xy}^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})(y_i - \mu_y^{(g)}).<br>\end{align}<br>$$<br>Armed with a set of $\mu_x$, $\mu_y$, $\sigma_x$, $\sigma_y$, and $\sigma_{xy}$ parameters for the next generation $(g+1)$, we can now sample the next generation of candidate solutions.</p><p>Below is a set of figures to visually illustrate how it uses the results from the current generation $(g)$ to construct the solutions in the next generation $(g+1)$:</p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step1.png" alt="img"></p><p><em>Step 1</em></p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step2.png" alt="img"></p><p><em>Step 2</em></p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step3.png" alt="img"></p><p><em>Step 3</em></p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step4.png" alt="img"></p><p><em>Step 4</em></p><ol><li>Calculate the fitness score of each candidate solution in generation $(g)$.</li><li>Isolates the best 25% of the population in generation $(g)$, in purple.</li><li>Using only the best solutions, along with the mean $\mu^{(g)}$ of the current generation (the green dot), calculate the covariance matrix $C^{(g+1)}$ of the next generation.</li><li>Sample a new set of candidate solutions using the updated mean $\mu^{(g+1)}$ and covariance matrix $C^{(g+1)}$.</li></ol><p>Let’s visualise the scheme one more time, on the entire search process on both problems:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/cmaes2.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes2.gif" alt="img"></p><p>Because CMA-ES can adapt both its mean and covariance matrix using information from the best solutions, it can decide to cast a wider net when the best solutions are far away, or narrow the search space when the best solutions are close by. My description of the CMA-ES algorithm for a 2D toy problem is highly simplified to get the idea across. For more details, I suggest reading the <a href="https://arxiv.org/abs/1604.00772" target="_blank" rel="external">CMA-ES Tutorial</a> prepared by Nikolaus Hansen, the author of CMA-ES.</p><p>This algorithm is one of the most popular gradient-free optimisation algorithms out there, and has been the algorithm of choice for many researchers and practitioners alike. The only real drawback is the performance if the number of model parameters we need to solve for is large, as the covariance calculation is $O(N^2)$, although recently there has been approximations to make it $O(N)$. CMA-ES is my algorithm of choice when the search space is less than a thousand parameters. I found it still usable up to ~ 10K parameters if I’m willing to be patient.</p><h2 id="Natural-Evolution-Strategies"><a href="#Natural-Evolution-Strategies" class="headerlink" title="Natural Evolution Strategies"></a>Natural Evolution Strategies</h2><hr><p><em>Imagine if you had built an artificial life simulator, and you sample a different neural network to control the behavior of each ant inside an ant colony. Using the Simple Evolution Strategy for this task will optimise for traits and behaviours that benefit individual ants, and with each successive generation, our population will be full of alpha ants who only care about their own well-being.</em></p><p><em>Instead of using a rule that is based on the survival of the fittest ants, what if you take an alternative approach where you take the sum of all fitness values of the entire ant population, and optimise for this sum instead to maximise the well-being of the entire ant population over successive generations? Well, you would end up creating a Marxist utopia.</em></p><hr><p>A perceived weakness of the algorithms mentioned so far is that they discard the majority of the solutions and only keep the best solutions. Weak solutions contain information about what <em>not</em> to do, and this is valuable information to calculate a better estimate for the next generation.</p><p>Many people who studied RL are familiar with the <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> paper. In this 1992 paper, Williams outlined an approach to estimate the gradient of the expected rewards with respect to the model parameters of a policy neural network. This paper also proposed using REINFORCE as an Evolution Strategy, in Section 6 of the paper. This special case of <em>REINFORCE-ES</em> was expanded later on in <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Parameter-Exploring Policy Gradients</a> (PEPG, 2009) and <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a> (NES, 2014).</p><p>In this approach, we want to use all of the information from each member of the population, good or bad, for estimating a gradient signal that can move the entire population to a better direction in the next generation. Since we are estimating a gradient, we can also use this gradient in a standard SGD update rule typically used for deep learning. We can even use this estimated gradient with Momentum SGD, RMSProp, or Adam if we want to.</p><p>The idea is to maximise the <em>expected value</em> of the fitness score of a sampled solution. If the expected result is good enough, then the best performing member within a sampled population will be even better, so optimising for the expectation might be a sensible approach. Maximising the expected fitness score of a sampled solution is almost the same as maximising the total fitness score of the entire population.</p><p>If $z$ is a solution vector sampled from a probability distribution function $\pi(z, \theta)$, we can define the expected value of the objective function $F$ as:<br>$$<br>J(\theta) = E_{\theta}[F(z)] = \int F(z) \; \pi(z, \theta) \; dz,<br>$$<br>where $\theta$ are the parameters of the probability distribution function. For example, if $\pi$ is a normal distribution, then $\theta$ would be \muμand $\sigma$. For our simple 2D toy problems, each ensemble $z$ is a 2D vector $(x, y)$.</p><p>The <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">NES paper</a> contains a nice derivation of the gradient of $J(\theta)$ with respect to $\theta$. Using the same <em>log-likelihood trick</em> as in the REINFORCE algorithm allows us to calculate the gradient of $J(\theta)$:<br>$$<br>\nabla_{\theta} J(\theta) = E_{\theta}[ \; F(z) \; \nabla_{\theta} \log \pi(z, \theta) \; ].<br>$$<br>In a population size of $N$, where we have solutions $z^1, z^2, … z^N$, we can estimate this gradient as a summation:<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).<br>$$<br>With this gradient $\nabla_{\theta} J(\theta)$, we can use a learning rate parameter \alphaα (such as 0.01) and start optimising the $\theta$ parameters of pdf $\pi$ so that our sampled solutions will likely get higher fitness scores on the objective function $F$. Using SGD (or Adam), we can update $\theta$ for the next generation:<br>$$<br>\theta \rightarrow \theta + \alpha \nabla_{\theta} J(\theta),<br>$$<br>and sample a new set of candidate solutions $z$ from this updated pdf, and continue until we arrive at a satisfactory solution.</p><p>In Section 6 of the <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> paper, Williams derived closed-form formulas of the gradient $\nabla_{\theta} \log \pi(z^i, \theta)$, for the special case where $ \pi(z, \theta)$ is a factored multi-variate normal distribution (i.e., the correlation parameters are zero). In this special case, $\theta$ are the $\mu$ and $\sigma$ vectors. Therefore, each element of a solution can be sampled from a univariate normal distribution $z_j \sim N(\mu_j, \sigma_j)$.</p><p>The closed-form formulas for $\nabla_{\theta} \log N(z^i, \theta)$, for each individual element of vector $\theta$ on each solution $i$ in the population can be derived as:<br>$$<br>\nabla_{\mu_{j}} \log N(z^i, \mu, \sigma) = \frac{z_j^i - \mu_j}{\sigma_j},<br>$$</p><p>$$<br>\nabla_{\sigma_{j}} \log N(z^i, \mu, \sigma) = \frac{(z_j^i - \mu_j)^2 - \sigma_j^2}{\sigma_j^3}.<br>$$</p><p>For clarity, I use the index of jj, to count across parameter space, and this is not to be confused with superscript $i$, used to count across each sampled member of the population. For our 2D problems, $z_1 = x, z_2 = y, \mu_1 = \mu_x, \mu_2 = \mu_y, \sigma_1 = \sigma_x, \sigma_2 = \sigma_y$ in this context.</p><p>These two formulas can be plugged back into the approximate gradient formula to derive explicit update rules for \muμ and \sigmaσ. In the papers mentioned above, they derived more explicit update rules, incorporated a <em>baseline</em>, and introduced other tricks such as antithetic sampling in PEPG, which is what my implementation is based on. NES proposed incorporating the inverse of the Fisher Information Matrix into the gradient update rule. But the concept is basically the same as other ES algorithms, where we update the mean and standard deviation of a multi-variate normal distribution at each new generation, and sample a new set of solutions from the updated distribution. Below is a visualization of this algorithm in action, following the formulas described above:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/pepg.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/pepg.gif" alt="img"></p><p>We see that this algorithm is able to dynamically change the $\sigma$’s to explore or fine tune the solution space as needed. Unlike CMA-ES, there is no correlation structure in our implementation, so we don’t get the diagonal ellipse samples, only the vertical or horizontal ones, although in principle we can derive update rules to incorporate the entire covariance matrix if we needed to, at the expense of computational efficiency.</p><p>I like this algorithm because like CMA-ES, the $\sigma$’s can adapt so our search space can be expanded or narrowed over time. Because the correlation parameter is not used in this implementation, the efficiency of the algorithm is $O(N)$ so I use PEPG if the performance of CMA-ES becomes an issue. I usually use PEPG when the number of model parameters exceed several thousand.</p><h2 id="OpenAI-Evolution-Strategy"><a href="#OpenAI-Evolution-Strategy" class="headerlink" title="OpenAI Evolution Strategy"></a>OpenAI Evolution Strategy</h2><p>In OpenAI’s <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">paper</a>, they implement an evolution strategy that is a special case of the REINFORCE-ES algorithm outlined earlier. In particular, \sigmaσ is fixed to a constant number, and only the \muμ parameter is updated at each generation. Below is how this strategy looks like, with a constant \sigmaσ parameter:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/openes.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/oes.gif" alt="img"></p><p>In addition to the simplification, this paper also proposed a modification of the update rule that is suitable for parallel computation across different worker machines. In their update rule, a large grid of random numbers have been pre-computed using a fixed seed. By doing this, each worker can reproduce the parameters of every other worker over time, and each worker needs only to communicate a single number, the final fitness result, to all of the other workers. This is important if we want to scale evolution strategies to thousands or even a million workers located on different machines, since while it may not be feasible to transmit an entire solution vector a million times at each generation update, it may be feasible to transmit only the final fitness results. In the paper, they showed that by using 1440 workers on Amazon EC2 they were able to solve the Mujoco Humanoid walking task in ~ 10 minutes.</p><p>I think in principle, this parallel update rule should work with the original algorithm where they can also adapt $\sigma$, but perhaps in practice, they wanted to keep the number of moving parts to a minimum for large-scale parallel computing experiments. This inspiring paper also discussed many other practical aspects of deploying ES for RL-style tasks, and I highly recommend going through it to learn more.</p><h2 id="Fitness-Shaping"><a href="#Fitness-Shaping" class="headerlink" title="Fitness Shaping"></a>Fitness Shaping</h2><p>Most of the algorithms above are usually combined with a <em>fitness shaping</em> method, such as the rank-based fitness shaping method I will discuss here. Fitness shaping allows us to avoid outliers in the population from dominating the approximate gradient calculation mentioned earlier:<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).<br>$$<br>If a particular $F(z^m)$ is much larger than other $F(z^i)$ in the population, then the gradient might become dominated by this outliers and increase the chance of the algorithm being stuck in a local optimum. To mitigate this, one can apply a rank transformation of the fitness. Rather than use the actual fitness function, we would rank the results and use an augmented fitness function which is proportional to the solution’s rank in the population. Below is a comparison of what the original set of fitness may look like, and what the ranked fitness looks like:</p><p><img src="http://blog.otoro.net/assets/20171031/ranked_fitness.svg" alt="img"></p><p>What this means is supposed we have a population size of 101. We would evaluate each population to the actual fitness function, and then sort the solutions based by their fitness. We will assign an augmented fitness value of -0.50 to the worse performer, -0.49 to the second worse solution, …, 0.49 to the second best solution, and finally a fitness value of 0.50 to the best solution. This augmented set of fitness values will be used to calculate the gradient update, instead of the actual fitness values. In a way, it is a similar to just applying Batch Normalization to the results, but more direct. There are alternative methods for fitness shaping but they all basically give similar results in the end.</p><p>I find fitness shaping to be very useful for RL tasks if the objective function is non-deterministic for a given policy network, which is often the cases on RL environments where maps are randomly generated and various opponents have random policies. It is less useful for optimising for well-behaved functions that are deterministic, and the use of fitness shaping can sometimes slow down the time it takes to find a good solution.</p><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><p>Although ES might be a way to search for more novel solutions that are difficult for gradient-based methods to find, it still vastly underperforms gradient-based methods on many problems where we can calculate high quality gradients. For instance, only an idiot would attempt to use a genetic algorithm for image classification. But sometimes <a href="https://blog.openai.com/nonlinear-computation-in-linear-networks/" target="_blank" rel="external">such people</a> do exist in the world, and sometimes these explorations can be fruitful!</p><p>Since all ML algorithms should be tested on MNIST, I also tried to apply these various ES algorithms to find weights for a small, simple 2-layer convnet used to classify MNIST, just to see where we stand compared to SGD. The convnet only has ~ 11k parameters so we can accommodate the slower CMA-ES algorithm. The code and the experiments are available <a href="https://github.com/hardmaru/pytorch_notebooks/tree/master/mnist_es" target="_blank" rel="external">here</a>.</p><p>Below are the results for various ES methods, using a population size of 101, over 300 epochs. We keep track of the model parameters that performed best on the entire training set at the end of each epoch, and evaluate this model once on the test set after 300 epochs. It is interesting how sometimes the test set’s accuracy is higher than the training set for the models that have lower scores.</p><table><thead><tr><th>Method</th><th>Train Set</th><th>Test Set</th></tr></thead><tbody><tr><td>Adam (BackProp) Baseline</td><td>99.8</td><td>98.9</td></tr><tr><td>Simple GA</td><td>82.1</td><td>82.4</td></tr><tr><td>CMA-ES</td><td>98.4</td><td>98.1</td></tr><tr><td>OpenAI-ES</td><td>96.0</td><td>96.2</td></tr><tr><td>PEPG</td><td>98.5</td><td>98.0</td></tr></tbody></table><p><img src="http://blog.otoro.net/assets/20171031/mnist_results.svg" alt="img"></p><p>We should take these results with a grain of salt, since they are based on a single run, rather than the average of 5-10 runs. The results based on a single-run seem to indicate that CMA-ES is the best at the MNIST task, but the PEPG algorithm is not that far off. Both of these algorithms achieved ~ 98% test accuracy, 1% lower than the SGD/ADAM baseline. Perhaps the ability to dynamically alter its covariance matrix, and standard deviation parameters over each generation allowed it to fine-tune its weights better than OpenAI’s simpler variation.</p><h2 id="Try-It-Yourself"><a href="#Try-It-Yourself" class="headerlink" title="Try It Yourself"></a>Try It Yourself</h2><p>There are probably open source implementations of all of the algorithms described in this article. The author of CMA-ES, Nikolaus Hansen, has been maintaining a numpy-based implementation of <a href="https://github.com/CMA-ES/pycma" target="_blank" rel="external">CMA-ES</a> with lots of bells and whistles. His python implementation introduced me to the training loop interface described earlier. Since this interface is quite easy to use, I also implemented the other algorithms such as Simple Genetic Algorithm, PEPG, and OpenAI’s ES using the same interface, and put it in a small python file called <code>es.py</code>, and also wrapped the original CMA-ES library in this small library. This way, I can quickly compare different ES algorithms by just changing one line:</p><hr><p><code>import es</code></p><p><code>#solver = es.SimpleGA(...)</code><br><code>#solver = es.PEPG(...)</code><br><code>#solver = es.OpenES(...)</code><br><code>solver = es.CMAES(...)</code></p><p><code>while True:</code></p><p><code>solutions = solver.ask()</code></p><p><code>fitness_list = np.zeros(solver.popsize)</code></p><p><code>for i in range(solver.popsize):</code><br><code>fitness_list[i] = evaluate(solutions[i])</code></p><p><code>solver.tell(fitness_list)</code></p><p><code>result = solver.result()</code></p><p><code>if result[1] &gt; MY_REQUIRED_FITNESS:</code><br><code>break</code></p><hr><p>You can look at <code>es.py</code> on <a href="https://github.com/hardmaru/estool/blob/master/es.py" target="_blank" rel="external">GitHub</a> and the IPython notebook <a href="https://github.com/hardmaru/estool/blob/master/simple_es_example.ipynb" target="_blank" rel="external">examples</a> using the various ES algorithms.</p><p>In this <a href="https://github.com/hardmaru/estool/blob/master/simple_es_example.ipynb" target="_blank" rel="external">IPython notebook</a> that accompanies <code>es.py</code>, I show how to use the ES solvers in <code>es.py</code> to solve a 100-Dimensional version of the Rastrigin function with even more local optimum points. The 100-D version is somewhat more challenging than the trivial 2D version used to produce the visualizations in this article. Below is a comparison of the performance for various algorithms discussed:</p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin10d.svg" alt="img"></p><p>On this 100-D Rastrigin problem, none of the optimisers got to the global optimum solution, although CMA-ES comes close. CMA-ES blows everything else away. PEPG is in 2nd place, and OpenAI-ES / Genetic Algorithm falls behind. I had to use an annealing schedule to gradually lower \sigmaσ for OpenAI-ES to make it perform better for this task.</p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin_cma_solution.png" alt="img"></p><p><em>Final solution that CMA-ES discovered for 100-D Rastrigin function.Global optimal solution is a 100-dimensional vector of exactly 10.</em></p><h2 id="References-and-Other-Links"><a href="#References-and-Other-Links" class="headerlink" title="References and Other Links"></a>References and Other Links</h2><p>Below are a few links to information related to evolutionary computing which I found useful or inspiring.</p><p>Image Credits of <a href="https://www.reddit.com/r/CryptoMarkets/comments/6qpla3/investing_in_icos_results_may_vary/" target="_blank" rel="external">Lemmings Jumping off a Cliff</a>. Your results may vary when investing in ICOs.</p><p>CMA-ES: <a href="https://github.com/CMA-ES" target="_blank" rel="external">Official Reference Implementation</a> on GitHub, <a href="https://arxiv.org/abs/1604.00772" target="_blank" rel="external">Tutorial</a>, Original CMA-ES <a href="http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaartic.pdf" target="_blank" rel="external">Paper</a> from 2001, Overview <a href="https://www.slideshare.net/OsamaSalaheldin2/cmaes-presentation" target="_blank" rel="external">Slides</a></p><p><a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</a> (REINFORCE), 1992.</p><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Parameter-Exploring Policy Gradients</a>, 2009.</p><p><a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a>, 2014.</p><p><a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a>, OpenAI, 2017.</p><p>Risto Miikkulainen’s <a href="http://nn.cs.utexas.edu/downloads/slides/miikkulainen.ijcnn13.pdf" target="_blank" rel="external">Slides</a> on Neuroevolution.</p><p>A Neuroevolution Approach to <a href="http://www.cs.utexas.edu/~ai-lab/?atari" target="_blank" rel="external">General Atari Game Playing</a>, 2013.</p><p>Kenneth Stanley’s Talk on <a href="https://youtu.be/dXQPL9GooyI" target="_blank" rel="external">Why Greatness Cannot Be Planned: The Myth of the Objective</a>, 2015.</p><p><a href="https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning" target="_blank" rel="external">Neuroevolution</a>: A Different Kind of Deep Learning. The quest to evolve neural networks through evolutionary algorithms.</p><p><a href="http://people.idsia.ch/~juergen/compressednetworksearch.html" target="_blank" rel="external">Compressed Network Search</a> Finds Complex Neural Controllers with a Million Weights.</p><p>Karl Sims <a href="https://youtu.be/JBgG_VSP7f8" target="_blank" rel="external">Evolved Virtual Creatures</a>, 1994.</p><p>Evolved <a href="https://youtu.be/euFvRfQRbLI" target="_blank" rel="external">Step Climbing</a> Creatures.</p><p>Super Mario World Agent <a href="https://youtu.be/qv6UVOQ0F44" target="_blank" rel="external">Mario I/O</a>, Mario Kart 64 <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/(https://github.com/nicknlsn/MarioKart64NEAT" target="_blank" rel="external">Controller using</a>) using <a href="https://www.cs.ucf.edu/~kstanley/neat.html" target="_blank" rel="external">NEAT Algorithm</a>.</p><p><a href="http://www.bionik.tu-berlin.de/institut/xstart.htm" target="_blank" rel="external">Ingo Rechenberg</a>, the inventor of Evolution Strategies.</p><p>A Tutorial on <a href="https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/" target="_blank" rel="external">Differential Evolution</a> with Python.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Source post is &lt;a href=&quot;http://blog.otoro.net/2017/10/29/visual-evolution-strategies/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;im
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Genetic Algorithm</title>
    <link href="http://yoursite.com/2018/01/23/Introduction-to-Genetic-Algorithm/"/>
    <id>http://yoursite.com/2018/01/23/Introduction-to-Genetic-Algorithm/</id>
    <published>2018-01-23T07:13:09.000Z</published>
    <updated>2018-01-23T07:14:51.190Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Intuition-behind-Genetic-Algorithms"><a href="#1-Intuition-behind-Genetic-Algorithms" class="headerlink" title="1. Intuition behind Genetic Algorithms"></a>1. Intuition behind Genetic Algorithms</h2><p>Let’s start with the famous quote by Charles Darwin:</p><blockquote><p><em>It is not the strongest of the species that survives, nor the most intelligent , but the one most responsive to change.</em></p></blockquote><p>You must be thinking what has this quote got to do with genetic algorithm? Actually, the entire concept of a genetic algorithm is based on the above line.</p><p>Let us understand with a basic example:</p><p>Let’s take a hypothetical situation where, you are head of a country, and in order to keep your city safe from bad things, you implement a policy like this.</p><ul><li>You select all the good people, and ask them to extend their generation by having their children.</li><li>This repeats for a few generations.</li><li>You will notice that now you have an entire population of good people.</li></ul><p>Now, that may not be entirely possible, but this example was just to help you understand the concept. So the basic idea was that we changed the input (i.e. population) such that we get better output (i.e. better country).</p><p>Now, I suppose you have got some intuition that the concept of a genetic algorithm is somewhat related to biology. So let’s us quickly grasp some little concepts, so that we can draw a parallel line between them.</p><h2 id="2-Biological-Inspiration"><a href="#2-Biological-Inspiration" class="headerlink" title="2. Biological Inspiration"></a>2. Biological Inspiration</h2><p>I am sure you would remember:</p><p><em>Cells are the basic building block of all living things.</em></p><p>Therefore in each cell, there is the same set of chromosomes. Chromosome are basically the strings of DNA.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153917/dna-300x194.png" alt="img"></p><p>Traditionally, these chromosomes are represented in binary as strings of 0’s and 1’s.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153928/gene.png" alt="img"></p><p>Source : <a href="https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_fundamentals.htm" target="_blank" rel="external">link</a></p><p>A chromosome consists of genes, commonly referred as blocks of DNA, where each gene encodes a specific trait, for example hair color or eye color.</p><p>I wanted you to recall these basics concept of biology before going further. Let’s get back and understand what actually is a genetic algorithm?</p><h2 id="3-What-is-a-Genetic-Algorithm"><a href="#3-What-is-a-Genetic-Algorithm" class="headerlink" title="3. What is a Genetic Algorithm?"></a>3. What is a Genetic Algorithm?</h2><p>Let’s get back to the example we discussed above and summarize what we did.</p><ol><li>Firstly, we defined our initial population as our countrymen.</li><li>We defined a function to classify whether is a person is good or bad.</li><li>Then we selected good people for mating to produce their off-springs.</li><li>And finally, these off-springs replace the bad people from the population and this process repeats.</li></ol><p>This is how genetic algorithm actually works, which basically tries to mimic the human evolution to some extent.</p><p>So to formalize a definition of a genetic algorithm, we can say that it is an optimization technique, which tries to find out such values of input so that we get the best output values or results.</p><p>The working of a genetic algorithm is also derived from biology, which is as shown in the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154007/steps-210x300.png" alt="img"></p><p>Source: <a href="https://www.neuraldesigner.com/blog/genetic_algorithms_for_feature_selection" target="_blank" rel="external">link</a></p><p>So, let us try to understand the steps one by one.</p><h2 id="4-Steps-Involved-in-Genetic-Algorithm"><a href="#4-Steps-Involved-in-Genetic-Algorithm" class="headerlink" title="4. Steps Involved in Genetic Algorithm"></a><strong>4. Steps Involved in Genetic Algorithm</strong></h2><p>Here, to make things easier, let us understand it by the famous <a href="https://en.wikipedia.org/wiki/Knapsack_problem" target="_blank" rel="external">Knapsack problem</a>.</p><p>If you haven’t come across this problem, let me introduce my version of this problem.</p><p>Let’s say, you are going to spend a month in the wilderness. Only thing you are carrying is the backpack which can hold a maximum weight of <strong>30 kg</strong>. Now you have different survival items, each having its own “Survival Points” (which are given for each item in the table). So, your objective is maximise the survival points.</p><p>Here is the table giving details about each item.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154400/table1-300x99.png" alt="img"></p><h3 id="4-1-Initialisation"><a href="#4-1-Initialisation" class="headerlink" title="4.1 Initialisation"></a>4.1 Initialisation</h3><p>To solve this problem using genetic algorithm, our first step would be defining our population. So our population will contain individuals, each having their own set of chromosomes.</p><p>We know that, chromosomes are binary strings, where for this problem 1 would mean that the following item is taken and 0 meaning that it is dropped.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22170200/Capture1-300x186.png" alt="img"></p><p>This set of chromosome is considered as our initial population.</p><h3 id="4-2-Fitness-Function"><a href="#4-2-Fitness-Function" class="headerlink" title="4.2 Fitness Function"></a>4.2 Fitness Function</h3><p>Let us calculate fitness points for our first two chromosomes.</p><p>For A1 chromosome [100110],</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/24105541/table-300x74.png" alt="img"></p><p>Similarly for A2 chromosome [001110],</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154417/table3-300x74.png" alt="img"></p><p>So, for this problem, our chromosome will be considered as more fit when it contains more survival points.</p><p>Therefore chromosome 1 is more fit than chromosome 2.</p><h3 id="4-3-Selection"><a href="#4-3-Selection" class="headerlink" title="4.3 Selection"></a>4.3 Selection</h3><p>Now, we can select fit chromosomes from our population which can mate and create their off-springs.</p><p>General thought is that we should select the fit chromosomes and allow them to produce off-springs. But that would lead to chromosomes that are more close to one another in a few next generation, and therefore less diversity.</p><p>Therefore, we generally use Roulette Wheel Selection method.</p><p>Don’t be afraid of name, just take a look at the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153953/roulette-wheel-300x188.jpg" alt="img"></p><p>I suppose we all have seen this, either in real or in movies. So, let’s build our roulette wheel.</p><p>Consider a wheel, and let’s divide that into m divisions, where m is the number of chromosomes in our populations. The area occupied by each chromosome will be proportional to its fitness value.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22155317/table4-300x72.png" alt="img"></p><p>Based on these values, let us create our roulette wheel.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22171149/roulette-300x204.png" alt="img"></p><p>So, now this wheel is rotated and the region of wheel which comes in front of the fixed point is chosen as the parent. For the second parent, the same process is repeated.</p><p>Sometimes we mark two fixed point as shown in the figure below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22171418/stio-300x194.png" alt="img"></p><p>So, in this method we can get both our parents in one go. This method is known as Stochastic Universal Selection method.</p><h3 id="4-4-Crossover"><a href="#4-4-Crossover" class="headerlink" title="4.4 Crossover"></a>4.4 Crossover</h3><p>So in this previous step, we have selected parent chromosomes that will produce off-springs. So in biological terms, crossover is nothing but reproduction.</p><p>So let us find the crossover of chromosome 1 and 4, which were selected in the previous step. Take a look at the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22173337/one-point-300x93.png" alt="img"></p><p>This is the most basic form of crossover, known as one point crossover. Here we select a random crossover point and the tails of both the chromosomes are swapped to produce a new off-springs.</p><p>If you take two crossover point, then it will called as multi point crossover which is as shown below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22174145/multi-point-300x101.png" alt="img"></p><h3 id="4-5-Mutation"><a href="#4-5-Mutation" class="headerlink" title="4.5 Mutation"></a>4.5 Mutation</h3><p>Now if you think in the biological sense, are the children produced have the same traits as their parents? The answer is NO. During their growth, there is some change in the genes of children which makes them different from its parents.</p><p>This process is known as mutation, which may be defined as a random tweak in the chromosome, which also promotes the idea of diversity in the population.</p><p>A simple method of mutation is shown in the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22174928/mutation-300x56.png" alt="img"></p><p>So the entire process is summarise as shown in the figure.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22175311/gadiagram-300x196.png" alt="img"></p><p>Source : <a href="http://www.jade-cheng.com/au/coalhmm/optimization/" target="_blank" rel="external">link</a></p><p>The off-springs thus produced are again validated using our fitness function, and if considered fit then will replace the less fit chromosomes from the population.</p><p>But the question is how we will get to know that we have reached our best possible solution?</p><p>So basically there are different termination conditions, which are listed below:</p><ol><li>There is no improvement in the population for over x iterations.</li><li>We have already predefined an absolute number of generation for our algorithm.</li><li>When our fitness function has reached a predefined value.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Intuition-behind-Genetic-Algorithms&quot;&gt;&lt;a href=&quot;#1-Intuition-behind-Genetic-Algorithms&quot; class=&quot;headerlink&quot; title=&quot;1. Intuition behin
    
    </summary>
    
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]</title>
    <link href="http://yoursite.com/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/"/>
    <id>http://yoursite.com/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/</id>
    <published>2018-01-22T08:49:43.000Z</published>
    <updated>2018-01-22T08:53:40.634Z</updated>
    
    <content type="html"><![CDATA[<p>Source blog is <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">here</a>.</p><hr><p>We’ve <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">discovered</a> that <strong>evolution strategies (ES)</strong>, an optimization technique that’s been known for decades, rivals the performance of standard <strong>reinforcement learning (RL)</strong>techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL’s inconveniences.</p><p>In particular, ES is simpler to implement (there is no need for <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">backpropagation</a>), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer <a href="https://www.quora.com/What-are-hyperparameters-in-machine-learning" target="_blank" rel="external">hyperparameters</a>. This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on <a href="https://en.wikipedia.org/wiki/Finite_difference" target="_blank" rel="external">finite differences</a> along a few random directions at each step.</p><p>Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">“AlexNet” paper</a> showed how to design, scale and train convolutional neural networks (CNNs) to achieve extremely strong results on image recognition tasks, at a time when most researchers thought that CNNs were not a promising approach to computer vision. Similarly, in 2013, the <a href="https://arxiv.org/abs/1312.5602" target="_blank" rel="external">Deep Q-Learning paper</a> showed how to combine Q-Learning with CNNs to successfully solve Atari games, reinvigorating RL as a research field with exciting experimental (rather than theoretical) results. Likewise, our work demonstrates that ES achieves strong performance on RL benchmarks, dispelling the common belief that ES methods are impossible to apply to high dimensional problems.</p><p>ES is easy to implement and scale. Running on a computing cluster of 80 machines and 1,440 CPU cores, our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes about 10 hours). Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour.</p><p>In what follows, we’ll first briefly describe the conventional RL approach, contrast that with our ES approach, discuss the tradeoffs between ES and RL, and finally highlight some of our experiments.</p><h4 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h4><p>Let’s briefly look at how RL works. Suppose we are given some environment (e.g. a game) that we’d like to train an agent on. To describe the behavior of the agent, we define a policy function (the brain of the agent), which computes how the agent should act in any given situation. In practice, the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions. A typical policy function might have about 1,000,000 parameters, so our task comes down to finding the precise setting of these parameters such that the policy plays well (i.e. wins a lot of games).</p><p><img src="https://blog.openai.com/content/images/2017/03/first-graphic-1.png" alt="img"></p><p><em>Above: In the game of Pong, the policy could take the pixels of the screen and compute the probability of moving the player’s paddle (in green, on right) Up, Down, or neither.</em></p><p>The training process for the policy works as follows. Starting from a random initialization, we let the agent interact with the environment for a while and collect episodes of interaction (e.g. each episode is one game of Pong). We thus obtain a complete recording of what happened: what sequence of states we encountered, what actions we took in each state, and what the reward was at each step. As an example, below is a diagram of three episodes that each took 10 time steps in a hypothetical environment. Each rectangle is a state, and rectangles are colored green if the reward was positive (e.g. we just got the ball past our opponent) and red if the reward was negative (e.g. we missed the ball):</p><p><img src="https://blog.openai.com/content/images/2017/03/second-graphic-1.png" alt="img"></p><p>This diagram suggests a recipe for how we can improve the policy; whatever we happened to do leading up to the green states was good, and whatever we happened to do in the states leading up to the red areas was bad. We can then use backpropagation to compute a small update on the network’s parameters that would make the green actions more likely in those states in the future, and the red actions less likely in those states in the future. We expect that the updated policy works a bit better as a result. We then iterate the process: collect another batch of episodes, do another update, etc.</p><p><strong>Exploration by injecting noise in the actions.</strong> The policies we usually use in RL are stochastic, in that they only compute probabilities of taking any action. This way, during the course of training, the agent may find itself in a particular state many times, and at different times it will take different actions due to the sampling. This provides the signal needed for learning; some of those actions will lead to good outcomes, and get encouraged, and some of them will not work out, and get discouraged. We therefore say that we introduce exploration into the learning process by injecting noise into the agent’s actions, which we do by sampling from the action distribution at each time step. This will be in contrast to ES, which we describe next.</p><h4 id="Evolution-Strategies"><a href="#Evolution-Strategies" class="headerlink" title="Evolution Strategies"></a>Evolution Strategies</h4><p><strong>On “Evolution”.</strong> Before we dive into the ES approach, it is important to note that despite the word “evolution”, ES has very little to do with biological evolution. Early versions of these techniques may have been inspired by biological evolution and the approach can, on an abstract level, be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations. However, the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black-box stochastic optimization techniques.</p><p><strong>Black-box optimization.</strong> In ES, we forget entirely that there is an agent, an environment, that there are neural networks involved, or that interactions take place over time, etc. The whole setup is that 1,000,000 numbers (which happen to describe the parameters of the policy network) go in, 1 number comes out (the total reward), and we want to find the best setting of the 1,000,000 numbers. Mathematically, we would say that we are optimizing a function <code>f(w)</code> with respect to the input vector <code>w</code>(the parameters / weights of the network), but we make no assumptions about the structure of <code>f</code>, except that we can evaluate it (hence “black box”).</p><p><strong>The ES algorithm.</strong> Intuitively, the optimization is a “guess and check” process, where we start with some random parameters and then repeatedly 1) tweak the guess a bit randomly, and 2) move our guess slightly towards whatever tweaks worked better. Concretely, at each step we take a parameter vector <code>w</code> and generate a population of, say, 100 slightly different parameter vectors <code>w1 ... w100</code> by jittering <code>w</code> with gaussian noise. We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while, and add up all the rewards in each case. The updated parameter vector then becomes the weighted sum of the 100 vectors, where each weight is proportional to the total reward (i.e. we want the more successful candidates to have a higher weight). Mathematically, you’ll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences, except we only do it along 100 random directions. Yet another way to see it is that we’re still doing RL (Policy Gradients, or <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> specifically), where the agent’s actions are to emit entire parameter vectors using a gaussian policy.</p><p><img src="https://blog.openai.com/content/images/2017/03/evo.png" alt="img"></p><p><em>Above: ES optimization process, in a setting with only two parameters and a reward function (red = high, blue = low). At each iteration we show the current parameter value (in white), a population of jittered samples (in black), and the estimated gradient (white arrow). We keep moving the parameters to the top of the arrow until we converge to a local optimum. You can reproduce this figure with this notebook.</em></p><p><strong>Code sample.</strong> To make the core algorithm concrete and to highlight its simplicity, here is a short example of optimizing a quadratic function using ES (or see this <a href="https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d" target="_blank" rel="external">longer version</a> with more comments):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># simple example: minimize a quadratic around some solution point</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">solution = np.array([<span class="number">0.5</span>, <span class="number">0.1</span>, <span class="number">-0.3</span>])</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(w)</span>:</span> <span class="keyword">return</span> -np.sum((w - solution)**<span class="number">2</span>)</div><div class="line"></div><div class="line">npop = <span class="number">50</span>      <span class="comment"># population size</span></div><div class="line">sigma = <span class="number">0.1</span>    <span class="comment"># noise standard deviation</span></div><div class="line">alpha = <span class="number">0.001</span>  <span class="comment"># learning rate</span></div><div class="line">w = np.random.randn(<span class="number">3</span>) <span class="comment"># initial guess</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</div><div class="line">  N = np.random.randn(npop, <span class="number">3</span>)</div><div class="line">  R = np.zeros(npop)</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(npop):</div><div class="line">    w_try = w + sigma*N[j]</div><div class="line">    R[j] = f(w_try)</div><div class="line">  A = (R - np.mean(R)) / np.std(R)</div><div class="line">  w = w + alpha/(npop*sigma) * np.dot(N.T, A)</div></pre></td></tr></table></figure><p><strong>Injecting noise in the parameters.</strong> Notice that the objective is identical to the one that RL optimizes: the expected reward. However, RL injects noise in the action space and uses backpropagation to compute the parameter updates, while ES injects noise directly in the parameter space. Another way to describe this is that RL is a “guess and check” on actions, while ES is a “guess and check” on parameters. Since we’re injecting noise in the parameters, it is possible to use deterministic policies (and we do, in our experiments). It is also possible to add noise in both actions and parameters to potentially combine the two approaches.</p><h4 id="Tradeoffs-between-ES-and-RL"><a href="#Tradeoffs-between-ES-and-RL" class="headerlink" title="Tradeoffs between ES and RL"></a>Tradeoffs between ES and RL</h4><p>ES enjoys multiple advantages over RL algorithms (some of them are a little technical):</p><ul><li><strong>No need for backpropagation</strong>. ES only requires the forward pass of the policy and does not require backpropagation (or value function estimation), which makes the code shorter and between 2-3 times faster in practice. On memory-constrained systems, it is also not necessary to keep a record of the episodes for a later update. There is also no need to worry about exploding gradients in RNNs. Lastly, we can explore a much larger function class of policies, including networks that are not differentiable (such as in binary networks), or ones that include complex modules (e.g. pathfinding, or various optimization layers).</li><li><strong>Highly parallelizable.</strong> ES only requires workers to communicate a few scalars between each other, while in RL it is necessary to synchronize entire parameter vectors (which can be millions of numbers). Intuitively, this is because we control the random seeds on each worker, so each worker can locally reconstruct the perturbations of the other workers. Thus, all that we need to communicate between workers is the reward of each perturbation. As a result, we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization.</li><li><strong>Higher robustness.</strong> Several hyperparameters that are difficult to set in RL implementations are side-stepped in ES. For example, RL is not “scale-free”, so one can achieve very different learning outcomes (including a complete failure) with different settings of the frame-skip hyperparameter in Atari. As we show in our work, ES works about equally well with any frame-skip.</li><li><strong>Structured exploration.</strong> Some RL algorithms (especially policy gradients) initialize with random policies, which often manifests as random jitter on spot for a long time. This effect is mitigated in Q-Learning due to epsilon-greedy policies, where the max operation can cause the agents to perform some consistent action for a while (e.g. holding down a left arrow). This is more likely to do something in a game than if the agent jitters on spot, as is the case with policy gradients. Similar to Q-learning, ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration.</li><li><strong>Credit assignment over long time scales.</strong> By studying both ES and RL gradient estimators mathematically we can see that ES is an attractive choice especially when the number of time steps in an episode is long, where actions have longlasting effects, or if no good value function estimates are available.</li></ul><p>Conversely, we also found some challenges to applying ES in practice. One core problem is that in order for ES to work, adding noise in parameters must lead to different outcomes to obtain some gradient signal. As we elaborate on in our paper, we found that the use of virtual batchnorm can help alleviate this problem, but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary. As an example of a related difficulty, we found that in Montezuma’s Revenge, one is very unlikely to get the key in the first level with a random network, while this is occasionally possible with random actions.</p><h4 id="ES-is-competitive-with-RL"><a href="#ES-is-competitive-with-RL" class="headerlink" title="ES is competitive with RL"></a>ES is competitive with RL</h4><p>We compared the performance of ES and RL on two standard RL benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo task (see examples below) contains a physically-simulated articulated figure, where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward. Below are some example agents trained on three MuJoCo control tasks, where the objective is to move forward:</p><p><img src="https://blog.openai.com/content/images/2017/03/out.gif" alt="img"></p><p>We usually compare the performance of algorithms by looking at their efficiency of learning from data; as a function of how many states we’ve seen, what is our average reward? Here are the example learning curves that we obtain, in comparison to RL (the <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="external">TRPO</a> algorithm in this case):</p><p><img src="https://blog.openai.com/content/images/2017/03/es_vs_trpo_full.png" alt="img"></p><p><strong>Data efficiency comparison</strong>. The comparisons above show that ES (orange) can reach a comparable performance to TRPO (blue), although it doesn’t quite match or surpass it in all cases. Moreover, by scanning horizontally we can see that ES is less efficient, but no worse than about a factor of 10 (note the x-axis is in log scale).</p><p><strong>Wall clock comparison</strong>. Instead of looking at the raw number of states seen, one can argue that the most important metric to look at is the wall clock time: how long (in number of seconds) does it take to solve a given problem? This quantity ultimately dictates the achievable speed of iteration for a researcher. Since ES requires negligible communication between workers, we were able to solve one of the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 machines in only 10 minutes. As a comparison, in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours. It is also possible that the performance of RL could also improve with more algorithmic and engineering effort, but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements.</p><p>Below are a few videos of 3D humanoid walkers trained with ES. As we can see, the results have quite a bit of variety, based on which local minimum the optimization ends up converging into.</p><p><img src="https://blog.openai.com/content/images/2017/03/out-1.gif" alt="img"></p><p>On Atari, ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day. Below are some result snippets on Pong, Seaquest and Beamrider. These videos show the preprocessed frames, which is exactly what the agent sees when it is playing:</p><p><img src="https://blog.openai.com/content/images/2017/03/atari.gif" alt="img"></p><p>In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels.</p><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p>ES is an algorithm from the neuroevolution literature, which has a long history in AI and a complete literature review is beyond the scope of this post. However, we encourage an interested reader to look at <a href="https://en.wikipedia.org/wiki/Neuroevolution" target="_blank" rel="external">Wikipedia</a>, <a href="http://www.scholarpedia.org/article/Neuroevolution" target="_blank" rel="external">Scholarpedia</a>, and Jürgen Schmidhuber’s <a href="https://arxiv.org/abs/1404.7828" target="_blank" rel="external">review article (Section 6.6)</a>. The work that most closely informed our approach is <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a> by Wierstra et al. 2014. Compared to this work and much of the work it has inspired, our focus is specifically on scaling these algorithms to large-scale, distributed settings, finding components that make the algorithms work better with deep neural networks (e.g. <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="external">virtual batch norm</a>), and evaluating them on modern RL benchmarks.</p><p>It is also worth noting that neuroevolution-related approaches have seen some recent resurgence in the machine learning literature, for example with <a href="https://arxiv.org/abs/1609.09106" target="_blank" rel="external">HyperNetworks</a>, <a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="external">“Large-Scale Evolution of Image Classifiers”</a> and <a href="https://arxiv.org/abs/1606.02580" target="_blank" rel="external">“Convolution by Evolution”</a>.</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Our work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent-environment benchmarks, while offering significant benefits related to code complexity and ease of scaling to large-scale distributed settings. We also expect that more exciting work can be done by revisiting other ideas from this line of work, such as indirect encoding methods, or evolving the network structure in addition to the parameters.</p><p><strong>Note on supervised learning</strong>. It is also important to note that supervised learning problems (e.g. image classification, speech recognition, or most other tasks in the industry), where one can compute the exact gradient of the loss function with backpropagation, are not directly impacted by these findings. For example, in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1,000 times slower than using backpropagation. It is only in RL settings, where one has to estimate the gradient of the expected reward by sampling, where ES becomes competitive.</p><p><strong>Code release</strong>. Finally, if you’d like to try running ES yourself, we encourage you to dive into the full details by reading <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">our paper</a> or looking at our code on this <a href="https://github.com/openai/evolution-strategies-starter" target="_blank" rel="external">Github repo</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Source blog is &lt;a href=&quot;https://blog.openai.com/evolution-strategies/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;We’ve &lt;a href=&quot;
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
</feed>
