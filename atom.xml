<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Abracadabra</title>
  <subtitle>Do it yourself</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-07-24T09:55:57.021Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Ewan Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Machine Learning Models Implemented By Tensorflow</title>
    <link href="http://yoursite.com/2017/07/24/Machine-Learning-Models-Implemented-By-Tensorflow/"/>
    <id>http://yoursite.com/2017/07/24/Machine-Learning-Models-Implemented-By-Tensorflow/</id>
    <published>2017-07-24T09:54:18.000Z</published>
    <updated>2017-07-24T09:55:57.021Z</updated>
    
    <content type="html"><![CDATA[<p>Project: <a href="https://github.com/ewanlee/finch/tree/master" target="_blank" rel="external">https://github.com/ewanlee/finch/tree/master</a></p><p>There are these algorithms in the tensorflow-models:</p><ul><li>Linear regression</li><li>Logistic regression</li><li>SVM</li><li>Autoencoder (MLP based and CNN based)</li><li>NMF</li><li>GAN</li><li>Conditional GAN</li><li>DCGAN</li><li>CNN</li><li>RNN (for classification and for regression)</li><li>Highway network (MLP based)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Project: &lt;a href=&quot;https://github.com/ewanlee/finch/tree/master&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/ewanlee/finch/tree/mast
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>sklearn-based feature engineering</title>
    <link href="http://yoursite.com/2017/07/20/sklearn-based-feature-engineering/"/>
    <id>http://yoursite.com/2017/07/20/sklearn-based-feature-engineering/</id>
    <published>2017-07-20T04:09:03.000Z</published>
    <updated>2017-07-20T04:23:44.320Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/feature-related/fe.ipynb" target="_blank" rel="external">feature engineering</a></li><li><a href="http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/feature-related/parallel.ipynb" target="_blank" rel="external">pipeline</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://nbviewer.jupyter.org/url/o7ie0tcjk.bkt.clouddn.com/feature-related/fe.ipynb&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;feature 
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>WGAN-GP [Repost]</title>
    <link href="http://yoursite.com/2017/07/18/WGAN-GP/"/>
    <id>http://yoursite.com/2017/07/18/WGAN-GP/</id>
    <published>2017-07-18T03:16:24.000Z</published>
    <updated>2017-07-18T13:49:35.700Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://ewanlee.github.io/2017/04/29/The-awesome-Wasserstein-GAN/" target="_blank" rel="external">WGAN</a>存在着训练困难、收敛速度慢等问题。<a href="https://link.zhihu.com/?target=https%3A//www.reddit.com/r/MachineLearning/comments/5zd4c0/d_survey_whats_the_most_stable_regiment_for/dexfhxu/%3Futm_content%3Dpermalink%26utm_medium%3Dfront%26utm_source%3Dreddit%26utm_name%3DMachineLearning" target="_blank" rel="external">WGAN的作者Martin Arjovsky不久后就在reddit上表示他也意识到了这个问题</a>，认为关键在于原设计中Lipschitz限制的施加方式不对：</p><blockquote><p>I am now pretty convinced that the problems that happen sometimes in WGANs is due to the specific way of how weight clipping works. It’s just a terrible way of enforcing a Lipschitz constraint, and better ways are out there. I feel like apologizing for being too lazy and sticking to what could be done in one line of torch code.</p><p>A simple alternative (less than 5 lines of code) has been found by Montréal students. It works on quite a few settings (inc 100 layer resnets) with default hyperparameters. Arxiv coming this or next week, stay tuned.</p></blockquote><p>并在新论文中提出了相应的改进方案：</p><ul><li>论文：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.00028" target="_blank" rel="external">[1704.00028] Improved Training of Wasserstein GANs</a></li><li>Tensorflow实现：<ul><li><a href="https://github.com/brianherman/improved_wgan_training" target="_blank" rel="external">brianherman/improved_wgan_training</a> (Python 3)</li><li><a href="https://github.com/igul222/improved_wgan_training" target="_blank" rel="external">igul222/improved_wgan_training</a> (Python 2)</li></ul></li></ul><p><strong>首先回顾一下WGAN的关键部分——Lipschitz限制是什么。</strong>WGAN中，判别器D和生成器G的loss函数分别是<br>$$<br>\begin{align}<br>L(D) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] \\<br>L(G) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)]<br>\end{align}<br>$$<br>公式1表示判别器希望尽可能拉高真样本的分数，拉低假样本的分数，公式2表示生成器希望尽可能拉高假样本的分数。</p><p>Lipschitz限制则体现为，在整个样本空间$\mathcal{X}$上，要求判别器函数$D(x)$梯度的$L_p$ norm大于一个有限的常数K：<br>$$<br>| \nabla_x D(x) |_p \leq K, \forall x \in \mathcal{X}<br>$$<br>直观上解释，就是当输入的样本稍微变化后，判别器给出的分数不能发生太过剧烈的变化。在原来的论文中，这个限制具体是通过weight clipping的方式实现的：每当更新完一次判别器的参数之后，就检查判别器的所有参数的绝对值有没有超过一个阈值，比如0.01，有的话就把这些参数clip回 [-0.01, 0.01] 范围内。通过在训练过程中保证判别器的所有参数有界，就保证了判别器不能对两个略微不同的样本给出天差地别的分数值，从而间接实现了Lipschitz限制。</p><p><strong>然而weight clipping的实现方式存在两个严重问题：</strong></p><p>第一，如公式1所言，判别器loss希望尽可能拉大真假样本的分数差，然而weight clipping独立地限制每一个网络参数的取值范围，在这种情况下我们可以想象，最优的策略就是尽可能让所有参数走极端，要么取最大值（如0.01）要么取最小值（如-0.01）！为了验证这一点，作者统计了经过充分训练的判别器中所有网络参数的数值分布，发现真的集中在最大和最小两个极端上：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-clip.png" alt="weight-clipping"></p><p>这样带来的结果就是，判别器会非常倾向于学习一个简单的映射函数（想想看，几乎所有参数都是正负0.01，都已经可以直接视为一个<a href="https://link.zhihu.com/?target=http%3A//synchuman.baijia.baidu.com/article/385441" target="_blank" rel="external">二值神经网络**</a>了，太简单了）。而作为一个深层神经网络来说，这实在是对自身强大拟合能力的巨大浪费！判别器没能充分利用自身的模型能力，经过它回传给生成器的梯度也会跟着变差。</p><p>在正式介绍gradient penalty之前，我们可以先看看在它的指导下，同样充分训练判别器之后，参数的数值分布就合理得多了，判别器也能够充分利用自身模型的拟合能力：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-weight-gp.png" alt="weight-gp"></p><p>第二个问题，weight clipping会导致很容易一不小心就梯度消失或者梯度爆炸。原因是判别器是一个多层网络，如果我们把clipping threshold设得稍微小了一点，每经过一层网络，梯度就变小一点点，多层之后就会指数衰减；反之，如果设得稍微大了一点，每经过一层网络，梯度变大一点点，多层之后就会指数爆炸。只有设得不大不小，才能让生成器获得恰到好处的回传梯度，然而在实际应用中这个平衡区域可能很狭窄，就会给调参工作带来麻烦。相比之下，gradient penalty就可以让梯度在后向传播的过程中保持平稳。论文通过下图体现了这一点，其中横轴代表判别器从低到高第几层，纵轴代表梯度回传到这一层之后的尺度大小（注意纵轴是对数刻度），c是clipping threshold：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-gradient-norm.png" alt="gradient-norm"></p><p><strong>说了这么多，gradient penalty到底是什么？</strong></p><p>前面提到，Lipschitz限制是要求判别器的梯度不超过K，那我们何不直接设置一个额外的loss项来体现这一点呢？比如说：<br>$$<br>ReLU[| \nabla_x D(x) |_p - K]<br>$$<br>不过，既然判别器希望尽可能拉大真假样本的分数差距，那自然是希望梯度越大越好，变化幅度越大越好，所以判别器在充分训练之后，其梯度norm其实就会是在K附近。知道了这一点，我们可以把上面的loss改成要求梯度norm离K越近越好，效果是类似的：<br>$$<br>[| \nabla_x D(x) |_p - K]^2<br>$$<br>究竟是公式4好还是公式5好，我看不出来，可能需要实验验证，反正论文作者选的是公式5。接着我们简单地把K定为1，再跟WGAN原来的判别器loss加权合并，就得到新的判别器loss：<br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \mathcal{X}} [| \nabla_x D(x) |_p - 1]^2<br>$$<br>这就是所谓的gradient penalty了吗？还没完。公式6有两个问题，首先是loss函数中存在梯度项，那么优化这个loss岂不是要算梯度的梯度？一些读者可能对此存在疑惑，不过这属于实现上的问题，放到后面说。</p><p>其次，3个loss项都是期望的形式，落到实现上肯定得变成采样的形式。前面两个期望的采样我们都熟悉，第一个期望是从真样本集里面采，第二个期望是从生成器的噪声输入分布采样后，再由生成器映射到样本空间。可是第三个分布要求我们在整个样本空间$\mathcal{X}$上采样，这完全不科学！由于所谓的维度灾难问题，如果要通过采样的方式在图片或自然语言这样的高维样本空间中估计期望值，所需样本量是指数级的，实际上没法做到。</p><p>所以，论文作者就非常机智地提出，我们其实没必要在整个样本空间上施加Lipschitz限制，只要重点抓住生成样本集中区域、真实样本集中区域以及夹在它们中间的区域就行了。具体来说，我们先随机采一对真假样本，还有一个0-1的随机数：<br>$$<br>x_r \sim P_r, x_g \sim P_g, \epsilon \sim Uniform[0, 1]<br>$$<br>然后在$x_r$和$x_g$的连线上随机插值采样：<br>$$<br>\hat{x} = \epsilon x_r + (1 - \epsilon) x_g<br>$$<br>把按照上述流程采样得到的$\hat{x}$所满足的分布记为$P_{\hat{x}}$, 就得到最终版本的判别器loss：<br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \hat{x}} [| \nabla_x D(x) |_p - 1]^2<br>$$<br><strong>这就是新论文所采用的gradient penalty方法，相应的新WGAN模型简称为WGAN-GP。</strong>我们可以做一个对比：</p><ul><li>weight clipping是对样本空间全局生效，但因为是间接限制判别器的梯度norm，会导致一不小心就梯度消失或者梯度爆炸；</li><li>gradient penalty只对真假样本集中区域、及其中间的过渡地带生效，但因为是直接把判别器的梯度norm限制在1附近，所以梯度可控性非常强，容易调整到合适的尺度大小。</li></ul><p>论文还讲了一些使用gradient penalty时需要注意的配套事项，这里只提一点：由于我们是对每个样本独立地施加梯度惩罚，所以判别器的模型架构中不能使用Batch Normalization，因为它会引入同个batch中不同样本的相互依赖关系。如果需要的话，可以选择其他normalization方法，如Layer Normalization、Weight Normalization和Instance Normalization，这些方法就不会引入样本之间的依赖。论文推荐的是Layer Normalization。</p><p>实验表明，gradient penalty能够显著提高训练速度，解决了原始WGAN收敛缓慢的问题：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-exper.png" alt="exper"></p><p>虽然还是比不过DCGAN，但是因为WGAN不存在平衡判别器与生成器的问题，所以会比DCGAN更稳定，还是很有优势的。不过，作者凭什么能这么说？因为下面的实验体现出，在各种不同的网络架构下，其他GAN变种能不能训练好，可以说是一件相当看人品的事情，但是WGAN-GP全都能够训练好，尤其是最下面一行所对应的101层残差神经网络：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-image-gen.png" alt="image-gen"></p><p><strong>剩下的实验结果中，比较厉害的是第一次成功做到了“纯粹的”的文本GAN训练！</strong>我们知道在图像上训练GAN是不需要额外的有监督信息的，但是之前就没有人能够像训练图像GAN一样训练好一个文本GAN，要么依赖于预训练一个语言模型，要么就是利用已有的有监督ground truth提供指导信息。而现在WGAN-GP终于在无需任何有监督信息的情况下，生成出下图所示的英文字符序列：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/wgan-gp/wgan-gp-text-gen.png" alt="text-gen"></p><p>它是怎么做到的呢？我认为关键之处是对样本形式的更改。<strong>以前我们一般会把文本这样的离散序列样本表示为sequence of index，但是它把文本表示成sequence of probability vector。</strong>对于生成样本来说，我们可以取网络softmax层输出的词典概率分布向量，作为序列中每一个位置的内容；而对于真实样本来说，每个probability vector实际上就蜕化为我们熟悉的onehot vector。</p><p>但是如果按照传统GAN的思路来分析，这不是作死吗？一边是hard onehot vector，另一边是soft probability vector，判别器一下子就能够区分它们，生成器还怎么学习？没关系，对于WGAN来说，真假样本好不好区分并不是问题，WGAN只是拉近两个分布之间的Wasserstein距离，就算是一边是hard onehot另一边是soft probability也可以拉近，在训练过程中，概率向量中的有些项可能会慢慢变成0.8、0.9到接近1，整个向量也会接近onehot，最后我们要真正输出sequence of index形式的样本时，只需要对这些概率向量取argmax得到最大概率的index就行了。</p><p>新的样本表示形式+WGAN的分布拉近能力是一个“黄金组合”，但除此之外，还有其他因素帮助论文作者跑出上图的效果，包括：</p><ul><li>文本粒度为英文字符，而非英文单词，所以字典大小才二三十，大大减小了搜索空间</li><li>文本长度也才32</li><li>生成器用的不是常见的LSTM架构，而是多层反卷积网络，输入一个高斯噪声向量，直接一次性转换出所有32个字符</li></ul><p><strong>最后说回gradient penalty的实现问题。</strong>loss中本身包含梯度，优化loss就需要求梯度的梯度，这个功能并不是现在所有深度学习框架的标配功能，不过好在Tensorflow就有提供这个接口—<code>tf.gradients</code>。开头链接的GitHub源码中就是这么写的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># interpolates就是随机插值采样得到的图像，gradients就是loss中的梯度惩罚项</div><div class="line">gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]</div></pre></td></tr></table></figure><p>完整的loss是这样实现的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">gen_cost = -tf.reduce_mean(disc_fake)</div><div class="line">disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)</div><div class="line"></div><div class="line">alpha = tf.random_uniform(</div><div class="line">	shape=[BATCH_SIZE,<span class="number">1</span>], </div><div class="line">	minval=<span class="number">0.</span>,</div><div class="line">	maxval=<span class="number">1.</span></div><div class="line">)</div><div class="line">differences = fake_data - real_data</div><div class="line">interpolates = real_data + (alpha*differences)</div><div class="line">gradients = tf.gradients(Discriminator(interpolates), [interpolates])[<span class="number">0</span>]</div><div class="line">slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[<span class="number">1</span>]))</div><div class="line">gradient_penalty = tf.reduce_mean((slopes<span class="number">-1.</span>)**<span class="number">2</span>)</div><div class="line">disc_cost += LAMBDA*gradient_penalty</div><div class="line"></div><div class="line">gen_train_op = tf.train.AdamOptimizer(</div><div class="line">	learning_rate=<span class="number">1e-4</span>, </div><div class="line">	beta1=<span class="number">0.5</span>,</div><div class="line">	beta2=<span class="number">0.9</span></div><div class="line">).minimize(gen_cost, var_list=gen_params)</div><div class="line">disc_train_op = tf.train.AdamOptimizer(</div><div class="line">	learning_rate=<span class="number">1e-4</span>, </div><div class="line">	beta1=<span class="number">0.5</span>, </div><div class="line">	beta2=<span class="number">0.9</span></div><div class="line">).minimize(disc_cost, var_list=disc_params)</div></pre></td></tr></table></figure><p>对于我这样的PyTorch党就非常不幸了，高阶梯度的功能还在开发，感兴趣的PyTorch党可以订阅这个GitHub的pull request：<a href="https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/pull/1016" target="_blank" rel="external">Autograd refactor</a>，如果它被merged了话就可以在最新版中使用高阶梯度的功能实现gradient penalty了。</p><p>但是除了等待我们就没有别的办法了吗？<strong>其实可能是有的，我想到了一种近似方法来实现gradient penalty，只需要把微分换成差分：</strong><br>$$<br>L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x_1 \sim \hat{x}, x_2 \sim \hat{x}} [ \frac{|D(x_1) - D(x_2)|}{| x_1 - x_2 |_p} - 1]^2<br>$$<br>也就是说，我们仍然是在分布 $P_{\hat{x}}$ 上随机采样，但是一次采两个，然后要求它们的连线斜率要接近1，这样理论上也可以起到跟公式9一样的效果，我自己在MNIST+MLP上简单验证过有作用，PyTorch党甚至Tensorflow党都可以尝试用一下。</p><hr><p><strong>作者：郑华滨链接：<a href="https://www.zhihu.com/question/52602529/answer/158727900" target="_blank" rel="external">https://www.zhihu.com/question/52602529/answer/158727900</a></strong></p><p><strong>来源：知乎</strong></p><p><strong>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://ewanlee.github.io/2017/04/29/The-awesome-Wasserstein-GAN/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;WGAN&lt;/a&gt;存在着训练困难、收敛速度慢等问题。&lt;a hr
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="WGAN" scheme="http://yoursite.com/tags/WGAN/"/>
    
  </entry>
  
  <entry>
    <title>Policy Gradient Methods</title>
    <link href="http://yoursite.com/2017/07/10/Policy-Gradient-Methods/"/>
    <id>http://yoursite.com/2017/07/10/Policy-Gradient-Methods/</id>
    <published>2017-07-10T09:23:16.000Z</published>
    <updated>2017-07-12T04:17:20.106Z</updated>
    
    <content type="html"><![CDATA[<p>Until now almost all the methods have learned the values of actions and then selected actions based on their estimated action values; their policies would not even exist without the action-value estimates. In this post we consider methods that instead learn a <em>parameterized policy</em> that can select actions without consulting a value function. A value function may still be used to <em>learn</em> the policy parameter, but is not required for action selection. We use the notation $\boldsymbol{\theta} \in \mathbb{R}^d$ for the policy’s parameter vector. Thus we write $\pi(a|s, \boldsymbol{\theta}) = \text{Pr}(A_t=a | S_t=s, \boldsymbol{\theta}_t=\boldsymbol{\theta})$ for the probability that action $a$ is taken at time $t$ given that the agent is in state $s$ at time $t$ with parameter $\boldsymbol{\theta}$. If a method uses a learned value function as well, then the value function’s weight vector is denoted $\mathbf{w} \in \mathbb{R}^m$, as in $\hat{v}(s, \mathbf{w})$.</p><p>In this chapter we consider methods for learning the policy parameter based on the gradient of some performance measure $J(\boldsymbol{\theta})$ with respect to the policy parameter. These methods seek to maximize performance, so their updates approximate gradient ascent in $J$ :<br>$$<br>\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \alpha \widehat{\nabla J(\boldsymbol{\theta}_t)}.<br>$$<br>All methods that follow this general schema we call <strong>policy gradient methods</strong>, whether or not they also learn an approximate value function. Methods that learn approximations to both policy and value functions are often called actor–critic methods, where ‘actor’ is a reference to the learned policy, and ‘critic’ refers to the learned value function, usually a state-value function.</p><h3 id="Policy-Approximation"><a href="#Policy-Approximation" class="headerlink" title="Policy Approximation"></a>Policy Approximation</h3><p>The most preferred actions in each state are given the highest probability of being selected, for example, according to an exponential softmax distribution:<br>$$<br>\pi(a|s, \boldsymbol{\theta}) = \frac{\exp(h(s, a, \boldsymbol{\theta}))}{\sum_b \exp(h(s, b, \boldsymbol{\theta}))}.<br>$$<br>For example, they might be computed by a deep neural network, where $\boldsymbol{\theta}$ is the vector of all the connection weights of the network. Or the preferences could simply be linear in features,<br>$$<br>h(s, a, \boldsymbol{\theta}) = \boldsymbol{\theta}^{\top} \mathbf{x}(s, a).<br>$$</p><h3 id="The-Policy-Gradient-Theorem"><a href="#The-Policy-Gradient-Theorem" class="headerlink" title="The Policy Gradient Theorem"></a>The Policy Gradient Theorem</h3><p>We deﬁne the performance measure as the value of the start state of the episode. We can simplify the notation without losing any meaningful generality by assuming that every episode starts in some particular (non-random) state s0. Then, in the episodic case we deﬁne performance as<br>$$<br>J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}}(s_0),<br>$$<br>where $ v_{\pi_{\boldsymbol{\theta}}}$ is the true value function for $\pi_{\boldsymbol{\theta}}$, the policy determined by $\boldsymbol{\theta}$.</p><p>The policy gradient theorem is that<br>$$<br>\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}),<br>$$<br>where $\mu_{\pi}(s)$ we mentioned in <a href="https://ewanlee.github.io/2017/07/05/On-policy-Prediction-with-Approximation/" target="_blank" rel="external">earlier</a>.</p><h3 id="REINFORCE-Monte-Carlo-Policy-Gradient"><a href="#REINFORCE-Monte-Carlo-Policy-Gradient" class="headerlink" title="REINFORCE: Monte Carlo Policy Gradient"></a>REINFORCE: Monte Carlo Policy Gradient</h3><p>$$<br>\begin{align}<br>\nabla J(\boldsymbol{\theta}) &amp;= \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}) \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a q_{\pi}(S_t, a) \nabla_{\boldsymbol{\theta}} \pi(a | S_t, \boldsymbol{\theta}) \Bigg] \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a \pi(a|S_t, \boldsymbol{\theta}) q_{\pi}(S_t, a) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})} \Bigg] \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t q_{\pi}(S_t, A_t) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \text{(replacing a by the sample } A_t \sim \pi \;) \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \;\;\; \;\;\; \;\;\; \;\; (\text{because } \mathbb{E}_{\pi}[G_t|S_t,A_t]=q_{\pi}(S_t,A_t)).<br>\end{align}<br>$$</p><p>So we get<br>$$<br>\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.<br>$$<br>This is shown explicitly in the boxed pseudocode below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/pgm/reinforce.png" alt="reinforce"></p><p>Notice that $\nabla \log x = \frac{\nabla x}{x}$.</p><h3 id="REINFORCE-with-Baseline"><a href="#REINFORCE-with-Baseline" class="headerlink" title="REINFORCE with Baseline"></a>REINFORCE with Baseline</h3><p>The policy gradient theorem can be generalized to include a comparison of the action value to an arbitrary <strong>baseline</strong> $b(s)$:</p><p>$$<br>\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a \big(q_{\pi}(s, a) - b(s)\big) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}).<br>$$<br>The baseline can be any function, even a random variable, as long as it does not vary with $a$; the equation remains true, because the subtracted quantity is zero:<br>$$<br>\sum_a b(s) \nabla_{\boldsymbol{\theta}} \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} \sum_a \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} 1 = 0 \;\;\;\; \forall s \in \mathcal{S}.<br>$$<br>The update rule that we end up with is a new version of REINFORCE that includes a general baseline:<br>$$<br>\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \big(G_t-b(S_t) \big) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.<br>$$<br>One natural choice for the baseline is an estimate of the state value, $\hat{v}(S_t, \mathbf{w})$, where $\mathbf{w} \in \mathbb{R}^m$ is a weight vector learned by one of the methods presented in previous posts. A complete pseudocode algorithm for REINFROCE with baseline is given in the box (use Monte Carlo method for learning the policy parameter and state-value weights).</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/pgm/reinforce_baseline.png" alt="reinforce_baseline"></p><h3 id="Actor-Critic-Methods"><a href="#Actor-Critic-Methods" class="headerlink" title="Actor-Critic Methods"></a>Actor-Critic Methods</h3><p>Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor-critic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating a state from the estimated values of subsequent states), but only as a baseline for the state being updated. In<br>order to gain these advantages in the case of policy gradient methods we use actor-critic methods with a true bootstrapping critic.</p><p>One-step actor-critic methods replace the full return of REINFORCE with the one-step return (and use a learned state-value function as the baseline) as follow:<br>$$<br>\begin{align}<br>\boldsymbol{\theta}_{t+1} &amp;\doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( G_{t:t+1} - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\<br>&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\<br>&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \delta_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})}.<br>\end{align}<br>$$<br>The natural state-value-function learning method to pair with this is semi-gradient TD(0). Pseudocode for the complete algorithm is given in the box below. Note that it is now a fully online, incremental algorithm, with states, actions, and rewards processed as they occur and then never revisited.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/pgm/one-step-ac.png" alt="one-step-ac"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Until now almost all the methods have learned the values of actions and then selected actions based on their estimated action values; the
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout</title>
    <link href="http://yoursite.com/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/"/>
    <id>http://yoursite.com/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/</id>
    <published>2017-07-09T03:34:18.000Z</published>
    <updated>2017-07-09T04:38:52.104Z</updated>
    
    <content type="html"><![CDATA[<p>In previous <a href="https://ewanlee.github.io/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" target="_blank" rel="external">blog</a>, we use the Keras to play the FlappyBird. Similarity, we will use another deep learning toolkit Tensorflow to develop the DQN and Double DQN and to play the another game Breakout (Atari 3600).</p><p>Here, we will use the OpenAI gym toolkit to construct out environment. Detail implementation is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env = gym.envs.make(<span class="string">"Breakout-v0"</span>)</div></pre></td></tr></table></figure><p>And then we look some demos:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"Action space size: &#123;&#125;"</span>.format(env.action_space.n))</div><div class="line"><span class="comment"># print(env.get_action_meanings())</span></div><div class="line"></div><div class="line">observation = env.reset()</div><div class="line">print(<span class="string">"Observation space shape: &#123;&#125;"</span>.format(observation.shape))</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">[env.step(<span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>)]</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">env.render(close=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/games/breakout-env.png" alt="breakout-env"></p><p>For deep learning purpose, we need to crop the image to a square image:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Check out what a cropped image looks like</span></div><div class="line">plt.imshow(observation[<span class="number">34</span>:<span class="number">-16</span>,:,:])</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/games/croped-breakout-image.png" alt="croped-breakout-image"></p><p>Not bad !</p><p>Ok, now let us to use the Tensorflow to develop the DQN algorithm first.</p><p>First of all, we need to reference some packages and initialize the environment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"></div><div class="line"><span class="keyword">import</span> gym</div><div class="line"><span class="keyword">from</span> gym.wrappers <span class="keyword">import</span> Monitor</div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">if</span> <span class="string">"../"</span> <span class="keyword">not</span> <span class="keyword">in</span> sys.path:</div><div class="line">  sys.path.append(<span class="string">"../"</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> lib <span class="keyword">import</span> plotting</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque, namedtuple</div><div class="line"></div><div class="line">env = gym.envs.make(<span class="string">"Breakout-v0"</span>)</div><div class="line"><span class="comment"># Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions</span></div><div class="line">VALID_ACTIONS = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div></pre></td></tr></table></figure><p>As mentioned above, we need to crop the image and preprocess the input before feed the raw image into the algorithm. So we define a <strong>StateProcessor</strong> class to do this.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StateProcessor</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Processes a raw Atari images. Resizes it and converts it to grayscale.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># Build the Tensorflow graph</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"state_processor"</span>):</div><div class="line">            self.input_state = tf.placeholder(shape=[<span class="number">210</span>, <span class="number">160</span>, <span class="number">3</span>], dtype=tf.uint8)</div><div class="line">            self.output = tf.image.rgb_to_grayscale(self.input_state)</div><div class="line">            self.output = tf.image.crop_to_bounding_box(self.output, <span class="number">34</span>, <span class="number">0</span>, <span class="number">160</span>, <span class="number">160</span>)</div><div class="line">            self.output = tf.image.resize_images(</div><div class="line">                self.output, [<span class="number">84</span>, <span class="number">84</span>], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</div><div class="line">            self.output = tf.squeeze(self.output)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, sess, state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Args:</div><div class="line">            sess: A Tensorflow session object</div><div class="line">            state: A [210, 160, 3] Atari RGB State</div><div class="line"></div><div class="line">        Returns:</div><div class="line">            A processed [84, 84, 1] state representing grayscale values.</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> sess.run(self.output, &#123; self.input_state: state &#125;)</div></pre></td></tr></table></figure><p>We first convert the image to gray image and then resize it to 84 by 84 (the size DQN paper used). Then, we construct the neural network to estimate the value function. The structure of the network as the same as the DQN paper.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Estimator</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""Q-Value Estimator neural network.</span></div><div class="line"></div><div class="line">    This network is used for both the Q-Network and the Target Network.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, scope=<span class="string">"estimator"</span>, summaries_dir=None)</span>:</span></div><div class="line">        self.scope = scope</div><div class="line">        <span class="comment"># Writes Tensorboard summaries to disk</span></div><div class="line">        self.summary_writer = <span class="keyword">None</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</div><div class="line">            <span class="comment"># Build the graph</span></div><div class="line">            self._build_model()</div><div class="line">            <span class="keyword">if</span> summaries_dir:</div><div class="line">                summary_dir = os.path.join(summaries_dir, <span class="string">"summaries_&#123;&#125;"</span>.format(scope))</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(summary_dir):</div><div class="line">                    os.makedirs(summary_dir)</div><div class="line">                self.summary_writer = tf.summary.FileWriter(summary_dir)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_model</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Builds the Tensorflow graph.</div><div class="line">        """</div><div class="line"></div><div class="line">        <span class="comment"># Placeholders for our input</span></div><div class="line">        <span class="comment"># Our input are 4 RGB frames of shape 160, 160 each</span></div><div class="line">        self.X_pl = tf.placeholder(shape=[<span class="keyword">None</span>, <span class="number">84</span>, <span class="number">84</span>, <span class="number">4</span>], dtype=tf.uint8, name=<span class="string">"X"</span>)</div><div class="line">        <span class="comment"># The TD target value</span></div><div class="line">        self.y_pl = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.float32, name=<span class="string">"y"</span>)</div><div class="line">        <span class="comment"># Integer id of which action was selected</span></div><div class="line">        self.actions_pl = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.int32, name=<span class="string">"actions"</span>)</div><div class="line"></div><div class="line">        X = tf.to_float(self.X_pl) / <span class="number">255.0</span></div><div class="line">        batch_size = tf.shape(self.X_pl)[<span class="number">0</span>]</div><div class="line"></div><div class="line">        <span class="comment"># Three convolutional layers</span></div><div class="line">        conv1 = tf.contrib.layers.conv2d(</div><div class="line">            X, <span class="number">32</span>, <span class="number">8</span>, <span class="number">4</span>, activation_fn=tf.nn.relu)</div><div class="line">        conv2 = tf.contrib.layers.conv2d(</div><div class="line">            conv1, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>, activation_fn=tf.nn.relu)</div><div class="line">        conv3 = tf.contrib.layers.conv2d(</div><div class="line">            conv2, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, activation_fn=tf.nn.relu)</div><div class="line"></div><div class="line">        <span class="comment"># Fully connected layers</span></div><div class="line">        flattened = tf.contrib.layers.flatten(conv3)</div><div class="line">        fc1 = tf.contrib.layers.fully_connected(flattened, <span class="number">512</span>)</div><div class="line">        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))</div><div class="line"></div><div class="line">        <span class="comment"># Get the predictions for the chosen actions only</span></div><div class="line">        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[<span class="number">1</span>] + self.actions_pl</div><div class="line">        self.action_predictions = tf.gather(tf.reshape(self.predictions, [<span class="number">-1</span>]), gather_indices)</div><div class="line"></div><div class="line">        <span class="comment"># Calcualte the loss</span></div><div class="line">        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)</div><div class="line">        self.loss = tf.reduce_mean(self.losses)</div><div class="line"></div><div class="line">        <span class="comment"># Optimizer Parameters from original paper</span></div><div class="line">        self.optimizer = tf.train.RMSPropOptimizer(<span class="number">0.00025</span>, <span class="number">0.99</span>, <span class="number">0.0</span>, <span class="number">1e-6</span>)</div><div class="line">        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())</div><div class="line"></div><div class="line">        <span class="comment"># Summaries for Tensorboard</span></div><div class="line">        self.summaries = tf.summary.merge([</div><div class="line">            tf.summary.scalar(<span class="string">"loss"</span>, self.loss),</div><div class="line">            tf.summary.histogram(<span class="string">"loss_hist"</span>, self.losses),</div><div class="line">            tf.summary.histogram(<span class="string">"q_values_hist"</span>, self.predictions),</div><div class="line">            tf.summary.scalar(<span class="string">"max_q_value"</span>, tf.reduce_max(self.predictions))</div><div class="line">        ])</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sess, s)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Predicts action values.</div><div class="line"></div><div class="line">        Args:</div><div class="line">          sess: Tensorflow session</div><div class="line">          s: State input of shape [batch_size, 4, 160, 160, 3]</div><div class="line"></div><div class="line">        Returns:</div><div class="line">          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated </div><div class="line">          action values.</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> sess.run(self.predictions, &#123; self.X_pl: s &#125;)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, sess, s, a, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Updates the estimator towards the given targets.</div><div class="line"></div><div class="line">        Args:</div><div class="line">          sess: Tensorflow session object</div><div class="line">          s: State input of shape [batch_size, 4, 160, 160, 3]</div><div class="line">          a: Chosen actions of shape [batch_size]</div><div class="line">          y: Targets of shape [batch_size]</div><div class="line"></div><div class="line">        Returns:</div><div class="line">          The calculated loss on the batch.</div><div class="line">        """</div><div class="line">        feed_dict = &#123; self.X_pl: s, self.y_pl: y, self.actions_pl: a &#125;</div><div class="line">        summaries, global_step, _, loss = sess.run(</div><div class="line">            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],</div><div class="line">            feed_dict)</div><div class="line">        <span class="keyword">if</span> self.summary_writer:</div><div class="line">            self.summary_writer.add_summary(summaries, global_step)</div><div class="line">        <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>As mentioned in DQN paper, there are two network that share the same parameters in DQN algorithm. We need to copy the parameters to the target network on each $t$ steps.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">copy_model_parameters</span><span class="params">(sess, estimator1, estimator2)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Copies the model parameters of one estimator to another.</div><div class="line"></div><div class="line">    Args:</div><div class="line">      sess: Tensorflow session instance</div><div class="line">      estimator1: Estimator to copy the paramters from</div><div class="line">      estimator2: Estimator to copy the parameters to</div><div class="line">    """</div><div class="line">    e1_params = [t <span class="keyword">for</span> t <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> t.name.startswith(estimator1.scope)]</div><div class="line">    e1_params = sorted(e1_params, key=<span class="keyword">lambda</span> v: v.name)</div><div class="line">    e2_params = [t <span class="keyword">for</span> t <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> t.name.startswith(estimator2.scope)]</div><div class="line">    e2_params = sorted(e2_params, key=<span class="keyword">lambda</span> v: v.name)</div><div class="line"></div><div class="line">    update_ops = []</div><div class="line">    <span class="keyword">for</span> e1_v, e2_v <span class="keyword">in</span> zip(e1_params, e2_params):</div><div class="line">        op = e2_v.assign(e1_v)</div><div class="line">        update_ops.append(op)</div><div class="line"></div><div class="line">    sess.run(update_ops)</div></pre></td></tr></table></figure><p>We also need a policy to take an action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(estimator, nA)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.</div><div class="line"></div><div class="line">    Args:</div><div class="line">        estimator: An estimator that returns q values for a given state</div><div class="line">        nA: Number of actions in the environment.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        A function that takes the (sess, observation, epsilon) as an argument and returns</div><div class="line">        the probabilities for each action in the form of a numpy array of length nA.</div><div class="line"></div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(sess, observation, epsilon)</span>:</span></div><div class="line">        A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">        q_values = estimator.predict(sess, np.expand_dims(observation, <span class="number">0</span>))[<span class="number">0</span>]</div><div class="line">        best_action = np.argmax(q_values)</div><div class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">        <span class="keyword">return</span> A</div><div class="line">    <span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Now let us to develop the DQN algorithm (we skip the details here because we explained it earlier).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">deep_q_learning</span><span class="params">(sess,</span></span></div><div class="line">                    env,</div><div class="line">                    q_estimator,</div><div class="line">                    target_estimator,</div><div class="line">                    state_processor,</div><div class="line">                    num_episodes,</div><div class="line">                    experiment_dir,</div><div class="line">                    replay_memory_size=<span class="number">500000</span>,</div><div class="line">                    replay_memory_init_size=<span class="number">50000</span>,</div><div class="line">                    update_target_estimator_every=<span class="number">10000</span>,</div><div class="line">                    discount_factor=<span class="number">0.99</span>,</div><div class="line">                    epsilon_start=<span class="number">1.0</span>,</div><div class="line">                    epsilon_end=<span class="number">0.1</span>,</div><div class="line">                    epsilon_decay_steps=<span class="number">500000</span>,</div><div class="line">                    batch_size=<span class="number">32</span>,</div><div class="line">                    record_video_every=<span class="number">50</span>):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm for fff-policy TD control using Function Approximation.</div><div class="line">    Finds the optimal greedy policy while following an epsilon-greedy policy.</div><div class="line"></div><div class="line">    Args:</div><div class="line">        sess: Tensorflow Session object</div><div class="line">        env: OpenAI environment</div><div class="line">        q_estimator: Estimator object used for the q values</div><div class="line">        target_estimator: Estimator object used for the targets</div><div class="line">        state_processor: A StateProcessor object</div><div class="line">        num_episodes: Number of episodes to run for</div><div class="line">        experiment_dir: Directory to save Tensorflow summaries in</div><div class="line">        replay_memory_size: Size of the replay memory</div><div class="line">        replay_memory_init_size: Number of random experiences to sampel when initializing </div><div class="line">          the reply memory.</div><div class="line">        update_target_estimator_every: Copy parameters from the Q estimator to the </div><div class="line">          target estimator every N steps</div><div class="line">        discount_factor: Lambda time discount factor</div><div class="line">        epsilon_start: Chance to sample a random action when taking an action.</div><div class="line">          Epsilon is decayed over time and this is the start value</div><div class="line">        epsilon_end: The final minimum value of epsilon after decaying is done</div><div class="line">        epsilon_decay_steps: Number of steps to decay epsilon over</div><div class="line">        batch_size: Size of batches to sample from the replay memory</div><div class="line">        record_video_every: Record a video every N episodes</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line"></div><div class="line">    Transition = namedtuple(<span class="string">"Transition"</span>, [<span class="string">"state"</span>, <span class="string">"action"</span>, <span class="string">"reward"</span>, <span class="string">"next_state"</span>, <span class="string">"done"</span>])</div><div class="line"></div><div class="line">    <span class="comment"># The replay memory</span></div><div class="line">    replay_memory = []</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))</div><div class="line"></div><div class="line">    <span class="comment"># Create directories for checkpoints and summaries</span></div><div class="line">    checkpoint_dir = os.path.join(experiment_dir, <span class="string">"checkpoints"</span>)</div><div class="line">    checkpoint_path = os.path.join(checkpoint_dir, <span class="string">"model"</span>)</div><div class="line">    monitor_path = os.path.join(experiment_dir, <span class="string">"monitor"</span>)</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):</div><div class="line">        os.makedirs(checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(monitor_path):</div><div class="line">        os.makedirs(monitor_path)</div><div class="line"></div><div class="line">    saver = tf.train.Saver()</div><div class="line">    <span class="comment"># Load a previous checkpoint if we find one</span></div><div class="line">    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> latest_checkpoint:</div><div class="line">        print(<span class="string">"Loading model checkpoint &#123;&#125;...\n"</span>.format(latest_checkpoint))</div><div class="line">        saver.restore(sess, latest_checkpoint)</div><div class="line">    </div><div class="line">    <span class="comment"># Get the current time step</span></div><div class="line">    total_t = sess.run(tf.contrib.framework.get_global_step())</div><div class="line"></div><div class="line">    <span class="comment"># The epsilon decay schedule</span></div><div class="line">    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)</div><div class="line"></div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(</div><div class="line">        q_estimator,</div><div class="line">        len(VALID_ACTIONS))</div><div class="line"></div><div class="line">    <span class="comment"># Populate the replay memory with initial experience</span></div><div class="line">    print(<span class="string">"Populating replay memory..."</span>)</div><div class="line">    state = env.reset()</div><div class="line">    state = state_processor.process(sess, state)</div><div class="line">    state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(replay_memory_init_size):</div><div class="line">        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps<span class="number">-1</span>)])</div><div class="line">        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])</div><div class="line">        next_state = state_processor.process(sess, next_state)</div><div class="line">        next_state = np.append(state[:,:,<span class="number">1</span>:], np.expand_dims(next_state, <span class="number">2</span>), axis=<span class="number">2</span>)</div><div class="line">        replay_memory.append(Transition(state, action, reward, next_state, done))</div><div class="line">        <span class="keyword">if</span> done:</div><div class="line">            state = env.reset()</div><div class="line">            state = state_processor.process(sess, state)</div><div class="line">            state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state = next_state</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment"># Record videos</span></div><div class="line">    <span class="comment"># Add env Monitor wrapper</span></div><div class="line">    env = Monitor(env, directory=monitor_path, video_callable=<span class="keyword">lambda</span> count: count % record_video_every == <span class="number">0</span>, resume=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line"></div><div class="line">        <span class="comment"># Save the current checkpoint</span></div><div class="line">        saver.save(tf.get_default_session(), checkpoint_path)</div><div class="line"></div><div class="line">        <span class="comment"># Reset the environment</span></div><div class="line">        state = env.reset()</div><div class="line">        state = state_processor.process(sess, state)</div><div class="line">        state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">        loss = <span class="keyword">None</span></div><div class="line"></div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line"></div><div class="line">            <span class="comment"># Epsilon for this time step</span></div><div class="line">            epsilon = epsilons[min(total_t, epsilon_decay_steps<span class="number">-1</span>)]</div><div class="line"></div><div class="line">            <span class="comment"># Add epsilon to Tensorboard</span></div><div class="line">            episode_summary = tf.Summary()</div><div class="line">            episode_summary.value.add(simple_value=epsilon, tag=<span class="string">"epsilon"</span>)</div><div class="line">            q_estimator.summary_writer.add_summary(episode_summary, total_t)</div><div class="line"></div><div class="line">            <span class="comment"># Maybe update the target estimator</span></div><div class="line">            <span class="keyword">if</span> total_t % update_target_estimator_every == <span class="number">0</span>:</div><div class="line">                copy_model_parameters(sess, q_estimator, target_estimator)</div><div class="line">                print(<span class="string">"\nCopied model parameters to target network."</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Print out which step we're on, useful for debugging.</span></div><div class="line">            print(<span class="string">"\rStep &#123;&#125; (&#123;&#125;) @ Episode &#123;&#125;/&#123;&#125;, loss: &#123;&#125;"</span>.format(</div><div class="line">                    t, total_t, i_episode + <span class="number">1</span>, num_episodes, loss), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line"></div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            action_probs = policy(sess, state, epsilon)</div><div class="line">            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])</div><div class="line">            next_state = state_processor.process(sess, next_state)</div><div class="line">            next_state = np.append(state[:,:,<span class="number">1</span>:], np.expand_dims(next_state, <span class="number">2</span>), axis=<span class="number">2</span>)</div><div class="line"></div><div class="line">            <span class="comment"># If our replay memory is full, pop the first element</span></div><div class="line">            <span class="keyword">if</span> len(replay_memory) == replay_memory_size:</div><div class="line">                replay_memory.pop(<span class="number">0</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Save transition to replay memory</span></div><div class="line">            replay_memory.append(Transition(state, action, reward, next_state, done))   </div><div class="line"></div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line"></div><div class="line">            <span class="comment"># Sample a minibatch from the replay memory</span></div><div class="line">            samples = random.sample(replay_memory, batch_size)</div><div class="line">            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))</div><div class="line"></div><div class="line">            <span class="comment"># Calculate q values and targets</span></div><div class="line">            q_values_next = target_estimator.predict(sess, next_states_batch)</div><div class="line">            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Perform gradient descent update</span></div><div class="line">            states_batch = np.array(states_batch)</div><div class="line">            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line"></div><div class="line">            state = next_state</div><div class="line">            total_t += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="comment"># Add summaries to tensorboard</span></div><div class="line">        episode_summary = tf.Summary()</div><div class="line">        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=<span class="string">"episode_reward"</span>, tag=<span class="string">"episode_reward"</span>)</div><div class="line">        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=<span class="string">"episode_length"</span>, tag=<span class="string">"episode_length"</span>)</div><div class="line">        q_estimator.summary_writer.add_summary(episode_summary, total_t)</div><div class="line">        q_estimator.summary_writer.flush()</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> total_t, plotting.EpisodeStats(</div><div class="line">            episode_lengths=stats.episode_lengths[:i_episode+<span class="number">1</span>],</div><div class="line">            episode_rewards=stats.episode_rewards[:i_episode+<span class="number">1</span>])</div><div class="line"></div><div class="line">    <span class="keyword">return</span> stats</div></pre></td></tr></table></figure><p>Finally, run it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">tf.reset_default_graph()</div><div class="line"></div><div class="line"><span class="comment"># Where we save our checkpoints and graphs</span></div><div class="line">experiment_dir = os.path.abspath(<span class="string">"./experiments/&#123;&#125;"</span>.format(env.spec.id))</div><div class="line"></div><div class="line"><span class="comment"># Create a glboal step variable</span></div><div class="line">global_step = tf.Variable(<span class="number">0</span>, name=<span class="string">'global_step'</span>, trainable=<span class="keyword">False</span>)</div><div class="line">    </div><div class="line"><span class="comment"># Create estimators</span></div><div class="line">q_estimator = Estimator(scope=<span class="string">"q"</span>, summaries_dir=experiment_dir)</div><div class="line">target_estimator = Estimator(scope=<span class="string">"target_q"</span>)</div><div class="line"></div><div class="line"><span class="comment"># State processor</span></div><div class="line">state_processor = StateProcessor()</div><div class="line"></div><div class="line"><span class="comment"># Run it!</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    <span class="keyword">for</span> t, stats <span class="keyword">in</span> deep_q_learning(sess,</div><div class="line">                                    env,</div><div class="line">                                    q_estimator=q_estimator,</div><div class="line">                                    target_estimator=target_estimator,</div><div class="line">                                    state_processor=state_processor,</div><div class="line">                                    experiment_dir=experiment_dir,</div><div class="line">                                    num_episodes=<span class="number">10000</span>,</div><div class="line">                                    replay_memory_size=<span class="number">500000</span>,</div><div class="line">                                    replay_memory_init_size=<span class="number">50000</span>,</div><div class="line">                                    update_target_estimator_every=<span class="number">10000</span>,</div><div class="line">                                    epsilon_start=<span class="number">1.0</span>,</div><div class="line">                                    epsilon_end=<span class="number">0.1</span>,</div><div class="line">                                    epsilon_decay_steps=<span class="number">500000</span>,</div><div class="line">                                    discount_factor=<span class="number">0.99</span>,</div><div class="line">                                    batch_size=<span class="number">32</span>):</div><div class="line"></div><div class="line">        print(<span class="string">"\nEpisode Reward: &#123;&#125;"</span>.format(stats.episode_rewards[<span class="number">-1</span>]))</div></pre></td></tr></table></figure><hr><p>Next, we will develop the Double-DQN algorithm. This algorithm only has a little changes.</p><p>In DQN <strong>q_learning</strong> method,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate q values and targets</span></div><div class="line">q_values_next = target_estimator.predict(sess, next_states_batch)</div><div class="line">targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p>we just change these codes to,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate q values and targets</span></div><div class="line"><span class="comment"># This is where Double Q-Learning comes in!</span></div><div class="line">q_values_next = q_estimator.predict(sess, next_states_batch)</div><div class="line">best_actions = np.argmax(q_values_next, axis=<span class="number">1</span>)</div><div class="line">q_values_next_target = target_estimator.predict(sess, next_states_batch)</div><div class="line">targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * q_values_next_target[np.arange(batch_size), best_actions]</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In previous &lt;a href=&quot;https://ewanlee.github.io/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/&quot; target=&quot;_blank&quot; rel=
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="deep reinforcement learning" scheme="http://yoursite.com/tags/deep-reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Summary of the Reinforcement Learning Papers</title>
    <link href="http://yoursite.com/2017/07/08/Summary-of-the-papers/"/>
    <id>http://yoursite.com/2017/07/08/Summary-of-the-papers/</id>
    <published>2017-07-08T09:49:20.000Z</published>
    <updated>2017-07-12T04:38:30.750Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Deep-Reinforcement-Learning-with-Double-Q-learning"><a href="#Deep-Reinforcement-Learning-with-Double-Q-learning" class="headerlink" title="Deep Reinforcement Learning with Double Q-learning"></a>Deep Reinforcement Learning with Double Q-learning</h3><p>The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having introduce additional networks. We therefore propose to evaluate the greedy policy according to the online network, but using the target network to estimate its value. In reference to both Double Q-learning and DQN, we refer to the resulting algorithm as Double DQN. Its update is the same as for DQN, but replacing the target $Y_t^{DQN}$ with<br>$$<br>Y_t^{\text{DoubleDQN}} \equiv R_{t+1} + \gamma Q(S_{t+1}, {\arg \max}_a Q(S_{t+1},a; \boldsymbol{\theta}_t),\boldsymbol{\theta}_t^{\mathbf{-}}).<br>$$<br>In comparison to Double Q-learning<br>$$<br>Y_t^{\text{DoubleDQN}} \equiv R_{t+1} + \gamma Q(S_{t+1}, {\arg \max}_a Q(S_{t+1},a; \boldsymbol{\theta}_t),\boldsymbol{\theta}_t^{\boldsymbol{\prime}}),<br>$$<br>the weights of the second network $\boldsymbol{\theta_t^{\prime}}$ are replaced with the weights of the target network $\boldsymbol{\theta_t^{-}}$ for the evaluation of the current greedy policy. The update to the target stays unchanged from DQN, and remains a periodic copy of the online network.</p><hr><h3 id="Prioritized-Experience-Replay"><a href="#Prioritized-Experience-Replay" class="headerlink" title="Prioritized Experience Replay"></a>Prioritized Experience Replay</h3><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/papersdouble-dqn-priori-exper-replay.png" alt="double-dqn-priori-exper-replay"></p><p>where $p_i &gt; 0$ is the priority of transition $i$. The exponent $\alpha$ determines how much prioritization is used, with $\alpha=0$ corresponding to the uniform case.</p><p>The first we consider is the direct, proportional prioritization where $p_i = |\delta_i| + \epsilon$, where $\delta_i$ is the TD-error of transition $i$ and $\epsilon$ is a small positive constant that prevent the edge-case of transitions not being revisited once their error is zero. The second variant is an indirect, rand-based prioritization where $p_i = \frac{1}{\text{rank}(i)}$, where $\text{rank}_i$ is the rank of transition $i$ when the replay memory is sorted according to $|\delta_i|$</p><hr><h3 id="Dueling-Network-Architectures-for-Deep-Reinforcement-Learning"><a href="#Dueling-Network-Architectures-for-Deep-Reinforcement-Learning" class="headerlink" title="Dueling Network Architectures for Deep Reinforcement Learning"></a>Dueling Network Architectures for Deep Reinforcement Learning</h3><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/papersduel-network-rl-arch.png" alt="duel-network-arch"></p><p>Let us consider the dueling network shown in above, where we make one stream of fully-connected layers output a scalar $V(s;\theta,\beta)$, and the other stream output an $\mathcal{A}$-dimensional vector $A(s, a; \theta, \alpha)$. Here, $\theta$ denotes the parameters of the convolutional layers, while $\alpha$ and $\beta$ are the parameters of the two streams of fully-connected layers. Using the definition of advantage, we might be tempted to construct the aggregating module as follows:<br>$$<br>Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + A(s, a; \theta, \alpha),<br>$$<br>Note that this expression applies to all $(s, a)$ instances; that is, to express equation above in matrix form we need to replicate the scalar, $V(s;\theta,\beta)$, $|\mathcal{A}|$ times.</p><p>Equation above is unidentifiable in the sense that given $Q$ we cannot recover V and A uniquely. To see this, add a constant to $V(s;\theta,\beta)$ and subtract the same constant from $A(s, a; \theta, \alpha)$. This constant cancels out resulting in the same $Q$ value. This lack of identifiability is mirrored by poor practical performance when this equation is used directly.</p><p>To address this issue of identifiability, we can replace the equation above to this one:<br>$$<br>Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \Big( A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a^{\prime}} A(s, a^{\prime}; \theta, \alpha) \Big).<br>$$</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Deep-Reinforcement-Learning-with-Double-Q-learning&quot;&gt;&lt;a href=&quot;#Deep-Reinforcement-Learning-with-Double-Q-learning&quot; class=&quot;headerlink&quot;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Using Keras and Deep Q-Network to Play FlappyBird (Repost)</title>
    <link href="http://yoursite.com/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/"/>
    <id>http://yoursite.com/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/</id>
    <published>2017-07-06T20:19:18.000Z</published>
    <updated>2017-07-06T20:34:48.861Z</updated>
    
    <content type="html"><![CDATA[<p>200 lines of python code to demonstrate DQN with Keras</p><p><img src="https://yanpanlau.github.io/img/animation1.gif" alt="img"></p><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>This project demonstrates how to use the Deep-Q Learning algorithm with Keras together to play FlappyBird.</p><p>This article is intended to target newcomers who are interested in Reinforcement Learning.</p><h1 id="Installation-Dependencies"><a href="#Installation-Dependencies" class="headerlink" title="Installation Dependencies:"></a>Installation Dependencies:</h1><p>(Update : 13 March 2017, code and weight file has been updated to support latest version of tensorflow and keras)</p><ul><li>Python 2.7</li><li>Keras 1.0</li><li>pygame</li><li>scikit-image</li></ul><h1 id="How-to-Run"><a href="#How-to-Run" class="headerlink" title="How to Run?"></a>How to Run?</h1><p><strong>CPU only/TensorFlow</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><p><strong>GPU version (Theano)</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">THEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=0.2 python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><p><strong>lib.cnmem=0.2</strong> means you assign only 20% of the GPU’s memory to the program.</p><p><strong>If you want to train the network from beginning, delete “model.h5” and run qlearn.py -m “Train”</strong></p><h1 id="What-is-Deep-Q-Network"><a href="#What-is-Deep-Q-Network" class="headerlink" title="What is Deep Q-Network?"></a>What is Deep Q-Network?</h1><p>Deep Q-Network is a learning algorithm developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the algorithm is generic enough to play various games.</p><p>The following post is a must-read for those who are interested in deep reinforcement learning.</p><p><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">Demystifying Deep Reinforcement Learning</a></p><h1 id="Code-Explanation-in-details"><a href="#Code-Explanation-in-details" class="headerlink" title="Code Explanation (in details)"></a>Code Explanation (in details)</h1><p>Let’s go though the example in <strong>qlearn.py</strong>, line by line. If you familiar with Keras and DQN, you can skip this session</p><p>The code simply does the following:</p><ol><li>The code receives the Game Screen Input in the form of a pixel array</li><li>The code does some image pre-processing</li><li>The processed image will be fed into a neural network (Convolution Neural Network), and the network will then decide the best action to execute (Flap or Not Flap)</li><li>The network will be trained millions of times, via an algorithm called Q-learning, to maximize the future expected reward.</li></ol><h3 id="Game-Screen-Input"><a href="#Game-Screen-Input" class="headerlink" title="Game Screen Input"></a>Game Screen Input</h3><p>First of all, the FlappyBird is already written in Python via pygame, so here is the code snippet to access the FlappyBird API</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import wrapped_flappy_bird as game</div><div class="line">x_t1_colored, r_t, terminal = game_state.frame_step(a_t)</div></pre></td></tr></table></figure><p>The idea is quite simple, the input is <strong>a_t</strong> (0 represent don’t flap, 1 represent flap), the API will give you the next frame <strong>x_t1_colored</strong>, the <strong>reward</strong> (0.1 if alive, +1 if pass the pipe, -1 if die) and <strong>terminal</strong> is a boolean flag indicates whether the game is FINISHED or NOT. We also followed DeepMind suggestion to clip the reward between [-1,+1] to improve the stability. I have not yet get a chance to test out different reward functions but it would be an interesting exercise to see how the performance is changed with different reward functions.</p><p>Interesting readers can modify the reward function in <strong>game/wrapped_flappy_bird.py”, under the function **def frame_step(self, input_actions)</strong></p><h3 id="Image-pre-processing"><a href="#Image-pre-processing" class="headerlink" title="Image pre-processing"></a>Image pre-processing</h3><p><img src="https://yanpanlau.github.io/img/bird.jpg" alt="img"></p><p>In order to make the code train faster, it is vital to do some image processing. Here are the key elements:</p><ol><li>I first convert the color image into grayscale</li><li>I crop down the image size into 80x80 pixel</li><li>I stack 4 frames together before I feed into neural network.</li></ol><p>Why do I need to stack 4 frames together? This is one way for the model to be able to infer the velocity information of the bird.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x_t1 = skimage.color.rgb2gray(x_t1_colored)</div><div class="line">x_t1 = skimage.transform.resize(x_t1,(<span class="number">80</span>,<span class="number">80</span>))</div><div class="line">x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(<span class="number">0</span>, <span class="number">255</span>))</div><div class="line"></div><div class="line">x_t1 = x_t1.reshape(<span class="number">1</span>, <span class="number">1</span>, x_t1.shape[<span class="number">0</span>], x_t1.shape[<span class="number">1</span>])</div><div class="line">s_t1 = np.append(x_t1, s_t[:, :<span class="number">3</span>, :, :], axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p><strong>x_t1</strong> is a single frame with shape (1x1x80x80) and <strong>s_t1</strong> is the stacked frame with shape (1x4x80x80). You might ask, why the input dimension is (1x4x80x80) but not (4x80x80)? Well, it is a requirement in Keras so let’s stick with it.</p><p>Note: Some readers may ask what is <strong>axis=1</strong>? It means that when I stack the frames, I want to stack on the “2nd” dimension. i.e. I am stacking under (1x<strong>4</strong>x80x80), the 2nd index.</p><h3 id="Convolution-Neural-Network"><a href="#Convolution-Neural-Network" class="headerlink" title="Convolution Neural Network"></a>Convolution Neural Network</h3><p>Now, we can input the pre-processed screen into the neural network, which is a convolution neural network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildmodel</span><span class="params">()</span>:</span></div><div class="line">    print(<span class="string">"Now we build the model"</span>)</div><div class="line">    model = Sequential()</div><div class="line">    model.add(Convolution2D(<span class="number">32</span>, <span class="number">8</span>, <span class="number">8</span>, subsample=(<span class="number">4</span>,<span class="number">4</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>,input_shape=(img_channels,img_rows,img_cols)))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">4</span>, <span class="number">4</span>, subsample=(<span class="number">2</span>,<span class="number">2</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, subsample=(<span class="number">1</span>,<span class="number">1</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Flatten())</div><div class="line">    model.add(Dense(<span class="number">512</span>, init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Dense(<span class="number">2</span>,init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)))</div><div class="line">   </div><div class="line">    adam = Adam(lr=<span class="number">1e-6</span>)</div><div class="line">    model.compile(loss=<span class="string">'mse'</span>,optimizer=adam)</div><div class="line">    print(<span class="string">"We finish building the model"</span>)</div><div class="line">    <span class="keyword">return</span> model</div></pre></td></tr></table></figure><p>The exact architecture is following : The input to the neural network consists of an 4x80x80 images. The first hidden layer convolves 32 filters of 8 x 8 with stride 4 and applies ReLU activation function. The 2nd layer convolves a 64 filters of 4 x 4 with stride 2 and applies ReLU activation function. The 3rd layer convolves a 64 filters of 3 x 3 with stride 1 and applies ReLU activation function. The final hidden layer is fully-connected consisted of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action.</p><p>So wait, what is convolution? The easiest way to understand a convolution is by thinking of it as a sliding window function apply to a matrix. The following gif file should help to understand.</p><p><img src="https://yanpanlau.github.io/img/Convolution_schematic.gif" alt="Convolution with 3×3 Filter. Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution"></p><p>You might ask what’s the purpose the convolution? It actually help computer to learn higher features like edges and shapes. The example below shows how the edges are stand out after a convolution filter is applied</p><p><img src="https://yanpanlau.github.io/img/generic-taj-convmatrix-edge-detect.jpg" alt="Using Convolution to detect Edges"></p><p>For more details about Convolution in Neural Network, please read <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_blank" rel="external">Understanding Convolution Neural Networks for NLP</a></p><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>Keras makes it very easy to build convolution neural network. However, there are few things I would like to highlight</p><p>A) It is important to choose a right initialization method. I choose normal distribution with $\sigma=0.01$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)</div></pre></td></tr></table></figure><p>B) The ordering of the dimension is important, the default setting is 4x80x80 (Theano setting), so if your input is 80x80x4 (Tensorflow setting) then you are in trouble because the dimension is wrong. <strong>Alert</strong>: If your input dimension is 80x80x4 (Tensorflow setting) you need to set <strong>dim_ordering = tf</strong> (tf means tensorflow, th means theano)</p><p>C) In Keras, <strong>subsample=(2,2)</strong> means you down sample the image size from (80x80) to (40x40). In ML literature it is often called “stride”</p><p>D) We have used an adaptive learning algorithm called ADAM to do the optimization. The learning rate is <strong>1-e6</strong>.</p><p>Interested readers who want to learn more various learning algoithms please read below</p><p><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">An overview of gradient descent optimization algorithms</a></p><h3 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h3><p>Finally, we can using the Q-learning algorithm to train the neural network.</p><p>So, what is Q-learning? In Q-learning the most important thing is the Q-function : Q(s, a) representing the maximum discounted future reward when we perform action <strong>a</strong> in state <strong>s</strong>. <strong>Q(s, a)</strong> gives you an estimation of how good to choose an action <strong>a</strong> in state <strong>s</strong>.</p><p>REPEAT : <strong>Q(s, a)</strong> representing the maximum discounted future reward when we perform action <strong>a</strong> in state <strong>s</strong></p><p>You might ask 1) Why Q-function is useful? 2) How can I get the Q-function?</p><p>Let me give you an analogy of the Q-function: Suppose you are playing a difficult RPG game and you don’t know how to play it well. If you have bought a strategy guide, which is an instruction book that contain hints or complete solutions to a specific video game, then playing that video game is easy. You just follow the guidiance from the strategy book. Here, Q-function is similar to a strategy guide. Suppose you are in state <strong>s</strong> and you need to decide whether you take action <strong>a</strong> or <strong>b</strong>. If you have this magical Q-function, the answers become really simple – pick the action with highest Q-value!</p><p>$$<br>{\pi(s) = {argmax}_{a} Q(s,a)}<br>$$<br>Here, $\pi$ represents the policy, which you will often see in the ML literature.</p><p>How do we get the Q-function? That’s where Q-learning is coming from. Let me quickly derive here:</p><p>Define total future reward from time <strong>t</strong> onward<br>$$<br>R_t = r_t + r_{t+1} + r_{t+2} … + r_n<br>$$<br>But because our environment is stochastic, we can never be sure, the more future we go, the more uncertain. Therefore, it is common to use <strong>discount future reward</strong> instead<br>$$<br>R_t = r_t + \gamma r_{t+1} + \gamma^{2} r_{t+2} … + \gamma^{n-t} r_n<br>$$<br>which, can be written as<br>$$<br>R_t = r_t + \gamma \ast R_{t+1}<br>$$<br>Recall the definition of Q-function (maximum discounted future reward if we choose action <strong>a</strong> in state <strong>s</strong>)<br>$$<br>Q(s_t, a_t) = max R_{t+1}<br>$$<br>therefore, we can rewrite the Q-function as below<br>$$<br>Q(s, a) = r + \gamma \ast max_{a^{‘}} Q(s^{\prime}, a^{\prime})<br>$$<br>In plain English, it means maximum future reward for this state and action (s,a) is the immediate reward r <strong>plus</strong> maximum future reward for the next state $s^{\prime}$ , action $a^{\prime}$</p><p>We could now use an iterative method to solve for the Q-function. Given a transition $(s, a, r, s^{\prime})$ , we are going to convert this episode into training set for the network. i.e. We want $r + \gamma max_a Q (s,a)$ to be equal to $Q(s,a)$. You can think of finding a Q-value is a regession task now, I have a estimator $r + \gamma max_a Q (s,a)$ and a predictor $Q(s,a)$, I can define the mean squared error (MSE), or the loss function, as below:</p><p>Define a loss function<br>$$<br>L = [r + \gamma max_{a^{\prime}} Q (s^{\prime},a^{\prime}) - Q(s,a)]^{2}<br>$$<br>Given a transition $(s, a, r, s^{\prime})$, how can I optimize my Q-function such that it returns the smallest mean squared error loss? If L getting smaller, we know the Q-function is getting converged into the optimal value, which is our “strategy book”.</p><p>Now, you might ask, where is the role of the neural network? This is where the <strong>DEEP Q-Learning</strong> comes in. You recall that $Q(s,a)$, is a stategy book, which contains millions or trillions of states and actions if you display as a table. The idea of the DQN is that I use the neural network to <strong>COMPRESS</strong> this Q-table, using some parameters \thetaθ <strong>(We called it weight in Neural Network)</strong>. So instead of handling a large table, I just need to worry the weights of the neural network. By smartly tuning the weight parameters, I can find the optimal Q-function via the various Neural Network training algorithm.<br>$$<br>Q(s,a) = f_{\theta}(s)<br>$$<br>where $f$ is our neural network with input $s$ and weight parameters $\theta$</p><p>Here is the code below to demonstrate how it works</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> t &gt; OBSERVE:</div><div class="line">    <span class="comment">#sample a minibatch to train on</span></div><div class="line">    minibatch = random.sample(D, BATCH)</div><div class="line"></div><div class="line">    inputs = np.zeros((BATCH, s_t.shape[<span class="number">1</span>], s_t.shape[<span class="number">2</span>], s_t.shape[<span class="number">3</span>]))   <span class="comment">#32, 80, 80, 4</span></div><div class="line">    targets = np.zeros((inputs.shape[<span class="number">0</span>], ACTIONS))                         <span class="comment">#32, 2</span></div><div class="line"></div><div class="line">    <span class="comment">#Now we do the experience replay</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(minibatch)):</div><div class="line">        state_t = minibatch[i][<span class="number">0</span>]</div><div class="line">        action_t = minibatch[i][<span class="number">1</span>]   <span class="comment">#This is action index</span></div><div class="line">        reward_t = minibatch[i][<span class="number">2</span>]</div><div class="line">        state_t1 = minibatch[i][<span class="number">3</span>]</div><div class="line">        terminal = minibatch[i][<span class="number">4</span>]</div><div class="line">        <span class="comment"># if terminated, only equals reward</span></div><div class="line"></div><div class="line">        inputs[i:i + <span class="number">1</span>] = state_t    <span class="comment">#I saved down s_t</span></div><div class="line"></div><div class="line">        targets[i] = model.predict(state_t)  <span class="comment"># Hitting each buttom probability</span></div><div class="line">        Q_sa = model.predict(state_t1)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> terminal:</div><div class="line">            targets[i, action_t] = reward_t</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)</div><div class="line"></div><div class="line">        loss += model.train_on_batch(inputs, targets)</div><div class="line"></div><div class="line">    s_t = s_t1</div><div class="line">    t = t + <span class="number">1</span></div></pre></td></tr></table></figure><h3 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h3><p>If you examine the code above, there is a comment called “Experience Replay”. Let me explain what it does: It was found that approximation of Q-value using non-linear functions like neural network is not very stable. The most important trick to solve this problem is called <strong>experience replay</strong>. During the gameplay all the episode $(s, a, r, s^{‘})$ are stored in replay memory <strong>D</strong>. (I use Python function deque() to store it). When training the network, random mini-batches from the replay memory are used instead of most the recent transition, which will greatly improve the stability.</p><h3 id="Exploration-vs-Exploitation"><a href="#Exploration-vs-Exploitation" class="headerlink" title="Exploration vs. Exploitation"></a>Exploration vs. Exploitation</h3><p>There is another issue in the reinforcement learning algorithm which called Exploration vs. Exploitation. How much of an agent’s time should be spent exploiting its existing known-good policy, and how much time should be focused on exploring new, possibility better, actions? We often encounter similar situations in real life too. For example, we face on which restaurant to go to on Saturday night. We all have a set of restaurants that we prefer, based on our policy/strategy book $Q(s,a)$. If we stick to our normal preference, there is a strong probability that we’ll pick a good restaurant. However, sometimes, we occasionally like to try new restaurants to see if they are better. The RL agents face the same problem. In order to maximize future reward, they need to balance the amount of time that they follow their current policy (this is called being “greedy”), and the time they spend exploring new possibilities that might be better. A popular approach is called $\epsilon$ greedy approach. Under this approach, the policy tells the agent to try a random action some percentage of the time, as defined by the variable $\epsilon$ (epsilon), which is a number between 0 and 1. The strategy will help the RL agent to occasionally try something new and see if we can achieve ultimate strategy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> random.random() &lt;= epsilon:</div><div class="line">    print(<span class="string">"----------Random Action----------"</span>)</div><div class="line">    action_index = random.randrange(ACTIONS)</div><div class="line">    a_t[action_index] = <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">    q = model.predict(s_t)       <span class="comment">#input a stack of 4 images, get the prediction</span></div><div class="line">    max_Q = np.argmax(q)</div><div class="line">    action_index = max_Q</div><div class="line">    a_t[max_Q] = <span class="number">1</span></div></pre></td></tr></table></figure><p>I think that’s it. I hope this blog will help you to understand how DQN works.</p><h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h3 id="My-training-is-very-slow"><a href="#My-training-is-very-slow" class="headerlink" title="My training is very slow"></a>My training is very slow</h3><p>You might need a GPU to accelerate the calculation. I used a TITAN X and train for at least 1 million frames to make it work</p><h1 id="Future-works-and-thoughts"><a href="#Future-works-and-thoughts" class="headerlink" title="Future works and thoughts"></a>Future works and thoughts</h1><ol><li>Current DQN depends on large experience replay. Is it possible to replace it or even remove it?</li><li>How can one decide on the optimal Convolution Neural Network?</li><li>Training is very slow, how to speed it up/to make the model converge faster?</li><li>What does the Neural Network actually learn? Is the knowledge transferable?</li></ol><p>I believe the questions are still not resolved and it’s an active research area in Machine Learning.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. <strong>Human-level Control through Deep Reinforcement Learning</strong>. Nature, 529-33, 2015.</p><h1 id="Disclaimer"><a href="#Disclaimer" class="headerlink" title="Disclaimer"></a>Disclaimer</h1><p>This work is highly based on the following repos:</p><p><a href="https://github.com/yenchenlin/DeepLearningFlappyBird" target="_blank" rel="external">https://github.com/yenchenlin/DeepLearningFlappyBird</a></p><p><a href="http://edersantana.github.io/articles/keras_rl/" target="_blank" rel="external">http://edersantana.github.io/articles/keras_rl/</a></p><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>I must thank to <a href="https://twitter.com/hardmaru" target="_blank" rel="external">@hardmaru</a> to encourage me to write this blog. I also thank to <a href="https://twitter.com/fchollet" target="_blank" rel="external">@fchollet</a> to help me on the weight initialization in Keras and <a href="https://twitter.com/edersantana" target="_blank" rel="external">@edersantana</a> his post on Keras and reinforcement learning which really help me to understand it.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;200 lines of python code to demonstrate DQN with Keras&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://yanpanlau.github.io/img/animation1.gif&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;&lt;h1 
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="deep reinforcement learning" scheme="http://yoursite.com/tags/deep-reinforcement-learning/"/>
    
      <category term="DQN" scheme="http://yoursite.com/tags/DQN/"/>
    
  </entry>
  
  <entry>
    <title>Demystifying Deep Reinforcement Learning (Repost)</title>
    <link href="http://yoursite.com/2017/07/07/Demystifying-Deep-Reinforcement-Learning/"/>
    <id>http://yoursite.com/2017/07/07/Demystifying-Deep-Reinforcement-Learning/</id>
    <published>2017-07-06T17:36:44.000Z</published>
    <updated>2017-07-06T17:45:40.451Z</updated>
    
    <content type="html"><![CDATA[<p>Two years ago, a small company in London called DeepMind uploaded their pioneering paper “<a href="http://arxiv.org/abs/1312.5602" target="_blank" rel="external">Playing Atari with Deep Reinforcement Learning</a>” to Arxiv. In this paper they demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable, because the games and the goals in every game were very different and designed to be challenging for humans. The same model architecture, without any change, was used to learn seven different games, and in three of them the algorithm performed even better than a human!</p><p>It has been hailed since then as the first step towards <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank" rel="external">general artificial intelligence</a> – an AI that can survive in a variety of environments, instead of being confined to strict realms such as playing chess. No wonder <a href="http://techcrunch.com/2014/01/26/google-deepmind/" target="_blank" rel="external">DeepMind was immediately bought by Google</a> and has been on the forefront of deep learning research ever since. In February 2015 their paper “<a href="http://www.nature.com/articles/nature14236" target="_blank" rel="external">Human-level control through deep reinforcement learning</a>” was featured on the cover of Nature, one of the most prestigious journals in science. In this paper they applied the same model to 49 different games and achieved superhuman performance in half of them.</p><p>Still, while deep models for supervised and unsupervised learning have seen widespread adoption in the community, deep reinforcement learning has remained a bit of a mystery. In this blog post I will be trying to demystify this technique and understand the rationale behind it. The intended audience is someone who already has background in machine learning and possibly in neural networks, but hasn’t had time to delve into reinforcement learning yet.</p><p>The roadmap ahead:</p><ol><li><strong>What are the main challenges in reinforcement learning?</strong> We will cover the credit assignment problem and the exploration-exploitation dilemma here.</li><li><strong>How to formalize reinforcement learning in mathematical terms?</strong> We will define Markov Decision Process and use it for reasoning about reinforcement learning.</li><li><strong>How do we form long-term strategies?</strong> We define “discounted future reward”, that forms the main basis for the algorithms in the next sections.</li><li><strong>How can we estimate or approximate the future reward?</strong> Simple table-based Q-learning algorithm is defined and explained here.</li><li><strong>What if our state space is too big?</strong> Here we see how Q-table can be replaced with a (deep) neural network.</li><li><strong>What do we need to make it actually work?</strong> Experience replay technique will be discussed here, that stabilizes the learning with neural networks.</li><li><strong>Are we done yet?</strong> Finally we will consider some simple solutions to the exploration-exploitation problem.</li></ol><h1 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h1><p>Consider the game Breakout. In this game you control a paddle at the bottom of the screen and have to bounce the ball back to clear all the bricks in the upper half of the screen. Each time you hit a brick, it disappears and your score increases – you get a reward.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.08.53-AM.png" alt="img">Figure 1: Atari Breakout game. Image credit: DeepMind.</p><p>Suppose you want to teach a neural network to play this game. Input to your network would be screen images, and output would be three actions: left, right or fire (to launch the ball). It would make sense to treat it as a classification problem – for each game screen you have to decide, whether you should move left, right or press fire. Sounds straightforward? Sure, but then you need training examples, and a lots of them. Of course you could go and record game sessions using expert players, but that’s not really how we learn. We don’t need somebody to tell us a million times which move to choose at each screen. We just need occasional feedback that we did the right thing and can then figure out everything else ourselves.</p><p>This is the task <strong>reinforcement learning </strong>tries to solve. Reinforcement learning lies somewhere in between supervised and unsupervised learning. Whereas in supervised learning one has a target label for each training example and in unsupervised learning one has no labels at all, in reinforcement learning one has sparse and time-delayed labels – the rewards. Based only on those rewards the agent has to learn to behave in the environment.</p><p>While the idea is quite intuitive, in practice there are numerous challenges. For example when you hit a brick and score a reward in the Breakout game, it often has nothing to do with the actions (paddle movements) you did just before getting the reward. All the hard work was already done, when you positioned the paddle correctly and bounced the ball back. This is called the <strong>credit assignment problem</strong> – i.e., which of the preceding actions was responsible for getting the reward and to what extent.</p><p>Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards? In the above Breakout game a simple strategy is to move to the left edge and wait there. When launched, the ball tends to fly left more often than right and you will easily score about 10 points before you die. Will you be satisfied with this or do you want more? This is called the <strong>explore-exploit dilemma</strong> – should you exploit the known working strategy or explore other, possibly better strategies.</p><p>Reinforcement learning is an important model of how we (and all animals in general) learn. Praise from our parents, grades in school, salary at work – these are all examples of rewards. Credit assignment problems and exploration-exploitation dilemmas come up every day both in business and in relationships. That’s why it is important to study this problem, and games form a wonderful sandbox for trying out new approaches.</p><h1 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h1><p>Now the question is, how do you formalize a reinforcement learning problem, so that you can reason about it? The most common method is to represent it as a Markov decision process.</p><p>Suppose you are an <strong>agent</strong>, situated in an <strong>environment</strong> (e.g. Breakout game). The environment is in a certain <strong>state</strong>(e.g. location of the paddle, location and direction of the ball, existence of every brick and so on). The agent can perform certain <strong>actions</strong> in the environment (e.g. move the paddle to the left or to the right). These actions sometimes result in a <strong>reward</strong> (e.g. increase in score). Actions transform the environment and lead to a new state, where the agent can perform another action, and so on. The rules for how you choose those actions are called <strong>policy</strong>. The environment in general is stochastic, which means the next state may be somewhat random (e.g. when you lose a ball and launch a new one, it goes towards a random direction).</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-12.01.04-PM.png" alt="img">Figure 2: <em>Left: </em>reinforcement learning problem. <em>Right: </em>Markov decision process.</p><p>The set of states and actions, together with rules for transitioning from one state to another, make up a <strong>Markov decision process</strong>. One <strong>episode</strong> of this process (e.g. one game) forms a finite sequence of states, actions and rewards:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.19-AM.png" alt="Screen Shot 2015-12-21 at 11.09.19 AM"></p><p>Here si represents the state, ai is the action and ri+1 is the reward after performing the action. The episode ends with <strong>terminal</strong> state sn (e.g. “game over” screen). A Markov decision process relies on the Markov assumption, that the probability of the next state si+1 depends only on current state si and action ai, but not on preceding states or actions.</p><h1 id="Discounted-Future-Reward"><a href="#Discounted-Future-Reward" class="headerlink" title="Discounted Future Reward"></a>Discounted Future Reward</h1><p>To perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. How should we go about that?</p><p>Given one run of the Markov decision process, we can easily calculate the <strong>total reward</strong> for one episode:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.26-AM.png" alt="Screen Shot 2015-12-21 at 11.09.26 AM"></p><p>Given that, the <strong>total future reward</strong> from time point <em>t</em> onward can be expressed as:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.32-AM.png" alt="Screen Shot 2015-12-21 at 11.09.32 AM"></p><p>But because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use <strong>discounted future reward </strong>instead:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.36-AM.png" alt="Screen Shot 2015-12-21 at 11.09.36 AM"></p><p>Here <em>γ</em> is the discount factor between 0 and 1 – the more into the future the reward is, the less we take it into consideration. It is easy to see, that discounted future reward at time step <em>t</em> can be expressed in terms of the same thing at time step <em>t+1</em>:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.40-AM.png" alt="Screen Shot 2015-12-21 at 11.09.40 AM"></p><p>If we set the discount factor <em>γ</em>=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like <em>γ=</em>0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor <em>γ</em>=1.</p><p>A good strategy for an agent would be to <strong>always choose an action that maximizes the (discounted) future reward</strong>.</p><h1 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h1><p>In Q-learning we define a function <em>Q(s, a)</em> representing <strong>the maximum discounted future reward when we perform action </strong>a<strong> in state </strong>s<strong>, and continue optimally from that point on.</strong></p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.47-AM.png" alt="Screen Shot 2015-12-21 at 11.09.47 AM"></p><p>The way to think about <em>Q(s, a)</em> is that it is “the best possible score at the end of the game after performing action a<strong>in state </strong>s<strong>“. It is called Q-function, because it represents the “quality” of a certain action in a given state.</strong></p><p>This may sound like quite a puzzling definition. How can we estimate the score at the end of game, if we know just the current state and action, and not the actions and rewards coming after that? We really can’t. But as a theoretical construct we can assume existence of such a function. Just close your eyes and repeat to yourself five times: “<em>Q(s, a) </em>exists, <em>Q(s, a) </em>exists, …”. Feel it?</p><p>If you’re still not convinced, then consider what the implications of having such a function would be. Suppose you are in state and pondering whether you should take action <em>a</em> or <em>b</em>. You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer becomes really simple – pick the action with the highest Q-value!</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.56-AM.png" alt="Screen Shot 2015-12-21 at 11.09.56 AM"></p><p>Here π represents the policy, the rule how we choose an action in each state.</p><p>OK, how do we get that Q-function then? Let’s focus on just one transition &lt;<em>s, a, r, s’</em>&gt;. Just like with discounted future rewards in the previous section, we can express the Q-value of state <em>s</em> and action <em>a</em> in terms of the Q-value of the next state <em>s’</em>.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.10.00-AM.png" alt="Screen Shot 2015-12-21 at 11.10.00 AM">This is called the <strong>Bellman equation</strong>. If you think about it, it is quite logical – maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state.</p><p>The main idea in Q-learning is that <strong>we can iteratively approximate the Q-function using the Bellman equation</strong>. In the simplest case the Q-function is implemented as a table, with states as rows and actions as columns. The gist of the Q-learning algorithm is as simple as the following<a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/#_ftn1" target="_blank" rel="external">[1]</a>:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.55-AM.png" alt="Screen Shot 2015-12-21 at 11.23.55 AM"></p><p><em>α</em> in the algorithm is a learning rate that controls how much of the difference between previous Q-value and newly proposed Q-value is taken into account. In particular, when <em>α</em>=1, then two <em>Q[s,a]</em> cancel and the update is exactly the same as the Bellman equation.</p><p>The max<em>a’</em> <em>Q</em>[<em>s’</em>,<em>a’</em>] that we use to update <em>Q</em>[<em>s</em>,<em>a</em>] is only an approximation and in early stages of learning it may be completely wrong. However the approximation get more and more accurate with every iteration and <a href="http://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/55/2015/12/ProofQlearning.pdf" target="_blank" rel="external">it has been shown</a>, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.</p><h1 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a>Deep Q Network</h1><p>The state of the environment in the Breakout game can be defined by the location of the paddle, location and direction of the ball and the presence or absence of each individual brick. This intuitive representation however is game specific. Could we come up with something more universal, that would be suitable for all the games? The obvious choice is screen pixels – they implicitly contain all of the relevant information about the game situation, except for the speed and direction of the ball. Two consecutive screens would have these covered as well.</p><p>If we apply the same preprocessing to game screens as in the DeepMind paper – take the four last screen images, resize them to 84×84 and convert to grayscale with 256 gray levels – we would have 25684x84x4 ≈ 1067970 possible game states. This means 1067970 rows in our imaginary Q-table – more than the number of atoms in the known universe! One could argue that many pixel combinations (and therefore states) never occur – we could possibly represent it as a sparse table containing only visited states. Even so, most of the states are very rarely visited and it would take a lifetime of the universe for the Q-table to converge. Ideally, we would also like to have a good guess for Q-values for states we have never seen before.</p><p>This is the point where deep learning steps in. Neural networks are exceptionally good at coming up with good features for highly structured data. We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. This approach has the advantage, that if we want to perform a Q-value update or pick the action with the highest Q-value, we only have to do one forward pass through the network and have all Q-values for all actions available immediately.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.27.12-AM.png" alt="img"></p><p>Figure 3: <em>Left: </em>Naive formulation of deep Q-network. <em>Right: </em>More optimized architecture of deep Q-network, used in DeepMind paper.</p><p>The network architecture that DeepMind used is as follows:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.28-AM.png" alt="Screen Shot 2015-12-21 at 11.23.28 AM"></p><p>This is a classical convolutional neural network with three convolutional layers, followed by two fully connected layers. People familiar with object recognition networks may notice that there are no pooling layers. But if you really think about it, pooling layers buy you translation invariance – the network becomes insensitive to the location of an object in the image. That makes perfectly sense for a classification task like ImageNet, but for games the location of the ball is crucial in determining the potential reward and we wouldn’t want to discard this information!</p><p>Input to the network are four 84×84 grayscale game screens. Outputs of the network are Q-values for each possible action (18 in Atari). Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/formula.png" alt="img"></p><p>Given a transition &lt;<em> s, a, r, s’</em> &gt;, the Q-table update rule in the previous algorithm must be replaced with the following:</p><ol><li>Do a feedforward pass for the current state <em>s</em> to get predicted Q-values for all actions.</li><li>Do a feedforward pass for the next state <em>s’ </em>and calculate maximum overall network outputs <em>max a’ Q(s’, a’).</em></li><li>Set Q-value target for action to <em>r + γmax a’ Q(s’, a’)</em> (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.</li><li>Update the weights using backpropagation.</li></ol><h1 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h1><p>By now we have an idea how to estimate the future reward in each state using Q-learning and approximate the Q-function using a convolutional neural network. But it turns out that approximation of Q-values using non-linear functions is not very stable. There is a whole bag of tricks that you have to use to actually make it converge. And it takes a long time, almost a week on a single GPU.</p><p>The most important trick is <strong>experience replay</strong>. During gameplay all the experiences &lt;<em> s, a, r, s’</em> &gt; are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.</p><h1 id="Exploration-Exploitation"><a href="#Exploration-Exploitation" class="headerlink" title="Exploration-Exploitation"></a>Exploration-Exploitation</h1><p>Q-learning attempts to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward. But we haven’t touched the exploration-exploitation dilemma yet…</p><p>Firstly observe, that when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude “exploration”. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is “greedy”, it settles with the first effective strategy it finds.</p><p>A simple and effective fix for the above problem is <strong>ε-greedy exploration</strong> – with probability <em>ε</em> choose a random action, otherwise go with the “greedy” action with the highest Q-value. In their system DeepMind actually decreases <em>ε</em> over time from 1 to 0.1 – in the beginning the system makes completely random moves to explore the state space maximally, and then it settles down to a fixed exploration rate.</p><h1 id="Deep-Q-learning-Algorithm"><a href="#Deep-Q-learning-Algorithm" class="headerlink" title="Deep Q-learning Algorithm"></a>Deep Q-learning Algorithm</h1><p>This gives us the final deep Q-learning algorithm with experience replay:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.43-AM-1.png" alt="Screen Shot 2015-12-21 at 11.23.43 AM"></p><p>There are many more tricks that DeepMind used to actually make it work – like target network, error clipping, reward clipping etc, but these are out of scope for this introduction.</p><p>The most amazing part of this algorithm is that it learns anything at all. Just think about it – because our Q-function is initialized randomly, it initially outputs complete garbage. And we are using this garbage (the maximum Q-value of the next state) as targets for the network, only occasionally folding in a tiny reward. That sounds insane, how could it learn anything meaningful at all? The fact is, that it does.</p><h1 id="Final-notes"><a href="#Final-notes" class="headerlink" title="Final notes"></a>Final notes</h1><p>Many improvements to deep Q-learning have been proposed since its first introduction – <a href="http://arxiv.org/abs/1509.06461" target="_blank" rel="external">Double Q-learning</a>, <a href="http://arxiv.org/abs/1511.05952" target="_blank" rel="external">Prioritized Experience Replay</a>, <a href="http://arxiv.org/abs/1511.06581" target="_blank" rel="external">Dueling Network Architecture</a> and <a href="http://arxiv.org/abs/1509.02971" target="_blank" rel="external">extension to continuous action space</a> to name a few. For latest advancements check out the <a href="http://rll.berkeley.edu/deeprlworkshop/" target="_blank" rel="external">NIPS 2015 deep reinforcement learning workshop</a> and <a href="https://cmt.research.microsoft.com/ICLR2016Conference/Protected/PublicComment.aspx" target="_blank" rel="external">ICLR 2016</a>(search for “reinforcement” in title). But beware, that <a href="http://www.google.com/patents/US20150100530" target="_blank" rel="external">deep Q-learning has been patented by Google</a>.</p><p>It is often said, that artificial intelligence is something we haven’t figured out yet. Once we know how it works, it doesn’t seem intelligent any more. But deep Q-networks still continue to amaze me. Watching them figure out a new game is like observing an animal in the wild – a rewarding experience by itself.</p><h1 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a>Credits</h1><p>Thanks to Ardi Tampuu, Tanel Pärnamaa, Jaan Aru, Ilya Kuzovkin, Arjun Bansal and Urs Köster for comments and suggestions on the drafts of this post.</p><h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><ul><li><a href="http://videolectures.net/rldm2015_silver_reinforcement_learning/" target="_blank" rel="external">David Silver’s lecture about deep reinforcement learning</a></li><li><a href="https://www.youtube.com/watch?v=b1a53hE0yQs" target="_blank" rel="external">Slightly awkward but accessible illustration of Q-learning</a></li><li><a href="http://rll.berkeley.edu/deeprlcourse/" target="_blank" rel="external">UC Berkley’s course on deep reinforcement learning</a></li><li><a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="external">David Silver’s reinforcement learning course</a></li><li><a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" target="_blank" rel="external">Nando de Freitas’ course on machine learning</a> (two lectures about reinforcement learning in the end)</li><li><a href="http://cs231n.github.io/" target="_blank" rel="external">Andrej Karpathy’s course on convolutional neural networks</a></li></ul><p><a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/#_ftnref1" target="_blank" rel="external">[1]</a> Algorithm adapted from <a href="http://artint.info/html/ArtInt_265.html" target="_blank" rel="external">http://artint.info/html/ArtInt_265.html</a><br>This blog was first published at: <a href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/</a></p><h4 id="This-is-the-part-1-of-my-series-on-deep-reinforcement-learning-Tune-in-next-week-for-“Deep-Reinforcement-Learning-with-Neon”-for-an-actual-implementation-with-Neon-deep-learning-toolkit"><a href="#This-is-the-part-1-of-my-series-on-deep-reinforcement-learning-Tune-in-next-week-for-“Deep-Reinforcement-Learning-with-Neon”-for-an-actual-implementation-with-Neon-deep-learning-toolkit" class="headerlink" title="This is the part 1 of my series on deep reinforcement learning. Tune in next week for “Deep Reinforcement Learning with Neon” for an actual implementation with Neon deep learning toolkit."></a>This is the part 1 of my series on deep reinforcement learning. Tune in next week for <a href="https://www.intelnervana.com/deep-reinforcement-learning-with-neon/" target="_blank" rel="external">“Deep Reinforcement Learning with Neon”</a> for an actual implementation with <a href="https://github.com/NervanaSystems/neon" target="_blank" rel="external">Neon</a> deep learning toolkit.</h4>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Two years ago, a small company in London called DeepMind uploaded their pioneering paper “&lt;a href=&quot;http://arxiv.org/abs/1312.5602&quot; target
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="deep reinforcement learning" scheme="http://yoursite.com/tags/deep-reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>On-policy Control with Approximation</title>
    <link href="http://yoursite.com/2017/07/06/On-policy-Control-with-Approximation/"/>
    <id>http://yoursite.com/2017/07/06/On-policy-Control-with-Approximation/</id>
    <published>2017-07-06T09:41:34.000Z</published>
    <updated>2017-07-06T17:06:06.685Z</updated>
    
    <content type="html"><![CDATA[<p>In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a finite-dimensional weight vector.</p><h3 id="Episodic-Semi-gradient-Control"><a href="#Episodic-Semi-gradient-Control" class="headerlink" title="Episodic Semi-gradient Control"></a>Episodic Semi-gradient Control</h3><p>The general gradient-descent update for action-value prediction is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>For example, the update for the one-step Sarsa method is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t)- \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>We call this method <strong>episode semi-gradient one-step sarsa</strong>.</p><p>To form control methods, we need to couple such action-value prediction methods with techniques for policy improvement and action selection. Suitable techniques applicable to continuous actions, or to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution. On the other hand, if the action set is discrete and not too large, then we can use the techniques already developed in previous chapters.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/episode-semi-grad-sarsa.png" alt="episode-semi-grad-sarsa"></p><h4 id="Example-Mountain-Car-Task"><a href="#Example-Mountain-Car-Task" class="headerlink" title="Example: Mountain-Car Task"></a>Example: Mountain-Car Task</h4><p>Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram in the below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mountain-car.png" alt="mountain-car"></p><p>The diﬃculty is that gravity is stronger than the car’s engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to ﬁrst move away from the goal and up the opposite slope on the left. Then, by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way. This is a simple example of a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. Many control methodologies have great diﬃculties with tasks of this kind unless explicitly aided by a human designer.</p><p>The reward in this problem is −1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (−1), and zero throttle (0). The car moves according to a simpliﬁed physics. Its position, $x_t$, and velocity, $\dot{x}_t$, are updated by<br>$$<br>\begin{align}<br>x_{t+1} &amp;\doteq bound \big[x_t + \dot{x}_{t+1} \big] \\<br>\dot{x}_{t+1} &amp;\doteq bound \big[\dot{x}_t + 0.001 A_t - 0.0025 \cos(3x_t) \big],<br>\end{align}<br>$$<br>where the bound operation enforces $-1.2 \leq x_{t+1} \leq 0.5 \;\; \text{and} \; -0.07 \leq \dot{x}_{t+1} \leq 0.07$. In addition, when $x_{t+1}$ reached the left bound, $\dot{x}_{t+1}$ was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position $x_t \in [−0.6, −0.4)$ and zero velocity. To convert the two continuous state variables to binary features, we used grid-tilings.</p><p>First of all, we define the environment of this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all possible actions</span></div><div class="line">ACTION_REVERSE = <span class="number">-1</span></div><div class="line">ACTION_ZERO = <span class="number">0</span></div><div class="line">ACTION_FORWARD = <span class="number">1</span></div><div class="line"><span class="comment"># order is important</span></div><div class="line">ACTIONS = [ACTION_REVERSE, ACTION_ZERO, ACTION_FORWARD]</div><div class="line"></div><div class="line"><span class="comment"># bound for position and velocity</span></div><div class="line">POSITION_MIN = <span class="number">-1.2</span></div><div class="line">POSITION_MAX = <span class="number">0.5</span></div><div class="line">VELOCITY_MIN = <span class="number">-0.07</span></div><div class="line">VELOCITY_MAX = <span class="number">0.07</span></div><div class="line"></div><div class="line"><span class="comment"># use optimistic initial value, so it's ok to set epsilon to 0</span></div><div class="line">EPSILON = <span class="number">0</span></div></pre></td></tr></table></figure><p>After take an action, we transition to a new state and get a reward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take an @action at @position and @velocity</span></div><div class="line"><span class="comment"># @return: new position, new velocity, reward (always -1)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(position, velocity, action)</span>:</span></div><div class="line">    newVelocity = velocity + <span class="number">0.001</span> * action - <span class="number">0.0025</span> * np.cos(<span class="number">3</span> * position)</div><div class="line">    newVelocity = min(max(VELOCITY_MIN, newVelocity), VELOCITY_MAX)</div><div class="line">    newPosition = position + newVelocity</div><div class="line">    newPosition = min(max(POSITION_MIN, newPosition), POSITION_MAX)</div><div class="line">    reward = <span class="number">-1.0</span></div><div class="line">    <span class="keyword">if</span> newPosition == POSITION_MIN:</div><div class="line">        newVelocity = <span class="number">0.0</span></div><div class="line">    <span class="keyword">return</span> newPosition, newVelocity, reward</div></pre></td></tr></table></figure><p>The $\varepsilon$-greedy policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get action at @position and @velocity based on epsilon greedy policy and @valueFunction</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(position, velocity, valueFunction)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(ACTIONS)</div><div class="line">    values = []</div><div class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">        values.append(valueFunction.value(position, velocity, action))</div><div class="line">    <span class="keyword">return</span> np.argmax(values) - <span class="number">1</span></div></pre></td></tr></table></figure><p>We need map out continuous state to discrete state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># wrapper class for state action value function</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValueFunction</span>:</span></div><div class="line">    <span class="comment"># In this example I use the tiling software instead of implementing standard tiling by myself</span></div><div class="line">    <span class="comment"># One important thing is that tiling is only a map from (state, action) to a series of indices</span></div><div class="line">    <span class="comment"># It doesn't matter whether the indices have meaning, only if this map satisfy some property</span></div><div class="line">    <span class="comment"># View the following webpage for more information</span></div><div class="line">    <span class="comment"># http://incompleteideas.net/sutton/tiles/tiles3.html</span></div><div class="line">    <span class="comment"># @maxSize: the maximum # of indices</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, stepSize, numOfTilings=<span class="number">8</span>, maxSize=<span class="number">2048</span>)</span>:</span></div><div class="line">        self.maxSize = maxSize</div><div class="line">        self.numOfTilings = numOfTilings</div><div class="line"></div><div class="line">        <span class="comment"># divide step size equally to each tiling</span></div><div class="line">        self.stepSize = stepSize / numOfTilings</div><div class="line"></div><div class="line">        self.hashTable = IHT(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># weight for each tile</span></div><div class="line">        self.weights = np.zeros(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># position and velocity needs scaling to satisfy the tile software</span></div><div class="line">        self.positionScale = self.numOfTilings / (POSITION_MAX - POSITION_MIN)</div><div class="line">        self.velocityScale = self.numOfTilings / (VELOCITY_MAX - VELOCITY_MIN)</div><div class="line"></div><div class="line">    <span class="comment"># get indices of active tiles for given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getActiveTiles</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="comment"># I think positionScale * (position - position_min) would be a good normalization.</span></div><div class="line">        <span class="comment"># However positionScale * position_min is a constant, so it's ok to ignore it.</span></div><div class="line">        activeTiles = tiles(self.hashTable, self.numOfTilings,</div><div class="line">                            [self.positionScale * position, self.velocityScale * velocity],</div><div class="line">                            [action])</div><div class="line">        <span class="keyword">return</span> activeTiles</div><div class="line"></div><div class="line">    <span class="comment"># estimate the value of given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="keyword">if</span> position == POSITION_MAX:</div><div class="line">            <span class="keyword">return</span> <span class="number">0.0</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        <span class="keyword">return</span> np.sum(self.weights[activeTiles])</div><div class="line"></div><div class="line">    <span class="comment"># learn with given state, action and target</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, position, velocity, action, target)</span>:</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        estimation = np.sum(self.weights[activeTiles])</div><div class="line">        delta = self.stepSize * (target - estimation)</div><div class="line">        <span class="keyword">for</span> activeTile <span class="keyword">in</span> activeTiles:</div><div class="line">            self.weights[activeTile] += delta</div><div class="line"></div><div class="line">    <span class="comment"># get # of steps to reach the goal under current state value function</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">costToGo</span><span class="params">(self, position, velocity)</span>:</span></div><div class="line">        costs = []</div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">            costs.append(self.value(position, velocity, action))</div><div class="line">        <span class="keyword">return</span> -np.max(costs)</div></pre></td></tr></table></figure><p>Because the one-step semi-gradient sarsa is a special case of n-step, so we develop the n-step algorithm<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \; n \geq 1,0 \leq t \leq T-n.<br>$$<br>The n-step equation is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ G_{t:t+n}- \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}), \;\;\; 0 \leq t \leq T.<br>$$<br>Complete pseudocode is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/n-step-sg-sarsa.png" alt="n-step-sg-sarsa"></p><p>So the code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># semi-gradient n-step Sarsa</span></div><div class="line"><span class="comment"># @valueFunction: state value function to learn</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">semiGradientNStepSarsa</span><span class="params">(valueFunction, n=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="comment"># start at a random position around the bottom of the valley</span></div><div class="line">    currentPosition = np.random.uniform(<span class="number">-0.6</span>, <span class="number">-0.4</span>)</div><div class="line">    <span class="comment"># initial velocity is 0</span></div><div class="line">    currentVelocity = <span class="number">0.0</span></div><div class="line">    <span class="comment"># get initial action</span></div><div class="line">    currentAction = getAction(currentPosition, currentVelocity, valueFunction)</div><div class="line"></div><div class="line">    <span class="comment"># track previous position, velocity, action and reward</span></div><div class="line">    positions = [currentPosition]</div><div class="line">    velocities = [currentVelocity]</div><div class="line">    actions = [currentAction]</div><div class="line">    rewards = [<span class="number">0.0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># take current action and go to the new state</span></div><div class="line">            newPostion, newVelocity, reward = takeAction(currentPosition, currentVelocity, currentAction)</div><div class="line">            <span class="comment"># choose new action</span></div><div class="line">            newAction = getAction(newPostion, newVelocity, valueFunction)</div><div class="line"></div><div class="line">            <span class="comment"># track new state and action</span></div><div class="line">            positions.append(newPostion)</div><div class="line">            velocities.append(newVelocity)</div><div class="line">            actions.append(newAction)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newPostion == POSITION_MAX:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += rewards[t]</div><div class="line">            <span class="comment"># add estimated state action value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += valueFunction.value(positions[updateTime + n],</div><div class="line">                                               velocities[updateTime + n],</div><div class="line">                                               actions[updateTime + n])</div><div class="line">            <span class="comment"># update the state value function</span></div><div class="line">            <span class="keyword">if</span> positions[updateTime] != POSITION_MAX:</div><div class="line">                valueFunction.learn(positions[updateTime], velocities[updateTime], actions[updateTime], returns)</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentPosition = newPostion</div><div class="line">        currentVelocity = newVelocity</div><div class="line">        currentAction = newAction</div><div class="line"></div><div class="line">    <span class="keyword">return</span> time</div></pre></td></tr></table></figure><p>Next, we use the method mentioned earlier to solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">episodes = <span class="number">9000</span></div><div class="line">targetEpisodes = [<span class="number">1</span><span class="number">-1</span>, <span class="number">12</span><span class="number">-1</span>, <span class="number">104</span><span class="number">-1</span>, <span class="number">1000</span><span class="number">-1</span>, episodes - <span class="number">1</span>]</div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alpha = <span class="number">0.3</span></div><div class="line">valueFunction = ValueFunction(alpha, numOfTilings)</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">    print(<span class="string">'episode:'</span>, episode)</div><div class="line">    semiGradientNStepSarsa(valueFunction)</div><div class="line">    <span class="keyword">if</span> episode <span class="keyword">in</span> targetEpisodes:</div><div class="line">        prettyPrint(valueFunction, <span class="string">'Episode: '</span> + str(episode + <span class="number">1</span>))</div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa.png" alt="mcar-sg-sarsa"></p><p>The result shows what typically happens while learning to solve this task with this form of function approximation. Shown is the negative of the value function (the cost-to-go function) learned on a single run. The initial action values were all zero, which was optimistic (all true values are negative in this task), causing extensive exploration to occur even though the exploration parameter, $\varepsilon$, was 0. All the states visited frequently are valued worse than unexplored states, because the actual rewards have been worse than what was (unrealistically) expected. This continually drives the agent away from wherever it has been, to explore new states, until a solution is found.</p><p>Next, let us test the performance of various step size (learning rate).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'alpha:'</span>, alphas[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'alpha = '</span>+str(alphas[i])+<span class="string">'/'</span>+str(numOfTilings))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha.png" alt="mcar-sg-sarsa-var-alpha"></p><p>And then, let us test the performance of one-step sarsa and multi-step sarsa on the Mountain Car task:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.5</span>, <span class="number">0.3</span>]</div><div class="line">nSteps = [<span class="number">1</span>, <span class="number">8</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nSteps[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index], nSteps[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-n.png" alt="mcar-sg-sarsa-var-n"></p><p>Finally, let me shows the results of a more detailed study of the eﬀect of the parameters $\alpha$ and $n$ on the rate of learning on this task.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">alphas = np.arange(<span class="number">0.25</span>, <span class="number">1.75</span>, <span class="number">0.25</span>)</div><div class="line">nSteps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">5</span>))</div><div class="line">episodes = <span class="number">50</span></div><div class="line">runs = <span class="number">5</span></div><div class="line"></div><div class="line">truncateStep = <span class="number">300</span></div><div class="line">steps = np.zeros((len(nSteps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> nStepIndex, nStep <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(nSteps)), nSteps):</div><div class="line">        <span class="keyword">for</span> alphaIndex, alpha <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(alphas)), alphas):</div><div class="line">            <span class="keyword">if</span> (nStep == <span class="number">8</span> <span class="keyword">and</span> alpha &gt; <span class="number">1</span>) <span class="keyword">or</span> \</div><div class="line">                    (nStep == <span class="number">16</span> <span class="keyword">and</span> alpha &gt; <span class="number">0.75</span>):</div><div class="line">                <span class="comment"># In these cases it won't converge, so ignore them</span></div><div class="line">                steps[nStepIndex, alphaIndex] += truncateStep * episodes</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            valueFunction = ValueFunction(alpha)</div><div class="line">            <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nStep, <span class="string">'alpha:'</span>, alpha, <span class="string">'episode:'</span>, episode)</div><div class="line">                step = semiGradientNStepSarsa(valueFunction, nStep)</div><div class="line">                steps[nStepIndex, alphaIndex] += step</div><div class="line"><span class="comment"># average over independent runs and episodes</span></div><div class="line">steps /= runs * episodes</div><div class="line"><span class="comment"># truncate high values for better display</span></div><div class="line">steps[steps &gt; truncateStep] = truncateStep</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(nSteps)):</div><div class="line">    plt.plot(alphas, steps[i, :], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha * number of tilings(8)'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha-n.png" alt="mcar-sg-sarsa-var-alpha-n"></p><h3 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h3><h4 id="Use-OpenAI-gym"><a href="#Use-OpenAI-gym" class="headerlink" title="Use OpenAI gym"></a>Use OpenAI gym</h4><p>Now, let us use the OpenAI gym toolkit to simply our Mountain Car task. As we know, we need to define the environment of task before develop the detail algorithm. It’s easy to make mistakes with some detail. So it is fantastic if we do not need to care about that. The gym toolkit help us to define the environment. Such as the Mountain Car task, there is a build-in model in the gym toolkit. We just need to write one row code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env = gym.envs.make(<span class="string">"MountainCar-v0"</span>)</div></pre></td></tr></table></figure><p>That is amazing!</p><p>We also can test the environment very convenience and get a pretty good user graphic:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">env.reset()</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line"><span class="comment"># for x in range(10000):</span></div><div class="line"><span class="comment">#     env.step(0)</span></div><div class="line"><span class="comment">#     plt.figure()</span></div><div class="line"><span class="comment">#     plt.imshow(env.render(mode='rgb_array'))  </span></div><div class="line">[env.step(<span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)]</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">env.render(close=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>These codes will return the result as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-gym-test.png" alt="mcar-gym-test"></p><p>Bravo~</p><p>Now, let us solve the task by to use the Q-Learning algorithm. Meanwhile, we will use the RBF kernel to construct the features and use the SGDRegressor gradient-descent method of scikit-learn package to update $\mathbf{w}$.</p><p>First of all, we need to normalized our features to accelerate the convergence speed and use the RBF kernel to reconstruct our feature space. The both methods (Normalization and Reconstruction) need to use some training set to train a model. Then we use this model to transform our raw data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Feature Preprocessing: Normalize to zero mean and unit variance</span></div><div class="line"><span class="comment"># We use a few samples from the observation space to do this</span></div><div class="line">observation_examples = np.array([env.observation_space.sample() <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)])</div><div class="line">scaler = sklearn.preprocessing.StandardScaler()</div><div class="line">scaler.fit(observation_examples)</div><div class="line"></div><div class="line"><span class="comment"># Used to converte a state to a featurizes represenation.</span></div><div class="line"><span class="comment"># We use RBF kernels with different variances to cover different parts of the space</span></div><div class="line">featurizer = sklearn.pipeline.FeatureUnion([</div><div class="line">        (<span class="string">"rbf1"</span>, RBFSampler(gamma=<span class="number">5.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf2"</span>, RBFSampler(gamma=<span class="number">2.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf3"</span>, RBFSampler(gamma=<span class="number">1.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf4"</span>, RBFSampler(gamma=<span class="number">0.5</span>, n_components=<span class="number">100</span>))</div><div class="line">        ])</div><div class="line">featurizer.fit(scaler.transform(observation_examples))</div></pre></td></tr></table></figure><p>Next, we define a class named Estimator to simply the gradient descent process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Estimator</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Value Function approximator. </div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># We create a separate model for each action in the environment's</span></div><div class="line">        <span class="comment"># action space. Alternatively we could somehow encode the action</span></div><div class="line">        <span class="comment"># into the features, but this way it's easier to code up.</span></div><div class="line">        self.models = []</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(env.action_space.n):</div><div class="line">            model = SGDRegressor(learning_rate=<span class="string">"constant"</span>)</div><div class="line">            <span class="comment"># We need to call partial_fit once to initialize the model</span></div><div class="line">            <span class="comment"># or we get a NotFittedError when trying to make a prediction</span></div><div class="line">            <span class="comment"># This is quite hacky.</span></div><div class="line">            model.partial_fit([self.featurize_state(env.reset())], [<span class="number">0</span>])</div><div class="line">            self.models.append(model)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">featurize_state</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Returns the featurized representation for a state.</div><div class="line">        """</div><div class="line">        scaled = scaler.transform([state])</div><div class="line">        featurized = featurizer.transform(scaled)</div><div class="line">        <span class="keyword">return</span> featurized[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, s, a=None)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Makes value function predictions.</div><div class="line">        </div><div class="line">        Args:</div><div class="line">            s: state to make a prediction for</div><div class="line">            a: (Optional) action to make a prediction for</div><div class="line">            </div><div class="line">        Returns</div><div class="line">            If an action a is given this returns a single number as the prediction.</div><div class="line">            If no action is given this returns a vector or predictions for all actions</div><div class="line">            in the environment where pred[i] is the prediction for action i.</div><div class="line">            </div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> a:</div><div class="line">            <span class="keyword">return</span> np.array([m.predict([features])[<span class="number">0</span>] <span class="keyword">for</span> m <span class="keyword">in</span> self.models])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self.models[a].predict([features])[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, s, a, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Updates the estimator parameters for a given state and action towards</div><div class="line">        the target y.</div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        self.models[a].partial_fit([features], [y])</div></pre></td></tr></table></figure><p>We also need a $\varepsilon$-greedy policy to select action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(estimator, epsilon, nA)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        estimator: An estimator that returns q values for a given state</div><div class="line">        epsilon: The probability to select a random action . float between 0 and 1.</div><div class="line">        nA: Number of actions in the environment.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A function that takes the observation as an argument and returns</div><div class="line">        the probabilities for each action in the form of a numpy array of length nA.</div><div class="line">    </div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></div><div class="line">        A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">        q_values = estimator.predict(observation)</div><div class="line">        best_action = np.argmax(q_values)</div><div class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">        <span class="keyword">return</span> A</div><div class="line">    <span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Then we develop the Q-Learning method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_learning</span><span class="params">(env, estimator, num_episodes, discount_factor=<span class="number">1.0</span>, epsilon=<span class="number">0.1</span>, epsilon_decay=<span class="number">1.0</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm for fff-policy TD control using Function Approximation.</div><div class="line">    Finds the optimal greedy policy while following an epsilon-greedy policy.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        estimator: Action-Value function estimator</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">        epsilon_decay: Each episode, epsilon is decayed by this factor</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))    </div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        </div><div class="line">        <span class="comment"># The policy we're following</span></div><div class="line">        policy = make_epsilon_greedy_policy(</div><div class="line">            estimator, epsilon * epsilon_decay**i_episode, env.action_space.n)</div><div class="line">        </div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="comment"># Also print reward for last episode</span></div><div class="line">        last_reward = stats.episode_rewards[i_episode - <span class="number">1</span>]</div><div class="line">        sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Reset the environment and pick the first action</span></div><div class="line">        state = env.reset()</div><div class="line">        </div><div class="line">        <span class="comment"># Only used for SARSA, not Q-Learning</span></div><div class="line">        next_action = <span class="keyword">None</span></div><div class="line">        </div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">                        </div><div class="line">            <span class="comment"># Choose an action to take</span></div><div class="line">            <span class="comment"># If we're using SARSA we already decided in the previous step</span></div><div class="line">            <span class="keyword">if</span> next_action <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                action_probs = policy(state)</div><div class="line">                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                action = next_action</div><div class="line">            </div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            next_state, reward, done, _ = env.step(action)</div><div class="line">    </div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            <span class="comment"># TD Update</span></div><div class="line">            q_values_next = estimator.predict(next_state)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for Q-Learning</span></div><div class="line">            <span class="comment"># Q-Value TD Target</span></div><div class="line">            td_target = reward + discount_factor * np.max(q_values_next)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for SARSA TD Target for on policy-training:</span></div><div class="line">            <span class="comment"># next_action_probs = policy(next_state)</span></div><div class="line">            <span class="comment"># next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)             </span></div><div class="line">            <span class="comment"># td_target = reward + discount_factor * q_values_next[next_action]</span></div><div class="line">            </div><div class="line">            <span class="comment"># Update the function approximator using our target</span></div><div class="line">            estimator.update(state, action, td_target)</div><div class="line">            </div><div class="line">            print(<span class="string">"\rStep &#123;&#125; @ Episode &#123;&#125;/&#123;&#125; (&#123;&#125;)"</span>.format(t, i_episode + <span class="number">1</span>, num_episodes, last_reward), end=<span class="string">""</span>)</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">                </div><div class="line">            state = next_state</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> stats</div></pre></td></tr></table></figure><p>Run this method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">estimator = Estimator()</div><div class="line"><span class="comment"># Note: For the Mountain Car we don't actually need an epsilon &gt; 0.0</span></div><div class="line"><span class="comment"># because our initial estimate for all states is too "optimistic" which leads</span></div><div class="line"><span class="comment"># to the exploration of all states.</span></div><div class="line">stats = q_learning(env, estimator, <span class="number">100</span>, epsilon=<span class="number">0.0</span>)</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-ql-gym.png" alt="mcar-ql-gym"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>On-policy Prediction with Approximation</title>
    <link href="http://yoursite.com/2017/07/05/On-policy-Prediction-with-Approximation/"/>
    <id>http://yoursite.com/2017/07/05/On-policy-Prediction-with-Approximation/</id>
    <published>2017-07-05T07:29:22.000Z</published>
    <updated>2017-07-06T09:39:43.665Z</updated>
    
    <content type="html"><![CDATA[<p>The novelty in this post is that the approximate value function is represented not as a table but as a parameterized functional form with weight vector $\mathbf{w} \in \mathbb{R}^d$. We will write $\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)$ for the approximated value of state $s$ given weight vector $\mathbf{w}$. For example, $\hat{v}$ might be a linear function in features of the state, with $\mathbf{w}$ the vector of feature weights. Consequently, when a single state is updated, the change generalizes from that state to affect the values of many other states.</p><h3 id="The-prediction-Objective-MSVE"><a href="#The-prediction-Objective-MSVE" class="headerlink" title="The prediction Objective (MSVE)"></a>The prediction Objective (MSVE)</h3><p>In the tabular case a continuous measure of prediction quality was not necessary because the learned value function could come to equal the true value function exactly. But with genuine approximation, an update at one state aﬀects many others, and it is not possible to get all states exactly correct. By assumption we have far more states than weights, so making one state’s estimate more accurate invariably means making others’ less accurate.</p><p>By the error in a state $s$ we mean the square of the diﬀerence between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_{\pi}(s)$. Weighting this over the state space by the distribution $\mu$, we obtain a natural objective function, the <strong>Mean Squared Value Error</strong>, or <strong>MSVE</strong>:<br>$$<br>\text{MSVE}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s) \Big[v_{\pi}(s) - \hat{v}(s, \mathbf{w}) \Big]^2.<br>$$<br>The square root of this measure, the root MSVE or RMSVE, gives a rough measure of how much the approximate values diﬀer from the true values and is often used in plots. Typically one chooses $\mu(s)$ to be the fraction of time spent in $s$ under the target policy $\pi$. This is called the <em>on-policy distribution</em>.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/on_policy_dist.png" alt="on-policy-distribution"></p><h3 id="Stochastic-gradient-Methods"><a href="#Stochastic-gradient-Methods" class="headerlink" title="Stochastic-gradient Methods"></a>Stochastic-gradient Methods</h3><p>We assume that states appear in examples with the same distribution, µ, over which we are trying to minimize the MSVE. A good strategy in this case is to try to minimize error on the observed examples. Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example by a small amount in the direction that would most reduce the error on that example:<br>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t - \frac{1}{2} \alpha \nabla \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big]^2 \\<br>&amp;= \mathbf{w}_t + \alpha \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).<br>\end{align}<br>$$<br>And<br>$$<br>\nabla f(\mathbf{w}) \doteq \Big( \frac{\partial f(\mathbf{w})}{\partial w_1}, \frac{\partial f(\mathbf{w})}{\partial w_2}, \cdots, \frac{\partial f(\mathbf{w})}{\partial w_d}\Big)^{\top}.<br>$$<br>Although $v_{\pi}(S_t)$ is unknown, but we can approximate it by substituting $U_t$ (the $t$th training example) in place of $v_{\pi}(S_t)$. This yields the following general SGD method for state-value prediction:<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).<br>$$<br>If $U_t$ is an <strong>unbiased</strong> estimate, that is, if $\mathbb{E}[U_t]=v_{\pi}(S_t)$, for each $t$, then $\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic approximation conditions for decreasing $\alpha$.</p><p>For example, suppose the states in the examples are the states generated by interaction (or simulated interaction) with the environment using policy $\pi$. Because the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t \doteq G_t$ is by deﬁnition an unbiased estimate of $v_{\pi}(S_t)$. Thus, the gradient-descent version of Monte Carlo state-value prediction is guaranteed to ﬁnd a locally optimal solution. Pseudocode for a complete algorithm<br>is shown in the box.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/gradient_mc.png" alt="gradient_mc"></p><h4 id="Example-State-Aggregation-on-the-1000-state-Random-Walk"><a href="#Example-State-Aggregation-on-the-1000-state-Random-Walk" class="headerlink" title="Example: State Aggregation on the 1000-state Random Walk"></a>Example: State Aggregation on the 1000-state Random Walk</h4><p>State aggregation is a simple form of generalizing function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group. The value of a state is estimated as its group’s component, and when the state is updated, that component alone is updated. State aggregation is a special case of SGD in which the gradient, $\nabla \hat{v}(S_t, \mathbf{w}_t)$, is <strong>1</strong> for $S_t$’s group’s component and <strong>0</strong> for the other components.</p><p>Consider a 1000-state version of the random walk task. The states are numbered from 1 to 1000, left to right, and all episodes begin near the center, in state 500. State transitions are from the current state to one of the 100 neighboring states to its left, or to one of the 100 neighboring states to its right, all with equal probability. Of course, if the current state is near an edge, then there may be fewer than 100 neighbors on that side of it. In this case, all the probability that would have gone into those missing neighbors goes into the probability of terminating on that side (thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right). As usual, termination on the left produces a reward of −1, and termination on the right produces a reward of +1. All other transitions have a reward of zero.</p><p>Now, let us solve this problem by gradient Monte Carlo algorithm. First of all, we define the environment of this problem.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># # of states except for terminal states</span></div><div class="line">N_STATES = <span class="number">1000</span></div><div class="line"></div><div class="line"><span class="comment"># true state values, just a promising guess</span></div><div class="line">trueStateValues = np.arange(<span class="number">-1001</span>, <span class="number">1003</span>, <span class="number">2</span>) / <span class="number">1001.0</span></div><div class="line"></div><div class="line"><span class="comment"># all states</span></div><div class="line">states = np.arange(<span class="number">1</span>, N_STATES + <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># start from a central state</span></div><div class="line">START_STATE = <span class="number">500</span></div><div class="line"></div><div class="line"><span class="comment"># terminal states</span></div><div class="line">END_STATES = [<span class="number">0</span>, N_STATES + <span class="number">1</span>]</div><div class="line"></div><div class="line"><span class="comment"># possible actions</span></div><div class="line">ACTION_LEFT = <span class="number">-1</span></div><div class="line">ACTION_RIGHT = <span class="number">1</span></div><div class="line">ACTIONS = [ACTION_LEFT, ACTION_RIGHT]</div><div class="line"></div><div class="line"><span class="comment"># maximum stride for an action</span></div><div class="line">STEP_RANGE = <span class="number">100</span></div></pre></td></tr></table></figure><p>We need a true value of each state, thus use the dynamic programming to get these value:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Dynamic programming to find the true state values, based on the promising guess above</span></div><div class="line"><span class="comment"># Assume all rewards are 0, given that we have already given value -1 and 1 to terminal states</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    oldTrueStateValues = np.copy(trueStateValues)</div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states:</div><div class="line">        trueStateValues[state] = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, STEP_RANGE + <span class="number">1</span>):</div><div class="line">                step *= action</div><div class="line">                newState = state + step</div><div class="line">                newState = max(min(newState, N_STATES + <span class="number">1</span>), <span class="number">0</span>)</div><div class="line">                <span class="comment"># asynchronous update for faster convergence</span></div><div class="line">                trueStateValues[state] += <span class="number">1.0</span> / (<span class="number">2</span> * STEP_RANGE) * trueStateValues[newState]</div><div class="line">    error = np.sum(np.abs(oldTrueStateValues - trueStateValues))</div><div class="line">    print(error)</div><div class="line">    <span class="keyword">if</span> error &lt; <span class="number">1e-2</span>:</div><div class="line">        <span class="keyword">break</span></div><div class="line"><span class="comment"># correct the state value for terminal states to 0</span></div><div class="line">trueStateValues[<span class="number">0</span>] = trueStateValues[<span class="number">-1</span>] = <span class="number">0</span></div></pre></td></tr></table></figure><p>The policy of episodes generation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take an @action at @state, return new state and reward for this transition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(state, action)</span>:</span></div><div class="line">    step = np.random.randint(<span class="number">1</span>, STEP_RANGE + <span class="number">1</span>)</div><div class="line">    step *= action</div><div class="line">    state += step</div><div class="line">    state = max(min(state, N_STATES + <span class="number">1</span>), <span class="number">0</span>)</div><div class="line">    <span class="keyword">if</span> state == <span class="number">0</span>:</div><div class="line">        reward = <span class="number">-1</span></div><div class="line">    <span class="keyword">elif</span> state == N_STATES + <span class="number">1</span>:</div><div class="line">        reward = <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        reward = <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> state, reward</div></pre></td></tr></table></figure><p>The reward after take an action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get an action, following random policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span></div></pre></td></tr></table></figure><p>And we have a special value function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># a wrapper class for aggregation value function</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValueFunction</span>:</span></div><div class="line">    <span class="comment"># @numOfGroups: # of aggregations</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, numOfGroups)</span>:</span></div><div class="line">        self.numOfGroups = numOfGroups</div><div class="line">        self.groupSize = N_STATES // numOfGroups</div><div class="line"></div><div class="line">        <span class="comment"># thetas</span></div><div class="line">        self.params = np.zeros(numOfGroups)</div><div class="line"></div><div class="line">    <span class="comment"># get the value of @state</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="keyword">if</span> state <span class="keyword">in</span> END_STATES:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        groupIndex = (state - <span class="number">1</span>) // self.groupSize</div><div class="line">        <span class="keyword">return</span> self.params[groupIndex]</div><div class="line"></div><div class="line">    <span class="comment"># update parameters</span></div><div class="line">    <span class="comment"># @delta: step size * (target - old estimation)</span></div><div class="line">    <span class="comment"># @state: state of current sample</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, delta, state)</span>:</span></div><div class="line">        groupIndex = (state - <span class="number">1</span>) // self.groupSize</div><div class="line">        self.params[groupIndex] += delta</div></pre></td></tr></table></figure><p>And the gradient MC algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># gradient Monte Carlo algorithm</span></div><div class="line"><span class="comment"># @valueFunction: an instance of class ValueFunction</span></div><div class="line"><span class="comment"># @alpha: step size</span></div><div class="line"><span class="comment"># @distribution: array to store the distribution statistics</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientMonteCarlo</span><span class="params">(valueFunction, alpha, distribution=None)</span>:</span></div><div class="line">    currentState = START_STATE</div><div class="line">    trajectory = [currentState]</div><div class="line"></div><div class="line">    <span class="comment"># We assume gamma = 1, so return is just the same as the latest reward</span></div><div class="line">    reward = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState <span class="keyword">not</span> <span class="keyword">in</span> END_STATES:</div><div class="line">        action = getAction()</div><div class="line">        newState, reward = takeAction(currentState, action)</div><div class="line">        trajectory.append(newState)</div><div class="line">        currentState = newState</div><div class="line"></div><div class="line">    <span class="comment"># Gradient update for each state in this trajectory</span></div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> trajectory[:<span class="number">-1</span>]:</div><div class="line">        delta = alpha * (reward - valueFunction.value(state))</div><div class="line">        valueFunction.update(delta, state)</div><div class="line">        <span class="keyword">if</span> distribution <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            distribution[state] += <span class="number">1</span></div></pre></td></tr></table></figure><p>Finally. let us solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">nEpisodes = int(<span class="number">1e5</span>)</div><div class="line">alpha = <span class="number">2e-5</span></div><div class="line"></div><div class="line"><span class="comment"># we have 10 aggregations in this example, each has 100 states</span></div><div class="line">valueFunction = ValueFunction(<span class="number">10</span>)</div><div class="line">distribution = np.zeros(N_STATES + <span class="number">2</span>)</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">    print(<span class="string">'episode:'</span>, episode)</div><div class="line">    gradientMonteCarlo(valueFunction, alpha, distribution)</div><div class="line"></div><div class="line">distribution /= np.sum(distribution)</div><div class="line">stateValues = [valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states]</div><div class="line"></div><div class="line">plt.figure(<span class="number">0</span>)</div><div class="line">plt.plot(states, stateValues, label=<span class="string">'Approximate MC value'</span>)</div><div class="line">plt.plot(states, trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'True value'</span>)</div><div class="line">plt.xlabel(<span class="string">'State'</span>)</div><div class="line">plt.ylabel(<span class="string">'Value'</span>)</div><div class="line">plt.legend()</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>)</div><div class="line">plt.plot(states, distribution[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'State distribution'</span>)</div><div class="line">plt.xlabel(<span class="string">'State'</span>)</div><div class="line">plt.ylabel(<span class="string">'Distribution'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_1_1.png" alt="distribution"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_1_2.png" alt="state_value"></p><h3 id="Semi-gradient-Methods"><a href="#Semi-gradient-Methods" class="headerlink" title="Semi-gradient Methods"></a>Semi-gradient Methods</h3><p>Bootstrapping target such as n-step returns $G_{t:t+n}$ or the DP target $\sum_{a, s^{\prime}, r} \pi(a|S_t)p(s^{\prime}, r|S_t, a)[r + \gamma \hat{v}(s^{\prime}, \mathbf{w}_t)]$ all depend on the current value of the weight vector $\mathbf{w}_t$, which implies that they will be biased and that they will not produce a true gradient=descent method. We call them <em>semi-gradient methods</em>.</p><p>Although semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear case. Moreover, they oﬀer important advantages which makes them often clearly preferred. One reason for this is that they are typically signiﬁcantly faster to learn. Another is that they enable learning to be continual and online, without waiting for the end of an episode. This enables them to be used on continuing problems and provides computational advantages. A prototypical semi-gradient method is semi-gradient TD(0), which uses $U_t \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w})$ as its target. Complete pseudocode for this method is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/semi_grad_td.png" alt="semi_grad_td"></p><h3 id="Linear-Methods"><a href="#Linear-Methods" class="headerlink" title="Linear Methods"></a>Linear Methods</h3><p>One of the most important special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot, \mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$. Corresponding to every state $s$, there is a real-valued vector of features $\mathbf{x}(s) \doteq (x_1(s),x_2(s),\cdots,x_d(s))^{\top}$, with the same number of components as $\mathbf{w}$. However the features are constructed, the approximate state-value function is given by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:<br>$$<br>\hat{v}(s, \mathbf{w}) \doteq \mathbf{w^{\top}x}(s) \doteq \sum_{i=1}^d w_i x_i(s).<br>$$<br>The individual functions $x_i:\mathcal{S} \rightarrow \mathbb{E}$ are called <strong>basis functions</strong>. The gradient of the approximate value function with respect to $\mathbf{w}$ in this case is<br>$$<br>\nabla \hat{v} (s, \mathbf{w}) = \mathbf{x}(s).<br>$$<br>The update at each time $t$ is</p><p>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t + \alpha \Big( R_{t+1} + \gamma \mathbf{w}_t^{\top}\mathbf{x}_{t+1} - \mathbf{w}_t^{\top}\mathbf{x}_{t}\Big)\mathbf{x}_t \\<br>&amp;= \mathbf{w}_t + \alpha \Big( R_{t+1}\mathbf{x}_t - \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \mathbf{w}_t\Big),<br>\end{align}<br>$$<br>where here we have used the notational shorthand $\mathbf{x}_t = \mathbf{x}(S_t)$. If the system converges, it must converge to the weight vector $\mathbf{w}_{TD}$ at which<br>$$<br>\mathbf{w}_{TD} \doteq \mathbf{A^{-1}b},<br>$$<br>where<br>$$<br>\mathbf{b} \doteq \mathbb{E}[R_{t+1}\mathbf{x}_t] \in \mathbb{R}^d \;\; \text{and} \;\; \mathbf{A} \doteq \mathbb{E}\big[ \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \big] \in \mathbb{R}^d \times \mathbb{R}^d.<br>$$<br>This quantity is called the TD <strong>fixedpoint</strong>. At this point we have:<br>$$<br>\text{MSVE}(\mathbf{w}_{TD}) \leq \frac{1}{1 - \gamma} \min_{\mathbf{w}} \text{MSVE}(\mathbf{w}).<br>$$<br>Now we use the state aggregation example again, but use the semi-gradient TD method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># semi-gradient n-step TD algorithm</span></div><div class="line"><span class="comment"># @valueFunction: an instance of class ValueFunction</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="comment"># @alpha: step size</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">semiGradientTemporalDifference</span><span class="params">(valueFunction, n, alpha)</span>:</span></div><div class="line">    <span class="comment"># initial starting state</span></div><div class="line">    currentState = START_STATE</div><div class="line"></div><div class="line">    <span class="comment"># arrays to store states and rewards for an episode</span></div><div class="line">    <span class="comment"># space isn't a major consideration, so I didn't use the mod trick</span></div><div class="line">    states = [currentState]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># choose an action randomly</span></div><div class="line">            action = getAction()</div><div class="line">            newState, reward = takeAction(currentState, action)</div><div class="line"></div><div class="line">            <span class="comment"># store new state and new reward</span></div><div class="line">            states.append(newState)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newState <span class="keyword">in</span> END_STATES:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += rewards[t]</div><div class="line">            <span class="comment"># add state value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += valueFunction.value(states[updateTime + n])</div><div class="line">            stateToUpdate = states[updateTime]</div><div class="line">            <span class="comment"># update the value function</span></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> stateToUpdate <span class="keyword">in</span> END_STATES:</div><div class="line">                delta = alpha * (returns - valueFunction.value(stateToUpdate))</div><div class="line">                valueFunction.update(delta, stateToUpdate)</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentState = newState</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">nEpisodes = int(<span class="number">1e5</span>)</div><div class="line">    alpha = <span class="number">2e-4</span></div><div class="line">    valueFunction = ValueFunction(<span class="number">10</span>)</div><div class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        print(<span class="string">'episode:'</span>, episode)</div><div class="line">        semiGradientTemporalDifference(valueFunction, <span class="number">1</span>, alpha)</div><div class="line"></div><div class="line">    stateValues = [valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states]</div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    plt.plot(states, stateValues, label=<span class="string">'Approximate TD value'</span>)</div><div class="line">    plt.plot(states, trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'True value'</span>)</div><div class="line">    plt.xlabel(<span class="string">'State'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Value'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_2_1.png" alt="semi_gradient_td"></p><p>We also could use the <a href="https://ewanlee.github.io/2017/07/04/n-step-TD/" target="_blank" rel="external">n-step semi-gradient TD method</a>. To obtain such quantitatively similar results we switched the state aggregation to 20 groups of 50 states each. The 20 groups are then quantitatively close to the 19 states of the tabular problem.</p><p>The semi-gradient n-step TD algorithm we used in this example is the natural extension of the tabular n-step TD algorithm. The key equation is<br>$$<br>\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \Big[ G_{t:t+n} - \hat{v}(S_t, \mathbf{w}_{t+n-1})\Big] \nabla \hat{v}(S_t, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T,<br>$$<br>where<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{v}(S_{t+n}, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T-n.<br>$$<br>Pseudocode for the complete algorithm is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/n_step_semi_gradient_td.png" alt="n_step_semi_gradient_td"></p><p>Now let us show the performance of different value of n:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># truncate value for better display</span></div><div class="line">truncateValue = <span class="number">0.55</span></div><div class="line"></div><div class="line"><span class="comment"># all possible steps</span></div><div class="line">steps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># all possible alphas</span></div><div class="line">alphas = np.arange(<span class="number">0</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="comment"># each run has 10 episodes</span></div><div class="line">episodes = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># perform 100 independent runs</span></div><div class="line">runs = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># track the errors for each (step, alpha) combination</span></div><div class="line">errors = np.zeros((len(steps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> stepInd, step <span class="keyword">in</span> zip(range(len(steps)), steps):</div><div class="line">        <span class="keyword">for</span> alphaInd, alpha <span class="keyword">in</span> zip(range(len(alphas)), alphas):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'step:'</span>, step, <span class="string">'alpha:'</span>, alpha)</div><div class="line">            <span class="comment"># we have 20 aggregations in this example</span></div><div class="line">            valueFunction = ValueFunction(<span class="number">20</span>)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                semiGradientTemporalDifference(valueFunction, step, alpha)</div><div class="line">                <span class="comment"># calculate the RMS error</span></div><div class="line">                currentStateValues = np.asarray([valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states])</div><div class="line">                errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], <span class="number">2</span>)) / N_STATES)</div><div class="line"><span class="comment"># take average</span></div><div class="line">errors /= episodes * runs</div><div class="line"><span class="comment"># truncate the error</span></div><div class="line">errors[errors &gt; truncateValue] = truncateValue</div><div class="line">plt.figure(<span class="number">3</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(steps)):</div><div class="line">    plt.plot(alphas, errors[i, :], label=<span class="string">'n = '</span> + str(steps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMS error'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_2_2.png" alt="n_step_semi_gradient_td_compare"></p><h3 id="Feature-Construction-for-Linear-Methods"><a href="#Feature-Construction-for-Linear-Methods" class="headerlink" title="Feature Construction for Linear Methods"></a>Feature Construction for Linear Methods</h3><p>Choosing features appropriate to the task is an important way of adding prior domain knowledge to reinforcement learning systems. Intuitively, the features should correspond to the natural features of the task, those along which generalization is most appropriate. If we are valuing geometric objects, for example, we might want to have features for each possible shape, color, size, or function. If we are valuing states of a mobile robot, then we might want to have features for locations, degrees of remaining battery power, recent sonar readings, and so on.</p><h4 id="Polynomials"><a href="#Polynomials" class="headerlink" title="Polynomials"></a>Polynomials</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/poly_feat.png" alt="poly"></p><h4 id="Fourier-Basis"><a href="#Fourier-Basis" class="headerlink" title="Fourier Basis"></a>Fourier Basis</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/fourier.png" alt="fourier"></p><p>Konidaris et al. (2011) found that when using Fourier cosine basis functions with a learning algorithm such as semi-gradient TD(0), or semi-gradient Sarsa, it is helpful to use a diﬀerent step-size parameter for each basis function. If $\alpha$ is the basic step-size parameter, they suggest setting the step-size parameter for basis function $x_i$ to $a_i=\alpha/\sqrt{(c_1^i)^2 + \cdots + (c_d^i)^2}$ (except when each $c_j^i=0$, in which case $\alpha_i=\alpha$).</p><p>Now, let us we compare the Fourier and polynomial bases on the 1000-state random walk example. <strong>In general, we do not recommend using the polynomial basis for online learning.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># a wrapper class for polynomial / Fourier -based value function</span></div><div class="line">POLYNOMIAL_BASES = <span class="number">0</span></div><div class="line">FOURIER_BASES = <span class="number">1</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasesValueFunction</span>:</span></div><div class="line">    <span class="comment"># @order: # of bases, each function also has one more constant parameter (called bias in machine learning)</span></div><div class="line">    <span class="comment"># @type: polynomial bases or Fourier bases</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, order, type)</span>:</span></div><div class="line">        self.order = order</div><div class="line">        self.weights = np.zeros(order + <span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># set up bases function</span></div><div class="line">        self.bases = []</div><div class="line">        <span class="keyword">if</span> type == POLYNOMIAL_BASES:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, order + <span class="number">1</span>):</div><div class="line">                self.bases.append(<span class="keyword">lambda</span> s, i=i: pow(s, i))</div><div class="line">        <span class="keyword">elif</span> type == FOURIER_BASES:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, order + <span class="number">1</span>):</div><div class="line">                self.bases.append(<span class="keyword">lambda</span> s, i=i: np.cos(i * np.pi * s))</div><div class="line"></div><div class="line">    <span class="comment"># get the value of @state</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="comment"># map the state space into [0, 1]</span></div><div class="line">        state /= float(N_STATES)</div><div class="line">        <span class="comment"># get the feature vector</span></div><div class="line">        feature = np.asarray([func(state) <span class="keyword">for</span> func <span class="keyword">in</span> self.bases])</div><div class="line">        <span class="keyword">return</span> np.dot(self.weights, feature)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, delta, state)</span>:</span></div><div class="line">        <span class="comment"># map the state space into [0, 1]</span></div><div class="line">        state /= float(N_STATES)</div><div class="line">        <span class="comment"># get derivative value</span></div><div class="line">        derivativeValue = np.asarray([func(state) <span class="keyword">for</span> func <span class="keyword">in</span> self.bases])</div><div class="line">        self.weights += delta * derivativeValue</div></pre></td></tr></table></figure><p>The function upper is used to construction the features of states (map states to features).</p><p>Next, we will compare different super-parameters’ (order) performance:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">1</span></div><div class="line"></div><div class="line">episodes = <span class="number">5000</span></div><div class="line"></div><div class="line"><span class="comment"># # of bases</span></div><div class="line">orders = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>]</div><div class="line"></div><div class="line">alphas = [<span class="number">1e-4</span>, <span class="number">5e-5</span>]</div><div class="line">labels = [[<span class="string">'polynomial basis'</span>] * <span class="number">3</span>, [<span class="string">'fourier basis'</span>] * <span class="number">3</span>]</div><div class="line"></div><div class="line"><span class="comment"># track errors for each episode</span></div><div class="line">errors = np.zeros((len(alphas), len(orders), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(orders)):</div><div class="line">        valueFunctions = [BasesValueFunction(orders[i], POLYNOMIAL_BASES), BasesValueFunction(orders[i], FOURIER_BASES)]</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">            <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'order:'</span>, orders[i], labels[j][i], <span class="string">'episode:'</span>, episode)</div><div class="line"></div><div class="line">                <span class="comment"># gradient Monte Carlo algorithm</span></div><div class="line">                gradientMonteCarlo(valueFunctions[j], alphas[j])</div><div class="line"></div><div class="line">                <span class="comment"># get state values under current value function</span></div><div class="line">                stateValues = [valueFunctions[j].value(state) <span class="keyword">for</span> state <span class="keyword">in</span> states]</div><div class="line"></div><div class="line">                <span class="comment"># get the root-mean-squared error</span></div><div class="line">                errors[j, i, episode] += np.sqrt(np.mean(np.power(trueStateValues[<span class="number">1</span>: <span class="number">-1</span>] - stateValues, <span class="number">2</span>)))</div><div class="line"></div><div class="line"><span class="comment"># average over independent runs</span></div><div class="line">errors /= runs</div><div class="line"></div><div class="line">plt.figure(<span class="number">5</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(orders)):</div><div class="line">        plt.plot(errors[i, j, :], label=labels[i][j]+<span class="string">' order = '</span> + str(orders[j]))</div><div class="line">plt.xlabel(<span class="string">'Episodes'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMSVE'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/poly_vs_four.png" alt="poly_vs_four"></p><h3 id="TODO-TILE-CODING"><a href="#TODO-TILE-CODING" class="headerlink" title="TODO: TILE CODING"></a>TODO: TILE CODING</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The novelty in this post is that the approximate value function is represented not as a table but as a parameterized functional form with
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>n-step TD</title>
    <link href="http://yoursite.com/2017/07/04/n-step-TD/"/>
    <id>http://yoursite.com/2017/07/04/n-step-TD/</id>
    <published>2017-07-04T03:16:06.000Z</published>
    <updated>2017-07-04T05:56:21.317Z</updated>
    
    <content type="html"><![CDATA[<p>In this post, we unify the Monte Carlo methods and the one-step TD methods. We first consider the prediction problem.</p><h3 id="n-step-TD-Prediction"><a href="#n-step-TD-Prediction" class="headerlink" title="n-step TD Prediction"></a>n-step TD Prediction</h3><p>Monte Carlo methods preform a backup for each state based on the entire sequence of observed rewards from that state until the end of the episode. One-step TD methods is based on just on next reward. So n-step TD methods perform a backup based on an intermediate number of rewards: more than one, but less than all of them until termination.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/ntd/nsteptd.png" alt="nstep_td"></p><p>More formally, consider the backup applied to state $S_t$ as a result of the state-reward sequence, $S_t, R_{t+1},S_{t+1}, R_{t+2}, \cdots, R_T, S_T$ (omitting the actions for simplicity). We know that in Monte Carlo backups the estimate of $v_{\pi}(S_t)$ updated in the direction of the complete return:<br>$$<br>G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T,<br>$$<br>where $T$ is the last time step of the episode. Let us call this quantity the <strong>target</strong> of the backup. Whereas in Monte Carlo backups the target is the return, in one-step backups the target is the first reward plus the discounted estimated value of the next state, which we call the one-step return:<br>$$<br>G_{t:t+1} \doteq R_{t+1} + \gamma V_t (S_{t+1}),<br>$$<br>where $V_t : \mathcal{S} \rightarrow \mathbb{R}$ here is an estimate at time $t$ of $v_{\pi}$. The subscripts on $G_{t:t+1}$ indicate that it is truncated return for time t using rewards up until time $t+1$. In the one-step return, $\gamma V_t (S_{t+1})$ takes the place of the other terms $ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$ of the full return. Our point now is that this idea makes just as much sense after two steps as it does after one. The target for a two-step backup is the two-step return:<br>$$<br>G_{t:t+2} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1} (S_{t+2}),<br>$$<br>where now $\gamma^2 V_{t+1}(S_{t+2})$ corrects for the absence of the terms $\gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$. Similarly, the target for an arbitrary n-step backup is the n-step return:<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n V_{t+n-1} (S_{t+n}),<br>$$<br>for all $n,t$ such that $n \ge 1$ and $0 \leq t \leq T-n$. If $t+n \ge T$, then all the missing terms are taken as zero, and the n-step return defined to be equal to the ordinary full return.</p><p>No real algorithm can use the n-step return until after it has seen $R_{t+n}$ and computed $V_{t+n-1}$. The first time these are available is $t+n$. The natural algorithm state-value learning algorithm for using n-step returns is thus<br>$$<br>V_{t+n}(S_t) \doteq V_{t+n-1}(S_t) + \alpha [G_{t:t+n} - V_{t+n-1}(S_t)], \;\;\;\;\;\; 0 \leq t \leq T<br>$$<br>while the values of all other states remain unchanged. Note that no changes at all are made during the first $n-1$ steps of each episode. Complete pseudocode is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/ntd/nstep_td_alg.png" alt="alg_box"></p><p>The worst error of the expected n-step return is guaranteed to be less than or equal to $\gamma^n$ times the worst error under $V_{t+n-1}$:<br>$$<br>\max_s \left |\mathbb{E}[G_{t:t+1}|S_t=s] - v_{\pi}(s) \right | \leq \gamma^n \max_s |V_{t+n-1}(s) - v_{\pi}(s)|,<br>$$<br>for all $n \geq 1$. This is called the <strong>error reduction property</strong> of n-step returns. The n-step TD methods thus form a family of sound methods, with one-step TD methods and Monte Carlo methods as extreme members.</p><h4 id="Example-n-step-TD-Methods-on-the-Random-Walk"><a href="#Example-n-step-TD-Methods-on-the-Random-Walk" class="headerlink" title="Example:  n-step TD Methods on the Random Walk"></a>Example: n-step TD Methods on the Random Walk</h4><p>Now we have a larger MDP (19 non-terminal states). First of all we need to define the new environment:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all states</span></div><div class="line">N_STATES = <span class="number">19</span></div><div class="line"></div><div class="line"><span class="comment"># discount</span></div><div class="line">GAMMA = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># initial state values</span></div><div class="line">stateValues = np.zeros(N_STATES + <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># all states but terminal states</span></div><div class="line">states = np.arange(<span class="number">1</span>, N_STATES + <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># start from the middle state</span></div><div class="line">START_STATE = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># two terminal states</span></div><div class="line"><span class="comment"># an action leading to the left terminal state has reward -1</span></div><div class="line"><span class="comment"># an action leading to the right terminal state has reward 1</span></div><div class="line">END_STATES = [<span class="number">0</span>, N_STATES + <span class="number">1</span>]</div><div class="line"></div><div class="line"><span class="comment"># true state value from bellman equation</span></div><div class="line">realStateValues = np.arange(<span class="number">-20</span>, <span class="number">22</span>, <span class="number">2</span>) / <span class="number">20.0</span></div><div class="line">realStateValues[<span class="number">0</span>] = realStateValues[<span class="number">-1</span>] = <span class="number">0</span></div></pre></td></tr></table></figure><p>And then develop the n-step TD algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># n-steps TD method</span></div><div class="line"><span class="comment"># @stateValues: values for each state, will be updated</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="comment"># @alpha: # step size</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">temporalDifference</span><span class="params">(stateValues, n, alpha)</span>:</span></div><div class="line">    <span class="comment"># initial starting state</span></div><div class="line">    currentState = START_STATE</div><div class="line"></div><div class="line">    <span class="comment"># arrays to store states and rewards for an episode</span></div><div class="line">    <span class="comment"># space isn't a major consideration, so I didn't use the mod trick</span></div><div class="line">    states = [currentState]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># choose an action randomly</span></div><div class="line">            <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">                newState = currentState + <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                newState = currentState - <span class="number">1</span></div><div class="line">            <span class="keyword">if</span> newState == <span class="number">0</span>:</div><div class="line">                reward = <span class="number">-1</span></div><div class="line">            <span class="keyword">elif</span> newState == <span class="number">20</span>:</div><div class="line">                reward = <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                reward = <span class="number">0</span></div><div class="line"></div><div class="line">            <span class="comment"># store new state and new reward</span></div><div class="line">            states.append(newState)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newState <span class="keyword">in</span> END_STATES:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += pow(GAMMA, t - updateTime - <span class="number">1</span>) * rewards[t]</div><div class="line">            <span class="comment"># add state value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += pow(GAMMA, n) * stateValues[states[(updateTime + n)]]</div><div class="line">            stateToUpdate = states[updateTime]</div><div class="line">            <span class="comment"># update the state value</span></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> stateToUpdate <span class="keyword">in</span> END_STATES:</div><div class="line">                stateValues[stateToUpdate] += alpha * (returns - stateValues[stateToUpdate])</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentState = newState</div></pre></td></tr></table></figure><p>Now, let us test the performance under different $n$ values and $\alpha$ values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># truncate value for better display</span></div><div class="line">truncateValue = <span class="number">0.55</span></div><div class="line"></div><div class="line"><span class="comment"># all possible steps</span></div><div class="line">steps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># all possible alphas</span></div><div class="line">alphas = np.arange(<span class="number">0</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="comment"># each run has 10 episodes</span></div><div class="line">episodes = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># perform 100 independent runs</span></div><div class="line">runs = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># track the errors for each (step, alpha) combination</span></div><div class="line">errors = np.zeros((len(steps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> stepInd, step <span class="keyword">in</span> zip(range(len(steps)), steps):</div><div class="line">        <span class="keyword">for</span> alphaInd, alpha <span class="keyword">in</span> zip(range(len(alphas)), alphas):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'step:'</span>, step, <span class="string">'alpha:'</span>, alpha)</div><div class="line">            currentStateValues = np.copy(stateValues)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                temporalDifference(currentStateValues, step, alpha)</div><div class="line">                <span class="comment"># calculate the RMS error</span></div><div class="line">                errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - realStateValues, <span class="number">2</span>)) / N_STATES)</div><div class="line"><span class="comment"># take average</span></div><div class="line">errors /= episodes * runs</div><div class="line"><span class="comment"># truncate the error</span></div><div class="line">errors[errors &gt; truncateValue] = truncateValue</div><div class="line">plt.figure()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(steps)):</div><div class="line">    plt.plot(alphas, errors[i, :], label=<span class="string">'n = '</span> + str(steps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMS error'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/ntd/n_step_td_random_walk.png" alt="n_step_td_random_walk_result"></p><h3 id="TODO-N-STEP-SARSA"><a href="#TODO-N-STEP-SARSA" class="headerlink" title="TODO: N-STEP SARSA"></a>TODO: N-STEP SARSA</h3><h3 id="TODO-N-STEP-OFF-POLICY-ALGORITHM"><a href="#TODO-N-STEP-OFF-POLICY-ALGORITHM" class="headerlink" title="TODO: N-STEP OFF-POLICY ALGORITHM"></a>TODO: N-STEP OFF-POLICY ALGORITHM</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In this post, we unify the Monte Carlo methods and the one-step TD methods. We first consider the prediction problem.&lt;/p&gt;&lt;h3 id=&quot;n-step-T
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="TD" scheme="http://yoursite.com/tags/TD/"/>
    
  </entry>
  
  <entry>
    <title>Temporal-Difference Learning</title>
    <link href="http://yoursite.com/2017/07/02/Temporal-Difference-Learning/"/>
    <id>http://yoursite.com/2017/07/02/Temporal-Difference-Learning/</id>
    <published>2017-07-02T04:44:00.000Z</published>
    <updated>2017-07-04T02:36:15.327Z</updated>
    
    <content type="html"><![CDATA[<p>If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be <em>temporal-difference</em> (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome.</p><h3 id="TD-0"><a href="#TD-0" class="headerlink" title="TD(0)"></a><strong>TD(0)</strong></h3><p>Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environment is<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)],<br>$$<br>where $G_t$ is the <strong>actual return</strong> following time $t$. Let us call this method $constant\text{-}\alpha \ MC$. Notice that, if we are in a stationary environment (like <a href="https://ewanlee.github.io/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" target="_blank" rel="external">earlier</a>. For some reason, don’t use incremental implementation), the $\alpha$ is equals to $\frac{1}{N(S_t)}$. whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t+1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]<br>$$<br>immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_t$, whereas the target for the TD update is $R_{t+1} + \gamma V(S_{t+1})$. This TD method is called $TD(0)$, or <strong>one-step</strong> TD. The box below specifies TD(0) completely in procedural form.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td_0.png" alt="td_0"></p><p>TD(0)’s backup diagram is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td0_bg.png" alt="td0bg"></p><p>Because the TD(0) bases its update in part on an existing estimate, we say that it is a <em>bootstrapping</em> method, like DP. We know that<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [G_t \ | \ S_t=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} \ | \ S_t=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_t=s].<br>\end{align}<br>$$<br>Roughly speaking, Monte Carlo methods use an estimate of (3) as a target, whereas DP methods use an estimate of (5) as a target, The Monte Carlo target is an estimate because the expected value in (3) is not known; a sample return is used in place of the real expected return. The DP target is an estimate not because of the excepted value, which are assumed to be completely provided by a model of the environment (the environment is known for the DP methods), but because $v_{\pi}(S_{t+1})$ is not known and the current estimate, $V(S_{t+1})$, is used instead. The TD target is an estimate for both reasons.</p><p>Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \gamma V(S_{t+1})$. This quantity, called the <strong>TD error</strong>, arises in various forms throughout reinforcement learning:<br>$$<br>\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t).<br>$$<br>Notice that the TD error at each time is the error in the estimate <strong>made at that time</strong>. Because the TD error depends on the next state and the next reward, it is not actually available until one time step later. Also note that if the array $V$ does not change during the episode (as it does not in Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:<br>$$<br>\begin{align}<br>G_t - V(S_t) &amp;= R_{t+1} + \gamma G(S_{t+1}) - V(S_t) + \gamma V(S_{t+1} ) - \gamma V(S_{t+1}) \\<br>&amp;= \delta_t + \gamma (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(G_t-V(S_T)) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(0 -0) \\<br>&amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k.<br>\end{align}<br>$$<br>This identity is not exact if $V$ is updated during the episode (as it is in TD(0)), but if the step size is small then it may still hold approximately. Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning.</p><h4 id="Example-Random-walk"><a href="#Example-Random-walk" class="headerlink" title="Example: Random walk"></a><strong>Example: Random walk</strong></h4><p>In this example we empirically compare the prediction abilities of TD(0) and constant-$\alpha$ MC applied to the small Markov reward process shown in the upper part of the figure below. All episodes start in the center state, <strong>C</strong>, and the proceed either left or right by one state on each step, with equal probability. This behavior can be thought of as due to the combined effect of a fixed policy and an environment’s state-transition probabilities, but we do not care which; we are concerned only with predicting returns however they are generated. Episodes terminates on the right, a reward of +1 occurs; all other reward are zero. For example, a typical episode might consist of the following state-and-reward sequence: <strong>C, 0, B, 0, C, 0, D, 0, E, 1.</strong> Because this task is undiscounted, the true value of each state is the probability of terminating on the right if starting from that state. Thus, the true value of the center state is $v_{\pi}(\text{C}) = 0.5$. The true values of all the states, <strong>A</strong> through <strong>E</strong>, are $\frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}$, and $\frac{5}{6}$. In all cases the approximate value function was initialized to the intermediate value $V(s)=0.5$, for all $s$.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk.png" alt="random_walk"></p><p>Now, let us develop the codes to solve problem.</p><p>The first, we initialize some truth.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 0 is the left terminal state</span></div><div class="line"><span class="comment"># 6 is the right terminal state</span></div><div class="line"><span class="comment"># 1 ... 5 represents A ... E</span></div><div class="line">states = np.zeros(<span class="number">7</span>)</div><div class="line">states[<span class="number">1</span>:<span class="number">6</span>] = <span class="number">0.5</span></div><div class="line"><span class="comment"># For convenience, we assume all rewards are 0</span></div><div class="line"><span class="comment"># and the left terminal state has value 0, the right terminal state has value 1</span></div><div class="line"><span class="comment"># This trick has been used in Gambler's Problem</span></div><div class="line">states[<span class="number">6</span>] = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># set up true state values</span></div><div class="line">trueValue = np.zeros(<span class="number">7</span>)</div><div class="line">trueValue[<span class="number">1</span>:<span class="number">6</span>] = np.arange(<span class="number">1</span>, <span class="number">6</span>) / <span class="number">6.0</span></div><div class="line">trueValue[<span class="number">6</span>] = <span class="number">1</span></div><div class="line"></div><div class="line">ACTION_LEFT = <span class="number">0</span></div><div class="line">ACTION_RIGHT = <span class="number">1</span></div></pre></td></tr></table></figure><p>The below box is the TD(0) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">temporalDifference</span><span class="params">(states, alpha=<span class="number">0.1</span>, batch=False)</span>:</span></div><div class="line">    state = <span class="number">3</span></div><div class="line">    trajectory = [state]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        oldState = state</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == ACTION_LEFT:</div><div class="line">            state -= <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state += <span class="number">1</span></div><div class="line">        <span class="comment"># Assume all rewards are 0</span></div><div class="line">        reward = <span class="number">0</span></div><div class="line">        trajectory.append(state)</div><div class="line">        <span class="comment"># TD update</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> batch:</div><div class="line">            states[oldState] += alpha * (reward + states[state] - states[oldState])</div><div class="line">        <span class="keyword">if</span> state == <span class="number">6</span> <span class="keyword">or</span> state == <span class="number">0</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        rewards.append(reward)</div><div class="line">    <span class="keyword">return</span> trajectory, rewards</div></pre></td></tr></table></figure><p>And below box is the constant-$\alpha$ Monte Carlo algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarlo</span><span class="params">(states, alpha=<span class="number">0.1</span>, batch=False)</span>:</span></div><div class="line">    state = <span class="number">3</span></div><div class="line">    trajectory = [<span class="number">3</span>]</div><div class="line">    <span class="comment"># if end up with left terminal state, all returns are 0</span></div><div class="line">    <span class="comment"># if end up with right terminal state, all returns are 1</span></div><div class="line">    returns = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == ACTION_LEFT:</div><div class="line">            state -= <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state += <span class="number">1</span></div><div class="line">        trajectory.append(state)</div><div class="line">        <span class="keyword">if</span> state == <span class="number">6</span>:</div><div class="line">            returns = <span class="number">1.0</span></div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">elif</span> state == <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="keyword">break</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> batch:</div><div class="line">        <span class="keyword">for</span> state_ <span class="keyword">in</span> trajectory[:<span class="number">-1</span>]:</div><div class="line">            <span class="comment"># MC update</span></div><div class="line">            states[state_] += alpha * (returns - states[state_])</div><div class="line">    <span class="keyword">return</span> trajectory, [returns] * (len(trajectory) - <span class="number">1</span>)</div></pre></td></tr></table></figure><p>First of all, let us test the performance of the TD(0) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stateValue</span><span class="params">()</span>:</span></div><div class="line">    episodes = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]</div><div class="line">    currentStates = np.copy(states)</div><div class="line">    plt.figure(<span class="number">1</span>)</div><div class="line">    axisX = np.arange(<span class="number">0</span>, <span class="number">7</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, episodes[<span class="number">-1</span>] + <span class="number">1</span>):</div><div class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> episodes:</div><div class="line">            plt.plot(axisX, currentStates, label=str(i) + <span class="string">' episodes'</span>)</div><div class="line">        temporalDifference(currentStates)</div><div class="line">    plt.plot(axisX, trueValue, label=<span class="string">'true values'</span>)</div><div class="line">    plt.xlabel(<span class="string">'state'</span>)</div><div class="line">    plt.legend()</div><div class="line">    </div><div class="line">stateValue()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_td0.png" alt="random_walk_td0"></p><p>And then let us show the RMS error of the TD(0) algorithm and constant-$\alpha$ Monte Carlo algorithm, for various $\alpha$ values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMSError</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># I'm lazy here, so do not let same alpha value appear in both arrays</span></div><div class="line">    <span class="comment"># For example, if in TD you want to use alpha = 0.2, then in MC you can use alpha = 0.201</span></div><div class="line">    TDAlpha = [<span class="number">0.15</span>, <span class="number">0.1</span>, <span class="number">0.05</span>]</div><div class="line">    MCAlpha = [<span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.03</span>, <span class="number">0.04</span>]</div><div class="line">    episodes = <span class="number">100</span> + <span class="number">1</span></div><div class="line">    runs = <span class="number">100</span></div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    axisX = np.arange(<span class="number">0</span>, episodes)</div><div class="line">    <span class="keyword">for</span> alpha <span class="keyword">in</span> TDAlpha + MCAlpha:</div><div class="line">        totalErrors = np.zeros(episodes)</div><div class="line">        <span class="keyword">if</span> alpha <span class="keyword">in</span> TDAlpha:</div><div class="line">            method = <span class="string">'TD'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            method = <span class="string">'MC'</span></div><div class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">            errors = []</div><div class="line">            currentStates = np.copy(states)</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                errors.append(np.sqrt(np.sum(np.power(trueValue - currentStates, <span class="number">2</span>)) / <span class="number">5.0</span>))</div><div class="line">                <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                    temporalDifference(currentStates, alpha=alpha)</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    monteCarlo(currentStates, alpha=alpha)</div><div class="line">            totalErrors += np.asarray(errors)</div><div class="line">        totalErrors /= runs</div><div class="line">        plt.plot(axisX, totalErrors, label=method + <span class="string">', alpha='</span> + str(alpha))</div><div class="line">    plt.xlabel(<span class="string">'episodes'</span>)</div><div class="line">    plt.legend()</div><div class="line">    </div><div class="line">RMSError()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_rmse.png" alt="random_walk_error"></p><p>We can see, the TD method was consistently better than the MC method on this task.</p><p>Now, suppose that there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning method is to present the experience repeatedly until the method converges upon an answer. We call this <em>batch updating</em>.</p><h4 id="Example-Random-walk-under-batch-updating"><a href="#Example-Random-walk-under-batch-updating" class="headerlink" title="Example: Random walk under batch updating"></a><strong>Example: Random walk under batch updating</strong></h4><p>After each new episodes, all episodes seen so far were treated as a batch. They were repeatedly presented to the algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchUpdating</span><span class="params">(method, episodes, alpha=<span class="number">0.001</span>)</span>:</span></div><div class="line">    <span class="comment"># perform 100 independent runs</span></div><div class="line">    runs = <span class="number">100</span></div><div class="line">    totalErrors = np.zeros(episodes - <span class="number">1</span>)</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        currentStates = np.copy(states)</div><div class="line">        errors = []</div><div class="line">        <span class="comment"># track shown trajectories and reward/return sequences</span></div><div class="line">        trajectories = []</div><div class="line">        rewards = []</div><div class="line">        <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">1</span>, episodes):</div><div class="line">            print(<span class="string">'Run:'</span>, run, <span class="string">'Episode:'</span>, ep)</div><div class="line">            <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                trajectory_, rewards_ = temporalDifference(currentStates, batch=<span class="keyword">True</span>)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                trajectory_, rewards_ = monteCarlo(currentStates, batch=<span class="keyword">True</span>)</div><div class="line">            trajectories.append(trajectory_)</div><div class="line">            rewards.append(rewards_)</div><div class="line">            <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">                <span class="comment"># keep feeding our algorithm with trajectories seen so far until state value function converges</span></div><div class="line">                updates = np.zeros(<span class="number">7</span>)</div><div class="line">                <span class="keyword">for</span> trajectory_, rewards_ <span class="keyword">in</span> zip(trajectories, rewards):</div><div class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(trajectory_) - <span class="number">1</span>):</div><div class="line">                        <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                            updates[trajectory_[i]] += rewards_[i] + currentStates[trajectory_[i + <span class="number">1</span>]] - currentStates[trajectory_[i]]</div><div class="line">                        <span class="keyword">else</span>:</div><div class="line">                            updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]]</div><div class="line">                updates *= alpha</div><div class="line">                <span class="keyword">if</span> np.sum(np.abs(updates)) &lt; <span class="number">1e-3</span>:</div><div class="line">                    <span class="keyword">break</span></div><div class="line">                <span class="comment"># perform batch updating</span></div><div class="line">                currentStates += updates</div><div class="line">            <span class="comment"># calculate rms error</span></div><div class="line">            errors.append(np.sqrt(np.sum(np.power(currentStates - trueValue, <span class="number">2</span>)) / <span class="number">5.0</span>))</div><div class="line">        totalErrors += np.asarray(errors)</div><div class="line">    totalErrors /= runs</div><div class="line">    <span class="keyword">return</span> totalErrors</div></pre></td></tr></table></figure><p>Notice that the core codes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep feeding our algorithm with trajectories seen so far until state</span></div><div class="line">    <span class="comment"># value function converges</span></div><div class="line">    updates = np.zeros(<span class="number">7</span>)</div><div class="line">    <span class="keyword">for</span> trajectory_, rewards_ <span class="keyword">in</span> zip(trajectories, rewards):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(trajectory_) - <span class="number">1</span>):</div><div class="line">            <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                updates[trajectory_[i]] += rewards_[i] + \</div><div class="line">                    currentStates[trajectory_[i + <span class="number">1</span>]] - currentStates[trajectory_[i]]</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]]</div><div class="line">    updates *= alpha</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(updates)) &lt; <span class="number">1e-3</span>:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># perform batch updating</span></div><div class="line">    currentStates += updates</div></pre></td></tr></table></figure><p>Either TD methods or MC methods, the target is to minimize the TD error (or MC error, I say).</p><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/batch_update.png" alt="batch_update"></p><p>Under batch training, constant-$\alpha$ MC converges to value, $V(s)$, that are sample averages of the actual returns experienced after visiting each state $s$. These are optimal estimate in the sense that they minimize the mean-squared error from the actual returns in the training set. In this sense it is surprising that the batch TD method was able to perform better according to the root mean-squared error measure shown in the top figure. How is it that batch TD was able to perform better than this optimal methods? Consider the example in below box:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/example_6_4.png" alt="example6_4"></p><p>Example illustrates a general difference between the estimates founds by batch TD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the <strong>certainty-equivalence estimate</strong>.</p><h3 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a><strong>Sarsa</strong></h3><p>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\right]<br>$$</p><p>This update is done after every transition from a nonterminal state $S_t$. If $S_{t+1}$ is terminal, then $Q(S_{t+1}, A_{t+1})$ is defined as zero. This rule uses every element of the quintuple of events, $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that make up a transition from one state-action pair to the next. This quintuple gives rise to the name <em>Sarsa</em> for the algorithm. The backup diagram for Sarsa is as shown to the bottom.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/sarsabg.png" alt="sarsa_bg"></p><p>The general form of the Sarsa control algorithm is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png" alt="sarsa"></p><h4 id="Example-Windy-Gridworld"><a href="#Example-Windy-Gridworld" class="headerlink" title="Example: Windy Gridworld"></a><strong>Example: Windy Gridworld</strong></h4><p>The figure below is a standard grid-world, with start and goal states, but with one diﬀerence: there is a crosswind upward through the middle of the grid. The actions are the standard four—up, down,right, and left—but in the middle region the resultant next states are shifted upward by a “wind,” the strength of which varies from column to column. The strength of the wind is given below each column, in number of cells shifted upward. For example, if you are one cell to the right of the goal, then the action left takes you to the cell just above the goal. Let us treat this as an undiscounted episodic task, with constant rewards of −1 until the goal state is reached.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_gridworld.png" alt="windy_gridworld"></p><p>To demonstrate the problem clearly, we use the <a href="https://gym.openai.com/" target="_blank" rel="external">OpenAI gym</a> toolkit to develop the algorithm.</p><p>First of all, we need to define a environment (the windy grid world):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># represents every action as a integer</span></div><div class="line">UP = <span class="number">0</span></div><div class="line">RIGHT = <span class="number">1</span></div><div class="line">DOWN = <span class="number">2</span></div><div class="line">LEFT = <span class="number">3</span></div></pre></td></tr></table></figure><p>The environment is a class that inherit the gym default class <strong>discrete.DiscreteEnv</strong> (shows that the states are discrete):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">WindyGridworldEnv</span><span class="params">(discrete.DiscreteEnv)</span></span></div></pre></td></tr></table></figure><p>First we need to construct our world:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">    self.shape = (<span class="number">7</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="comment"># the number of all states</span></div><div class="line">    nS = np.prod(self.shape)</div><div class="line">    <span class="comment"># the number of all actions</span></div><div class="line">    nA = <span class="number">4</span></div><div class="line"></div><div class="line">    <span class="comment"># Wind strength</span></div><div class="line">    winds = np.zeros(self.shape)</div><div class="line">    winds[:,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>]] = <span class="number">1</span></div><div class="line">    winds[:,[<span class="number">6</span>,<span class="number">7</span>]] = <span class="number">2</span></div><div class="line"></div><div class="line">    <span class="comment"># Calculate transition probabilities</span></div><div class="line">    <span class="comment"># P is the transition matrix</span></div><div class="line">    P = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(nS):</div><div class="line">        position = np.unravel_index(s, self.shape)</div><div class="line">        P[s] = &#123; a : [] <span class="keyword">for</span> a <span class="keyword">in</span> range(nA) &#125;</div><div class="line">        P[s][UP] = self._calculate_transition_prob(position, [<span class="number">-1</span>, <span class="number">0</span>], winds)</div><div class="line">        P[s][RIGHT] = self._calculate_transition_prob(position, [<span class="number">0</span>, <span class="number">1</span>], winds)</div><div class="line">        P[s][DOWN] = self._calculate_transition_prob(position, [<span class="number">1</span>, <span class="number">0</span>], winds)</div><div class="line">        P[s][LEFT] = self._calculate_transition_prob(position, [<span class="number">0</span>, <span class="number">-1</span>], winds)</div><div class="line"></div><div class="line">    <span class="comment"># We always start in state (3, 0)</span></div><div class="line">    isd = np.zeros(nS)</div><div class="line">    isd[np.ravel_multi_index((<span class="number">3</span>,<span class="number">0</span>), self.shape)] = <span class="number">1.0</span></div><div class="line"></div><div class="line">    super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)</div></pre></td></tr></table></figure><p>This is natural, uh? Notice that there is a method called <strong>_calculate_transition_prob</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_calculate_transition_prob</span><span class="params">(self, current, delta, winds)</span>:</span></div><div class="line">        new_position = np.array(current) + np.array(delta) + np.array([<span class="number">-1</span>, <span class="number">0</span>]) * winds[tuple(current)]</div><div class="line">        new_position = self._limit_coordinates(new_position).astype(int)</div><div class="line">        new_state = np.ravel_multi_index(tuple(new_position), self.shape)</div><div class="line">        is_done = tuple(new_position) == (<span class="number">3</span>, <span class="number">7</span>)</div><div class="line">        <span class="keyword">return</span> [(<span class="number">1.0</span>, new_state, <span class="number">-1.0</span>, is_done)]</div></pre></td></tr></table></figure><p>and <strong>_limit_corrdinates</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_limit_coordinates</span><span class="params">(self, coord)</span>:</span></div><div class="line">    coord[<span class="number">0</span>] = min(coord[<span class="number">0</span>], self.shape[<span class="number">0</span>] - <span class="number">1</span>)</div><div class="line">    coord[<span class="number">0</span>] = max(coord[<span class="number">0</span>], <span class="number">0</span>)</div><div class="line">    coord[<span class="number">1</span>] = min(coord[<span class="number">1</span>], self.shape[<span class="number">1</span>] - <span class="number">1</span>)</div><div class="line">    coord[<span class="number">1</span>] = max(coord[<span class="number">1</span>], <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> coord</div></pre></td></tr></table></figure><p>It is worth to mention that the default gym environment class has some useful parameters: <strong>nS</strong>, <strong>nA</strong>, <strong>P</strong> and <strong>is_done</strong>. nS is the total number of states and nA is the total number of actions (here assume all states only could take the same fixed actions). P is the state transition matrix, the default environment class has a <strong>step</strong> method (accept a parameter <strong>action</strong>) that could generates episode automatically according the P and is_done that represents whether a state is terminal state or not.</p><p>Finally, we define a output method for pretty show the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_render</span><span class="params">(self, mode=<span class="string">'human'</span>, close=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> close:</div><div class="line">        <span class="keyword">return</span></div><div class="line"></div><div class="line">    outfile = StringIO() <span class="keyword">if</span> mode == <span class="string">'ansi'</span> <span class="keyword">else</span> sys.stdout</div><div class="line"></div><div class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(self.nS):</div><div class="line">        position = np.unravel_index(s, self.shape)</div><div class="line">        <span class="comment"># print(self.s)</span></div><div class="line">        <span class="keyword">if</span> self.s == s:</div><div class="line">            output = <span class="string">" x "</span></div><div class="line">        <span class="keyword">elif</span> position == (<span class="number">3</span>,<span class="number">7</span>):</div><div class="line">            output = <span class="string">" T "</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            output = <span class="string">" o "</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> position[<span class="number">1</span>] == <span class="number">0</span>:</div><div class="line">            output = output.lstrip()</div><div class="line">        <span class="keyword">if</span> position[<span class="number">1</span>] == self.shape[<span class="number">1</span>] - <span class="number">1</span>:</div><div class="line">            output = output.rstrip()</div><div class="line">            output += <span class="string">"\n"</span></div><div class="line"></div><div class="line">        outfile.write(output)</div><div class="line">    outfile.write(<span class="string">"\n"</span>)</div></pre></td></tr></table></figure><p>Then, let us test our model：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">env = WindyGridworldEnv()</div><div class="line"></div><div class="line">print(env.reset())</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">2</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_render.png" alt="windy_show"></p><p>Each state transition, the step method return a tuple <strong>(next_state, reward, is_done, some_extra_info)</strong>.</p><p>Next, we define the episodes generation policy:</p><p>def make_epsilon_greedy_policy(Q, epsilon, nA):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">Creates an epsilon-greedy policy based on a given Q-function and epsilon.</div><div class="line"></div><div class="line">Args:</div><div class="line">    Q: A dictionary that maps from state -&gt; action-values.</div><div class="line">        Each value is a numpy array of length nA (see below)</div><div class="line">    epsilon: The probability to select a random action . float between 0 and 1.</div><div class="line">    nA: Number of actions in the environment.</div><div class="line"></div><div class="line">Returns:</div><div class="line">    A function that takes the observation as an argument and returns</div><div class="line">    the probabilities for each action in the form of a numpy array of length nA.</div><div class="line"></div><div class="line">"""</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></div><div class="line">    A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">    best_action = np.argmax(Q[observation])</div><div class="line">    A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">    <span class="keyword">return</span> A</div><div class="line"><span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Now, let us implement the sarsa algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sarsa</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, alpha=<span class="number">0.5</span>, epsilon=<span class="number">0.1</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        alpha: TD learning rate.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A tuple (Q, stats).</div><div class="line">        Q is the optimal action-value function, a dictionary mapping state -&gt; action values.</div><div class="line">        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># The final action-value function.</span></div><div class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></div><div class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</div><div class="line">    </div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))</div><div class="line"></div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</div><div class="line">    </div><div class="line"></div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode + <span class="number">1</span>, num_episodes), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Implement this!</span></div><div class="line">        state = env.reset()</div><div class="line">        action_probs = policy(state)</div><div class="line">        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">        </div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">            next_state, reward, is_done, _ = env.step(action)</div><div class="line">            next_action_probs = policy(next_state)</div><div class="line">            </div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)</div><div class="line">            Q[state][action] += alpha * (reward + discount_factor * Q[next_state][next_action] - Q[state][action])</div><div class="line">            </div><div class="line">            <span class="keyword">if</span> is_done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">            </div><div class="line">            state = next_state</div><div class="line">            action = next_action</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Q, stats</div></pre></td></tr></table></figure><p>For understand easily, we put the pesudo-code here again:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png" alt="sarsa"></p><p>The results (with $\varepsilon=0.1,\ \alpha=0.5$) are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png" alt="sarsa_result"></p><p>The increasing slope (bottom figure) of the graph shows that the goal is reached more and more quickly over time. Note that Monte Carlo methods cannot easily be used on this task because termination is not guaranteed for all policies. If a policy was ever found that caused the agent to stay in the same state, then the next episode would never end. Step-by-step learning methods such as Sarsa do not have this problem because they quickly learn <strong>during the episode</strong> that such<br>policies are poor, and switch to something else.</p><h3 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h3><p>One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning, defined by<br>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)\right]<br>$$<br>The algorithm is shown in procedural form in the box below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning.png" alt="q_learning"></p><p>And below is the backup diagram:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_bg.png" alt="q_bg"></p><h4 id="Example-Cliff-Walking"><a href="#Example-Cliff-Walking" class="headerlink" title="Example: Cliff Walking"></a>Example: Cliff Walking</h4><p>This grid world example compares Sarsa and Q-learning, highlighting the difference between on-policy (Sarsa) and off-policy (Q-learning) methods. Consider the grid world shown in the figure below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_world.png" alt="cliff_world"></p><p>The same as earlier, we define the environment first. But the new environment just changes a little, so we just paste the code <a href="https://github.com/ewanlee/reinforcement-learning/blob/master/lib/envs/cliff_walking.py" target="_blank" rel="external">here</a>.</p><p>Let us test the environment first:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">env = CliffWalkingEnv()</div><div class="line"></div><div class="line">print(env.reset())</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">0</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">2</span>))</div><div class="line">env.render()</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_show.png" alt="cliff_walk_show"></p><p>Not bad.</p><p>Then, let us develop the Q-learning algorithm (the episodes generation policy is not change):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_learning</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, alpha=<span class="number">0.5</span>, epsilon=<span class="number">0.1</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy</div><div class="line">    while following an epsilon-greedy policy</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        alpha: TD learning rate.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A tuple (Q, episode_lengths).</div><div class="line">        Q is the optimal action-value function, a dictionary mapping state -&gt; action values.</div><div class="line">        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># The final action-value function.</span></div><div class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></div><div class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))    </div><div class="line">    </div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode + <span class="number">1</span>, num_episodes), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Reset the environment and pick the first action</span></div><div class="line">        state = env.reset()</div><div class="line">        </div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="comment"># total_reward = 0.0</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">            </div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            action_probs = policy(state)</div><div class="line">            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            next_state, reward, done, _ = env.step(action)</div><div class="line"></div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            <span class="comment"># TD Update</span></div><div class="line">            best_next_action = np.argmax(Q[next_state])    </div><div class="line">            td_target = reward + discount_factor * Q[next_state][best_next_action]</div><div class="line">            td_delta = td_target - Q[state][action]</div><div class="line">            Q[state][action] += alpha * td_delta</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">                </div><div class="line">            state = next_state</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Q, stats</div></pre></td></tr></table></figure><p>Results ($\varepsilon=0.1$) are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning_result.png" alt="q_learning_result"></p><p>For compare convenience, we put the result of Sarsa here again:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png" alt="sarsa_result"></p><p>We can see, for average, After an initial transient, Q-learning learns values for the optimal policy, that which travels right along the edge of the cliﬀ. Unfortunately, this results in its occasionally falling oﬀ the cliﬀ because of the ε-greedy action selection. Sarsa, on the other hand, takes the action selection into account and learns the longer but safer path through the upper part of the<br>grid. Although Q-learning actually learns the values of the optimal policy, its online performance is worse than that of Sarsa, which learns the roundabout policy. Of course, if ε were gradually reduced, then both methods would asymptotically converge to the optimal policy.</p><h3 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h3><p>Consider the learning algorithm that is just like Q-learning except that instead of the maximum over next state-action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule<br>$$<br>\begin{align}<br>Q(S_t, A_t) &amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \mathbb{E}[Q(S_{t+1}, A_{t+1} \ | \ S_{t+1})] - Q(S_t, A_t) \right ] \\<br>&amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1}, a) - Q(S_t, A_t) \right ],<br>\end{align}<br>$$<br>but that otherwise follows the schema of Q-learning. Its backup diagram is shown below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/esarsa_bg.png" alt="esarsa_bg"></p><p>For compare the results on the cliff-walking task with Excepted Sarsa with Sarsa and Q-learning, we develop another <a href="https://github.com/ewanlee/reinforcement-learning-an-introduction/blob/master/chapter06/CliffWalking.py" target="_blank" rel="external">codes</a> (here we are not use the OpenAI gym toolkit).</p><p>The first we define some truth of the environment:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># world height</span></div><div class="line">WORLD_HEIGHT = <span class="number">4</span></div><div class="line"></div><div class="line"><span class="comment"># world width</span></div><div class="line">WORLD_WIDTH = <span class="number">12</span></div><div class="line"></div><div class="line"><span class="comment"># probability for exploration</span></div><div class="line">EPSILON = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># step size</span></div><div class="line">ALPHA = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># gamma for Q-Learning and Expected Sarsa</span></div><div class="line">GAMMA = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">ACTION_UP = <span class="number">0</span></div><div class="line">ACTION_DOWN = <span class="number">1</span></div><div class="line">ACTION_LEFT = <span class="number">2</span></div><div class="line">ACTION_RIGHT = <span class="number">3</span></div><div class="line">actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]</div><div class="line"></div><div class="line"><span class="comment"># initial state action pair values</span></div><div class="line">stateActionValues = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, <span class="number">4</span>))</div><div class="line">startState = [<span class="number">3</span>, <span class="number">0</span>]</div><div class="line">goalState = [<span class="number">3</span>, <span class="number">11</span>]</div><div class="line"></div><div class="line"><span class="comment"># reward for each action in each state</span></div><div class="line">actionRewards = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, <span class="number">4</span>))</div><div class="line">actionRewards[:, :, :] = <span class="number">-1.0</span></div><div class="line">actionRewards[<span class="number">2</span>, <span class="number">1</span>:<span class="number">11</span>, ACTION_DOWN] = <span class="number">-100.0</span></div><div class="line">actionRewards[<span class="number">3</span>, <span class="number">0</span>, ACTION_RIGHT] = <span class="number">-100.0</span></div></pre></td></tr></table></figure><p>And then we define the state transitions:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># set up destinations for each action in each state</span></div><div class="line">actionDestination = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_HEIGHT):</div><div class="line">    actionDestination.append([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_WIDTH):</div><div class="line">        destinaion = dict()</div><div class="line">        destinaion[ACTION_UP] = [max(i - <span class="number">1</span>, <span class="number">0</span>), j]</div><div class="line">        destinaion[ACTION_LEFT] = [i, max(j - <span class="number">1</span>, <span class="number">0</span>)]</div><div class="line">        destinaion[ACTION_RIGHT] = [i, min(j + <span class="number">1</span>, WORLD_WIDTH - <span class="number">1</span>)]</div><div class="line">        <span class="keyword">if</span> i == <span class="number">2</span> <span class="keyword">and</span> <span class="number">1</span> &lt;= j &lt;= <span class="number">10</span>:</div><div class="line">            destinaion[ACTION_DOWN] = startState</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            destinaion[ACTION_DOWN] = [min(i + <span class="number">1</span>, WORLD_HEIGHT - <span class="number">1</span>), j]</div><div class="line">        actionDestination[<span class="number">-1</span>].append(destinaion)</div><div class="line">actionDestination[<span class="number">3</span>][<span class="number">0</span>][ACTION_RIGHT] = startState</div></pre></td></tr></table></figure><p>We also need a policy to generate the next action according to the current state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># choose an action based on epsilon greedy algorithm</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseAction</span><span class="params">(state, stateActionValues)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(actions)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> np.argmax(stateActionValues[state[<span class="number">0</span>], state[<span class="number">1</span>], :])</div></pre></td></tr></table></figure><p>The <strong>stateActionValues</strong> just is the Q.</p><p>Then, let us develop the Sarsa (and Excepted Sarsa) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># an episode with Sarsa</span></div><div class="line"><span class="comment"># @stateActionValues: values for state action pair, will be updated</span></div><div class="line"><span class="comment"># @expected: if True, will use expected Sarsa algorithm</span></div><div class="line"><span class="comment"># @stepSize: step size for updating</span></div><div class="line"><span class="comment"># @return: total rewards within this episode</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sarsa</span><span class="params">(stateActionValues, expected=False, stepSize=ALPHA)</span>:</span></div><div class="line">    currentState = startState</div><div class="line">    currentAction = chooseAction(currentState, stateActionValues)</div><div class="line">    rewards = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState != goalState:</div><div class="line">        newState = actionDestination[currentState[<span class="number">0</span>]][currentState[<span class="number">1</span>]][currentAction]</div><div class="line">        newAction = chooseAction(newState, stateActionValues)</div><div class="line">        reward = actionRewards[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction]</div><div class="line">        rewards += reward</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> expected:</div><div class="line">            valueTarget = stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], newAction]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># calculate the expected value of new state</span></div><div class="line">            valueTarget = <span class="number">0.0</span></div><div class="line">            actions_list = stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], :]</div><div class="line">            bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist()</div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                <span class="keyword">if</span> action <span class="keyword">in</span> bestActions:</div><div class="line">                    valueTarget += ((<span class="number">1.0</span> - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    valueTarget += EPSILON / len(actions) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div><div class="line">        valueTarget *= GAMMA</div><div class="line">        <span class="comment"># Sarsa update</span></div><div class="line">        stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction] += stepSize * (reward +</div><div class="line">            valueTarget - stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction])</div><div class="line">        currentState = newState</div><div class="line">        currentAction = newAction</div><div class="line">    <span class="keyword">return</span> rewards</div></pre></td></tr></table></figure><p>Because we develop the Sarsa algorithm earlier, so we just concentrate on the Excepted Sarsa algorithm here:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># calculate the expected value of new state</span></div><div class="line">valueTarget = <span class="number">0.0</span></div><div class="line">actions_list = stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], :]</div><div class="line">bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist()</div><div class="line"><span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">    <span class="keyword">if</span> action <span class="keyword">in</span> bestActions:</div><div class="line">        valueTarget += ((<span class="number">1.0</span> - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        valueTarget += EPSILON / len(actions) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div></pre></td></tr></table></figure><p>By the way, let us develop the Q-learning algorithm again:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># an episode with Q-Learning</span></div><div class="line"><span class="comment"># @stateActionValues: values for state action pair, will be updated</span></div><div class="line"><span class="comment"># @expected: if True, will use expected Sarsa algorithm</span></div><div class="line"><span class="comment"># @stepSize: step size for updating</span></div><div class="line"><span class="comment"># @return: total rewards within this episode</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">qLearning</span><span class="params">(stateActionValues, stepSize=ALPHA)</span>:</span></div><div class="line">    currentState = startState</div><div class="line">    rewards = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState != goalState:</div><div class="line">        currentAction = chooseAction(currentState, stateActionValues)</div><div class="line">        reward = actionRewards[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction]</div><div class="line">        rewards += reward</div><div class="line">        newState = actionDestination[currentState[<span class="number">0</span>]][currentState[<span class="number">1</span>]][currentAction]</div><div class="line">        <span class="comment"># Q-Learning update</span></div><div class="line">        stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction] += stepSize * (</div><div class="line">            reward + GAMMA * np.max(stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], :]) -</div><div class="line">            stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction])</div><div class="line">        currentState = newState</div><div class="line">    <span class="keyword">return</span> rewards</div></pre></td></tr></table></figure><p>Now we can see the optimal policy in each state of both algorithm (we are not mentioned earlier):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># print optimal policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">printOptimalPolicy</span><span class="params">(stateActionValues)</span>:</span></div><div class="line">    optimalPolicy = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_HEIGHT):</div><div class="line">        optimalPolicy.append([])</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_WIDTH):</div><div class="line">            <span class="keyword">if</span> [i, j] == goalState:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'G'</span>)</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            bestAction = np.argmax(stateActionValues[i, j, :])</div><div class="line">            <span class="keyword">if</span> bestAction == ACTION_UP:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'U'</span>)</div><div class="line">            <span class="keyword">elif</span> bestAction == ACTION_DOWN:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'D'</span>)</div><div class="line">            <span class="keyword">elif</span> bestAction == ACTION_LEFT:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'L'</span>)</div><div class="line">            <span class="keyword">elif</span> bestAction == ACTION_RIGHT:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'R'</span>)</div><div class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> optimalPolicy:</div><div class="line">        print(row)</div><div class="line"></div><div class="line"><span class="comment"># averaging the reward sums from 10 successive episodes</span></div><div class="line">averageRange = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># episodes of each run</span></div><div class="line">nEpisodes = <span class="number">500</span></div><div class="line"></div><div class="line"><span class="comment"># perform 20 independent runs</span></div><div class="line">runs = <span class="number">20</span></div><div class="line"></div><div class="line">rewardsSarsa = np.zeros(nEpisodes)</div><div class="line">rewardsQLearning = np.zeros(nEpisodes)</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    stateActionValuesSarsa = np.copy(stateActionValues)</div><div class="line">    stateActionValuesQLearning = np.copy(stateActionValues)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        <span class="comment"># cut off the value by -100 to draw the figure more elegantly</span></div><div class="line">        rewardsSarsa[i] += max(sarsa(stateActionValuesSarsa), <span class="number">-100</span>)</div><div class="line">        rewardsQLearning[i] += max(qLearning(stateActionValuesQLearning), <span class="number">-100</span>)</div><div class="line"></div><div class="line"><span class="comment"># averaging over independt runs</span></div><div class="line">rewardsSarsa /= runs</div><div class="line">rewardsQLearning /= runs</div><div class="line"></div><div class="line"><span class="comment"># averaging over successive episodes</span></div><div class="line">smoothedRewardsSarsa = np.copy(rewardsSarsa)</div><div class="line">smoothedRewardsQLearning = np.copy(rewardsQLearning)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(averageRange, nEpisodes):</div><div class="line">    smoothedRewardsSarsa[i] = np.mean(rewardsSarsa[i - averageRange: i + <span class="number">1</span>])</div><div class="line">    smoothedRewardsQLearning[i] = np.mean(rewardsQLearning[i - averageRange: i + <span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="comment"># display optimal policy</span></div><div class="line">print(<span class="string">'Sarsa Optimal Policy:'</span>)</div><div class="line">printOptimalPolicy(stateActionValuesSarsa)</div><div class="line">print(<span class="string">'Q-Learning Optimal Policy:'</span>)</div><div class="line">printOptimalPolicy(stateActionValuesQLearning)</div></pre></td></tr></table></figure><p>The results are as follows (emits the results of the changes of reward):</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_opti_policy.png" alt="cliff_walk_optimal_policy"></p><p>Now let us compare the three algorithms:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">stepSizes = np.arange(<span class="number">0.1</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line">    nEpisodes = <span class="number">1000</span></div><div class="line">    runs = <span class="number">10</span></div><div class="line"></div><div class="line">    ASY_SARSA = <span class="number">0</span></div><div class="line">    ASY_EXPECTED_SARSA = <span class="number">1</span></div><div class="line">    ASY_QLEARNING = <span class="number">2</span></div><div class="line">    INT_SARSA = <span class="number">3</span></div><div class="line">    INT_EXPECTED_SARSA = <span class="number">4</span></div><div class="line">    INT_QLEARNING = <span class="number">5</span></div><div class="line">    methods = range(<span class="number">0</span>, <span class="number">6</span>)</div><div class="line"></div><div class="line">    performace = np.zeros((<span class="number">6</span>, len(stepSizes)))</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        <span class="keyword">for</span> ind, stepSize <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(stepSizes)), stepSizes):</div><div class="line">            stateActionValuesSarsa = np.copy(stateActionValues)</div><div class="line">            stateActionValuesExpectedSarsa = np.copy(stateActionValues)</div><div class="line">            stateActionValuesQLearning = np.copy(stateActionValues)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'step size:'</span>, stepSize, <span class="string">'episode:'</span>, ep)</div><div class="line">                sarsaReward = sarsa(stateActionValuesSarsa, expected=<span class="keyword">False</span>, stepSize=stepSize)</div><div class="line">                expectedSarsaReward = sarsa(stateActionValuesExpectedSarsa, expected=<span class="keyword">True</span>, stepSize=stepSize)</div><div class="line">                qLearningReward = qLearning(stateActionValuesQLearning, stepSize=stepSize)</div><div class="line">                performace[ASY_SARSA, ind] += sarsaReward</div><div class="line">                performace[ASY_EXPECTED_SARSA, ind] += expectedSarsaReward</div><div class="line">                performace[ASY_QLEARNING, ind] += qLearningReward</div><div class="line"></div><div class="line">                <span class="keyword">if</span> ep &lt; <span class="number">100</span>:</div><div class="line">                    performace[INT_SARSA, ind] += sarsaReward</div><div class="line">                    performace[INT_EXPECTED_SARSA, ind] += expectedSarsaReward</div><div class="line">                    performace[INT_QLEARNING, ind] += qLearningReward</div><div class="line"></div><div class="line">    performace[:<span class="number">3</span>, :] /= nEpisodes * runs</div><div class="line">    performace[<span class="number">3</span>:, :] /= runs * <span class="number">100</span></div><div class="line">    labels = [<span class="string">'Asymptotic Sarsa'</span>, <span class="string">'Asymptotic Expected Sarsa'</span>, <span class="string">'Asymptotic Q-Learning'</span>,</div><div class="line">              <span class="string">'Interim Sarsa'</span>, <span class="string">'Interim Expected Sarsa'</span>, <span class="string">'Interim Q-Learning'</span>]</div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    <span class="keyword">for</span> method, label <span class="keyword">in</span> zip(methods, labels):</div><div class="line">        plt.plot(stepSizes, performace[method, :], label=label)</div><div class="line">    plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">    plt.ylabel(<span class="string">'reward per episode'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_3_results.png" alt="compare3algo_cliff_walk"></p><p>As an on-policy method, Expected Sarsa retains the signiﬁcant advantage of Sarsa over Q-learning on this problem. In addition, Expected Sarsa shows a signiﬁcant improvement over Sarsa over a wide range of values for the step-size parameter α. In cliﬀ walking the state transitions are all deterministic and all randomness comes from the policy. In such cases, Expected Sarsa can safely set α = 1 without suﬀering any degradation of asymptotic performance, whereas Sarsa can only perform well in the long run at a small value of α, at which short-term performance is poor. In this and other examples there is a consistent empirical advantage of Expected Sarsa over Sarsa. Except for the small additional computational cost, Expected Sarsa may completely dominate both of the other more-well-known TD control algorithms.</p><h3 id="Double-Q-learning"><a href="#Double-Q-learning" class="headerlink" title="Double Q-learning"></a>Double Q-learning</h3><p>All the control algorithms that we have discussed so far involve maximization in the construction of their target policies. For example, in Q-learning the target policy is the greedy policy given the current action values, which is deﬁned with a max, and in Sarsa the policy is often ε-greedy, which also involves a maximization operation. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a signiﬁcant positive bias. To see why, consider a single state s where there are many actions a whose true values, $q(s, a)$ are all zero but whose estimated values, $Q(s, a)$, are uncertain and thus distributed some above and some below zero. The maximum of the true values is zero, but the maximum of the estimates is positive, a positive bias. We call this maximization<br>bias.</p><h4 id="Example-Maximization-Bias"><a href="#Example-Maximization-Bias" class="headerlink" title="Example: Maximization Bias"></a>Example: Maximization Bias</h4><p>We have a small MDP:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/mb.png" alt="mb"></p><p>the expected return for any trajectory starting with left (from <strong>B</strong>) is −0.1, and thus taking left in state A is always a mistake. Nevertheless, our control methods may favor left because of maximization bias making B appear to have a positive value. The results (paste later) shows that Q-learning with ε-greedy action selection initially learns to strongly favor the left action on this example. Even at asymptote, Q-learning takes the left action about 5% more often than is optimal at our parameter settings (ε = 0.1, α = 0.1, and γ = 1).</p><p>We could use the Double Q-learning algorithm to avoid this problem. One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. Suppose we divided the plays in two sets and used them to learn two independent estimates.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/dbq.png" alt="dbq"></p><p>Of course there are also doubled versions of Sarsa and Expected Sarsa.</p><p>Now let us develop the both algorithms and compare their performance on the earlier example. First we define the problem environment:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># state A</span></div><div class="line">STATE_A = <span class="number">0</span></div><div class="line"></div><div class="line"><span class="comment"># state B</span></div><div class="line">STATE_B = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># use one terminal state</span></div><div class="line">STATE_TERMINAL = <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># starts from state A</span></div><div class="line">STATE_START = STATE_A</div><div class="line"></div><div class="line"><span class="comment"># possible actions in A</span></div><div class="line">ACTION_A_RIGHT = <span class="number">0</span></div><div class="line">ACTION_A_LEFT = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># possible actions in B, maybe 10 actions</span></div><div class="line">actionsOfB = range(<span class="number">0</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">stateActions = [[ACTION_A_RIGHT, ACTION_A_LEFT], actionsOfB]</div><div class="line"></div><div class="line"><span class="comment"># state action pair values, if a state is a terminal state, then the value is always 0</span></div><div class="line">stateActionValues = [np.zeros(<span class="number">2</span>), np.zeros(len(actionsOfB)), np.zeros(<span class="number">1</span>)]</div><div class="line"></div><div class="line"><span class="comment"># set up destination for each state and each action</span></div><div class="line">actionDestination = [[STATE_TERMINAL, STATE_B], [STATE_TERMINAL] * len(actionsOfB)]</div><div class="line"></div><div class="line"><span class="comment"># probability for exploration</span></div><div class="line">EPSILON = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># step size</span></div><div class="line">ALPHA = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># discount for max value</span></div><div class="line">GAMMA = <span class="number">1.0</span></div></pre></td></tr></table></figure><p>And we need a policy to take an action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># choose an action based on epsilon greedy algorithm</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseAction</span><span class="params">(state, stateActionValues)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(stateActions[state])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> argmax(stateActionValues[state])</div></pre></td></tr></table></figure><p>After take an action, we get the reward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take @action in @state, return the reward</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(state, action)</span>:</span></div><div class="line">    <span class="keyword">if</span> state == STATE_A:</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> np.random.normal(<span class="number">-0.1</span>, <span class="number">1</span>)</div></pre></td></tr></table></figure><p>Next, we develop the Double Q-learning algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># if there are two state action pair value array, use double Q-Learning</span></div><div class="line"><span class="comment"># otherwise use normal Q-Learning</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">qLearning</span><span class="params">(stateActionValues, stateActionValues2=None)</span>:</span></div><div class="line">    currentState = STATE_START</div><div class="line">    <span class="comment"># track the # of action left in state A</span></div><div class="line">    leftCount = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> currentState != STATE_TERMINAL:</div><div class="line">        <span class="keyword">if</span> stateActionValues2 <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            currentAction = chooseAction(currentState, stateActionValues)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># derive a action form Q1 and Q2</span></div><div class="line">            currentAction = chooseAction(currentState, [item1 + item2 <span class="keyword">for</span> item1, item2 <span class="keyword">in</span> zip(stateActionValues, stateActionValues2)])</div><div class="line">        <span class="keyword">if</span> currentState == STATE_A <span class="keyword">and</span> currentAction == ACTION_A_LEFT:</div><div class="line">            leftCount += <span class="number">1</span></div><div class="line">        reward = takeAction(currentState, currentAction)</div><div class="line">        newState = actionDestination[currentState][currentAction]</div><div class="line">        <span class="keyword">if</span> stateActionValues2 <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            currentStateActionValues = stateActionValues</div><div class="line">            targetValue = np.max(currentStateActionValues[newState])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">                currentStateActionValues = stateActionValues</div><div class="line">                anotherStateActionValues = stateActionValues2</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                currentStateActionValues = stateActionValues2</div><div class="line">                anotherStateActionValues = stateActionValues</div><div class="line">            bestAction = argmax(currentStateActionValues[newState])</div><div class="line">            targetValue = anotherStateActionValues[newState][bestAction]</div><div class="line"></div><div class="line">        <span class="comment"># Q-Learning update</span></div><div class="line">        currentStateActionValues[currentState][currentAction] += ALPHA * (</div><div class="line">            reward + GAMMA * targetValue - currentStateActionValues[currentState][currentAction])</div><div class="line">        currentState = newState</div><div class="line">    <span class="keyword">return</span> leftCount</div></pre></td></tr></table></figure><p>And now, let us solve the example problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># each independent run has 300 episodes</span></div><div class="line">    episodes = <span class="number">300</span></div><div class="line">    leftCountsQ = np.zeros(episodes)</div><div class="line">    leftCountsDoubleQ = np.zeros(episodes)</div><div class="line">    runs = <span class="number">1000</span></div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        print(<span class="string">'run:'</span>, run)</div><div class="line">        stateActionValuesQ = [np.copy(item) <span class="keyword">for</span> item <span class="keyword">in</span> stateActionValues]</div><div class="line">        stateActionValuesDoubleQ1 = [np.copy(item) <span class="keyword">for</span> item <span class="keyword">in</span> stateActionValues]</div><div class="line">        stateActionValuesDoubleQ2 = [np.copy(item) <span class="keyword">for</span> item <span class="keyword">in</span> stateActionValues]</div><div class="line">        leftCountsQ_ = [<span class="number">0</span>]</div><div class="line">        leftCountsDoubleQ_ = [<span class="number">0</span>]</div><div class="line">        <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            leftCountsQ_.append(leftCountsQ_[<span class="number">-1</span>] + qLearning(stateActionValuesQ))</div><div class="line">            leftCountsDoubleQ_.append(leftCountsDoubleQ_[<span class="number">-1</span>] + qLearning(stateActionValuesDoubleQ1, stateActionValuesDoubleQ2))</div><div class="line">        <span class="keyword">del</span> leftCountsQ_[<span class="number">0</span>]</div><div class="line">        <span class="keyword">del</span> leftCountsDoubleQ_[<span class="number">0</span>]</div><div class="line">        leftCountsQ += np.asarray(leftCountsQ_, dtype=<span class="string">'float'</span>) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">        leftCountsDoubleQ += np.asarray(leftCountsDoubleQ_, dtype=<span class="string">'float'</span>) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">    leftCountsQ /= runs</div><div class="line">    leftCountsDoubleQ /= runs</div><div class="line">    plt.figure()</div><div class="line">    plt.plot(leftCountsQ, label=<span class="string">'Q-Learning'</span>)</div><div class="line">    plt.plot(leftCountsDoubleQ, label=<span class="string">'Double Q-Learning'</span>)</div><div class="line">    plt.plot(np.ones(episodes) * <span class="number">0.05</span>, label=<span class="string">'Optimal'</span>)</div><div class="line">    plt.xlabel(<span class="string">'episodes'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% left actions from A'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Ok, results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/dbq_result.png" alt="dbq_result"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be &lt;em&gt;temporal-difference&lt;/em&gt; (TD)
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
      <category term="TD" scheme="http://yoursite.com/tags/TD/"/>
    
  </entry>
  
  <entry>
    <title>Reinforcement Learning Resources</title>
    <link href="http://yoursite.com/2017/06/30/Reinforcement-Learning-Resources/"/>
    <id>http://yoursite.com/2017/06/30/Reinforcement-Learning-Resources/</id>
    <published>2017-06-30T13:07:44.000Z</published>
    <updated>2017-07-12T04:54:24.816Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Books"><a href="#Books" class="headerlink" title="Books"></a>Books</h1><ul><li>Sutton’s book has new <a href="https://github.com/ewanlee/RL-Resources/blob/master/book/bookdraft2017june.pdf" target="_blank" rel="external">update</a> (draft, version 2017) !</li><li><a href="https://github.com/ewanlee/RL-Resources/blob/master/book/RLAlgsInMDPs.pdf" target="_blank" rel="external">Algorithms for Reinforcement Learning (Morgan)</a></li></ul><h1 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h1><ul><li><a href="https://arxiv.org/pdf/1509.06461.pdf" target="_blank" rel="external">Deep Reinforcement Learning with Double Q-learning</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Deep-Reinforcement-Learning-with-Double-Q-learning" target="_blank" rel="external">Summary</a></li><li><a href="https://ewanlee.github.io/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/" target="_blank" rel="external">Project</a></li></ul></li><li><a href="http://arxiv.org/abs/1511.05952" target="_blank" rel="external">Prioritized Experience Replay</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Prioritized-Experience-Replay" target="_blank" rel="external">Summary</a></li></ul></li><li><a href="https://arxiv.org/pdf/1511.06581.pdf" target="_blank" rel="external">Dueling Network Architectures for Deep Reinforcement Learning</a><ul><li><a href="https://ewanlee.github.io/2017/07/08/Summary-of-the-papers/#Dueling-Network-Architectures-for-Deep-Reinforcement-Learning" target="_blank" rel="external">Summary</a></li></ul></li></ul><h1 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h1><ul><li><a href="https://ewanlee.github.io/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" target="_blank" rel="external">Using Keras and Deep Q-Network to Play FlappyBird</a></li></ul><h1 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h1><ul><li><a href="https://ewanlee.github.io/2017/07/07/Demystifying-Deep-Reinforcement-Learning/" target="_blank" rel="external">Demystifying Deep Reinforcement Learning</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Books&quot;&gt;&lt;a href=&quot;#Books&quot; class=&quot;headerlink&quot; title=&quot;Books&quot;&gt;&lt;/a&gt;Books&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;Sutton’s book has new &lt;a href=&quot;https://github.com/ewa
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>A simple AI car</title>
    <link href="http://yoursite.com/2017/06/27/A-simple-AI-car/"/>
    <id>http://yoursite.com/2017/06/27/A-simple-AI-car/</id>
    <published>2017-06-27T05:56:15.000Z</published>
    <updated>2017-06-28T04:11:01.612Z</updated>
    
    <content type="html"><![CDATA[<h3 id="I-定义"><a href="#I-定义" class="headerlink" title="I. 定义"></a>I. 定义</h3><h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>项目地址：<a href="https://github.com/ewanlee/rl_car" target="_blank" rel="external">https://github.com/ewanlee/rl_car</a></p><p>最近，自动驾驶汽车十分火热。但是，自动驾驶问题是一个机器学习集大成的问题，十分的复杂。因此，我们希望可以设计出一个简单的学习环境能够对自动驾驶问题进行模拟，并且不需要GPU （主要是太贵）。</p><p>我们的学习环境借鉴了Matt Harvey’s virtual car[1] 的环境设置。运用了 TensorFlow， Python 2.7 以及 PyGame 5.0. 本项目中运用了深度Q强化学习算法，但是为了符合我们上面提到的要求，我们去掉了该算法中 “深度” 的部分。代码设计的一些思想借鉴了 songotrek’s Q学习算法的TensorFlow实现 [2].</p><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/game.jpg" alt="fig1"></p><p><em>图片来源于[1]</em></p><p>我们所要解决的问题就是设计一个算法使得模拟小车能够自动行驶。</p><p>上图就是我们实验用的环境。可以看出，它足够简单，但是足够进行一些强化学习算法的验证。最小的圆圈是我们模拟的小车，它拥有三个声纳感应器 （三条白色的虚线）。三个较大的圆圈代表障碍物，它会随着时间的变化缓慢移动。左上角的圆圈代表一只在环境中游走（速度相比于障碍物要快很多）的猫。圆圈上的缺口表示朝向。我们所要解决的问题就是希望小车可以尽可能长时间的运动，但不会撞到障碍物或者猫。</p><h4 id="环境需求"><a href="#环境需求" class="headerlink" title="环境需求"></a>环境需求</h4><ul><li>Anaconda Python Distribution 2.7 [3]</li><li>TensorFlow for Anaconda [4]</li><li>PyGame [5]，用于展示图形界面</li><li>PyMunk [6]，为了模拟游戏中的物理环境</li><li>Numpy [7]</li><li>Scipy [8]</li></ul><p>实验运行的环境为 Ubuntu 16.04 LTS 虚拟机， 虚拟机为VMware Workstation 12.5.2 build-4638234。虚拟机运行在Windows 10 Pro上。</p><h4 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h4><p>我们的baseline是一个随机（行为随机选择）小车，最后的评价指标是我们定义的指标score，代表小车存活的时间（在游戏中代表小车存活的frame）。并且，score是进行1000次实验的平均值。</p><h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>我们使用的是 Deep Q Learning [9] 论文中定义的 QMax 值。</p><p>QMax 值指的是在一定时间范围内，对于所有的训练样本，Q 函数（使用神经网络进行拟合）输出的最大的 Q-value。随着agent（模拟小车）不断进行学习，它将采取更加优秀的策略，因此存活时间会更长，那么 Q-value (在我们的实验中便是score) 会越大。如果我们的优化目标是增大 Q-value 的上界，也便相应的增大了 Q-value 值。</p><h4 id="学习过程监测"><a href="#学习过程监测" class="headerlink" title="学习过程监测"></a>学习过程监测</h4><p>我们使用的是Tensorflow自带的TensorBoard来监测QMax以及最大score的变化情况（希望整体趋势是逐渐增大的）</p><p>下面是运行过程中的截图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl_car/tensdorboard_sing.png" alt="s"></p><p>下面是各网络参数的分布变化情况：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl_car/tensdorboard_h.png" alt="h"></p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p>由于强化学习任务的数据集一般都是实验中产生的，因此不需要收集数据。在每一次迭代过程中，模拟环境提供以下数据（自己设计的）：</p><ul><li>s1, s2, s3 三个声纳传感器的数值，范围是[0, 40]，整数值，代表三个方向上障碍物的距离。范围确定为这样的原因是，为了检测障碍物，声纳传感器从源头开始，逐渐往外探测，每向外探测一次，距离就加1（可以看成虚线的点数，即虚线是由多少个点组成的）。</li><li>x 代表x轴的位置，范围是[0, 1]</li><li>y 代表y轴的位置，范围是[0, 1]</li><li>theta 代表小车的方向，弧度表示，范围是[0, 2$\pi$]</li></ul><p>小车能够采取的动作如下：</p><ul><li>0，代表直走</li><li>1， 往左转0.2弧度</li><li>2， 往右转0.2弧度</li></ul><p>小车每进行一次动作会使得状态发生变化，并且有以下返回值：</p><ul><li>Reward，一个[-100, 10]之间的整数，负数代表动作产生的结果不好，正数则相反</li><li>Termianl，布尔型数据，代表小车是否存活（是否撞到障碍物）</li></ul><p>我们和原始模型[1]不同的是，输了$s_1, s_2, s_3$三个特征之外，额外增加了$x, y, theta$三个特征。因为我们希望小车能够尽可能往地图中间运行，远离墙壁。并且当它们靠近障碍物时，能够选择更加合理的方向躲避。</p><p>值得说明的一点是，小车如何检测是否撞到障碍物的问题。实验中使用的方法是检测声纳传感器的数值，如果数值是1（而不是0）就认为小车撞上了障碍物，并给出一个-100的reward。此时实验将重新开始，小车位置的选择是根据物理定律模拟的，即根据碰撞的角度给小车一个反向的速度，并且小车的朝向随机变化。这样模拟出一种碰撞后的混乱状态。</p><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>下面介绍Deep Q-learning算法。</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/rel.png" alt="Reinforcement Learning"></p><p>以上的实验环境可以形式化的表述，如上图所示。我们拥有一个agent（小车），在时间$t$时必须要选择一个动作$a_t$。Agent采取动作与环境进行交互，使得在时间$t+1$时状态变为$s_{t+1}$。同时agent接收到环境给它的一个反馈$r_t$。这样agent就根据$(s_t, a_t, s_{t+1}, r_t)$来决定采取的动作$a_{t+1}$是什么。整个问题就是不断重复上述过程直到到达某个结束条件。</p><p>机器学习领域将这个问题称为强化学习问题。每一个动作通过reward被 “强化”，使得agent不断接近我们期望它到达的状态。但是在强化学习中存在一个reward延迟的问题，也就是说，某一个action的回报可能不是即时的，需要很多时间步之后才能确定。举个例子，下棋的过程中需要布局，但是这个布局并不会马上给你带来好处，需要在以后的某个特定时间，你的对上掉入了你很久前设置的陷阱里，这时候才给你带来好处。所以，我们需要采用一种方式来对这个问题进行建模。我们定义一个价值函数$Q(s_t, a_t)$，它表示在状态$s_t$是采取$a_t$这个动作带来的 “价值”，而不是reward，reward是即时的，但是价值是若干时间步带来的reward的某种综合考量，更具实际意义。那么接下来的问题就是价值函数应当如何定义。</p><p>最直观的想法就是，我们可以把强化学习问题定义为一个动态规划的问题。这里我直接列出公式，也就是非常著名的贝尔曼方程（Bellman equation）：</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/bellman.png" alt="Bellman equation"></p><p>可以看到，解决强化学习问题是一个不断迭代的过程，那么如何初始化Q非常重要。但实际上，如果迭代次数趋紧无穷大时，Q的初始值对于最终的结果并没有影响，因此一般来说只要初始化为均值为0的高斯白噪音。</p><p>对于小规模的强化学习问题，由于状态的Q值随着迭代次数的增加会不断更新，那么我们需要一个地方来存储这些值。传统的强化学习算法一般采用一张表格（数组或字典）来存储这些值。但是随着问题规模的增大，状态会显著增加。对于我们的问题，状态空间更是无限的，因为状态是由浮点数组成的。这样我们就不可能把这些状态对应的Q值都存储下来。</p><p>我们采用一个如下所示的神经网络来代替这些表格，即找出状态和Q值之间的一个映射。这里值得说明的是，网络输出的是所有动作对应的Q值，这是Deep Q-learning算法的一个创新点。</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/network.png" alt="Neural Network"></p><p>在我们的实验中，输入维度是6维（$s_1, s_2, s_3, x, y, theta$），输出是3维（对应三个动作0， 1， 2）。我们采用白噪音来初始化网络。具体来说，权重采用标准高斯噪音，偏差初始化为0.01。</p><p>至于训练过程，Deep Q-learning算法采用了一个trick。该算法采用了两个完全相同的网络，其中一个用来训练，另一个则用来预测。这样还可以防止过拟合。用于训练网络的训练集并不是agent当前的四元组$(s_t, a_t, s_{t+1}, r_t)$， 而是从最近四元组历史（之前某一个时间窗口中的所有四元组）中随机采样出的一个minibatch。我们通过这些训练样本来更新训练网络的参数，经过一定时间的训练之后，把训练网络的参数复制给预测网络，用预测网络来继续产生训练样本，供训练网络使用。整个算法就是不断重复上述过程直至收敛。具体算法的伪代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">Initialize replay memory D to size N</div><div class="line">Initialize action-value function Q <span class="keyword">with</span> random weights</div><div class="line"><span class="keyword">for</span> episode = <span class="number">1</span>, M do</div><div class="line">	Initialize state s_1</div><div class="line">	<span class="keyword">for</span> t = <span class="number">1</span>, T do</div><div class="line">		With probability ϵ select random action a_t</div><div class="line">		otherwise select a_t=argmax_a  Q(s_t,a; θ_i)</div><div class="line">		Execute action a_t <span class="keyword">in</span> emulator <span class="keyword">and</span> observe r_t <span class="keyword">and</span> s_(t+<span class="number">1</span>)</div><div class="line">		Store transition (s_t,a_t,r_t,s_(t+<span class="number">1</span>)) <span class="keyword">in</span> D</div><div class="line">		Sample a minibatch of transitions (s_j,a_j,r_j,s_(j+<span class="number">1</span>)) <span class="keyword">from</span> D</div><div class="line">		Set y_j:=</div><div class="line">			r_j <span class="keyword">for</span> terminal s_(j+<span class="number">1</span>)</div><div class="line">			r_j+γ*max_(a^<span class="string">') Q(s_(j+1),a'</span>; θ_i) <span class="keyword">for</span> non-terminal s_(j+<span class="number">1</span>)</div><div class="line">		Perform a gradient step on (y_j-Q(s_j,a_j; θ_i))^<span class="number">2</span> <span class="keyword">with</span> respect to θ</div><div class="line">	end <span class="keyword">for</span></div><div class="line">end <span class="keyword">for</span></div></pre></td></tr></table></figure><h4 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h4><p>我们希望算法能够比随机选择更好。下面是进行1000次实验随机算法的结果：</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/table1.png" alt="Table1"></p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>我们在实验之前进行了数据的标准化，使得所有数据都处于0到1之间，这样可以避免梯度爆炸等现象的发生。</p><p>$x， y$ 这两个特征没有进行标准化，因为已经符合要求。$theta$通过除以$2\pi$进行标准化。在没有进行标准化之前，我们在实验中发现，$theta$的值会达到$10^3$这个数量级，使得网络发生了bias shift现象。$s_1, s_2, s_3$通过除以40来进行标准化。</p><p>我们同样试着能够将reward也进行标准化，将其范围缩小到[-1, 1]。因为DQN论文中同样使用了这种方法，使得该算法应用在不同的Atari游戏上时不用对算法进行参数的调整。但是，我们在网络训练的前一百万步并没有发现性能有明显的提升。因为reward的值更大的话，学习将会更容易，这样reward信号会更加明显，不会被淹没在网络的高斯噪声中。所以我们希望reward能够大一点，但是多大比较合适又是一个问题。</p><p>我们所借鉴的算法[1]，将这个reward的最小值设置成了-500（小车撞上了障碍物），但我们实验发现这个值设置的过小（下面将会解释），所以最后的范围调整为[-100, 10] （通过裁剪）。我们把这个过程称之为reward正则化。</p><h4 id="Reward-正则化"><a href="#Reward-正则化" class="headerlink" title="Reward 正则化"></a>Reward 正则化</h4><p>在网络训练（反向传播）的过程中，我们希望最小化代价函数。我们的代价函数选为训练网络输出的Q值与训练样本的Q值之间的MSE。在试验过程中，我们发现，对于$s_1, s_2, s_3$值都比较大的状态，其reward都会落在[0, 40]的范围内，并且均值为20。但是网络刚开始训练时，输出值为均值为0的高斯噪声。也就是说初始的loss处于[400-1600]的范围内（由于最后的loss需要除以样本的数量，所以loss等于一个样本的loss）。</p><p>现在我们假定网络处于一个最优点附近，这时候小车突然撞上了某个障碍物，那么唯一的可能就是猫出现在了小车后面。这时候就会引入一个250000的loss（如果将reward的最小值设置为-500）。但是网络初始时的loss都只处于[400, 1600]的范围内，这个loss是初始loss的100倍。这么大的loss所引入的梯度将会使得网络走一段非常大的距离，这就很可能直接跳过了局部最优点。不断如此的话，网络就会震荡的非常厉害。</p><p>让我们用数学的观点来解释这个问题。当reward的负值设置的过大，将会使得原始问题空间距离最优空间有一个非常大的偏差，很难通过梯度下降靠近。这个大的偏差在问题空间创造了一些非常陡峭的cliff。就像我们爬山一样，好不容易爬到了山顶附近，一不小心就掉下了悬崖，那么我们只能一步一步非常慢的爬上来，花很久的时间才能到达刚才的位置。如果一不小心又掉下去了，那么又要重新爬。</p><p>因此，减小reward的范围十分重要，这样可以减小cliff的坡度，使得网络训练更快更容易。但是又不能太小，以免被噪声淹没。最后我们选定了[-100, 10]这个范围。</p><h4 id="模型迭代过程"><a href="#模型迭代过程" class="headerlink" title="模型迭代过程"></a>模型迭代过程</h4><p>我们最开始直接采用现成的模型，是一个两层的神经网络（不包括输入层），效果已经不错了，但是小车总是撞上障碍物。因此我们做了一些改变：</p><ul><li>类似DQN，我们使用了最近四次的state，将其映射为一个input，这使得我们的QMax值提高到了120</li><li>我们继续进行改变，从使用最近四次改为最近16次，使得我们的QMax值提高到了140</li><li>我们尝试了使用一个更小的网络进行学习（2层，每层32维），并且只使用一个state进行输入，但是结果比随机算法更差。</li><li>继续尝试使用grid search选择模型，还是两层网络，每一层的维数从32到512，训练迭代次数为200, 000，但是最后的QMax值还是不能超过140。</li><li>我们尝试了更小的时间窗口，更大的minibatch，网络训练时震荡的十分厉害</li><li>我们尝试在小车的背面增加一个声纳传感器，发i按网络训练速度变快了，但是最后的QMax值还是不能达到更高。</li></ul><p>这些尝试说明应当是两层网络的特征表达能力不够，我们尝试使用更深的网络。最后使用的网络有8层（不算输入输出层），输入层和输出层各有32维，中间6层为64维。最后取得了很好的效果，QMax达到了之前的10倍。</p><p>同时，我们在每一层网络后都加入了一个20%的dropout层（除了输入层以及输出层之前），激活函数选用的ReLU函数。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>算法的训练过程如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">In(<span class="number">4</span>): ai.cycle()</div><div class="line">t= <span class="number">11000</span></div><div class="line">[<span class="number">654.53412</span>, <span class="number">322.84866</span>, <span class="number">86.578796</span>, <span class="number">1414.0239</span>]</div><div class="line">Games played  <span class="number">539</span></div><div class="line">Epoch Max score <span class="number">144</span></div><div class="line">Epoch Mean score <span class="number">30.3580705009</span></div><div class="line">t= <span class="number">21000</span></div><div class="line">[<span class="number">474.16202</span>, <span class="number">251.2959</span>, <span class="number">79.489487</span>, <span class="number">1243.3118</span>]</div><div class="line">Games played  <span class="number">774</span></div><div class="line">Epoch Max score <span class="number">223</span></div><div class="line">Epoch Mean score <span class="number">42.6255319149</span></div><div class="line">t= <span class="number">31000</span></div><div class="line">[<span class="number">388.32297</span>, <span class="number">202.05305</span>, <span class="number">79.290771</span>, <span class="number">1086.0581</span>]</div><div class="line">Games played  <span class="number">1020</span></div><div class="line">Epoch Max score <span class="number">153</span></div><div class="line">Epoch Mean score <span class="number">40.5081300813</span></div><div class="line">t= <span class="number">41000</span></div><div class="line">[<span class="number">470.96552</span>, <span class="number">234.70471</span>, <span class="number">129.87579</span>, <span class="number">1320.3688</span>]</div><div class="line">Games played  <span class="number">1281</span></div><div class="line">Epoch Max score <span class="number">251</span></div><div class="line">Epoch Mean score <span class="number">38.3908045977</span></div><div class="line">t= <span class="number">51000</span></div><div class="line">[<span class="number">549.32666</span>, <span class="number">203.20442</span>, <span class="number">176.22263</span>, <span class="number">1079.8307</span>]</div><div class="line">Games played  <span class="number">1546</span></div><div class="line">Epoch Max score <span class="number">226</span></div><div class="line">Epoch Mean score <span class="number">37.7773584906</span></div><div class="line">t= <span class="number">61000</span></div><div class="line">[<span class="number">610.16583</span>, <span class="number">232.79211</span>, <span class="number">224.97626</span>, <span class="number">1264.9712</span>]</div><div class="line">Games played  <span class="number">1759</span></div><div class="line">Epoch Max score <span class="number">484</span></div><div class="line">Epoch Mean score <span class="number">46.5774647887</span></div><div class="line">...</div></pre></td></tr></table></figure><p>实验结果：</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/table2.png" alt="Table2"></p><p>可以看出，我们的算法性能完全超越了随机算法。</p><p>下面是我们训练大概250,000次后的结果：</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/qmax_win.png" alt="Best Qmax"></p><p>关于随机算法以及Q-learning算法的动画展示可以参照项目地址。</p><p>但是我们发现小车还是会撞到障碍物，这经常发生在小车碰撞之后的恢复过程中。这时候小车可能到达地图的角落，充满障碍物。但是因为小车只有三个传感器，即使在背面加上还是太少了，所以信息捕捉不够。这是模型需要改进的地方。我们可以事先在小车中存储一个类似于地图的数据。</p><p>另外，由于小车一直是匀速行驶，如果加入加速，减速等过程，应当会使得性能更好。但是由于时间原因，我们并没有进一步改进。</p><h4 id="进一步工作"><a href="#进一步工作" class="headerlink" title="进一步工作"></a>进一步工作</h4><p>本次实验仅仅是在二维环境中进行的。但是严格来说并不是复杂环境的最佳简化。三维环境更加贴近现实情况，例如我们可以设计一个飞行的环境模拟。</p><h3 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h3><p>[1]. <a href="https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.58wi2s7ct" target="_blank" rel="external">https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.58wi2s7ct</a></p><p>[2]. <a href="https://github.com/songrotek/DQN-Atari-Tensorflow/blob/master/BrainDQN_Nature.py" target="_blank" rel="external">https://github.com/songrotek/DQN-Atari-Tensorflow/blob/master/BrainDQN_Nature.py</a></p><p>[3]. <a href="https://www.continuum.io/why-anaconda" target="_blank" rel="external">https://www.continuum.io/why-anaconda</a></p><p>[4]. <a href="https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#anaconda-installation" target="_blank" rel="external">https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#anaconda-installation</a></p><p>[5]. <a href="http://www.pygame.org/wiki/GettingStarted" target="_blank" rel="external">http://www.pygame.org/wiki/GettingStarted</a></p><p>[6]. <a href="http://www.pymunk.org/en/latest/" target="_blank" rel="external">http://www.pymunk.org/en/latest/</a></p><p>[7]. <a href="http://www.numpy.org/" target="_blank" rel="external">http://www.numpy.org/</a></p><p>[8]. <a href="http://www.scipy.org/" target="_blank" rel="external">http://www.scipy.org/</a></p><p>[9]. <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" target="_blank" rel="external">https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;I-定义&quot;&gt;&lt;a href=&quot;#I-定义&quot; class=&quot;headerlink&quot; title=&quot;I. 定义&quot;&gt;&lt;/a&gt;I. 定义&lt;/h3&gt;&lt;h4 id=&quot;项目概述&quot;&gt;&lt;a href=&quot;#项目概述&quot; class=&quot;headerlink&quot; title=&quot;项目概述&quot;&gt;&lt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Store Management System</title>
    <link href="http://yoursite.com/2017/06/27/Store-Management-System/"/>
    <id>http://yoursite.com/2017/06/27/Store-Management-System/</id>
    <published>2017-06-27T03:32:49.000Z</published>
    <updated>2017-06-27T05:32:52.802Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SMS"><a href="#SMS" class="headerlink" title="SMS"></a>SMS</h1><p>SMS (Store Management System), 一个简单的网店管理系统。</p><p>源码：<a href="https://github.com/ewanlee/sms" target="_blank" rel="external">https://github.com/ewanlee/sms</a></p><p>这是一个用于展示微服务的 proof-of-concept 应用，运用了Spring Boot, Spring Cloud 以及 Docker部署。</p><h2 id="核心服务"><a href="#核心服务" class="headerlink" title="核心服务"></a>核心服务</h2><p>SHOP 分成了三个核心微服务，它们都是独立开发的，采用了Spring MVC架构：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/services.png" alt="services"></p><p>Order service</p><p>进行订单的添加，删除，以及显示</p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Path</th><th style="text-align:center">Description</th><th style="text-align:center">User authenticated</th><th style="text-align:center">Available from UI</th></tr></thead><tbody><tr><td style="text-align:center">GET</td><td style="text-align:center">/</td><td style="text-align:center">返回订单列表</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/form</td><td style="text-align:center">增加订单，并进行用户选择</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/line</td><td style="text-align:center">增加一条订单到数据库</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/{id}</td><td style="text-align:center">显示某一条订单的详情</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/</td><td style="text-align:center">增加订单行为</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">DELETE</td><td style="text-align:center">/{id}</td><td style="text-align:center">删除订单</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr></tbody></table><p>Customer service</p><p>进行用户的添加，删除，以及显示</p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Path</th><th style="text-align:center">Description</th><th style="text-align:center">User authenticated</th><th style="text-align:center">Available from UI</th></tr></thead><tbody><tr><td style="text-align:center">GET</td><td style="text-align:center">/list</td><td style="text-align:center">返回用户列表</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/{id}</td><td style="text-align:center">返回指定id的用户详情</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/form</td><td style="text-align:center">返回增加用户界面</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/form</td><td style="text-align:center">增加用户</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr><td style="text-align:center">PUT</td><td style="text-align:center">/{id}</td><td style="text-align:center">增加用户行为</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">DELETE</td><td style="text-align:center">/{id}</td><td style="text-align:center">删除用户</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr></tbody></table><p>Catalog service</p><p>进行商品的添加，删除，以及显示</p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Path</th><th style="text-align:center">Description</th><th style="text-align:center">User authenticated</th><th style="text-align:center">Available from UI</th></tr></thead><tbody><tr><td style="text-align:center">GET</td><td style="text-align:center">/list</td><td style="text-align:center">返回商品列表</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/{id}</td><td style="text-align:center">返回指定id的商品详情</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/form</td><td style="text-align:center">返回增加商品界面</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/form</td><td style="text-align:center">增加商品</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr><td style="text-align:center">PUT</td><td style="text-align:center">/{id}</td><td style="text-align:center">增加商品行为</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">DELETE</td><td style="text-align:center">/{id}</td><td style="text-align:center">删除商品</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">TEXT_HTML_VALUE</td><td style="text-align:center">/searchForm</td><td style="text-align:center">返回搜索界面</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">TEXT_HTML_VALUE</td><td style="text-align:center">/searchByName</td><td style="text-align:center">返回搜索结果</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr></tbody></table><p>注意</p><ul><li>每个微服务都有自己的数据库，因此互相之间没有直接访问数据库的接口</li><li>这里的数据库使用的是spring框架自带的数据库</li><li>服务到服务的通信非常简单，通过暴露的接口即可</li></ul><h2 id="架构服务"><a href="#架构服务" class="headerlink" title="架构服务"></a>架构服务</h2><p>分布式系统中有一些通用的模式，Spring Cloud框架都有提供，在本项目中仅仅运用了一小部分：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/arch.png" alt="arch"></p><h3 id="API-网关"><a href="#API-网关" class="headerlink" title="API 网关"></a>API 网关</h3><p>可以看到，有三个核心服务，它将外部API暴露给客户端。在一个现实世界的系统中，核心服务的数量可以非常快速地增长，并且整个系统的复杂性更是急剧增加。实际上，一个复杂的网页可能需要渲染数百个服务。</p><p>理论上，客户端可以直接向每个微服务器发出请求。但是显然，这将面临很大的挑战以及限制。比如必须要知道所有端点的地址。</p><p>通常一个更好的方法是使用API网关。它是系统中的单个入口点，用于通过将请求路由到适当的后端服务或通过调用多个后端服务并聚合结果来处理请求。此外，它还可以用于认证，压力测试，服务迁移，静态响应处理，主动流量管理等</p><p>Netflix开辟了这样一个优势服务，现在使用Spring Cloud，我们可以通过一个@EnableZuulProxy注释来实现。在这个项目中使用了Zuul存储静态内容（ui应用程序），并将请求路由到适当的微服务器。</p><p>Zuul使用服务发现机制来定位服务实例以及断路器和负载平衡器，如下所述。</p><h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p>另外一个众所周知的架构模式便是服务发现机制。它可以进行服务实例网络位置的动态检测。当应用需要扩展、容错或者升级的时候就可以自动为服务实例分配地址。</p><p>服务发现机制的核心是注册阶段。本项目使用了 Netflix Eureka。 Eureka是一个客户端的发现模式，因为很多网络应用都需要客户端自己去确定特定服务的地址（使用注册服务器）并且进行请求的负载均衡。</p><p>使用Spring Boot时，只要在pom文件中加入spring-cloud-starter-eureka-server依赖并且使用@EnableEurekaServer注解即可使用该服务。</p><h3 id="负载均衡、断路器以及Http客户端"><a href="#负载均衡、断路器以及Http客户端" class="headerlink" title="负载均衡、断路器以及Http客户端"></a>负载均衡、断路器以及Http客户端</h3><p>Netflix还提供了另外一些十分好用的工具。</p><h4 id="Ribbon"><a href="#Ribbon" class="headerlink" title="Ribbon"></a>Ribbon</h4><p>Ribbon 是一个客户端的负载均衡器。相比传统的均衡器，你可以之间链接到相关服务。Ribbon已经和Spring Cloud以及服务发现机制集成在了一起。 Eureka Client 提供了一个可用服务器的动态列表供 Ribbon 进行服务器之间的均衡。</p><h4 id="Hystrix"><a href="#Hystrix" class="headerlink" title="Hystrix"></a>Hystrix</h4><p>Hystrix 是断路器模式的具体实现，其可以调节网络访问依赖中经常出现的延迟以及错误。其主要目的是为了阻断在分布式环境中大量微服务极易出现的级联错误，使得系统尽快重新上线。Hystrix还提供了一个监控页面 （下面将会看到）。</p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>前期准备：</p><ul><li>网络</li></ul><ul><li>安装 Docker 以及 Docker compose</li></ul><p>运行命令：</p><ul><li><code>cd microservice-demo/</code>执行<code>mvn clean package</code></li><li><code>cd ../docker/</code>执行<code>docker-compose build</code>以及<code>docker-compose up</code></li></ul><p>重要端口：</p><ul><li><a href="http://127.0.0.1:8080" target="_blank" rel="external">http://127.0.0.1:8080</a> - 网关</li><li><a href="http://127.0.0.1:8761" target="_blank" rel="external">http://127.0.0.1:8761</a> - Eureka Dashboard</li></ul><p>注意：</p><p><strong>应用启动之后如果遇到 Whitelabel Error Page 错误请刷新页面</strong></p><h2 id="UI"><a href="#UI" class="headerlink" title="UI"></a>UI</h2><p><strong>Index</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/index_1.png" alt="index_1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/index_2.png" alt="index_2"></p><p><strong>Customer Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/customers_list.png" alt="customers_list"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/add_customer.png" alt="add_customer"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/successful.png" alt="success"></p><p><strong>Catalog Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/items_list.png" alt="items_list"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/add_item.png" alt="add_item"></p><p><strong>Order Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/orders_list.png" alt="orders_list"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/add_order.png" alt="add_order"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/order_details.png" alt="order_details"></p><p><strong>Eukera Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/eukera.png" alt="eukera"></p><p><strong>Hystrix Dashboard</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/order_hystrix.png" alt="order_hystrix"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;SMS&quot;&gt;&lt;a href=&quot;#SMS&quot; class=&quot;headerlink&quot; title=&quot;SMS&quot;&gt;&lt;/a&gt;SMS&lt;/h1&gt;&lt;p&gt;SMS (Store Management System), 一个简单的网店管理系统。&lt;/p&gt;&lt;p&gt;源码：&lt;a href=&quot;http
    
    </summary>
    
    
      <category term="microservice" scheme="http://yoursite.com/tags/microservice/"/>
    
      <category term="spring" scheme="http://yoursite.com/tags/spring/"/>
    
      <category term="spring boot" scheme="http://yoursite.com/tags/spring-boot/"/>
    
      <category term="spring cloud" scheme="http://yoursite.com/tags/spring-cloud/"/>
    
      <category term="docker" scheme="http://yoursite.com/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Learning to act by predicting the future</title>
    <link href="http://yoursite.com/2017/06/14/Learning-to-act-by-predicting-the-future/"/>
    <id>http://yoursite.com/2017/06/14/Learning-to-act-by-predicting-the-future/</id>
    <published>2017-06-14T11:46:19.000Z</published>
    <updated>2017-06-15T09:41:12.096Z</updated>
    
    <content type="html"><![CDATA[<p><em>论文 <a href="https://openreview.net/forum?id=rJLS7qKel" target="_blank" rel="external">Learning to act by predicting the future</a></em></p><p>这篇论文提出的 DFP (Direct Future Prediction) 赢得了2016年 Virtual Doom AI Competition 的 “Full Deathmatch” 环节的比赛。Virtual Doom 是一个对战性的第一人称射击型游戏，根据玩家击杀数判定胜负。为了体现出模型的泛化能力， 训练过程中使用的地图不在比赛过程中出现。DFP的性能超出了第二名（Deep LSTM Q-Network）50%，并且其模型以及训练数据更加简洁，表现出了DFP模型的优越性。</p><p>机器学习问题可以分为监督学习问题，无监督学习问题以及强化学习问题。监督学习主要是学习一个输入到输出的映射函数，无监督学习更加关注如何挖掘数据本身的隐含结构，强化学习是一个面向目标的策略学习问题。</p><p>因此采用强化学习的方法使得机器人在Deathmatch游戏中表现良好十分合适。因为这是一个直接面向目标的问题 （在游戏中取得最大的击杀数）。所以 DQN 以及 A3C 这样的算法应运而生，并且取得了巨大的成功。但是这篇论文提出了一个不同的观点。它引用了Jordan &amp; Rumelhart (1992) 这篇论文中提出的一个观点：</p><blockquote><p>对于一个可以与环境进行交互的学习问题，如果环境提供的反馈是稀疏的标量 （例如，对于一个五子棋问题，反馈只在最后胜负已分时给出，并且只是一个类似+1，-1的标量反馈），采用传统的强化学习算法会十分有效；但是如果环境给出的反馈是一个即时密集的多维度反馈 （在短时间内具有很大的信息比特率），监督学习算法更具优势。</p></blockquote><p>由于监督学习方面的研究已经非常成熟，最近十分火热的深度学习更是在很多方面都取得了很好的结果，因此，如果我们能够把强化学习问题在某种程度上转化为一个监督学习问题，可以使得问题的求解大大简化。</p><p>那么现在的问题是，我们要如何设计模型，从而可以得到一个监督信号呢？可以想到，我们唯一拥有的数据就是机器人通过与环境的交互得到的状态转移 （对于游戏来说就是玩家在游戏中采取不同的行为得到的环境的反馈，例如，玩家使用一个血包可以是的生命值回复；向左转可以使得画面发生变化等等）。我们可以对这些数据进行特殊的设计，从而能够满足我们的要求。</p><p>具体来说，我们不再简单使用传统强化学习问题中单一的状态 （例如游戏中的一帧画面）与对应的回报。我们把单一的状态拆分开来，对于原始的图像，声音等信息原样保留，形成一个 ”感觉输入流 (sensory input stream)“ ，很明显它是一个高维的变量；另外，我们从这些原始的信息中提取出能够代表我们学习目标的测度 （例如健康度，剩余弹药数以及击杀数等），形成一个 ”测度流 (measurement stream)“ ，它是一个低维的变量 （因为只包含几个重要的变量）。<strong>注意，这里的stream不是代表了好几个时间步，而是代表它是多个测度的一个集合。</strong></p><p>这样做有什么好处呢？一个传统的强化学习问题，其训练对象就是最大化一个关于reward的函数。一般reward都是人为给定的 （还是拿五子棋举例，最后玩家赢了，回报就是一个正数， 反之就是负数），但是这就使得学习问题的方差变得很大，训练过程十分不稳定，收敛速度慢，甚至可能不收敛。因此，<strong>我们希望reward的值不要过于随机化，能够通过某些监督信号来减少其方差</strong>。这里就可以体现出我们之前进行状态分解的优势。我们可以将reward表示成 measurement stream 的函数，由于measurement是agent与真实环境进行交互时得到的，属于一种监督信号，这很好的满足了我们的需求。所以最后我们的训练对象由最大化一个关于reward的函数变成了最大化一个关于measurement stream的函数。而这个<strong>measurement stream可以认为是传统强化学习问题中的reward</strong>。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>现在我们正式地定义DFP模型。在每一个时间步$t$，agent接收一个观察 （转移到一个状态）$O_t$, 根据这个观察 （状态）的某些固有属性从可行的动作集合中选取一个动作执行。$O_t$详细定义如下：<br>$$<br>\mathbf{o}_t = (\mathbf{s}_t, \mathbf{m}_t)<br>$$<br>整个状态转移过程中，我们希望最大化目标，前面提到了，它是关于measurement stream的函数：<br>$$<br>\mathbf{f} = (\mathbf{m}_{t+\tau_1}-\mathbf{m}_t, \cdots, \mathbf{m}_{t+\tau_n}-\mathbf{m}_t)<br>$$<br>$\tau_1, \cdots, \tau_n$ 代表与当前时间步$t$的一个偏差。至于为什么不直接最大化measurement stream而是最大化一个差值，我认为作者可能是有如下考虑：</p><ol><li>借鉴了n-step Q-learning 的做法。</li><li>由于模型是为了预测当前时间步$t$的measurement stream，因此优化对象中应该包含当前的measurement stream。</li></ol><p>最后，<br>$$<br>\mathbf{Goal} \; = \; u(\mathbf{f};\mathbf{g})<br>$$<br>一般线性函数即可满足我们的需求，即<br>$$<br>u(\mathbf{f};\mathbf{g}) = \mathbf{g}^{\text{T}}\mathbf{f}<br>$$<br>注意到现在我们的问题变成了一个监督学习的问题。为了训练模型，我们需要预测目标，然后再与真实的目标比较，通过最小化误差来进行学习。那么我们现在定义这个预测过程。注意到，由于目标只是measurement stream的函数，而且<strong>参数一般都是确定的，不需要进行学习</strong>。因此，我们的预测对象是measurement stream而不是目标。下面我们定义一个预测器F:<br>$$<br>\mathbf{p}_t^a = F(\mathbf{o}_t, a, \mathbf{g};\theta)<br>$$<br><strong>注意，这里的$\text{g}$和(4)中是不一样的，它代表目标</strong>。$p_t^a$代表在$t$时间步下，执行行为$a$所得到的reward，也即measurement stream。</p><p>当训练完成的时候，我们就要用这个预测器F进行决策，策略定义如下：<br>$$<br>a_t = {\arg\max}_{a \in \mathcal{A}} \mathbf{g}^{\text{T}}F(\mathbf{o}_t, a, \mathbf{g};\theta)<br>$$<br>注意到，模型实际训练的过程中采用的是$\varepsilon\text{-greedy}$策略。这里可以看出，在训练过程中或者测试过程中，我们要手动的计算出$u(\text{f};\text{g})$的值。下面我们详细的剖析模型的训练过程。</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>对于传统的强化学习算法，例如Q-learning，其训练过程是一个在线学习的过程，也即其训练集是一个一个进行输入的，每输入一次都进行一次参数的更新。由于Q-learning以及DFP都是采用了MC (Monte Carlo) 策略，这种训练过程可能十分不稳定 （由于训练最开始时我们的训练数据是通过一个随机策略与环境交互产生的），致使收敛速度很慢，需要很多的episodes进行训练。这里采用了和DQN (Deep Q-Network) 相同的 experience replay技术。具体来说，就是保存每次agent与环境交互后产生的数据对</p><p>$\langle \mathbf{o}_i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle$ 到数据集$\mathcal{D}$中，即$\mathcal{D} = \{\langle \mathbf{o}_ i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle \}_{i=1}^N$. 注意这里的$N$个数据对并不是直接顺序产生的，而是从当前episode中到当前时间步时，所有的数据对中选取最近的$M$个，再从其中随机抽样$N$个。另外，每隔k步才进行一次参数的更新，因为$\mathbf{f}$的计算需要考虑到32个时间步之后的数据，因此$k \ge 32$（实验部分将详细介绍）。DQN 给出了具体的实现：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/dqn_alg.png" alt="dqn_alg"></p><p>另外需要注意的是</p><p>有了训练集，我们现在定义w代价函数：<br>$$<br>\mathcal{L}(\theta) = \sum_{i=1}^{N} |F(\mathbf{o}_i, a_i, \mathbf{g}_i;\theta) - \mathbf{f}_i|^2<br>$$<br>我们来对比一下 DQN 的代价函数：<br>$$<br>L_i(\theta_i)= \mathbb{E}_{s, a \sim \rho(\cdot)} \left[ y_i - Q(s, a;\theta_i) \right],<br>$$<br>其中$y_i = \mathbb{E}_{s^{\prime} \sim \varepsilon}[ r + \gamma \max_{a^{\prime}} Q(s^{\prime}, a^{\prime};\theta_{i-1}) ]$ 。</p><p>这里的$y_i$是上一次模型的输出，其值随着更新次数的增加也在不断变化。因此从这里也能看出DFP是一个监督学习算法。</p><p>训练过程中我们为了解决报告最开始提出的目标随着时间发生改变的问题，采用了两种目标进行测试：</p><ol><li>目标向量$\mathbf{g}$ （不是目标）在整个训练过程中不变</li><li>目标向量在每个episode结束时随机变化</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>下图是DFP模型的网络结构：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/network.png" alt="network"></p><p>从图中可以看出，该网络有三个输入模块。一个感知模块$S(s)$，一个测度模型$M(m)$以及一个目标模块$G(g)$。在实验中，$s$代表一张图片，$S$代表一个卷积神经网络。测度模块以及目标模块都是由一个全连接神经网络构成。三者的输出连接在一起，形成一个联合的输入表示，供后续算法使用：<br>$$<br>\mathbf{j} = J(\mathbf{s, m, g}) = \langle S(\mathbf{s}), M(\mathbf{m}), G(\mathbf{g}) \rangle<br>$$<br>DFP网络采用了DQN的做法，一次性输出所有action对应的measurement stream。但是我们希望能够着重关注对action之间差异的学习。因此采用了Wang et al. (ICML 2016) 这篇文章中才去的做法，将预测模块分为两个stream，一个期望stream $E(\text{j})$ 以及一个action stream $A(\text{j})$。注意这两个stream都是一个全连接的神经网络。期望stream的目标是预测所有action能够获得的measurement stream的期望。Action stream关注不同action之间的差异。其中，$A(\text{j}) = \langle A^1(\text{j}), \cdots, A^{w}(\text{j}) \rangle$，$w = |\mathcal{A}|$代表所有可能action的个数。同时我们还在加入了一个正则化层：<br>$$<br>\overline{A^{i}}(\mathbf{j}) = A^{i}(\mathbf{j}) - \frac{1}{w}\sum_{k=1}^{w} A^{k}(\mathbf{j})<br>$$<br>正则化层对每一个action的预测值减去了所有action预测值的期望，这样就强制期望stream去学习这个期望，这样action stream就可以着重关注不同action之间的差异。最后，网络的输出如下：<br>$$<br>\mathbf{p} = \langle \mathbf{p}^{a_1}, \cdots, \mathbf{p}^{a_w} \rangle = \langle \overline{A^1}(\mathbf{j})+E(\mathbf{j}), \cdots, \overline{A^w}(\mathbf{j})+E(\mathbf{j}) \rangle<br>$$<br>为了验证网络中使用的三个辅助结构（measurement stream输入，expectation-action分解以及action正则化层）的作用，我们进行了测试。我们基于D3场景（下面实验部分提及）随机产生了100个地图场景用以训练。同时采用basic网络 （下面实验部分提及），最后的实验结果如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t3.png" alt="t3"></p><p>可以看出，expectation-action分解的作用最大，同时我们设计的measurement stream也是十分重要的。</p><h2 id="实验及结果"><a href="#实验及结果" class="headerlink" title="实验及结果"></a>实验及结果</h2><p>具体的实验场景见下图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/game.png" alt="game"></p><p>在前两个场景中，agent可以采取三个动作，向前移动、向左转、向右转。这样一共就有8种动作组合。采用的测度只有一种，就是血量。在后两个场景中，agent可以采取八个动作组合，分别是向前移动、向后移动、向左转、向右转、向左扫射，向右扫射、奔跑以及射击。这样一共就有256个动作组合。采用的测度一共有三种，血量，弹药数以及击杀数。这里我认为存在一个可以改进的地方，应该排除掉不合理的动作组合，例如同时向左转以及向右转。这样可以减少搜索空间，加速收敛，同时可以提高策略的质量。</p><p>实验中网络的结构与DQN的结构十分类似，参数也尽可能相近，就是为了比较起来比较公平。具体来说，实验中采用了两种网络，basic以及large，结构相同，但是参数数量不同：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/bl.png" alt="bl"></p><p>Basic网络的参数与DQN比较接近，以便比较。两个网络在所有的非终止层后都加入了一个非线性层，采用的激活函数为Leaky ReLU，具体函数为：<br>$$<br>\mathbf{LReLU}(x) = \max(x, 0.2x)<br>$$<br>参数初始化方法采用了He Initialization，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">W = np.random.randn(node_in, node_out) / np.sqrt(node_in / <span class="number">2</span>)</div></pre></td></tr></table></figure><p>Agent以episode为单位进行训练和测试。每一个episode拥有525个时间步（大约一分钟），如果agent死亡那么episode也会终止。同时将时间偏置$\tau_1, \cdots, \tau_n$设置为1, 2, 4, 8, 16, 32。最后结果表明只有最新的三个时间步（8, 16, 32）对结果有贡献，贡献比例为 1:1:2。</p><p>另外，输入图像被转换成灰度图像，measurement stream并不是直接输入，而是进行了正则化 （除以标准差）。同时，我们还在训练以及测试过程中使用frame skipping技术。Agent每隔4帧采取一次action。这些被忽略的帧所采取的action与其之后的帧的action一致，相当于进行了一次简单的复制。另外，由于人类的反应速度肯定是比不上计算机的，因此fram skipping使得agent的行为更加接近人类。</p><p>对于之前提到的experience replay技术，实验中将M值设为20000， N设为64，k也设为64（$\ge32$）。同时为了能够更高效的获得训练集$\mathcal{D}$，我们同时采用8个agent并行运行。训练时采用的梯度下降算法为Adam算法，参数设置如下：$\beta_1=0.95, \;\beta_2=0.999,\;\varepsilon=10^{-4}$。Basic网络训练了800,000次mini-batch迭代，large网络训练了2,000,000次。算法实现<a href="https://github.com/IntelVCL/DirectFuturePrediction。" target="_blank" rel="external">https://github.com/IntelVCL/DirectFuturePrediction。</a></p><p>下面介绍我们的baselines。我们同三个算法进行了比较：DQN (Mnih et al., 2015), A3C (Mnih et al., 2016), 以及 DSR (Kulkarni et al., 2016b)。DQN由于其在Atari游戏上的优异效果成为了视觉控制的标准baseline。A3C更是这个领域中的最好的算法。DSR也在Virtual Doom平台上进行了实验。所以我们挑选了这三个具有代表意义的算法。</p><p>对于这三个算法我们都使用了Github上的开源实现：DQN (<a href="https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner" target="_blank" rel="external">https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner</a>) 、DSR (<a href="https://github.com/Ardavans/DSR" target="_blank" rel="external">https://github.com/Ardavans/DSR</a>), 以及 A3C (<a href="https://github.com/muupan/async-rl)。前两个都是作者提供的源码，最后的A3C是一个独立开发者的个人实现。" target="_blank" rel="external">https://github.com/muupan/async-rl)。前两个都是作者提供的源码，最后的A3C是一个独立开发者的个人实现。</a></p><p>对于DQN以及DSR我们测试了三个学习速率：默认值（0.00025），0.00005以及0.00002。其他参数直接采用默认值。对于A3C算法，为了训练更快，前两个任务我们采用了5个学习速率 ({2, 4, 8, 16, 32} · $10^{-4}$)。后两个任务我们训练了20个模型，每个模型的学习速率从一个范围从$10^{-4}$到$10^{-2}$的log-uniform分布中进行采样，$\beta$值（熵正则项）从一个范围从$10^{-4}$到$10^{-}$的lo1g-uniform分布中进行采样。结果选取最好的。</p><p>最终结果如下所示：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t1.png" alt="t1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/f3.png" alt="f3"></p><p>在前两个游戏场景中，模型尝试最大化血量；在后两个场景中尝试最大化血量、弹药数以及击杀数的一个线性组合，参数为0.5, 0.5, 1。因为游戏更加侧重于通过击杀数判断胜负。所有的数据都是对三次训练结果进行平均，曲线图采样点的个数为$3 \times 50,000$。可以看出，DFP模型取得了最好的结果。其中DSR算法由于训练速度过慢，所以我们只在D1场景（也进行了将近10天的训练）进行了测试。</p><p>下面进行模型泛化能力的测试，我们基于D3以及D4两个场景分别随机产生100个随机场景。其中90个用于训练，剩下10个用于测试。最后结果如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/f2.png" alt="f2"></p><p>其中最后一列采用了large网络。可以看出，从复杂场景训练之后，在简单场景上的泛化能力往往不错，虽然两者规则不同。但是反之则不可以。</p><p>接下来进行学习变化目标能力的测试。结果见下图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t3g.png" alt="t3g"></p><p>其中采用第二列的策略时，agent并不知道每一个measurement的相对重要性；最后一列，agent事先并不知道哪一个measurement是不需要考虑的。但是最后测试时，效果都很好，而且在固定目标策略没有见过的目标上的效果要更好。说明DFP模型对于变化目标的学习能力优异。</p><p>最后我们单独对measurement stream时间偏置的重要性进行测试，我们采用的是D3-tx训练集，最后结果如图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t4.png" alt="t4"></p><p>相比较而言，采用更多的时间偏置可以达到更好的效果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>现在强化学习问题关注的重点还是在value function的估计上，深度强化学习模型一般采用一个深度网络直接对value function进行估计。这篇论文的创新点在于，在使用深度网络之前，对value function进行了两次额外的映射。第一次是用measurement stream来代替reward，使得reward具有更强的状态表示能力；其次，对measurement stream再次进行了一个函数映射，采用了时间偏置，借鉴了n-step Q-learning的思想。最后，再将输出作为深度网络的输入，进行value function的估计。最后的实验结果证明这种想法是有其正确性的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;em&gt;论文 &lt;a href=&quot;https://openreview.net/forum?id=rJLS7qKel&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Learning to act by predicting the future&lt;/a&gt;&lt;/e
    
    </summary>
    
    
      <category term="deep learning" scheme="http://yoursite.com/tags/deep-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Monte Carlo Methods (Reinforcement Learning)</title>
    <link href="http://yoursite.com/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/"/>
    <id>http://yoursite.com/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/</id>
    <published>2017-06-02T06:10:57.000Z</published>
    <updated>2017-06-02T06:11:31.491Z</updated>
    
    <content type="html"><![CDATA[<p>Here we consider our first learning methods for <strong>estimating</strong> value functions and discovering optimal policy. Unlike the previous algorithms, we do not assume complete knowledge of the environment. Monte Carlo methods require only <em>experience</em> – sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Although a model is required, the model need only generate sample transition, not the complete probability distributions of all possible transitions that is required for <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">dynamic programming (DP)</a>.</p><p>Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample return. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. Only on the completion of an episode are value estimates and policies changed.</p><p>To handle the nonstationarity, we adapt the idea of general policy iteration (GPI) developed in earlier for <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">DP</a>.</p><p>Suppose we wish to estimate $v_{\pi}(s)$, the value of a state $s$ under policy $\pi$, given a set of episodes obtained by following $\pi$ and passing though $s$. Each occurrence of state $s$ in an episode is called a <em>visit</em> to $s$. Let us call the first time it is visited in an episode the <em>first visit</em> to $s$. The <em>first-visit MC method</em> estimates $v_{\pi}(s)$ as the average of the returns following first visits to $s$, whereas the <em>every-visit MC method</em> averages the returns following all visits to $s$.</p><blockquote><p><strong>First-visit MC policy evaluation (returns $V \approx v_{\pi}$)</strong></p><p>Initialize:</p><p>​ $\pi \leftarrow$ policy to be evaluated</p><p>​ $V \leftarrow $ an arbitrary state-value function</p><p>​ $Return(s) \leftarrow$ an empty list, for all $s \in \mathcal{S}$</p><p>Repeat forever:</p><p>​ Generate an episode using $\pi$</p><p>​ For each state $s$ appearing in the episode:</p><p>​ $G \leftarrow$ return following the first occurrence of $s$</p><p>​ Append $G$ to $Return(s)$</p><p>​ $V(s) \leftarrow$ $\text{average}(Return(s))$</p></blockquote><p>Next, we’ll use this algorithm to solve a naive problem that defined as follows:</p><blockquote><p>The object of the popular casino card game of <em>blackjack</em> is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a <em>natural</em>. He then wins unless the dealer also has a natural, in which case the game is draw. If the player does not have a natural, then he can request additional cards, one by one (<em>hits</em>), until he either stop (<em>sticks</em>) or excepted 21 (<em>goes bust</em>). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome–win, lose, draw–is determined by whose final sum is closer to 21.</p></blockquote><p>Playing blackjack is naturally formulated as an episode finite MDP. Each game of blackjack is an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount ($\gamma=1$); therefore these terminal rewards are also returns. The player’s actions are to hit or to stick. We assume that cards are dealt from an infinite deck (i.e., with replacement). If the player holds an ace that he could count as 11 without going bust, then the ace is said to be <em>usable</em>. Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits. Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.</p><p>Note that, although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events–in particular, they require the quantities $p(s^{\prime}, r|s, a)$–and it is not easy to determine these for blackjack. For example, suppose the play’s sum is 14 and he chooses to sticks. What is his excepted reward as a function of the dealer’s showing card? All of these rewards and transition probabilities must be computed <em>before</em> DP can be applied, and such computations are often complex and error-prone.</p><p>The conceptual diagram of the experimental results is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/blackjack_c.png" alt="blackjack_c"></p><p><em>Figure 1</em></p><p>The first we define some auxiliary variables and methods:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># actions: hit or stand (stick)</span></div><div class="line">ACTION_HIT = <span class="number">0</span></div><div class="line">ACTION_STAND = <span class="number">1</span></div><div class="line">actions = [ACTION_HIT, ACTION_STAND]</div><div class="line"></div><div class="line"><span class="comment"># policy for player</span></div><div class="line">policyPlayer = np.zeros(<span class="number">22</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">20</span>):</div><div class="line">    policyPlayer[i] = ACTION_HIT</div><div class="line">policyPlayer[<span class="number">20</span>] = ACTION_STAND</div><div class="line">policyPlayer[<span class="number">21</span>] = ACTION_STAND</div><div class="line"></div><div class="line"><span class="comment"># function form of target policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">return</span> policyPlayer[playerSum]</div><div class="line"></div><div class="line"><span class="comment"># function form of behavior policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> ACTION_STAND</div><div class="line">    <span class="keyword">return</span> ACTION_HIT</div><div class="line"></div><div class="line"><span class="comment"># policy for dealer</span></div><div class="line">policyDealer = np.zeros(<span class="number">22</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">17</span>):</div><div class="line">    policyDealer[i] = ACTION_HIT</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">17</span>, <span class="number">22</span>):</div><div class="line">    policyDealer[i] = ACTION_STAND</div><div class="line"></div><div class="line"><span class="comment"># get a new card</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCard</span><span class="params">()</span>:</span></div><div class="line">    card = np.random.randint(<span class="number">1</span>, <span class="number">14</span>)</div><div class="line">    card = min(card, <span class="number">10</span>)</div><div class="line">    <span class="keyword">return</span> card</div></pre></td></tr></table></figure><p>Furthermore, we also have a print method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># print the state value</span></div><div class="line">figureIndex = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span><span class="params">(data, tile, zlabel=<span class="string">'reward'</span>)</span>:</span></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    fig = plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    fig.suptitle(tile)</div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</div><div class="line">    axisX = []</div><div class="line">    axisY = []</div><div class="line">    axisZ = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">22</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</div><div class="line">            axisX.append(i)</div><div class="line">            axisY.append(j)</div><div class="line">            axisZ.append(data[i - <span class="number">12</span>, j - <span class="number">1</span>])</div><div class="line">    ax.scatter(axisX, axisY, axisZ)</div><div class="line">    ax.set_xlabel(<span class="string">'player sum'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'dealer showing'</span>)</div><div class="line">    ax.set_zlabel(zlabel)</div></pre></td></tr></table></figure><p>In order to get the figure above, we wrote the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onPolicy</span><span class="params">()</span>:</span></div><div class="line">    statesUsableAce1, statesNoUsableAce1 = monteCarloOnPolicy(<span class="number">10000</span>)</div><div class="line">    statesUsableAce2, statesNoUsableAce2 = monteCarloOnPolicy(<span class="number">500000</span>)</div><div class="line">    prettyPrint(statesUsableAce1, <span class="string">'Usable Ace, 10000 Episodes'</span>)</div><div class="line">    prettyPrint(statesNoUsableAce1, <span class="string">'No Usable Ace, 10000 Episodes'</span>)</div><div class="line">    prettyPrint(statesUsableAce2, <span class="string">'Usable Ace, 500000 Episodes'</span>)</div><div class="line">    prettyPrint(statesNoUsableAce2, <span class="string">'No Usable Ace, 500000 Episodes'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>There is a term named <em>on policy</em>, we’ll explain this term later. Now let us jump into the <strong>monteCarloOnPolicy</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo Sample with On-Policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOnPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    statesUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    statesNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesNoUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        state, reward, _ = play(targetPolicyPlayer)</div><div class="line">        state[<span class="number">1</span>] -= <span class="number">12</span></div><div class="line">        state[<span class="number">2</span>] -= <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> state[<span class="number">0</span>]:</div><div class="line">            statesUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            statesNoUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesNoUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">    <span class="keyword">return</span> statesUsableAce / statesUsableAceCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>We ignore he first four variables now and explain them later. <strong>nEpisodes</strong> represents the number of the episodes and the <strong>play</strong> method simulates the process of the blackjack cards game. So what is it like? This method is too long (more than 100 rows) so we partially explain it. In the first place, this method define some auxiliary variables:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># play a game</span></div><div class="line"><span class="comment"># @policyPlayerFn: specify policy for player</span></div><div class="line"><span class="comment"># @initialState: [whether player has a usable Ace, sum of player's cards, one card of dealer]</span></div><div class="line"><span class="comment"># @initialAction: the initial action</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">(policyPlayerFn, initialState=None, initialAction=None)</span>:</span></div><div class="line">    <span class="comment"># player status</span></div><div class="line">	<span class="comment"># sum of player</span></div><div class="line">    playerSum = <span class="number">0</span></div><div class="line">    <span class="comment"># trajectory of player</span></div><div class="line">    playerTrajectory = []</div><div class="line">    <span class="comment"># whether player uses Ace as 11</span></div><div class="line">    usableAcePlayer = <span class="keyword">False</span></div><div class="line">    <span class="comment"># dealer status</span></div><div class="line">    dealerCard1 = <span class="number">0</span></div><div class="line">    dealerCard2 = <span class="number">0</span></div><div class="line">    usableAceDealer = <span class="keyword">False</span></div></pre></td></tr></table></figure><p>Then, we generate a random initial state if the initial state is null. If not, we load the initial state from initialState variable:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> initialState <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">    <span class="comment"># generate a random initial state</span></div><div class="line">    numOfAce = <span class="number">0</span></div><div class="line">    <span class="comment"># initialize cards of player</span></div><div class="line">    <span class="keyword">while</span> playerSum &lt; <span class="number">12</span>:</div><div class="line">        <span class="comment"># if sum of player is less than 12, always hit</span></div><div class="line">        card = getCard()</div><div class="line">        <span class="comment"># if get an Ace, use it as 11</span></div><div class="line">        <span class="keyword">if</span> card == <span class="number">1</span>:</div><div class="line">            numOfAce += <span class="number">1</span></div><div class="line">            card = <span class="number">11</span></div><div class="line">            usableAcePlayer = <span class="keyword">True</span></div><div class="line">        playerSum += card</div><div class="line">    <span class="comment"># if player's sum is larger than 21, he must hold at least one Ace, two Aces are possible</span></div><div class="line">    <span class="keyword">if</span> playerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="comment"># use the Ace as 1 rather than 11</span></div><div class="line">        playerSum -= <span class="number">10</span></div><div class="line">        <span class="comment"># if the player only has one Ace, then he doesn't have usable Ace any more</span></div><div class="line">        <span class="keyword">if</span> numOfAce == <span class="number">1</span>:</div><div class="line">            usableAcePlayer = <span class="keyword">False</span></div><div class="line">    <span class="comment"># initialize cards of dealer, suppose dealer will show the first card he gets</span></div><div class="line">    dealerCard1 = getCard()</div><div class="line">    dealerCard2 = getCard()</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">    	<span class="comment"># use specified initial state</span></div><div class="line">    	usableAcePlayer = initialState[<span class="number">0</span>]</div><div class="line">    	playerSum = initialState[<span class="number">1</span>]</div><div class="line">    	dealerCard1 = initialState[<span class="number">2</span>]</div><div class="line">    	dealerCard2 = getCard()</div><div class="line"></div><div class="line">	<span class="comment"># initial state of the game</span></div><div class="line">	state = [usableAcePlayer, playerSum, dealerCard1]</div><div class="line"></div><div class="line">	<span class="comment"># initialize dealer's sum</span></div><div class="line">    dealerSum = <span class="number">0</span></div><div class="line">    <span class="keyword">if</span> dealerCard1 == <span class="number">1</span> <span class="keyword">and</span> dealerCard2 != <span class="number">1</span>:</div><div class="line">        dealerSum += <span class="number">11</span> + dealerCard2</div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> dealerCard1 != <span class="number">1</span> <span class="keyword">and</span> dealerCard2 == <span class="number">1</span>:</div><div class="line">        dealerSum += dealerCard1 + <span class="number">11</span></div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> dealerCard1 == <span class="number">1</span> <span class="keyword">and</span> dealerCard2 == <span class="number">1</span>:</div><div class="line">        dealerSum += <span class="number">1</span> + <span class="number">11</span></div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        dealerSum += dealerCard1 + dealerCard2</div></pre></td></tr></table></figure><p>Game start! Above all is player’s turn:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># player's turn</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">if</span> initialAction <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        action = initialAction</div><div class="line">        initialAction = <span class="keyword">None</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># get action based on current sum</span></div><div class="line">        action = policyPlayerFn(usableAcePlayer, playerSum, dealerCard1)</div><div class="line"></div><div class="line">    <span class="comment"># track player's trajectory for importance sampling</span></div><div class="line">    playerTrajectory.append([action, (usableAcePlayer, playerSum, dealerCard1)])</div><div class="line"></div><div class="line">    <span class="keyword">if</span> action == ACTION_STAND:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># if hit, get new card</span></div><div class="line">    playerSum += getCard()</div><div class="line"></div><div class="line">    <span class="comment"># player busts</span></div><div class="line">    <span class="keyword">if</span> playerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="comment"># if player has a usable Ace, use it as 1 to avoid busting and continue</span></div><div class="line">        <span class="keyword">if</span> usableAcePlayer == <span class="keyword">True</span>:</div><div class="line">            playerSum -= <span class="number">10</span></div><div class="line">            usableAcePlayer = <span class="keyword">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># otherwise player loses</span></div><div class="line">            <span class="keyword">return</span> state, <span class="number">-1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>Then is the dealer’s turn if the player’s turn is end:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># get action based on current sum</span></div><div class="line">    action = policyDealer[dealerSum]</div><div class="line">    <span class="keyword">if</span> action == ACTION_STAND:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># if hit, get a new card</span></div><div class="line">    dealerSum += getCard()</div><div class="line">    <span class="comment"># dealer busts</span></div><div class="line">    <span class="keyword">if</span> dealerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="keyword">if</span> usableAceDealer == <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># if dealer has a usable Ace, use it as 1 to avoid busting and continue</span></div><div class="line">            dealerSum -= <span class="number">10</span></div><div class="line">            usableAceDealer = <span class="keyword">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># otherwise dealer loses</span></div><div class="line">            <span class="keyword">return</span> state, <span class="number">1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>If the both sides have finished the game:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compare the sum between player and dealer</span></div><div class="line"><span class="keyword">if</span> playerSum &gt; dealerSum:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">1</span>, playerTrajectory</div><div class="line"><span class="keyword">elif</span> playerSum == dealerSum:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">0</span>, playerTrajectory</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">-1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>Now, let us come back the <strong>mentoCarloOnPolicy</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOnPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    statesUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    statesNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesNoUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        state, reward, _ = play(targetPolicyPlayer)</div><div class="line">        state[<span class="number">1</span>] -= <span class="number">12</span></div><div class="line">        state[<span class="number">2</span>] -= <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> state[<span class="number">0</span>]:</div><div class="line">            statesUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            statesNoUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesNoUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">    <span class="keyword">return</span> statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>In this method we ignore the player’s trajectory (represent by the <strong>playerTrajectory</strong> variable). If you remember a sentence in the game definition (as follows) it will easy to understand.</p><blockquote><p>Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.</p></blockquote><p>This row (as follows) is to calculate the average returns of each state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">return</span> statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>Recall the beginning of the code and let’s see what results are like:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_10000.png" alt="usable_ace_10000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_10000.png" alt="no_usable_ace_10000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_500000.png" alt="usable_ace_500000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_500000.png" alt="no_usable_ace_500000"></p><p><em>Figure 2 (from up to down). (1) Usable Ace, 10000 Episodes. (2) No Usable Ace, 10000 Episodes. (3) Usable Ace, 500000 Episodes. (4) No Usable Ace, 500000 Episodes.</em></p><p>If a model is not available, then it is particularly useful to estimate <em>action values</em> (the value of state-value pairs) rather than <em>state values</em>. With a model, state values alone are sufficient to determine a policy; one simply look ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the earlier on <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">DP</a>. Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of out primary goals for Monte Carlo methods is to estimate $q_{\star}$. To achieve this, we first consider the policy evaluation problem for action values.</p><p>The policy evaluation problem for action values is estimate $q_{\pi}(s, a)$. The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a state-action pair rather than to a state. The only complication is that many state-value pairs may never be visited. This is the general problem of <em>maintaining exploration</em>, as discussed in the context of the k-armed bandit problem in <a href="https://ewanlee.github.io/2017/05/27/k-Armed-Bandit-Problem/" target="_blank" rel="external">here</a>. One way to do this is by specifying that the episodes <em>start in a state-action pair</em>, and that every pair has a nonzero probability of being selected as the start. We call this the assumption of <em>exploring starts</em>.</p><p>We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal policies. To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with a arbitrary policy $\pi_{0}$ and ending with the optimal policy and optimal action-value function:<br>$$<br>\pi_{0} \stackrel{E}\longrightarrow q_{\pi{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow q_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow q_{\star},<br>$$<br>We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes. We postpone consideration of the first assumption until later.</p><p>The primary approach to avoiding the infinite number of episodes nominally required for policy evaluation is to forgo to complete policy evaluation before returning to policy improvement. For Monte Carlo policy evaluation it it natural to alternate between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation,and then the policy is improved at all the states visited in the episode.</p><blockquote><p><strong>Monte Carlo ES (Exploring Starts)</strong></p><p>Initialize, for all $s \in \mathcal{S},\; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow \text{arbitrary}$</p><p>​ $\pi(s) \leftarrow \text{arbitrary}$</p><p>​ $Returns(s,a) \leftarrow \text{empty list}$</p><p>Repeat forever:</p><p>​ Choose $S_{0} \in \mathcal{S}$ and $A_{0} \in \mathcal{A(S_{0})}$ s.t. all pairs have probability &gt; 0</p><p>​ Generate an episode starting from $S_0, A_0$, following $\pi$</p><p>​ For each pair $s, a$ appearing in the episode:</p><p>​ $G \leftarrow \text{return following the first occurrence of} \; s, a$</p><p>​ Append $G$ to $Returns(s,a)$</p><p>​ $Q(s,a) \leftarrow \text{average}(Returns(s,a))$</p><p>​ For each $s$ in the episode:</p><p>​ $\pi(s) \leftarrow \arg\max_{a} Q(s,a)$</p></blockquote><p>Recall the blackjack card game. It is straightforward to apply Monte Carlo ES to blackjack.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">figure5_3</span><span class="params">()</span>:</span></div><div class="line">    stateActionValues = monteCarloES(<span class="number">500000</span>)</div><div class="line">    stateValueUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    stateValueNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># get the optimal policy</span></div><div class="line">    actionUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="string">'int'</span>)</div><div class="line">    actionNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="string">'int'</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">            stateValueNoUsableAce[i, j] = np.max(stateActionValues[i, j, <span class="number">0</span>, :])</div><div class="line">            stateValueUsableAce[i, j] = np.max(stateActionValues[i, j, <span class="number">1</span>, :])</div><div class="line">            actionNoUsableAce[i, j] = argmax(stateActionValues[i, j, <span class="number">0</span>, :])</div><div class="line">            actionUsableAce[i, j] = argmax(stateActionValues[i, j, <span class="number">1</span>, :])</div><div class="line">    prettyPrint(stateValueUsableAce, <span class="string">'Optimal state value with usable Ace'</span>)</div><div class="line">    prettyPrint(stateValueNoUsableAce, <span class="string">'Optimal state value with no usable Ace'</span>)</div><div class="line">    prettyPrint(actionUsableAce, <span class="string">'Optimal policy with usable Ace'</span>, <span class="string">'Action (0 Hit, 1 Stick)'</span>)</div><div class="line">    prettyPrint(actionNoUsableAce, <span class="string">'Optimal policy with no usable Ace'</span>, <span class="string">'Action (0 Hit, 1 Stick)'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Run the code we’ll get the conceptual diagram like follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces.png" alt="mces"></p><p>Let us to see the implementation (<strong>monteCarloES</strong> method) of this algorithm. Note that, some auxiliary variables are defined earlier.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo with Exploring Starts</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloES</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    <span class="comment"># (playerSum, dealerCard, usableAce, action)</span></div><div class="line">    stateActionValues = np.zeros((<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">2</span>))</div><div class="line">    <span class="comment"># set default to 1 to avoid being divided by 0</span></div><div class="line">    stateActionPairCount = np.ones((<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">2</span>))</div><div class="line">    <span class="comment"># behavior policy is greedy</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicy</span><span class="params">(usableAce, playerSum, dealerCard)</span>:</span></div><div class="line">        usableAce = int(usableAce)</div><div class="line">        playerSum -= <span class="number">12</span></div><div class="line">        dealerCard -= <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> argmax(stateActionValues[playerSum, dealerCard, usableAce, :])</div><div class="line"></div><div class="line">    <span class="comment"># play for several episodes</span></div><div class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(nEpisodes):</div><div class="line">        print(<span class="string">'episode:'</span>, episode)</div><div class="line">        <span class="comment"># for each episode, use a randomly initialized state and action</span></div><div class="line">        initialState = [bool(np.random.choice([<span class="number">0</span>, <span class="number">1</span>])),</div><div class="line">                       np.random.choice(range(<span class="number">12</span>, <span class="number">22</span>)),</div><div class="line">                       np.random.choice(range(<span class="number">1</span>, <span class="number">11</span>))]</div><div class="line">        initialAction = np.random.choice(actions)</div><div class="line">        _, reward, trajectory = play(behaviorPolicy, initialState, initialAction)</div><div class="line">        <span class="keyword">for</span> action, (usableAce, playerSum, dealerCard) <span class="keyword">in</span> trajectory:</div><div class="line">            usableAce = int(usableAce)</div><div class="line">            playerSum -= <span class="number">12</span></div><div class="line">            dealerCard -= <span class="number">1</span></div><div class="line">            <span class="comment"># update values of state-action pairs</span></div><div class="line">            stateActionValues[playerSum, dealerCard, usableAce, action] += reward</div><div class="line">            stateActionPairCount[playerSum, dealerCard, usableAce, action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> stateActionValues / stateActionPairCount</div></pre></td></tr></table></figure><p>You can see we use the <strong>trajectory</strong> variable now. The difference between Monte Carlo On Policy and Monte Carlo ES is prior calculates the average return of each state and later calculates the average return of each state-action pair.</p><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_policy.png" alt="mcse_usbale_ace_optimal_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_policy.png" alt="mcse_usbale_ace_optimal_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_state_value.png" alt="mcse_usable_ace_optimal_state_value"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_state_value.png" alt="mcse_no_usable_ace_optimal_state_value"></p><p>How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call <em>on-policy</em> (Do you remember this term?) methods and <em>off-policy</em> methods. On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a policy different from that used to generate the data.. The First-visit Monte Carlo method and the Monte Carlo ES method are an example of an on-policy method. Here we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. Off-policy methods are considered later.</p><p>The on-policy method we presented in here uses $\epsilon \text{-} greedy$ policies. The complete algorithm is given below.</p><blockquote><p><strong>On-policy first-visit MC control (for $\epsilon \text{-soft}$ policies)</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a ) \leftarrow \text{arbitrary}$</p><p>​ $Returns(s,a) \leftarrow \text{empty list}$</p><p>​ $\pi(a|s) \leftarrow \text{an arbitrary} \; \epsilon \text{-soft policy}$</p><p>Repeat forever:</p><p>​ (a) Generate an episode using $\pi$</p><p>​ (b) For each pair $s, a$ appearing in the episode:</p><p>​ $G \leftarrow $ return following the first occurrence of $s, a$</p><p>​ Append $G$ to $Returns(s,a)$</p><p>​ $Q(s, a) \leftarrow \text{average}(Returns(s,a))$</p><p>​ (c) For each s in the episode:</p><p>​ $A^{\star} \leftarrow \arg\max_{a} Q(s,a)$</p><p>​ For all $a \in \mathcal{A(s)}$:</p><p>​ $\pi(a|s) \leftarrow \left\{ \begin{array}{c} 1 - \epsilon + \epsilon/|\mathcal{A(s)}|\;\;\;\text{if} \;a=A^{\star} \\ \epsilon/|\mathcal{A(s)}|\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{if} \;a\neq A^{\star} \end{array} \right.$</p></blockquote><p>All learning control methods face a dilemma: They seek to learn action values conditional on subsequent <em>optimal</em> behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromise–it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to user two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the <em>target policy</em>, and the policy used to generate behavior is called the <em>behavior policy</em>. In this case we say that learning is from data “off” the target policy, and the overall process is termed <em>off-policy learning</em>.</p><p>We begin the study of off-policy methods by considering the <em>prediction</em> problem, in which both target and behavior policies are fixed. That is, suppose we wish to estimate $v_{\pi}$ or $q_{\pi}$, but all we have are episodes following another policy $\mu$, where $\mu \neq \pi$. In this case, $\pi$ is the target policy, $\mu$ is the behavior policy, and both policies are considered fixed and given.</p><p>In order to use episodes from $\mu$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $\mu$. That is, we require that $\pi(a|s) &gt; 0$ implies $\mu(a|s) &gt; 0$. This is called the assumption of <em>converge</em>. It follows from converge that $\mu$ must be stochastic in states where it is not identical to $\pi$. The target policy $\pi$, on the other hand, may be deterministic. In here, we consider the prediction problem, in which $\pi$ is unchanging and given.</p><p>Almost all off-policy methods utilize <em>importance sampling</em>ddd, a general technique for estimating excepted values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectory occurring under the target and behavior policies, called the <em>importance-sampling ratio</em>. Given a starting state $S_{t}$, the probability of the subsequent state-action trajectory , $A_{t}, S_{t+1}, A_{t+1, \cdots, S_{T}}$, occurring under any policy $\pi$ is<br>$$<br>\prod_{k=t}^{T-1} \pi(A_k | S_k)p(S_{k+1} | S_k, A_k),<br>$$<br>where $p$ here is the state-transition probability function. Thus, the relative probability of the trajectory under the target and behavior policies (the important-sampling ratio) is<br>$$<br>\rho_{t}^{T} \doteq \frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1} \mu(A_k|S_k)p(S_{k+1}|S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi (A_k | S_k)}{\mu (A_k | S_k)}<br>$$<br>Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy $\mu$ to estimate $v_{\pi}(s)$. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time $t=101$. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state $s$ is visited, denote $\tau(s)$. This is for an every-visit method; for a first-visit method, $\tau(s)$ would only include time steps that were first visits to $s$ within their episodes. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up though $T(t)$. Then $\{G_t\}_{t \in \tau(s)}$ are returns that pertain to state $s$, and $\{\rho_{t}^{T(t)}\}_{t \in \tau(s)}$ are the corresponding importance-sampling ratios. To estimate $v_{\pi}(s)$, we simply scale the returns by the ratios and average the results:<br>$$<br>V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{|\tau(s)|}.<br>$$<br>When importance sampling is done as a simple average in this way it is called <em>ordinary importance sampling</em>.</p><p>An important alternative is <em>weighted importance sampling</em>, which uses a weighted average, defined as<br>$$<br>V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{\sum_{t \in \tau(s)} \rho_{t}^{T(t)}},<br>$$<br>or zero if the denominator is zero.</p><p>We applied both ordinary and weighted importance-sampling methods to estimate the value of as single blackjack state from off-policy data. Recall that one of the advantages of Monte Carlo methods is that they can be used to evaluate a single state without forming estimated for any other states. In this example, we evaluated the state in which the dealer is showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three aces). The data was generate by starting in this state then choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21. The value of this state under the target policy is approximately -0.27726 (this was determined by separately generating one-hundred million episodes using the target policy and averaging their returns). Both off-policy methods closely approximated this value after 1000 off-policy episodes using the random policy. To make sure they did this reliably, we performed 100 independent runs, each starting from estimates of zero and learning for 10,000 episodes. The implementation details are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo Sample with Off-Policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOffPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    initialState = [<span class="keyword">True</span>, <span class="number">13</span>, <span class="number">2</span>]</div><div class="line">    sumOfImportanceRatio = [<span class="number">0</span>]</div><div class="line">    sumOfRewards = [<span class="number">0</span>]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        _, reward, playerTrajectory = play(behaviorPolicyPlayer, initialState=initialState)</div><div class="line"></div><div class="line">        <span class="comment"># get the importance ratio</span></div><div class="line">        importanceRatioAbove = <span class="number">1.0</span></div><div class="line">        importanceRatioBelow = <span class="number">1.0</span></div><div class="line">        <span class="keyword">for</span> action, (usableAce, playerSum, dealerCard) <span class="keyword">in</span> playerTrajectory:</div><div class="line">            <span class="keyword">if</span> action == targetPolicyPlayer(usableAce, playerSum, dealerCard):</div><div class="line">                importanceRatioBelow *= <span class="number">0.5</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                importanceRatioAbove = <span class="number">0.0</span></div><div class="line">                <span class="keyword">break</span></div><div class="line">        importanceRatio = importanceRatioAbove / importanceRatioBelow</div><div class="line">        sumOfImportanceRatio.append(sumOfImportanceRatio[<span class="number">-1</span>] + importanceRatio)</div><div class="line">        sumOfRewards.append(sumOfRewards[<span class="number">-1</span>] + reward * importanceRatio)</div><div class="line">    <span class="keyword">del</span> sumOfImportanceRatio[<span class="number">0</span>]</div><div class="line">    <span class="keyword">del</span> sumOfRewards[<span class="number">0</span>]</div><div class="line"></div><div class="line">    sumOfRewards= np.asarray(sumOfRewards)</div><div class="line">    sumOfImportanceRatio= np.asarray(sumOfImportanceRatio)</div><div class="line">    ordinarySampling = sumOfRewards / np.arange(<span class="number">1</span>, nEpisodes + <span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> np.errstate(divide=<span class="string">'ignore'</span>,invalid=<span class="string">'ignore'</span>):</div><div class="line">        weightedSampling = np.where(sumOfImportanceRatio != <span class="number">0</span>, sumOfRewards / sumOfImportanceRatio, <span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> ordinarySampling, weightedSampling</div></pre></td></tr></table></figure><p>Note that the <strong>behaviorPolicyPlayer</strong> that is a function that define the behavior policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># function form of behavior policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> ACTION_STAND</div><div class="line">    <span class="keyword">return</span> ACTION_HIT</div></pre></td></tr></table></figure><p>And the calculation of the numerator and denominator of the importance-sampling are the summation operation. As the number of iterations increases, we need to accumulate the result from each episode. So we need to store the previous results. The <strong>sumOfRewards</strong> and <strong>sumOfImportanceRatio</strong> are used for this purpose.</p><p>Then we need to show the result (mean square error):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Figure 5.4</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">offPolicy</span><span class="params">()</span>:</span></div><div class="line">    trueValue = <span class="number">-0.27726</span></div><div class="line">    nEpisodes = <span class="number">10000</span></div><div class="line">    nRuns = <span class="number">100</span></div><div class="line">    ordinarySampling = np.zeros(nEpisodes)</div><div class="line">    weightedSampling = np.zeros(nEpisodes)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nRuns):</div><div class="line">        ordinarySampling_, weightedSampling_ = monteCarloOffPolicy(nEpisodes)</div><div class="line">        <span class="comment"># get the squared error</span></div><div class="line">        ordinarySampling += np.power(ordinarySampling_ - trueValue, <span class="number">2</span>)</div><div class="line">        weightedSampling += np.power(weightedSampling_ - trueValue, <span class="number">2</span>)</div><div class="line">    ordinarySampling /= nRuns</div><div class="line">    weightedSampling /= nRuns</div><div class="line">    axisX = np.log10(np.arange(<span class="number">1</span>, nEpisodes + <span class="number">1</span>))</div><div class="line">    plt.plot(axisX, ordinarySampling, label=<span class="string">'Ordinary Importance Sampling'</span>)</div><div class="line">    plt.plot(axisX, weightedSampling, label=<span class="string">'Weighted Importance Sampling'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Episodes (10^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Mean square error'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/off_policy.png" alt="off_policy"></p><p>Formally, the difference between the two kinds of importance sampling is expressed in their biases and variances. The ordinary importance-sampling estimator is unbiased whereas the weighted importance-sampling estimator is biased (the bias converges asymptotically to zero). On the other hand, the variance of the ordinary importance-sampling estimator is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximate that we explore later.</p><p>The estimates of ordinary importance sampling will typical have infinite variance, and this can easily happen in off-policy learning when trajectories contains loops. A simple example is shown below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance_example.png" alt="infinite_variance_example"></p><p>There is only one nonterminal state $s$ and two action, <strong>end</strong> and <strong>back</strong>. The <strong>end</strong> action causes a deterministic transition to termination, whereas the <strong>back</strong> action transitions, with probability 0.9, back to $s$ or, with probability 0.1, on to termination. The rewards are +1 on latter transition and otherwise zero. Consider the target policy that always selects back. All episodes under this policy consist of some number (possibly zero) of transitions back to $s$ followed by termination with a reward and return of +1. Thus the value of $s$ under the target policy is 1. Suppose we are estimating this value from off-policy data using the behavior policy that selects <strong>end</strong> and <strong>back</strong> with equal probability.</p><p>The implementation details are as follows. We first define the two policies:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">ACTION_BACK = <span class="number">0</span></div><div class="line">ACTION_END = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># behavior policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicy</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># target policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetPolicy</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> ACTION_BACK</div></pre></td></tr></table></figure><p>Then we define how an episode runs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># one turn</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># track the action for importance ratio</span></div><div class="line">    trajectory = []</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        action = behaviorPolicy()</div><div class="line">        trajectory.append(action)</div><div class="line">        <span class="keyword">if</span> action == ACTION_END:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span>, trajectory</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.9</span>) == <span class="number">0</span>:</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span>, trajectory</div></pre></td></tr></table></figure><p>Now we start our off-policy (first-visit MC) learning process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Figure 5.5</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloSample</span><span class="params">()</span>:</span></div><div class="line">    runs = <span class="number">10</span></div><div class="line">    episodes = <span class="number">100000</span></div><div class="line">    axisX = np.log10(np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>))</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        sumOfRewards = [<span class="number">0</span>]</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            reward, trajectory = play()</div><div class="line">            <span class="keyword">if</span> trajectory[<span class="number">-1</span>] == ACTION_END:</div><div class="line">                importanceRatio = <span class="number">0</span> <span class="comment"># Because it is impossible on the target policy</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                importanceRatio = <span class="number">1.0</span> / pow(<span class="number">0.5</span>, len(trajectory))</div><div class="line">            sumOfRewards.append(sumOfRewards[<span class="number">-1</span>] + importanceRatio * reward)</div><div class="line">        <span class="keyword">del</span> sumOfRewards[<span class="number">0</span>]</div><div class="line">        estimations = np.asarray(sumOfRewards) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">        plt.plot(axisX, estimations)</div><div class="line">    plt.xlabel(<span class="string">'Episodes (10^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Ordinary Importance Sampling'</span>)</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span></div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance.png" alt="inifinite_variance"></p><p>The correct estimate here is 1, and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite, and the estimates do not convergence to this value.</p><p>At last, we proposed two fancy algorithms, that is, the <strong>Incremental off-policy every-visit MC policy evaluation</strong> and the <strong>Off-policy every-visit MC control</strong>.</p><blockquote><p><strong>Incremental off-policy every-visit MC policy evaluation</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow$ arbitrary</p><p>​ $C(s,a) \leftarrow$ 0</p><p>​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy</p><p>​ $\pi(a|s) \leftarrow$ an arbitrary target policy</p><p>Repeat forever:</p><p>​ Generate an episode using $\mu$:</p><p>​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$</p><p>​ $G \leftarrow 0$</p><p>​ $W \leftarrow 1$</p><p>​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:</p><p>​ $G \leftarrow \gamma G + R_{t+1}$</p><p>​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$</p><p>​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$</p><p>​ $W \leftarrow W\frac{\pi(A_t | S_t)}{\mu(A_t | S_t)}$</p><p>​ If $W = 0$ then ExitForLoop</p><p><strong>Off-policy every-visit MC control (returns $\pi \approx \pi_{\star}$)</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow$ arbitrary</p><p>​ $C(s,a) \leftarrow$ 0</p><p>​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy</p><p>​ $\pi(s) \leftarrow$ an deterministic policy that is greedy with respect $Q$</p><p>Repeat forever:</p><p>​ Generate an episode using $\mu$:</p><p>​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$</p><p>​ $G \leftarrow 0$</p><p>​ $W \leftarrow 1$</p><p>​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:</p><p>​ $G \leftarrow \gamma G + R_{t+1}$</p><p>​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$</p><p>​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$</p><p>​ $\pi(S_t) \leftarrow \arg \max_{a}Q(S_t, a)$ (with ties broken consistently)</p><p>​ If $A_t \neq \pi(S_t)$ then ExitForLoop</p><p>​ $W \leftarrow W\frac{1}{\mu(A_t | S_t)}$</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Here we consider our first learning methods for &lt;strong&gt;estimating&lt;/strong&gt; value functions and discovering optimal policy. Unlike the pr
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>Dynamic Programming</title>
    <link href="http://yoursite.com/2017/05/31/Dynamic-Programming/"/>
    <id>http://yoursite.com/2017/05/31/Dynamic-Programming/</id>
    <published>2017-05-31T02:19:52.000Z</published>
    <updated>2017-05-31T03:31:21.833Z</updated>
    
    <content type="html"><![CDATA[<p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are provides an essential foundation for the understanding of the methods presented later. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.</p><p>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. In here we show how DP can be used to compute the value functions defined in earlier. As discussed there, we can easily obtain optimal policies once we have found the optimal value functions $v_{\star}$ or $q_{\star}$ which satisfy the Bellman optimality equations:<br>$$<br>\begin{align}<br>v_{\star}(s) &amp;= \max_{a} \mathbb{E} [R_{t+1} + \gamma v_{\star}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \max_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) \left[r + \gamma v_{\star}(s^{\prime})\right]<br>\end{align}<br>$$<br>or<br>$$<br>\begin{align}<br>q_{\star}(s, a) &amp;= \mathbb{E} [R_{t+1} + \gamma \max_{a^{\prime}} q_{\star}(S_{t+1}, a^{\prime}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma \max_{a^{\prime}} q_{\star}(s^{\prime}, a^{\prime})]<br>\end{align}<br>$$<br>for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}, \; \text{and} \; s^{\prime} \in \mathcal{S^{+}}$. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.</p><p>First we consider how to compute the state-value function $v_{\pi}$ for an arbitrary policy $\pi$. This is called <em>policy evaluation</em> in the DP literature. We also refer to it as the <em>prediction problem</em>. Recall that for all $s \in \mathcal{S}$,<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots \ | \ S_{t}=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_{t}=s] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{\pi}(s^{\prime})]<br>\end{align}<br>$$<br>If the environment’s dynamics are complete known, then (7) is a system of $|\mathcal{S}|$ simultaneous linear equations in $|\mathcal{S}|$ unknowns (the $v_{\pi}(s), s \in \mathcal{S}$). In principle, its solution is a straightforward, if tedious, computation. For our purpose, iterative solution methods are most suitable. The initial approximation, $v_0$, is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation for $v_{\pi}$ as an update rule:<br>$$<br>\begin{align}<br>v_{k+1}(s) &amp;\doteq \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r|s, a) [r + \gamma v_{k} (s^{\prime})]<br>\end{align}<br>$$<br>This algorithm is called <em>iterative policy evaluation</em>.</p><blockquote><p><strong>Iterative policy evaluation</strong></p><p>Input $\pi$, the policy to be evaluated</p><p>Initialize an array $V(s) = 0$, for all $s \in \mathcal{S^{+}}$</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ for each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p><p>Output $V \approx v_{\pi}$</p></blockquote><p>We can see the algorithm used in the <a href="https://ewanlee.github.io/2017/05/29/The-GridWorld-problem/" target="_blank" rel="external">grid world problem</a> just is the <em>iterative policy evaluation</em>.</p><p>Our reason for computing the value function for a policy is to help find better policies. Once a policy $\pi$ has been improved using $v_{\pi}$ to yield a better policy $\pi^{\prime}$, we can then compute $v_{\pi^{\prime}}$and improve it again to yield an even better $\pi^{\prime\prime}$. We can thus obtain a sequence of monotonically improving policies and value functions:<br>$$<br>\pi_{0} \stackrel{E}\longrightarrow v_{\pi_{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow v_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow v_{\star},<br>$$<br>where $\stackrel{E}\longrightarrow$ denotes a policy <em>evaluation</em> and $\stackrel{I}\longrightarrow$ denotes a policy <em>improvement</em>. This way of finding an optimal policy is called <em>policy iteration</em>.</p><blockquote><p><strong>Policy iteration (using iterative policy evaluation)</strong></p><ol><li><p>Initialization</p><p>$V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A(s)}$ arbitrarily for all $s \in \mathcal{S}$</p></li><li><p>Policy Evaluation</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ For each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \sum_{s^{\prime}, r} p(s^{\prime}, r | s, \pi(s)) [r + \gamma v(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p></li><li><p>Policy Improvement</p><p><em>policy-stable</em> $\leftarrow$ <em>true</em></p><p>For each $s \in \mathcal{S}$:</p><p>​ <em>old-action</em> $\leftarrow$ $\pi_(s)$</p><p>​ $\pi (s) \leftarrow argmax_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$</p><p>​ If <em>old-action</em> $\neq \pi(s)$, then <em>policy-stable</em> $\leftarrow$ <em>false</em></p><p>If <em>policy-stable</em>, then stop and return $V \approx v_{\star} \; \text{and} \; \pi \approx \pi_{\star}$; else go to 2.</p></li></ol></blockquote><p>Let us solve a problem used by <strong>policy iteration</strong>. The problem defined as follows:</p><blockquote><p>Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and it credited \$10 by the national company. If he out of cats at that location, then the business is lost. Cars become available for renting the day after they are returned. To ensure that cars are available where they are needed, Jack ca move them between the two locations overnight, at a cost of \$2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is $n$ is $\frac{\lambda^{n}}{n!}e^{-\lambda}$, where $\lambda$ is the excepted number. Suppose $\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\lambda=0.9$ and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight.</p></blockquote><p>The excepted result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental.png" alt="car_rental"></p><p><em>Figure 1</em></p><p>The first, we define some facts of this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># maximum # of cars in each location</span></div><div class="line">MAX_CARS = <span class="number">20</span></div><div class="line"><span class="comment"># maximum # of cars to move during night</span></div><div class="line">MAX_MOVE_OF_CARS = <span class="number">5</span></div><div class="line"><span class="comment"># expectation for rental requests in first location</span></div><div class="line">RENTAL_REQUEST_FIRST_LOC = <span class="number">3</span></div><div class="line"><span class="comment"># expectation for rental requests in second location</span></div><div class="line">RENTAL_REQUEST_SECOND_LOC = <span class="number">4</span></div><div class="line"><span class="comment"># expectation for # of cars returned in first location</span></div><div class="line">RETURNS_FIRST_LOC = <span class="number">3</span></div><div class="line"><span class="comment"># expectation for # of cars returned in second location</span></div><div class="line">RETURNS_SECOND_LOC = <span class="number">2</span></div><div class="line">DISCOUNT = <span class="number">0.9</span></div><div class="line"><span class="comment"># credit earned by a car</span></div><div class="line">RENTAL_CREDIT = <span class="number">10</span></div><div class="line"><span class="comment"># cost of moving a car</span></div><div class="line">MOVE_CAR_COST = <span class="number">2</span></div></pre></td></tr></table></figure><p>From the problem definition, we know that in this MDP the states is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight. Each action is a integer that positive number represents the number of cars moving from the first location to second location and vice verse.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># current policy</span></div><div class="line">policy = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line"><span class="comment"># current state value</span></div><div class="line">stateValue = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line"><span class="comment"># all possible states</span></div><div class="line">states = []</div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">actions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + <span class="number">1</span>)</div></pre></td></tr></table></figure><p>For visualization (Figure 1) convenient, we define a method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># axes for printing use</span></div><div class="line">AxisXPrint = []</div><div class="line">AxisYPrint = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, MAX_CARS + <span class="number">1</span>):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, MAX_CARS + <span class="number">1</span>):</div><div class="line">        AxisXPrint.append(i)</div><div class="line">        AxisYPrint.append(j)</div><div class="line">        states.append([i, j])</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># plot a policy/state value matrix</span></div><div class="line">figureIndex = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span><span class="params">(data, labels)</span>:</span></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    fig = plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</div><div class="line">    AxisZ = []</div><div class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">        AxisZ.append(data[i, j])</div><div class="line">    ax.scatter(AxisXPrint, AxisYPrint, AxisZ)</div><div class="line">    ax.set_xlabel(labels[<span class="number">0</span>])</div><div class="line">    ax.set_ylabel(labels[<span class="number">1</span>])</div><div class="line">    ax.set_zlabel(labels[<span class="number">2</span>])</div></pre></td></tr></table></figure><p>Next, we define a Poisson function that return the probability:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># An up bound for poisson distribution</span></div><div class="line"><span class="comment"># If n is greater than this value, then the probability of getting n is truncated to 0</span></div><div class="line">POISSON_UP_BOUND = <span class="number">11</span></div><div class="line"></div><div class="line"><span class="comment"># Probability for poisson distribution</span></div><div class="line"><span class="comment"># @lam: lambda should be less than 10 for this function</span></div><div class="line">poissonBackup = dict()</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">poisson</span><span class="params">(n, lam)</span>:</span></div><div class="line">    <span class="keyword">global</span> poissonBackup</div><div class="line">    key = n * <span class="number">10</span> + lam</div><div class="line">    <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> poissonBackup.keys():</div><div class="line">        poissonBackup[key] = exp(-lam) * pow(lam, n) / factorial(n)</div><div class="line">    <span class="keyword">return</span> poissonBackup[key]</div></pre></td></tr></table></figure><p>Now, the preparation is done. We’ll implement the policy iteration algorithm as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">newStateValue = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line">improvePolicy = <span class="keyword">False</span></div><div class="line">policyImprovementInd = <span class="number">0</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">if</span> improvePolicy == <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># start policy improvement</span></div><div class="line">        print(<span class="string">'Policy improvement'</span>, policyImprovementInd)</div><div class="line">        policyImprovementInd += <span class="number">1</span></div><div class="line">        newPolicy = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line">        <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">            actionReturns = []</div><div class="line">            <span class="comment"># go through all actions and select the best one</span></div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                <span class="keyword">if</span> (action &gt;= <span class="number">0</span> <span class="keyword">and</span> i &gt;= action) <span class="keyword">or</span> (action &lt; <span class="number">0</span> <span class="keyword">and</span> j &gt;= abs(action)):</div><div class="line">                    actionReturns.append(expectedReturn([i, j], action, stateValue))</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    actionReturns.append(-float(<span class="string">'inf'</span>))</div><div class="line">            bestAction = argmax(actionReturns)</div><div class="line">            newPolicy[i, j] = actions[bestAction]</div><div class="line"></div><div class="line">        <span class="comment"># if policy is stable</span></div><div class="line">        policyChanges = np.sum(newPolicy != policy)</div><div class="line">        print(<span class="string">'Policy for'</span>, policyChanges, <span class="string">'states changed'</span>)</div><div class="line">        <span class="keyword">if</span> policyChanges == <span class="number">0</span>:</div><div class="line">            policy = newPolicy</div><div class="line">            <span class="keyword">break</span></div><div class="line">        policy = newPolicy</div><div class="line">        improvePolicy = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="comment"># start policy evaluation</span></div><div class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">        newStateValue[i, j] = expectedReturn([i, j], policy[i, j], stateValue)</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(newStateValue - stateValue)) &lt; <span class="number">1e-4</span>:</div><div class="line">        stateValue[:] = newStateValue</div><div class="line">        improvePolicy = <span class="keyword">True</span></div><div class="line">        <span class="keyword">continue</span></div><div class="line">    stateValue[:] = newStateValue</div></pre></td></tr></table></figure><p>We can see the logistic is the same as the pseudocode of the policy iteration algorithm. There is a core method in the code, that is, <strong>exceptedReturn()</strong> is used to calculate the reward of cars rental.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># @state: [# of cars in first location, # of cars in second location]</span></div><div class="line"><span class="comment"># @action: positive if moving cars from first location to second location,</span></div><div class="line"><span class="comment">#          negative if moving cars from second location to first location</span></div><div class="line"><span class="comment"># @stateValue: state value matrix</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">expectedReturn</span><span class="params">(state, action, stateValue)</span>:</span></div><div class="line">    <span class="comment"># initailize total return</span></div><div class="line">    returns = <span class="number">0.0</span></div><div class="line"></div><div class="line">    <span class="comment"># cost for moving cars</span></div><div class="line">    returns -= MOVE_CAR_COST * abs(action)</div><div class="line"></div><div class="line">    <span class="comment"># go through all possible rental requests</span></div><div class="line">    <span class="keyword">for</span> rentalRequestFirstLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">        <span class="keyword">for</span> rentalRequestSecondLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">            <span class="comment"># moving cars</span></div><div class="line">            numOfCarsFirstLoc = int(min(state[<span class="number">0</span>] - action, MAX_CARS))</div><div class="line">            numOfCarsSecondLoc = int(min(state[<span class="number">1</span>] + action, MAX_CARS))</div><div class="line"></div><div class="line">            <span class="comment"># valid rental requests should be less than actual # of cars</span></div><div class="line">            realRentalFirstLoc = min(numOfCarsFirstLoc, rentalRequestFirstLoc)</div><div class="line">            realRentalSecondLoc = min(numOfCarsSecondLoc, rentalRequestSecondLoc)</div><div class="line"></div><div class="line">            <span class="comment"># get credits for renting</span></div><div class="line">            reward = (realRentalFirstLoc + realRentalSecondLoc) * RENTAL_CREDIT</div><div class="line">            numOfCarsFirstLoc -= realRentalFirstLoc</div><div class="line">            numOfCarsSecondLoc -= realRentalSecondLoc</div><div class="line"></div><div class="line">            <span class="comment"># probability for current combination of rental requests</span></div><div class="line">            prob = poisson(rentalRequestFirstLoc, RENTAL_REQUEST_FIRST_LOC) * \</div><div class="line">                         poisson(rentalRequestSecondLoc, RENTAL_REQUEST_SECOND_LOC)</div><div class="line"></div><div class="line">            <span class="comment"># if set True, model is simplified such that the # of cars returned in daytime becomes constant</span></div><div class="line">            <span class="comment"># rather than a random value from poisson distribution, which will reduce calculation time</span></div><div class="line">            <span class="comment"># and leave the optimal policy/value state matrix almost the same</span></div><div class="line">            constantReturnedCars = <span class="keyword">True</span></div><div class="line">            <span class="keyword">if</span> constantReturnedCars:</div><div class="line">                <span class="comment"># get returned cars, those cars can be used for renting tomorrow</span></div><div class="line">                returnedCarsFirstLoc = RETURNS_FIRST_LOC</div><div class="line">                returnedCarsSecondLoc = RETURNS_SECOND_LOC</div><div class="line">                numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS)</div><div class="line">                numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS)</div><div class="line">                returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc])</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                numOfCarsFirstLoc_ = numOfCarsFirstLoc</div><div class="line">                numOfCarsSecondLoc_ = numOfCarsSecondLoc</div><div class="line">                prob_ = prob</div><div class="line">                <span class="keyword">for</span> returnedCarsFirstLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">                    <span class="keyword">for</span> returnedCarsSecondLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">                        numOfCarsFirstLoc = numOfCarsFirstLoc_</div><div class="line">                        numOfCarsSecondLoc = numOfCarsSecondLoc_</div><div class="line">                        prob = prob_</div><div class="line">                        numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS)</div><div class="line">                        numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS)</div><div class="line">                        prob = poisson(returnedCarsFirstLoc, RETURNS_FIRST_LOC) * \</div><div class="line">                               poisson(returnedCarsSecondLoc, RETURNS_SECOND_LOC) * prob</div><div class="line">                        returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc])</div><div class="line">    <span class="keyword">return</span> returns</div></pre></td></tr></table></figure><p>The comments are very clear, and we’re going to do a lot of this. Finally, let us print the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">prettyPrint(policy, [<span class="string">'# of cars in first location'</span>, <span class="string">'# of cars in second location'</span>, <span class="string">'# of cars to move during night'</span>])</div><div class="line">prettyPrint(stateValue, [<span class="string">'# of cars in first location'</span>, <span class="string">'# of cars in second location'</span>, <span class="string">'expected returns'</span>])</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>The results are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Policy improvement <span class="number">0</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">332</span> states changed</div><div class="line">Policy improvement <span class="number">1</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">286</span> states changed</div><div class="line">Policy improvement <span class="number">2</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">83</span> states changed</div><div class="line">Policy improvement <span class="number">3</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">19</span> states changed</div><div class="line">Policy improvement <span class="number">4</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">0</span> states changed</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_policy.png" alt="car_rental_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_return.png" alt="car_rental_return"></p><p>One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to $v_{\pi}$ occurs only in the limit. In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This algorithm is called <em>value iteration</em>. It can be written as a particular simple backup operation that combines the policy improvement and truncated policy evaluation steps:<br>$$<br>\begin{align}<br>v_{k+1} &amp;\doteq \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{k}(s^{\prime})],<br>\end{align}<br>$$<br>for all $s \in \mathcal{S}$.</p><blockquote><p><strong>Value iteration</strong></p><p>Initialize array $V$ arbitrarily (e.g. $V(s) = 0$ for all $s \in \mathcal{S^{+}}$)</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ For each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p><p>Output a deterministic policy, $\pi \approx \pi_{\star}$, such that</p><p>​ $\pi(s) = \arg\max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$</p></blockquote><p>Let us use the value iteration algorithm to solve a Gambler’s Problem. The problem defined as follows:</p><blockquote><p>A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he staked on the flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of \$100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer number of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gambler’s capital, $s \in \{1, 2, \cdots, 99\}$ and the actions are stakes, $a \in \{0, 1, \cdots, \min(s, 100-s)\}$. The reward is zero on all transitions excepted those on which the gambler reaches his goal, when it is +1. The state-value function then gives the probability of winning from each state. A policy is mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal. Let $p_h$ denote the probability of the coin coming up heads. If $p_h$ is known, then the entire problem is known and it can be solved, for instance, by value iteration.</p></blockquote><p>OK, now let us to solve this problem by use the value iteration algorithm.</p><p>The first we defined some facts and some auxiliary data structure:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># goal</span></div><div class="line">GOAL = <span class="number">100</span></div><div class="line"><span class="comment"># all states, including state 0 and state 100</span></div><div class="line">states = np.arange(GOAL + <span class="number">1</span>)</div><div class="line"><span class="comment"># probability of head</span></div><div class="line">headProb = <span class="number">0.4</span></div><div class="line"><span class="comment"># optimal policy</span></div><div class="line">policy = np.zeros(GOAL + <span class="number">1</span>)</div><div class="line"><span class="comment"># state value</span></div><div class="line">stateValue = np.zeros(GOAL + <span class="number">1</span>)</div><div class="line">stateValue[GOAL] = <span class="number">1.0</span></div></pre></td></tr></table></figure><p>The step of value iteration:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># value iteration</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    delta = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states[<span class="number">1</span>:GOAL]:</div><div class="line">        <span class="comment"># get possilbe actions for current state</span></div><div class="line">        actions = np.arange(min(state, GOAL - state) + <span class="number">1</span>)</div><div class="line">        actionReturns = []</div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">            actionReturns.append(headProb * stateValue[state + action] + (<span class="number">1</span> - headProb) * stateValue[state - action])</div><div class="line">        newValue = np.max(actionReturns)</div><div class="line">        delta += np.abs(stateValue[state] - newValue)</div><div class="line">        <span class="comment"># update state value</span></div><div class="line">        stateValue[state] = newValue</div><div class="line">    <span class="keyword">if</span> delta &lt; <span class="number">1e-9</span>:</div><div class="line">        <span class="keyword">break</span></div></pre></td></tr></table></figure><p>Calculate the optimal policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># calculate the optimal policy</span></div><div class="line"><span class="keyword">for</span> state <span class="keyword">in</span> states[<span class="number">1</span>:GOAL]:</div><div class="line">    actions = np.arange(min(state, GOAL - state) + <span class="number">1</span>)</div><div class="line">    actionReturns = []</div><div class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">        actionReturns.append(headProb * stateValue[state + action] + (<span class="number">1</span> - headProb) * stateValue[state - action])</div><div class="line">    <span class="comment"># due to tie and precision, can't reproduce the optimal policy in book</span></div><div class="line">    policy[state] = actions[argmax(actionReturns)]</div></pre></td></tr></table></figure><p>Print the results:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">plt.figure(<span class="number">1</span>)</div><div class="line">plt.xlabel(<span class="string">'Capital'</span>)</div><div class="line">plt.ylabel(<span class="string">'Value estimates'</span>)</div><div class="line">plt.plot(stateValue)</div><div class="line">plt.figure(<span class="number">2</span>)</div><div class="line">plt.scatter(states, policy)</div><div class="line">plt.xlabel(<span class="string">'Capital'</span>)</div><div class="line">plt.ylabel(<span class="string">'Final policy (stake)'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_policy.png" alt="gambler_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_value.png" alt="gambler_value"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>The GridWorld problem</title>
    <link href="http://yoursite.com/2017/05/29/The-GridWorld-problem/"/>
    <id>http://yoursite.com/2017/05/29/The-GridWorld-problem/</id>
    <published>2017-05-29T06:09:26.000Z</published>
    <updated>2017-05-29T06:14:07.620Z</updated>
    
    <content type="html"><![CDATA[<p>A reinforcement learning task that satisfied the Markov property is called <em>Markov decision process</em>, or <em>MDP</em>. If the state and action spaces are finite, then it is called a <em>finite Markov decision process</em> (<em>finite MDP</em>).</p><p>A particular finite MDP is defined by its state and action sets and by the one-step dynamics of the environment. Given any state and action <em>s</em> and <em>a</em>, the probability of each possible pair of next state and reward, <em>s’</em>, <em>r</em>, is denoted<br>$$<br>p(s^{\prime}, r | s, a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime}, R_{t+1}=r \ | \ S_{t}=s, A_{t}=a \}<br>$$<br>Given that, one can compute anything else one might want to know about the environment, such as the excepted rewards of state-action pairs,<br>$$<br>r(s,a) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a] = \sum_{r \in \mathcal{R}}r\sum_{s^{\prime} \in \mathcal{S}}p(s^{\prime},r|s, a)<br>$$<br>the <em>state-transition probabilities</em>,<br>$$<br>p(s^{\prime}|s,a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime} \ | \ S_{t}=s, A_{t}=a\} = \sum_{r \in \mathcal{R}} p(s^{\prime},r|s, a)<br>$$<br>and the excepted rewards for state-action-next-state triples,<br>$$<br>r(s, a, s^{\prime}) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a, S_{t+1}=s^{\prime}] = \frac{\sum_{r \in \mathcal{R}}rp(s^{\prime},r|s,a)}{p(s^{\prime}|s,a)}<br>$$<br>Almost all reinforcement learning algorithms involve estimating <em>value functions</em>–functions of states (or of state-action pairs) that estimate <em>how good</em> it is for the agent to be in a given state (or how good it is to perform a given action in a given state).</p><p>Recall that a policy, $\pi$, is a mapping from a each state, $s \in \mathcal{S}$, and action, $a \in \mathcal{A}(s)$, to the probability $\pi(a|s)$ of taking action <em>a</em> when in state <em>s</em>. Informally, the <em>value</em> of a state <em>s</em> under a policy $\pi$, denoted $v_{\pi}(s)$, is the excepted return when starting in <em>s</em> and following $\pi$ thereafter. For MDPs, we can define $v_{\pi}(s)$ formally as<br>$$<br>v_{\pi}(s) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right]<br>$$<br>Note that the value of the terminal state, if any, is always zero. We call the function $v_{\pi}$ the <em>state-value function for policy $\pi$.</em></p><p>Similarly, we define the value of taking action <em>a</em> in state <em>s</em> under a policy $\pi$, denoted $q_{\pi}(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:<br>$$<br>q_{\pi}(s,a) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s, A_{t}=a] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s, A_{t}=a\right]<br>$$<br>We call $q_{\pi}$ the <em>action-value function for policy $\pi$</em>.</p><p>A fundamental property of the value functions used in reinforcement learning and dynamic programming is that they satisfy particular recursive relationships. For any policy $\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E_{\pi}[G_{t} \ | \ S_{t}=s]} \\<br>&amp;= \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right] \\<br>&amp;= \mathbb{E_{\pi}}\left[R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} \ | \ S_{t}=s \right] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}}\sum_{r}p(s^{\prime},r|s,a) \left[ r + \gamma \mathbb{E_{\pi}} \left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} | S_{t+1}=s^{\prime} \right] \right] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},<br>\end{align}<br>$$<br>Equation (11) is the <em>Bellman equation for $v_{\pi}$</em>.</p><p>Figure 1 (left) shows a rectangular grid world representation of a simple finite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: <strong>north</strong>, <strong>south</strong>, <strong>east</strong>, and <strong>west</strong>, which deterministically cause the agent to move one cell in the respective direction on the grid. Action would take the agent off the grid leave its location unchanged, but also result in a reward of -1. Other actions result in a reward of 0, excepted those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to $\mathrm{A^{\prime}}$. From state B, all actions yield a reward +5 and take the agent to $\mathrm{B^{\prime}}$.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/grid_world/grid_world.png" alt="grid_world"></p><p><em>Figure 1</em></p><p>Suppose the agent selects all four actions with equal probability in all states. Figure 1 (right) shows the value function, $v_{\pi}$, for this policy, for the discounted reward case with $\gamma = 0.9$. This value function was computed by solving the system of linear equations (11).</p><p>OK, now let us solve this problem. The first, we need to define the grid world by code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">WORLD_SIZE = <span class="number">5</span></div><div class="line">A_POS = [<span class="number">0</span>, <span class="number">1</span>]</div><div class="line">A_PRIME_POS = [<span class="number">4</span>, <span class="number">1</span>]</div><div class="line">B_POS = [<span class="number">0</span>, <span class="number">3</span>]</div><div class="line">B_PRIME_POS = [<span class="number">2</span>, <span class="number">3</span>]</div><div class="line">discount = <span class="number">0.9</span></div><div class="line"></div><div class="line">world = np.zeros((WORLD_SIZE, WORLD_SIZE))</div></pre></td></tr></table></figure><p>This world has 5 by 5 cells, and there are four special cells: A, A’, B, B’. Discount represents the $\gamma $ in equation (11). We know that the agent in the world selects all four actions with equal probability in all states (cells). So we have:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># left, up, right, down</span></div><div class="line">actions = [<span class="string">'L'</span>, <span class="string">'U'</span>, <span class="string">'R'</span>, <span class="string">'D'</span>]</div><div class="line"></div><div class="line">actionProb = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">    actionProb.append([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        actionProb[i].append(dict(&#123;<span class="string">'L'</span>:<span class="number">0.25</span>, <span class="string">'U'</span>:<span class="number">0.25</span>, <span class="string">'R'</span>:<span class="number">0.25</span>, <span class="string">'D'</span>:<span class="number">0.25</span>&#125;))</div></pre></td></tr></table></figure><p>The <strong>actionProb</strong> is a list that has five items. Each item represents a row in the grid and it also is a list that has five items that represents a column in corresponding row, that is, each item in a row represents a cell in the grid. In all cells (states), there are four direction could be selected with equal probability 0.25. Then, we’ll define a undirected graph with weights. The node represented the cell in grid. If between two node has a edge then the agent could move between this two nodes (cells). The weight on the edges represents the reward do this move.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">nextState = []</div><div class="line">actionReward = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">    nextState.append([])</div><div class="line">    actionReward.append([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        next = dict()</div><div class="line">        reward = dict()</div><div class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</div><div class="line">            next[<span class="string">'U'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'U'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'U'</span>] = [i - <span class="number">1</span>, j]</div><div class="line">            reward[<span class="string">'U'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> i == WORLD_SIZE - <span class="number">1</span>:</div><div class="line">            next[<span class="string">'D'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'D'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'D'</span>] = [i + <span class="number">1</span>, j]</div><div class="line">            reward[<span class="string">'D'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</div><div class="line">            next[<span class="string">'L'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'L'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'L'</span>] = [i, j - <span class="number">1</span>]</div><div class="line">            reward[<span class="string">'L'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> j == WORLD_SIZE - <span class="number">1</span>:</div><div class="line">            next[<span class="string">'R'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'R'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'R'</span>] = [i, j + <span class="number">1</span>]</div><div class="line">            reward[<span class="string">'R'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> [i, j] == A_POS:</div><div class="line">            next[<span class="string">'L'</span>] = next[<span class="string">'R'</span>] = next[<span class="string">'D'</span>] = next[<span class="string">'U'</span>] = A_PRIME_POS</div><div class="line">            reward[<span class="string">'L'</span>] = reward[<span class="string">'R'</span>] = reward[<span class="string">'D'</span>] = reward[<span class="string">'U'</span>] = <span class="number">10.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> [i, j] == B_POS:</div><div class="line">            next[<span class="string">'L'</span>] = next[<span class="string">'R'</span>] = next[<span class="string">'D'</span>] = next[<span class="string">'U'</span>] = B_PRIME_POS</div><div class="line">            reward[<span class="string">'L'</span>] = reward[<span class="string">'R'</span>] = reward[<span class="string">'D'</span>] = reward[<span class="string">'U'</span>] = <span class="number">5.0</span></div><div class="line"></div><div class="line">        nextState[i].append(next)</div><div class="line">        actionReward[i].append(reward)</div></pre></td></tr></table></figure><p>The <strong>nextState</strong> and <strong>actionReward</strong> are the same as <strong>actionProb</strong> that we explained earlier.</p><p>Now, we could solve this problem by use the equation (11):<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},<br>\end{align}<br>$$<br>Let us jump into the implementation detail.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep iteration until convergence</span></div><div class="line">    newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                newPosition = nextState[i][j][action]</div><div class="line">                <span class="comment"># bellman equation</span></div><div class="line">                newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + discount * world[newPosition[<span class="number">0</span>], newPosition[<span class="number">1</span>]])</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(world - newWorld)) &lt; <span class="number">1e-4</span>:</div><div class="line">        print(<span class="string">'Random Policy'</span>)</div><div class="line">        print(newWorld)</div><div class="line">        <span class="keyword">break</span></div><div class="line">    world = newWorld</div></pre></td></tr></table></figure><p>The core code is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + 		 discount * world[newPosition[<span class="number">0</span>], newPosition[<span class="number">1</span>]])</div></pre></td></tr></table></figure><p>The <code>+=</code> represents the first sum notation in the equation (11). If we ensure the current state (cell) and action will take in this world, then the next state and reward also will be ensured. So $\sum_{s^{\prime},r} p(s^{\prime}, r | s, a)$ is equal to 1.</p><p>The result as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Random Policy</div><div class="line">[[ <span class="number">3.30902999</span>  <span class="number">8.78932551</span>  <span class="number">4.42765281</span>  <span class="number">5.3224012</span>   <span class="number">1.49221235</span>]</div><div class="line"> [ <span class="number">1.52162172</span>  <span class="number">2.9923515</span>   <span class="number">2.25017358</span>  <span class="number">1.90760531</span>  <span class="number">0.5474363</span> ]</div><div class="line"> [ <span class="number">0.05085614</span>  <span class="number">0.73820423</span>  <span class="number">0.67314689</span>  <span class="number">0.35821982</span> <span class="number">-0.40310755</span>]</div><div class="line"> [<span class="number">-0.97355865</span> <span class="number">-0.43546179</span> <span class="number">-0.35484864</span> <span class="number">-0.58557148</span> <span class="number">-1.18304148</span>]</div><div class="line"> [<span class="number">-1.8576669</span>  <span class="number">-1.34519762</span> <span class="number">-1.22923364</span> <span class="number">-1.42288454</span> <span class="number">-1.97514545</span>]]</div></pre></td></tr></table></figure><p>We can see the value of all states is the same as the Figure 1.</p><p>Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi^{\prime}$ if its excepted return is greater than or equal to that of $\pi^{\prime}$ for all states. In other words, $\pi \ge \pi^{\prime}$ if and only if $v_{\pi}(s) \ge v_{\pi^{\prime}}(s)$ for all $s \in \mathcal{S}$. There is always at least one policy that is better than or equal to all other policies. This is an <em>optimal policy</em>. Although there may be more than one, we denote all the optimal policies by $\pi_{\star}$. They share the same state-value function, called the <em>optimal state-value function</em>, denote $v_{\star}$, and defined as<br>$$<br>v_{\star}(s) \doteq \max_{\pi} v_{\pi}(s),<br>$$<br>for all $s \in \mathcal{S}$.</p><p>Optimal policies also share the same <em>optimal action-value function</em>, denoted $q_{\star}$, and defined as<br>$$<br>q_{\star}(s, a) \doteq \max_{\pi} q_{\pi}(s, a)<br>$$<br>for all $s \in \mathcal{S}$ and $a \in \mathcal{A(s)}$. For the state-action pair <em>(s, a)</em>, this function gives the excepted return for taking action <em>a</em> in state <em>s</em> and thereafter following an optimal policy. Thus, we can write $q_{\star}$ in terms of $v_{\star}$ as follows:<br>$$<br>q_{\star}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\star} \ | \ S_{t}=s, A_{t}=a]<br>$$<br>Suppose we solve the Bellman equation for $v_{\star}$ for the simple grid task introduced in earlier and shown again in Figure 2 (left). Recall that state A is followed by a reward of +10 and transition to state A’. while state B is followed by a reward of +5 and transition to state B’. Figure 2 (middle) shows the optimal value function, and Figure 2 (right) shows the corresponding optimal policies. Where there are multiple arrows in a cell, any of the corresponding actions are optimal.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/grid_world/optimal_value.png" alt="optimal_value"></p><p><em>Figure 2</em></p><p>Now, let us solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">world = np.zeros((WORLD_SIZE, WORLD_SIZE))</div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep iteration until convergence</span></div><div class="line">    newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">            values = []</div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                newPosition = nextState[i][j][action]</div><div class="line">                <span class="comment"># value iteration</span></div><div class="line">                values.append(actionReward[i][j][action] + discount * world[newPosition[<span class="number">0</span>], newPosition[<span class="number">1</span>]])</div><div class="line">            newWorld[i][j] = np.max(values)</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(world - newWorld)) &lt; <span class="number">1e-4</span>:</div><div class="line">        print(<span class="string">'Optimal Policy'</span>)</div><div class="line">        print(newWorld)</div><div class="line">        <span class="keyword">break</span></div><div class="line">    world = newWorld</div></pre></td></tr></table></figure><p>We can see the core code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">newWorld[i][j] = np.max(values)</div></pre></td></tr></table></figure><p>The only difference between this code and the earlier code is the prior only uses the maximum value and the latter uses the weighted average.</p><p>The result is below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Optimal Policy</div><div class="line">[[ <span class="number">21.97744338</span>  <span class="number">24.41938153</span>  <span class="number">21.97744338</span>  <span class="number">19.41938153</span>  <span class="number">17.47744338</span>]</div><div class="line"> [ <span class="number">19.77969904</span>  <span class="number">21.97744338</span>  <span class="number">19.77969904</span>  <span class="number">17.80172914</span>  <span class="number">16.02153504</span>]</div><div class="line"> [ <span class="number">17.80172914</span>  <span class="number">19.77969904</span>  <span class="number">17.80172914</span>  <span class="number">16.02153504</span>  <span class="number">14.41938153</span>]</div><div class="line"> [ <span class="number">16.02153504</span>  <span class="number">17.80172914</span>  <span class="number">16.02153504</span>  <span class="number">14.41938153</span>  <span class="number">12.97744338</span>]</div><div class="line"> [ <span class="number">14.41938153</span>  <span class="number">16.02153504</span>  <span class="number">14.41938153</span>  <span class="number">12.97744338</span>  <span class="number">11.67969904</span>]]</div></pre></td></tr></table></figure><p>It is not doubt that the result is the same as the Figure 2 (middle).</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A reinforcement learning task that satisfied the Markov property is called &lt;em&gt;Markov decision process&lt;/em&gt;, or &lt;em&gt;MDP&lt;/em&gt;. If the stat
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>k-Armed Bandit Problem</title>
    <link href="http://yoursite.com/2017/05/27/k-Armed-Bandit-Problem/"/>
    <id>http://yoursite.com/2017/05/27/k-Armed-Bandit-Problem/</id>
    <published>2017-05-27T04:27:49.000Z</published>
    <updated>2017-06-13T06:00:32.852Z</updated>
    
    <content type="html"><![CDATA[<p>Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or <em>time steps</em>.</p><p>This is the original form of the <em>k-armed bandit problem</em>. Each of the <em>k</em> actions has an excepted or mean reward given that action is selected; let us call this <em>value</em> of that action. We denote the action selected on time step <em>t</em> as $A_{t}$, and the corresponding reward as $R_{t}$. The value then of an arbitrary action <em>a</em>, denoted $q_{\star}(a)$, is the excepted reward given that <em>a</em> is selected:<br>$$<br>q_{\star}(a) = \mathbb{E}[R_t|A_t=a]<br>$$<br>If you knew the value of each action, then it would be trivial to solve the <em>k</em>-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action <em>a</em> at time <em>t</em> as $Q_{t}(a) \approx q_{\star}(a)$.</p><p>We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when action is selected. One natural way to estimate this is by averaging the rewards actually received:</p><p>$$Q_t(a) \doteq \frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbf{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbf{1}_{A_i=a}}$$</p><p>where $\mathbf{1}_{\text{predicate}}$ denotes the random variable that is 1 if <em>predicate</em> is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as $Q_1(a) = 0$. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_{\star} (a)$. We call this the <strong><em>sample-average</em></strong> method for estimating action values because each estimate is an average of the sample of relevant rewards.</p><p>The simplest action selection rule is to select the action (or one of the actions) with highest estimated action value, that is, to select at step <em>t</em> one of the greedy actions, $A_t^\star$ for which $Q_t(A_t^\star) = \max_a Q_t(a)$. This <em>greedy</em> action selection method can be written as<br>$$<br>A_t \doteq argmax_a Q_t(a)<br>$$<br>Naturally, we could use the <em>$\epsilon$-greedy</em> method rather the <em>greedy</em> method. We’ll show their difference on the performance. Now, let’s jump into the implementation details. In order to be able to see the results quickly, we set to <em>k</em> to be <em>10</em>. The first, we generate 10 stationary probability distributions that we’ll sample from to generate action values. The generate method is below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data=np.random.randn(<span class="number">200</span>,<span class="number">10</span>) + np.random.randn(<span class="number">10</span>)</div></pre></td></tr></table></figure><p>We first generate randomly 10 true excepted values by <code>np.random.randn(10)</code>, then I’m going to change the variance to 1. So we get 10 stationary probability distributions. The visualizations are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/action_value_distributions.png" alt="action_value_distributions"></p><p>We’re going to compare how different $\epsilon$ values affect the end result.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">epsilonGreedy</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    epsilons = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.01</span>]</div><div class="line">    bandits = []</div><div class="line">    <span class="keyword">for</span> epsInd, eps <span class="keyword">in</span> enumerate(epsilons):</div><div class="line">        bandits.append([Bandit(epsilon=eps, sampleAverages=<span class="keyword">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)])</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, counts <span class="keyword">in</span> zip(epsilons, bestActionCounts):</div><div class="line">        plt.plot(counts, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, rewards <span class="keyword">in</span> zip(epsilons, averageRewards):</div><div class="line">        plt.plot(rewards, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Before we go into the details, we introduce the <strong>Bandit</strong> object first.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bandit</span>:</span></div><div class="line">    <span class="comment"># @kArm: # of arms</span></div><div class="line">    <span class="comment"># @epsilon: probability for exploration in epsilon-greedy algorithm</span></div><div class="line">    <span class="comment"># @initial: initial estimation for each action</span></div><div class="line">    <span class="comment"># @stepSize: constant step size for updating estimations</span></div><div class="line">    <span class="comment"># @sampleAverages: if True, use sample averages to update estimations instead of constant step size</span></div><div class="line">    <span class="comment"># @UCB: if not None, use UCB algorithm to select action</span></div><div class="line">    <span class="comment"># @gradient: if True, use gradient based bandit algorithm</span></div><div class="line">    <span class="comment"># @gradientBaseline: if True, use average reward as baseline for gradient based bandit algorithm</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kArm=<span class="number">10</span>, epsilon=<span class="number">0.</span>, initial=<span class="number">0.</span>, stepSize=<span class="number">0.1</span>, sampleAverages=False, UCBParam=None, gradient=False, gradientBaseline=False, trueReward=<span class="number">0.</span>)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div></pre></td></tr></table></figure><p>For now we just introduce <em>sample-average</em> method, so skip other methods parameters. Let us see the initialization method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kArm=<span class="number">10</span>, epsilon=<span class="number">0.</span>, initial=<span class="number">0.</span>, stepSize=<span class="number">0.1</span>, sampleAverages=False, UCBParam=None,</span></span></div><div class="line">             gradient=False, gradientBaseline=False, trueReward=<span class="number">0.</span>):</div><div class="line">    self.k = kArm</div><div class="line">    self.stepSize = stepSize</div><div class="line">    self.sampleAverages = sampleAverages</div><div class="line">    self.indices = np.arange(self.k)</div><div class="line">    self.time = <span class="number">0</span></div><div class="line">    self.UCBParam = UCBParam</div><div class="line">    self.gradient = gradient</div><div class="line">    self.gradientBaseline = gradientBaseline</div><div class="line">    self.averageReward = <span class="number">0</span></div><div class="line">    self.trueReward = trueReward</div><div class="line"></div><div class="line">    <span class="comment"># real reward for each action</span></div><div class="line">    self.qTrue = []</div><div class="line"></div><div class="line">    <span class="comment"># estimation for each action</span></div><div class="line">    self.qEst = np.zeros(self.k)</div><div class="line"></div><div class="line">    <span class="comment"># # of chosen times for each action</span></div><div class="line">    self.actionCount = []</div><div class="line"></div><div class="line">    self.epsilon = epsilon</div><div class="line"></div><div class="line">    <span class="comment"># initialize real rewards with N(0,1) distribution and estimations with desired initial value</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.k):</div><div class="line">        self.qTrue.append(np.random.randn() + trueReward)</div><div class="line">        self.qEst[i] = initial</div><div class="line">        self.actionCount.append(<span class="number">0</span>)</div><div class="line"></div><div class="line">    self.bestAction = np.argmax(self.qTrue)</div></pre></td></tr></table></figure><p>There are some important attributes. <strong>time</strong> is a number that represents the time steps now. <strong>actionCount</strong> is the times that correspond actions have been taken prior to current time steps. <strong>qTrue</strong> is a list. And each item is the true excepted value corresponding to each action. <strong>qEst</strong> is the estimate value of each action. It’s initialized to zero. <strong>epsilon</strong> is the $\epsilon$. Next, in the for loop, we initialize real rewards with N(0, 1) distribution and estimations with desired initial value. At last, the <strong>bestAction</strong> store the current best action will be take.</p><p>The next method tell us how to get the next action should be take:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># explore</span></div><div class="line">    <span class="keyword">if</span> self.epsilon &gt; <span class="number">0</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, self.epsilon) == <span class="number">1</span>:</div><div class="line">            np.random.shuffle(self.indices)</div><div class="line">            <span class="keyword">return</span> self.indices[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># exploit</span></div><div class="line">    <span class="keyword">if</span> self.UCBParam <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        UCBEst = self.qEst + \</div><div class="line">                 self.UCBParam * np.sqrt(np.log(self.time + <span class="number">1</span>) / (np.asarray(self.actionCount) + <span class="number">1</span>))</div><div class="line">        <span class="keyword">return</span> np.argmax(UCBEst)</div><div class="line">    <span class="keyword">if</span> self.gradient:</div><div class="line">        expEst = np.exp(self.qEst)</div><div class="line">        self.actionProb = expEst / np.sum(expEst)</div><div class="line">        <span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div><div class="line">    <span class="keyword">return</span> np.argmax(self.qEst)</div></pre></td></tr></table></figure><p>We can skip the second and the third if statements (we’ll introduce this two methods later). If we use <em>greedy</em> method, we just return the action that has highest value. Otherwise, we’re choosing randomly at $\epsilon$ probability.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div><div class="line">    <span class="comment"># generate the reward under N(real reward, 1)</span></div><div class="line">    reward = np.random.randn() + self.qTrue[action]</div><div class="line">    self.time += <span class="number">1</span></div><div class="line">    self.averageReward = (self.time - <span class="number">1.0</span>) / self.time * self.averageReward + reward / self.time</div><div class="line">    self.actionCount[action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> self.sampleAverages:</div><div class="line">        <span class="comment"># update estimation using sample averages</span></div><div class="line">        self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">elif</span> self.gradient:</div><div class="line">        oneHot = np.zeros(self.k)</div><div class="line">        oneHot[action] = <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> self.gradientBaseline:</div><div class="line">            baseline = self.averageReward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            baseline = <span class="number">0</span></div><div class="line">        self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># update estimation with constant step size</span></div><div class="line">        self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">return</span> reward</div></pre></td></tr></table></figure><p>Similarly, we just skip other if statements and focus on this row:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>This formula seems different with the earlier one. The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. We now turn to the question of how these averages can be computed in a computationally efficient manner, in particular, with constant memory and per-time-step computation.</p><p>To simplify notation we concentrate on a single action. Let $R_i$ now denote the reward received after <em>i</em>th selection of <em>this action</em>, and let $Q_n$ denote the estimate of its action value after it has been select $n-1$ times, which we can now write simply as<br>$$<br>Q_n \doteq \frac{R_1 + R_2 + \cdots + R_{n-1}}{n-1}<br>$$<br>The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, in this case the memory and computational requirements would grow over time as more rewards are seen.</p><p>It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given $Q_n$ and the <em>n</em>th reward, $R_n$, the new average of all <em>n</em> rewards can be computed by<br>$$<br>\begin{align}<br>Q_{n+1} &amp;\doteq \frac{1}{n}\sum_{i=1}^{n} R_i \\<br>&amp;= \frac{1}{n}(R_n + \sum_{i=1}^{n-1} R_i) \\<br>&amp;= \frac{1}{n}(R_n + (n-1) \frac{1}{n-1}\sum_{i=1}^{n-1} R_i) \\<br>&amp;= \frac{1}{n}(R_n + (n-1) Q_n) \\<br>&amp;= \frac{1}{n}(R_n + n Q_n - Q_n) \\<br>&amp;= Q_n + \frac{1}{n}[R_n - Q_n]<br>\end{align}<br>$$<br>So this is why the code is look like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>Back to <strong>epsilonGreedy()</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">epsilonGreedy</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    epsilons = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.01</span>]</div><div class="line">    bandits = []</div><div class="line">    <span class="keyword">for</span> epsInd, eps <span class="keyword">in</span> enumerate(epsilons):</div><div class="line">        bandits.append([Bandit(epsilon=eps, sampleAverages=<span class="keyword">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)])</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, counts <span class="keyword">in</span> zip(epsilons, bestActionCounts):</div><div class="line">        plt.plot(counts, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, rewards <span class="keyword">in</span> zip(epsilons, averageRewards):</div><div class="line">        plt.plot(rewards, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Now, we get <strong>nBandits</strong> bandits and each bandit has 10 arm. Then we use a bandit simulation to simulate this process.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">banditSimulation</span><span class="params">(nBandits, time, bandits)</span>:</span></div><div class="line">    bestActionCounts = [np.zeros(time, dtype=<span class="string">'float'</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, len(bandits))]</div><div class="line">    averageRewards = [np.zeros(time, dtype=<span class="string">'float'</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, len(bandits))]</div><div class="line">    <span class="keyword">for</span> banditInd, bandit <span class="keyword">in</span> enumerate(bandits):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nBandits):</div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, time):</div><div class="line">                action = bandit[i].getAction()</div><div class="line">                reward = bandit[i].takeAction(action)</div><div class="line">                averageRewards[banditInd][t] += reward</div><div class="line">                <span class="keyword">if</span> action == bandit[i].bestAction:</div><div class="line">                    bestActionCounts[banditInd][t] += <span class="number">1</span></div><div class="line">        bestActionCounts[banditInd] /= nBandits</div><div class="line">        averageRewards[banditInd] /= nBandits</div><div class="line">    <span class="keyword">return</span> bestActionCounts, averageRewards</div></pre></td></tr></table></figure><p>The <strong>bandits</strong> is a list that has three item. Each item is a list that contains <strong>nBandits</strong> bandits that has a corresponding epsilon value. We calculate the average total reward of 2000 bandits is to eliminate the effect of noise. Ok, let us see the final result.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_optimal_action.png" alt="epsilon_greedy_optimal_action"><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_average_reward.png" alt="epsilon_greedy_average_reward"></p><p>We can see the algorithm reaches the best performance when epsilon is set to 0.1.</p><p>The averaging methods discussed so far are appropriate in a stationary environment, but not if the bandit is changing over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to weight recent rewards more heavily than long-past ones. One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule for updating an average $Q_n$ of the $n-1$ past rewards is modified to be<br>$$<br>Q_{n+1} \doteq Q_n + \alpha [R_n - Q_n]<br>$$<br>where the step-size parameter $\alpha \in (0, 1]$ is constant. This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_1$:<br>$$<br>\begin{align}<br>Q_{n+1} &amp;\doteq Q_n + \alpha [R_n - Q_n] \\<br>&amp;= \alpha R_n + (1 - \alpha) Q_n \\<br>&amp;= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] \\<br>&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 Q_{n-1} \\<br>&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 R_{n-1} + \cdots + (1 - \alpha) ^ {n-1} \alpha R_1 + (1 - \alpha) ^ n Q_1 \\<br>&amp;= (1 - \alpha) ^ n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha) ^ {n-i} R_i<br>\end{align}<br>$$<br>We call this a <em>weighted average</em> because the sum of the weights is 1. In fact, the weight decays exponentially according to the exponent on $1-\alpha$. According, this is sometimes called an <em>exponential, recency-weighted average</em>.</p><p>Sometimes it is convenient to vary the step-size parameter from step to step. Let $\alpha_n(a)$ denote the step-size parameter used to process the reward received after <em>n</em>th selection of action <em>a</em>. As we noted, the choice $\alpha_n(a)=\frac{1}{n}$ results in the sample-average method, which is guaranteed to converge to the true action values by the law of the large numbers. A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:<br>$$<br>\sum_{n=1}^{\infty}\alpha_{n}(a) = \infty \; \text{and} \; \sum_{n=1}^{\infty}\alpha_{n}^{2}(a) &lt; \infty<br>$$<br>All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, $Q_1(a)$. <strong>In the language of statistics, these methods are <em>biased</em> by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant $\alpha$ (weighted avetrage), the bias is permanent, though decreasing over time. In practice, this kind of bias is usually not a problem and can sometimes be very helpful.</strong> The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be excepted. So, let us do a experiment. The first we set them all to zero and the we set them all to a constant, 5.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimisticInitialValues</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    bandits = [[], []]</div><div class="line">    bandits[<span class="number">0</span>] = [Bandit(epsilon=<span class="number">0</span>, initial=<span class="number">5</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bandits[<span class="number">1</span>] = [Bandit(epsilon=<span class="number">0.1</span>, initial=<span class="number">0</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(bestActionCounts[<span class="number">0</span>], label=<span class="string">'epsilon = 0, q = 5'</span>)</div><div class="line">    plt.plot(bestActionCounts[<span class="number">1</span>], label=<span class="string">'epsilon = 0.1, q = 0'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(averageRewards[<span class="number">0</span>], label=<span class="string">'epsilon=0, initial=5, stepSize=0.1'</span>)</div><div class="line">    plt.plot(averageRewards[<span class="number">1</span>], label=<span class="string">'epsilon=0.1, initial=0, stepSize=0.1'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The <strong>Bandit</strong> object’s <strong>takeAction()</strong> has a little difference:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values.png" alt="optimistic_initial_value_optimal_action"><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values_average_reward.png" alt="optimistic_initial_value_optimal_action"></p><p>We can see it reaches the better performance than the $\epsilon$-greedy method, when they are all used the weighted average method. Notice that the $\epsilon$-greedy with weighted-average method is worsen than the $\epsilon$-greedy with sample-average method.</p><p>We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial state in any special way is unlikely to help with the general nonstationary case.</p><p>Exploration is needed because the estimates of the action values are uncertain. The greedy actions are those that look best at present, but some of the other actions may actually be better. $\epsilon$-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions as<br>$$<br>A_t \doteq argmax_a \left [Q_t(a) + c\sqrt{\frac{\ln{t}}{N_t(a)}}\ \right]<br>$$<br>where $N_t(a)$ denotes the number of times that action <em>a</em> has been selected prior to time <em>t</em>, and the number $c&gt;0$ controls the degree of exploration. If $N_t(a)=0$, then <em>a</em> is considered to be a maximizing action. The idea of this is called <em>upper confidence bound</em> (UCB). Let us implement it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ucb</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    bandits = [[], []]</div><div class="line">    bandits[<span class="number">0</span>] = [Bandit(epsilon=<span class="number">0</span>, stepSize=<span class="number">0.1</span>, UCBParam=<span class="number">2</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bandits[<span class="number">1</span>] = [Bandit(epsilon=<span class="number">0.1</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    _, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(averageRewards[<span class="number">0</span>], label=<span class="string">'UCB c = 2'</span>)</div><div class="line">    plt.plot(averageRewards[<span class="number">1</span>], label=<span class="string">'epsilon greedy epsilon = 0.1'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>We note that the <strong>UCBParam=2</strong>. The Bandit object explains this. The <strong>getAction()</strong> method and <strong>takeAction()</strong> method are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># explore</span></div><div class="line">    <span class="keyword">if</span> self.epsilon &gt; <span class="number">0</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, self.epsilon) == <span class="number">1</span>:</div><div class="line">            np.random.shuffle(self.indices)</div><div class="line">            <span class="keyword">return</span> self.indices[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># exploit</span></div><div class="line">    <span class="keyword">if</span> self.UCBParam <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        UCBEst = self.qEst + \</div><div class="line">                 self.UCBParam * np.sqrt(np.log(self.time + <span class="number">1</span>) / (np.asarray(self.actionCount) + <span class="number">1</span>))</div><div class="line">        <span class="keyword">return</span> np.argmax(UCBEst)</div><div class="line">    <span class="keyword">if</span> self.gradient:</div><div class="line">        expEst = np.exp(self.qEst)</div><div class="line">        self.actionProb = expEst / np.sum(expEst)</div><div class="line">        <span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div><div class="line">    <span class="keyword">return</span> np.argmax(self.qEst)</div><div class="line"></div><div class="line"><span class="comment"># take an action, update estimation for this action</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div><div class="line">    <span class="comment"># generate the reward under N(real reward, 1)</span></div><div class="line">    reward = np.random.randn() + self.qTrue[action]</div><div class="line">    self.time += <span class="number">1</span></div><div class="line">    self.averageReward = (self.time - <span class="number">1.0</span>) / self.time * self.averageReward + reward / self.time</div><div class="line">    self.actionCount[action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> self.sampleAverages:</div><div class="line">        <span class="comment"># update estimation using sample averages</span></div><div class="line">        self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">elif</span> self.gradient:</div><div class="line">        oneHot = np.zeros(self.k)</div><div class="line">        oneHot[action] = <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> self.gradientBaseline:</div><div class="line">            baseline = self.averageReward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            baseline = <span class="number">0</span></div><div class="line">        self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># update estimation with constant step size</span></div><div class="line">        self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">return</span> reward</div></pre></td></tr></table></figure><p>We can see the policy get next action has changed but the update policy has not changed. The result is here:<img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/ucb_average_reward.png" alt="ucb_average_reward"></p><p>We omit the figure about the optimal action because the trend of this two figures are the same. UCB will often perform well, but is more difficult than $\epsilon$-greedy to extend beyond bandits to the more general reinforcement learning setting considered in the more advanced problems. In these more advanced settings there is currently no known practical way of utilizing the idea of UCB action selection.</p><p>So far we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. Now, we consider learning a numerical <em>preference</em> $H_t(a)$ for each action <em>a</em>. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the preferences there is no effect on the action probabilities, which are determined according to a softmax distribution as follows:<br>$$<br>Pr\{A_t=a\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} \doteq \pi_t(a)<br>$$<br>where here we have also introduced a useful new notation $\pi_t(a)$ for the probability of taking action <em>a</em> at time <em>t</em>. Initially all preferences are the same (e.g., $H_1(a)=0, \forall a$) so that all actions have an equal probability of being selected.</p><p>There is a natural learning algorithm for this setting based on the idea of stochastic gradient ascent. On each step, after selection the action $A_t$ and receiving the reward $R_t$, the preferences are updated by:<br>$$<br>\begin{align}<br>H_{t+1}(A_t) \doteq H_t(A_t) + \alpha (R_t - \overline{R_t}) (1 - \pi_t(A_t)), \; \text{and} \\<br>H_{t+1}(a) \doteq H_t(a) - \alpha (R_t - \overline{R_t}) \pi_t(a), \;\;\;\;\;\; \forall{a \neq A_t}<br>\end{align}<br>$$<br>where $\alpha&gt;0$ is a ste-size parameter, and $\overline{R_t} \in \mathbb{R}$ is the average of all the rewards up through and including time <em>t</em>, which can be computed incrementally. The $\overline{R_t}$ term serves as a <strong><em>baseline</em></strong> with which the reward is compared. Let us see the implement details (takeAction() method and getAction() method).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> self.gradient:</div><div class="line">	expEst = np.exp(self.qEst)</div><div class="line">	self.actionProb = expEst / np.sum(expEst)</div><div class="line">	<span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">elif</span> self.gradient:</div><div class="line">	oneHot = np.zeros(self.k)</div><div class="line">	oneHot[action] = <span class="number">1</span></div><div class="line">	<span class="keyword">if</span> self.gradientBaseline:</div><div class="line">		baseline = self.averageReward</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		baseline = <span class="number">0</span></div><div class="line">	self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot self.actionProb)</div></pre></td></tr></table></figure><p>The below figure shows results with the gradient bandit algorithm on a variant of the 10-armed testbed in which the true excepted rewards were selected according to a normal distribution with a mean of +4 instead of zero (and with unit variance as before).</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/gradient_bandit_optimal_action.png" alt="gradient_bandit_optimal_action"></p><p>Despite their simplicity, in our opinion the methods presented in here can fairly be considered the state of the art. Finally, we do a parameter study of the various bandit algorithms presented in here.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">figure2_6</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    labels = [<span class="string">'epsilon-greedy'</span>, <span class="string">'gradient bandit'</span>,</div><div class="line">              <span class="string">'UCB'</span>, <span class="string">'optimistic initialization'</span>]</div><div class="line">    generators = [<span class="keyword">lambda</span> epsilon: Bandit(epsilon=epsilon, sampleAverages=<span class="keyword">True</span>),</div><div class="line">                  <span class="keyword">lambda</span> alpha: Bandit(gradient=<span class="keyword">True</span>, stepSize=alpha, gradientBaseline=<span class="keyword">True</span>),</div><div class="line">                  <span class="keyword">lambda</span> coef: Bandit(epsilon=<span class="number">0</span>, stepSize=<span class="number">0.1</span>, UCBParam=coef),</div><div class="line">                  <span class="keyword">lambda</span> initial: Bandit(epsilon=<span class="number">0</span>, initial=initial, stepSize=<span class="number">0.1</span>)]</div><div class="line">    parameters = [np.arange(<span class="number">-7</span>, <span class="number">-1</span>),</div><div class="line">                  np.arange(<span class="number">-5</span>, <span class="number">2</span>),</div><div class="line">                  np.arange(<span class="number">-4</span>, <span class="number">3</span>),</div><div class="line">                  np.arange(<span class="number">-2</span>, <span class="number">3</span>)]</div><div class="line"></div><div class="line">    bandits = [[generator(math.pow(<span class="number">2</span>, param)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)] <span class="keyword">for</span> generator, parameter <span class="keyword">in</span> zip(generators, parameters) <span class="keyword">for</span> param <span class="keyword">in</span> parameter]</div><div class="line">    _, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    rewards = np.sum(averageRewards, axis=<span class="number">1</span>)/time</div><div class="line"></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    i = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> label, parameter <span class="keyword">in</span> zip(labels, parameters):</div><div class="line">        l = len(parameter)</div><div class="line">        plt.plot(parameter, rewards[i:i+l], label=label)</div><div class="line">        i += l</div><div class="line">    plt.xlabel(<span class="string">'Parameter(2^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The results as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/bandit_algorithms_parameter_study.png" alt="parameters_study"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choic
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
      <category term="reinforcement learning" scheme="http://yoursite.com/tags/reinforcement-learning/"/>
    
  </entry>
  
</feed>
