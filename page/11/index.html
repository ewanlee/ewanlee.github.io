<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/11/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/11/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/27/Store-Management-System/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/06/27/Store-Management-System/" itemprop="url">Store Management System</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-27T11:32:49+08:00">2017-06-27 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/06/27/Store-Management-System/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/06/27/Store-Management-System/" itemprop="commentsCount"></span> </a></span><span id="/2017/06/27/Store-Management-System/" class="leancloud_visitors" data-flag-title="Store Management System"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="SMS"><a href="#SMS" class="headerlink" title="SMS"></a>SMS</h1><p>SMS (Store Management System), 一个简单的网店管理系统。</p><p>源码：<a href="https://github.com/ewanlee/sms" target="_blank" rel="external">https://github.com/ewanlee/sms</a></p><p>这是一个用于展示微服务的 proof-of-concept 应用，运用了Spring Boot, Spring Cloud 以及 Docker部署。</p><h2 id="核心服务"><a href="#核心服务" class="headerlink" title="核心服务"></a>核心服务</h2><p>SHOP 分成了三个核心微服务，它们都是独立开发的，采用了Spring MVC架构：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/services.png" alt="services"></p><p>Order service</p><p>进行订单的添加，删除，以及显示</p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Path</th><th style="text-align:center">Description</th><th style="text-align:center">User authenticated</th><th style="text-align:center">Available from UI</th></tr></thead><tbody><tr><td style="text-align:center">GET</td><td style="text-align:center">/</td><td style="text-align:center">返回订单列表</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/form</td><td style="text-align:center">增加订单，并进行用户选择</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/line</td><td style="text-align:center">增加一条订单到数据库</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/{id}</td><td style="text-align:center">显示某一条订单的详情</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/</td><td style="text-align:center">增加订单行为</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">DELETE</td><td style="text-align:center">/{id}</td><td style="text-align:center">删除订单</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr></tbody></table><p>Customer service</p><p>进行用户的添加，删除，以及显示</p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Path</th><th style="text-align:center">Description</th><th style="text-align:center">User authenticated</th><th style="text-align:center">Available from UI</th></tr></thead><tbody><tr><td style="text-align:center">GET</td><td style="text-align:center">/list</td><td style="text-align:center">返回用户列表</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/{id}</td><td style="text-align:center">返回指定id的用户详情</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/form</td><td style="text-align:center">返回增加用户界面</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/form</td><td style="text-align:center">增加用户</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr><td style="text-align:center">PUT</td><td style="text-align:center">/{id}</td><td style="text-align:center">增加用户行为</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">DELETE</td><td style="text-align:center">/{id}</td><td style="text-align:center">删除用户</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr></tbody></table><p>Catalog service</p><p>进行商品的添加，删除，以及显示</p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Path</th><th style="text-align:center">Description</th><th style="text-align:center">User authenticated</th><th style="text-align:center">Available from UI</th></tr></thead><tbody><tr><td style="text-align:center">GET</td><td style="text-align:center">/list</td><td style="text-align:center">返回商品列表</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/{id}</td><td style="text-align:center">返回指定id的商品详情</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/form</td><td style="text-align:center">返回增加商品界面</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/form</td><td style="text-align:center">增加商品</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr><td style="text-align:center">PUT</td><td style="text-align:center">/{id}</td><td style="text-align:center">增加商品行为</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">DELETE</td><td style="text-align:center">/{id}</td><td style="text-align:center">删除商品</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">TEXT_HTML_VALUE</td><td style="text-align:center">/searchForm</td><td style="text-align:center">返回搜索界面</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">TEXT_HTML_VALUE</td><td style="text-align:center">/searchByName</td><td style="text-align:center">返回搜索结果</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr></tbody></table><p>注意</p><ul><li>每个微服务都有自己的数据库，因此互相之间没有直接访问数据库的接口</li><li>这里的数据库使用的是spring框架自带的数据库</li><li>服务到服务的通信非常简单，通过暴露的接口即可</li></ul><h2 id="架构服务"><a href="#架构服务" class="headerlink" title="架构服务"></a>架构服务</h2><p>分布式系统中有一些通用的模式，Spring Cloud框架都有提供，在本项目中仅仅运用了一小部分：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/arch.png" alt="arch"></p><h3 id="API-网关"><a href="#API-网关" class="headerlink" title="API 网关"></a>API 网关</h3><p>可以看到，有三个核心服务，它将外部API暴露给客户端。在一个现实世界的系统中，核心服务的数量可以非常快速地增长，并且整个系统的复杂性更是急剧增加。实际上，一个复杂的网页可能需要渲染数百个服务。</p><p>理论上，客户端可以直接向每个微服务器发出请求。但是显然，这将面临很大的挑战以及限制。比如必须要知道所有端点的地址。</p><p>通常一个更好的方法是使用API网关。它是系统中的单个入口点，用于通过将请求路由到适当的后端服务或通过调用多个后端服务并聚合结果来处理请求。此外，它还可以用于认证，压力测试，服务迁移，静态响应处理，主动流量管理等</p><p>Netflix开辟了这样一个优势服务，现在使用Spring Cloud，我们可以通过一个@EnableZuulProxy注释来实现。在这个项目中使用了Zuul存储静态内容（ui应用程序），并将请求路由到适当的微服务器。</p><p>Zuul使用服务发现机制来定位服务实例以及断路器和负载平衡器，如下所述。</p><h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p>另外一个众所周知的架构模式便是服务发现机制。它可以进行服务实例网络位置的动态检测。当应用需要扩展、容错或者升级的时候就可以自动为服务实例分配地址。</p><p>服务发现机制的核心是注册阶段。本项目使用了 Netflix Eureka。 Eureka是一个客户端的发现模式，因为很多网络应用都需要客户端自己去确定特定服务的地址（使用注册服务器）并且进行请求的负载均衡。</p><p>使用Spring Boot时，只要在pom文件中加入spring-cloud-starter-eureka-server依赖并且使用@EnableEurekaServer注解即可使用该服务。</p><h3 id="负载均衡、断路器以及Http客户端"><a href="#负载均衡、断路器以及Http客户端" class="headerlink" title="负载均衡、断路器以及Http客户端"></a>负载均衡、断路器以及Http客户端</h3><p>Netflix还提供了另外一些十分好用的工具。</p><h4 id="Ribbon"><a href="#Ribbon" class="headerlink" title="Ribbon"></a>Ribbon</h4><p>Ribbon 是一个客户端的负载均衡器。相比传统的均衡器，你可以之间链接到相关服务。Ribbon已经和Spring Cloud以及服务发现机制集成在了一起。 Eureka Client 提供了一个可用服务器的动态列表供 Ribbon 进行服务器之间的均衡。</p><h4 id="Hystrix"><a href="#Hystrix" class="headerlink" title="Hystrix"></a>Hystrix</h4><p>Hystrix 是断路器模式的具体实现，其可以调节网络访问依赖中经常出现的延迟以及错误。其主要目的是为了阻断在分布式环境中大量微服务极易出现的级联错误，使得系统尽快重新上线。Hystrix还提供了一个监控页面 （下面将会看到）。</p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>前期准备：</p><ul><li>网络</li></ul><ul><li>安装 Docker 以及 Docker compose</li></ul><p>运行命令：</p><ul><li><code>cd microservice-demo/</code>执行<code>mvn clean package</code></li><li><code>cd ../docker/</code>执行<code>docker-compose build</code>以及<code>docker-compose up</code></li></ul><p>重要端口：</p><ul><li><a href="http://127.0.0.1:8080" target="_blank" rel="external">http://127.0.0.1:8080</a> - 网关</li><li><a href="http://127.0.0.1:8761" target="_blank" rel="external">http://127.0.0.1:8761</a> - Eureka Dashboard</li></ul><p>注意：</p><p><strong>应用启动之后如果遇到 Whitelabel Error Page 错误请刷新页面</strong></p><h2 id="UI"><a href="#UI" class="headerlink" title="UI"></a>UI</h2><p><strong>Index</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/index_1.png" alt="index_1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/index_2.png" alt="index_2"></p><p><strong>Customer Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/customers_list.png" alt="customers_list"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/add_customer.png" alt="add_customer"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/successful.png" alt="success"></p><p><strong>Catalog Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/items_list.png" alt="items_list"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/add_item.png" alt="add_item"></p><p><strong>Order Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/orders_list.png" alt="orders_list"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/add_order.png" alt="add_order"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/order_details.png" alt="order_details"></p><p><strong>Eukera Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/eukera.png" alt="eukera"></p><p><strong>Hystrix Dashboard</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/order_hystrix.png" alt="order_hystrix"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/14/Learning-to-act-by-predicting-the-future/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/06/14/Learning-to-act-by-predicting-the-future/" itemprop="url">Learning to act by predicting the future</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-14T19:46:19+08:00">2017-06-14 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/06/14/Learning-to-act-by-predicting-the-future/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/06/14/Learning-to-act-by-predicting-the-future/" itemprop="commentsCount"></span> </a></span><span id="/2017/06/14/Learning-to-act-by-predicting-the-future/" class="leancloud_visitors" data-flag-title="Learning to act by predicting the future"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><em>论文 <a href="https://openreview.net/forum?id=rJLS7qKel" target="_blank" rel="external">Learning to act by predicting the future</a></em></p><p>这篇论文提出的 DFP (Direct Future Prediction) 赢得了2016年 Virtual Doom AI Competition 的 “Full Deathmatch” 环节的比赛。Virtual Doom 是一个对战性的第一人称射击型游戏，根据玩家击杀数判定胜负。为了体现出模型的泛化能力， 训练过程中使用的地图不在比赛过程中出现。DFP的性能超出了第二名（Deep LSTM Q-Network）50%，并且其模型以及训练数据更加简洁，表现出了DFP模型的优越性。</p><p>机器学习问题可以分为监督学习问题，无监督学习问题以及强化学习问题。监督学习主要是学习一个输入到输出的映射函数，无监督学习更加关注如何挖掘数据本身的隐含结构，强化学习是一个面向目标的策略学习问题。</p><p>因此采用强化学习的方法使得机器人在Deathmatch游戏中表现良好十分合适。因为这是一个直接面向目标的问题 （在游戏中取得最大的击杀数）。所以 DQN 以及 A3C 这样的算法应运而生，并且取得了巨大的成功。但是这篇论文提出了一个不同的观点。它引用了Jordan &amp; Rumelhart (1992) 这篇论文中提出的一个观点：</p><blockquote><p>对于一个可以与环境进行交互的学习问题，如果环境提供的反馈是稀疏的标量 （例如，对于一个五子棋问题，反馈只在最后胜负已分时给出，并且只是一个类似+1，-1的标量反馈），采用传统的强化学习算法会十分有效；但是如果环境给出的反馈是一个即时密集的多维度反馈 （在短时间内具有很大的信息比特率），监督学习算法更具优势。</p></blockquote><p>由于监督学习方面的研究已经非常成熟，最近十分火热的深度学习更是在很多方面都取得了很好的结果，因此，如果我们能够把强化学习问题在某种程度上转化为一个监督学习问题，可以使得问题的求解大大简化。</p><p>那么现在的问题是，我们要如何设计模型，从而可以得到一个监督信号呢？可以想到，我们唯一拥有的数据就是机器人通过与环境的交互得到的状态转移 （对于游戏来说就是玩家在游戏中采取不同的行为得到的环境的反馈，例如，玩家使用一个血包可以是的生命值回复；向左转可以使得画面发生变化等等）。我们可以对这些数据进行特殊的设计，从而能够满足我们的要求。</p><p>具体来说，我们不再简单使用传统强化学习问题中单一的状态 （例如游戏中的一帧画面）与对应的回报。我们把单一的状态拆分开来，对于原始的图像，声音等信息原样保留，形成一个 ”感觉输入流 (sensory input stream)“ ，很明显它是一个高维的变量；另外，我们从这些原始的信息中提取出能够代表我们学习目标的测度 （例如健康度，剩余弹药数以及击杀数等），形成一个 ”测度流 (measurement stream)“ ，它是一个低维的变量 （因为只包含几个重要的变量）。<strong>注意，这里的stream不是代表了好几个时间步，而是代表它是多个测度的一个集合。</strong></p><p>这样做有什么好处呢？一个传统的强化学习问题，其训练对象就是最大化一个关于reward的函数。一般reward都是人为给定的 （还是拿五子棋举例，最后玩家赢了，回报就是一个正数， 反之就是负数），但是这就使得学习问题的方差变得很大，训练过程十分不稳定，收敛速度慢，甚至可能不收敛。因此，<strong>我们希望reward的值不要过于随机化，能够通过某些监督信号来减少其方差</strong>。这里就可以体现出我们之前进行状态分解的优势。我们可以将reward表示成 measurement stream 的函数，由于measurement是agent与真实环境进行交互时得到的，属于一种监督信号，这很好的满足了我们的需求。所以最后我们的训练对象由最大化一个关于reward的函数变成了最大化一个关于measurement stream的函数。而这个<strong>measurement stream可以认为是传统强化学习问题中的reward</strong>。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>现在我们正式地定义DFP模型。在每一个时间步$t$，agent接收一个观察 （转移到一个状态）$O_t$, 根据这个观察 （状态）的某些固有属性从可行的动作集合中选取一个动作执行。$O_t$详细定义如下：<br>$$<br>\mathbf{o}_t = (\mathbf{s}_t, \mathbf{m}_t)<br>$$<br>整个状态转移过程中，我们希望最大化目标，前面提到了，它是关于measurement stream的函数：<br>$$<br>\mathbf{f} = (\mathbf{m}_{t+\tau_1}-\mathbf{m}_t, \cdots, \mathbf{m}_{t+\tau_n}-\mathbf{m}_t)<br>$$<br>$\tau_1, \cdots, \tau_n$ 代表与当前时间步$t$的一个偏差。至于为什么不直接最大化measurement stream而是最大化一个差值，我认为作者可能是有如下考虑：</p><ol><li>借鉴了n-step Q-learning 的做法。</li><li>由于模型是为了预测当前时间步$t$的measurement stream，因此优化对象中应该包含当前的measurement stream。</li></ol><p>最后，<br>$$<br>\mathbf{Goal} \; = \; u(\mathbf{f};\mathbf{g})<br>$$<br>一般线性函数即可满足我们的需求，即<br>$$<br>u(\mathbf{f};\mathbf{g}) = \mathbf{g}^{\text{T}}\mathbf{f}<br>$$<br>注意到现在我们的问题变成了一个监督学习的问题。为了训练模型，我们需要预测目标，然后再与真实的目标比较，通过最小化误差来进行学习。那么我们现在定义这个预测过程。注意到，由于目标只是measurement stream的函数，而且<strong>参数一般都是确定的，不需要进行学习</strong>。因此，我们的预测对象是measurement stream而不是目标。下面我们定义一个预测器F:<br>$$<br>\mathbf{p}_t^a = F(\mathbf{o}_t, a, \mathbf{g};\theta)<br>$$<br><strong>注意，这里的$\text{g}$和(4)中是不一样的，它代表目标</strong>。$p_t^a$代表在$t$时间步下，执行行为$a$所得到的reward，也即measurement stream。</p><p>当训练完成的时候，我们就要用这个预测器F进行决策，策略定义如下：<br>$$<br>a_t = {\arg\max}_{a \in \mathcal{A}} \mathbf{g}^{\text{T}}F(\mathbf{o}_t, a, \mathbf{g};\theta)<br>$$<br>注意到，模型实际训练的过程中采用的是$\varepsilon\text{-greedy}$策略。这里可以看出，在训练过程中或者测试过程中，我们要手动的计算出$u(\text{f};\text{g})$的值。下面我们详细的剖析模型的训练过程。</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>对于传统的强化学习算法，例如Q-learning，其训练过程是一个在线学习的过程，也即其训练集是一个一个进行输入的，每输入一次都进行一次参数的更新。由于Q-learning以及DFP都是采用了MC (Monte Carlo) 策略，这种训练过程可能十分不稳定 （由于训练最开始时我们的训练数据是通过一个随机策略与环境交互产生的），致使收敛速度很慢，需要很多的episodes进行训练。这里采用了和DQN (Deep Q-Network) 相同的 experience replay技术。具体来说，就是保存每次agent与环境交互后产生的数据对</p><p>$\langle \mathbf{o}_i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle$ 到数据集$\mathcal{D}$中，即$\mathcal{D} = \{\langle \mathbf{o}_ i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle \}_{i=1}^N$. 注意这里的$N$个数据对并不是直接顺序产生的，而是从当前episode中到当前时间步时，所有的数据对中选取最近的$M$个，再从其中随机抽样$N$个。另外，每隔k步才进行一次参数的更新，因为$\mathbf{f}$的计算需要考虑到32个时间步之后的数据，因此$k \ge 32$（实验部分将详细介绍）。DQN 给出了具体的实现：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/dqn_alg.png" alt="dqn_alg"></p><p>另外需要注意的是</p><p>有了训练集，我们现在定义w代价函数：<br>$$<br>\mathcal{L}(\theta) = \sum_{i=1}^{N} |F(\mathbf{o}_i, a_i, \mathbf{g}_i;\theta) - \mathbf{f}_i|^2<br>$$<br>我们来对比一下 DQN 的代价函数：<br>$$<br>L_i(\theta_i)= \mathbb{E}_{s, a \sim \rho(\cdot)} \left[ y_i - Q(s, a;\theta_i) \right],<br>$$<br>其中$y_i = \mathbb{E}_{s^{\prime} \sim \varepsilon}[ r + \gamma \max_{a^{\prime}} Q(s^{\prime}, a^{\prime};\theta_{i-1}) ]$ 。</p><p>这里的$y_i$是上一次模型的输出，其值随着更新次数的增加也在不断变化。因此从这里也能看出DFP是一个监督学习算法。</p><p>训练过程中我们为了解决报告最开始提出的目标随着时间发生改变的问题，采用了两种目标进行测试：</p><ol><li>目标向量$\mathbf{g}$ （不是目标）在整个训练过程中不变</li><li>目标向量在每个episode结束时随机变化</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>下图是DFP模型的网络结构：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/network.png" alt="network"></p><p>从图中可以看出，该网络有三个输入模块。一个感知模块$S(s)$，一个测度模型$M(m)$以及一个目标模块$G(g)$。在实验中，$s$代表一张图片，$S$代表一个卷积神经网络。测度模块以及目标模块都是由一个全连接神经网络构成。三者的输出连接在一起，形成一个联合的输入表示，供后续算法使用：<br>$$<br>\mathbf{j} = J(\mathbf{s, m, g}) = \langle S(\mathbf{s}), M(\mathbf{m}), G(\mathbf{g}) \rangle<br>$$<br>DFP网络采用了DQN的做法，一次性输出所有action对应的measurement stream。但是我们希望能够着重关注对action之间差异的学习。因此采用了Wang et al. (ICML 2016) 这篇文章中才去的做法，将预测模块分为两个stream，一个期望stream $E(\text{j})$ 以及一个action stream $A(\text{j})$。注意这两个stream都是一个全连接的神经网络。期望stream的目标是预测所有action能够获得的measurement stream的期望。Action stream关注不同action之间的差异。其中，$A(\text{j}) = \langle A^1(\text{j}), \cdots, A^{w}(\text{j}) \rangle$，$w = |\mathcal{A}|$代表所有可能action的个数。同时我们还在加入了一个正则化层：<br>$$<br>\overline{A^{i}}(\mathbf{j}) = A^{i}(\mathbf{j}) - \frac{1}{w}\sum_{k=1}^{w} A^{k}(\mathbf{j})<br>$$<br>正则化层对每一个action的预测值减去了所有action预测值的期望，这样就强制期望stream去学习这个期望，这样action stream就可以着重关注不同action之间的差异。最后，网络的输出如下：<br>$$<br>\mathbf{p} = \langle \mathbf{p}^{a_1}, \cdots, \mathbf{p}^{a_w} \rangle = \langle \overline{A^1}(\mathbf{j})+E(\mathbf{j}), \cdots, \overline{A^w}(\mathbf{j})+E(\mathbf{j}) \rangle<br>$$<br>为了验证网络中使用的三个辅助结构（measurement stream输入，expectation-action分解以及action正则化层）的作用，我们进行了测试。我们基于D3场景（下面实验部分提及）随机产生了100个地图场景用以训练。同时采用basic网络 （下面实验部分提及），最后的实验结果如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t3.png" alt="t3"></p><p>可以看出，expectation-action分解的作用最大，同时我们设计的measurement stream也是十分重要的。</p><h2 id="实验及结果"><a href="#实验及结果" class="headerlink" title="实验及结果"></a>实验及结果</h2><p>具体的实验场景见下图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/game.png" alt="game"></p><p>在前两个场景中，agent可以采取三个动作，向前移动、向左转、向右转。这样一共就有8种动作组合。采用的测度只有一种，就是血量。在后两个场景中，agent可以采取八个动作组合，分别是向前移动、向后移动、向左转、向右转、向左扫射，向右扫射、奔跑以及射击。这样一共就有256个动作组合。采用的测度一共有三种，血量，弹药数以及击杀数。这里我认为存在一个可以改进的地方，应该排除掉不合理的动作组合，例如同时向左转以及向右转。这样可以减少搜索空间，加速收敛，同时可以提高策略的质量。</p><p>实验中网络的结构与DQN的结构十分类似，参数也尽可能相近，就是为了比较起来比较公平。具体来说，实验中采用了两种网络，basic以及large，结构相同，但是参数数量不同：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/bl.png" alt="bl"></p><p>Basic网络的参数与DQN比较接近，以便比较。两个网络在所有的非终止层后都加入了一个非线性层，采用的激活函数为Leaky ReLU，具体函数为：<br>$$<br>\mathbf{LReLU}(x) = \max(x, 0.2x)<br>$$<br>参数初始化方法采用了He Initialization，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">W = np.random.randn(node_in, node_out) / np.sqrt(node_in / <span class="number">2</span>)</div></pre></td></tr></table></figure><p>Agent以episode为单位进行训练和测试。每一个episode拥有525个时间步（大约一分钟），如果agent死亡那么episode也会终止。同时将时间偏置$\tau_1, \cdots, \tau_n$设置为1, 2, 4, 8, 16, 32。最后结果表明只有最新的三个时间步（8, 16, 32）对结果有贡献，贡献比例为 1:1:2。</p><p>另外，输入图像被转换成灰度图像，measurement stream并不是直接输入，而是进行了正则化 （除以标准差）。同时，我们还在训练以及测试过程中使用frame skipping技术。Agent每隔4帧采取一次action。这些被忽略的帧所采取的action与其之后的帧的action一致，相当于进行了一次简单的复制。另外，由于人类的反应速度肯定是比不上计算机的，因此fram skipping使得agent的行为更加接近人类。</p><p>对于之前提到的experience replay技术，实验中将M值设为20000， N设为64，k也设为64（$\ge32$）。同时为了能够更高效的获得训练集$\mathcal{D}$，我们同时采用8个agent并行运行。训练时采用的梯度下降算法为Adam算法，参数设置如下：$\beta_1=0.95, \;\beta_2=0.999,\;\varepsilon=10^{-4}$。Basic网络训练了800,000次mini-batch迭代，large网络训练了2,000,000次。算法实现<a href="https://github.com/IntelVCL/DirectFuturePrediction。" target="_blank" rel="external">https://github.com/IntelVCL/DirectFuturePrediction。</a></p><p>下面介绍我们的baselines。我们同三个算法进行了比较：DQN (Mnih et al., 2015), A3C (Mnih et al., 2016), 以及 DSR (Kulkarni et al., 2016b)。DQN由于其在Atari游戏上的优异效果成为了视觉控制的标准baseline。A3C更是这个领域中的最好的算法。DSR也在Virtual Doom平台上进行了实验。所以我们挑选了这三个具有代表意义的算法。</p><p>对于这三个算法我们都使用了Github上的开源实现：DQN (<a href="https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner" target="_blank" rel="external">https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner</a>) 、DSR (<a href="https://github.com/Ardavans/DSR" target="_blank" rel="external">https://github.com/Ardavans/DSR</a>), 以及 A3C (<a href="https://github.com/muupan/async-rl)。前两个都是作者提供的源码，最后的A3C是一个独立开发者的个人实现。" target="_blank" rel="external">https://github.com/muupan/async-rl)。前两个都是作者提供的源码，最后的A3C是一个独立开发者的个人实现。</a></p><p>对于DQN以及DSR我们测试了三个学习速率：默认值（0.00025），0.00005以及0.00002。其他参数直接采用默认值。对于A3C算法，为了训练更快，前两个任务我们采用了5个学习速率 ({2, 4, 8, 16, 32} · $10^{-4}$)。后两个任务我们训练了20个模型，每个模型的学习速率从一个范围从$10^{-4}$到$10^{-2}$的log-uniform分布中进行采样，$\beta$值（熵正则项）从一个范围从$10^{-4}$到$10^{-}$的lo1g-uniform分布中进行采样。结果选取最好的。</p><p>最终结果如下所示：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t1.png" alt="t1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/f3.png" alt="f3"></p><p>在前两个游戏场景中，模型尝试最大化血量；在后两个场景中尝试最大化血量、弹药数以及击杀数的一个线性组合，参数为0.5, 0.5, 1。因为游戏更加侧重于通过击杀数判断胜负。所有的数据都是对三次训练结果进行平均，曲线图采样点的个数为$3 \times 50,000$。可以看出，DFP模型取得了最好的结果。其中DSR算法由于训练速度过慢，所以我们只在D1场景（也进行了将近10天的训练）进行了测试。</p><p>下面进行模型泛化能力的测试，我们基于D3以及D4两个场景分别随机产生100个随机场景。其中90个用于训练，剩下10个用于测试。最后结果如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/f2.png" alt="f2"></p><p>其中最后一列采用了large网络。可以看出，从复杂场景训练之后，在简单场景上的泛化能力往往不错，虽然两者规则不同。但是反之则不可以。</p><p>接下来进行学习变化目标能力的测试。结果见下图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t3g.png" alt="t3g"></p><p>其中采用第二列的策略时，agent并不知道每一个measurement的相对重要性；最后一列，agent事先并不知道哪一个measurement是不需要考虑的。但是最后测试时，效果都很好，而且在固定目标策略没有见过的目标上的效果要更好。说明DFP模型对于变化目标的学习能力优异。</p><p>最后我们单独对measurement stream时间偏置的重要性进行测试，我们采用的是D3-tx训练集，最后结果如图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t4.png" alt="t4"></p><p>相比较而言，采用更多的时间偏置可以达到更好的效果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>现在强化学习问题关注的重点还是在value function的估计上，深度强化学习模型一般采用一个深度网络直接对value function进行估计。这篇论文的创新点在于，在使用深度网络之前，对value function进行了两次额外的映射。第一次是用measurement stream来代替reward，使得reward具有更强的状态表示能力；其次，对measurement stream再次进行了一个函数映射，采用了时间偏置，借鉴了n-step Q-learning的思想。最后，再将输出作为深度网络的输入，进行value function的估计。最后的实验结果证明这种想法是有其正确性的。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/27/k-Armed-Bandit-Problem/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/27/k-Armed-Bandit-Problem/" itemprop="url">k-Armed Bandit Problem</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-27T12:27:49+08:00">2017-05-27 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/27/k-Armed-Bandit-Problem/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/27/k-Armed-Bandit-Problem/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/27/k-Armed-Bandit-Problem/" class="leancloud_visitors" data-flag-title="k-Armed Bandit Problem"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or <em>time steps</em>.</p><p>This is the original form of the <em>k-armed bandit problem</em>. Each of the <em>k</em> actions has an excepted or mean reward given that action is selected; let us call this <em>value</em> of that action. We denote the action selected on time step <em>t</em> as $A_{t}$, and the corresponding reward as $R_{t}$. The value then of an arbitrary action <em>a</em>, denoted $q_{\star}(a)$, is the excepted reward given that <em>a</em> is selected:<br>$$<br>q_{\star}(a) = \mathbb{E}[R_t|A_t=a]<br>$$<br>If you knew the value of each action, then it would be trivial to solve the <em>k</em>-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action <em>a</em> at time <em>t</em> as $Q_{t}(a) \approx q_{\star}(a)$.</p><p>We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when action is selected. One natural way to estimate this is by averaging the rewards actually received:</p><p>$$Q_t(a) \doteq \frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbf{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbf{1}_{A_i=a}}$$</p><p>where $\mathbf{1}_{\text{predicate}}$ denotes the random variable that is 1 if <em>predicate</em> is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as $Q_1(a) = 0$. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_{\star} (a)$. We call this the <strong><em>sample-average</em></strong> method for estimating action values because each estimate is an average of the sample of relevant rewards.</p><p>The simplest action selection rule is to select the action (or one of the actions) with highest estimated action value, that is, to select at step <em>t</em> one of the greedy actions, $A_t^\star$ for which $Q_t(A_t^\star) = \max_a Q_t(a)$. This <em>greedy</em> action selection method can be written as<br>$$<br>A_t \doteq argmax_a Q_t(a)<br>$$<br>Naturally, we could use the <em>$\epsilon$-greedy</em> method rather the <em>greedy</em> method. We’ll show their difference on the performance. Now, let’s jump into the implementation details. In order to be able to see the results quickly, we set to <em>k</em> to be <em>10</em>. The first, we generate 10 stationary probability distributions that we’ll sample from to generate action values. The generate method is below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data=np.random.randn(<span class="number">200</span>,<span class="number">10</span>) + np.random.randn(<span class="number">10</span>)</div></pre></td></tr></table></figure><p>We first generate randomly 10 true excepted values by <code>np.random.randn(10)</code>, then I’m going to change the variance to 1. So we get 10 stationary probability distributions. The visualizations are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/action_value_distributions.png" alt="action_value_distributions"></p><p>We’re going to compare how different $\epsilon$ values affect the end result.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">epsilonGreedy</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    epsilons = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.01</span>]</div><div class="line">    bandits = []</div><div class="line">    <span class="keyword">for</span> epsInd, eps <span class="keyword">in</span> enumerate(epsilons):</div><div class="line">        bandits.append([Bandit(epsilon=eps, sampleAverages=<span class="keyword">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)])</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, counts <span class="keyword">in</span> zip(epsilons, bestActionCounts):</div><div class="line">        plt.plot(counts, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, rewards <span class="keyword">in</span> zip(epsilons, averageRewards):</div><div class="line">        plt.plot(rewards, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Before we go into the details, we introduce the <strong>Bandit</strong> object first.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bandit</span>:</span></div><div class="line">    <span class="comment"># @kArm: # of arms</span></div><div class="line">    <span class="comment"># @epsilon: probability for exploration in epsilon-greedy algorithm</span></div><div class="line">    <span class="comment"># @initial: initial estimation for each action</span></div><div class="line">    <span class="comment"># @stepSize: constant step size for updating estimations</span></div><div class="line">    <span class="comment"># @sampleAverages: if True, use sample averages to update estimations instead of constant step size</span></div><div class="line">    <span class="comment"># @UCB: if not None, use UCB algorithm to select action</span></div><div class="line">    <span class="comment"># @gradient: if True, use gradient based bandit algorithm</span></div><div class="line">    <span class="comment"># @gradientBaseline: if True, use average reward as baseline for gradient based bandit algorithm</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kArm=<span class="number">10</span>, epsilon=<span class="number">0.</span>, initial=<span class="number">0.</span>, stepSize=<span class="number">0.1</span>, sampleAverages=False, UCBParam=None, gradient=False, gradientBaseline=False, trueReward=<span class="number">0.</span>)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div></pre></td></tr></table></figure><p>For now we just introduce <em>sample-average</em> method, so skip other methods parameters. Let us see the initialization method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kArm=<span class="number">10</span>, epsilon=<span class="number">0.</span>, initial=<span class="number">0.</span>, stepSize=<span class="number">0.1</span>, sampleAverages=False, UCBParam=None,</span></span></div><div class="line">             gradient=False, gradientBaseline=False, trueReward=<span class="number">0.</span>):</div><div class="line">    self.k = kArm</div><div class="line">    self.stepSize = stepSize</div><div class="line">    self.sampleAverages = sampleAverages</div><div class="line">    self.indices = np.arange(self.k)</div><div class="line">    self.time = <span class="number">0</span></div><div class="line">    self.UCBParam = UCBParam</div><div class="line">    self.gradient = gradient</div><div class="line">    self.gradientBaseline = gradientBaseline</div><div class="line">    self.averageReward = <span class="number">0</span></div><div class="line">    self.trueReward = trueReward</div><div class="line"></div><div class="line">    <span class="comment"># real reward for each action</span></div><div class="line">    self.qTrue = []</div><div class="line"></div><div class="line">    <span class="comment"># estimation for each action</span></div><div class="line">    self.qEst = np.zeros(self.k)</div><div class="line"></div><div class="line">    <span class="comment"># # of chosen times for each action</span></div><div class="line">    self.actionCount = []</div><div class="line"></div><div class="line">    self.epsilon = epsilon</div><div class="line"></div><div class="line">    <span class="comment"># initialize real rewards with N(0,1) distribution and estimations with desired initial value</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.k):</div><div class="line">        self.qTrue.append(np.random.randn() + trueReward)</div><div class="line">        self.qEst[i] = initial</div><div class="line">        self.actionCount.append(<span class="number">0</span>)</div><div class="line"></div><div class="line">    self.bestAction = np.argmax(self.qTrue)</div></pre></td></tr></table></figure><p>There are some important attributes. <strong>time</strong> is a number that represents the time steps now. <strong>actionCount</strong> is the times that correspond actions have been taken prior to current time steps. <strong>qTrue</strong> is a list. And each item is the true excepted value corresponding to each action. <strong>qEst</strong> is the estimate value of each action. It’s initialized to zero. <strong>epsilon</strong> is the $\epsilon$. Next, in the for loop, we initialize real rewards with N(0, 1) distribution and estimations with desired initial value. At last, the <strong>bestAction</strong> store the current best action will be take.</p><p>The next method tell us how to get the next action should be take:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># explore</span></div><div class="line">    <span class="keyword">if</span> self.epsilon &gt; <span class="number">0</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, self.epsilon) == <span class="number">1</span>:</div><div class="line">            np.random.shuffle(self.indices)</div><div class="line">            <span class="keyword">return</span> self.indices[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># exploit</span></div><div class="line">    <span class="keyword">if</span> self.UCBParam <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        UCBEst = self.qEst + \</div><div class="line">                 self.UCBParam * np.sqrt(np.log(self.time + <span class="number">1</span>) / (np.asarray(self.actionCount) + <span class="number">1</span>))</div><div class="line">        <span class="keyword">return</span> np.argmax(UCBEst)</div><div class="line">    <span class="keyword">if</span> self.gradient:</div><div class="line">        expEst = np.exp(self.qEst)</div><div class="line">        self.actionProb = expEst / np.sum(expEst)</div><div class="line">        <span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div><div class="line">    <span class="keyword">return</span> np.argmax(self.qEst)</div></pre></td></tr></table></figure><p>We can skip the second and the third if statements (we’ll introduce this two methods later). If we use <em>greedy</em> method, we just return the action that has highest value. Otherwise, we’re choosing randomly at $\epsilon$ probability.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div><div class="line">    <span class="comment"># generate the reward under N(real reward, 1)</span></div><div class="line">    reward = np.random.randn() + self.qTrue[action]</div><div class="line">    self.time += <span class="number">1</span></div><div class="line">    self.averageReward = (self.time - <span class="number">1.0</span>) / self.time * self.averageReward + reward / self.time</div><div class="line">    self.actionCount[action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> self.sampleAverages:</div><div class="line">        <span class="comment"># update estimation using sample averages</span></div><div class="line">        self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">elif</span> self.gradient:</div><div class="line">        oneHot = np.zeros(self.k)</div><div class="line">        oneHot[action] = <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> self.gradientBaseline:</div><div class="line">            baseline = self.averageReward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            baseline = <span class="number">0</span></div><div class="line">        self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># update estimation with constant step size</span></div><div class="line">        self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">return</span> reward</div></pre></td></tr></table></figure><p>Similarly, we just skip other if statements and focus on this row:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>This formula seems different with the earlier one. The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. We now turn to the question of how these averages can be computed in a computationally efficient manner, in particular, with constant memory and per-time-step computation.</p><p>To simplify notation we concentrate on a single action. Let $R_i$ now denote the reward received after <em>i</em>th selection of <em>this action</em>, and let $Q_n$ denote the estimate of its action value after it has been select $n-1$ times, which we can now write simply as<br>$$<br>Q_n \doteq \frac{R_1 + R_2 + \cdots + R_{n-1}}{n-1}<br>$$<br>The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, in this case the memory and computational requirements would grow over time as more rewards are seen.</p><p>It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given $Q_n$ and the <em>n</em>th reward, $R_n$, the new average of all <em>n</em> rewards can be computed by<br>$$<br>\begin{align}<br>Q_{n+1} &amp;\doteq \frac{1}{n}\sum_{i=1}^{n} R_i \\<br>&amp;= \frac{1}{n}(R_n + \sum_{i=1}^{n-1} R_i) \\<br>&amp;= \frac{1}{n}(R_n + (n-1) \frac{1}{n-1}\sum_{i=1}^{n-1} R_i) \\<br>&amp;= \frac{1}{n}(R_n + (n-1) Q_n) \\<br>&amp;= \frac{1}{n}(R_n + n Q_n - Q_n) \\<br>&amp;= Q_n + \frac{1}{n}[R_n - Q_n]<br>\end{align}<br>$$<br>So this is why the code is look like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>Back to <strong>epsilonGreedy()</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">epsilonGreedy</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    epsilons = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.01</span>]</div><div class="line">    bandits = []</div><div class="line">    <span class="keyword">for</span> epsInd, eps <span class="keyword">in</span> enumerate(epsilons):</div><div class="line">        bandits.append([Bandit(epsilon=eps, sampleAverages=<span class="keyword">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)])</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, counts <span class="keyword">in</span> zip(epsilons, bestActionCounts):</div><div class="line">        plt.plot(counts, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, rewards <span class="keyword">in</span> zip(epsilons, averageRewards):</div><div class="line">        plt.plot(rewards, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Now, we get <strong>nBandits</strong> bandits and each bandit has 10 arm. Then we use a bandit simulation to simulate this process.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">banditSimulation</span><span class="params">(nBandits, time, bandits)</span>:</span></div><div class="line">    bestActionCounts = [np.zeros(time, dtype=<span class="string">'float'</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, len(bandits))]</div><div class="line">    averageRewards = [np.zeros(time, dtype=<span class="string">'float'</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, len(bandits))]</div><div class="line">    <span class="keyword">for</span> banditInd, bandit <span class="keyword">in</span> enumerate(bandits):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nBandits):</div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, time):</div><div class="line">                action = bandit[i].getAction()</div><div class="line">                reward = bandit[i].takeAction(action)</div><div class="line">                averageRewards[banditInd][t] += reward</div><div class="line">                <span class="keyword">if</span> action == bandit[i].bestAction:</div><div class="line">                    bestActionCounts[banditInd][t] += <span class="number">1</span></div><div class="line">        bestActionCounts[banditInd] /= nBandits</div><div class="line">        averageRewards[banditInd] /= nBandits</div><div class="line">    <span class="keyword">return</span> bestActionCounts, averageRewards</div></pre></td></tr></table></figure><p>The <strong>bandits</strong> is a list that has three item. Each item is a list that contains <strong>nBandits</strong> bandits that has a corresponding epsilon value. We calculate the average total reward of 2000 bandits is to eliminate the effect of noise. Ok, let us see the final result.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_optimal_action.png" alt="epsilon_greedy_optimal_action"><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_average_reward.png" alt="epsilon_greedy_average_reward"></p><p>We can see the algorithm reaches the best performance when epsilon is set to 0.1.</p><p>The averaging methods discussed so far are appropriate in a stationary environment, but not if the bandit is changing over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to weight recent rewards more heavily than long-past ones. One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule for updating an average $Q_n$ of the $n-1$ past rewards is modified to be<br>$$<br>Q_{n+1} \doteq Q_n + \alpha [R_n - Q_n]<br>$$<br>where the step-size parameter $\alpha \in (0, 1]$ is constant. This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_1$:<br>$$<br>\begin{align}<br>Q_{n+1} &amp;\doteq Q_n + \alpha [R_n - Q_n] \\<br>&amp;= \alpha R_n + (1 - \alpha) Q_n \\<br>&amp;= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] \\<br>&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 Q_{n-1} \\<br>&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 R_{n-1} + \cdots + (1 - \alpha) ^ {n-1} \alpha R_1 + (1 - \alpha) ^ n Q_1 \\<br>&amp;= (1 - \alpha) ^ n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha) ^ {n-i} R_i<br>\end{align}<br>$$<br>We call this a <em>weighted average</em> because the sum of the weights is 1. In fact, the weight decays exponentially according to the exponent on $1-\alpha$. According, this is sometimes called an <em>exponential, recency-weighted average</em>.</p><p>Sometimes it is convenient to vary the step-size parameter from step to step. Let $\alpha_n(a)$ denote the step-size parameter used to process the reward received after <em>n</em>th selection of action <em>a</em>. As we noted, the choice $\alpha_n(a)=\frac{1}{n}$ results in the sample-average method, which is guaranteed to converge to the true action values by the law of the large numbers. A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:<br>$$<br>\sum_{n=1}^{\infty}\alpha_{n}(a) = \infty \; \text{and} \; \sum_{n=1}^{\infty}\alpha_{n}^{2}(a) &lt; \infty<br>$$<br>All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, $Q_1(a)$. <strong>In the language of statistics, these methods are <em>biased</em> by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant $\alpha$ (weighted avetrage), the bias is permanent, though decreasing over time. In practice, this kind of bias is usually not a problem and can sometimes be very helpful.</strong> The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be excepted. So, let us do a experiment. The first we set them all to zero and the we set them all to a constant, 5.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimisticInitialValues</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    bandits = [[], []]</div><div class="line">    bandits[<span class="number">0</span>] = [Bandit(epsilon=<span class="number">0</span>, initial=<span class="number">5</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bandits[<span class="number">1</span>] = [Bandit(epsilon=<span class="number">0.1</span>, initial=<span class="number">0</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(bestActionCounts[<span class="number">0</span>], label=<span class="string">'epsilon = 0, q = 5'</span>)</div><div class="line">    plt.plot(bestActionCounts[<span class="number">1</span>], label=<span class="string">'epsilon = 0.1, q = 0'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(averageRewards[<span class="number">0</span>], label=<span class="string">'epsilon=0, initial=5, stepSize=0.1'</span>)</div><div class="line">    plt.plot(averageRewards[<span class="number">1</span>], label=<span class="string">'epsilon=0.1, initial=0, stepSize=0.1'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The <strong>Bandit</strong> object’s <strong>takeAction()</strong> has a little difference:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values.png" alt="optimistic_initial_value_optimal_action"><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values_average_reward.png" alt="optimistic_initial_value_optimal_action"></p><p>We can see it reaches the better performance than the $\epsilon$-greedy method, when they are all used the weighted average method. Notice that the $\epsilon$-greedy with weighted-average method is worsen than the $\epsilon$-greedy with sample-average method.</p><p>We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial state in any special way is unlikely to help with the general nonstationary case.</p><p>Exploration is needed because the estimates of the action values are uncertain. The greedy actions are those that look best at present, but some of the other actions may actually be better. $\epsilon$-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions as<br>$$<br>A_t \doteq argmax_a \left [Q_t(a) + c\sqrt{\frac{\ln{t}}{N_t(a)}}\ \right]<br>$$<br>where $N_t(a)$ denotes the number of times that action <em>a</em> has been selected prior to time <em>t</em>, and the number $c&gt;0$ controls the degree of exploration. If $N_t(a)=0$, then <em>a</em> is considered to be a maximizing action. The idea of this is called <em>upper confidence bound</em> (UCB). Let us implement it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ucb</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    bandits = [[], []]</div><div class="line">    bandits[<span class="number">0</span>] = [Bandit(epsilon=<span class="number">0</span>, stepSize=<span class="number">0.1</span>, UCBParam=<span class="number">2</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bandits[<span class="number">1</span>] = [Bandit(epsilon=<span class="number">0.1</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    _, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(averageRewards[<span class="number">0</span>], label=<span class="string">'UCB c = 2'</span>)</div><div class="line">    plt.plot(averageRewards[<span class="number">1</span>], label=<span class="string">'epsilon greedy epsilon = 0.1'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>We note that the <strong>UCBParam=2</strong>. The Bandit object explains this. The <strong>getAction()</strong> method and <strong>takeAction()</strong> method are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># explore</span></div><div class="line">    <span class="keyword">if</span> self.epsilon &gt; <span class="number">0</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, self.epsilon) == <span class="number">1</span>:</div><div class="line">            np.random.shuffle(self.indices)</div><div class="line">            <span class="keyword">return</span> self.indices[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># exploit</span></div><div class="line">    <span class="keyword">if</span> self.UCBParam <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        UCBEst = self.qEst + \</div><div class="line">                 self.UCBParam * np.sqrt(np.log(self.time + <span class="number">1</span>) / (np.asarray(self.actionCount) + <span class="number">1</span>))</div><div class="line">        <span class="keyword">return</span> np.argmax(UCBEst)</div><div class="line">    <span class="keyword">if</span> self.gradient:</div><div class="line">        expEst = np.exp(self.qEst)</div><div class="line">        self.actionProb = expEst / np.sum(expEst)</div><div class="line">        <span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div><div class="line">    <span class="keyword">return</span> np.argmax(self.qEst)</div><div class="line"></div><div class="line"><span class="comment"># take an action, update estimation for this action</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div><div class="line">    <span class="comment"># generate the reward under N(real reward, 1)</span></div><div class="line">    reward = np.random.randn() + self.qTrue[action]</div><div class="line">    self.time += <span class="number">1</span></div><div class="line">    self.averageReward = (self.time - <span class="number">1.0</span>) / self.time * self.averageReward + reward / self.time</div><div class="line">    self.actionCount[action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> self.sampleAverages:</div><div class="line">        <span class="comment"># update estimation using sample averages</span></div><div class="line">        self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">elif</span> self.gradient:</div><div class="line">        oneHot = np.zeros(self.k)</div><div class="line">        oneHot[action] = <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> self.gradientBaseline:</div><div class="line">            baseline = self.averageReward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            baseline = <span class="number">0</span></div><div class="line">        self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># update estimation with constant step size</span></div><div class="line">        self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">return</span> reward</div></pre></td></tr></table></figure><p>We can see the policy get next action has changed but the update policy has not changed. The result is here:<img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/ucb_average_reward.png" alt="ucb_average_reward"></p><p>We omit the figure about the optimal action because the trend of this two figures are the same. UCB will often perform well, but is more difficult than $\epsilon$-greedy to extend beyond bandits to the more general reinforcement learning setting considered in the more advanced problems. In these more advanced settings there is currently no known practical way of utilizing the idea of UCB action selection.</p><p>So far we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. Now, we consider learning a numerical <em>preference</em> $H_t(a)$ for each action <em>a</em>. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the preferences there is no effect on the action probabilities, which are determined according to a softmax distribution as follows:<br>$$<br>Pr\{A_t=a\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} \doteq \pi_t(a)<br>$$<br>where here we have also introduced a useful new notation $\pi_t(a)$ for the probability of taking action <em>a</em> at time <em>t</em>. Initially all preferences are the same (e.g., $H_1(a)=0, \forall a$) so that all actions have an equal probability of being selected.</p><p>There is a natural learning algorithm for this setting based on the idea of stochastic gradient ascent. On each step, after selection the action $A_t$ and receiving the reward $R_t$, the preferences are updated by:<br>$$<br>\begin{align}<br>H_{t+1}(A_t) \doteq H_t(A_t) + \alpha (R_t - \overline{R_t}) (1 - \pi_t(A_t)), \; \text{and} \\<br>H_{t+1}(a) \doteq H_t(a) - \alpha (R_t - \overline{R_t}) \pi_t(a), \;\;\;\;\;\; \forall{a \neq A_t}<br>\end{align}<br>$$<br>where $\alpha&gt;0$ is a ste-size parameter, and $\overline{R_t} \in \mathbb{R}$ is the average of all the rewards up through and including time <em>t</em>, which can be computed incrementally. The $\overline{R_t}$ term serves as a <strong><em>baseline</em></strong> with which the reward is compared. Let us see the implement details (takeAction() method and getAction() method).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> self.gradient:</div><div class="line">	expEst = np.exp(self.qEst)</div><div class="line">	self.actionProb = expEst / np.sum(expEst)</div><div class="line">	<span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">elif</span> self.gradient:</div><div class="line">	oneHot = np.zeros(self.k)</div><div class="line">	oneHot[action] = <span class="number">1</span></div><div class="line">	<span class="keyword">if</span> self.gradientBaseline:</div><div class="line">		baseline = self.averageReward</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		baseline = <span class="number">0</span></div><div class="line">	self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot self.actionProb)</div></pre></td></tr></table></figure><p>The below figure shows results with the gradient bandit algorithm on a variant of the 10-armed testbed in which the true excepted rewards were selected according to a normal distribution with a mean of +4 instead of zero (and with unit variance as before).</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/gradient_bandit_optimal_action.png" alt="gradient_bandit_optimal_action"></p><p>Despite their simplicity, in our opinion the methods presented in here can fairly be considered the state of the art. Finally, we do a parameter study of the various bandit algorithms presented in here.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">figure2_6</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    labels = [<span class="string">'epsilon-greedy'</span>, <span class="string">'gradient bandit'</span>,</div><div class="line">              <span class="string">'UCB'</span>, <span class="string">'optimistic initialization'</span>]</div><div class="line">    generators = [<span class="keyword">lambda</span> epsilon: Bandit(epsilon=epsilon, sampleAverages=<span class="keyword">True</span>),</div><div class="line">                  <span class="keyword">lambda</span> alpha: Bandit(gradient=<span class="keyword">True</span>, stepSize=alpha, gradientBaseline=<span class="keyword">True</span>),</div><div class="line">                  <span class="keyword">lambda</span> coef: Bandit(epsilon=<span class="number">0</span>, stepSize=<span class="number">0.1</span>, UCBParam=coef),</div><div class="line">                  <span class="keyword">lambda</span> initial: Bandit(epsilon=<span class="number">0</span>, initial=initial, stepSize=<span class="number">0.1</span>)]</div><div class="line">    parameters = [np.arange(<span class="number">-7</span>, <span class="number">-1</span>),</div><div class="line">                  np.arange(<span class="number">-5</span>, <span class="number">2</span>),</div><div class="line">                  np.arange(<span class="number">-4</span>, <span class="number">3</span>),</div><div class="line">                  np.arange(<span class="number">-2</span>, <span class="number">3</span>)]</div><div class="line"></div><div class="line">    bandits = [[generator(math.pow(<span class="number">2</span>, param)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)] <span class="keyword">for</span> generator, parameter <span class="keyword">in</span> zip(generators, parameters) <span class="keyword">for</span> param <span class="keyword">in</span> parameter]</div><div class="line">    _, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    rewards = np.sum(averageRewards, axis=<span class="number">1</span>)/time</div><div class="line"></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    i = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> label, parameter <span class="keyword">in</span> zip(labels, parameters):</div><div class="line">        l = len(parameter)</div><div class="line">        plt.plot(parameter, rewards[i:i+l], label=label)</div><div class="line">        i += l</div><div class="line">    plt.xlabel(<span class="string">'Parameter(2^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The results as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/bandit_algorithms_parameter_study.png" alt="parameters_study"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" itemprop="url">Monte Carlo Methods (Reinforcement Learning)</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-02T14:10:57+08:00">2017-06-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" class="leancloud_visitors" data-flag-title="Monte Carlo Methods (Reinforcement Learning)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Here we consider our first learning methods for <strong>estimating</strong> value functions and discovering optimal policy. Unlike the previous algorithms, we do not assume complete knowledge of the environment. Monte Carlo methods require only <em>experience</em> – sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Although a model is required, the model need only generate sample transition, not the complete probability distributions of all possible transitions that is required for <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">dynamic programming (DP)</a>.</p><p>Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample return. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. Only on the completion of an episode are value estimates and policies changed.</p><p>To handle the nonstationarity, we adapt the idea of general policy iteration (GPI) developed in earlier for <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">DP</a>.</p><p>Suppose we wish to estimate $v_{\pi}(s)$, the value of a state $s$ under policy $\pi$, given a set of episodes obtained by following $\pi$ and passing though $s$. Each occurrence of state $s$ in an episode is called a <em>visit</em> to $s$. Let us call the first time it is visited in an episode the <em>first visit</em> to $s$. The <em>first-visit MC method</em> estimates $v_{\pi}(s)$ as the average of the returns following first visits to $s$, whereas the <em>every-visit MC method</em> averages the returns following all visits to $s$.</p><blockquote><p><strong>First-visit MC policy evaluation (returns $V \approx v_{\pi}$)</strong></p><p>Initialize:</p><p>​ $\pi \leftarrow$ policy to be evaluated</p><p>​ $V \leftarrow $ an arbitrary state-value function</p><p>​ $Return(s) \leftarrow$ an empty list, for all $s \in \mathcal{S}$</p><p>Repeat forever:</p><p>​ Generate an episode using $\pi$</p><p>​ For each state $s$ appearing in the episode:</p><p>​ $G \leftarrow$ return following the first occurrence of $s$</p><p>​ Append $G$ to $Return(s)$</p><p>​ $V(s) \leftarrow$ $\text{average}(Return(s))$</p></blockquote><p>Next, we’ll use this algorithm to solve a naive problem that defined as follows:</p><blockquote><p>The object of the popular casino card game of <em>blackjack</em> is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a <em>natural</em>. He then wins unless the dealer also has a natural, in which case the game is draw. If the player does not have a natural, then he can request additional cards, one by one (<em>hits</em>), until he either stop (<em>sticks</em>) or excepted 21 (<em>goes bust</em>). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome–win, lose, draw–is determined by whose final sum is closer to 21.</p></blockquote><p>Playing blackjack is naturally formulated as an episode finite MDP. Each game of blackjack is an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount ($\gamma=1$); therefore these terminal rewards are also returns. The player’s actions are to hit or to stick. We assume that cards are dealt from an infinite deck (i.e., with replacement). If the player holds an ace that he could count as 11 without going bust, then the ace is said to be <em>usable</em>. Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits. Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.</p><p>Note that, although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events–in particular, they require the quantities $p(s^{\prime}, r|s, a)$–and it is not easy to determine these for blackjack. For example, suppose the play’s sum is 14 and he chooses to sticks. What is his excepted reward as a function of the dealer’s showing card? All of these rewards and transition probabilities must be computed <em>before</em> DP can be applied, and such computations are often complex and error-prone.</p><p>The conceptual diagram of the experimental results is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/blackjack_c.png" alt="blackjack_c"></p><p><em>Figure 1</em></p><p>The first we define some auxiliary variables and methods:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># actions: hit or stand (stick)</span></div><div class="line">ACTION_HIT = <span class="number">0</span></div><div class="line">ACTION_STAND = <span class="number">1</span></div><div class="line">actions = [ACTION_HIT, ACTION_STAND]</div><div class="line"></div><div class="line"><span class="comment"># policy for player</span></div><div class="line">policyPlayer = np.zeros(<span class="number">22</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">20</span>):</div><div class="line">    policyPlayer[i] = ACTION_HIT</div><div class="line">policyPlayer[<span class="number">20</span>] = ACTION_STAND</div><div class="line">policyPlayer[<span class="number">21</span>] = ACTION_STAND</div><div class="line"></div><div class="line"><span class="comment"># function form of target policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">return</span> policyPlayer[playerSum]</div><div class="line"></div><div class="line"><span class="comment"># function form of behavior policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> ACTION_STAND</div><div class="line">    <span class="keyword">return</span> ACTION_HIT</div><div class="line"></div><div class="line"><span class="comment"># policy for dealer</span></div><div class="line">policyDealer = np.zeros(<span class="number">22</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">17</span>):</div><div class="line">    policyDealer[i] = ACTION_HIT</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">17</span>, <span class="number">22</span>):</div><div class="line">    policyDealer[i] = ACTION_STAND</div><div class="line"></div><div class="line"><span class="comment"># get a new card</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCard</span><span class="params">()</span>:</span></div><div class="line">    card = np.random.randint(<span class="number">1</span>, <span class="number">14</span>)</div><div class="line">    card = min(card, <span class="number">10</span>)</div><div class="line">    <span class="keyword">return</span> card</div></pre></td></tr></table></figure><p>Furthermore, we also have a print method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># print the state value</span></div><div class="line">figureIndex = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span><span class="params">(data, tile, zlabel=<span class="string">'reward'</span>)</span>:</span></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    fig = plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    fig.suptitle(tile)</div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</div><div class="line">    axisX = []</div><div class="line">    axisY = []</div><div class="line">    axisZ = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">22</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</div><div class="line">            axisX.append(i)</div><div class="line">            axisY.append(j)</div><div class="line">            axisZ.append(data[i - <span class="number">12</span>, j - <span class="number">1</span>])</div><div class="line">    ax.scatter(axisX, axisY, axisZ)</div><div class="line">    ax.set_xlabel(<span class="string">'player sum'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'dealer showing'</span>)</div><div class="line">    ax.set_zlabel(zlabel)</div></pre></td></tr></table></figure><p>In order to get the figure above, we wrote the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onPolicy</span><span class="params">()</span>:</span></div><div class="line">    statesUsableAce1, statesNoUsableAce1 = monteCarloOnPolicy(<span class="number">10000</span>)</div><div class="line">    statesUsableAce2, statesNoUsableAce2 = monteCarloOnPolicy(<span class="number">500000</span>)</div><div class="line">    prettyPrint(statesUsableAce1, <span class="string">'Usable Ace, 10000 Episodes'</span>)</div><div class="line">    prettyPrint(statesNoUsableAce1, <span class="string">'No Usable Ace, 10000 Episodes'</span>)</div><div class="line">    prettyPrint(statesUsableAce2, <span class="string">'Usable Ace, 500000 Episodes'</span>)</div><div class="line">    prettyPrint(statesNoUsableAce2, <span class="string">'No Usable Ace, 500000 Episodes'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>There is a term named <em>on policy</em>, we’ll explain this term later. Now let us jump into the <strong>monteCarloOnPolicy</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo Sample with On-Policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOnPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    statesUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    statesNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesNoUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        state, reward, _ = play(targetPolicyPlayer)</div><div class="line">        state[<span class="number">1</span>] -= <span class="number">12</span></div><div class="line">        state[<span class="number">2</span>] -= <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> state[<span class="number">0</span>]:</div><div class="line">            statesUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            statesNoUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesNoUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">    <span class="keyword">return</span> statesUsableAce / statesUsableAceCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>We ignore he first four variables now and explain them later. <strong>nEpisodes</strong> represents the number of the episodes and the <strong>play</strong> method simulates the process of the blackjack cards game. So what is it like? This method is too long (more than 100 rows) so we partially explain it. In the first place, this method define some auxiliary variables:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># play a game</span></div><div class="line"><span class="comment"># @policyPlayerFn: specify policy for player</span></div><div class="line"><span class="comment"># @initialState: [whether player has a usable Ace, sum of player's cards, one card of dealer]</span></div><div class="line"><span class="comment"># @initialAction: the initial action</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">(policyPlayerFn, initialState=None, initialAction=None)</span>:</span></div><div class="line">    <span class="comment"># player status</span></div><div class="line">	<span class="comment"># sum of player</span></div><div class="line">    playerSum = <span class="number">0</span></div><div class="line">    <span class="comment"># trajectory of player</span></div><div class="line">    playerTrajectory = []</div><div class="line">    <span class="comment"># whether player uses Ace as 11</span></div><div class="line">    usableAcePlayer = <span class="keyword">False</span></div><div class="line">    <span class="comment"># dealer status</span></div><div class="line">    dealerCard1 = <span class="number">0</span></div><div class="line">    dealerCard2 = <span class="number">0</span></div><div class="line">    usableAceDealer = <span class="keyword">False</span></div></pre></td></tr></table></figure><p>Then, we generate a random initial state if the initial state is null. If not, we load the initial state from initialState variable:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> initialState <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">    <span class="comment"># generate a random initial state</span></div><div class="line">    numOfAce = <span class="number">0</span></div><div class="line">    <span class="comment"># initialize cards of player</span></div><div class="line">    <span class="keyword">while</span> playerSum &lt; <span class="number">12</span>:</div><div class="line">        <span class="comment"># if sum of player is less than 12, always hit</span></div><div class="line">        card = getCard()</div><div class="line">        <span class="comment"># if get an Ace, use it as 11</span></div><div class="line">        <span class="keyword">if</span> card == <span class="number">1</span>:</div><div class="line">            numOfAce += <span class="number">1</span></div><div class="line">            card = <span class="number">11</span></div><div class="line">            usableAcePlayer = <span class="keyword">True</span></div><div class="line">        playerSum += card</div><div class="line">    <span class="comment"># if player's sum is larger than 21, he must hold at least one Ace, two Aces are possible</span></div><div class="line">    <span class="keyword">if</span> playerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="comment"># use the Ace as 1 rather than 11</span></div><div class="line">        playerSum -= <span class="number">10</span></div><div class="line">        <span class="comment"># if the player only has one Ace, then he doesn't have usable Ace any more</span></div><div class="line">        <span class="keyword">if</span> numOfAce == <span class="number">1</span>:</div><div class="line">            usableAcePlayer = <span class="keyword">False</span></div><div class="line">    <span class="comment"># initialize cards of dealer, suppose dealer will show the first card he gets</span></div><div class="line">    dealerCard1 = getCard()</div><div class="line">    dealerCard2 = getCard()</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">    	<span class="comment"># use specified initial state</span></div><div class="line">    	usableAcePlayer = initialState[<span class="number">0</span>]</div><div class="line">    	playerSum = initialState[<span class="number">1</span>]</div><div class="line">    	dealerCard1 = initialState[<span class="number">2</span>]</div><div class="line">    	dealerCard2 = getCard()</div><div class="line"></div><div class="line">	<span class="comment"># initial state of the game</span></div><div class="line">	state = [usableAcePlayer, playerSum, dealerCard1]</div><div class="line"></div><div class="line">	<span class="comment"># initialize dealer's sum</span></div><div class="line">    dealerSum = <span class="number">0</span></div><div class="line">    <span class="keyword">if</span> dealerCard1 == <span class="number">1</span> <span class="keyword">and</span> dealerCard2 != <span class="number">1</span>:</div><div class="line">        dealerSum += <span class="number">11</span> + dealerCard2</div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> dealerCard1 != <span class="number">1</span> <span class="keyword">and</span> dealerCard2 == <span class="number">1</span>:</div><div class="line">        dealerSum += dealerCard1 + <span class="number">11</span></div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> dealerCard1 == <span class="number">1</span> <span class="keyword">and</span> dealerCard2 == <span class="number">1</span>:</div><div class="line">        dealerSum += <span class="number">1</span> + <span class="number">11</span></div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        dealerSum += dealerCard1 + dealerCard2</div></pre></td></tr></table></figure><p>Game start! Above all is player’s turn:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># player's turn</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">if</span> initialAction <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        action = initialAction</div><div class="line">        initialAction = <span class="keyword">None</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># get action based on current sum</span></div><div class="line">        action = policyPlayerFn(usableAcePlayer, playerSum, dealerCard1)</div><div class="line"></div><div class="line">    <span class="comment"># track player's trajectory for importance sampling</span></div><div class="line">    playerTrajectory.append([action, (usableAcePlayer, playerSum, dealerCard1)])</div><div class="line"></div><div class="line">    <span class="keyword">if</span> action == ACTION_STAND:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># if hit, get new card</span></div><div class="line">    playerSum += getCard()</div><div class="line"></div><div class="line">    <span class="comment"># player busts</span></div><div class="line">    <span class="keyword">if</span> playerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="comment"># if player has a usable Ace, use it as 1 to avoid busting and continue</span></div><div class="line">        <span class="keyword">if</span> usableAcePlayer == <span class="keyword">True</span>:</div><div class="line">            playerSum -= <span class="number">10</span></div><div class="line">            usableAcePlayer = <span class="keyword">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># otherwise player loses</span></div><div class="line">            <span class="keyword">return</span> state, <span class="number">-1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>Then is the dealer’s turn if the player’s turn is end:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># get action based on current sum</span></div><div class="line">    action = policyDealer[dealerSum]</div><div class="line">    <span class="keyword">if</span> action == ACTION_STAND:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># if hit, get a new card</span></div><div class="line">    dealerSum += getCard()</div><div class="line">    <span class="comment"># dealer busts</span></div><div class="line">    <span class="keyword">if</span> dealerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="keyword">if</span> usableAceDealer == <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># if dealer has a usable Ace, use it as 1 to avoid busting and continue</span></div><div class="line">            dealerSum -= <span class="number">10</span></div><div class="line">            usableAceDealer = <span class="keyword">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># otherwise dealer loses</span></div><div class="line">            <span class="keyword">return</span> state, <span class="number">1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>If the both sides have finished the game:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compare the sum between player and dealer</span></div><div class="line"><span class="keyword">if</span> playerSum &gt; dealerSum:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">1</span>, playerTrajectory</div><div class="line"><span class="keyword">elif</span> playerSum == dealerSum:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">0</span>, playerTrajectory</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">-1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>Now, let us come back the <strong>mentoCarloOnPolicy</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOnPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    statesUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    statesNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesNoUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        state, reward, _ = play(targetPolicyPlayer)</div><div class="line">        state[<span class="number">1</span>] -= <span class="number">12</span></div><div class="line">        state[<span class="number">2</span>] -= <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> state[<span class="number">0</span>]:</div><div class="line">            statesUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            statesNoUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesNoUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">    <span class="keyword">return</span> statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>In this method we ignore the player’s trajectory (represent by the <strong>playerTrajectory</strong> variable). If you remember a sentence in the game definition (as follows) it will easy to understand.</p><blockquote><p>Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.</p></blockquote><p>This row (as follows) is to calculate the average returns of each state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">return</span> statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>Recall the beginning of the code and let’s see what results are like:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_10000.png" alt="usable_ace_10000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_10000.png" alt="no_usable_ace_10000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_500000.png" alt="usable_ace_500000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_500000.png" alt="no_usable_ace_500000"></p><p><em>Figure 2 (from up to down). (1) Usable Ace, 10000 Episodes. (2) No Usable Ace, 10000 Episodes. (3) Usable Ace, 500000 Episodes. (4) No Usable Ace, 500000 Episodes.</em></p><p>If a model is not available, then it is particularly useful to estimate <em>action values</em> (the value of state-value pairs) rather than <em>state values</em>. With a model, state values alone are sufficient to determine a policy; one simply look ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the earlier on <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">DP</a>. Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of out primary goals for Monte Carlo methods is to estimate $q_{\star}$. To achieve this, we first consider the policy evaluation problem for action values.</p><p>The policy evaluation problem for action values is estimate $q_{\pi}(s, a)$. The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a state-action pair rather than to a state. The only complication is that many state-value pairs may never be visited. This is the general problem of <em>maintaining exploration</em>, as discussed in the context of the k-armed bandit problem in <a href="https://ewanlee.github.io/2017/05/27/k-Armed-Bandit-Problem/" target="_blank" rel="external">here</a>. One way to do this is by specifying that the episodes <em>start in a state-action pair</em>, and that every pair has a nonzero probability of being selected as the start. We call this the assumption of <em>exploring starts</em>.</p><p>We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal policies. To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with a arbitrary policy $\pi_{0}$ and ending with the optimal policy and optimal action-value function:<br>$$<br>\pi_{0} \stackrel{E}\longrightarrow q_{\pi{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow q_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow q_{\star},<br>$$<br>We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes. We postpone consideration of the first assumption until later.</p><p>The primary approach to avoiding the infinite number of episodes nominally required for policy evaluation is to forgo to complete policy evaluation before returning to policy improvement. For Monte Carlo policy evaluation it it natural to alternate between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation,and then the policy is improved at all the states visited in the episode.</p><blockquote><p><strong>Monte Carlo ES (Exploring Starts)</strong></p><p>Initialize, for all $s \in \mathcal{S},\; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow \text{arbitrary}$</p><p>​ $\pi(s) \leftarrow \text{arbitrary}$</p><p>​ $Returns(s,a) \leftarrow \text{empty list}$</p><p>Repeat forever:</p><p>​ Choose $S_{0} \in \mathcal{S}$ and $A_{0} \in \mathcal{A(S_{0})}$ s.t. all pairs have probability &gt; 0</p><p>​ Generate an episode starting from $S_0, A_0$, following $\pi$</p><p>​ For each pair $s, a$ appearing in the episode:</p><p>​ $G \leftarrow \text{return following the first occurrence of} \; s, a$</p><p>​ Append $G$ to $Returns(s,a)$</p><p>​ $Q(s,a) \leftarrow \text{average}(Returns(s,a))$</p><p>​ For each $s$ in the episode:</p><p>​ $\pi(s) \leftarrow \arg\max_{a} Q(s,a)$</p></blockquote><p>Recall the blackjack card game. It is straightforward to apply Monte Carlo ES to blackjack.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">figure5_3</span><span class="params">()</span>:</span></div><div class="line">    stateActionValues = monteCarloES(<span class="number">500000</span>)</div><div class="line">    stateValueUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    stateValueNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># get the optimal policy</span></div><div class="line">    actionUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="string">'int'</span>)</div><div class="line">    actionNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="string">'int'</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">            stateValueNoUsableAce[i, j] = np.max(stateActionValues[i, j, <span class="number">0</span>, :])</div><div class="line">            stateValueUsableAce[i, j] = np.max(stateActionValues[i, j, <span class="number">1</span>, :])</div><div class="line">            actionNoUsableAce[i, j] = argmax(stateActionValues[i, j, <span class="number">0</span>, :])</div><div class="line">            actionUsableAce[i, j] = argmax(stateActionValues[i, j, <span class="number">1</span>, :])</div><div class="line">    prettyPrint(stateValueUsableAce, <span class="string">'Optimal state value with usable Ace'</span>)</div><div class="line">    prettyPrint(stateValueNoUsableAce, <span class="string">'Optimal state value with no usable Ace'</span>)</div><div class="line">    prettyPrint(actionUsableAce, <span class="string">'Optimal policy with usable Ace'</span>, <span class="string">'Action (0 Hit, 1 Stick)'</span>)</div><div class="line">    prettyPrint(actionNoUsableAce, <span class="string">'Optimal policy with no usable Ace'</span>, <span class="string">'Action (0 Hit, 1 Stick)'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Run the code we’ll get the conceptual diagram like follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces.png" alt="mces"></p><p>Let us to see the implementation (<strong>monteCarloES</strong> method) of this algorithm. Note that, some auxiliary variables are defined earlier.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo with Exploring Starts</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloES</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    <span class="comment"># (playerSum, dealerCard, usableAce, action)</span></div><div class="line">    stateActionValues = np.zeros((<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">2</span>))</div><div class="line">    <span class="comment"># set default to 1 to avoid being divided by 0</span></div><div class="line">    stateActionPairCount = np.ones((<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">2</span>))</div><div class="line">    <span class="comment"># behavior policy is greedy</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicy</span><span class="params">(usableAce, playerSum, dealerCard)</span>:</span></div><div class="line">        usableAce = int(usableAce)</div><div class="line">        playerSum -= <span class="number">12</span></div><div class="line">        dealerCard -= <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> argmax(stateActionValues[playerSum, dealerCard, usableAce, :])</div><div class="line"></div><div class="line">    <span class="comment"># play for several episodes</span></div><div class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(nEpisodes):</div><div class="line">        print(<span class="string">'episode:'</span>, episode)</div><div class="line">        <span class="comment"># for each episode, use a randomly initialized state and action</span></div><div class="line">        initialState = [bool(np.random.choice([<span class="number">0</span>, <span class="number">1</span>])),</div><div class="line">                       np.random.choice(range(<span class="number">12</span>, <span class="number">22</span>)),</div><div class="line">                       np.random.choice(range(<span class="number">1</span>, <span class="number">11</span>))]</div><div class="line">        initialAction = np.random.choice(actions)</div><div class="line">        _, reward, trajectory = play(behaviorPolicy, initialState, initialAction)</div><div class="line">        <span class="keyword">for</span> action, (usableAce, playerSum, dealerCard) <span class="keyword">in</span> trajectory:</div><div class="line">            usableAce = int(usableAce)</div><div class="line">            playerSum -= <span class="number">12</span></div><div class="line">            dealerCard -= <span class="number">1</span></div><div class="line">            <span class="comment"># update values of state-action pairs</span></div><div class="line">            stateActionValues[playerSum, dealerCard, usableAce, action] += reward</div><div class="line">            stateActionPairCount[playerSum, dealerCard, usableAce, action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> stateActionValues / stateActionPairCount</div></pre></td></tr></table></figure><p>You can see we use the <strong>trajectory</strong> variable now. The difference between Monte Carlo On Policy and Monte Carlo ES is prior calculates the average return of each state and later calculates the average return of each state-action pair.</p><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_policy.png" alt="mcse_usbale_ace_optimal_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_policy.png" alt="mcse_usbale_ace_optimal_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_state_value.png" alt="mcse_usable_ace_optimal_state_value"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_state_value.png" alt="mcse_no_usable_ace_optimal_state_value"></p><p>How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call <em>on-policy</em> (Do you remember this term?) methods and <em>off-policy</em> methods. On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a policy different from that used to generate the data.. The First-visit Monte Carlo method and the Monte Carlo ES method are an example of an on-policy method. Here we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. Off-policy methods are considered later.</p><p>The on-policy method we presented in here uses $\epsilon \text{-} greedy$ policies. The complete algorithm is given below.</p><blockquote><p><strong>On-policy first-visit MC control (for $\epsilon \text{-soft}$ policies)</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a ) \leftarrow \text{arbitrary}$</p><p>​ $Returns(s,a) \leftarrow \text{empty list}$</p><p>​ $\pi(a|s) \leftarrow \text{an arbitrary} \; \epsilon \text{-soft policy}$</p><p>Repeat forever:</p><p>​ (a) Generate an episode using $\pi$</p><p>​ (b) For each pair $s, a$ appearing in the episode:</p><p>​ $G \leftarrow $ return following the first occurrence of $s, a$</p><p>​ Append $G$ to $Returns(s,a)$</p><p>​ $Q(s, a) \leftarrow \text{average}(Returns(s,a))$</p><p>​ (c) For each s in the episode:</p><p>​ $A^{\star} \leftarrow \arg\max_{a} Q(s,a)$</p><p>​ For all $a \in \mathcal{A(s)}$:</p><p>​ $\pi(a|s) \leftarrow \left\{ \begin{array}{c} 1 - \epsilon + \epsilon/|\mathcal{A(s)}|\;\;\;\text{if} \;a=A^{\star} \\ \epsilon/|\mathcal{A(s)}|\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{if} \;a\neq A^{\star} \end{array} \right.$</p></blockquote><p>All learning control methods face a dilemma: They seek to learn action values conditional on subsequent <em>optimal</em> behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromise–it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to user two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the <em>target policy</em>, and the policy used to generate behavior is called the <em>behavior policy</em>. In this case we say that learning is from data “off” the target policy, and the overall process is termed <em>off-policy learning</em>.</p><p>We begin the study of off-policy methods by considering the <em>prediction</em> problem, in which both target and behavior policies are fixed. That is, suppose we wish to estimate $v_{\pi}$ or $q_{\pi}$, but all we have are episodes following another policy $\mu$, where $\mu \neq \pi$. In this case, $\pi$ is the target policy, $\mu$ is the behavior policy, and both policies are considered fixed and given.</p><p>In order to use episodes from $\mu$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $\mu$. That is, we require that $\pi(a|s) &gt; 0$ implies $\mu(a|s) &gt; 0$. This is called the assumption of <em>converge</em>. It follows from converge that $\mu$ must be stochastic in states where it is not identical to $\pi$. The target policy $\pi$, on the other hand, may be deterministic. In here, we consider the prediction problem, in which $\pi$ is unchanging and given.</p><p>Almost all off-policy methods utilize <em>importance sampling</em>ddd, a general technique for estimating excepted values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectory occurring under the target and behavior policies, called the <em>importance-sampling ratio</em>. Given a starting state $S_{t}$, the probability of the subsequent state-action trajectory , $A_{t}, S_{t+1}, A_{t+1, \cdots, S_{T}}$, occurring under any policy $\pi$ is<br>$$<br>\prod_{k=t}^{T-1} \pi(A_k | S_k)p(S_{k+1} | S_k, A_k),<br>$$<br>where $p$ here is the state-transition probability function. Thus, the relative probability of the trajectory under the target and behavior policies (the important-sampling ratio) is<br>$$<br>\rho_{t}^{T} \doteq \frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1} \mu(A_k|S_k)p(S_{k+1}|S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi (A_k | S_k)}{\mu (A_k | S_k)}<br>$$<br>Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy $\mu$ to estimate $v_{\pi}(s)$. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time $t=101$. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state $s$ is visited, denote $\tau(s)$. This is for an every-visit method; for a first-visit method, $\tau(s)$ would only include time steps that were first visits to $s$ within their episodes. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up though $T(t)$. Then $\{G_t\}_{t \in \tau(s)}$ are returns that pertain to state $s$, and $\{\rho_{t}^{T(t)}\}_{t \in \tau(s)}$ are the corresponding importance-sampling ratios. To estimate $v_{\pi}(s)$, we simply scale the returns by the ratios and average the results:<br>$$<br>V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{|\tau(s)|}.<br>$$<br>When importance sampling is done as a simple average in this way it is called <em>ordinary importance sampling</em>.</p><p>An important alternative is <em>weighted importance sampling</em>, which uses a weighted average, defined as<br>$$<br>V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{\sum_{t \in \tau(s)} \rho_{t}^{T(t)}},<br>$$<br>or zero if the denominator is zero.</p><p>We applied both ordinary and weighted importance-sampling methods to estimate the value of as single blackjack state from off-policy data. Recall that one of the advantages of Monte Carlo methods is that they can be used to evaluate a single state without forming estimated for any other states. In this example, we evaluated the state in which the dealer is showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three aces). The data was generate by starting in this state then choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21. The value of this state under the target policy is approximately -0.27726 (this was determined by separately generating one-hundred million episodes using the target policy and averaging their returns). Both off-policy methods closely approximated this value after 1000 off-policy episodes using the random policy. To make sure they did this reliably, we performed 100 independent runs, each starting from estimates of zero and learning for 10,000 episodes. The implementation details are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo Sample with Off-Policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOffPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    initialState = [<span class="keyword">True</span>, <span class="number">13</span>, <span class="number">2</span>]</div><div class="line">    sumOfImportanceRatio = [<span class="number">0</span>]</div><div class="line">    sumOfRewards = [<span class="number">0</span>]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        _, reward, playerTrajectory = play(behaviorPolicyPlayer, initialState=initialState)</div><div class="line"></div><div class="line">        <span class="comment"># get the importance ratio</span></div><div class="line">        importanceRatioAbove = <span class="number">1.0</span></div><div class="line">        importanceRatioBelow = <span class="number">1.0</span></div><div class="line">        <span class="keyword">for</span> action, (usableAce, playerSum, dealerCard) <span class="keyword">in</span> playerTrajectory:</div><div class="line">            <span class="keyword">if</span> action == targetPolicyPlayer(usableAce, playerSum, dealerCard):</div><div class="line">                importanceRatioBelow *= <span class="number">0.5</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                importanceRatioAbove = <span class="number">0.0</span></div><div class="line">                <span class="keyword">break</span></div><div class="line">        importanceRatio = importanceRatioAbove / importanceRatioBelow</div><div class="line">        sumOfImportanceRatio.append(sumOfImportanceRatio[<span class="number">-1</span>] + importanceRatio)</div><div class="line">        sumOfRewards.append(sumOfRewards[<span class="number">-1</span>] + reward * importanceRatio)</div><div class="line">    <span class="keyword">del</span> sumOfImportanceRatio[<span class="number">0</span>]</div><div class="line">    <span class="keyword">del</span> sumOfRewards[<span class="number">0</span>]</div><div class="line"></div><div class="line">    sumOfRewards= np.asarray(sumOfRewards)</div><div class="line">    sumOfImportanceRatio= np.asarray(sumOfImportanceRatio)</div><div class="line">    ordinarySampling = sumOfRewards / np.arange(<span class="number">1</span>, nEpisodes + <span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> np.errstate(divide=<span class="string">'ignore'</span>,invalid=<span class="string">'ignore'</span>):</div><div class="line">        weightedSampling = np.where(sumOfImportanceRatio != <span class="number">0</span>, sumOfRewards / sumOfImportanceRatio, <span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> ordinarySampling, weightedSampling</div></pre></td></tr></table></figure><p>Note that the <strong>behaviorPolicyPlayer</strong> that is a function that define the behavior policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># function form of behavior policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> ACTION_STAND</div><div class="line">    <span class="keyword">return</span> ACTION_HIT</div></pre></td></tr></table></figure><p>And the calculation of the numerator and denominator of the importance-sampling are the summation operation. As the number of iterations increases, we need to accumulate the result from each episode. So we need to store the previous results. The <strong>sumOfRewards</strong> and <strong>sumOfImportanceRatio</strong> are used for this purpose.</p><p>Then we need to show the result (mean square error):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Figure 5.4</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">offPolicy</span><span class="params">()</span>:</span></div><div class="line">    trueValue = <span class="number">-0.27726</span></div><div class="line">    nEpisodes = <span class="number">10000</span></div><div class="line">    nRuns = <span class="number">100</span></div><div class="line">    ordinarySampling = np.zeros(nEpisodes)</div><div class="line">    weightedSampling = np.zeros(nEpisodes)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nRuns):</div><div class="line">        ordinarySampling_, weightedSampling_ = monteCarloOffPolicy(nEpisodes)</div><div class="line">        <span class="comment"># get the squared error</span></div><div class="line">        ordinarySampling += np.power(ordinarySampling_ - trueValue, <span class="number">2</span>)</div><div class="line">        weightedSampling += np.power(weightedSampling_ - trueValue, <span class="number">2</span>)</div><div class="line">    ordinarySampling /= nRuns</div><div class="line">    weightedSampling /= nRuns</div><div class="line">    axisX = np.log10(np.arange(<span class="number">1</span>, nEpisodes + <span class="number">1</span>))</div><div class="line">    plt.plot(axisX, ordinarySampling, label=<span class="string">'Ordinary Importance Sampling'</span>)</div><div class="line">    plt.plot(axisX, weightedSampling, label=<span class="string">'Weighted Importance Sampling'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Episodes (10^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Mean square error'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/off_policy.png" alt="off_policy"></p><p>Formally, the difference between the two kinds of importance sampling is expressed in their biases and variances. The ordinary importance-sampling estimator is unbiased whereas the weighted importance-sampling estimator is biased (the bias converges asymptotically to zero). On the other hand, the variance of the ordinary importance-sampling estimator is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximate that we explore later.</p><p>The estimates of ordinary importance sampling will typical have infinite variance, and this can easily happen in off-policy learning when trajectories contains loops. A simple example is shown below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance_example.png" alt="infinite_variance_example"></p><p>There is only one nonterminal state $s$ and two action, <strong>end</strong> and <strong>back</strong>. The <strong>end</strong> action causes a deterministic transition to termination, whereas the <strong>back</strong> action transitions, with probability 0.9, back to $s$ or, with probability 0.1, on to termination. The rewards are +1 on latter transition and otherwise zero. Consider the target policy that always selects back. All episodes under this policy consist of some number (possibly zero) of transitions back to $s$ followed by termination with a reward and return of +1. Thus the value of $s$ under the target policy is 1. Suppose we are estimating this value from off-policy data using the behavior policy that selects <strong>end</strong> and <strong>back</strong> with equal probability.</p><p>The implementation details are as follows. We first define the two policies:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">ACTION_BACK = <span class="number">0</span></div><div class="line">ACTION_END = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># behavior policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicy</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># target policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetPolicy</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> ACTION_BACK</div></pre></td></tr></table></figure><p>Then we define how an episode runs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># one turn</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># track the action for importance ratio</span></div><div class="line">    trajectory = []</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        action = behaviorPolicy()</div><div class="line">        trajectory.append(action)</div><div class="line">        <span class="keyword">if</span> action == ACTION_END:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span>, trajectory</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.9</span>) == <span class="number">0</span>:</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span>, trajectory</div></pre></td></tr></table></figure><p>Now we start our off-policy (first-visit MC) learning process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Figure 5.5</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloSample</span><span class="params">()</span>:</span></div><div class="line">    runs = <span class="number">10</span></div><div class="line">    episodes = <span class="number">100000</span></div><div class="line">    axisX = np.log10(np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>))</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        sumOfRewards = [<span class="number">0</span>]</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            reward, trajectory = play()</div><div class="line">            <span class="keyword">if</span> trajectory[<span class="number">-1</span>] == ACTION_END:</div><div class="line">                importanceRatio = <span class="number">0</span> <span class="comment"># Because it is impossible on the target policy</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                importanceRatio = <span class="number">1.0</span> / pow(<span class="number">0.5</span>, len(trajectory))</div><div class="line">            sumOfRewards.append(sumOfRewards[<span class="number">-1</span>] + importanceRatio * reward)</div><div class="line">        <span class="keyword">del</span> sumOfRewards[<span class="number">0</span>]</div><div class="line">        estimations = np.asarray(sumOfRewards) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">        plt.plot(axisX, estimations)</div><div class="line">    plt.xlabel(<span class="string">'Episodes (10^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Ordinary Importance Sampling'</span>)</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span></div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance.png" alt="inifinite_variance"></p><p>The correct estimate here is 1, and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite, and the estimates do not convergence to this value.</p><p>At last, we proposed two fancy algorithms, that is, the <strong>Incremental off-policy every-visit MC policy evaluation</strong> and the <strong>Off-policy every-visit MC control</strong>.</p><blockquote><p><strong>Incremental off-policy every-visit MC policy evaluation</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow$ arbitrary</p><p>​ $C(s,a) \leftarrow$ 0</p><p>​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy</p><p>​ $\pi(a|s) \leftarrow$ an arbitrary target policy</p><p>Repeat forever:</p><p>​ Generate an episode using $\mu$:</p><p>​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$</p><p>​ $G \leftarrow 0$</p><p>​ $W \leftarrow 1$</p><p>​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:</p><p>​ $G \leftarrow \gamma G + R_{t+1}$</p><p>​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$</p><p>​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$</p><p>​ $W \leftarrow W\frac{\pi(A_t | S_t)}{\mu(A_t | S_t)}$</p><p>​ If $W = 0$ then ExitForLoop</p><p><strong>Off-policy every-visit MC control (returns $\pi \approx \pi_{\star}$)</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow$ arbitrary</p><p>​ $C(s,a) \leftarrow$ 0</p><p>​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy</p><p>​ $\pi(s) \leftarrow$ an deterministic policy that is greedy with respect $Q$</p><p>Repeat forever:</p><p>​ Generate an episode using $\mu$:</p><p>​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$</p><p>​ $G \leftarrow 0$</p><p>​ $W \leftarrow 1$</p><p>​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:</p><p>​ $G \leftarrow \gamma G + R_{t+1}$</p><p>​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$</p><p>​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$</p><p>​ $\pi(S_t) \leftarrow \arg \max_{a}Q(S_t, a)$ (with ties broken consistently)</p><p>​ If $A_t \neq \pi(S_t)$ then ExitForLoop</p><p>​ $W \leftarrow W\frac{1}{\mu(A_t | S_t)}$</p></blockquote></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/31/Dynamic-Programming/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/31/Dynamic-Programming/" itemprop="url">Dynamic Programming</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-31T10:19:52+08:00">2017-05-31 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/31/Dynamic-Programming/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/31/Dynamic-Programming/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/31/Dynamic-Programming/" class="leancloud_visitors" data-flag-title="Dynamic Programming"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are provides an essential foundation for the understanding of the methods presented later. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.</p><p>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. In here we show how DP can be used to compute the value functions defined in earlier. As discussed there, we can easily obtain optimal policies once we have found the optimal value functions $v_{\star}$ or $q_{\star}$ which satisfy the Bellman optimality equations:<br>$$<br>\begin{align}<br>v_{\star}(s) &amp;= \max_{a} \mathbb{E} [R_{t+1} + \gamma v_{\star}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \max_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) \left[r + \gamma v_{\star}(s^{\prime})\right]<br>\end{align}<br>$$<br>or<br>$$<br>\begin{align}<br>q_{\star}(s, a) &amp;= \mathbb{E} [R_{t+1} + \gamma \max_{a^{\prime}} q_{\star}(S_{t+1}, a^{\prime}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma \max_{a^{\prime}} q_{\star}(s^{\prime}, a^{\prime})]<br>\end{align}<br>$$<br>for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}, \; \text{and} \; s^{\prime} \in \mathcal{S^{+}}$. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.</p><p>First we consider how to compute the state-value function $v_{\pi}$ for an arbitrary policy $\pi$. This is called <em>policy evaluation</em> in the DP literature. We also refer to it as the <em>prediction problem</em>. Recall that for all $s \in \mathcal{S}$,<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots \ | \ S_{t}=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_{t}=s] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{\pi}(s^{\prime})]<br>\end{align}<br>$$<br>If the environment’s dynamics are complete known, then (7) is a system of $|\mathcal{S}|$ simultaneous linear equations in $|\mathcal{S}|$ unknowns (the $v_{\pi}(s), s \in \mathcal{S}$). In principle, its solution is a straightforward, if tedious, computation. For our purpose, iterative solution methods are most suitable. The initial approximation, $v_0$, is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation for $v_{\pi}$ as an update rule:<br>$$<br>\begin{align}<br>v_{k+1}(s) &amp;\doteq \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r|s, a) [r + \gamma v_{k} (s^{\prime})]<br>\end{align}<br>$$<br>This algorithm is called <em>iterative policy evaluation</em>.</p><blockquote><p><strong>Iterative policy evaluation</strong></p><p>Input $\pi$, the policy to be evaluated</p><p>Initialize an array $V(s) = 0$, for all $s \in \mathcal{S^{+}}$</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ for each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p><p>Output $V \approx v_{\pi}$</p></blockquote><p>We can see the algorithm used in the <a href="https://ewanlee.github.io/2017/05/29/The-GridWorld-problem/" target="_blank" rel="external">grid world problem</a> just is the <em>iterative policy evaluation</em>.</p><p>Our reason for computing the value function for a policy is to help find better policies. Once a policy $\pi$ has been improved using $v_{\pi}$ to yield a better policy $\pi^{\prime}$, we can then compute $v_{\pi^{\prime}}$and improve it again to yield an even better $\pi^{\prime\prime}$. We can thus obtain a sequence of monotonically improving policies and value functions:<br>$$<br>\pi_{0} \stackrel{E}\longrightarrow v_{\pi_{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow v_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow v_{\star},<br>$$<br>where $\stackrel{E}\longrightarrow$ denotes a policy <em>evaluation</em> and $\stackrel{I}\longrightarrow$ denotes a policy <em>improvement</em>. This way of finding an optimal policy is called <em>policy iteration</em>.</p><blockquote><p><strong>Policy iteration (using iterative policy evaluation)</strong></p><ol><li><p>Initialization</p><p>$V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A(s)}$ arbitrarily for all $s \in \mathcal{S}$</p></li><li><p>Policy Evaluation</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ For each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \sum_{s^{\prime}, r} p(s^{\prime}, r | s, \pi(s)) [r + \gamma v(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p></li><li><p>Policy Improvement</p><p><em>policy-stable</em> $\leftarrow$ <em>true</em></p><p>For each $s \in \mathcal{S}$:</p><p>​ <em>old-action</em> $\leftarrow$ $\pi_(s)$</p><p>​ $\pi (s) \leftarrow argmax_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$</p><p>​ If <em>old-action</em> $\neq \pi(s)$, then <em>policy-stable</em> $\leftarrow$ <em>false</em></p><p>If <em>policy-stable</em>, then stop and return $V \approx v_{\star} \; \text{and} \; \pi \approx \pi_{\star}$; else go to 2.</p></li></ol></blockquote><p>Let us solve a problem used by <strong>policy iteration</strong>. The problem defined as follows:</p><blockquote><p>Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and it credited \$10 by the national company. If he out of cats at that location, then the business is lost. Cars become available for renting the day after they are returned. To ensure that cars are available where they are needed, Jack ca move them between the two locations overnight, at a cost of \$2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is $n$ is $\frac{\lambda^{n}}{n!}e^{-\lambda}$, where $\lambda$ is the excepted number. Suppose $\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\lambda=0.9$ and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight.</p></blockquote><p>The excepted result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental.png" alt="car_rental"></p><p><em>Figure 1</em></p><p>The first, we define some facts of this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># maximum # of cars in each location</span></div><div class="line">MAX_CARS = <span class="number">20</span></div><div class="line"><span class="comment"># maximum # of cars to move during night</span></div><div class="line">MAX_MOVE_OF_CARS = <span class="number">5</span></div><div class="line"><span class="comment"># expectation for rental requests in first location</span></div><div class="line">RENTAL_REQUEST_FIRST_LOC = <span class="number">3</span></div><div class="line"><span class="comment"># expectation for rental requests in second location</span></div><div class="line">RENTAL_REQUEST_SECOND_LOC = <span class="number">4</span></div><div class="line"><span class="comment"># expectation for # of cars returned in first location</span></div><div class="line">RETURNS_FIRST_LOC = <span class="number">3</span></div><div class="line"><span class="comment"># expectation for # of cars returned in second location</span></div><div class="line">RETURNS_SECOND_LOC = <span class="number">2</span></div><div class="line">DISCOUNT = <span class="number">0.9</span></div><div class="line"><span class="comment"># credit earned by a car</span></div><div class="line">RENTAL_CREDIT = <span class="number">10</span></div><div class="line"><span class="comment"># cost of moving a car</span></div><div class="line">MOVE_CAR_COST = <span class="number">2</span></div></pre></td></tr></table></figure><p>From the problem definition, we know that in this MDP the states is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight. Each action is a integer that positive number represents the number of cars moving from the first location to second location and vice verse.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># current policy</span></div><div class="line">policy = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line"><span class="comment"># current state value</span></div><div class="line">stateValue = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line"><span class="comment"># all possible states</span></div><div class="line">states = []</div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">actions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + <span class="number">1</span>)</div></pre></td></tr></table></figure><p>For visualization (Figure 1) convenient, we define a method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># axes for printing use</span></div><div class="line">AxisXPrint = []</div><div class="line">AxisYPrint = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, MAX_CARS + <span class="number">1</span>):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, MAX_CARS + <span class="number">1</span>):</div><div class="line">        AxisXPrint.append(i)</div><div class="line">        AxisYPrint.append(j)</div><div class="line">        states.append([i, j])</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># plot a policy/state value matrix</span></div><div class="line">figureIndex = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span><span class="params">(data, labels)</span>:</span></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    fig = plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</div><div class="line">    AxisZ = []</div><div class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">        AxisZ.append(data[i, j])</div><div class="line">    ax.scatter(AxisXPrint, AxisYPrint, AxisZ)</div><div class="line">    ax.set_xlabel(labels[<span class="number">0</span>])</div><div class="line">    ax.set_ylabel(labels[<span class="number">1</span>])</div><div class="line">    ax.set_zlabel(labels[<span class="number">2</span>])</div></pre></td></tr></table></figure><p>Next, we define a Poisson function that return the probability:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># An up bound for poisson distribution</span></div><div class="line"><span class="comment"># If n is greater than this value, then the probability of getting n is truncated to 0</span></div><div class="line">POISSON_UP_BOUND = <span class="number">11</span></div><div class="line"></div><div class="line"><span class="comment"># Probability for poisson distribution</span></div><div class="line"><span class="comment"># @lam: lambda should be less than 10 for this function</span></div><div class="line">poissonBackup = dict()</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">poisson</span><span class="params">(n, lam)</span>:</span></div><div class="line">    <span class="keyword">global</span> poissonBackup</div><div class="line">    key = n * <span class="number">10</span> + lam</div><div class="line">    <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> poissonBackup.keys():</div><div class="line">        poissonBackup[key] = exp(-lam) * pow(lam, n) / factorial(n)</div><div class="line">    <span class="keyword">return</span> poissonBackup[key]</div></pre></td></tr></table></figure><p>Now, the preparation is done. We’ll implement the policy iteration algorithm as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">newStateValue = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line">improvePolicy = <span class="keyword">False</span></div><div class="line">policyImprovementInd = <span class="number">0</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">if</span> improvePolicy == <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># start policy improvement</span></div><div class="line">        print(<span class="string">'Policy improvement'</span>, policyImprovementInd)</div><div class="line">        policyImprovementInd += <span class="number">1</span></div><div class="line">        newPolicy = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line">        <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">            actionReturns = []</div><div class="line">            <span class="comment"># go through all actions and select the best one</span></div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                <span class="keyword">if</span> (action &gt;= <span class="number">0</span> <span class="keyword">and</span> i &gt;= action) <span class="keyword">or</span> (action &lt; <span class="number">0</span> <span class="keyword">and</span> j &gt;= abs(action)):</div><div class="line">                    actionReturns.append(expectedReturn([i, j], action, stateValue))</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    actionReturns.append(-float(<span class="string">'inf'</span>))</div><div class="line">            bestAction = argmax(actionReturns)</div><div class="line">            newPolicy[i, j] = actions[bestAction]</div><div class="line"></div><div class="line">        <span class="comment"># if policy is stable</span></div><div class="line">        policyChanges = np.sum(newPolicy != policy)</div><div class="line">        print(<span class="string">'Policy for'</span>, policyChanges, <span class="string">'states changed'</span>)</div><div class="line">        <span class="keyword">if</span> policyChanges == <span class="number">0</span>:</div><div class="line">            policy = newPolicy</div><div class="line">            <span class="keyword">break</span></div><div class="line">        policy = newPolicy</div><div class="line">        improvePolicy = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="comment"># start policy evaluation</span></div><div class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">        newStateValue[i, j] = expectedReturn([i, j], policy[i, j], stateValue)</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(newStateValue - stateValue)) &lt; <span class="number">1e-4</span>:</div><div class="line">        stateValue[:] = newStateValue</div><div class="line">        improvePolicy = <span class="keyword">True</span></div><div class="line">        <span class="keyword">continue</span></div><div class="line">    stateValue[:] = newStateValue</div></pre></td></tr></table></figure><p>We can see the logistic is the same as the pseudocode of the policy iteration algorithm. There is a core method in the code, that is, <strong>exceptedReturn()</strong> is used to calculate the reward of cars rental.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># @state: [# of cars in first location, # of cars in second location]</span></div><div class="line"><span class="comment"># @action: positive if moving cars from first location to second location,</span></div><div class="line"><span class="comment">#          negative if moving cars from second location to first location</span></div><div class="line"><span class="comment"># @stateValue: state value matrix</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">expectedReturn</span><span class="params">(state, action, stateValue)</span>:</span></div><div class="line">    <span class="comment"># initailize total return</span></div><div class="line">    returns = <span class="number">0.0</span></div><div class="line"></div><div class="line">    <span class="comment"># cost for moving cars</span></div><div class="line">    returns -= MOVE_CAR_COST * abs(action)</div><div class="line"></div><div class="line">    <span class="comment"># go through all possible rental requests</span></div><div class="line">    <span class="keyword">for</span> rentalRequestFirstLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">        <span class="keyword">for</span> rentalRequestSecondLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">            <span class="comment"># moving cars</span></div><div class="line">            numOfCarsFirstLoc = int(min(state[<span class="number">0</span>] - action, MAX_CARS))</div><div class="line">            numOfCarsSecondLoc = int(min(state[<span class="number">1</span>] + action, MAX_CARS))</div><div class="line"></div><div class="line">            <span class="comment"># valid rental requests should be less than actual # of cars</span></div><div class="line">            realRentalFirstLoc = min(numOfCarsFirstLoc, rentalRequestFirstLoc)</div><div class="line">            realRentalSecondLoc = min(numOfCarsSecondLoc, rentalRequestSecondLoc)</div><div class="line"></div><div class="line">            <span class="comment"># get credits for renting</span></div><div class="line">            reward = (realRentalFirstLoc + realRentalSecondLoc) * RENTAL_CREDIT</div><div class="line">            numOfCarsFirstLoc -= realRentalFirstLoc</div><div class="line">            numOfCarsSecondLoc -= realRentalSecondLoc</div><div class="line"></div><div class="line">            <span class="comment"># probability for current combination of rental requests</span></div><div class="line">            prob = poisson(rentalRequestFirstLoc, RENTAL_REQUEST_FIRST_LOC) * \</div><div class="line">                         poisson(rentalRequestSecondLoc, RENTAL_REQUEST_SECOND_LOC)</div><div class="line"></div><div class="line">            <span class="comment"># if set True, model is simplified such that the # of cars returned in daytime becomes constant</span></div><div class="line">            <span class="comment"># rather than a random value from poisson distribution, which will reduce calculation time</span></div><div class="line">            <span class="comment"># and leave the optimal policy/value state matrix almost the same</span></div><div class="line">            constantReturnedCars = <span class="keyword">True</span></div><div class="line">            <span class="keyword">if</span> constantReturnedCars:</div><div class="line">                <span class="comment"># get returned cars, those cars can be used for renting tomorrow</span></div><div class="line">                returnedCarsFirstLoc = RETURNS_FIRST_LOC</div><div class="line">                returnedCarsSecondLoc = RETURNS_SECOND_LOC</div><div class="line">                numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS)</div><div class="line">                numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS)</div><div class="line">                returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc])</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                numOfCarsFirstLoc_ = numOfCarsFirstLoc</div><div class="line">                numOfCarsSecondLoc_ = numOfCarsSecondLoc</div><div class="line">                prob_ = prob</div><div class="line">                <span class="keyword">for</span> returnedCarsFirstLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">                    <span class="keyword">for</span> returnedCarsSecondLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">                        numOfCarsFirstLoc = numOfCarsFirstLoc_</div><div class="line">                        numOfCarsSecondLoc = numOfCarsSecondLoc_</div><div class="line">                        prob = prob_</div><div class="line">                        numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS)</div><div class="line">                        numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS)</div><div class="line">                        prob = poisson(returnedCarsFirstLoc, RETURNS_FIRST_LOC) * \</div><div class="line">                               poisson(returnedCarsSecondLoc, RETURNS_SECOND_LOC) * prob</div><div class="line">                        returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc])</div><div class="line">    <span class="keyword">return</span> returns</div></pre></td></tr></table></figure><p>The comments are very clear, and we’re going to do a lot of this. Finally, let us print the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">prettyPrint(policy, [<span class="string">'# of cars in first location'</span>, <span class="string">'# of cars in second location'</span>, <span class="string">'# of cars to move during night'</span>])</div><div class="line">prettyPrint(stateValue, [<span class="string">'# of cars in first location'</span>, <span class="string">'# of cars in second location'</span>, <span class="string">'expected returns'</span>])</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>The results are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Policy improvement <span class="number">0</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">332</span> states changed</div><div class="line">Policy improvement <span class="number">1</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">286</span> states changed</div><div class="line">Policy improvement <span class="number">2</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">83</span> states changed</div><div class="line">Policy improvement <span class="number">3</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">19</span> states changed</div><div class="line">Policy improvement <span class="number">4</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">0</span> states changed</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_policy.png" alt="car_rental_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_return.png" alt="car_rental_return"></p><p>One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to $v_{\pi}$ occurs only in the limit. In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This algorithm is called <em>value iteration</em>. It can be written as a particular simple backup operation that combines the policy improvement and truncated policy evaluation steps:<br>$$<br>\begin{align}<br>v_{k+1} &amp;\doteq \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{k}(s^{\prime})],<br>\end{align}<br>$$<br>for all $s \in \mathcal{S}$.</p><blockquote><p><strong>Value iteration</strong></p><p>Initialize array $V$ arbitrarily (e.g. $V(s) = 0$ for all $s \in \mathcal{S^{+}}$)</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ For each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p><p>Output a deterministic policy, $\pi \approx \pi_{\star}$, such that</p><p>​ $\pi(s) = \arg\max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$</p></blockquote><p>Let us use the value iteration algorithm to solve a Gambler’s Problem. The problem defined as follows:</p><blockquote><p>A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he staked on the flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of \$100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer number of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gambler’s capital, $s \in \{1, 2, \cdots, 99\}$ and the actions are stakes, $a \in \{0, 1, \cdots, \min(s, 100-s)\}$. The reward is zero on all transitions excepted those on which the gambler reaches his goal, when it is +1. The state-value function then gives the probability of winning from each state. A policy is mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal. Let $p_h$ denote the probability of the coin coming up heads. If $p_h$ is known, then the entire problem is known and it can be solved, for instance, by value iteration.</p></blockquote><p>OK, now let us to solve this problem by use the value iteration algorithm.</p><p>The first we defined some facts and some auxiliary data structure:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># goal</span></div><div class="line">GOAL = <span class="number">100</span></div><div class="line"><span class="comment"># all states, including state 0 and state 100</span></div><div class="line">states = np.arange(GOAL + <span class="number">1</span>)</div><div class="line"><span class="comment"># probability of head</span></div><div class="line">headProb = <span class="number">0.4</span></div><div class="line"><span class="comment"># optimal policy</span></div><div class="line">policy = np.zeros(GOAL + <span class="number">1</span>)</div><div class="line"><span class="comment"># state value</span></div><div class="line">stateValue = np.zeros(GOAL + <span class="number">1</span>)</div><div class="line">stateValue[GOAL] = <span class="number">1.0</span></div></pre></td></tr></table></figure><p>The step of value iteration:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># value iteration</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    delta = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states[<span class="number">1</span>:GOAL]:</div><div class="line">        <span class="comment"># get possilbe actions for current state</span></div><div class="line">        actions = np.arange(min(state, GOAL - state) + <span class="number">1</span>)</div><div class="line">        actionReturns = []</div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">            actionReturns.append(headProb * stateValue[state + action] + (<span class="number">1</span> - headProb) * stateValue[state - action])</div><div class="line">        newValue = np.max(actionReturns)</div><div class="line">        delta += np.abs(stateValue[state] - newValue)</div><div class="line">        <span class="comment"># update state value</span></div><div class="line">        stateValue[state] = newValue</div><div class="line">    <span class="keyword">if</span> delta &lt; <span class="number">1e-9</span>:</div><div class="line">        <span class="keyword">break</span></div></pre></td></tr></table></figure><p>Calculate the optimal policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># calculate the optimal policy</span></div><div class="line"><span class="keyword">for</span> state <span class="keyword">in</span> states[<span class="number">1</span>:GOAL]:</div><div class="line">    actions = np.arange(min(state, GOAL - state) + <span class="number">1</span>)</div><div class="line">    actionReturns = []</div><div class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">        actionReturns.append(headProb * stateValue[state + action] + (<span class="number">1</span> - headProb) * stateValue[state - action])</div><div class="line">    <span class="comment"># due to tie and precision, can't reproduce the optimal policy in book</span></div><div class="line">    policy[state] = actions[argmax(actionReturns)]</div></pre></td></tr></table></figure><p>Print the results:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">plt.figure(<span class="number">1</span>)</div><div class="line">plt.xlabel(<span class="string">'Capital'</span>)</div><div class="line">plt.ylabel(<span class="string">'Value estimates'</span>)</div><div class="line">plt.plot(stateValue)</div><div class="line">plt.figure(<span class="number">2</span>)</div><div class="line">plt.scatter(states, policy)</div><div class="line">plt.xlabel(<span class="string">'Capital'</span>)</div><div class="line">plt.ylabel(<span class="string">'Final policy (stake)'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_policy.png" alt="gambler_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_value.png" alt="gambler_value"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/10/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/12/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">119</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">58</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>