<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/12/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/12/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/" itemprop="url">Some Paper Summaries of Semantic Segmentation with Deep Learning</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-10T20:01:58+08:00">2018-04-10 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/" class="leancloud_visitors" data-flag-title="Some Paper Summaries of Semantic Segmentation with Deep Learning"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="What-exactly-is-semantic-segmentation"><a href="#What-exactly-is-semantic-segmentation" class="headerlink" title="What exactly is semantic segmentation?"></a>What exactly is semantic segmentation?</h3><p>Semantic segmentation is understanding an image at pixel level i.e, we want to assign each pixel in the image an object class. For example, check out the following images.</p><p><img src="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21.jpg" alt="biker"> <img src="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21_class.png" alt="biker"><br><em>Left</em>: Input image. <em>Right</em>: It’s semantic segmentation. <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html" target="_blank" rel="external">Source.</a></p><p>Apart from recognizing the bike and the person riding it, we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models.</p><p><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" target="_blank" rel="external">VOC2012</a> and <a href="http://mscoco.org/explore/" target="_blank" rel="external">MSCOCO</a> are the most important datasets for semantic segmentation.</p><h3 id="What-are-the-different-approaches"><a href="#What-are-the-different-approaches" class="headerlink" title="What are the different approaches?"></a>What are the different approaches?</h3><p>Before deep learning took over computer vision, people used approaches like <a href="http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2008-CVPR-semantic-texton-forests.pdf" target="_blank" rel="external">TextonForest</a> and <a href="http://www.cse.chalmers.se/edu/year/2011/course/TDA361/Advanced%20Computer%20Graphics/BodyPartRecognition.pdf" target="_blank" rel="external">Random Forest based classifiers</a> for semantic segmentation. As with image classification, convolutional neural networks (CNN) have had enormous success on segmentation problems.</p><p>One of the popular initial deep learning approaches was <a href="http://people.idsia.ch/~juergen/nips2012.pdf" target="_blank" rel="external">patch classification</a> where each pixel was separately classified into classes using a patch of image around it. Main reason to use patches was that classification networks usually have full connected layers and therefore required fixed size images.</p><p>In 2014, <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#fcn" target="_blank" rel="external">Fully Convolutional Networks (FCN)</a> by Long et al. from Berkeley, popularized CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for image of any size and was also much faster compared to the patch classification approach. Almost all the subsequent state of the art approaches on semantic segmentation adopted this paradigm.</p><p>Apart from fully connected layers, one of the main problems with using CNNs for segmentation is <em>pooling layers</em>. Pooling layers increase the field of view and are able to aggregate the context while discarding the ‘where’ information. However, semantic segmentation requires the exact alignment of class maps and thus, needs the ‘where’ information to be preserved. Two different classes of architectures evolved in the literature to tackle this issue.</p><p>First one is encoder-decoder architecture. Encoder gradually reduces the spatial dimension with pooling layers and decoder gradually recovers the object details and spatial dimension. There are usually shortcut connections from encoder to decoder to help decoder recover the object details better. <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="external">U-Net</a> is a popular architecture from this class.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/unet.png" alt="U-Net architecture"><br>U-Net: An encoder-decoder architecture. <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="external">Source</a>.</p><p>Architectures in the second class use what are called as <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated/atrous convolutions</a>and do away with pooling layers.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/dilated_conv.png" alt="Dilated/atrous convolutions"><br>Dilated/atrous convolutions. rate=1 is same as normal convolutions. <a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="external">Source</a>.</p><p><a href="https://arxiv.org/abs/1210.5644" target="_blank" rel="external">Conditional Random Field (CRF) postprocessing</a> are usually used to improve the segmentation. CRFs are graphical models which ‘smooth’ segmentation based on the underlying image intensities. They work based on the observation that similar intensity pixels tend to be labeled as the same class. CRFs can boost scores by 1-2%.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/crf.png" alt="CRF"><br>CRF illustration. (b) Unary classifiers is the segmentation input to the CRF. (c, d, e) are variants of CRF with (e) being the widely used one. <a href="https://arxiv.org/abs/1210.5644" target="_blank" rel="external">Source</a>.</p><p>In the next section, I’ll summarize a few papers that represent the evolution of segmentation architectures starting from FCN. All these architectures are benchmarked on <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php" target="_blank" rel="external">VOC2012 evaluation server</a>.</p><h3 id="Summaries"><a href="#Summaries" class="headerlink" title="Summaries"></a>Summaries</h3><p>Following papers are summarized (in chronological order):</p><ol><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#fcn" target="_blank" rel="external">FCN</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#segnet" target="_blank" rel="external">SegNet</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">Dilated Convolutions</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLab (v1 &amp; v2)</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#refinenet" target="_blank" rel="external">RefineNet</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#pspnet" target="_blank" rel="external">PSPNet</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#large-kernel" target="_blank" rel="external">Large Kernel Matters</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplabv3" target="_blank" rel="external">DeepLab v3</a></li></ol><p>For each of these papers, I list down their key contributions and explain them. I also show their benchmark scores (mean IOU) on VOC2012 test dataset.</p><h4 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h4><ul><li>Fully Convolutional Networks for Semantic Segmentation</li><li>Submitted on 14 Nov 2014</li><li><a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Popularize the use of end to end convolutional networks for semantic segmentation</li><li>Re-purpose imagenet pretrained networks for segmentation</li><li>Upsample using <em>deconvolutional</em> layers</li><li>Introduce skip connections to improve over the coarseness of upsampling</li></ul><p><em>Explanation</em>:</p><p>Key observation is that fully connected layers in classification networks can be viewed as convolutions with kernels that cover their entire input regions. This is equivalent to evaluating the original classification network on overlapping input patches but is much more efficient because computation is shared over the overlapping regions of patches. Although this observation is not unique to this paper (see <a href="https://arxiv.org/abs/1312.6229" target="_blank" rel="external">overfeat</a>, <a href="https://plus.google.com/+PierreSermanet/posts/VngsFR3tug9" target="_blank" rel="external">this post</a>), it improved the state of the art on VOC2012 significantly.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/FCN%20-%20illustration.png" alt="FCN architecture"><br>Fully connected layers as a convolution. <a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="external">Source</a>.</p><p>After convolutionalizing fully connected layers in a imagenet pretrained network like VGG, feature maps still need to be upsampled because of pooling operations in CNNs. Instead of using simple bilinear interpolation, <em>deconvolutional layers</em> can learn the interpolation. This layer is also known as upconvolution, full convolution, transposed convolution or fractionally-strided convolution.</p><p>However, upsampling (even with deconvolutional layers) produces coarse segmentation maps because of loss of information during pooling. Therefore, shortcut/skip connections are introduced from higher resolution feature maps.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>62.2</td><td>-</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6&amp;submid=6103#KEY_FCN-8s" target="_blank" rel="external">leaderboard</a></td></tr><tr><td>67.2</td><td>More momentum. Not described in paper</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6&amp;submid=6103#KEY_FCN-8s-heavy" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><p><em>My Comments</em>:</p><ul><li>This was an important contribution but state of the art has improved a lot by now though.</li></ul><h4 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h4><ul><li>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</li><li>Submitted on 2 Nov 2015</li><li><a href="https://arxiv.org/abs/1511.00561" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Maxpooling indices transferred to decoder to improve the segmentation resolution.</li></ul><p><em>Explanation</em>:</p><p>FCN, despite upconvolutional layers and a few shortcut connections produces coarse segmentation maps. Therefore, more shortcut connections are introduced. However, instead of copying the encoder features as in FCN, indices from maxpooling are copied. This makes SegNet more memory efficient than FCN.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/segnet_architecture.png" alt="SegNet Architecture"><br>Segnet Architecture. <a href="https://arxiv.org/abs/1511.00561" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>59.9</td><td>-</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_SegNet" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><p><em>My comments</em>:</p><ul><li>FCN and SegNet are one of the first encoder-decoder architectures.</li><li>Benchmarks for SegNet are not good enough to be used anymore.</li></ul><h4 id="Dilated-Convolutions"><a href="#Dilated-Convolutions" class="headerlink" title="Dilated Convolutions"></a>Dilated Convolutions</h4><ul><li>Multi-Scale Context Aggregation by Dilated Convolutions</li><li>Submitted on 23 Nov 2015</li><li><a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Use dilated convolutions, a convolutional layer for dense predictions.</li><li>Propose ‘context module’ which uses dilated convolutions for multi scale aggregation.</li></ul><p><em>Explanation</em>:</p><p>Pooling helps in classification networks because receptive field increases. But this is not the best thing to do for segmentation because pooling decreases the resolution. Therefore, authors use <em>dilated convolution</em> layer which works like this:</p><p><img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif" alt="Dilated/Atrous Convolutions"><br>Dilated/Atrous Convolutions. <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="external">Source</a></p><p>Dilated convolutional layer (also called as atrous convolution in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLab</a>) allows for exponential increase in field of view without decrease of spatial dimensions.</p><p>Last two pooling layers from pretrained classification network (here, VGG) are removed and subsequent convolutional layers are replaced with dilated convolutions. In particular, convolutions between the pool-3 and pool-4 have dilation 2 and convolutions after pool-4 have dilation 4. With this module (called <em>frontend module</em> in the paper), dense predictions are obtained without any increase in number of parameters.</p><p>A module (called <em>context module</em> in the paper) is trained separately with the outputs of frontend module as inputs. This module is a cascade of dilated convolutions of different dilations so that multi scale context is aggregated and predictions from frontend are improved.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>71.3</td><td>frontend</td><td>reported in the paper</td></tr><tr><td>73.5</td><td>frontend + context</td><td>reported in the paper</td></tr><tr><td>74.7</td><td>frontend + context + CRF</td><td>reported in the paper</td></tr><tr><td>75.3</td><td>frontend + context + CRF-RNN</td><td>reported in the paper</td></tr></tbody></table><p><em>My comments</em>:</p><ul><li>Note that predicted segmentation map’s size is 1/8th of that of the image. This is the case with almost all the approaches. They are interpolated to get the final segmentation map.</li></ul><h4 id="DeepLab-v1-amp-v2"><a href="#DeepLab-v1-amp-v2" class="headerlink" title="DeepLab (v1 &amp; v2)"></a>DeepLab (v1 &amp; v2)</h4><ul><li><strong>v1</strong> : Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</li><li>Submitted on 22 Dec 2014</li><li><a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="external">Arxiv Link</a></li><li></li><li><strong>v2</strong> : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</li><li>Submitted on 2 Jun 2016</li><li><a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Use atrous/dilated convolutions.</li><li>Propose atrous spatial pyramid pooling (ASPP)</li><li>Use Fully connected CRF</li></ul><p><em>Explanation</em>:</p><p>Atrous/Dilated convolutions increase the field of view without increasing the number of parameters. Net is modified like in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions paper</a>.</p><p>Multiscale processing is achieved either by passing multiple rescaled versions of original images to parallel CNN branches (Image pyramid) and/or by using multiple parallel atrous convolutional layers with different sampling rates (ASPP).</p><p>Structured prediction is done by fully connected CRF. CRF is trained/tuned separately as a post processing step.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/deeplabv2.png" alt="DeepLab2 Pipeline"><br>DeepLab2 Pipeline. <a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>79.7</td><td>ResNet-101 + atrous Convolutions + ASPP + CRF</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6&amp;submid=6103#KEY_DeepLabv2-CRF" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><h4 id="RefineNet"><a href="#RefineNet" class="headerlink" title="RefineNet"></a>RefineNet</h4><ul><li>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</li><li>Submitted on 20 Nov 2016</li><li><a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Encoder-Decoder architecture with well thought-out decoder blocks</li><li>All the components follow residual connection design</li></ul><p><em>Explanation</em>:</p><p>Approach of using dilated/atrous convolutions are not without downsides. Dilated convolutions are computationally expensive and take a lot of memory because they have to be applied on large number of high resolution feature maps. This hampers the computation of high-res predictions. <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLab’s</a> predictions, for example are 1/8th the size of original input.</p><p>So, the paper proposes to use encoder-decoder architecture. Encoder part is ResNet-101 blocks. Decoder has RefineNet blocks which concatenate/fuse high resolution features from encoder and low resolution features from previous RefineNet block.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/refinenet%20-%20architecture.png" alt="RefineNet Architecture"><br>RefineNet Architecture. <a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">Source</a>.</p><p>Each RefineNet block has a component to fuse the multi resolution features by upsampling the lower resolution features and a component to capture context based on repeated 5 x 5 <em>stride 1</em> pool layers. Each of these components employ the residual connection design following the identity map mindset.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/refinenet%20-%20block.png" alt="RefineNet Block"><br>RefineNet Block. <a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>84.2</td><td>Uses CRF, Multiscale inputs, COCO pretraining</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_Multipath-RefineNet" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><h4 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h4><ul><li>Pyramid Scene Parsing Network</li><li>Submitted on 4 Dec 2016</li><li><a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Propose pyramid pooling module to aggregate the context.</li><li>Use auxiliary loss</li></ul><p><em>Explanation</em>:</p><p>Global scene categories matter because it provides clues on the distribution of the segmentation classes. Pyramid pooling module captures this information by applying large kernel pooling layers.</p><p>Dilated convolutions are used as in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions paper</a> to modify Resnet and a pyramid pooling module is added to it. This module concatenates the feature maps from ResNet with upsampled output of parallel pooling layers with kernels covering whole, half of and small portions of image.</p><p>An auxiliary loss, additional to the loss on main branch, is applied after the fourth stage of ResNet (i.e input to pyramid pooling module). This idea was also called as intermediate supervision elsewhere.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/pspnet.png" alt="PSPNet Architecture"><br>PSPNet Architecture. <a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>85.4</td><td>MSCOCO pretraining, multi scale input, no CRF</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_PSPNet" target="_blank" rel="external">leaderboard</a></td></tr><tr><td>82.6</td><td>no MSCOCO pretraining, multi scale input, no CRF</td><td>reported in the paper</td></tr></tbody></table><h4 id="Large-Kernel-Matters"><a href="#Large-Kernel-Matters" class="headerlink" title="Large Kernel Matters"></a>Large Kernel Matters</h4><ul><li>Large Kernel Matters – Improve Semantic Segmentation by Global Convolutional Network</li><li>Submitted on 8 Mar 2017</li><li><a href="https://arxiv.org/abs/1703.02719" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Propose a encoder-decoder architecture with very large kernels convolutions</li></ul><p><em>Explanation</em>:</p><p>Semantic segmentation requires both segmentation and classification of the segmented objects. Since fully connected layers cannot be present in a segmentation architecture, convolutions with very large kernels are adopted instead.</p><p>Another reason to adopt large kernels is that although deeper networks like ResNet have very large receptive field, <a href="https://arxiv.org/abs/1412.6856" target="_blank" rel="external">studies</a> show that the network tends to gather information from a much smaller region (valid receptive filed).</p><p>Larger kernels are computationally expensive and have a lot of parameters. Therefore, k x k convolution is approximated with sum of 1 x k + k x 1 and k x 1 and 1 x k convolutions. This module is called as <em>Global Convolutional Network</em> (GCN) in the paper.</p><p>Coming to architecture, ResNet(without any dilated convolutions) forms encoder part of the architecture while GCNs and deconvolutions form decoder. A simple residual block called <em>Boundary Refinement</em> (BR) is also used.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/large_kernel_matter.png" alt="GCN Architecture"><br>GCN Architecture. <a href="https://arxiv.org/abs/1703.02719" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>82.2</td><td>-</td><td>reported in the paper</td></tr><tr><td>83.6</td><td>Improved training, not described in the paper</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_Large_Kernel_Matters" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><h4 id="DeepLab-v3"><a href="#DeepLab-v3" class="headerlink" title="DeepLab v3"></a>DeepLab v3</h4><ul><li>Rethinking Atrous Convolution for Semantic Image Segmentation</li><li>Submitted on 17 Jun 2017</li><li><a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Improved atrous spatial pyramid pooling (ASPP)</li><li>Module which employ atrous convolutions in cascade</li></ul><p><em>Explanation</em>:</p><p>ResNet model is modified to use dilated/atrous convolutions as in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLabv2</a> and <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions</a>. Improved ASPP involves concatenation of image-level features, a 1x1 convolution and three 3x3 atrous convolutions with different rates. Batch normalization is used after each of the parallel convolutional layers.</p><p>Cascaded module is a resnet block except that component convolution layers are made atrous with different rates. This module is similar to context module used in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions paper</a> but this is applied directly on intermediate feature maps instead of belief maps (belief maps are final CNN feature maps with channels equal to number of classes).</p><p>Both the proposed models are evaluated independently and attempt to combine the both did not improve the performance. Both of them performed very similarly on val set with ASPP performing slightly better. CRF is not used.</p><p>Both these models outperform the best model from <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLabv2</a>. Authors note that the improvement comes from the batch normalization and better way to encode multi scale context.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/deeplabv3.png" alt="DeepLabv3 ASPP"><br>DeepLabv3 ASPP (used for submission). <a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>85.7</td><td>used ASPP (no cascaded modules)</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_DeepLabv3" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><p>Reblog from <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#sec-2" target="_blank" rel="external">here</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/11/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/118/">118</a><a class="extend next" rel="next" href="/page/13/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">118</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">58</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>