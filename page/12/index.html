<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/12/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/12/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/07/Demystifying-Deep-Reinforcement-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/07/Demystifying-Deep-Reinforcement-Learning/" itemprop="url">Demystifying Deep Reinforcement Learning (Repost)</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-07T01:36:44+08:00">2017-07-07 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/07/Demystifying-Deep-Reinforcement-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/07/Demystifying-Deep-Reinforcement-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/07/Demystifying-Deep-Reinforcement-Learning/" class="leancloud_visitors" data-flag-title="Demystifying Deep Reinforcement Learning (Repost)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Two years ago, a small company in London called DeepMind uploaded their pioneering paper “<a href="http://arxiv.org/abs/1312.5602" target="_blank" rel="external">Playing Atari with Deep Reinforcement Learning</a>” to Arxiv. In this paper they demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable, because the games and the goals in every game were very different and designed to be challenging for humans. The same model architecture, without any change, was used to learn seven different games, and in three of them the algorithm performed even better than a human!</p><p>It has been hailed since then as the first step towards <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank" rel="external">general artificial intelligence</a> – an AI that can survive in a variety of environments, instead of being confined to strict realms such as playing chess. No wonder <a href="http://techcrunch.com/2014/01/26/google-deepmind/" target="_blank" rel="external">DeepMind was immediately bought by Google</a> and has been on the forefront of deep learning research ever since. In February 2015 their paper “<a href="http://www.nature.com/articles/nature14236" target="_blank" rel="external">Human-level control through deep reinforcement learning</a>” was featured on the cover of Nature, one of the most prestigious journals in science. In this paper they applied the same model to 49 different games and achieved superhuman performance in half of them.</p><p>Still, while deep models for supervised and unsupervised learning have seen widespread adoption in the community, deep reinforcement learning has remained a bit of a mystery. In this blog post I will be trying to demystify this technique and understand the rationale behind it. The intended audience is someone who already has background in machine learning and possibly in neural networks, but hasn’t had time to delve into reinforcement learning yet.</p><p>The roadmap ahead:</p><ol><li><strong>What are the main challenges in reinforcement learning?</strong> We will cover the credit assignment problem and the exploration-exploitation dilemma here.</li><li><strong>How to formalize reinforcement learning in mathematical terms?</strong> We will define Markov Decision Process and use it for reasoning about reinforcement learning.</li><li><strong>How do we form long-term strategies?</strong> We define “discounted future reward”, that forms the main basis for the algorithms in the next sections.</li><li><strong>How can we estimate or approximate the future reward?</strong> Simple table-based Q-learning algorithm is defined and explained here.</li><li><strong>What if our state space is too big?</strong> Here we see how Q-table can be replaced with a (deep) neural network.</li><li><strong>What do we need to make it actually work?</strong> Experience replay technique will be discussed here, that stabilizes the learning with neural networks.</li><li><strong>Are we done yet?</strong> Finally we will consider some simple solutions to the exploration-exploitation problem.</li></ol><h1 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h1><p>Consider the game Breakout. In this game you control a paddle at the bottom of the screen and have to bounce the ball back to clear all the bricks in the upper half of the screen. Each time you hit a brick, it disappears and your score increases – you get a reward.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.08.53-AM.png" alt="img">Figure 1: Atari Breakout game. Image credit: DeepMind.</p><p>Suppose you want to teach a neural network to play this game. Input to your network would be screen images, and output would be three actions: left, right or fire (to launch the ball). It would make sense to treat it as a classification problem – for each game screen you have to decide, whether you should move left, right or press fire. Sounds straightforward? Sure, but then you need training examples, and a lots of them. Of course you could go and record game sessions using expert players, but that’s not really how we learn. We don’t need somebody to tell us a million times which move to choose at each screen. We just need occasional feedback that we did the right thing and can then figure out everything else ourselves.</p><p>This is the task <strong>reinforcement learning </strong>tries to solve. Reinforcement learning lies somewhere in between supervised and unsupervised learning. Whereas in supervised learning one has a target label for each training example and in unsupervised learning one has no labels at all, in reinforcement learning one has sparse and time-delayed labels – the rewards. Based only on those rewards the agent has to learn to behave in the environment.</p><p>While the idea is quite intuitive, in practice there are numerous challenges. For example when you hit a brick and score a reward in the Breakout game, it often has nothing to do with the actions (paddle movements) you did just before getting the reward. All the hard work was already done, when you positioned the paddle correctly and bounced the ball back. This is called the <strong>credit assignment problem</strong> – i.e., which of the preceding actions was responsible for getting the reward and to what extent.</p><p>Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards? In the above Breakout game a simple strategy is to move to the left edge and wait there. When launched, the ball tends to fly left more often than right and you will easily score about 10 points before you die. Will you be satisfied with this or do you want more? This is called the <strong>explore-exploit dilemma</strong> – should you exploit the known working strategy or explore other, possibly better strategies.</p><p>Reinforcement learning is an important model of how we (and all animals in general) learn. Praise from our parents, grades in school, salary at work – these are all examples of rewards. Credit assignment problems and exploration-exploitation dilemmas come up every day both in business and in relationships. That’s why it is important to study this problem, and games form a wonderful sandbox for trying out new approaches.</p><h1 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h1><p>Now the question is, how do you formalize a reinforcement learning problem, so that you can reason about it? The most common method is to represent it as a Markov decision process.</p><p>Suppose you are an <strong>agent</strong>, situated in an <strong>environment</strong> (e.g. Breakout game). The environment is in a certain <strong>state</strong>(e.g. location of the paddle, location and direction of the ball, existence of every brick and so on). The agent can perform certain <strong>actions</strong> in the environment (e.g. move the paddle to the left or to the right). These actions sometimes result in a <strong>reward</strong> (e.g. increase in score). Actions transform the environment and lead to a new state, where the agent can perform another action, and so on. The rules for how you choose those actions are called <strong>policy</strong>. The environment in general is stochastic, which means the next state may be somewhat random (e.g. when you lose a ball and launch a new one, it goes towards a random direction).</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-12.01.04-PM.png" alt="img">Figure 2: <em>Left: </em>reinforcement learning problem. <em>Right: </em>Markov decision process.</p><p>The set of states and actions, together with rules for transitioning from one state to another, make up a <strong>Markov decision process</strong>. One <strong>episode</strong> of this process (e.g. one game) forms a finite sequence of states, actions and rewards:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.19-AM.png" alt="Screen Shot 2015-12-21 at 11.09.19 AM"></p><p>Here si represents the state, ai is the action and ri+1 is the reward after performing the action. The episode ends with <strong>terminal</strong> state sn (e.g. “game over” screen). A Markov decision process relies on the Markov assumption, that the probability of the next state si+1 depends only on current state si and action ai, but not on preceding states or actions.</p><h1 id="Discounted-Future-Reward"><a href="#Discounted-Future-Reward" class="headerlink" title="Discounted Future Reward"></a>Discounted Future Reward</h1><p>To perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. How should we go about that?</p><p>Given one run of the Markov decision process, we can easily calculate the <strong>total reward</strong> for one episode:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.26-AM.png" alt="Screen Shot 2015-12-21 at 11.09.26 AM"></p><p>Given that, the <strong>total future reward</strong> from time point <em>t</em> onward can be expressed as:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.32-AM.png" alt="Screen Shot 2015-12-21 at 11.09.32 AM"></p><p>But because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use <strong>discounted future reward </strong>instead:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.36-AM.png" alt="Screen Shot 2015-12-21 at 11.09.36 AM"></p><p>Here <em>γ</em> is the discount factor between 0 and 1 – the more into the future the reward is, the less we take it into consideration. It is easy to see, that discounted future reward at time step <em>t</em> can be expressed in terms of the same thing at time step <em>t+1</em>:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.40-AM.png" alt="Screen Shot 2015-12-21 at 11.09.40 AM"></p><p>If we set the discount factor <em>γ</em>=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like <em>γ=</em>0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor <em>γ</em>=1.</p><p>A good strategy for an agent would be to <strong>always choose an action that maximizes the (discounted) future reward</strong>.</p><h1 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h1><p>In Q-learning we define a function <em>Q(s, a)</em> representing <strong>the maximum discounted future reward when we perform action </strong>a<strong> in state </strong>s<strong>, and continue optimally from that point on.</strong></p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.47-AM.png" alt="Screen Shot 2015-12-21 at 11.09.47 AM"></p><p>The way to think about <em>Q(s, a)</em> is that it is “the best possible score at the end of the game after performing action a<strong>in state </strong>s<strong>“. It is called Q-function, because it represents the “quality” of a certain action in a given state.</strong></p><p>This may sound like quite a puzzling definition. How can we estimate the score at the end of game, if we know just the current state and action, and not the actions and rewards coming after that? We really can’t. But as a theoretical construct we can assume existence of such a function. Just close your eyes and repeat to yourself five times: “<em>Q(s, a) </em>exists, <em>Q(s, a) </em>exists, …”. Feel it?</p><p>If you’re still not convinced, then consider what the implications of having such a function would be. Suppose you are in state and pondering whether you should take action <em>a</em> or <em>b</em>. You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer becomes really simple – pick the action with the highest Q-value!</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.56-AM.png" alt="Screen Shot 2015-12-21 at 11.09.56 AM"></p><p>Here π represents the policy, the rule how we choose an action in each state.</p><p>OK, how do we get that Q-function then? Let’s focus on just one transition &lt;<em>s, a, r, s’</em>&gt;. Just like with discounted future rewards in the previous section, we can express the Q-value of state <em>s</em> and action <em>a</em> in terms of the Q-value of the next state <em>s’</em>.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.10.00-AM.png" alt="Screen Shot 2015-12-21 at 11.10.00 AM">This is called the <strong>Bellman equation</strong>. If you think about it, it is quite logical – maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state.</p><p>The main idea in Q-learning is that <strong>we can iteratively approximate the Q-function using the Bellman equation</strong>. In the simplest case the Q-function is implemented as a table, with states as rows and actions as columns. The gist of the Q-learning algorithm is as simple as the following<a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/#_ftn1" target="_blank" rel="external">[1]</a>:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.55-AM.png" alt="Screen Shot 2015-12-21 at 11.23.55 AM"></p><p><em>α</em> in the algorithm is a learning rate that controls how much of the difference between previous Q-value and newly proposed Q-value is taken into account. In particular, when <em>α</em>=1, then two <em>Q[s,a]</em> cancel and the update is exactly the same as the Bellman equation.</p><p>The max<em>a’</em> <em>Q</em>[<em>s’</em>,<em>a’</em>] that we use to update <em>Q</em>[<em>s</em>,<em>a</em>] is only an approximation and in early stages of learning it may be completely wrong. However the approximation get more and more accurate with every iteration and <a href="http://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/55/2015/12/ProofQlearning.pdf" target="_blank" rel="external">it has been shown</a>, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.</p><h1 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a>Deep Q Network</h1><p>The state of the environment in the Breakout game can be defined by the location of the paddle, location and direction of the ball and the presence or absence of each individual brick. This intuitive representation however is game specific. Could we come up with something more universal, that would be suitable for all the games? The obvious choice is screen pixels – they implicitly contain all of the relevant information about the game situation, except for the speed and direction of the ball. Two consecutive screens would have these covered as well.</p><p>If we apply the same preprocessing to game screens as in the DeepMind paper – take the four last screen images, resize them to 84×84 and convert to grayscale with 256 gray levels – we would have 25684x84x4 ≈ 1067970 possible game states. This means 1067970 rows in our imaginary Q-table – more than the number of atoms in the known universe! One could argue that many pixel combinations (and therefore states) never occur – we could possibly represent it as a sparse table containing only visited states. Even so, most of the states are very rarely visited and it would take a lifetime of the universe for the Q-table to converge. Ideally, we would also like to have a good guess for Q-values for states we have never seen before.</p><p>This is the point where deep learning steps in. Neural networks are exceptionally good at coming up with good features for highly structured data. We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. This approach has the advantage, that if we want to perform a Q-value update or pick the action with the highest Q-value, we only have to do one forward pass through the network and have all Q-values for all actions available immediately.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.27.12-AM.png" alt="img"></p><p>Figure 3: <em>Left: </em>Naive formulation of deep Q-network. <em>Right: </em>More optimized architecture of deep Q-network, used in DeepMind paper.</p><p>The network architecture that DeepMind used is as follows:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.28-AM.png" alt="Screen Shot 2015-12-21 at 11.23.28 AM"></p><p>This is a classical convolutional neural network with three convolutional layers, followed by two fully connected layers. People familiar with object recognition networks may notice that there are no pooling layers. But if you really think about it, pooling layers buy you translation invariance – the network becomes insensitive to the location of an object in the image. That makes perfectly sense for a classification task like ImageNet, but for games the location of the ball is crucial in determining the potential reward and we wouldn’t want to discard this information!</p><p>Input to the network are four 84×84 grayscale game screens. Outputs of the network are Q-values for each possible action (18 in Atari). Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/formula.png" alt="img"></p><p>Given a transition &lt;<em> s, a, r, s’</em> &gt;, the Q-table update rule in the previous algorithm must be replaced with the following:</p><ol><li>Do a feedforward pass for the current state <em>s</em> to get predicted Q-values for all actions.</li><li>Do a feedforward pass for the next state <em>s’ </em>and calculate maximum overall network outputs <em>max a’ Q(s’, a’).</em></li><li>Set Q-value target for action to <em>r + γmax a’ Q(s’, a’)</em> (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.</li><li>Update the weights using backpropagation.</li></ol><h1 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h1><p>By now we have an idea how to estimate the future reward in each state using Q-learning and approximate the Q-function using a convolutional neural network. But it turns out that approximation of Q-values using non-linear functions is not very stable. There is a whole bag of tricks that you have to use to actually make it converge. And it takes a long time, almost a week on a single GPU.</p><p>The most important trick is <strong>experience replay</strong>. During gameplay all the experiences &lt;<em> s, a, r, s’</em> &gt; are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.</p><h1 id="Exploration-Exploitation"><a href="#Exploration-Exploitation" class="headerlink" title="Exploration-Exploitation"></a>Exploration-Exploitation</h1><p>Q-learning attempts to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward. But we haven’t touched the exploration-exploitation dilemma yet…</p><p>Firstly observe, that when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude “exploration”. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is “greedy”, it settles with the first effective strategy it finds.</p><p>A simple and effective fix for the above problem is <strong>ε-greedy exploration</strong> – with probability <em>ε</em> choose a random action, otherwise go with the “greedy” action with the highest Q-value. In their system DeepMind actually decreases <em>ε</em> over time from 1 to 0.1 – in the beginning the system makes completely random moves to explore the state space maximally, and then it settles down to a fixed exploration rate.</p><h1 id="Deep-Q-learning-Algorithm"><a href="#Deep-Q-learning-Algorithm" class="headerlink" title="Deep Q-learning Algorithm"></a>Deep Q-learning Algorithm</h1><p>This gives us the final deep Q-learning algorithm with experience replay:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.43-AM-1.png" alt="Screen Shot 2015-12-21 at 11.23.43 AM"></p><p>There are many more tricks that DeepMind used to actually make it work – like target network, error clipping, reward clipping etc, but these are out of scope for this introduction.</p><p>The most amazing part of this algorithm is that it learns anything at all. Just think about it – because our Q-function is initialized randomly, it initially outputs complete garbage. And we are using this garbage (the maximum Q-value of the next state) as targets for the network, only occasionally folding in a tiny reward. That sounds insane, how could it learn anything meaningful at all? The fact is, that it does.</p><h1 id="Final-notes"><a href="#Final-notes" class="headerlink" title="Final notes"></a>Final notes</h1><p>Many improvements to deep Q-learning have been proposed since its first introduction – <a href="http://arxiv.org/abs/1509.06461" target="_blank" rel="external">Double Q-learning</a>, <a href="http://arxiv.org/abs/1511.05952" target="_blank" rel="external">Prioritized Experience Replay</a>, <a href="http://arxiv.org/abs/1511.06581" target="_blank" rel="external">Dueling Network Architecture</a> and <a href="http://arxiv.org/abs/1509.02971" target="_blank" rel="external">extension to continuous action space</a> to name a few. For latest advancements check out the <a href="http://rll.berkeley.edu/deeprlworkshop/" target="_blank" rel="external">NIPS 2015 deep reinforcement learning workshop</a> and <a href="https://cmt.research.microsoft.com/ICLR2016Conference/Protected/PublicComment.aspx" target="_blank" rel="external">ICLR 2016</a>(search for “reinforcement” in title). But beware, that <a href="http://www.google.com/patents/US20150100530" target="_blank" rel="external">deep Q-learning has been patented by Google</a>.</p><p>It is often said, that artificial intelligence is something we haven’t figured out yet. Once we know how it works, it doesn’t seem intelligent any more. But deep Q-networks still continue to amaze me. Watching them figure out a new game is like observing an animal in the wild – a rewarding experience by itself.</p><h1 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a>Credits</h1><p>Thanks to Ardi Tampuu, Tanel Pärnamaa, Jaan Aru, Ilya Kuzovkin, Arjun Bansal and Urs Köster for comments and suggestions on the drafts of this post.</p><h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><ul><li><a href="http://videolectures.net/rldm2015_silver_reinforcement_learning/" target="_blank" rel="external">David Silver’s lecture about deep reinforcement learning</a></li><li><a href="https://www.youtube.com/watch?v=b1a53hE0yQs" target="_blank" rel="external">Slightly awkward but accessible illustration of Q-learning</a></li><li><a href="http://rll.berkeley.edu/deeprlcourse/" target="_blank" rel="external">UC Berkley’s course on deep reinforcement learning</a></li><li><a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="external">David Silver’s reinforcement learning course</a></li><li><a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" target="_blank" rel="external">Nando de Freitas’ course on machine learning</a> (two lectures about reinforcement learning in the end)</li><li><a href="http://cs231n.github.io/" target="_blank" rel="external">Andrej Karpathy’s course on convolutional neural networks</a></li></ul><p><a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/#_ftnref1" target="_blank" rel="external">[1]</a> Algorithm adapted from <a href="http://artint.info/html/ArtInt_265.html" target="_blank" rel="external">http://artint.info/html/ArtInt_265.html</a><br>This blog was first published at: <a href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/</a></p><h4 id="This-is-the-part-1-of-my-series-on-deep-reinforcement-learning-Tune-in-next-week-for-“Deep-Reinforcement-Learning-with-Neon”-for-an-actual-implementation-with-Neon-deep-learning-toolkit"><a href="#This-is-the-part-1-of-my-series-on-deep-reinforcement-learning-Tune-in-next-week-for-“Deep-Reinforcement-Learning-with-Neon”-for-an-actual-implementation-with-Neon-deep-learning-toolkit" class="headerlink" title="This is the part 1 of my series on deep reinforcement learning. Tune in next week for “Deep Reinforcement Learning with Neon” for an actual implementation with Neon deep learning toolkit."></a>This is the part 1 of my series on deep reinforcement learning. Tune in next week for <a href="https://www.intelnervana.com/deep-reinforcement-learning-with-neon/" target="_blank" rel="external">“Deep Reinforcement Learning with Neon”</a> for an actual implementation with <a href="https://github.com/NervanaSystems/neon" target="_blank" rel="external">Neon</a> deep learning toolkit.</h4></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/06/On-policy-Control-with-Approximation/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/06/On-policy-Control-with-Approximation/" itemprop="url">On-policy Control with Approximation</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-06T17:41:34+08:00">2017-07-06 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/06/On-policy-Control-with-Approximation/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/06/On-policy-Control-with-Approximation/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/06/On-policy-Control-with-Approximation/" class="leancloud_visitors" data-flag-title="On-policy Control with Approximation"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a finite-dimensional weight vector.</p><h3 id="Episodic-Semi-gradient-Control"><a href="#Episodic-Semi-gradient-Control" class="headerlink" title="Episodic Semi-gradient Control"></a>Episodic Semi-gradient Control</h3><p>The general gradient-descent update for action-value prediction is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>For example, the update for the one-step Sarsa method is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t)- \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>We call this method <strong>episode semi-gradient one-step sarsa</strong>.</p><p>To form control methods, we need to couple such action-value prediction methods with techniques for policy improvement and action selection. Suitable techniques applicable to continuous actions, or to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution. On the other hand, if the action set is discrete and not too large, then we can use the techniques already developed in previous chapters.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/episode-semi-grad-sarsa.png" alt="episode-semi-grad-sarsa"></p><h4 id="Example-Mountain-Car-Task"><a href="#Example-Mountain-Car-Task" class="headerlink" title="Example: Mountain-Car Task"></a>Example: Mountain-Car Task</h4><p>Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram in the below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mountain-car.png" alt="mountain-car"></p><p>The diﬃculty is that gravity is stronger than the car’s engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to ﬁrst move away from the goal and up the opposite slope on the left. Then, by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way. This is a simple example of a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. Many control methodologies have great diﬃculties with tasks of this kind unless explicitly aided by a human designer.</p><p>The reward in this problem is −1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (−1), and zero throttle (0). The car moves according to a simpliﬁed physics. Its position, $x_t$, and velocity, $\dot{x}_t$, are updated by<br>$$<br>\begin{align}<br>x_{t+1} &amp;\doteq bound \big[x_t + \dot{x}_{t+1} \big] \\<br>\dot{x}_{t+1} &amp;\doteq bound \big[\dot{x}_t + 0.001 A_t - 0.0025 \cos(3x_t) \big],<br>\end{align}<br>$$<br>where the bound operation enforces $-1.2 \leq x_{t+1} \leq 0.5 \;\; \text{and} \; -0.07 \leq \dot{x}_{t+1} \leq 0.07$. In addition, when $x_{t+1}$ reached the left bound, $\dot{x}_{t+1}$ was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position $x_t \in [−0.6, −0.4)$ and zero velocity. To convert the two continuous state variables to binary features, we used grid-tilings.</p><p>First of all, we define the environment of this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all possible actions</span></div><div class="line">ACTION_REVERSE = <span class="number">-1</span></div><div class="line">ACTION_ZERO = <span class="number">0</span></div><div class="line">ACTION_FORWARD = <span class="number">1</span></div><div class="line"><span class="comment"># order is important</span></div><div class="line">ACTIONS = [ACTION_REVERSE, ACTION_ZERO, ACTION_FORWARD]</div><div class="line"></div><div class="line"><span class="comment"># bound for position and velocity</span></div><div class="line">POSITION_MIN = <span class="number">-1.2</span></div><div class="line">POSITION_MAX = <span class="number">0.5</span></div><div class="line">VELOCITY_MIN = <span class="number">-0.07</span></div><div class="line">VELOCITY_MAX = <span class="number">0.07</span></div><div class="line"></div><div class="line"><span class="comment"># use optimistic initial value, so it's ok to set epsilon to 0</span></div><div class="line">EPSILON = <span class="number">0</span></div></pre></td></tr></table></figure><p>After take an action, we transition to a new state and get a reward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take an @action at @position and @velocity</span></div><div class="line"><span class="comment"># @return: new position, new velocity, reward (always -1)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(position, velocity, action)</span>:</span></div><div class="line">    newVelocity = velocity + <span class="number">0.001</span> * action - <span class="number">0.0025</span> * np.cos(<span class="number">3</span> * position)</div><div class="line">    newVelocity = min(max(VELOCITY_MIN, newVelocity), VELOCITY_MAX)</div><div class="line">    newPosition = position + newVelocity</div><div class="line">    newPosition = min(max(POSITION_MIN, newPosition), POSITION_MAX)</div><div class="line">    reward = <span class="number">-1.0</span></div><div class="line">    <span class="keyword">if</span> newPosition == POSITION_MIN:</div><div class="line">        newVelocity = <span class="number">0.0</span></div><div class="line">    <span class="keyword">return</span> newPosition, newVelocity, reward</div></pre></td></tr></table></figure><p>The $\varepsilon$-greedy policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get action at @position and @velocity based on epsilon greedy policy and @valueFunction</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(position, velocity, valueFunction)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(ACTIONS)</div><div class="line">    values = []</div><div class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">        values.append(valueFunction.value(position, velocity, action))</div><div class="line">    <span class="keyword">return</span> np.argmax(values) - <span class="number">1</span></div></pre></td></tr></table></figure><p>We need map out continuous state to discrete state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># wrapper class for state action value function</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValueFunction</span>:</span></div><div class="line">    <span class="comment"># In this example I use the tiling software instead of implementing standard tiling by myself</span></div><div class="line">    <span class="comment"># One important thing is that tiling is only a map from (state, action) to a series of indices</span></div><div class="line">    <span class="comment"># It doesn't matter whether the indices have meaning, only if this map satisfy some property</span></div><div class="line">    <span class="comment"># View the following webpage for more information</span></div><div class="line">    <span class="comment"># http://incompleteideas.net/sutton/tiles/tiles3.html</span></div><div class="line">    <span class="comment"># @maxSize: the maximum # of indices</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, stepSize, numOfTilings=<span class="number">8</span>, maxSize=<span class="number">2048</span>)</span>:</span></div><div class="line">        self.maxSize = maxSize</div><div class="line">        self.numOfTilings = numOfTilings</div><div class="line"></div><div class="line">        <span class="comment"># divide step size equally to each tiling</span></div><div class="line">        self.stepSize = stepSize / numOfTilings</div><div class="line"></div><div class="line">        self.hashTable = IHT(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># weight for each tile</span></div><div class="line">        self.weights = np.zeros(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># position and velocity needs scaling to satisfy the tile software</span></div><div class="line">        self.positionScale = self.numOfTilings / (POSITION_MAX - POSITION_MIN)</div><div class="line">        self.velocityScale = self.numOfTilings / (VELOCITY_MAX - VELOCITY_MIN)</div><div class="line"></div><div class="line">    <span class="comment"># get indices of active tiles for given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getActiveTiles</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="comment"># I think positionScale * (position - position_min) would be a good normalization.</span></div><div class="line">        <span class="comment"># However positionScale * position_min is a constant, so it's ok to ignore it.</span></div><div class="line">        activeTiles = tiles(self.hashTable, self.numOfTilings,</div><div class="line">                            [self.positionScale * position, self.velocityScale * velocity],</div><div class="line">                            [action])</div><div class="line">        <span class="keyword">return</span> activeTiles</div><div class="line"></div><div class="line">    <span class="comment"># estimate the value of given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="keyword">if</span> position == POSITION_MAX:</div><div class="line">            <span class="keyword">return</span> <span class="number">0.0</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        <span class="keyword">return</span> np.sum(self.weights[activeTiles])</div><div class="line"></div><div class="line">    <span class="comment"># learn with given state, action and target</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, position, velocity, action, target)</span>:</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        estimation = np.sum(self.weights[activeTiles])</div><div class="line">        delta = self.stepSize * (target - estimation)</div><div class="line">        <span class="keyword">for</span> activeTile <span class="keyword">in</span> activeTiles:</div><div class="line">            self.weights[activeTile] += delta</div><div class="line"></div><div class="line">    <span class="comment"># get # of steps to reach the goal under current state value function</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">costToGo</span><span class="params">(self, position, velocity)</span>:</span></div><div class="line">        costs = []</div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">            costs.append(self.value(position, velocity, action))</div><div class="line">        <span class="keyword">return</span> -np.max(costs)</div></pre></td></tr></table></figure><p>Because the one-step semi-gradient sarsa is a special case of n-step, so we develop the n-step algorithm<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \; n \geq 1,0 \leq t \leq T-n.<br>$$<br>The n-step equation is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ G_{t:t+n}- \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}), \;\;\; 0 \leq t \leq T.<br>$$<br>Complete pseudocode is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/n-step-sg-sarsa.png" alt="n-step-sg-sarsa"></p><p>So the code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># semi-gradient n-step Sarsa</span></div><div class="line"><span class="comment"># @valueFunction: state value function to learn</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">semiGradientNStepSarsa</span><span class="params">(valueFunction, n=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="comment"># start at a random position around the bottom of the valley</span></div><div class="line">    currentPosition = np.random.uniform(<span class="number">-0.6</span>, <span class="number">-0.4</span>)</div><div class="line">    <span class="comment"># initial velocity is 0</span></div><div class="line">    currentVelocity = <span class="number">0.0</span></div><div class="line">    <span class="comment"># get initial action</span></div><div class="line">    currentAction = getAction(currentPosition, currentVelocity, valueFunction)</div><div class="line"></div><div class="line">    <span class="comment"># track previous position, velocity, action and reward</span></div><div class="line">    positions = [currentPosition]</div><div class="line">    velocities = [currentVelocity]</div><div class="line">    actions = [currentAction]</div><div class="line">    rewards = [<span class="number">0.0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># take current action and go to the new state</span></div><div class="line">            newPostion, newVelocity, reward = takeAction(currentPosition, currentVelocity, currentAction)</div><div class="line">            <span class="comment"># choose new action</span></div><div class="line">            newAction = getAction(newPostion, newVelocity, valueFunction)</div><div class="line"></div><div class="line">            <span class="comment"># track new state and action</span></div><div class="line">            positions.append(newPostion)</div><div class="line">            velocities.append(newVelocity)</div><div class="line">            actions.append(newAction)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newPostion == POSITION_MAX:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += rewards[t]</div><div class="line">            <span class="comment"># add estimated state action value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += valueFunction.value(positions[updateTime + n],</div><div class="line">                                               velocities[updateTime + n],</div><div class="line">                                               actions[updateTime + n])</div><div class="line">            <span class="comment"># update the state value function</span></div><div class="line">            <span class="keyword">if</span> positions[updateTime] != POSITION_MAX:</div><div class="line">                valueFunction.learn(positions[updateTime], velocities[updateTime], actions[updateTime], returns)</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentPosition = newPostion</div><div class="line">        currentVelocity = newVelocity</div><div class="line">        currentAction = newAction</div><div class="line"></div><div class="line">    <span class="keyword">return</span> time</div></pre></td></tr></table></figure><p>Next, we use the method mentioned earlier to solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">episodes = <span class="number">9000</span></div><div class="line">targetEpisodes = [<span class="number">1</span><span class="number">-1</span>, <span class="number">12</span><span class="number">-1</span>, <span class="number">104</span><span class="number">-1</span>, <span class="number">1000</span><span class="number">-1</span>, episodes - <span class="number">1</span>]</div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alpha = <span class="number">0.3</span></div><div class="line">valueFunction = ValueFunction(alpha, numOfTilings)</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">    print(<span class="string">'episode:'</span>, episode)</div><div class="line">    semiGradientNStepSarsa(valueFunction)</div><div class="line">    <span class="keyword">if</span> episode <span class="keyword">in</span> targetEpisodes:</div><div class="line">        prettyPrint(valueFunction, <span class="string">'Episode: '</span> + str(episode + <span class="number">1</span>))</div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa.png" alt="mcar-sg-sarsa"></p><p>The result shows what typically happens while learning to solve this task with this form of function approximation. Shown is the negative of the value function (the cost-to-go function) learned on a single run. The initial action values were all zero, which was optimistic (all true values are negative in this task), causing extensive exploration to occur even though the exploration parameter, $\varepsilon$, was 0. All the states visited frequently are valued worse than unexplored states, because the actual rewards have been worse than what was (unrealistically) expected. This continually drives the agent away from wherever it has been, to explore new states, until a solution is found.</p><p>Next, let us test the performance of various step size (learning rate).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'alpha:'</span>, alphas[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'alpha = '</span>+str(alphas[i])+<span class="string">'/'</span>+str(numOfTilings))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha.png" alt="mcar-sg-sarsa-var-alpha"></p><p>And then, let us test the performance of one-step sarsa and multi-step sarsa on the Mountain Car task:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.5</span>, <span class="number">0.3</span>]</div><div class="line">nSteps = [<span class="number">1</span>, <span class="number">8</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nSteps[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index], nSteps[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-n.png" alt="mcar-sg-sarsa-var-n"></p><p>Finally, let me shows the results of a more detailed study of the eﬀect of the parameters $\alpha$ and $n$ on the rate of learning on this task.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">alphas = np.arange(<span class="number">0.25</span>, <span class="number">1.75</span>, <span class="number">0.25</span>)</div><div class="line">nSteps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">5</span>))</div><div class="line">episodes = <span class="number">50</span></div><div class="line">runs = <span class="number">5</span></div><div class="line"></div><div class="line">truncateStep = <span class="number">300</span></div><div class="line">steps = np.zeros((len(nSteps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> nStepIndex, nStep <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(nSteps)), nSteps):</div><div class="line">        <span class="keyword">for</span> alphaIndex, alpha <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(alphas)), alphas):</div><div class="line">            <span class="keyword">if</span> (nStep == <span class="number">8</span> <span class="keyword">and</span> alpha &gt; <span class="number">1</span>) <span class="keyword">or</span> \</div><div class="line">                    (nStep == <span class="number">16</span> <span class="keyword">and</span> alpha &gt; <span class="number">0.75</span>):</div><div class="line">                <span class="comment"># In these cases it won't converge, so ignore them</span></div><div class="line">                steps[nStepIndex, alphaIndex] += truncateStep * episodes</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            valueFunction = ValueFunction(alpha)</div><div class="line">            <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nStep, <span class="string">'alpha:'</span>, alpha, <span class="string">'episode:'</span>, episode)</div><div class="line">                step = semiGradientNStepSarsa(valueFunction, nStep)</div><div class="line">                steps[nStepIndex, alphaIndex] += step</div><div class="line"><span class="comment"># average over independent runs and episodes</span></div><div class="line">steps /= runs * episodes</div><div class="line"><span class="comment"># truncate high values for better display</span></div><div class="line">steps[steps &gt; truncateStep] = truncateStep</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(nSteps)):</div><div class="line">    plt.plot(alphas, steps[i, :], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha * number of tilings(8)'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha-n.png" alt="mcar-sg-sarsa-var-alpha-n"></p><h3 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h3><h4 id="Use-OpenAI-gym"><a href="#Use-OpenAI-gym" class="headerlink" title="Use OpenAI gym"></a>Use OpenAI gym</h4><p>Now, let us use the OpenAI gym toolkit to simply our Mountain Car task. As we know, we need to define the environment of task before develop the detail algorithm. It’s easy to make mistakes with some detail. So it is fantastic if we do not need to care about that. The gym toolkit help us to define the environment. Such as the Mountain Car task, there is a build-in model in the gym toolkit. We just need to write one row code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env = gym.envs.make(<span class="string">"MountainCar-v0"</span>)</div></pre></td></tr></table></figure><p>That is amazing!</p><p>We also can test the environment very convenience and get a pretty good user graphic:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">env.reset()</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line"><span class="comment"># for x in range(10000):</span></div><div class="line"><span class="comment">#     env.step(0)</span></div><div class="line"><span class="comment">#     plt.figure()</span></div><div class="line"><span class="comment">#     plt.imshow(env.render(mode='rgb_array'))  </span></div><div class="line">[env.step(<span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)]</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">env.render(close=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>These codes will return the result as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-gym-test.png" alt="mcar-gym-test"></p><p>Bravo~</p><p>Now, let us solve the task by to use the Q-Learning algorithm. Meanwhile, we will use the RBF kernel to construct the features and use the SGDRegressor gradient-descent method of scikit-learn package to update $\mathbf{w}$.</p><p>First of all, we need to normalized our features to accelerate the convergence speed and use the RBF kernel to reconstruct our feature space. The both methods (Normalization and Reconstruction) need to use some training set to train a model. Then we use this model to transform our raw data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Feature Preprocessing: Normalize to zero mean and unit variance</span></div><div class="line"><span class="comment"># We use a few samples from the observation space to do this</span></div><div class="line">observation_examples = np.array([env.observation_space.sample() <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)])</div><div class="line">scaler = sklearn.preprocessing.StandardScaler()</div><div class="line">scaler.fit(observation_examples)</div><div class="line"></div><div class="line"><span class="comment"># Used to converte a state to a featurizes represenation.</span></div><div class="line"><span class="comment"># We use RBF kernels with different variances to cover different parts of the space</span></div><div class="line">featurizer = sklearn.pipeline.FeatureUnion([</div><div class="line">        (<span class="string">"rbf1"</span>, RBFSampler(gamma=<span class="number">5.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf2"</span>, RBFSampler(gamma=<span class="number">2.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf3"</span>, RBFSampler(gamma=<span class="number">1.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf4"</span>, RBFSampler(gamma=<span class="number">0.5</span>, n_components=<span class="number">100</span>))</div><div class="line">        ])</div><div class="line">featurizer.fit(scaler.transform(observation_examples))</div></pre></td></tr></table></figure><p>Next, we define a class named Estimator to simply the gradient descent process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Estimator</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Value Function approximator. </div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># We create a separate model for each action in the environment's</span></div><div class="line">        <span class="comment"># action space. Alternatively we could somehow encode the action</span></div><div class="line">        <span class="comment"># into the features, but this way it's easier to code up.</span></div><div class="line">        self.models = []</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(env.action_space.n):</div><div class="line">            model = SGDRegressor(learning_rate=<span class="string">"constant"</span>)</div><div class="line">            <span class="comment"># We need to call partial_fit once to initialize the model</span></div><div class="line">            <span class="comment"># or we get a NotFittedError when trying to make a prediction</span></div><div class="line">            <span class="comment"># This is quite hacky.</span></div><div class="line">            model.partial_fit([self.featurize_state(env.reset())], [<span class="number">0</span>])</div><div class="line">            self.models.append(model)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">featurize_state</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Returns the featurized representation for a state.</div><div class="line">        """</div><div class="line">        scaled = scaler.transform([state])</div><div class="line">        featurized = featurizer.transform(scaled)</div><div class="line">        <span class="keyword">return</span> featurized[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, s, a=None)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Makes value function predictions.</div><div class="line">        </div><div class="line">        Args:</div><div class="line">            s: state to make a prediction for</div><div class="line">            a: (Optional) action to make a prediction for</div><div class="line">            </div><div class="line">        Returns</div><div class="line">            If an action a is given this returns a single number as the prediction.</div><div class="line">            If no action is given this returns a vector or predictions for all actions</div><div class="line">            in the environment where pred[i] is the prediction for action i.</div><div class="line">            </div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> a:</div><div class="line">            <span class="keyword">return</span> np.array([m.predict([features])[<span class="number">0</span>] <span class="keyword">for</span> m <span class="keyword">in</span> self.models])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self.models[a].predict([features])[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, s, a, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Updates the estimator parameters for a given state and action towards</div><div class="line">        the target y.</div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        self.models[a].partial_fit([features], [y])</div></pre></td></tr></table></figure><p>We also need a $\varepsilon$-greedy policy to select action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(estimator, epsilon, nA)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        estimator: An estimator that returns q values for a given state</div><div class="line">        epsilon: The probability to select a random action . float between 0 and 1.</div><div class="line">        nA: Number of actions in the environment.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A function that takes the observation as an argument and returns</div><div class="line">        the probabilities for each action in the form of a numpy array of length nA.</div><div class="line">    </div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></div><div class="line">        A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">        q_values = estimator.predict(observation)</div><div class="line">        best_action = np.argmax(q_values)</div><div class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">        <span class="keyword">return</span> A</div><div class="line">    <span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Then we develop the Q-Learning method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_learning</span><span class="params">(env, estimator, num_episodes, discount_factor=<span class="number">1.0</span>, epsilon=<span class="number">0.1</span>, epsilon_decay=<span class="number">1.0</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm for fff-policy TD control using Function Approximation.</div><div class="line">    Finds the optimal greedy policy while following an epsilon-greedy policy.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        estimator: Action-Value function estimator</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">        epsilon_decay: Each episode, epsilon is decayed by this factor</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))    </div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        </div><div class="line">        <span class="comment"># The policy we're following</span></div><div class="line">        policy = make_epsilon_greedy_policy(</div><div class="line">            estimator, epsilon * epsilon_decay**i_episode, env.action_space.n)</div><div class="line">        </div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="comment"># Also print reward for last episode</span></div><div class="line">        last_reward = stats.episode_rewards[i_episode - <span class="number">1</span>]</div><div class="line">        sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Reset the environment and pick the first action</span></div><div class="line">        state = env.reset()</div><div class="line">        </div><div class="line">        <span class="comment"># Only used for SARSA, not Q-Learning</span></div><div class="line">        next_action = <span class="keyword">None</span></div><div class="line">        </div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">                        </div><div class="line">            <span class="comment"># Choose an action to take</span></div><div class="line">            <span class="comment"># If we're using SARSA we already decided in the previous step</span></div><div class="line">            <span class="keyword">if</span> next_action <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                action_probs = policy(state)</div><div class="line">                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                action = next_action</div><div class="line">            </div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            next_state, reward, done, _ = env.step(action)</div><div class="line">    </div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            <span class="comment"># TD Update</span></div><div class="line">            q_values_next = estimator.predict(next_state)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for Q-Learning</span></div><div class="line">            <span class="comment"># Q-Value TD Target</span></div><div class="line">            td_target = reward + discount_factor * np.max(q_values_next)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for SARSA TD Target for on policy-training:</span></div><div class="line">            <span class="comment"># next_action_probs = policy(next_state)</span></div><div class="line">            <span class="comment"># next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)             </span></div><div class="line">            <span class="comment"># td_target = reward + discount_factor * q_values_next[next_action]</span></div><div class="line">            </div><div class="line">            <span class="comment"># Update the function approximator using our target</span></div><div class="line">            estimator.update(state, action, td_target)</div><div class="line">            </div><div class="line">            print(<span class="string">"\rStep &#123;&#125; @ Episode &#123;&#125;/&#123;&#125; (&#123;&#125;)"</span>.format(t, i_episode + <span class="number">1</span>, num_episodes, last_reward), end=<span class="string">""</span>)</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">                </div><div class="line">            state = next_state</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> stats</div></pre></td></tr></table></figure><p>Run this method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">estimator = Estimator()</div><div class="line"><span class="comment"># Note: For the Mountain Car we don't actually need an epsilon &gt; 0.0</span></div><div class="line"><span class="comment"># because our initial estimate for all states is too "optimistic" which leads</span></div><div class="line"><span class="comment"># to the exploration of all states.</span></div><div class="line">stats = q_learning(env, estimator, <span class="number">100</span>, epsilon=<span class="number">0.0</span>)</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-ql-gym.png" alt="mcar-ql-gym"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/05/On-policy-Prediction-with-Approximation/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/05/On-policy-Prediction-with-Approximation/" itemprop="url">On-policy Prediction with Approximation</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-05T15:29:22+08:00">2017-07-05 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/05/On-policy-Prediction-with-Approximation/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/05/On-policy-Prediction-with-Approximation/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/05/On-policy-Prediction-with-Approximation/" class="leancloud_visitors" data-flag-title="On-policy Prediction with Approximation"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>The novelty in this post is that the approximate value function is represented not as a table but as a parameterized functional form with weight vector $\mathbf{w} \in \mathbb{R}^d$. We will write $\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)$ for the approximated value of state $s$ given weight vector $\mathbf{w}$. For example, $\hat{v}$ might be a linear function in features of the state, with $\mathbf{w}$ the vector of feature weights. Consequently, when a single state is updated, the change generalizes from that state to affect the values of many other states.</p><h3 id="The-prediction-Objective-MSVE"><a href="#The-prediction-Objective-MSVE" class="headerlink" title="The prediction Objective (MSVE)"></a>The prediction Objective (MSVE)</h3><p>In the tabular case a continuous measure of prediction quality was not necessary because the learned value function could come to equal the true value function exactly. But with genuine approximation, an update at one state aﬀects many others, and it is not possible to get all states exactly correct. By assumption we have far more states than weights, so making one state’s estimate more accurate invariably means making others’ less accurate.</p><p>By the error in a state $s$ we mean the square of the diﬀerence between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_{\pi}(s)$. Weighting this over the state space by the distribution $\mu$, we obtain a natural objective function, the <strong>Mean Squared Value Error</strong>, or <strong>MSVE</strong>:<br>$$<br>\text{MSVE}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s) \Big[v_{\pi}(s) - \hat{v}(s, \mathbf{w}) \Big]^2.<br>$$<br>The square root of this measure, the root MSVE or RMSVE, gives a rough measure of how much the approximate values diﬀer from the true values and is often used in plots. Typically one chooses $\mu(s)$ to be the fraction of time spent in $s$ under the target policy $\pi$. This is called the <em>on-policy distribution</em>.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/on_policy_dist.png" alt="on-policy-distribution"></p><h3 id="Stochastic-gradient-Methods"><a href="#Stochastic-gradient-Methods" class="headerlink" title="Stochastic-gradient Methods"></a>Stochastic-gradient Methods</h3><p>We assume that states appear in examples with the same distribution, µ, over which we are trying to minimize the MSVE. A good strategy in this case is to try to minimize error on the observed examples. Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example by a small amount in the direction that would most reduce the error on that example:<br>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t - \frac{1}{2} \alpha \nabla \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big]^2 \\<br>&amp;= \mathbf{w}_t + \alpha \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).<br>\end{align}<br>$$<br>And<br>$$<br>\nabla f(\mathbf{w}) \doteq \Big( \frac{\partial f(\mathbf{w})}{\partial w_1}, \frac{\partial f(\mathbf{w})}{\partial w_2}, \cdots, \frac{\partial f(\mathbf{w})}{\partial w_d}\Big)^{\top}.<br>$$<br>Although $v_{\pi}(S_t)$ is unknown, but we can approximate it by substituting $U_t$ (the $t$th training example) in place of $v_{\pi}(S_t)$. This yields the following general SGD method for state-value prediction:<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).<br>$$<br>If $U_t$ is an <strong>unbiased</strong> estimate, that is, if $\mathbb{E}[U_t]=v_{\pi}(S_t)$, for each $t$, then $\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic approximation conditions for decreasing $\alpha$.</p><p>For example, suppose the states in the examples are the states generated by interaction (or simulated interaction) with the environment using policy $\pi$. Because the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t \doteq G_t$ is by deﬁnition an unbiased estimate of $v_{\pi}(S_t)$. Thus, the gradient-descent version of Monte Carlo state-value prediction is guaranteed to ﬁnd a locally optimal solution. Pseudocode for a complete algorithm<br>is shown in the box.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/gradient_mc.png" alt="gradient_mc"></p><h4 id="Example-State-Aggregation-on-the-1000-state-Random-Walk"><a href="#Example-State-Aggregation-on-the-1000-state-Random-Walk" class="headerlink" title="Example: State Aggregation on the 1000-state Random Walk"></a>Example: State Aggregation on the 1000-state Random Walk</h4><p>State aggregation is a simple form of generalizing function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group. The value of a state is estimated as its group’s component, and when the state is updated, that component alone is updated. State aggregation is a special case of SGD in which the gradient, $\nabla \hat{v}(S_t, \mathbf{w}_t)$, is <strong>1</strong> for $S_t$’s group’s component and <strong>0</strong> for the other components.</p><p>Consider a 1000-state version of the random walk task. The states are numbered from 1 to 1000, left to right, and all episodes begin near the center, in state 500. State transitions are from the current state to one of the 100 neighboring states to its left, or to one of the 100 neighboring states to its right, all with equal probability. Of course, if the current state is near an edge, then there may be fewer than 100 neighbors on that side of it. In this case, all the probability that would have gone into those missing neighbors goes into the probability of terminating on that side (thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right). As usual, termination on the left produces a reward of −1, and termination on the right produces a reward of +1. All other transitions have a reward of zero.</p><p>Now, let us solve this problem by gradient Monte Carlo algorithm. First of all, we define the environment of this problem.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># # of states except for terminal states</span></div><div class="line">N_STATES = <span class="number">1000</span></div><div class="line"></div><div class="line"><span class="comment"># true state values, just a promising guess</span></div><div class="line">trueStateValues = np.arange(<span class="number">-1001</span>, <span class="number">1003</span>, <span class="number">2</span>) / <span class="number">1001.0</span></div><div class="line"></div><div class="line"><span class="comment"># all states</span></div><div class="line">states = np.arange(<span class="number">1</span>, N_STATES + <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># start from a central state</span></div><div class="line">START_STATE = <span class="number">500</span></div><div class="line"></div><div class="line"><span class="comment"># terminal states</span></div><div class="line">END_STATES = [<span class="number">0</span>, N_STATES + <span class="number">1</span>]</div><div class="line"></div><div class="line"><span class="comment"># possible actions</span></div><div class="line">ACTION_LEFT = <span class="number">-1</span></div><div class="line">ACTION_RIGHT = <span class="number">1</span></div><div class="line">ACTIONS = [ACTION_LEFT, ACTION_RIGHT]</div><div class="line"></div><div class="line"><span class="comment"># maximum stride for an action</span></div><div class="line">STEP_RANGE = <span class="number">100</span></div></pre></td></tr></table></figure><p>We need a true value of each state, thus use the dynamic programming to get these value:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Dynamic programming to find the true state values, based on the promising guess above</span></div><div class="line"><span class="comment"># Assume all rewards are 0, given that we have already given value -1 and 1 to terminal states</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    oldTrueStateValues = np.copy(trueStateValues)</div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states:</div><div class="line">        trueStateValues[state] = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, STEP_RANGE + <span class="number">1</span>):</div><div class="line">                step *= action</div><div class="line">                newState = state + step</div><div class="line">                newState = max(min(newState, N_STATES + <span class="number">1</span>), <span class="number">0</span>)</div><div class="line">                <span class="comment"># asynchronous update for faster convergence</span></div><div class="line">                trueStateValues[state] += <span class="number">1.0</span> / (<span class="number">2</span> * STEP_RANGE) * trueStateValues[newState]</div><div class="line">    error = np.sum(np.abs(oldTrueStateValues - trueStateValues))</div><div class="line">    print(error)</div><div class="line">    <span class="keyword">if</span> error &lt; <span class="number">1e-2</span>:</div><div class="line">        <span class="keyword">break</span></div><div class="line"><span class="comment"># correct the state value for terminal states to 0</span></div><div class="line">trueStateValues[<span class="number">0</span>] = trueStateValues[<span class="number">-1</span>] = <span class="number">0</span></div></pre></td></tr></table></figure><p>The policy of episodes generation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take an @action at @state, return new state and reward for this transition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(state, action)</span>:</span></div><div class="line">    step = np.random.randint(<span class="number">1</span>, STEP_RANGE + <span class="number">1</span>)</div><div class="line">    step *= action</div><div class="line">    state += step</div><div class="line">    state = max(min(state, N_STATES + <span class="number">1</span>), <span class="number">0</span>)</div><div class="line">    <span class="keyword">if</span> state == <span class="number">0</span>:</div><div class="line">        reward = <span class="number">-1</span></div><div class="line">    <span class="keyword">elif</span> state == N_STATES + <span class="number">1</span>:</div><div class="line">        reward = <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        reward = <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> state, reward</div></pre></td></tr></table></figure><p>The reward after take an action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get an action, following random policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span></div></pre></td></tr></table></figure><p>And we have a special value function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># a wrapper class for aggregation value function</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValueFunction</span>:</span></div><div class="line">    <span class="comment"># @numOfGroups: # of aggregations</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, numOfGroups)</span>:</span></div><div class="line">        self.numOfGroups = numOfGroups</div><div class="line">        self.groupSize = N_STATES // numOfGroups</div><div class="line"></div><div class="line">        <span class="comment"># thetas</span></div><div class="line">        self.params = np.zeros(numOfGroups)</div><div class="line"></div><div class="line">    <span class="comment"># get the value of @state</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="keyword">if</span> state <span class="keyword">in</span> END_STATES:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        groupIndex = (state - <span class="number">1</span>) // self.groupSize</div><div class="line">        <span class="keyword">return</span> self.params[groupIndex]</div><div class="line"></div><div class="line">    <span class="comment"># update parameters</span></div><div class="line">    <span class="comment"># @delta: step size * (target - old estimation)</span></div><div class="line">    <span class="comment"># @state: state of current sample</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, delta, state)</span>:</span></div><div class="line">        groupIndex = (state - <span class="number">1</span>) // self.groupSize</div><div class="line">        self.params[groupIndex] += delta</div></pre></td></tr></table></figure><p>And the gradient MC algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># gradient Monte Carlo algorithm</span></div><div class="line"><span class="comment"># @valueFunction: an instance of class ValueFunction</span></div><div class="line"><span class="comment"># @alpha: step size</span></div><div class="line"><span class="comment"># @distribution: array to store the distribution statistics</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientMonteCarlo</span><span class="params">(valueFunction, alpha, distribution=None)</span>:</span></div><div class="line">    currentState = START_STATE</div><div class="line">    trajectory = [currentState]</div><div class="line"></div><div class="line">    <span class="comment"># We assume gamma = 1, so return is just the same as the latest reward</span></div><div class="line">    reward = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState <span class="keyword">not</span> <span class="keyword">in</span> END_STATES:</div><div class="line">        action = getAction()</div><div class="line">        newState, reward = takeAction(currentState, action)</div><div class="line">        trajectory.append(newState)</div><div class="line">        currentState = newState</div><div class="line"></div><div class="line">    <span class="comment"># Gradient update for each state in this trajectory</span></div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> trajectory[:<span class="number">-1</span>]:</div><div class="line">        delta = alpha * (reward - valueFunction.value(state))</div><div class="line">        valueFunction.update(delta, state)</div><div class="line">        <span class="keyword">if</span> distribution <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            distribution[state] += <span class="number">1</span></div></pre></td></tr></table></figure><p>Finally. let us solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">nEpisodes = int(<span class="number">1e5</span>)</div><div class="line">alpha = <span class="number">2e-5</span></div><div class="line"></div><div class="line"><span class="comment"># we have 10 aggregations in this example, each has 100 states</span></div><div class="line">valueFunction = ValueFunction(<span class="number">10</span>)</div><div class="line">distribution = np.zeros(N_STATES + <span class="number">2</span>)</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">    print(<span class="string">'episode:'</span>, episode)</div><div class="line">    gradientMonteCarlo(valueFunction, alpha, distribution)</div><div class="line"></div><div class="line">distribution /= np.sum(distribution)</div><div class="line">stateValues = [valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states]</div><div class="line"></div><div class="line">plt.figure(<span class="number">0</span>)</div><div class="line">plt.plot(states, stateValues, label=<span class="string">'Approximate MC value'</span>)</div><div class="line">plt.plot(states, trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'True value'</span>)</div><div class="line">plt.xlabel(<span class="string">'State'</span>)</div><div class="line">plt.ylabel(<span class="string">'Value'</span>)</div><div class="line">plt.legend()</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>)</div><div class="line">plt.plot(states, distribution[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'State distribution'</span>)</div><div class="line">plt.xlabel(<span class="string">'State'</span>)</div><div class="line">plt.ylabel(<span class="string">'Distribution'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_1_1.png" alt="distribution"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_1_2.png" alt="state_value"></p><h3 id="Semi-gradient-Methods"><a href="#Semi-gradient-Methods" class="headerlink" title="Semi-gradient Methods"></a>Semi-gradient Methods</h3><p>Bootstrapping target such as n-step returns $G_{t:t+n}$ or the DP target $\sum_{a, s^{\prime}, r} \pi(a|S_t)p(s^{\prime}, r|S_t, a)[r + \gamma \hat{v}(s^{\prime}, \mathbf{w}_t)]$ all depend on the current value of the weight vector $\mathbf{w}_t$, which implies that they will be biased and that they will not produce a true gradient=descent method. We call them <em>semi-gradient methods</em>.</p><p>Although semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear case. Moreover, they oﬀer important advantages which makes them often clearly preferred. One reason for this is that they are typically signiﬁcantly faster to learn. Another is that they enable learning to be continual and online, without waiting for the end of an episode. This enables them to be used on continuing problems and provides computational advantages. A prototypical semi-gradient method is semi-gradient TD(0), which uses $U_t \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w})$ as its target. Complete pseudocode for this method is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/semi_grad_td.png" alt="semi_grad_td"></p><h3 id="Linear-Methods"><a href="#Linear-Methods" class="headerlink" title="Linear Methods"></a>Linear Methods</h3><p>One of the most important special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot, \mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$. Corresponding to every state $s$, there is a real-valued vector of features $\mathbf{x}(s) \doteq (x_1(s),x_2(s),\cdots,x_d(s))^{\top}$, with the same number of components as $\mathbf{w}$. However the features are constructed, the approximate state-value function is given by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:<br>$$<br>\hat{v}(s, \mathbf{w}) \doteq \mathbf{w^{\top}x}(s) \doteq \sum_{i=1}^d w_i x_i(s).<br>$$<br>The individual functions $x_i:\mathcal{S} \rightarrow \mathbb{E}$ are called <strong>basis functions</strong>. The gradient of the approximate value function with respect to $\mathbf{w}$ in this case is<br>$$<br>\nabla \hat{v} (s, \mathbf{w}) = \mathbf{x}(s).<br>$$<br>The update at each time $t$ is</p><p>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t + \alpha \Big( R_{t+1} + \gamma \mathbf{w}_t^{\top}\mathbf{x}_{t+1} - \mathbf{w}_t^{\top}\mathbf{x}_{t}\Big)\mathbf{x}_t \\<br>&amp;= \mathbf{w}_t + \alpha \Big( R_{t+1}\mathbf{x}_t - \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \mathbf{w}_t\Big),<br>\end{align}<br>$$<br>where here we have used the notational shorthand $\mathbf{x}_t = \mathbf{x}(S_t)$. If the system converges, it must converge to the weight vector $\mathbf{w}_{TD}$ at which<br>$$<br>\mathbf{w}_{TD} \doteq \mathbf{A^{-1}b},<br>$$<br>where<br>$$<br>\mathbf{b} \doteq \mathbb{E}[R_{t+1}\mathbf{x}_t] \in \mathbb{R}^d \;\; \text{and} \;\; \mathbf{A} \doteq \mathbb{E}\big[ \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \big] \in \mathbb{R}^d \times \mathbb{R}^d.<br>$$<br>This quantity is called the TD <strong>fixedpoint</strong>. At this point we have:<br>$$<br>\text{MSVE}(\mathbf{w}_{TD}) \leq \frac{1}{1 - \gamma} \min_{\mathbf{w}} \text{MSVE}(\mathbf{w}).<br>$$<br>Now we use the state aggregation example again, but use the semi-gradient TD method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># semi-gradient n-step TD algorithm</span></div><div class="line"><span class="comment"># @valueFunction: an instance of class ValueFunction</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="comment"># @alpha: step size</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">semiGradientTemporalDifference</span><span class="params">(valueFunction, n, alpha)</span>:</span></div><div class="line">    <span class="comment"># initial starting state</span></div><div class="line">    currentState = START_STATE</div><div class="line"></div><div class="line">    <span class="comment"># arrays to store states and rewards for an episode</span></div><div class="line">    <span class="comment"># space isn't a major consideration, so I didn't use the mod trick</span></div><div class="line">    states = [currentState]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># choose an action randomly</span></div><div class="line">            action = getAction()</div><div class="line">            newState, reward = takeAction(currentState, action)</div><div class="line"></div><div class="line">            <span class="comment"># store new state and new reward</span></div><div class="line">            states.append(newState)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newState <span class="keyword">in</span> END_STATES:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += rewards[t]</div><div class="line">            <span class="comment"># add state value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += valueFunction.value(states[updateTime + n])</div><div class="line">            stateToUpdate = states[updateTime]</div><div class="line">            <span class="comment"># update the value function</span></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> stateToUpdate <span class="keyword">in</span> END_STATES:</div><div class="line">                delta = alpha * (returns - valueFunction.value(stateToUpdate))</div><div class="line">                valueFunction.update(delta, stateToUpdate)</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentState = newState</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">nEpisodes = int(<span class="number">1e5</span>)</div><div class="line">    alpha = <span class="number">2e-4</span></div><div class="line">    valueFunction = ValueFunction(<span class="number">10</span>)</div><div class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        print(<span class="string">'episode:'</span>, episode)</div><div class="line">        semiGradientTemporalDifference(valueFunction, <span class="number">1</span>, alpha)</div><div class="line"></div><div class="line">    stateValues = [valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states]</div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    plt.plot(states, stateValues, label=<span class="string">'Approximate TD value'</span>)</div><div class="line">    plt.plot(states, trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'True value'</span>)</div><div class="line">    plt.xlabel(<span class="string">'State'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Value'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_2_1.png" alt="semi_gradient_td"></p><p>We also could use the <a href="https://ewanlee.github.io/2017/07/04/n-step-TD/" target="_blank" rel="external">n-step semi-gradient TD method</a>. To obtain such quantitatively similar results we switched the state aggregation to 20 groups of 50 states each. The 20 groups are then quantitatively close to the 19 states of the tabular problem.</p><p>The semi-gradient n-step TD algorithm we used in this example is the natural extension of the tabular n-step TD algorithm. The key equation is<br>$$<br>\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \Big[ G_{t:t+n} - \hat{v}(S_t, \mathbf{w}_{t+n-1})\Big] \nabla \hat{v}(S_t, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T,<br>$$<br>where<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{v}(S_{t+n}, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T-n.<br>$$<br>Pseudocode for the complete algorithm is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/n_step_semi_gradient_td.png" alt="n_step_semi_gradient_td"></p><p>Now let us show the performance of different value of n:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># truncate value for better display</span></div><div class="line">truncateValue = <span class="number">0.55</span></div><div class="line"></div><div class="line"><span class="comment"># all possible steps</span></div><div class="line">steps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># all possible alphas</span></div><div class="line">alphas = np.arange(<span class="number">0</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="comment"># each run has 10 episodes</span></div><div class="line">episodes = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># perform 100 independent runs</span></div><div class="line">runs = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># track the errors for each (step, alpha) combination</span></div><div class="line">errors = np.zeros((len(steps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> stepInd, step <span class="keyword">in</span> zip(range(len(steps)), steps):</div><div class="line">        <span class="keyword">for</span> alphaInd, alpha <span class="keyword">in</span> zip(range(len(alphas)), alphas):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'step:'</span>, step, <span class="string">'alpha:'</span>, alpha)</div><div class="line">            <span class="comment"># we have 20 aggregations in this example</span></div><div class="line">            valueFunction = ValueFunction(<span class="number">20</span>)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                semiGradientTemporalDifference(valueFunction, step, alpha)</div><div class="line">                <span class="comment"># calculate the RMS error</span></div><div class="line">                currentStateValues = np.asarray([valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states])</div><div class="line">                errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], <span class="number">2</span>)) / N_STATES)</div><div class="line"><span class="comment"># take average</span></div><div class="line">errors /= episodes * runs</div><div class="line"><span class="comment"># truncate the error</span></div><div class="line">errors[errors &gt; truncateValue] = truncateValue</div><div class="line">plt.figure(<span class="number">3</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(steps)):</div><div class="line">    plt.plot(alphas, errors[i, :], label=<span class="string">'n = '</span> + str(steps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMS error'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_2_2.png" alt="n_step_semi_gradient_td_compare"></p><h3 id="Feature-Construction-for-Linear-Methods"><a href="#Feature-Construction-for-Linear-Methods" class="headerlink" title="Feature Construction for Linear Methods"></a>Feature Construction for Linear Methods</h3><p>Choosing features appropriate to the task is an important way of adding prior domain knowledge to reinforcement learning systems. Intuitively, the features should correspond to the natural features of the task, those along which generalization is most appropriate. If we are valuing geometric objects, for example, we might want to have features for each possible shape, color, size, or function. If we are valuing states of a mobile robot, then we might want to have features for locations, degrees of remaining battery power, recent sonar readings, and so on.</p><h4 id="Polynomials"><a href="#Polynomials" class="headerlink" title="Polynomials"></a>Polynomials</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/poly_feat.png" alt="poly"></p><h4 id="Fourier-Basis"><a href="#Fourier-Basis" class="headerlink" title="Fourier Basis"></a>Fourier Basis</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/fourier.png" alt="fourier"></p><p>Konidaris et al. (2011) found that when using Fourier cosine basis functions with a learning algorithm such as semi-gradient TD(0), or semi-gradient Sarsa, it is helpful to use a diﬀerent step-size parameter for each basis function. If $\alpha$ is the basic step-size parameter, they suggest setting the step-size parameter for basis function $x_i$ to $a_i=\alpha/\sqrt{(c_1^i)^2 + \cdots + (c_d^i)^2}$ (except when each $c_j^i=0$, in which case $\alpha_i=\alpha$).</p><p>Now, let us we compare the Fourier and polynomial bases on the 1000-state random walk example. <strong>In general, we do not recommend using the polynomial basis for online learning.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># a wrapper class for polynomial / Fourier -based value function</span></div><div class="line">POLYNOMIAL_BASES = <span class="number">0</span></div><div class="line">FOURIER_BASES = <span class="number">1</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasesValueFunction</span>:</span></div><div class="line">    <span class="comment"># @order: # of bases, each function also has one more constant parameter (called bias in machine learning)</span></div><div class="line">    <span class="comment"># @type: polynomial bases or Fourier bases</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, order, type)</span>:</span></div><div class="line">        self.order = order</div><div class="line">        self.weights = np.zeros(order + <span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># set up bases function</span></div><div class="line">        self.bases = []</div><div class="line">        <span class="keyword">if</span> type == POLYNOMIAL_BASES:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, order + <span class="number">1</span>):</div><div class="line">                self.bases.append(<span class="keyword">lambda</span> s, i=i: pow(s, i))</div><div class="line">        <span class="keyword">elif</span> type == FOURIER_BASES:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, order + <span class="number">1</span>):</div><div class="line">                self.bases.append(<span class="keyword">lambda</span> s, i=i: np.cos(i * np.pi * s))</div><div class="line"></div><div class="line">    <span class="comment"># get the value of @state</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="comment"># map the state space into [0, 1]</span></div><div class="line">        state /= float(N_STATES)</div><div class="line">        <span class="comment"># get the feature vector</span></div><div class="line">        feature = np.asarray([func(state) <span class="keyword">for</span> func <span class="keyword">in</span> self.bases])</div><div class="line">        <span class="keyword">return</span> np.dot(self.weights, feature)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, delta, state)</span>:</span></div><div class="line">        <span class="comment"># map the state space into [0, 1]</span></div><div class="line">        state /= float(N_STATES)</div><div class="line">        <span class="comment"># get derivative value</span></div><div class="line">        derivativeValue = np.asarray([func(state) <span class="keyword">for</span> func <span class="keyword">in</span> self.bases])</div><div class="line">        self.weights += delta * derivativeValue</div></pre></td></tr></table></figure><p>The function upper is used to construction the features of states (map states to features).</p><p>Next, we will compare different super-parameters’ (order) performance:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">1</span></div><div class="line"></div><div class="line">episodes = <span class="number">5000</span></div><div class="line"></div><div class="line"><span class="comment"># # of bases</span></div><div class="line">orders = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>]</div><div class="line"></div><div class="line">alphas = [<span class="number">1e-4</span>, <span class="number">5e-5</span>]</div><div class="line">labels = [[<span class="string">'polynomial basis'</span>] * <span class="number">3</span>, [<span class="string">'fourier basis'</span>] * <span class="number">3</span>]</div><div class="line"></div><div class="line"><span class="comment"># track errors for each episode</span></div><div class="line">errors = np.zeros((len(alphas), len(orders), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(orders)):</div><div class="line">        valueFunctions = [BasesValueFunction(orders[i], POLYNOMIAL_BASES), BasesValueFunction(orders[i], FOURIER_BASES)]</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">            <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'order:'</span>, orders[i], labels[j][i], <span class="string">'episode:'</span>, episode)</div><div class="line"></div><div class="line">                <span class="comment"># gradient Monte Carlo algorithm</span></div><div class="line">                gradientMonteCarlo(valueFunctions[j], alphas[j])</div><div class="line"></div><div class="line">                <span class="comment"># get state values under current value function</span></div><div class="line">                stateValues = [valueFunctions[j].value(state) <span class="keyword">for</span> state <span class="keyword">in</span> states]</div><div class="line"></div><div class="line">                <span class="comment"># get the root-mean-squared error</span></div><div class="line">                errors[j, i, episode] += np.sqrt(np.mean(np.power(trueStateValues[<span class="number">1</span>: <span class="number">-1</span>] - stateValues, <span class="number">2</span>)))</div><div class="line"></div><div class="line"><span class="comment"># average over independent runs</span></div><div class="line">errors /= runs</div><div class="line"></div><div class="line">plt.figure(<span class="number">5</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(orders)):</div><div class="line">        plt.plot(errors[i, j, :], label=labels[i][j]+<span class="string">' order = '</span> + str(orders[j]))</div><div class="line">plt.xlabel(<span class="string">'Episodes'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMSVE'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/poly_vs_four.png" alt="poly_vs_four"></p><h3 id="TODO-TILE-CODING"><a href="#TODO-TILE-CODING" class="headerlink" title="TODO: TILE CODING"></a>TODO: TILE CODING</h3></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/04/n-step-TD/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/04/n-step-TD/" itemprop="url">n-step TD</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-04T11:16:06+08:00">2017-07-04 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/04/n-step-TD/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/04/n-step-TD/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/04/n-step-TD/" class="leancloud_visitors" data-flag-title="n-step TD"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In this post, we unify the Monte Carlo methods and the one-step TD methods. We first consider the prediction problem.</p><h3 id="n-step-TD-Prediction"><a href="#n-step-TD-Prediction" class="headerlink" title="n-step TD Prediction"></a>n-step TD Prediction</h3><p>Monte Carlo methods preform a backup for each state based on the entire sequence of observed rewards from that state until the end of the episode. One-step TD methods is based on just on next reward. So n-step TD methods perform a backup based on an intermediate number of rewards: more than one, but less than all of them until termination.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/ntd/nsteptd.png" alt="nstep_td"></p><p>More formally, consider the backup applied to state $S_t$ as a result of the state-reward sequence, $S_t, R_{t+1},S_{t+1}, R_{t+2}, \cdots, R_T, S_T$ (omitting the actions for simplicity). We know that in Monte Carlo backups the estimate of $v_{\pi}(S_t)$ updated in the direction of the complete return:<br>$$<br>G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T,<br>$$<br>where $T$ is the last time step of the episode. Let us call this quantity the <strong>target</strong> of the backup. Whereas in Monte Carlo backups the target is the return, in one-step backups the target is the first reward plus the discounted estimated value of the next state, which we call the one-step return:<br>$$<br>G_{t:t+1} \doteq R_{t+1} + \gamma V_t (S_{t+1}),<br>$$<br>where $V_t : \mathcal{S} \rightarrow \mathbb{R}$ here is an estimate at time $t$ of $v_{\pi}$. The subscripts on $G_{t:t+1}$ indicate that it is truncated return for time t using rewards up until time $t+1$. In the one-step return, $\gamma V_t (S_{t+1})$ takes the place of the other terms $ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$ of the full return. Our point now is that this idea makes just as much sense after two steps as it does after one. The target for a two-step backup is the two-step return:<br>$$<br>G_{t:t+2} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1} (S_{t+2}),<br>$$<br>where now $\gamma^2 V_{t+1}(S_{t+2})$ corrects for the absence of the terms $\gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$. Similarly, the target for an arbitrary n-step backup is the n-step return:<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n V_{t+n-1} (S_{t+n}),<br>$$<br>for all $n,t$ such that $n \ge 1$ and $0 \leq t \leq T-n$. If $t+n \ge T$, then all the missing terms are taken as zero, and the n-step return defined to be equal to the ordinary full return.</p><p>No real algorithm can use the n-step return until after it has seen $R_{t+n}$ and computed $V_{t+n-1}$. The first time these are available is $t+n$. The natural algorithm state-value learning algorithm for using n-step returns is thus<br>$$<br>V_{t+n}(S_t) \doteq V_{t+n-1}(S_t) + \alpha [G_{t:t+n} - V_{t+n-1}(S_t)], \;\;\;\;\;\; 0 \leq t \leq T<br>$$<br>while the values of all other states remain unchanged. Note that no changes at all are made during the first $n-1$ steps of each episode. Complete pseudocode is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/ntd/nstep_td_alg.png" alt="alg_box"></p><p>The worst error of the expected n-step return is guaranteed to be less than or equal to $\gamma^n$ times the worst error under $V_{t+n-1}$:<br>$$<br>\max_s \left |\mathbb{E}[G_{t:t+1}|S_t=s] - v_{\pi}(s) \right | \leq \gamma^n \max_s |V_{t+n-1}(s) - v_{\pi}(s)|,<br>$$<br>for all $n \geq 1$. This is called the <strong>error reduction property</strong> of n-step returns. The n-step TD methods thus form a family of sound methods, with one-step TD methods and Monte Carlo methods as extreme members.</p><h4 id="Example-n-step-TD-Methods-on-the-Random-Walk"><a href="#Example-n-step-TD-Methods-on-the-Random-Walk" class="headerlink" title="Example:  n-step TD Methods on the Random Walk"></a>Example: n-step TD Methods on the Random Walk</h4><p>Now we have a larger MDP (19 non-terminal states). First of all we need to define the new environment:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all states</span></div><div class="line">N_STATES = <span class="number">19</span></div><div class="line"></div><div class="line"><span class="comment"># discount</span></div><div class="line">GAMMA = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># initial state values</span></div><div class="line">stateValues = np.zeros(N_STATES + <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># all states but terminal states</span></div><div class="line">states = np.arange(<span class="number">1</span>, N_STATES + <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># start from the middle state</span></div><div class="line">START_STATE = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># two terminal states</span></div><div class="line"><span class="comment"># an action leading to the left terminal state has reward -1</span></div><div class="line"><span class="comment"># an action leading to the right terminal state has reward 1</span></div><div class="line">END_STATES = [<span class="number">0</span>, N_STATES + <span class="number">1</span>]</div><div class="line"></div><div class="line"><span class="comment"># true state value from bellman equation</span></div><div class="line">realStateValues = np.arange(<span class="number">-20</span>, <span class="number">22</span>, <span class="number">2</span>) / <span class="number">20.0</span></div><div class="line">realStateValues[<span class="number">0</span>] = realStateValues[<span class="number">-1</span>] = <span class="number">0</span></div></pre></td></tr></table></figure><p>And then develop the n-step TD algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># n-steps TD method</span></div><div class="line"><span class="comment"># @stateValues: values for each state, will be updated</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="comment"># @alpha: # step size</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">temporalDifference</span><span class="params">(stateValues, n, alpha)</span>:</span></div><div class="line">    <span class="comment"># initial starting state</span></div><div class="line">    currentState = START_STATE</div><div class="line"></div><div class="line">    <span class="comment"># arrays to store states and rewards for an episode</span></div><div class="line">    <span class="comment"># space isn't a major consideration, so I didn't use the mod trick</span></div><div class="line">    states = [currentState]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># choose an action randomly</span></div><div class="line">            <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">                newState = currentState + <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                newState = currentState - <span class="number">1</span></div><div class="line">            <span class="keyword">if</span> newState == <span class="number">0</span>:</div><div class="line">                reward = <span class="number">-1</span></div><div class="line">            <span class="keyword">elif</span> newState == <span class="number">20</span>:</div><div class="line">                reward = <span class="number">1</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                reward = <span class="number">0</span></div><div class="line"></div><div class="line">            <span class="comment"># store new state and new reward</span></div><div class="line">            states.append(newState)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newState <span class="keyword">in</span> END_STATES:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += pow(GAMMA, t - updateTime - <span class="number">1</span>) * rewards[t]</div><div class="line">            <span class="comment"># add state value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += pow(GAMMA, n) * stateValues[states[(updateTime + n)]]</div><div class="line">            stateToUpdate = states[updateTime]</div><div class="line">            <span class="comment"># update the state value</span></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> stateToUpdate <span class="keyword">in</span> END_STATES:</div><div class="line">                stateValues[stateToUpdate] += alpha * (returns - stateValues[stateToUpdate])</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentState = newState</div></pre></td></tr></table></figure><p>Now, let us test the performance under different $n$ values and $\alpha$ values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># truncate value for better display</span></div><div class="line">truncateValue = <span class="number">0.55</span></div><div class="line"></div><div class="line"><span class="comment"># all possible steps</span></div><div class="line">steps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># all possible alphas</span></div><div class="line">alphas = np.arange(<span class="number">0</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="comment"># each run has 10 episodes</span></div><div class="line">episodes = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># perform 100 independent runs</span></div><div class="line">runs = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># track the errors for each (step, alpha) combination</span></div><div class="line">errors = np.zeros((len(steps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> stepInd, step <span class="keyword">in</span> zip(range(len(steps)), steps):</div><div class="line">        <span class="keyword">for</span> alphaInd, alpha <span class="keyword">in</span> zip(range(len(alphas)), alphas):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'step:'</span>, step, <span class="string">'alpha:'</span>, alpha)</div><div class="line">            currentStateValues = np.copy(stateValues)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                temporalDifference(currentStateValues, step, alpha)</div><div class="line">                <span class="comment"># calculate the RMS error</span></div><div class="line">                errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - realStateValues, <span class="number">2</span>)) / N_STATES)</div><div class="line"><span class="comment"># take average</span></div><div class="line">errors /= episodes * runs</div><div class="line"><span class="comment"># truncate the error</span></div><div class="line">errors[errors &gt; truncateValue] = truncateValue</div><div class="line">plt.figure()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(steps)):</div><div class="line">    plt.plot(alphas, errors[i, :], label=<span class="string">'n = '</span> + str(steps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMS error'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/ntd/n_step_td_random_walk.png" alt="n_step_td_random_walk_result"></p><h3 id="TODO-N-STEP-SARSA"><a href="#TODO-N-STEP-SARSA" class="headerlink" title="TODO: N-STEP SARSA"></a>TODO: N-STEP SARSA</h3><h3 id="TODO-N-STEP-OFF-POLICY-ALGORITHM"><a href="#TODO-N-STEP-OFF-POLICY-ALGORITHM" class="headerlink" title="TODO: N-STEP OFF-POLICY ALGORITHM"></a>TODO: N-STEP OFF-POLICY ALGORITHM</h3></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/02/Temporal-Difference-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/02/Temporal-Difference-Learning/" itemprop="url">Temporal-Difference Learning</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-02T12:44:00+08:00">2017-07-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/02/Temporal-Difference-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/02/Temporal-Difference-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/02/Temporal-Difference-Learning/" class="leancloud_visitors" data-flag-title="Temporal-Difference Learning"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be <em>temporal-difference</em> (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome.</p><h3 id="TD-0"><a href="#TD-0" class="headerlink" title="TD(0)"></a><strong>TD(0)</strong></h3><p>Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environment is<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)],<br>$$<br>where $G_t$ is the <strong>actual return</strong> following time $t$. Let us call this method $constant\text{-}\alpha \ MC$. Notice that, if we are in a stationary environment (like <a href="https://ewanlee.github.io/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" target="_blank" rel="external">earlier</a>. For some reason, don’t use incremental implementation), the $\alpha$ is equals to $\frac{1}{N(S_t)}$. whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t+1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]<br>$$<br>immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_t$, whereas the target for the TD update is $R_{t+1} + \gamma V(S_{t+1})$. This TD method is called $TD(0)$, or <strong>one-step</strong> TD. The box below specifies TD(0) completely in procedural form.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td_0.png" alt="td_0"></p><p>TD(0)’s backup diagram is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/td0_bg.png" alt="td0bg"></p><p>Because the TD(0) bases its update in part on an existing estimate, we say that it is a <em>bootstrapping</em> method, like DP. We know that<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [G_t \ | \ S_t=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} \ | \ S_t=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_t=s].<br>\end{align}<br>$$<br>Roughly speaking, Monte Carlo methods use an estimate of (3) as a target, whereas DP methods use an estimate of (5) as a target, The Monte Carlo target is an estimate because the expected value in (3) is not known; a sample return is used in place of the real expected return. The DP target is an estimate not because of the excepted value, which are assumed to be completely provided by a model of the environment (the environment is known for the DP methods), but because $v_{\pi}(S_{t+1})$ is not known and the current estimate, $V(S_{t+1})$, is used instead. The TD target is an estimate for both reasons.</p><p>Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \gamma V(S_{t+1})$. This quantity, called the <strong>TD error</strong>, arises in various forms throughout reinforcement learning:<br>$$<br>\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t).<br>$$<br>Notice that the TD error at each time is the error in the estimate <strong>made at that time</strong>. Because the TD error depends on the next state and the next reward, it is not actually available until one time step later. Also note that if the array $V$ does not change during the episode (as it does not in Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:<br>$$<br>\begin{align}<br>G_t - V(S_t) &amp;= R_{t+1} + \gamma G(S_{t+1}) - V(S_t) + \gamma V(S_{t+1} ) - \gamma V(S_{t+1}) \\<br>&amp;= \delta_t + \gamma (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(G_t-V(S_T)) \\<br>&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(0 -0) \\<br>&amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k.<br>\end{align}<br>$$<br>This identity is not exact if $V$ is updated during the episode (as it is in TD(0)), but if the step size is small then it may still hold approximately. Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning.</p><h4 id="Example-Random-walk"><a href="#Example-Random-walk" class="headerlink" title="Example: Random walk"></a><strong>Example: Random walk</strong></h4><p>In this example we empirically compare the prediction abilities of TD(0) and constant-$\alpha$ MC applied to the small Markov reward process shown in the upper part of the figure below. All episodes start in the center state, <strong>C</strong>, and the proceed either left or right by one state on each step, with equal probability. This behavior can be thought of as due to the combined effect of a fixed policy and an environment’s state-transition probabilities, but we do not care which; we are concerned only with predicting returns however they are generated. Episodes terminates on the right, a reward of +1 occurs; all other reward are zero. For example, a typical episode might consist of the following state-and-reward sequence: <strong>C, 0, B, 0, C, 0, D, 0, E, 1.</strong> Because this task is undiscounted, the true value of each state is the probability of terminating on the right if starting from that state. Thus, the true value of the center state is $v_{\pi}(\text{C}) = 0.5$. The true values of all the states, <strong>A</strong> through <strong>E</strong>, are $\frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}$, and $\frac{5}{6}$. In all cases the approximate value function was initialized to the intermediate value $V(s)=0.5$, for all $s$.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk.png" alt="random_walk"></p><p>Now, let us develop the codes to solve problem.</p><p>The first, we initialize some truth.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 0 is the left terminal state</span></div><div class="line"><span class="comment"># 6 is the right terminal state</span></div><div class="line"><span class="comment"># 1 ... 5 represents A ... E</span></div><div class="line">states = np.zeros(<span class="number">7</span>)</div><div class="line">states[<span class="number">1</span>:<span class="number">6</span>] = <span class="number">0.5</span></div><div class="line"><span class="comment"># For convenience, we assume all rewards are 0</span></div><div class="line"><span class="comment"># and the left terminal state has value 0, the right terminal state has value 1</span></div><div class="line"><span class="comment"># This trick has been used in Gambler's Problem</span></div><div class="line">states[<span class="number">6</span>] = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># set up true state values</span></div><div class="line">trueValue = np.zeros(<span class="number">7</span>)</div><div class="line">trueValue[<span class="number">1</span>:<span class="number">6</span>] = np.arange(<span class="number">1</span>, <span class="number">6</span>) / <span class="number">6.0</span></div><div class="line">trueValue[<span class="number">6</span>] = <span class="number">1</span></div><div class="line"></div><div class="line">ACTION_LEFT = <span class="number">0</span></div><div class="line">ACTION_RIGHT = <span class="number">1</span></div></pre></td></tr></table></figure><p>The below box is the TD(0) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">temporalDifference</span><span class="params">(states, alpha=<span class="number">0.1</span>, batch=False)</span>:</span></div><div class="line">    state = <span class="number">3</span></div><div class="line">    trajectory = [state]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        oldState = state</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == ACTION_LEFT:</div><div class="line">            state -= <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state += <span class="number">1</span></div><div class="line">        <span class="comment"># Assume all rewards are 0</span></div><div class="line">        reward = <span class="number">0</span></div><div class="line">        trajectory.append(state)</div><div class="line">        <span class="comment"># TD update</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> batch:</div><div class="line">            states[oldState] += alpha * (reward + states[state] - states[oldState])</div><div class="line">        <span class="keyword">if</span> state == <span class="number">6</span> <span class="keyword">or</span> state == <span class="number">0</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        rewards.append(reward)</div><div class="line">    <span class="keyword">return</span> trajectory, rewards</div></pre></td></tr></table></figure><p>And below box is the constant-$\alpha$ Monte Carlo algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarlo</span><span class="params">(states, alpha=<span class="number">0.1</span>, batch=False)</span>:</span></div><div class="line">    state = <span class="number">3</span></div><div class="line">    trajectory = [<span class="number">3</span>]</div><div class="line">    <span class="comment"># if end up with left terminal state, all returns are 0</span></div><div class="line">    <span class="comment"># if end up with right terminal state, all returns are 1</span></div><div class="line">    returns = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == ACTION_LEFT:</div><div class="line">            state -= <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state += <span class="number">1</span></div><div class="line">        trajectory.append(state)</div><div class="line">        <span class="keyword">if</span> state == <span class="number">6</span>:</div><div class="line">            returns = <span class="number">1.0</span></div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">elif</span> state == <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="keyword">break</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> batch:</div><div class="line">        <span class="keyword">for</span> state_ <span class="keyword">in</span> trajectory[:<span class="number">-1</span>]:</div><div class="line">            <span class="comment"># MC update</span></div><div class="line">            states[state_] += alpha * (returns - states[state_])</div><div class="line">    <span class="keyword">return</span> trajectory, [returns] * (len(trajectory) - <span class="number">1</span>)</div></pre></td></tr></table></figure><p>First of all, let us test the performance of the TD(0) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stateValue</span><span class="params">()</span>:</span></div><div class="line">    episodes = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>]</div><div class="line">    currentStates = np.copy(states)</div><div class="line">    plt.figure(<span class="number">1</span>)</div><div class="line">    axisX = np.arange(<span class="number">0</span>, <span class="number">7</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, episodes[<span class="number">-1</span>] + <span class="number">1</span>):</div><div class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> episodes:</div><div class="line">            plt.plot(axisX, currentStates, label=str(i) + <span class="string">' episodes'</span>)</div><div class="line">        temporalDifference(currentStates)</div><div class="line">    plt.plot(axisX, trueValue, label=<span class="string">'true values'</span>)</div><div class="line">    plt.xlabel(<span class="string">'state'</span>)</div><div class="line">    plt.legend()</div><div class="line">    </div><div class="line">stateValue()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_td0.png" alt="random_walk_td0"></p><p>And then let us show the RMS error of the TD(0) algorithm and constant-$\alpha$ Monte Carlo algorithm, for various $\alpha$ values:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMSError</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># I'm lazy here, so do not let same alpha value appear in both arrays</span></div><div class="line">    <span class="comment"># For example, if in TD you want to use alpha = 0.2, then in MC you can use alpha = 0.201</span></div><div class="line">    TDAlpha = [<span class="number">0.15</span>, <span class="number">0.1</span>, <span class="number">0.05</span>]</div><div class="line">    MCAlpha = [<span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.03</span>, <span class="number">0.04</span>]</div><div class="line">    episodes = <span class="number">100</span> + <span class="number">1</span></div><div class="line">    runs = <span class="number">100</span></div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    axisX = np.arange(<span class="number">0</span>, episodes)</div><div class="line">    <span class="keyword">for</span> alpha <span class="keyword">in</span> TDAlpha + MCAlpha:</div><div class="line">        totalErrors = np.zeros(episodes)</div><div class="line">        <span class="keyword">if</span> alpha <span class="keyword">in</span> TDAlpha:</div><div class="line">            method = <span class="string">'TD'</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            method = <span class="string">'MC'</span></div><div class="line">        <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">            errors = []</div><div class="line">            currentStates = np.copy(states)</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                errors.append(np.sqrt(np.sum(np.power(trueValue - currentStates, <span class="number">2</span>)) / <span class="number">5.0</span>))</div><div class="line">                <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                    temporalDifference(currentStates, alpha=alpha)</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    monteCarlo(currentStates, alpha=alpha)</div><div class="line">            totalErrors += np.asarray(errors)</div><div class="line">        totalErrors /= runs</div><div class="line">        plt.plot(axisX, totalErrors, label=method + <span class="string">', alpha='</span> + str(alpha))</div><div class="line">    plt.xlabel(<span class="string">'episodes'</span>)</div><div class="line">    plt.legend()</div><div class="line">    </div><div class="line">RMSError()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/random_walk_rmse.png" alt="random_walk_error"></p><p>We can see, the TD method was consistently better than the MC method on this task.</p><p>Now, suppose that there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning method is to present the experience repeatedly until the method converges upon an answer. We call this <em>batch updating</em>.</p><h4 id="Example-Random-walk-under-batch-updating"><a href="#Example-Random-walk-under-batch-updating" class="headerlink" title="Example: Random walk under batch updating"></a><strong>Example: Random walk under batch updating</strong></h4><p>After each new episodes, all episodes seen so far were treated as a batch. They were repeatedly presented to the algorithm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchUpdating</span><span class="params">(method, episodes, alpha=<span class="number">0.001</span>)</span>:</span></div><div class="line">    <span class="comment"># perform 100 independent runs</span></div><div class="line">    runs = <span class="number">100</span></div><div class="line">    totalErrors = np.zeros(episodes - <span class="number">1</span>)</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        currentStates = np.copy(states)</div><div class="line">        errors = []</div><div class="line">        <span class="comment"># track shown trajectories and reward/return sequences</span></div><div class="line">        trajectories = []</div><div class="line">        rewards = []</div><div class="line">        <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">1</span>, episodes):</div><div class="line">            print(<span class="string">'Run:'</span>, run, <span class="string">'Episode:'</span>, ep)</div><div class="line">            <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                trajectory_, rewards_ = temporalDifference(currentStates, batch=<span class="keyword">True</span>)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                trajectory_, rewards_ = monteCarlo(currentStates, batch=<span class="keyword">True</span>)</div><div class="line">            trajectories.append(trajectory_)</div><div class="line">            rewards.append(rewards_)</div><div class="line">            <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">                <span class="comment"># keep feeding our algorithm with trajectories seen so far until state value function converges</span></div><div class="line">                updates = np.zeros(<span class="number">7</span>)</div><div class="line">                <span class="keyword">for</span> trajectory_, rewards_ <span class="keyword">in</span> zip(trajectories, rewards):</div><div class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(trajectory_) - <span class="number">1</span>):</div><div class="line">                        <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                            updates[trajectory_[i]] += rewards_[i] + currentStates[trajectory_[i + <span class="number">1</span>]] - currentStates[trajectory_[i]]</div><div class="line">                        <span class="keyword">else</span>:</div><div class="line">                            updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]]</div><div class="line">                updates *= alpha</div><div class="line">                <span class="keyword">if</span> np.sum(np.abs(updates)) &lt; <span class="number">1e-3</span>:</div><div class="line">                    <span class="keyword">break</span></div><div class="line">                <span class="comment"># perform batch updating</span></div><div class="line">                currentStates += updates</div><div class="line">            <span class="comment"># calculate rms error</span></div><div class="line">            errors.append(np.sqrt(np.sum(np.power(currentStates - trueValue, <span class="number">2</span>)) / <span class="number">5.0</span>))</div><div class="line">        totalErrors += np.asarray(errors)</div><div class="line">    totalErrors /= runs</div><div class="line">    <span class="keyword">return</span> totalErrors</div></pre></td></tr></table></figure><p>Notice that the core codes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep feeding our algorithm with trajectories seen so far until state</span></div><div class="line">    <span class="comment"># value function converges</span></div><div class="line">    updates = np.zeros(<span class="number">7</span>)</div><div class="line">    <span class="keyword">for</span> trajectory_, rewards_ <span class="keyword">in</span> zip(trajectories, rewards):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(trajectory_) - <span class="number">1</span>):</div><div class="line">            <span class="keyword">if</span> method == <span class="string">'TD'</span>:</div><div class="line">                updates[trajectory_[i]] += rewards_[i] + \</div><div class="line">                    currentStates[trajectory_[i + <span class="number">1</span>]] - currentStates[trajectory_[i]]</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]]</div><div class="line">    updates *= alpha</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(updates)) &lt; <span class="number">1e-3</span>:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># perform batch updating</span></div><div class="line">    currentStates += updates</div></pre></td></tr></table></figure><p>Either TD methods or MC methods, the target is to minimize the TD error (or MC error, I say).</p><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/batch_update.png" alt="batch_update"></p><p>Under batch training, constant-$\alpha$ MC converges to value, $V(s)$, that are sample averages of the actual returns experienced after visiting each state $s$. These are optimal estimate in the sense that they minimize the mean-squared error from the actual returns in the training set. In this sense it is surprising that the batch TD method was able to perform better according to the root mean-squared error measure shown in the top figure. How is it that batch TD was able to perform better than this optimal methods? Consider the example in below box:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/example_6_4.png" alt="example6_4"></p><p>Example illustrates a general difference between the estimates founds by batch TD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the <strong>certainty-equivalence estimate</strong>.</p><h3 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a><strong>Sarsa</strong></h3><p>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\right]<br>$$</p><p>This update is done after every transition from a nonterminal state $S_t$. If $S_{t+1}$ is terminal, then $Q(S_{t+1}, A_{t+1})$ is defined as zero. This rule uses every element of the quintuple of events, $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that make up a transition from one state-action pair to the next. This quintuple gives rise to the name <em>Sarsa</em> for the algorithm. The backup diagram for Sarsa is as shown to the bottom.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/sarsabg.png" alt="sarsa_bg"></p><p>The general form of the Sarsa control algorithm is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png" alt="sarsa"></p><h4 id="Example-Windy-Gridworld"><a href="#Example-Windy-Gridworld" class="headerlink" title="Example: Windy Gridworld"></a><strong>Example: Windy Gridworld</strong></h4><p>The figure below is a standard grid-world, with start and goal states, but with one diﬀerence: there is a crosswind upward through the middle of the grid. The actions are the standard four—up, down,right, and left—but in the middle region the resultant next states are shifted upward by a “wind,” the strength of which varies from column to column. The strength of the wind is given below each column, in number of cells shifted upward. For example, if you are one cell to the right of the goal, then the action left takes you to the cell just above the goal. Let us treat this as an undiscounted episodic task, with constant rewards of −1 until the goal state is reached.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_gridworld.png" alt="windy_gridworld"></p><p>To demonstrate the problem clearly, we use the <a href="https://gym.openai.com/" target="_blank" rel="external">OpenAI gym</a> toolkit to develop the algorithm.</p><p>First of all, we need to define a environment (the windy grid world):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># represents every action as a integer</span></div><div class="line">UP = <span class="number">0</span></div><div class="line">RIGHT = <span class="number">1</span></div><div class="line">DOWN = <span class="number">2</span></div><div class="line">LEFT = <span class="number">3</span></div></pre></td></tr></table></figure><p>The environment is a class that inherit the gym default class <strong>discrete.DiscreteEnv</strong> (shows that the states are discrete):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">WindyGridworldEnv</span><span class="params">(discrete.DiscreteEnv)</span></span></div></pre></td></tr></table></figure><p>First we need to construct our world:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">    self.shape = (<span class="number">7</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line">    <span class="comment"># the number of all states</span></div><div class="line">    nS = np.prod(self.shape)</div><div class="line">    <span class="comment"># the number of all actions</span></div><div class="line">    nA = <span class="number">4</span></div><div class="line"></div><div class="line">    <span class="comment"># Wind strength</span></div><div class="line">    winds = np.zeros(self.shape)</div><div class="line">    winds[:,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">8</span>]] = <span class="number">1</span></div><div class="line">    winds[:,[<span class="number">6</span>,<span class="number">7</span>]] = <span class="number">2</span></div><div class="line"></div><div class="line">    <span class="comment"># Calculate transition probabilities</span></div><div class="line">    <span class="comment"># P is the transition matrix</span></div><div class="line">    P = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(nS):</div><div class="line">        position = np.unravel_index(s, self.shape)</div><div class="line">        P[s] = &#123; a : [] <span class="keyword">for</span> a <span class="keyword">in</span> range(nA) &#125;</div><div class="line">        P[s][UP] = self._calculate_transition_prob(position, [<span class="number">-1</span>, <span class="number">0</span>], winds)</div><div class="line">        P[s][RIGHT] = self._calculate_transition_prob(position, [<span class="number">0</span>, <span class="number">1</span>], winds)</div><div class="line">        P[s][DOWN] = self._calculate_transition_prob(position, [<span class="number">1</span>, <span class="number">0</span>], winds)</div><div class="line">        P[s][LEFT] = self._calculate_transition_prob(position, [<span class="number">0</span>, <span class="number">-1</span>], winds)</div><div class="line"></div><div class="line">    <span class="comment"># We always start in state (3, 0)</span></div><div class="line">    isd = np.zeros(nS)</div><div class="line">    isd[np.ravel_multi_index((<span class="number">3</span>,<span class="number">0</span>), self.shape)] = <span class="number">1.0</span></div><div class="line"></div><div class="line">    super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)</div></pre></td></tr></table></figure><p>This is natural, uh? Notice that there is a method called <strong>_calculate_transition_prob</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_calculate_transition_prob</span><span class="params">(self, current, delta, winds)</span>:</span></div><div class="line">        new_position = np.array(current) + np.array(delta) + np.array([<span class="number">-1</span>, <span class="number">0</span>]) * winds[tuple(current)]</div><div class="line">        new_position = self._limit_coordinates(new_position).astype(int)</div><div class="line">        new_state = np.ravel_multi_index(tuple(new_position), self.shape)</div><div class="line">        is_done = tuple(new_position) == (<span class="number">3</span>, <span class="number">7</span>)</div><div class="line">        <span class="keyword">return</span> [(<span class="number">1.0</span>, new_state, <span class="number">-1.0</span>, is_done)]</div></pre></td></tr></table></figure><p>and <strong>_limit_corrdinates</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_limit_coordinates</span><span class="params">(self, coord)</span>:</span></div><div class="line">    coord[<span class="number">0</span>] = min(coord[<span class="number">0</span>], self.shape[<span class="number">0</span>] - <span class="number">1</span>)</div><div class="line">    coord[<span class="number">0</span>] = max(coord[<span class="number">0</span>], <span class="number">0</span>)</div><div class="line">    coord[<span class="number">1</span>] = min(coord[<span class="number">1</span>], self.shape[<span class="number">1</span>] - <span class="number">1</span>)</div><div class="line">    coord[<span class="number">1</span>] = max(coord[<span class="number">1</span>], <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> coord</div></pre></td></tr></table></figure><p>It is worth to mention that the default gym environment class has some useful parameters: <strong>nS</strong>, <strong>nA</strong>, <strong>P</strong> and <strong>is_done</strong>. nS is the total number of states and nA is the total number of actions (here assume all states only could take the same fixed actions). P is the state transition matrix, the default environment class has a <strong>step</strong> method (accept a parameter <strong>action</strong>) that could generates episode automatically according the P and is_done that represents whether a state is terminal state or not.</p><p>Finally, we define a output method for pretty show the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_render</span><span class="params">(self, mode=<span class="string">'human'</span>, close=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> close:</div><div class="line">        <span class="keyword">return</span></div><div class="line"></div><div class="line">    outfile = StringIO() <span class="keyword">if</span> mode == <span class="string">'ansi'</span> <span class="keyword">else</span> sys.stdout</div><div class="line"></div><div class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> range(self.nS):</div><div class="line">        position = np.unravel_index(s, self.shape)</div><div class="line">        <span class="comment"># print(self.s)</span></div><div class="line">        <span class="keyword">if</span> self.s == s:</div><div class="line">            output = <span class="string">" x "</span></div><div class="line">        <span class="keyword">elif</span> position == (<span class="number">3</span>,<span class="number">7</span>):</div><div class="line">            output = <span class="string">" T "</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            output = <span class="string">" o "</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> position[<span class="number">1</span>] == <span class="number">0</span>:</div><div class="line">            output = output.lstrip()</div><div class="line">        <span class="keyword">if</span> position[<span class="number">1</span>] == self.shape[<span class="number">1</span>] - <span class="number">1</span>:</div><div class="line">            output = output.rstrip()</div><div class="line">            output += <span class="string">"\n"</span></div><div class="line"></div><div class="line">        outfile.write(output)</div><div class="line">    outfile.write(<span class="string">"\n"</span>)</div></pre></td></tr></table></figure><p>Then, let us test our model：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">env = WindyGridworldEnv()</div><div class="line"></div><div class="line">print(env.reset())</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">2</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/windy_render.png" alt="windy_show"></p><p>Each state transition, the step method return a tuple <strong>(next_state, reward, is_done, some_extra_info)</strong>.</p><p>Next, we define the episodes generation policy:</p><p>def make_epsilon_greedy_policy(Q, epsilon, nA):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">Creates an epsilon-greedy policy based on a given Q-function and epsilon.</div><div class="line"></div><div class="line">Args:</div><div class="line">    Q: A dictionary that maps from state -&gt; action-values.</div><div class="line">        Each value is a numpy array of length nA (see below)</div><div class="line">    epsilon: The probability to select a random action . float between 0 and 1.</div><div class="line">    nA: Number of actions in the environment.</div><div class="line"></div><div class="line">Returns:</div><div class="line">    A function that takes the observation as an argument and returns</div><div class="line">    the probabilities for each action in the form of a numpy array of length nA.</div><div class="line"></div><div class="line">"""</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></div><div class="line">    A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">    best_action = np.argmax(Q[observation])</div><div class="line">    A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">    <span class="keyword">return</span> A</div><div class="line"><span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Now, let us implement the sarsa algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sarsa</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, alpha=<span class="number">0.5</span>, epsilon=<span class="number">0.1</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        alpha: TD learning rate.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A tuple (Q, stats).</div><div class="line">        Q is the optimal action-value function, a dictionary mapping state -&gt; action values.</div><div class="line">        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># The final action-value function.</span></div><div class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></div><div class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</div><div class="line">    </div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))</div><div class="line"></div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</div><div class="line">    </div><div class="line"></div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode + <span class="number">1</span>, num_episodes), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Implement this!</span></div><div class="line">        state = env.reset()</div><div class="line">        action_probs = policy(state)</div><div class="line">        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">        </div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">            next_state, reward, is_done, _ = env.step(action)</div><div class="line">            next_action_probs = policy(next_state)</div><div class="line">            </div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)</div><div class="line">            Q[state][action] += alpha * (reward + discount_factor * Q[next_state][next_action] - Q[state][action])</div><div class="line">            </div><div class="line">            <span class="keyword">if</span> is_done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">            </div><div class="line">            state = next_state</div><div class="line">            action = next_action</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Q, stats</div></pre></td></tr></table></figure><p>For understand easily, we put the pesudo-code here again:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa.png" alt="sarsa"></p><p>The results (with $\varepsilon=0.1,\ \alpha=0.5$) are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png" alt="sarsa_result"></p><p>The increasing slope (bottom figure) of the graph shows that the goal is reached more and more quickly over time. Note that Monte Carlo methods cannot easily be used on this task because termination is not guaranteed for all policies. If a policy was ever found that caused the agent to stay in the same state, then the next episode would never end. Step-by-step learning methods such as Sarsa do not have this problem because they quickly learn <strong>during the episode</strong> that such<br>policies are poor, and switch to something else.</p><h3 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h3><p>One of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning, defined by<br>$$<br>Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)\right]<br>$$<br>The algorithm is shown in procedural form in the box below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning.png" alt="q_learning"></p><p>And below is the backup diagram:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_bg.png" alt="q_bg"></p><h4 id="Example-Cliff-Walking"><a href="#Example-Cliff-Walking" class="headerlink" title="Example: Cliff Walking"></a>Example: Cliff Walking</h4><p>This grid world example compares Sarsa and Q-learning, highlighting the difference between on-policy (Sarsa) and off-policy (Q-learning) methods. Consider the grid world shown in the figure below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_world.png" alt="cliff_world"></p><p>The same as earlier, we define the environment first. But the new environment just changes a little, so we just paste the code <a href="https://github.com/ewanlee/reinforcement-learning/blob/master/lib/envs/cliff_walking.py" target="_blank" rel="external">here</a>.</p><p>Let us test the environment first:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">env = CliffWalkingEnv()</div><div class="line"></div><div class="line">print(env.reset())</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">0</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">1</span>))</div><div class="line">env.render()</div><div class="line"></div><div class="line">print(env.step(<span class="number">2</span>))</div><div class="line">env.render()</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_show.png" alt="cliff_walk_show"></p><p>Not bad.</p><p>Then, let us develop the Q-learning algorithm (the episodes generation policy is not change):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_learning</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, alpha=<span class="number">0.5</span>, epsilon=<span class="number">0.1</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy</div><div class="line">    while following an epsilon-greedy policy</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        alpha: TD learning rate.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A tuple (Q, episode_lengths).</div><div class="line">        Q is the optimal action-value function, a dictionary mapping state -&gt; action values.</div><div class="line">        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="comment"># The final action-value function.</span></div><div class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></div><div class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))    </div><div class="line">    </div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="keyword">if</span> (i_episode + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode + <span class="number">1</span>, num_episodes), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Reset the environment and pick the first action</span></div><div class="line">        state = env.reset()</div><div class="line">        </div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="comment"># total_reward = 0.0</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">            </div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            action_probs = policy(state)</div><div class="line">            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            next_state, reward, done, _ = env.step(action)</div><div class="line"></div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            <span class="comment"># TD Update</span></div><div class="line">            best_next_action = np.argmax(Q[next_state])    </div><div class="line">            td_target = reward + discount_factor * Q[next_state][best_next_action]</div><div class="line">            td_delta = td_target - Q[state][action]</div><div class="line">            Q[state][action] += alpha * td_delta</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">                </div><div class="line">            state = next_state</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> Q, stats</div></pre></td></tr></table></figure><p>Results ($\varepsilon=0.1$) are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/q_learning_result.png" alt="q_learning_result"></p><p>For compare convenience, we put the result of Sarsa here again:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/sarsa_result.png" alt="sarsa_result"></p><p>We can see, for average, After an initial transient, Q-learning learns values for the optimal policy, that which travels right along the edge of the cliﬀ. Unfortunately, this results in its occasionally falling oﬀ the cliﬀ because of the ε-greedy action selection. Sarsa, on the other hand, takes the action selection into account and learns the longer but safer path through the upper part of the<br>grid. Although Q-learning actually learns the values of the optimal policy, its online performance is worse than that of Sarsa, which learns the roundabout policy. Of course, if ε were gradually reduced, then both methods would asymptotically converge to the optimal policy.</p><h3 id="Expected-Sarsa"><a href="#Expected-Sarsa" class="headerlink" title="Expected Sarsa"></a>Expected Sarsa</h3><p>Consider the learning algorithm that is just like Q-learning except that instead of the maximum over next state-action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule<br>$$<br>\begin{align}<br>Q(S_t, A_t) &amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \mathbb{E}[Q(S_{t+1}, A_{t+1} \ | \ S_{t+1})] - Q(S_t, A_t) \right ] \\<br>&amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1}, a) - Q(S_t, A_t) \right ],<br>\end{align}<br>$$<br>but that otherwise follows the schema of Q-learning. Its backup diagram is shown below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/esarsa_bg.png" alt="esarsa_bg"></p><p>For compare the results on the cliff-walking task with Excepted Sarsa with Sarsa and Q-learning, we develop another <a href="https://github.com/ewanlee/reinforcement-learning-an-introduction/blob/master/chapter06/CliffWalking.py" target="_blank" rel="external">codes</a> (here we are not use the OpenAI gym toolkit).</p><p>The first we define some truth of the environment:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># world height</span></div><div class="line">WORLD_HEIGHT = <span class="number">4</span></div><div class="line"></div><div class="line"><span class="comment"># world width</span></div><div class="line">WORLD_WIDTH = <span class="number">12</span></div><div class="line"></div><div class="line"><span class="comment"># probability for exploration</span></div><div class="line">EPSILON = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># step size</span></div><div class="line">ALPHA = <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="comment"># gamma for Q-Learning and Expected Sarsa</span></div><div class="line">GAMMA = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">ACTION_UP = <span class="number">0</span></div><div class="line">ACTION_DOWN = <span class="number">1</span></div><div class="line">ACTION_LEFT = <span class="number">2</span></div><div class="line">ACTION_RIGHT = <span class="number">3</span></div><div class="line">actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]</div><div class="line"></div><div class="line"><span class="comment"># initial state action pair values</span></div><div class="line">stateActionValues = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, <span class="number">4</span>))</div><div class="line">startState = [<span class="number">3</span>, <span class="number">0</span>]</div><div class="line">goalState = [<span class="number">3</span>, <span class="number">11</span>]</div><div class="line"></div><div class="line"><span class="comment"># reward for each action in each state</span></div><div class="line">actionRewards = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, <span class="number">4</span>))</div><div class="line">actionRewards[:, :, :] = <span class="number">-1.0</span></div><div class="line">actionRewards[<span class="number">2</span>, <span class="number">1</span>:<span class="number">11</span>, ACTION_DOWN] = <span class="number">-100.0</span></div><div class="line">actionRewards[<span class="number">3</span>, <span class="number">0</span>, ACTION_RIGHT] = <span class="number">-100.0</span></div></pre></td></tr></table></figure><p>And then we define the state transitions:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># set up destinations for each action in each state</span></div><div class="line">actionDestination = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_HEIGHT):</div><div class="line">    actionDestination.append([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_WIDTH):</div><div class="line">        destinaion = dict()</div><div class="line">        destinaion[ACTION_UP] = [max(i - <span class="number">1</span>, <span class="number">0</span>), j]</div><div class="line">        destinaion[ACTION_LEFT] = [i, max(j - <span class="number">1</span>, <span class="number">0</span>)]</div><div class="line">        destinaion[ACTION_RIGHT] = [i, min(j + <span class="number">1</span>, WORLD_WIDTH - <span class="number">1</span>)]</div><div class="line">        <span class="keyword">if</span> i == <span class="number">2</span> <span class="keyword">and</span> <span class="number">1</span> &lt;= j &lt;= <span class="number">10</span>:</div><div class="line">            destinaion[ACTION_DOWN] = startState</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            destinaion[ACTION_DOWN] = [min(i + <span class="number">1</span>, WORLD_HEIGHT - <span class="number">1</span>), j]</div><div class="line">        actionDestination[<span class="number">-1</span>].append(destinaion)</div><div class="line">actionDestination[<span class="number">3</span>][<span class="number">0</span>][ACTION_RIGHT] = startState</div></pre></td></tr></table></figure><p>We also need a policy to generate the next action according to the current state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># choose an action based on epsilon greedy algorithm</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseAction</span><span class="params">(state, stateActionValues)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(actions)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> np.argmax(stateActionValues[state[<span class="number">0</span>], state[<span class="number">1</span>], :])</div></pre></td></tr></table></figure><p>The <strong>stateActionValues</strong> just is the Q.</p><p>Then, let us develop the Sarsa (and Excepted Sarsa) algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># an episode with Sarsa</span></div><div class="line"><span class="comment"># @stateActionValues: values for state action pair, will be updated</span></div><div class="line"><span class="comment"># @expected: if True, will use expected Sarsa algorithm</span></div><div class="line"><span class="comment"># @stepSize: step size for updating</span></div><div class="line"><span class="comment"># @return: total rewards within this episode</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sarsa</span><span class="params">(stateActionValues, expected=False, stepSize=ALPHA)</span>:</span></div><div class="line">    currentState = startState</div><div class="line">    currentAction = chooseAction(currentState, stateActionValues)</div><div class="line">    rewards = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState != goalState:</div><div class="line">        newState = actionDestination[currentState[<span class="number">0</span>]][currentState[<span class="number">1</span>]][currentAction]</div><div class="line">        newAction = chooseAction(newState, stateActionValues)</div><div class="line">        reward = actionRewards[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction]</div><div class="line">        rewards += reward</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> expected:</div><div class="line">            valueTarget = stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], newAction]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># calculate the expected value of new state</span></div><div class="line">            valueTarget = <span class="number">0.0</span></div><div class="line">            actions_list = stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], :]</div><div class="line">            bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist()</div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                <span class="keyword">if</span> action <span class="keyword">in</span> bestActions:</div><div class="line">                    valueTarget += ((<span class="number">1.0</span> - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    valueTarget += EPSILON / len(actions) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div><div class="line">        valueTarget *= GAMMA</div><div class="line">        <span class="comment"># Sarsa update</span></div><div class="line">        stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction] += stepSize * (reward +</div><div class="line">            valueTarget - stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction])</div><div class="line">        currentState = newState</div><div class="line">        currentAction = newAction</div><div class="line">    <span class="keyword">return</span> rewards</div></pre></td></tr></table></figure><p>Because we develop the Sarsa algorithm earlier, so we just concentrate on the Excepted Sarsa algorithm here:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># calculate the expected value of new state</span></div><div class="line">valueTarget = <span class="number">0.0</span></div><div class="line">actions_list = stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], :]</div><div class="line">bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist()</div><div class="line"><span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">    <span class="keyword">if</span> action <span class="keyword">in</span> bestActions:</div><div class="line">        valueTarget += ((<span class="number">1.0</span> - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        valueTarget += EPSILON / len(actions) * stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], action]</div></pre></td></tr></table></figure><p>By the way, let us develop the Q-learning algorithm again:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># an episode with Q-Learning</span></div><div class="line"><span class="comment"># @stateActionValues: values for state action pair, will be updated</span></div><div class="line"><span class="comment"># @expected: if True, will use expected Sarsa algorithm</span></div><div class="line"><span class="comment"># @stepSize: step size for updating</span></div><div class="line"><span class="comment"># @return: total rewards within this episode</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">qLearning</span><span class="params">(stateActionValues, stepSize=ALPHA)</span>:</span></div><div class="line">    currentState = startState</div><div class="line">    rewards = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState != goalState:</div><div class="line">        currentAction = chooseAction(currentState, stateActionValues)</div><div class="line">        reward = actionRewards[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction]</div><div class="line">        rewards += reward</div><div class="line">        newState = actionDestination[currentState[<span class="number">0</span>]][currentState[<span class="number">1</span>]][currentAction]</div><div class="line">        <span class="comment"># Q-Learning update</span></div><div class="line">        stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction] += stepSize * (</div><div class="line">            reward + GAMMA * np.max(stateActionValues[newState[<span class="number">0</span>], newState[<span class="number">1</span>], :]) -</div><div class="line">            stateActionValues[currentState[<span class="number">0</span>], currentState[<span class="number">1</span>], currentAction])</div><div class="line">        currentState = newState</div><div class="line">    <span class="keyword">return</span> rewards</div></pre></td></tr></table></figure><p>Now we can see the optimal policy in each state of both algorithm (we are not mentioned earlier):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># print optimal policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">printOptimalPolicy</span><span class="params">(stateActionValues)</span>:</span></div><div class="line">    optimalPolicy = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_HEIGHT):</div><div class="line">        optimalPolicy.append([])</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_WIDTH):</div><div class="line">            <span class="keyword">if</span> [i, j] == goalState:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'G'</span>)</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            bestAction = np.argmax(stateActionValues[i, j, :])</div><div class="line">            <span class="keyword">if</span> bestAction == ACTION_UP:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'U'</span>)</div><div class="line">            <span class="keyword">elif</span> bestAction == ACTION_DOWN:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'D'</span>)</div><div class="line">            <span class="keyword">elif</span> bestAction == ACTION_LEFT:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'L'</span>)</div><div class="line">            <span class="keyword">elif</span> bestAction == ACTION_RIGHT:</div><div class="line">                optimalPolicy[<span class="number">-1</span>].append(<span class="string">'R'</span>)</div><div class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> optimalPolicy:</div><div class="line">        print(row)</div><div class="line"></div><div class="line"><span class="comment"># averaging the reward sums from 10 successive episodes</span></div><div class="line">averageRange = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># episodes of each run</span></div><div class="line">nEpisodes = <span class="number">500</span></div><div class="line"></div><div class="line"><span class="comment"># perform 20 independent runs</span></div><div class="line">runs = <span class="number">20</span></div><div class="line"></div><div class="line">rewardsSarsa = np.zeros(nEpisodes)</div><div class="line">rewardsQLearning = np.zeros(nEpisodes)</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    stateActionValuesSarsa = np.copy(stateActionValues)</div><div class="line">    stateActionValuesQLearning = np.copy(stateActionValues)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        <span class="comment"># cut off the value by -100 to draw the figure more elegantly</span></div><div class="line">        rewardsSarsa[i] += max(sarsa(stateActionValuesSarsa), <span class="number">-100</span>)</div><div class="line">        rewardsQLearning[i] += max(qLearning(stateActionValuesQLearning), <span class="number">-100</span>)</div><div class="line"></div><div class="line"><span class="comment"># averaging over independt runs</span></div><div class="line">rewardsSarsa /= runs</div><div class="line">rewardsQLearning /= runs</div><div class="line"></div><div class="line"><span class="comment"># averaging over successive episodes</span></div><div class="line">smoothedRewardsSarsa = np.copy(rewardsSarsa)</div><div class="line">smoothedRewardsQLearning = np.copy(rewardsQLearning)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(averageRange, nEpisodes):</div><div class="line">    smoothedRewardsSarsa[i] = np.mean(rewardsSarsa[i - averageRange: i + <span class="number">1</span>])</div><div class="line">    smoothedRewardsQLearning[i] = np.mean(rewardsQLearning[i - averageRange: i + <span class="number">1</span>])</div><div class="line"></div><div class="line"><span class="comment"># display optimal policy</span></div><div class="line">print(<span class="string">'Sarsa Optimal Policy:'</span>)</div><div class="line">printOptimalPolicy(stateActionValuesSarsa)</div><div class="line">print(<span class="string">'Q-Learning Optimal Policy:'</span>)</div><div class="line">printOptimalPolicy(stateActionValuesQLearning)</div></pre></td></tr></table></figure><p>The results are as follows (emits the results of the changes of reward):</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_opti_policy.png" alt="cliff_walk_optimal_policy"></p><p>Now let us compare the three algorithms:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">stepSizes = np.arange(<span class="number">0.1</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line">    nEpisodes = <span class="number">1000</span></div><div class="line">    runs = <span class="number">10</span></div><div class="line"></div><div class="line">    ASY_SARSA = <span class="number">0</span></div><div class="line">    ASY_EXPECTED_SARSA = <span class="number">1</span></div><div class="line">    ASY_QLEARNING = <span class="number">2</span></div><div class="line">    INT_SARSA = <span class="number">3</span></div><div class="line">    INT_EXPECTED_SARSA = <span class="number">4</span></div><div class="line">    INT_QLEARNING = <span class="number">5</span></div><div class="line">    methods = range(<span class="number">0</span>, <span class="number">6</span>)</div><div class="line"></div><div class="line">    performace = np.zeros((<span class="number">6</span>, len(stepSizes)))</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        <span class="keyword">for</span> ind, stepSize <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(stepSizes)), stepSizes):</div><div class="line">            stateActionValuesSarsa = np.copy(stateActionValues)</div><div class="line">            stateActionValuesExpectedSarsa = np.copy(stateActionValues)</div><div class="line">            stateActionValuesQLearning = np.copy(stateActionValues)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'step size:'</span>, stepSize, <span class="string">'episode:'</span>, ep)</div><div class="line">                sarsaReward = sarsa(stateActionValuesSarsa, expected=<span class="keyword">False</span>, stepSize=stepSize)</div><div class="line">                expectedSarsaReward = sarsa(stateActionValuesExpectedSarsa, expected=<span class="keyword">True</span>, stepSize=stepSize)</div><div class="line">                qLearningReward = qLearning(stateActionValuesQLearning, stepSize=stepSize)</div><div class="line">                performace[ASY_SARSA, ind] += sarsaReward</div><div class="line">                performace[ASY_EXPECTED_SARSA, ind] += expectedSarsaReward</div><div class="line">                performace[ASY_QLEARNING, ind] += qLearningReward</div><div class="line"></div><div class="line">                <span class="keyword">if</span> ep &lt; <span class="number">100</span>:</div><div class="line">                    performace[INT_SARSA, ind] += sarsaReward</div><div class="line">                    performace[INT_EXPECTED_SARSA, ind] += expectedSarsaReward</div><div class="line">                    performace[INT_QLEARNING, ind] += qLearningReward</div><div class="line"></div><div class="line">    performace[:<span class="number">3</span>, :] /= nEpisodes * runs</div><div class="line">    performace[<span class="number">3</span>:, :] /= runs * <span class="number">100</span></div><div class="line">    labels = [<span class="string">'Asymptotic Sarsa'</span>, <span class="string">'Asymptotic Expected Sarsa'</span>, <span class="string">'Asymptotic Q-Learning'</span>,</div><div class="line">              <span class="string">'Interim Sarsa'</span>, <span class="string">'Interim Expected Sarsa'</span>, <span class="string">'Interim Q-Learning'</span>]</div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    <span class="keyword">for</span> method, label <span class="keyword">in</span> zip(methods, labels):</div><div class="line">        plt.plot(stepSizes, performace[method, :], label=label)</div><div class="line">    plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">    plt.ylabel(<span class="string">'reward per episode'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/cliff_walk_3_results.png" alt="compare3algo_cliff_walk"></p><p>As an on-policy method, Expected Sarsa retains the signiﬁcant advantage of Sarsa over Q-learning on this problem. In addition, Expected Sarsa shows a signiﬁcant improvement over Sarsa over a wide range of values for the step-size parameter α. In cliﬀ walking the state transitions are all deterministic and all randomness comes from the policy. In such cases, Expected Sarsa can safely set α = 1 without suﬀering any degradation of asymptotic performance, whereas Sarsa can only perform well in the long run at a small value of α, at which short-term performance is poor. In this and other examples there is a consistent empirical advantage of Expected Sarsa over Sarsa. Except for the small additional computational cost, Expected Sarsa may completely dominate both of the other more-well-known TD control algorithms.</p><h3 id="Double-Q-learning"><a href="#Double-Q-learning" class="headerlink" title="Double Q-learning"></a>Double Q-learning</h3><p>All the control algorithms that we have discussed so far involve maximization in the construction of their target policies. For example, in Q-learning the target policy is the greedy policy given the current action values, which is deﬁned with a max, and in Sarsa the policy is often ε-greedy, which also involves a maximization operation. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a signiﬁcant positive bias. To see why, consider a single state s where there are many actions a whose true values, $q(s, a)$ are all zero but whose estimated values, $Q(s, a)$, are uncertain and thus distributed some above and some below zero. The maximum of the true values is zero, but the maximum of the estimates is positive, a positive bias. We call this maximization<br>bias.</p><h4 id="Example-Maximization-Bias"><a href="#Example-Maximization-Bias" class="headerlink" title="Example: Maximization Bias"></a>Example: Maximization Bias</h4><p>We have a small MDP:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/mb.png" alt="mb"></p><p>the expected return for any trajectory starting with left (from <strong>B</strong>) is −0.1, and thus taking left in state A is always a mistake. Nevertheless, our control methods may favor left because of maximization bias making B appear to have a positive value. The results (paste later) shows that Q-learning with ε-greedy action selection initially learns to strongly favor the left action on this example. Even at asymptote, Q-learning takes the left action about 5% more often than is optimal at our parameter settings (ε = 0.1, α = 0.1, and γ = 1).</p><p>We could use the Double Q-learning algorithm to avoid this problem. One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. Suppose we divided the plays in two sets and used them to learn two independent estimates.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/dbq.png" alt="dbq"></p><p>Of course there are also doubled versions of Sarsa and Expected Sarsa.</p><p>Now let us develop the both algorithms and compare their performance on the earlier example. First we define the problem environment:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># state A</span></div><div class="line">STATE_A = <span class="number">0</span></div><div class="line"></div><div class="line"><span class="comment"># state B</span></div><div class="line">STATE_B = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># use one terminal state</span></div><div class="line">STATE_TERMINAL = <span class="number">2</span></div><div class="line"></div><div class="line"><span class="comment"># starts from state A</span></div><div class="line">STATE_START = STATE_A</div><div class="line"></div><div class="line"><span class="comment"># possible actions in A</span></div><div class="line">ACTION_A_RIGHT = <span class="number">0</span></div><div class="line">ACTION_A_LEFT = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># possible actions in B, maybe 10 actions</span></div><div class="line">actionsOfB = range(<span class="number">0</span>, <span class="number">10</span>)</div><div class="line"></div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">stateActions = [[ACTION_A_RIGHT, ACTION_A_LEFT], actionsOfB]</div><div class="line"></div><div class="line"><span class="comment"># state action pair values, if a state is a terminal state, then the value is always 0</span></div><div class="line">stateActionValues = [np.zeros(<span class="number">2</span>), np.zeros(len(actionsOfB)), np.zeros(<span class="number">1</span>)]</div><div class="line"></div><div class="line"><span class="comment"># set up destination for each state and each action</span></div><div class="line">actionDestination = [[STATE_TERMINAL, STATE_B], [STATE_TERMINAL] * len(actionsOfB)]</div><div class="line"></div><div class="line"><span class="comment"># probability for exploration</span></div><div class="line">EPSILON = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># step size</span></div><div class="line">ALPHA = <span class="number">0.1</span></div><div class="line"></div><div class="line"><span class="comment"># discount for max value</span></div><div class="line">GAMMA = <span class="number">1.0</span></div></pre></td></tr></table></figure><p>And we need a policy to take an action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># choose an action based on epsilon greedy algorithm</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseAction</span><span class="params">(state, stateActionValues)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(stateActions[state])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> argmax(stateActionValues[state])</div></pre></td></tr></table></figure><p>After take an action, we get the reward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take @action in @state, return the reward</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(state, action)</span>:</span></div><div class="line">    <span class="keyword">if</span> state == STATE_A:</div><div class="line">        <span class="keyword">return</span> <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> np.random.normal(<span class="number">-0.1</span>, <span class="number">1</span>)</div></pre></td></tr></table></figure><p>Next, we develop the Double Q-learning algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># if there are two state action pair value array, use double Q-Learning</span></div><div class="line"><span class="comment"># otherwise use normal Q-Learning</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">qLearning</span><span class="params">(stateActionValues, stateActionValues2=None)</span>:</span></div><div class="line">    currentState = STATE_START</div><div class="line">    <span class="comment"># track the # of action left in state A</span></div><div class="line">    leftCount = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> currentState != STATE_TERMINAL:</div><div class="line">        <span class="keyword">if</span> stateActionValues2 <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            currentAction = chooseAction(currentState, stateActionValues)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># derive a action form Q1 and Q2</span></div><div class="line">            currentAction = chooseAction(currentState, [item1 + item2 <span class="keyword">for</span> item1, item2 <span class="keyword">in</span> zip(stateActionValues, stateActionValues2)])</div><div class="line">        <span class="keyword">if</span> currentState == STATE_A <span class="keyword">and</span> currentAction == ACTION_A_LEFT:</div><div class="line">            leftCount += <span class="number">1</span></div><div class="line">        reward = takeAction(currentState, currentAction)</div><div class="line">        newState = actionDestination[currentState][currentAction]</div><div class="line">        <span class="keyword">if</span> stateActionValues2 <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            currentStateActionValues = stateActionValues</div><div class="line">            targetValue = np.max(currentStateActionValues[newState])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">                currentStateActionValues = stateActionValues</div><div class="line">                anotherStateActionValues = stateActionValues2</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                currentStateActionValues = stateActionValues2</div><div class="line">                anotherStateActionValues = stateActionValues</div><div class="line">            bestAction = argmax(currentStateActionValues[newState])</div><div class="line">            targetValue = anotherStateActionValues[newState][bestAction]</div><div class="line"></div><div class="line">        <span class="comment"># Q-Learning update</span></div><div class="line">        currentStateActionValues[currentState][currentAction] += ALPHA * (</div><div class="line">            reward + GAMMA * targetValue - currentStateActionValues[currentState][currentAction])</div><div class="line">        currentState = newState</div><div class="line">    <span class="keyword">return</span> leftCount</div></pre></td></tr></table></figure><p>And now, let us solve the example problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># each independent run has 300 episodes</span></div><div class="line">    episodes = <span class="number">300</span></div><div class="line">    leftCountsQ = np.zeros(episodes)</div><div class="line">    leftCountsDoubleQ = np.zeros(episodes)</div><div class="line">    runs = <span class="number">1000</span></div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        print(<span class="string">'run:'</span>, run)</div><div class="line">        stateActionValuesQ = [np.copy(item) <span class="keyword">for</span> item <span class="keyword">in</span> stateActionValues]</div><div class="line">        stateActionValuesDoubleQ1 = [np.copy(item) <span class="keyword">for</span> item <span class="keyword">in</span> stateActionValues]</div><div class="line">        stateActionValuesDoubleQ2 = [np.copy(item) <span class="keyword">for</span> item <span class="keyword">in</span> stateActionValues]</div><div class="line">        leftCountsQ_ = [<span class="number">0</span>]</div><div class="line">        leftCountsDoubleQ_ = [<span class="number">0</span>]</div><div class="line">        <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            leftCountsQ_.append(leftCountsQ_[<span class="number">-1</span>] + qLearning(stateActionValuesQ))</div><div class="line">            leftCountsDoubleQ_.append(leftCountsDoubleQ_[<span class="number">-1</span>] + qLearning(stateActionValuesDoubleQ1, stateActionValuesDoubleQ2))</div><div class="line">        <span class="keyword">del</span> leftCountsQ_[<span class="number">0</span>]</div><div class="line">        <span class="keyword">del</span> leftCountsDoubleQ_[<span class="number">0</span>]</div><div class="line">        leftCountsQ += np.asarray(leftCountsQ_, dtype=<span class="string">'float'</span>) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">        leftCountsDoubleQ += np.asarray(leftCountsDoubleQ_, dtype=<span class="string">'float'</span>) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">    leftCountsQ /= runs</div><div class="line">    leftCountsDoubleQ /= runs</div><div class="line">    plt.figure()</div><div class="line">    plt.plot(leftCountsQ, label=<span class="string">'Q-Learning'</span>)</div><div class="line">    plt.plot(leftCountsDoubleQ, label=<span class="string">'Double Q-Learning'</span>)</div><div class="line">    plt.plot(np.ones(episodes) * <span class="number">0.05</span>, label=<span class="string">'Optimal'</span>)</div><div class="line">    plt.xlabel(<span class="string">'episodes'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% left actions from A'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Ok, results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/td/dbq_result.png" alt="dbq_result"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/11/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/13/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">131</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">64</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/tomaxent" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>