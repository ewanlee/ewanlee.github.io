<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/12/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/12/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/27/A-simple-AI-car/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/06/27/A-simple-AI-car/" itemprop="url">A simple AI car</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-27T13:56:15+08:00">2017-06-27 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/06/27/A-simple-AI-car/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/06/27/A-simple-AI-car/" itemprop="commentsCount"></span> </a></span><span id="/2017/06/27/A-simple-AI-car/" class="leancloud_visitors" data-flag-title="A simple AI car"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="I-定义"><a href="#I-定义" class="headerlink" title="I. 定义"></a>I. 定义</h3><h4 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h4><p>项目地址：<a href="https://github.com/ewanlee/rl_car" target="_blank" rel="external">https://github.com/ewanlee/rl_car</a></p><p>最近，自动驾驶汽车十分火热。但是，自动驾驶问题是一个机器学习集大成的问题，十分的复杂。因此，我们希望可以设计出一个简单的学习环境能够对自动驾驶问题进行模拟，并且不需要GPU （主要是太贵）。</p><p>我们的学习环境借鉴了Matt Harvey’s virtual car[1] 的环境设置。运用了 TensorFlow， Python 2.7 以及 PyGame 5.0. 本项目中运用了深度Q强化学习算法，但是为了符合我们上面提到的要求，我们去掉了该算法中 “深度” 的部分。代码设计的一些思想借鉴了 songotrek’s Q学习算法的TensorFlow实现 [2].</p><h4 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h4><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/game.jpg" alt="fig1"></p><p><em>图片来源于[1]</em></p><p>我们所要解决的问题就是设计一个算法使得模拟小车能够自动行驶。</p><p>上图就是我们实验用的环境。可以看出，它足够简单，但是足够进行一些强化学习算法的验证。最小的圆圈是我们模拟的小车，它拥有三个声纳感应器 （三条白色的虚线）。三个较大的圆圈代表障碍物，它会随着时间的变化缓慢移动。左上角的圆圈代表一只在环境中游走（速度相比于障碍物要快很多）的猫。圆圈上的缺口表示朝向。我们所要解决的问题就是希望小车可以尽可能长时间的运动，但不会撞到障碍物或者猫。</p><h4 id="环境需求"><a href="#环境需求" class="headerlink" title="环境需求"></a>环境需求</h4><ul><li>Anaconda Python Distribution 2.7 [3]</li><li>TensorFlow for Anaconda [4]</li><li>PyGame [5]，用于展示图形界面</li><li>PyMunk [6]，为了模拟游戏中的物理环境</li><li>Numpy [7]</li><li>Scipy [8]</li></ul><p>实验运行的环境为 Ubuntu 16.04 LTS 虚拟机， 虚拟机为VMware Workstation 12.5.2 build-4638234。虚拟机运行在Windows 10 Pro上。</p><h4 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h4><p>我们的baseline是一个随机（行为随机选择）小车，最后的评价指标是我们定义的指标score，代表小车存活的时间（在游戏中代表小车存活的frame）。并且，score是进行1000次实验的平均值。</p><h4 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h4><p>我们使用的是 Deep Q Learning [9] 论文中定义的 QMax 值。</p><p>QMax 值指的是在一定时间范围内，对于所有的训练样本，Q 函数（使用神经网络进行拟合）输出的最大的 Q-value。随着agent（模拟小车）不断进行学习，它将采取更加优秀的策略，因此存活时间会更长，那么 Q-value (在我们的实验中便是score) 会越大。如果我们的优化目标是增大 Q-value 的上界，也便相应的增大了 Q-value 值。</p><h4 id="学习过程监测"><a href="#学习过程监测" class="headerlink" title="学习过程监测"></a>学习过程监测</h4><p>我们使用的是Tensorflow自带的TensorBoard来监测QMax以及最大score的变化情况（希望整体趋势是逐渐增大的）</p><p>下面是运行过程中的截图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl_car/tensdorboard_sing.png" alt="s"></p><p>下面是各网络参数的分布变化情况：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl_car/tensdorboard_h.png" alt="h"></p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p>由于强化学习任务的数据集一般都是实验中产生的，因此不需要收集数据。在每一次迭代过程中，模拟环境提供以下数据（自己设计的）：</p><ul><li>s1, s2, s3 三个声纳传感器的数值，范围是[0, 40]，整数值，代表三个方向上障碍物的距离。范围确定为这样的原因是，为了检测障碍物，声纳传感器从源头开始，逐渐往外探测，每向外探测一次，距离就加1（可以看成虚线的点数，即虚线是由多少个点组成的）。</li><li>x 代表x轴的位置，范围是[0, 1]</li><li>y 代表y轴的位置，范围是[0, 1]</li><li>theta 代表小车的方向，弧度表示，范围是[0, 2$\pi$]</li></ul><p>小车能够采取的动作如下：</p><ul><li>0，代表直走</li><li>1， 往左转0.2弧度</li><li>2， 往右转0.2弧度</li></ul><p>小车每进行一次动作会使得状态发生变化，并且有以下返回值：</p><ul><li>Reward，一个[-100, 10]之间的整数，负数代表动作产生的结果不好，正数则相反</li><li>Termianl，布尔型数据，代表小车是否存活（是否撞到障碍物）</li></ul><p>我们和原始模型[1]不同的是，输了$s_1, s_2, s_3$三个特征之外，额外增加了$x, y, theta$三个特征。因为我们希望小车能够尽可能往地图中间运行，远离墙壁。并且当它们靠近障碍物时，能够选择更加合理的方向躲避。</p><p>值得说明的一点是，小车如何检测是否撞到障碍物的问题。实验中使用的方法是检测声纳传感器的数值，如果数值是1（而不是0）就认为小车撞上了障碍物，并给出一个-100的reward。此时实验将重新开始，小车位置的选择是根据物理定律模拟的，即根据碰撞的角度给小车一个反向的速度，并且小车的朝向随机变化。这样模拟出一种碰撞后的混乱状态。</p><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>下面介绍Deep Q-learning算法。</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/rel.png" alt="Reinforcement Learning"></p><p>以上的实验环境可以形式化的表述，如上图所示。我们拥有一个agent（小车），在时间$t$时必须要选择一个动作$a_t$。Agent采取动作与环境进行交互，使得在时间$t+1$时状态变为$s_{t+1}$。同时agent接收到环境给它的一个反馈$r_t$。这样agent就根据$(s_t, a_t, s_{t+1}, r_t)$来决定采取的动作$a_{t+1}$是什么。整个问题就是不断重复上述过程直到到达某个结束条件。</p><p>机器学习领域将这个问题称为强化学习问题。每一个动作通过reward被 “强化”，使得agent不断接近我们期望它到达的状态。但是在强化学习中存在一个reward延迟的问题，也就是说，某一个action的回报可能不是即时的，需要很多时间步之后才能确定。举个例子，下棋的过程中需要布局，但是这个布局并不会马上给你带来好处，需要在以后的某个特定时间，你的对上掉入了你很久前设置的陷阱里，这时候才给你带来好处。所以，我们需要采用一种方式来对这个问题进行建模。我们定义一个价值函数$Q(s_t, a_t)$，它表示在状态$s_t$是采取$a_t$这个动作带来的 “价值”，而不是reward，reward是即时的，但是价值是若干时间步带来的reward的某种综合考量，更具实际意义。那么接下来的问题就是价值函数应当如何定义。</p><p>最直观的想法就是，我们可以把强化学习问题定义为一个动态规划的问题。这里我直接列出公式，也就是非常著名的贝尔曼方程（Bellman equation）：</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/bellman.png" alt="Bellman equation"></p><p>可以看到，解决强化学习问题是一个不断迭代的过程，那么如何初始化Q非常重要。但实际上，如果迭代次数趋紧无穷大时，Q的初始值对于最终的结果并没有影响，因此一般来说只要初始化为均值为0的高斯白噪音。</p><p>对于小规模的强化学习问题，由于状态的Q值随着迭代次数的增加会不断更新，那么我们需要一个地方来存储这些值。传统的强化学习算法一般采用一张表格（数组或字典）来存储这些值。但是随着问题规模的增大，状态会显著增加。对于我们的问题，状态空间更是无限的，因为状态是由浮点数组成的。这样我们就不可能把这些状态对应的Q值都存储下来。</p><p>我们采用一个如下所示的神经网络来代替这些表格，即找出状态和Q值之间的一个映射。这里值得说明的是，网络输出的是所有动作对应的Q值，这是Deep Q-learning算法的一个创新点。</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/network.png" alt="Neural Network"></p><p>在我们的实验中，输入维度是6维（$s_1, s_2, s_3, x, y, theta$），输出是3维（对应三个动作0， 1， 2）。我们采用白噪音来初始化网络。具体来说，权重采用标准高斯噪音，偏差初始化为0.01。</p><p>至于训练过程，Deep Q-learning算法采用了一个trick。该算法采用了两个完全相同的网络，其中一个用来训练，另一个则用来预测。这样还可以防止过拟合。用于训练网络的训练集并不是agent当前的四元组$(s_t, a_t, s_{t+1}, r_t)$， 而是从最近四元组历史（之前某一个时间窗口中的所有四元组）中随机采样出的一个minibatch。我们通过这些训练样本来更新训练网络的参数，经过一定时间的训练之后，把训练网络的参数复制给预测网络，用预测网络来继续产生训练样本，供训练网络使用。整个算法就是不断重复上述过程直至收敛。具体算法的伪代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">Initialize replay memory D to size N</div><div class="line">Initialize action-value function Q <span class="keyword">with</span> random weights</div><div class="line"><span class="keyword">for</span> episode = <span class="number">1</span>, M do</div><div class="line">	Initialize state s_1</div><div class="line">	<span class="keyword">for</span> t = <span class="number">1</span>, T do</div><div class="line">		With probability ϵ select random action a_t</div><div class="line">		otherwise select a_t=argmax_a  Q(s_t,a; θ_i)</div><div class="line">		Execute action a_t <span class="keyword">in</span> emulator <span class="keyword">and</span> observe r_t <span class="keyword">and</span> s_(t+<span class="number">1</span>)</div><div class="line">		Store transition (s_t,a_t,r_t,s_(t+<span class="number">1</span>)) <span class="keyword">in</span> D</div><div class="line">		Sample a minibatch of transitions (s_j,a_j,r_j,s_(j+<span class="number">1</span>)) <span class="keyword">from</span> D</div><div class="line">		Set y_j:=</div><div class="line">			r_j <span class="keyword">for</span> terminal s_(j+<span class="number">1</span>)</div><div class="line">			r_j+γ*max_(a^<span class="string">') Q(s_(j+1),a'</span>; θ_i) <span class="keyword">for</span> non-terminal s_(j+<span class="number">1</span>)</div><div class="line">		Perform a gradient step on (y_j-Q(s_j,a_j; θ_i))^<span class="number">2</span> <span class="keyword">with</span> respect to θ</div><div class="line">	end <span class="keyword">for</span></div><div class="line">end <span class="keyword">for</span></div></pre></td></tr></table></figure><h4 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h4><p>我们希望算法能够比随机选择更好。下面是进行1000次实验随机算法的结果：</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/table1.png" alt="Table1"></p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p>我们在实验之前进行了数据的标准化，使得所有数据都处于0到1之间，这样可以避免梯度爆炸等现象的发生。</p><p>$x， y$ 这两个特征没有进行标准化，因为已经符合要求。$theta$通过除以$2\pi$进行标准化。在没有进行标准化之前，我们在实验中发现，$theta$的值会达到$10^3$这个数量级，使得网络发生了bias shift现象。$s_1, s_2, s_3$通过除以40来进行标准化。</p><p>我们同样试着能够将reward也进行标准化，将其范围缩小到[-1, 1]。因为DQN论文中同样使用了这种方法，使得该算法应用在不同的Atari游戏上时不用对算法进行参数的调整。但是，我们在网络训练的前一百万步并没有发现性能有明显的提升。因为reward的值更大的话，学习将会更容易，这样reward信号会更加明显，不会被淹没在网络的高斯噪声中。所以我们希望reward能够大一点，但是多大比较合适又是一个问题。</p><p>我们所借鉴的算法[1]，将这个reward的最小值设置成了-500（小车撞上了障碍物），但我们实验发现这个值设置的过小（下面将会解释），所以最后的范围调整为[-100, 10] （通过裁剪）。我们把这个过程称之为reward正则化。</p><h4 id="Reward-正则化"><a href="#Reward-正则化" class="headerlink" title="Reward 正则化"></a>Reward 正则化</h4><p>在网络训练（反向传播）的过程中，我们希望最小化代价函数。我们的代价函数选为训练网络输出的Q值与训练样本的Q值之间的MSE。在试验过程中，我们发现，对于$s_1, s_2, s_3$值都比较大的状态，其reward都会落在[0, 40]的范围内，并且均值为20。但是网络刚开始训练时，输出值为均值为0的高斯噪声。也就是说初始的loss处于[400-1600]的范围内（由于最后的loss需要除以样本的数量，所以loss等于一个样本的loss）。</p><p>现在我们假定网络处于一个最优点附近，这时候小车突然撞上了某个障碍物，那么唯一的可能就是猫出现在了小车后面。这时候就会引入一个250000的loss（如果将reward的最小值设置为-500）。但是网络初始时的loss都只处于[400, 1600]的范围内，这个loss是初始loss的100倍。这么大的loss所引入的梯度将会使得网络走一段非常大的距离，这就很可能直接跳过了局部最优点。不断如此的话，网络就会震荡的非常厉害。</p><p>让我们用数学的观点来解释这个问题。当reward的负值设置的过大，将会使得原始问题空间距离最优空间有一个非常大的偏差，很难通过梯度下降靠近。这个大的偏差在问题空间创造了一些非常陡峭的cliff。就像我们爬山一样，好不容易爬到了山顶附近，一不小心就掉下了悬崖，那么我们只能一步一步非常慢的爬上来，花很久的时间才能到达刚才的位置。如果一不小心又掉下去了，那么又要重新爬。</p><p>因此，减小reward的范围十分重要，这样可以减小cliff的坡度，使得网络训练更快更容易。但是又不能太小，以免被噪声淹没。最后我们选定了[-100, 10]这个范围。</p><h4 id="模型迭代过程"><a href="#模型迭代过程" class="headerlink" title="模型迭代过程"></a>模型迭代过程</h4><p>我们最开始直接采用现成的模型，是一个两层的神经网络（不包括输入层），效果已经不错了，但是小车总是撞上障碍物。因此我们做了一些改变：</p><ul><li>类似DQN，我们使用了最近四次的state，将其映射为一个input，这使得我们的QMax值提高到了120</li><li>我们继续进行改变，从使用最近四次改为最近16次，使得我们的QMax值提高到了140</li><li>我们尝试了使用一个更小的网络进行学习（2层，每层32维），并且只使用一个state进行输入，但是结果比随机算法更差。</li><li>继续尝试使用grid search选择模型，还是两层网络，每一层的维数从32到512，训练迭代次数为200, 000，但是最后的QMax值还是不能超过140。</li><li>我们尝试了更小的时间窗口，更大的minibatch，网络训练时震荡的十分厉害</li><li>我们尝试在小车的背面增加一个声纳传感器，发i按网络训练速度变快了，但是最后的QMax值还是不能达到更高。</li></ul><p>这些尝试说明应当是两层网络的特征表达能力不够，我们尝试使用更深的网络。最后使用的网络有8层（不算输入输出层），输入层和输出层各有32维，中间6层为64维。最后取得了很好的效果，QMax达到了之前的10倍。</p><p>同时，我们在每一层网络后都加入了一个20%的dropout层（除了输入层以及输出层之前），激活函数选用的ReLU函数。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>算法的训练过程如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">In(<span class="number">4</span>): ai.cycle()</div><div class="line">t= <span class="number">11000</span></div><div class="line">[<span class="number">654.53412</span>, <span class="number">322.84866</span>, <span class="number">86.578796</span>, <span class="number">1414.0239</span>]</div><div class="line">Games played  <span class="number">539</span></div><div class="line">Epoch Max score <span class="number">144</span></div><div class="line">Epoch Mean score <span class="number">30.3580705009</span></div><div class="line">t= <span class="number">21000</span></div><div class="line">[<span class="number">474.16202</span>, <span class="number">251.2959</span>, <span class="number">79.489487</span>, <span class="number">1243.3118</span>]</div><div class="line">Games played  <span class="number">774</span></div><div class="line">Epoch Max score <span class="number">223</span></div><div class="line">Epoch Mean score <span class="number">42.6255319149</span></div><div class="line">t= <span class="number">31000</span></div><div class="line">[<span class="number">388.32297</span>, <span class="number">202.05305</span>, <span class="number">79.290771</span>, <span class="number">1086.0581</span>]</div><div class="line">Games played  <span class="number">1020</span></div><div class="line">Epoch Max score <span class="number">153</span></div><div class="line">Epoch Mean score <span class="number">40.5081300813</span></div><div class="line">t= <span class="number">41000</span></div><div class="line">[<span class="number">470.96552</span>, <span class="number">234.70471</span>, <span class="number">129.87579</span>, <span class="number">1320.3688</span>]</div><div class="line">Games played  <span class="number">1281</span></div><div class="line">Epoch Max score <span class="number">251</span></div><div class="line">Epoch Mean score <span class="number">38.3908045977</span></div><div class="line">t= <span class="number">51000</span></div><div class="line">[<span class="number">549.32666</span>, <span class="number">203.20442</span>, <span class="number">176.22263</span>, <span class="number">1079.8307</span>]</div><div class="line">Games played  <span class="number">1546</span></div><div class="line">Epoch Max score <span class="number">226</span></div><div class="line">Epoch Mean score <span class="number">37.7773584906</span></div><div class="line">t= <span class="number">61000</span></div><div class="line">[<span class="number">610.16583</span>, <span class="number">232.79211</span>, <span class="number">224.97626</span>, <span class="number">1264.9712</span>]</div><div class="line">Games played  <span class="number">1759</span></div><div class="line">Epoch Max score <span class="number">484</span></div><div class="line">Epoch Mean score <span class="number">46.5774647887</span></div><div class="line">...</div></pre></td></tr></table></figure><p>实验结果：</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/table2.png" alt="Table2"></p><p>可以看出，我们的算法性能完全超越了随机算法。</p><p>下面是我们训练大概250,000次后的结果：</p><p><img src="https://github.com/drscott173/ml_capstone/raw/master/figures/qmax_win.png" alt="Best Qmax"></p><p>关于随机算法以及Q-learning算法的动画展示可以参照项目地址。</p><p>但是我们发现小车还是会撞到障碍物，这经常发生在小车碰撞之后的恢复过程中。这时候小车可能到达地图的角落，充满障碍物。但是因为小车只有三个传感器，即使在背面加上还是太少了，所以信息捕捉不够。这是模型需要改进的地方。我们可以事先在小车中存储一个类似于地图的数据。</p><p>另外，由于小车一直是匀速行驶，如果加入加速，减速等过程，应当会使得性能更好。但是由于时间原因，我们并没有进一步改进。</p><h4 id="进一步工作"><a href="#进一步工作" class="headerlink" title="进一步工作"></a>进一步工作</h4><p>本次实验仅仅是在二维环境中进行的。但是严格来说并不是复杂环境的最佳简化。三维环境更加贴近现实情况，例如我们可以设计一个飞行的环境模拟。</p><h3 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h3><p>[1]. <a href="https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.58wi2s7ct" target="_blank" rel="external">https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.58wi2s7ct</a></p><p>[2]. <a href="https://github.com/songrotek/DQN-Atari-Tensorflow/blob/master/BrainDQN_Nature.py" target="_blank" rel="external">https://github.com/songrotek/DQN-Atari-Tensorflow/blob/master/BrainDQN_Nature.py</a></p><p>[3]. <a href="https://www.continuum.io/why-anaconda" target="_blank" rel="external">https://www.continuum.io/why-anaconda</a></p><p>[4]. <a href="https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#anaconda-installation" target="_blank" rel="external">https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#anaconda-installation</a></p><p>[5]. <a href="http://www.pygame.org/wiki/GettingStarted" target="_blank" rel="external">http://www.pygame.org/wiki/GettingStarted</a></p><p>[6]. <a href="http://www.pymunk.org/en/latest/" target="_blank" rel="external">http://www.pymunk.org/en/latest/</a></p><p>[7]. <a href="http://www.numpy.org/" target="_blank" rel="external">http://www.numpy.org/</a></p><p>[8]. <a href="http://www.scipy.org/" target="_blank" rel="external">http://www.scipy.org/</a></p><p>[9]. <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" target="_blank" rel="external">https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf</a></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/27/Store-Management-System/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/06/27/Store-Management-System/" itemprop="url">Store Management System</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-27T11:32:49+08:00">2017-06-27 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/06/27/Store-Management-System/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/06/27/Store-Management-System/" itemprop="commentsCount"></span> </a></span><span id="/2017/06/27/Store-Management-System/" class="leancloud_visitors" data-flag-title="Store Management System"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="SMS"><a href="#SMS" class="headerlink" title="SMS"></a>SMS</h1><p>SMS (Store Management System), 一个简单的网店管理系统。</p><p>源码：<a href="https://github.com/ewanlee/sms" target="_blank" rel="external">https://github.com/ewanlee/sms</a></p><p>这是一个用于展示微服务的 proof-of-concept 应用，运用了Spring Boot, Spring Cloud 以及 Docker部署。</p><h2 id="核心服务"><a href="#核心服务" class="headerlink" title="核心服务"></a>核心服务</h2><p>SHOP 分成了三个核心微服务，它们都是独立开发的，采用了Spring MVC架构：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/services.png" alt="services"></p><p>Order service</p><p>进行订单的添加，删除，以及显示</p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Path</th><th style="text-align:center">Description</th><th style="text-align:center">User authenticated</th><th style="text-align:center">Available from UI</th></tr></thead><tbody><tr><td style="text-align:center">GET</td><td style="text-align:center">/</td><td style="text-align:center">返回订单列表</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/form</td><td style="text-align:center">增加订单，并进行用户选择</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/line</td><td style="text-align:center">增加一条订单到数据库</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/{id}</td><td style="text-align:center">显示某一条订单的详情</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/</td><td style="text-align:center">增加订单行为</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">DELETE</td><td style="text-align:center">/{id}</td><td style="text-align:center">删除订单</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr></tbody></table><p>Customer service</p><p>进行用户的添加，删除，以及显示</p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Path</th><th style="text-align:center">Description</th><th style="text-align:center">User authenticated</th><th style="text-align:center">Available from UI</th></tr></thead><tbody><tr><td style="text-align:center">GET</td><td style="text-align:center">/list</td><td style="text-align:center">返回用户列表</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/{id}</td><td style="text-align:center">返回指定id的用户详情</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/form</td><td style="text-align:center">返回增加用户界面</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/form</td><td style="text-align:center">增加用户</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr><td style="text-align:center">PUT</td><td style="text-align:center">/{id}</td><td style="text-align:center">增加用户行为</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">DELETE</td><td style="text-align:center">/{id}</td><td style="text-align:center">删除用户</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr></tbody></table><p>Catalog service</p><p>进行商品的添加，删除，以及显示</p><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">Path</th><th style="text-align:center">Description</th><th style="text-align:center">User authenticated</th><th style="text-align:center">Available from UI</th></tr></thead><tbody><tr><td style="text-align:center">GET</td><td style="text-align:center">/list</td><td style="text-align:center">返回商品列表</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/{id}</td><td style="text-align:center">返回指定id的商品详情</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">GET</td><td style="text-align:center">/form</td><td style="text-align:center">返回增加商品界面</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">POST</td><td style="text-align:center">/form</td><td style="text-align:center">增加商品</td><td style="text-align:center">无</td><td style="text-align:center">无</td></tr><tr><td style="text-align:center">PUT</td><td style="text-align:center">/{id}</td><td style="text-align:center">增加商品行为</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">DELETE</td><td style="text-align:center">/{id}</td><td style="text-align:center">删除商品</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">TEXT_HTML_VALUE</td><td style="text-align:center">/searchForm</td><td style="text-align:center">返回搜索界面</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr><tr><td style="text-align:center">TEXT_HTML_VALUE</td><td style="text-align:center">/searchByName</td><td style="text-align:center">返回搜索结果</td><td style="text-align:center">无</td><td style="text-align:center">有</td></tr></tbody></table><p>注意</p><ul><li>每个微服务都有自己的数据库，因此互相之间没有直接访问数据库的接口</li><li>这里的数据库使用的是spring框架自带的数据库</li><li>服务到服务的通信非常简单，通过暴露的接口即可</li></ul><h2 id="架构服务"><a href="#架构服务" class="headerlink" title="架构服务"></a>架构服务</h2><p>分布式系统中有一些通用的模式，Spring Cloud框架都有提供，在本项目中仅仅运用了一小部分：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/arch.png" alt="arch"></p><h3 id="API-网关"><a href="#API-网关" class="headerlink" title="API 网关"></a>API 网关</h3><p>可以看到，有三个核心服务，它将外部API暴露给客户端。在一个现实世界的系统中，核心服务的数量可以非常快速地增长，并且整个系统的复杂性更是急剧增加。实际上，一个复杂的网页可能需要渲染数百个服务。</p><p>理论上，客户端可以直接向每个微服务器发出请求。但是显然，这将面临很大的挑战以及限制。比如必须要知道所有端点的地址。</p><p>通常一个更好的方法是使用API网关。它是系统中的单个入口点，用于通过将请求路由到适当的后端服务或通过调用多个后端服务并聚合结果来处理请求。此外，它还可以用于认证，压力测试，服务迁移，静态响应处理，主动流量管理等</p><p>Netflix开辟了这样一个优势服务，现在使用Spring Cloud，我们可以通过一个@EnableZuulProxy注释来实现。在这个项目中使用了Zuul存储静态内容（ui应用程序），并将请求路由到适当的微服务器。</p><p>Zuul使用服务发现机制来定位服务实例以及断路器和负载平衡器，如下所述。</p><h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p>另外一个众所周知的架构模式便是服务发现机制。它可以进行服务实例网络位置的动态检测。当应用需要扩展、容错或者升级的时候就可以自动为服务实例分配地址。</p><p>服务发现机制的核心是注册阶段。本项目使用了 Netflix Eureka。 Eureka是一个客户端的发现模式，因为很多网络应用都需要客户端自己去确定特定服务的地址（使用注册服务器）并且进行请求的负载均衡。</p><p>使用Spring Boot时，只要在pom文件中加入spring-cloud-starter-eureka-server依赖并且使用@EnableEurekaServer注解即可使用该服务。</p><h3 id="负载均衡、断路器以及Http客户端"><a href="#负载均衡、断路器以及Http客户端" class="headerlink" title="负载均衡、断路器以及Http客户端"></a>负载均衡、断路器以及Http客户端</h3><p>Netflix还提供了另外一些十分好用的工具。</p><h4 id="Ribbon"><a href="#Ribbon" class="headerlink" title="Ribbon"></a>Ribbon</h4><p>Ribbon 是一个客户端的负载均衡器。相比传统的均衡器，你可以之间链接到相关服务。Ribbon已经和Spring Cloud以及服务发现机制集成在了一起。 Eureka Client 提供了一个可用服务器的动态列表供 Ribbon 进行服务器之间的均衡。</p><h4 id="Hystrix"><a href="#Hystrix" class="headerlink" title="Hystrix"></a>Hystrix</h4><p>Hystrix 是断路器模式的具体实现，其可以调节网络访问依赖中经常出现的延迟以及错误。其主要目的是为了阻断在分布式环境中大量微服务极易出现的级联错误，使得系统尽快重新上线。Hystrix还提供了一个监控页面 （下面将会看到）。</p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p>前期准备：</p><ul><li>网络</li></ul><ul><li>安装 Docker 以及 Docker compose</li></ul><p>运行命令：</p><ul><li><code>cd microservice-demo/</code>执行<code>mvn clean package</code></li><li><code>cd ../docker/</code>执行<code>docker-compose build</code>以及<code>docker-compose up</code></li></ul><p>重要端口：</p><ul><li><a href="http://127.0.0.1:8080" target="_blank" rel="external">http://127.0.0.1:8080</a> - 网关</li><li><a href="http://127.0.0.1:8761" target="_blank" rel="external">http://127.0.0.1:8761</a> - Eureka Dashboard</li></ul><p>注意：</p><p><strong>应用启动之后如果遇到 Whitelabel Error Page 错误请刷新页面</strong></p><h2 id="UI"><a href="#UI" class="headerlink" title="UI"></a>UI</h2><p><strong>Index</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/index_1.png" alt="index_1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/index_2.png" alt="index_2"></p><p><strong>Customer Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/customers_list.png" alt="customers_list"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/add_customer.png" alt="add_customer"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/successful.png" alt="success"></p><p><strong>Catalog Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/items_list.png" alt="items_list"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/add_item.png" alt="add_item"></p><p><strong>Order Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/orders_list.png" alt="orders_list"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/add_order.png" alt="add_order"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/order_details.png" alt="order_details"></p><p><strong>Eukera Service</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/eukera.png" alt="eukera"></p><p><strong>Hystrix Dashboard</strong></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/microservice/lec-proj/ui/order_hystrix.png" alt="order_hystrix"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/14/Learning-to-act-by-predicting-the-future/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/06/14/Learning-to-act-by-predicting-the-future/" itemprop="url">Learning to act by predicting the future</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-14T19:46:19+08:00">2017-06-14 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/06/14/Learning-to-act-by-predicting-the-future/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/06/14/Learning-to-act-by-predicting-the-future/" itemprop="commentsCount"></span> </a></span><span id="/2017/06/14/Learning-to-act-by-predicting-the-future/" class="leancloud_visitors" data-flag-title="Learning to act by predicting the future"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><em>论文 <a href="https://openreview.net/forum?id=rJLS7qKel" target="_blank" rel="external">Learning to act by predicting the future</a></em></p><p>这篇论文提出的 DFP (Direct Future Prediction) 赢得了2016年 Virtual Doom AI Competition 的 “Full Deathmatch” 环节的比赛。Virtual Doom 是一个对战性的第一人称射击型游戏，根据玩家击杀数判定胜负。为了体现出模型的泛化能力， 训练过程中使用的地图不在比赛过程中出现。DFP的性能超出了第二名（Deep LSTM Q-Network）50%，并且其模型以及训练数据更加简洁，表现出了DFP模型的优越性。</p><p>机器学习问题可以分为监督学习问题，无监督学习问题以及强化学习问题。监督学习主要是学习一个输入到输出的映射函数，无监督学习更加关注如何挖掘数据本身的隐含结构，强化学习是一个面向目标的策略学习问题。</p><p>因此采用强化学习的方法使得机器人在Deathmatch游戏中表现良好十分合适。因为这是一个直接面向目标的问题 （在游戏中取得最大的击杀数）。所以 DQN 以及 A3C 这样的算法应运而生，并且取得了巨大的成功。但是这篇论文提出了一个不同的观点。它引用了Jordan &amp; Rumelhart (1992) 这篇论文中提出的一个观点：</p><blockquote><p>对于一个可以与环境进行交互的学习问题，如果环境提供的反馈是稀疏的标量 （例如，对于一个五子棋问题，反馈只在最后胜负已分时给出，并且只是一个类似+1，-1的标量反馈），采用传统的强化学习算法会十分有效；但是如果环境给出的反馈是一个即时密集的多维度反馈 （在短时间内具有很大的信息比特率），监督学习算法更具优势。</p></blockquote><p>由于监督学习方面的研究已经非常成熟，最近十分火热的深度学习更是在很多方面都取得了很好的结果，因此，如果我们能够把强化学习问题在某种程度上转化为一个监督学习问题，可以使得问题的求解大大简化。</p><p>那么现在的问题是，我们要如何设计模型，从而可以得到一个监督信号呢？可以想到，我们唯一拥有的数据就是机器人通过与环境的交互得到的状态转移 （对于游戏来说就是玩家在游戏中采取不同的行为得到的环境的反馈，例如，玩家使用一个血包可以是的生命值回复；向左转可以使得画面发生变化等等）。我们可以对这些数据进行特殊的设计，从而能够满足我们的要求。</p><p>具体来说，我们不再简单使用传统强化学习问题中单一的状态 （例如游戏中的一帧画面）与对应的回报。我们把单一的状态拆分开来，对于原始的图像，声音等信息原样保留，形成一个 ”感觉输入流 (sensory input stream)“ ，很明显它是一个高维的变量；另外，我们从这些原始的信息中提取出能够代表我们学习目标的测度 （例如健康度，剩余弹药数以及击杀数等），形成一个 ”测度流 (measurement stream)“ ，它是一个低维的变量 （因为只包含几个重要的变量）。<strong>注意，这里的stream不是代表了好几个时间步，而是代表它是多个测度的一个集合。</strong></p><p>这样做有什么好处呢？一个传统的强化学习问题，其训练对象就是最大化一个关于reward的函数。一般reward都是人为给定的 （还是拿五子棋举例，最后玩家赢了，回报就是一个正数， 反之就是负数），但是这就使得学习问题的方差变得很大，训练过程十分不稳定，收敛速度慢，甚至可能不收敛。因此，<strong>我们希望reward的值不要过于随机化，能够通过某些监督信号来减少其方差</strong>。这里就可以体现出我们之前进行状态分解的优势。我们可以将reward表示成 measurement stream 的函数，由于measurement是agent与真实环境进行交互时得到的，属于一种监督信号，这很好的满足了我们的需求。所以最后我们的训练对象由最大化一个关于reward的函数变成了最大化一个关于measurement stream的函数。而这个<strong>measurement stream可以认为是传统强化学习问题中的reward</strong>。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>现在我们正式地定义DFP模型。在每一个时间步$t$，agent接收一个观察 （转移到一个状态）$O_t$, 根据这个观察 （状态）的某些固有属性从可行的动作集合中选取一个动作执行。$O_t$详细定义如下：<br>$$<br>\mathbf{o}_t = (\mathbf{s}_t, \mathbf{m}_t)<br>$$<br>整个状态转移过程中，我们希望最大化目标，前面提到了，它是关于measurement stream的函数：<br>$$<br>\mathbf{f} = (\mathbf{m}_{t+\tau_1}-\mathbf{m}_t, \cdots, \mathbf{m}_{t+\tau_n}-\mathbf{m}_t)<br>$$<br>$\tau_1, \cdots, \tau_n$ 代表与当前时间步$t$的一个偏差。至于为什么不直接最大化measurement stream而是最大化一个差值，我认为作者可能是有如下考虑：</p><ol><li>借鉴了n-step Q-learning 的做法。</li><li>由于模型是为了预测当前时间步$t$的measurement stream，因此优化对象中应该包含当前的measurement stream。</li></ol><p>最后，<br>$$<br>\mathbf{Goal} \; = \; u(\mathbf{f};\mathbf{g})<br>$$<br>一般线性函数即可满足我们的需求，即<br>$$<br>u(\mathbf{f};\mathbf{g}) = \mathbf{g}^{\text{T}}\mathbf{f}<br>$$<br>注意到现在我们的问题变成了一个监督学习的问题。为了训练模型，我们需要预测目标，然后再与真实的目标比较，通过最小化误差来进行学习。那么我们现在定义这个预测过程。注意到，由于目标只是measurement stream的函数，而且<strong>参数一般都是确定的，不需要进行学习</strong>。因此，我们的预测对象是measurement stream而不是目标。下面我们定义一个预测器F:<br>$$<br>\mathbf{p}_t^a = F(\mathbf{o}_t, a, \mathbf{g};\theta)<br>$$<br><strong>注意，这里的$\text{g}$和(4)中是不一样的，它代表目标</strong>。$p_t^a$代表在$t$时间步下，执行行为$a$所得到的reward，也即measurement stream。</p><p>当训练完成的时候，我们就要用这个预测器F进行决策，策略定义如下：<br>$$<br>a_t = {\arg\max}_{a \in \mathcal{A}} \mathbf{g}^{\text{T}}F(\mathbf{o}_t, a, \mathbf{g};\theta)<br>$$<br>注意到，模型实际训练的过程中采用的是$\varepsilon\text{-greedy}$策略。这里可以看出，在训练过程中或者测试过程中，我们要手动的计算出$u(\text{f};\text{g})$的值。下面我们详细的剖析模型的训练过程。</p><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>对于传统的强化学习算法，例如Q-learning，其训练过程是一个在线学习的过程，也即其训练集是一个一个进行输入的，每输入一次都进行一次参数的更新。由于Q-learning以及DFP都是采用了MC (Monte Carlo) 策略，这种训练过程可能十分不稳定 （由于训练最开始时我们的训练数据是通过一个随机策略与环境交互产生的），致使收敛速度很慢，需要很多的episodes进行训练。这里采用了和DQN (Deep Q-Network) 相同的 experience replay技术。具体来说，就是保存每次agent与环境交互后产生的数据对</p><p>$\langle \mathbf{o}_i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle$ 到数据集$\mathcal{D}$中，即$\mathcal{D} = \{\langle \mathbf{o}_ i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle \}_{i=1}^N$. 注意这里的$N$个数据对并不是直接顺序产生的，而是从当前episode中到当前时间步时，所有的数据对中选取最近的$M$个，再从其中随机抽样$N$个。另外，每隔k步才进行一次参数的更新，因为$\mathbf{f}$的计算需要考虑到32个时间步之后的数据，因此$k \ge 32$（实验部分将详细介绍）。DQN 给出了具体的实现：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/dqn_alg.png" alt="dqn_alg"></p><p>另外需要注意的是</p><p>有了训练集，我们现在定义w代价函数：<br>$$<br>\mathcal{L}(\theta) = \sum_{i=1}^{N} |F(\mathbf{o}_i, a_i, \mathbf{g}_i;\theta) - \mathbf{f}_i|^2<br>$$<br>我们来对比一下 DQN 的代价函数：<br>$$<br>L_i(\theta_i)= \mathbb{E}_{s, a \sim \rho(\cdot)} \left[ y_i - Q(s, a;\theta_i) \right],<br>$$<br>其中$y_i = \mathbb{E}_{s^{\prime} \sim \varepsilon}[ r + \gamma \max_{a^{\prime}} Q(s^{\prime}, a^{\prime};\theta_{i-1}) ]$ 。</p><p>这里的$y_i$是上一次模型的输出，其值随着更新次数的增加也在不断变化。因此从这里也能看出DFP是一个监督学习算法。</p><p>训练过程中我们为了解决报告最开始提出的目标随着时间发生改变的问题，采用了两种目标进行测试：</p><ol><li>目标向量$\mathbf{g}$ （不是目标）在整个训练过程中不变</li><li>目标向量在每个episode结束时随机变化</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>下图是DFP模型的网络结构：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/network.png" alt="network"></p><p>从图中可以看出，该网络有三个输入模块。一个感知模块$S(s)$，一个测度模型$M(m)$以及一个目标模块$G(g)$。在实验中，$s$代表一张图片，$S$代表一个卷积神经网络。测度模块以及目标模块都是由一个全连接神经网络构成。三者的输出连接在一起，形成一个联合的输入表示，供后续算法使用：<br>$$<br>\mathbf{j} = J(\mathbf{s, m, g}) = \langle S(\mathbf{s}), M(\mathbf{m}), G(\mathbf{g}) \rangle<br>$$<br>DFP网络采用了DQN的做法，一次性输出所有action对应的measurement stream。但是我们希望能够着重关注对action之间差异的学习。因此采用了Wang et al. (ICML 2016) 这篇文章中才去的做法，将预测模块分为两个stream，一个期望stream $E(\text{j})$ 以及一个action stream $A(\text{j})$。注意这两个stream都是一个全连接的神经网络。期望stream的目标是预测所有action能够获得的measurement stream的期望。Action stream关注不同action之间的差异。其中，$A(\text{j}) = \langle A^1(\text{j}), \cdots, A^{w}(\text{j}) \rangle$，$w = |\mathcal{A}|$代表所有可能action的个数。同时我们还在加入了一个正则化层：<br>$$<br>\overline{A^{i}}(\mathbf{j}) = A^{i}(\mathbf{j}) - \frac{1}{w}\sum_{k=1}^{w} A^{k}(\mathbf{j})<br>$$<br>正则化层对每一个action的预测值减去了所有action预测值的期望，这样就强制期望stream去学习这个期望，这样action stream就可以着重关注不同action之间的差异。最后，网络的输出如下：<br>$$<br>\mathbf{p} = \langle \mathbf{p}^{a_1}, \cdots, \mathbf{p}^{a_w} \rangle = \langle \overline{A^1}(\mathbf{j})+E(\mathbf{j}), \cdots, \overline{A^w}(\mathbf{j})+E(\mathbf{j}) \rangle<br>$$<br>为了验证网络中使用的三个辅助结构（measurement stream输入，expectation-action分解以及action正则化层）的作用，我们进行了测试。我们基于D3场景（下面实验部分提及）随机产生了100个地图场景用以训练。同时采用basic网络 （下面实验部分提及），最后的实验结果如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t3.png" alt="t3"></p><p>可以看出，expectation-action分解的作用最大，同时我们设计的measurement stream也是十分重要的。</p><h2 id="实验及结果"><a href="#实验及结果" class="headerlink" title="实验及结果"></a>实验及结果</h2><p>具体的实验场景见下图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/game.png" alt="game"></p><p>在前两个场景中，agent可以采取三个动作，向前移动、向左转、向右转。这样一共就有8种动作组合。采用的测度只有一种，就是血量。在后两个场景中，agent可以采取八个动作组合，分别是向前移动、向后移动、向左转、向右转、向左扫射，向右扫射、奔跑以及射击。这样一共就有256个动作组合。采用的测度一共有三种，血量，弹药数以及击杀数。这里我认为存在一个可以改进的地方，应该排除掉不合理的动作组合，例如同时向左转以及向右转。这样可以减少搜索空间，加速收敛，同时可以提高策略的质量。</p><p>实验中网络的结构与DQN的结构十分类似，参数也尽可能相近，就是为了比较起来比较公平。具体来说，实验中采用了两种网络，basic以及large，结构相同，但是参数数量不同：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/bl.png" alt="bl"></p><p>Basic网络的参数与DQN比较接近，以便比较。两个网络在所有的非终止层后都加入了一个非线性层，采用的激活函数为Leaky ReLU，具体函数为：<br>$$<br>\mathbf{LReLU}(x) = \max(x, 0.2x)<br>$$<br>参数初始化方法采用了He Initialization，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">W = np.random.randn(node_in, node_out) / np.sqrt(node_in / <span class="number">2</span>)</div></pre></td></tr></table></figure><p>Agent以episode为单位进行训练和测试。每一个episode拥有525个时间步（大约一分钟），如果agent死亡那么episode也会终止。同时将时间偏置$\tau_1, \cdots, \tau_n$设置为1, 2, 4, 8, 16, 32。最后结果表明只有最新的三个时间步（8, 16, 32）对结果有贡献，贡献比例为 1:1:2。</p><p>另外，输入图像被转换成灰度图像，measurement stream并不是直接输入，而是进行了正则化 （除以标准差）。同时，我们还在训练以及测试过程中使用frame skipping技术。Agent每隔4帧采取一次action。这些被忽略的帧所采取的action与其之后的帧的action一致，相当于进行了一次简单的复制。另外，由于人类的反应速度肯定是比不上计算机的，因此fram skipping使得agent的行为更加接近人类。</p><p>对于之前提到的experience replay技术，实验中将M值设为20000， N设为64，k也设为64（$\ge32$）。同时为了能够更高效的获得训练集$\mathcal{D}$，我们同时采用8个agent并行运行。训练时采用的梯度下降算法为Adam算法，参数设置如下：$\beta_1=0.95, \;\beta_2=0.999,\;\varepsilon=10^{-4}$。Basic网络训练了800,000次mini-batch迭代，large网络训练了2,000,000次。算法实现<a href="https://github.com/IntelVCL/DirectFuturePrediction。" target="_blank" rel="external">https://github.com/IntelVCL/DirectFuturePrediction。</a></p><p>下面介绍我们的baselines。我们同三个算法进行了比较：DQN (Mnih et al., 2015), A3C (Mnih et al., 2016), 以及 DSR (Kulkarni et al., 2016b)。DQN由于其在Atari游戏上的优异效果成为了视觉控制的标准baseline。A3C更是这个领域中的最好的算法。DSR也在Virtual Doom平台上进行了实验。所以我们挑选了这三个具有代表意义的算法。</p><p>对于这三个算法我们都使用了Github上的开源实现：DQN (<a href="https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner" target="_blank" rel="external">https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner</a>) 、DSR (<a href="https://github.com/Ardavans/DSR" target="_blank" rel="external">https://github.com/Ardavans/DSR</a>), 以及 A3C (<a href="https://github.com/muupan/async-rl)。前两个都是作者提供的源码，最后的A3C是一个独立开发者的个人实现。" target="_blank" rel="external">https://github.com/muupan/async-rl)。前两个都是作者提供的源码，最后的A3C是一个独立开发者的个人实现。</a></p><p>对于DQN以及DSR我们测试了三个学习速率：默认值（0.00025），0.00005以及0.00002。其他参数直接采用默认值。对于A3C算法，为了训练更快，前两个任务我们采用了5个学习速率 ({2, 4, 8, 16, 32} · $10^{-4}$)。后两个任务我们训练了20个模型，每个模型的学习速率从一个范围从$10^{-4}$到$10^{-2}$的log-uniform分布中进行采样，$\beta$值（熵正则项）从一个范围从$10^{-4}$到$10^{-}$的lo1g-uniform分布中进行采样。结果选取最好的。</p><p>最终结果如下所示：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t1.png" alt="t1"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/f3.png" alt="f3"></p><p>在前两个游戏场景中，模型尝试最大化血量；在后两个场景中尝试最大化血量、弹药数以及击杀数的一个线性组合，参数为0.5, 0.5, 1。因为游戏更加侧重于通过击杀数判断胜负。所有的数据都是对三次训练结果进行平均，曲线图采样点的个数为$3 \times 50,000$。可以看出，DFP模型取得了最好的结果。其中DSR算法由于训练速度过慢，所以我们只在D1场景（也进行了将近10天的训练）进行了测试。</p><p>下面进行模型泛化能力的测试，我们基于D3以及D4两个场景分别随机产生100个随机场景。其中90个用于训练，剩下10个用于测试。最后结果如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/f2.png" alt="f2"></p><p>其中最后一列采用了large网络。可以看出，从复杂场景训练之后，在简单场景上的泛化能力往往不错，虽然两者规则不同。但是反之则不可以。</p><p>接下来进行学习变化目标能力的测试。结果见下图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t3g.png" alt="t3g"></p><p>其中采用第二列的策略时，agent并不知道每一个measurement的相对重要性；最后一列，agent事先并不知道哪一个measurement是不需要考虑的。但是最后测试时，效果都很好，而且在固定目标策略没有见过的目标上的效果要更好。说明DFP模型对于变化目标的学习能力优异。</p><p>最后我们单独对measurement stream时间偏置的重要性进行测试，我们采用的是D3-tx训练集，最后结果如图：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/l2a_p2f/t4.png" alt="t4"></p><p>相比较而言，采用更多的时间偏置可以达到更好的效果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>现在强化学习问题关注的重点还是在value function的估计上，深度强化学习模型一般采用一个深度网络直接对value function进行估计。这篇论文的创新点在于，在使用深度网络之前，对value function进行了两次额外的映射。第一次是用measurement stream来代替reward，使得reward具有更强的状态表示能力；其次，对measurement stream再次进行了一个函数映射，采用了时间偏置，借鉴了n-step Q-learning的思想。最后，再将输出作为深度网络的输入，进行value function的估计。最后的实验结果证明这种想法是有其正确性的。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/27/k-Armed-Bandit-Problem/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/27/k-Armed-Bandit-Problem/" itemprop="url">k-Armed Bandit Problem</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-27T12:27:49+08:00">2017-05-27 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/27/k-Armed-Bandit-Problem/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/27/k-Armed-Bandit-Problem/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/27/k-Armed-Bandit-Problem/" class="leancloud_visitors" data-flag-title="k-Armed Bandit Problem"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or <em>time steps</em>.</p><p>This is the original form of the <em>k-armed bandit problem</em>. Each of the <em>k</em> actions has an excepted or mean reward given that action is selected; let us call this <em>value</em> of that action. We denote the action selected on time step <em>t</em> as $A_{t}$, and the corresponding reward as $R_{t}$. The value then of an arbitrary action <em>a</em>, denoted $q_{\star}(a)$, is the excepted reward given that <em>a</em> is selected:<br>$$<br>q_{\star}(a) = \mathbb{E}[R_t|A_t=a]<br>$$<br>If you knew the value of each action, then it would be trivial to solve the <em>k</em>-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action <em>a</em> at time <em>t</em> as $Q_{t}(a) \approx q_{\star}(a)$.</p><p>We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when action is selected. One natural way to estimate this is by averaging the rewards actually received:</p><p>$$Q_t(a) \doteq \frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbf{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbf{1}_{A_i=a}}$$</p><p>where $\mathbf{1}_{\text{predicate}}$ denotes the random variable that is 1 if <em>predicate</em> is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as $Q_1(a) = 0$. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_{\star} (a)$. We call this the <strong><em>sample-average</em></strong> method for estimating action values because each estimate is an average of the sample of relevant rewards.</p><p>The simplest action selection rule is to select the action (or one of the actions) with highest estimated action value, that is, to select at step <em>t</em> one of the greedy actions, $A_t^\star$ for which $Q_t(A_t^\star) = \max_a Q_t(a)$. This <em>greedy</em> action selection method can be written as<br>$$<br>A_t \doteq argmax_a Q_t(a)<br>$$<br>Naturally, we could use the <em>$\epsilon$-greedy</em> method rather the <em>greedy</em> method. We’ll show their difference on the performance. Now, let’s jump into the implementation details. In order to be able to see the results quickly, we set to <em>k</em> to be <em>10</em>. The first, we generate 10 stationary probability distributions that we’ll sample from to generate action values. The generate method is below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data=np.random.randn(<span class="number">200</span>,<span class="number">10</span>) + np.random.randn(<span class="number">10</span>)</div></pre></td></tr></table></figure><p>We first generate randomly 10 true excepted values by <code>np.random.randn(10)</code>, then I’m going to change the variance to 1. So we get 10 stationary probability distributions. The visualizations are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/action_value_distributions.png" alt="action_value_distributions"></p><p>We’re going to compare how different $\epsilon$ values affect the end result.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">epsilonGreedy</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    epsilons = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.01</span>]</div><div class="line">    bandits = []</div><div class="line">    <span class="keyword">for</span> epsInd, eps <span class="keyword">in</span> enumerate(epsilons):</div><div class="line">        bandits.append([Bandit(epsilon=eps, sampleAverages=<span class="keyword">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)])</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, counts <span class="keyword">in</span> zip(epsilons, bestActionCounts):</div><div class="line">        plt.plot(counts, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, rewards <span class="keyword">in</span> zip(epsilons, averageRewards):</div><div class="line">        plt.plot(rewards, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Before we go into the details, we introduce the <strong>Bandit</strong> object first.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bandit</span>:</span></div><div class="line">    <span class="comment"># @kArm: # of arms</span></div><div class="line">    <span class="comment"># @epsilon: probability for exploration in epsilon-greedy algorithm</span></div><div class="line">    <span class="comment"># @initial: initial estimation for each action</span></div><div class="line">    <span class="comment"># @stepSize: constant step size for updating estimations</span></div><div class="line">    <span class="comment"># @sampleAverages: if True, use sample averages to update estimations instead of constant step size</span></div><div class="line">    <span class="comment"># @UCB: if not None, use UCB algorithm to select action</span></div><div class="line">    <span class="comment"># @gradient: if True, use gradient based bandit algorithm</span></div><div class="line">    <span class="comment"># @gradientBaseline: if True, use average reward as baseline for gradient based bandit algorithm</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kArm=<span class="number">10</span>, epsilon=<span class="number">0.</span>, initial=<span class="number">0.</span>, stepSize=<span class="number">0.1</span>, sampleAverages=False, UCBParam=None, gradient=False, gradientBaseline=False, trueReward=<span class="number">0.</span>)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div></pre></td></tr></table></figure><p>For now we just introduce <em>sample-average</em> method, so skip other methods parameters. Let us see the initialization method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, kArm=<span class="number">10</span>, epsilon=<span class="number">0.</span>, initial=<span class="number">0.</span>, stepSize=<span class="number">0.1</span>, sampleAverages=False, UCBParam=None,</span></span></div><div class="line">             gradient=False, gradientBaseline=False, trueReward=<span class="number">0.</span>):</div><div class="line">    self.k = kArm</div><div class="line">    self.stepSize = stepSize</div><div class="line">    self.sampleAverages = sampleAverages</div><div class="line">    self.indices = np.arange(self.k)</div><div class="line">    self.time = <span class="number">0</span></div><div class="line">    self.UCBParam = UCBParam</div><div class="line">    self.gradient = gradient</div><div class="line">    self.gradientBaseline = gradientBaseline</div><div class="line">    self.averageReward = <span class="number">0</span></div><div class="line">    self.trueReward = trueReward</div><div class="line"></div><div class="line">    <span class="comment"># real reward for each action</span></div><div class="line">    self.qTrue = []</div><div class="line"></div><div class="line">    <span class="comment"># estimation for each action</span></div><div class="line">    self.qEst = np.zeros(self.k)</div><div class="line"></div><div class="line">    <span class="comment"># # of chosen times for each action</span></div><div class="line">    self.actionCount = []</div><div class="line"></div><div class="line">    self.epsilon = epsilon</div><div class="line"></div><div class="line">    <span class="comment"># initialize real rewards with N(0,1) distribution and estimations with desired initial value</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.k):</div><div class="line">        self.qTrue.append(np.random.randn() + trueReward)</div><div class="line">        self.qEst[i] = initial</div><div class="line">        self.actionCount.append(<span class="number">0</span>)</div><div class="line"></div><div class="line">    self.bestAction = np.argmax(self.qTrue)</div></pre></td></tr></table></figure><p>There are some important attributes. <strong>time</strong> is a number that represents the time steps now. <strong>actionCount</strong> is the times that correspond actions have been taken prior to current time steps. <strong>qTrue</strong> is a list. And each item is the true excepted value corresponding to each action. <strong>qEst</strong> is the estimate value of each action. It’s initialized to zero. <strong>epsilon</strong> is the $\epsilon$. Next, in the for loop, we initialize real rewards with N(0, 1) distribution and estimations with desired initial value. At last, the <strong>bestAction</strong> store the current best action will be take.</p><p>The next method tell us how to get the next action should be take:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># explore</span></div><div class="line">    <span class="keyword">if</span> self.epsilon &gt; <span class="number">0</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, self.epsilon) == <span class="number">1</span>:</div><div class="line">            np.random.shuffle(self.indices)</div><div class="line">            <span class="keyword">return</span> self.indices[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># exploit</span></div><div class="line">    <span class="keyword">if</span> self.UCBParam <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        UCBEst = self.qEst + \</div><div class="line">                 self.UCBParam * np.sqrt(np.log(self.time + <span class="number">1</span>) / (np.asarray(self.actionCount) + <span class="number">1</span>))</div><div class="line">        <span class="keyword">return</span> np.argmax(UCBEst)</div><div class="line">    <span class="keyword">if</span> self.gradient:</div><div class="line">        expEst = np.exp(self.qEst)</div><div class="line">        self.actionProb = expEst / np.sum(expEst)</div><div class="line">        <span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div><div class="line">    <span class="keyword">return</span> np.argmax(self.qEst)</div></pre></td></tr></table></figure><p>We can skip the second and the third if statements (we’ll introduce this two methods later). If we use <em>greedy</em> method, we just return the action that has highest value. Otherwise, we’re choosing randomly at $\epsilon$ probability.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div><div class="line">    <span class="comment"># generate the reward under N(real reward, 1)</span></div><div class="line">    reward = np.random.randn() + self.qTrue[action]</div><div class="line">    self.time += <span class="number">1</span></div><div class="line">    self.averageReward = (self.time - <span class="number">1.0</span>) / self.time * self.averageReward + reward / self.time</div><div class="line">    self.actionCount[action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> self.sampleAverages:</div><div class="line">        <span class="comment"># update estimation using sample averages</span></div><div class="line">        self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">elif</span> self.gradient:</div><div class="line">        oneHot = np.zeros(self.k)</div><div class="line">        oneHot[action] = <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> self.gradientBaseline:</div><div class="line">            baseline = self.averageReward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            baseline = <span class="number">0</span></div><div class="line">        self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># update estimation with constant step size</span></div><div class="line">        self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">return</span> reward</div></pre></td></tr></table></figure><p>Similarly, we just skip other if statements and focus on this row:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>This formula seems different with the earlier one. The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. We now turn to the question of how these averages can be computed in a computationally efficient manner, in particular, with constant memory and per-time-step computation.</p><p>To simplify notation we concentrate on a single action. Let $R_i$ now denote the reward received after <em>i</em>th selection of <em>this action</em>, and let $Q_n$ denote the estimate of its action value after it has been select $n-1$ times, which we can now write simply as<br>$$<br>Q_n \doteq \frac{R_1 + R_2 + \cdots + R_{n-1}}{n-1}<br>$$<br>The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, in this case the memory and computational requirements would grow over time as more rewards are seen.</p><p>It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given $Q_n$ and the <em>n</em>th reward, $R_n$, the new average of all <em>n</em> rewards can be computed by<br>$$<br>\begin{align}<br>Q_{n+1} &amp;\doteq \frac{1}{n}\sum_{i=1}^{n} R_i \\<br>&amp;= \frac{1}{n}(R_n + \sum_{i=1}^{n-1} R_i) \\<br>&amp;= \frac{1}{n}(R_n + (n-1) \frac{1}{n-1}\sum_{i=1}^{n-1} R_i) \\<br>&amp;= \frac{1}{n}(R_n + (n-1) Q_n) \\<br>&amp;= \frac{1}{n}(R_n + n Q_n - Q_n) \\<br>&amp;= Q_n + \frac{1}{n}[R_n - Q_n]<br>\end{align}<br>$$<br>So this is why the code is look like this:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>Back to <strong>epsilonGreedy()</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">epsilonGreedy</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    epsilons = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.01</span>]</div><div class="line">    bandits = []</div><div class="line">    <span class="keyword">for</span> epsInd, eps <span class="keyword">in</span> enumerate(epsilons):</div><div class="line">        bandits.append([Bandit(epsilon=eps, sampleAverages=<span class="keyword">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)])</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, counts <span class="keyword">in</span> zip(epsilons, bestActionCounts):</div><div class="line">        plt.plot(counts, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    <span class="keyword">for</span> eps, rewards <span class="keyword">in</span> zip(epsilons, averageRewards):</div><div class="line">        plt.plot(rewards, label=<span class="string">'epsilon = '</span>+str(eps))</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Now, we get <strong>nBandits</strong> bandits and each bandit has 10 arm. Then we use a bandit simulation to simulate this process.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">banditSimulation</span><span class="params">(nBandits, time, bandits)</span>:</span></div><div class="line">    bestActionCounts = [np.zeros(time, dtype=<span class="string">'float'</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, len(bandits))]</div><div class="line">    averageRewards = [np.zeros(time, dtype=<span class="string">'float'</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, len(bandits))]</div><div class="line">    <span class="keyword">for</span> banditInd, bandit <span class="keyword">in</span> enumerate(bandits):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nBandits):</div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, time):</div><div class="line">                action = bandit[i].getAction()</div><div class="line">                reward = bandit[i].takeAction(action)</div><div class="line">                averageRewards[banditInd][t] += reward</div><div class="line">                <span class="keyword">if</span> action == bandit[i].bestAction:</div><div class="line">                    bestActionCounts[banditInd][t] += <span class="number">1</span></div><div class="line">        bestActionCounts[banditInd] /= nBandits</div><div class="line">        averageRewards[banditInd] /= nBandits</div><div class="line">    <span class="keyword">return</span> bestActionCounts, averageRewards</div></pre></td></tr></table></figure><p>The <strong>bandits</strong> is a list that has three item. Each item is a list that contains <strong>nBandits</strong> bandits that has a corresponding epsilon value. We calculate the average total reward of 2000 bandits is to eliminate the effect of noise. Ok, let us see the final result.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_optimal_action.png" alt="epsilon_greedy_optimal_action"><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/epsilon_greedy_average_reward.png" alt="epsilon_greedy_average_reward"></p><p>We can see the algorithm reaches the best performance when epsilon is set to 0.1.</p><p>The averaging methods discussed so far are appropriate in a stationary environment, but not if the bandit is changing over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to weight recent rewards more heavily than long-past ones. One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule for updating an average $Q_n$ of the $n-1$ past rewards is modified to be<br>$$<br>Q_{n+1} \doteq Q_n + \alpha [R_n - Q_n]<br>$$<br>where the step-size parameter $\alpha \in (0, 1]$ is constant. This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_1$:<br>$$<br>\begin{align}<br>Q_{n+1} &amp;\doteq Q_n + \alpha [R_n - Q_n] \\<br>&amp;= \alpha R_n + (1 - \alpha) Q_n \\<br>&amp;= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] \\<br>&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 Q_{n-1} \\<br>&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 R_{n-1} + \cdots + (1 - \alpha) ^ {n-1} \alpha R_1 + (1 - \alpha) ^ n Q_1 \\<br>&amp;= (1 - \alpha) ^ n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha) ^ {n-i} R_i<br>\end{align}<br>$$<br>We call this a <em>weighted average</em> because the sum of the weights is 1. In fact, the weight decays exponentially according to the exponent on $1-\alpha$. According, this is sometimes called an <em>exponential, recency-weighted average</em>.</p><p>Sometimes it is convenient to vary the step-size parameter from step to step. Let $\alpha_n(a)$ denote the step-size parameter used to process the reward received after <em>n</em>th selection of action <em>a</em>. As we noted, the choice $\alpha_n(a)=\frac{1}{n}$ results in the sample-average method, which is guaranteed to converge to the true action values by the law of the large numbers. A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:<br>$$<br>\sum_{n=1}^{\infty}\alpha_{n}(a) = \infty \; \text{and} \; \sum_{n=1}^{\infty}\alpha_{n}^{2}(a) &lt; \infty<br>$$<br>All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, $Q_1(a)$. <strong>In the language of statistics, these methods are <em>biased</em> by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant $\alpha$ (weighted avetrage), the bias is permanent, though decreasing over time. In practice, this kind of bias is usually not a problem and can sometimes be very helpful.</strong> The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be excepted. So, let us do a experiment. The first we set them all to zero and the we set them all to a constant, 5.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimisticInitialValues</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    bandits = [[], []]</div><div class="line">    bandits[<span class="number">0</span>] = [Bandit(epsilon=<span class="number">0</span>, initial=<span class="number">5</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bandits[<span class="number">1</span>] = [Bandit(epsilon=<span class="number">0.1</span>, initial=<span class="number">0</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(bestActionCounts[<span class="number">0</span>], label=<span class="string">'epsilon = 0, q = 5'</span>)</div><div class="line">    plt.plot(bestActionCounts[<span class="number">1</span>], label=<span class="string">'epsilon = 0.1, q = 0'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'% optimal action'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(averageRewards[<span class="number">0</span>], label=<span class="string">'epsilon=0, initial=5, stepSize=0.1'</span>)</div><div class="line">    plt.plot(averageRewards[<span class="number">1</span>], label=<span class="string">'epsilon=0.1, initial=0, stepSize=0.1'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The <strong>Bandit</strong> object’s <strong>takeAction()</strong> has a little difference:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values.png" alt="optimistic_initial_value_optimal_action"><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/optimistic_initial_values_average_reward.png" alt="optimistic_initial_value_optimal_action"></p><p>We can see it reaches the better performance than the $\epsilon$-greedy method, when they are all used the weighted average method. Notice that the $\epsilon$-greedy with weighted-average method is worsen than the $\epsilon$-greedy with sample-average method.</p><p>We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial state in any special way is unlikely to help with the general nonstationary case.</p><p>Exploration is needed because the estimates of the action values are uncertain. The greedy actions are those that look best at present, but some of the other actions may actually be better. $\epsilon$-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions as<br>$$<br>A_t \doteq argmax_a \left [Q_t(a) + c\sqrt{\frac{\ln{t}}{N_t(a)}}\ \right]<br>$$<br>where $N_t(a)$ denotes the number of times that action <em>a</em> has been selected prior to time <em>t</em>, and the number $c&gt;0$ controls the degree of exploration. If $N_t(a)=0$, then <em>a</em> is considered to be a maximizing action. The idea of this is called <em>upper confidence bound</em> (UCB). Let us implement it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ucb</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    bandits = [[], []]</div><div class="line">    bandits[<span class="number">0</span>] = [Bandit(epsilon=<span class="number">0</span>, stepSize=<span class="number">0.1</span>, UCBParam=<span class="number">2</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    bandits[<span class="number">1</span>] = [Bandit(epsilon=<span class="number">0.1</span>, stepSize=<span class="number">0.1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)]</div><div class="line">    _, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    plt.plot(averageRewards[<span class="number">0</span>], label=<span class="string">'UCB c = 2'</span>)</div><div class="line">    plt.plot(averageRewards[<span class="number">1</span>], label=<span class="string">'epsilon greedy epsilon = 0.1'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Steps'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>We note that the <strong>UCBParam=2</strong>. The Bandit object explains this. The <strong>getAction()</strong> method and <strong>takeAction()</strong> method are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># explore</span></div><div class="line">    <span class="keyword">if</span> self.epsilon &gt; <span class="number">0</span>:</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, self.epsilon) == <span class="number">1</span>:</div><div class="line">            np.random.shuffle(self.indices)</div><div class="line">            <span class="keyword">return</span> self.indices[<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># exploit</span></div><div class="line">    <span class="keyword">if</span> self.UCBParam <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        UCBEst = self.qEst + \</div><div class="line">                 self.UCBParam * np.sqrt(np.log(self.time + <span class="number">1</span>) / (np.asarray(self.actionCount) + <span class="number">1</span>))</div><div class="line">        <span class="keyword">return</span> np.argmax(UCBEst)</div><div class="line">    <span class="keyword">if</span> self.gradient:</div><div class="line">        expEst = np.exp(self.qEst)</div><div class="line">        self.actionProb = expEst / np.sum(expEst)</div><div class="line">        <span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div><div class="line">    <span class="keyword">return</span> np.argmax(self.qEst)</div><div class="line"></div><div class="line"><span class="comment"># take an action, update estimation for this action</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(self, action)</span>:</span></div><div class="line">    <span class="comment"># generate the reward under N(real reward, 1)</span></div><div class="line">    reward = np.random.randn() + self.qTrue[action]</div><div class="line">    self.time += <span class="number">1</span></div><div class="line">    self.averageReward = (self.time - <span class="number">1.0</span>) / self.time * self.averageReward + reward / self.time</div><div class="line">    self.actionCount[action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> self.sampleAverages:</div><div class="line">        <span class="comment"># update estimation using sample averages</span></div><div class="line">        self.qEst[action] += <span class="number">1.0</span> / self.actionCount[action] * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">elif</span> self.gradient:</div><div class="line">        oneHot = np.zeros(self.k)</div><div class="line">        oneHot[action] = <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> self.gradientBaseline:</div><div class="line">            baseline = self.averageReward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            baseline = <span class="number">0</span></div><div class="line">        self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># update estimation with constant step size</span></div><div class="line">        self.qEst[action] += self.stepSize * (reward - self.qEst[action])</div><div class="line">    <span class="keyword">return</span> reward</div></pre></td></tr></table></figure><p>We can see the policy get next action has changed but the update policy has not changed. The result is here:<img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/ucb_average_reward.png" alt="ucb_average_reward"></p><p>We omit the figure about the optimal action because the trend of this two figures are the same. UCB will often perform well, but is more difficult than $\epsilon$-greedy to extend beyond bandits to the more general reinforcement learning setting considered in the more advanced problems. In these more advanced settings there is currently no known practical way of utilizing the idea of UCB action selection.</p><p>So far we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. Now, we consider learning a numerical <em>preference</em> $H_t(a)$ for each action <em>a</em>. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the preferences there is no effect on the action probabilities, which are determined according to a softmax distribution as follows:<br>$$<br>Pr\{A_t=a\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} \doteq \pi_t(a)<br>$$<br>where here we have also introduced a useful new notation $\pi_t(a)$ for the probability of taking action <em>a</em> at time <em>t</em>. Initially all preferences are the same (e.g., $H_1(a)=0, \forall a$) so that all actions have an equal probability of being selected.</p><p>There is a natural learning algorithm for this setting based on the idea of stochastic gradient ascent. On each step, after selection the action $A_t$ and receiving the reward $R_t$, the preferences are updated by:<br>$$<br>\begin{align}<br>H_{t+1}(A_t) \doteq H_t(A_t) + \alpha (R_t - \overline{R_t}) (1 - \pi_t(A_t)), \; \text{and} \\<br>H_{t+1}(a) \doteq H_t(a) - \alpha (R_t - \overline{R_t}) \pi_t(a), \;\;\;\;\;\; \forall{a \neq A_t}<br>\end{align}<br>$$<br>where $\alpha&gt;0$ is a ste-size parameter, and $\overline{R_t} \in \mathbb{R}$ is the average of all the rewards up through and including time <em>t</em>, which can be computed incrementally. The $\overline{R_t}$ term serves as a <strong><em>baseline</em></strong> with which the reward is compared. Let us see the implement details (takeAction() method and getAction() method).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> self.gradient:</div><div class="line">	expEst = np.exp(self.qEst)</div><div class="line">	self.actionProb = expEst / np.sum(expEst)</div><div class="line">	<span class="keyword">return</span> np.random.choice(self.indices, p=self.actionProb)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">elif</span> self.gradient:</div><div class="line">	oneHot = np.zeros(self.k)</div><div class="line">	oneHot[action] = <span class="number">1</span></div><div class="line">	<span class="keyword">if</span> self.gradientBaseline:</div><div class="line">		baseline = self.averageReward</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">		baseline = <span class="number">0</span></div><div class="line">	self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot self.actionProb)</div></pre></td></tr></table></figure><p>The below figure shows results with the gradient bandit algorithm on a variant of the 10-armed testbed in which the true excepted rewards were selected according to a normal distribution with a mean of +4 instead of zero (and with unit variance as before).</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/gradient_bandit_optimal_action.png" alt="gradient_bandit_optimal_action"></p><p>Despite their simplicity, in our opinion the methods presented in here can fairly be considered the state of the art. Finally, we do a parameter study of the various bandit algorithms presented in here.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">figure2_6</span><span class="params">(nBandits, time)</span>:</span></div><div class="line">    labels = [<span class="string">'epsilon-greedy'</span>, <span class="string">'gradient bandit'</span>,</div><div class="line">              <span class="string">'UCB'</span>, <span class="string">'optimistic initialization'</span>]</div><div class="line">    generators = [<span class="keyword">lambda</span> epsilon: Bandit(epsilon=epsilon, sampleAverages=<span class="keyword">True</span>),</div><div class="line">                  <span class="keyword">lambda</span> alpha: Bandit(gradient=<span class="keyword">True</span>, stepSize=alpha, gradientBaseline=<span class="keyword">True</span>),</div><div class="line">                  <span class="keyword">lambda</span> coef: Bandit(epsilon=<span class="number">0</span>, stepSize=<span class="number">0.1</span>, UCBParam=coef),</div><div class="line">                  <span class="keyword">lambda</span> initial: Bandit(epsilon=<span class="number">0</span>, initial=initial, stepSize=<span class="number">0.1</span>)]</div><div class="line">    parameters = [np.arange(<span class="number">-7</span>, <span class="number">-1</span>),</div><div class="line">                  np.arange(<span class="number">-5</span>, <span class="number">2</span>),</div><div class="line">                  np.arange(<span class="number">-4</span>, <span class="number">3</span>),</div><div class="line">                  np.arange(<span class="number">-2</span>, <span class="number">3</span>)]</div><div class="line"></div><div class="line">    bandits = [[generator(math.pow(<span class="number">2</span>, param)) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">0</span>, nBandits)] <span class="keyword">for</span> generator, parameter <span class="keyword">in</span> zip(generators, parameters) <span class="keyword">for</span> param <span class="keyword">in</span> parameter]</div><div class="line">    _, averageRewards = banditSimulation(nBandits, time, bandits)</div><div class="line">    rewards = np.sum(averageRewards, axis=<span class="number">1</span>)/time</div><div class="line"></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    i = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> label, parameter <span class="keyword">in</span> zip(labels, parameters):</div><div class="line">        l = len(parameter)</div><div class="line">        plt.plot(parameter, rewards[i:i+l], label=label)</div><div class="line">        i += l</div><div class="line">    plt.xlabel(<span class="string">'Parameter(2^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Average reward'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>The results as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/k-armed-bandit/bandit_algorithms_parameter_study.png" alt="parameters_study"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" itemprop="url">Monte Carlo Methods (Reinforcement Learning)</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-02T14:10:57+08:00">2017-06-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2017/06/02/Monte-Carlo-Methods-Reinforcement-Learning/" class="leancloud_visitors" data-flag-title="Monte Carlo Methods (Reinforcement Learning)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Here we consider our first learning methods for <strong>estimating</strong> value functions and discovering optimal policy. Unlike the previous algorithms, we do not assume complete knowledge of the environment. Monte Carlo methods require only <em>experience</em> – sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Although a model is required, the model need only generate sample transition, not the complete probability distributions of all possible transitions that is required for <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">dynamic programming (DP)</a>.</p><p>Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample return. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. Only on the completion of an episode are value estimates and policies changed.</p><p>To handle the nonstationarity, we adapt the idea of general policy iteration (GPI) developed in earlier for <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">DP</a>.</p><p>Suppose we wish to estimate $v_{\pi}(s)$, the value of a state $s$ under policy $\pi$, given a set of episodes obtained by following $\pi$ and passing though $s$. Each occurrence of state $s$ in an episode is called a <em>visit</em> to $s$. Let us call the first time it is visited in an episode the <em>first visit</em> to $s$. The <em>first-visit MC method</em> estimates $v_{\pi}(s)$ as the average of the returns following first visits to $s$, whereas the <em>every-visit MC method</em> averages the returns following all visits to $s$.</p><blockquote><p><strong>First-visit MC policy evaluation (returns $V \approx v_{\pi}$)</strong></p><p>Initialize:</p><p>​ $\pi \leftarrow$ policy to be evaluated</p><p>​ $V \leftarrow $ an arbitrary state-value function</p><p>​ $Return(s) \leftarrow$ an empty list, for all $s \in \mathcal{S}$</p><p>Repeat forever:</p><p>​ Generate an episode using $\pi$</p><p>​ For each state $s$ appearing in the episode:</p><p>​ $G \leftarrow$ return following the first occurrence of $s$</p><p>​ Append $G$ to $Return(s)$</p><p>​ $V(s) \leftarrow$ $\text{average}(Return(s))$</p></blockquote><p>Next, we’ll use this algorithm to solve a naive problem that defined as follows:</p><blockquote><p>The object of the popular casino card game of <em>blackjack</em> is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a <em>natural</em>. He then wins unless the dealer also has a natural, in which case the game is draw. If the player does not have a natural, then he can request additional cards, one by one (<em>hits</em>), until he either stop (<em>sticks</em>) or excepted 21 (<em>goes bust</em>). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome–win, lose, draw–is determined by whose final sum is closer to 21.</p></blockquote><p>Playing blackjack is naturally formulated as an episode finite MDP. Each game of blackjack is an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount ($\gamma=1$); therefore these terminal rewards are also returns. The player’s actions are to hit or to stick. We assume that cards are dealt from an infinite deck (i.e., with replacement). If the player holds an ace that he could count as 11 without going bust, then the ace is said to be <em>usable</em>. Consider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits. Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.</p><p>Note that, although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events–in particular, they require the quantities $p(s^{\prime}, r|s, a)$–and it is not easy to determine these for blackjack. For example, suppose the play’s sum is 14 and he chooses to sticks. What is his excepted reward as a function of the dealer’s showing card? All of these rewards and transition probabilities must be computed <em>before</em> DP can be applied, and such computations are often complex and error-prone.</p><p>The conceptual diagram of the experimental results is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/blackjack_c.png" alt="blackjack_c"></p><p><em>Figure 1</em></p><p>The first we define some auxiliary variables and methods:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># actions: hit or stand (stick)</span></div><div class="line">ACTION_HIT = <span class="number">0</span></div><div class="line">ACTION_STAND = <span class="number">1</span></div><div class="line">actions = [ACTION_HIT, ACTION_STAND]</div><div class="line"></div><div class="line"><span class="comment"># policy for player</span></div><div class="line">policyPlayer = np.zeros(<span class="number">22</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">20</span>):</div><div class="line">    policyPlayer[i] = ACTION_HIT</div><div class="line">policyPlayer[<span class="number">20</span>] = ACTION_STAND</div><div class="line">policyPlayer[<span class="number">21</span>] = ACTION_STAND</div><div class="line"></div><div class="line"><span class="comment"># function form of target policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">return</span> policyPlayer[playerSum]</div><div class="line"></div><div class="line"><span class="comment"># function form of behavior policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> ACTION_STAND</div><div class="line">    <span class="keyword">return</span> ACTION_HIT</div><div class="line"></div><div class="line"><span class="comment"># policy for dealer</span></div><div class="line">policyDealer = np.zeros(<span class="number">22</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">17</span>):</div><div class="line">    policyDealer[i] = ACTION_HIT</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">17</span>, <span class="number">22</span>):</div><div class="line">    policyDealer[i] = ACTION_STAND</div><div class="line"></div><div class="line"><span class="comment"># get a new card</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCard</span><span class="params">()</span>:</span></div><div class="line">    card = np.random.randint(<span class="number">1</span>, <span class="number">14</span>)</div><div class="line">    card = min(card, <span class="number">10</span>)</div><div class="line">    <span class="keyword">return</span> card</div></pre></td></tr></table></figure><p>Furthermore, we also have a print method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># print the state value</span></div><div class="line">figureIndex = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span><span class="params">(data, tile, zlabel=<span class="string">'reward'</span>)</span>:</span></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    fig = plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    fig.suptitle(tile)</div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</div><div class="line">    axisX = []</div><div class="line">    axisY = []</div><div class="line">    axisZ = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">12</span>, <span class="number">22</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">11</span>):</div><div class="line">            axisX.append(i)</div><div class="line">            axisY.append(j)</div><div class="line">            axisZ.append(data[i - <span class="number">12</span>, j - <span class="number">1</span>])</div><div class="line">    ax.scatter(axisX, axisY, axisZ)</div><div class="line">    ax.set_xlabel(<span class="string">'player sum'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'dealer showing'</span>)</div><div class="line">    ax.set_zlabel(zlabel)</div></pre></td></tr></table></figure><p>In order to get the figure above, we wrote the following code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">onPolicy</span><span class="params">()</span>:</span></div><div class="line">    statesUsableAce1, statesNoUsableAce1 = monteCarloOnPolicy(<span class="number">10000</span>)</div><div class="line">    statesUsableAce2, statesNoUsableAce2 = monteCarloOnPolicy(<span class="number">500000</span>)</div><div class="line">    prettyPrint(statesUsableAce1, <span class="string">'Usable Ace, 10000 Episodes'</span>)</div><div class="line">    prettyPrint(statesNoUsableAce1, <span class="string">'No Usable Ace, 10000 Episodes'</span>)</div><div class="line">    prettyPrint(statesUsableAce2, <span class="string">'Usable Ace, 500000 Episodes'</span>)</div><div class="line">    prettyPrint(statesNoUsableAce2, <span class="string">'No Usable Ace, 500000 Episodes'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>There is a term named <em>on policy</em>, we’ll explain this term later. Now let us jump into the <strong>monteCarloOnPolicy</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo Sample with On-Policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOnPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    statesUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    statesNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesNoUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        state, reward, _ = play(targetPolicyPlayer)</div><div class="line">        state[<span class="number">1</span>] -= <span class="number">12</span></div><div class="line">        state[<span class="number">2</span>] -= <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> state[<span class="number">0</span>]:</div><div class="line">            statesUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            statesNoUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesNoUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">    <span class="keyword">return</span> statesUsableAce / statesUsableAceCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>We ignore he first four variables now and explain them later. <strong>nEpisodes</strong> represents the number of the episodes and the <strong>play</strong> method simulates the process of the blackjack cards game. So what is it like? This method is too long (more than 100 rows) so we partially explain it. In the first place, this method define some auxiliary variables:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># play a game</span></div><div class="line"><span class="comment"># @policyPlayerFn: specify policy for player</span></div><div class="line"><span class="comment"># @initialState: [whether player has a usable Ace, sum of player's cards, one card of dealer]</span></div><div class="line"><span class="comment"># @initialAction: the initial action</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">(policyPlayerFn, initialState=None, initialAction=None)</span>:</span></div><div class="line">    <span class="comment"># player status</span></div><div class="line">	<span class="comment"># sum of player</span></div><div class="line">    playerSum = <span class="number">0</span></div><div class="line">    <span class="comment"># trajectory of player</span></div><div class="line">    playerTrajectory = []</div><div class="line">    <span class="comment"># whether player uses Ace as 11</span></div><div class="line">    usableAcePlayer = <span class="keyword">False</span></div><div class="line">    <span class="comment"># dealer status</span></div><div class="line">    dealerCard1 = <span class="number">0</span></div><div class="line">    dealerCard2 = <span class="number">0</span></div><div class="line">    usableAceDealer = <span class="keyword">False</span></div></pre></td></tr></table></figure><p>Then, we generate a random initial state if the initial state is null. If not, we load the initial state from initialState variable:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> initialState <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">    <span class="comment"># generate a random initial state</span></div><div class="line">    numOfAce = <span class="number">0</span></div><div class="line">    <span class="comment"># initialize cards of player</span></div><div class="line">    <span class="keyword">while</span> playerSum &lt; <span class="number">12</span>:</div><div class="line">        <span class="comment"># if sum of player is less than 12, always hit</span></div><div class="line">        card = getCard()</div><div class="line">        <span class="comment"># if get an Ace, use it as 11</span></div><div class="line">        <span class="keyword">if</span> card == <span class="number">1</span>:</div><div class="line">            numOfAce += <span class="number">1</span></div><div class="line">            card = <span class="number">11</span></div><div class="line">            usableAcePlayer = <span class="keyword">True</span></div><div class="line">        playerSum += card</div><div class="line">    <span class="comment"># if player's sum is larger than 21, he must hold at least one Ace, two Aces are possible</span></div><div class="line">    <span class="keyword">if</span> playerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="comment"># use the Ace as 1 rather than 11</span></div><div class="line">        playerSum -= <span class="number">10</span></div><div class="line">        <span class="comment"># if the player only has one Ace, then he doesn't have usable Ace any more</span></div><div class="line">        <span class="keyword">if</span> numOfAce == <span class="number">1</span>:</div><div class="line">            usableAcePlayer = <span class="keyword">False</span></div><div class="line">    <span class="comment"># initialize cards of dealer, suppose dealer will show the first card he gets</span></div><div class="line">    dealerCard1 = getCard()</div><div class="line">    dealerCard2 = getCard()</div><div class="line">	<span class="keyword">else</span>:</div><div class="line">    	<span class="comment"># use specified initial state</span></div><div class="line">    	usableAcePlayer = initialState[<span class="number">0</span>]</div><div class="line">    	playerSum = initialState[<span class="number">1</span>]</div><div class="line">    	dealerCard1 = initialState[<span class="number">2</span>]</div><div class="line">    	dealerCard2 = getCard()</div><div class="line"></div><div class="line">	<span class="comment"># initial state of the game</span></div><div class="line">	state = [usableAcePlayer, playerSum, dealerCard1]</div><div class="line"></div><div class="line">	<span class="comment"># initialize dealer's sum</span></div><div class="line">    dealerSum = <span class="number">0</span></div><div class="line">    <span class="keyword">if</span> dealerCard1 == <span class="number">1</span> <span class="keyword">and</span> dealerCard2 != <span class="number">1</span>:</div><div class="line">        dealerSum += <span class="number">11</span> + dealerCard2</div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> dealerCard1 != <span class="number">1</span> <span class="keyword">and</span> dealerCard2 == <span class="number">1</span>:</div><div class="line">        dealerSum += dealerCard1 + <span class="number">11</span></div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">elif</span> dealerCard1 == <span class="number">1</span> <span class="keyword">and</span> dealerCard2 == <span class="number">1</span>:</div><div class="line">        dealerSum += <span class="number">1</span> + <span class="number">11</span></div><div class="line">        usableAceDealer = <span class="keyword">True</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        dealerSum += dealerCard1 + dealerCard2</div></pre></td></tr></table></figure><p>Game start! Above all is player’s turn:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># player's turn</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">if</span> initialAction <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        action = initialAction</div><div class="line">        initialAction = <span class="keyword">None</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># get action based on current sum</span></div><div class="line">        action = policyPlayerFn(usableAcePlayer, playerSum, dealerCard1)</div><div class="line"></div><div class="line">    <span class="comment"># track player's trajectory for importance sampling</span></div><div class="line">    playerTrajectory.append([action, (usableAcePlayer, playerSum, dealerCard1)])</div><div class="line"></div><div class="line">    <span class="keyword">if</span> action == ACTION_STAND:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># if hit, get new card</span></div><div class="line">    playerSum += getCard()</div><div class="line"></div><div class="line">    <span class="comment"># player busts</span></div><div class="line">    <span class="keyword">if</span> playerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="comment"># if player has a usable Ace, use it as 1 to avoid busting and continue</span></div><div class="line">        <span class="keyword">if</span> usableAcePlayer == <span class="keyword">True</span>:</div><div class="line">            playerSum -= <span class="number">10</span></div><div class="line">            usableAcePlayer = <span class="keyword">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="comment"># otherwise player loses</span></div><div class="line">            <span class="keyword">return</span> state, <span class="number">-1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>Then is the dealer’s turn if the player’s turn is end:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># get action based on current sum</span></div><div class="line">    action = policyDealer[dealerSum]</div><div class="line">    <span class="keyword">if</span> action == ACTION_STAND:</div><div class="line">        <span class="keyword">break</span></div><div class="line">    <span class="comment"># if hit, get a new card</span></div><div class="line">    dealerSum += getCard()</div><div class="line">    <span class="comment"># dealer busts</span></div><div class="line">    <span class="keyword">if</span> dealerSum &gt; <span class="number">21</span>:</div><div class="line">        <span class="keyword">if</span> usableAceDealer == <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># if dealer has a usable Ace, use it as 1 to avoid busting and continue</span></div><div class="line">            dealerSum -= <span class="number">10</span></div><div class="line">            usableAceDealer = <span class="keyword">False</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># otherwise dealer loses</span></div><div class="line">            <span class="keyword">return</span> state, <span class="number">1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>If the both sides have finished the game:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compare the sum between player and dealer</span></div><div class="line"><span class="keyword">if</span> playerSum &gt; dealerSum:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">1</span>, playerTrajectory</div><div class="line"><span class="keyword">elif</span> playerSum == dealerSum:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">0</span>, playerTrajectory</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    <span class="keyword">return</span> state, <span class="number">-1</span>, playerTrajectory</div></pre></td></tr></table></figure><p>Now, let us come back the <strong>mentoCarloOnPolicy</strong> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOnPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    statesUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    statesNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># initialze counts to 1 to avoid 0 being divided</span></div><div class="line">    statesNoUsableAceCount = np.ones((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        state, reward, _ = play(targetPolicyPlayer)</div><div class="line">        state[<span class="number">1</span>] -= <span class="number">12</span></div><div class="line">        state[<span class="number">2</span>] -= <span class="number">1</span></div><div class="line">        <span class="keyword">if</span> state[<span class="number">0</span>]:</div><div class="line">            statesUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            statesNoUsableAceCount[state[<span class="number">1</span>], state[<span class="number">2</span>]] += <span class="number">1</span></div><div class="line">            statesNoUsableAce[state[<span class="number">1</span>], state[<span class="number">2</span>]] += reward</div><div class="line">    <span class="keyword">return</span> statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>In this method we ignore the player’s trajectory (represent by the <strong>playerTrajectory</strong> variable). If you remember a sentence in the game definition (as follows) it will easy to understand.</p><blockquote><p>Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealer’s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.</p></blockquote><p>This row (as follows) is to calculate the average returns of each state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">return</span> statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCount</div></pre></td></tr></table></figure><p>Recall the beginning of the code and let’s see what results are like:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_10000.png" alt="usable_ace_10000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_10000.png" alt="no_usable_ace_10000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/usable_ace_500000.png" alt="usable_ace_500000"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/no_usable_ace_500000.png" alt="no_usable_ace_500000"></p><p><em>Figure 2 (from up to down). (1) Usable Ace, 10000 Episodes. (2) No Usable Ace, 10000 Episodes. (3) Usable Ace, 500000 Episodes. (4) No Usable Ace, 500000 Episodes.</em></p><p>If a model is not available, then it is particularly useful to estimate <em>action values</em> (the value of state-value pairs) rather than <em>state values</em>. With a model, state values alone are sufficient to determine a policy; one simply look ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the earlier on <a href="https://ewanlee.github.io/2017/05/31/Dynamic-Programming/" target="_blank" rel="external">DP</a>. Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of out primary goals for Monte Carlo methods is to estimate $q_{\star}$. To achieve this, we first consider the policy evaluation problem for action values.</p><p>The policy evaluation problem for action values is estimate $q_{\pi}(s, a)$. The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a state-action pair rather than to a state. The only complication is that many state-value pairs may never be visited. This is the general problem of <em>maintaining exploration</em>, as discussed in the context of the k-armed bandit problem in <a href="https://ewanlee.github.io/2017/05/27/k-Armed-Bandit-Problem/" target="_blank" rel="external">here</a>. One way to do this is by specifying that the episodes <em>start in a state-action pair</em>, and that every pair has a nonzero probability of being selected as the start. We call this the assumption of <em>exploring starts</em>.</p><p>We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal policies. To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with a arbitrary policy $\pi_{0}$ and ending with the optimal policy and optimal action-value function:<br>$$<br>\pi_{0} \stackrel{E}\longrightarrow q_{\pi{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow q_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow q_{\star},<br>$$<br>We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes. We postpone consideration of the first assumption until later.</p><p>The primary approach to avoiding the infinite number of episodes nominally required for policy evaluation is to forgo to complete policy evaluation before returning to policy improvement. For Monte Carlo policy evaluation it it natural to alternate between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation,and then the policy is improved at all the states visited in the episode.</p><blockquote><p><strong>Monte Carlo ES (Exploring Starts)</strong></p><p>Initialize, for all $s \in \mathcal{S},\; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow \text{arbitrary}$</p><p>​ $\pi(s) \leftarrow \text{arbitrary}$</p><p>​ $Returns(s,a) \leftarrow \text{empty list}$</p><p>Repeat forever:</p><p>​ Choose $S_{0} \in \mathcal{S}$ and $A_{0} \in \mathcal{A(S_{0})}$ s.t. all pairs have probability &gt; 0</p><p>​ Generate an episode starting from $S_0, A_0$, following $\pi$</p><p>​ For each pair $s, a$ appearing in the episode:</p><p>​ $G \leftarrow \text{return following the first occurrence of} \; s, a$</p><p>​ Append $G$ to $Returns(s,a)$</p><p>​ $Q(s,a) \leftarrow \text{average}(Returns(s,a))$</p><p>​ For each $s$ in the episode:</p><p>​ $\pi(s) \leftarrow \arg\max_{a} Q(s,a)$</p></blockquote><p>Recall the blackjack card game. It is straightforward to apply Monte Carlo ES to blackjack.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">figure5_3</span><span class="params">()</span>:</span></div><div class="line">    stateActionValues = monteCarloES(<span class="number">500000</span>)</div><div class="line">    stateValueUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    stateValueNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>))</div><div class="line">    <span class="comment"># get the optimal policy</span></div><div class="line">    actionUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="string">'int'</span>)</div><div class="line">    actionNoUsableAce = np.zeros((<span class="number">10</span>, <span class="number">10</span>), dtype=<span class="string">'int'</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10</span>):</div><div class="line">            stateValueNoUsableAce[i, j] = np.max(stateActionValues[i, j, <span class="number">0</span>, :])</div><div class="line">            stateValueUsableAce[i, j] = np.max(stateActionValues[i, j, <span class="number">1</span>, :])</div><div class="line">            actionNoUsableAce[i, j] = argmax(stateActionValues[i, j, <span class="number">0</span>, :])</div><div class="line">            actionUsableAce[i, j] = argmax(stateActionValues[i, j, <span class="number">1</span>, :])</div><div class="line">    prettyPrint(stateValueUsableAce, <span class="string">'Optimal state value with usable Ace'</span>)</div><div class="line">    prettyPrint(stateValueNoUsableAce, <span class="string">'Optimal state value with no usable Ace'</span>)</div><div class="line">    prettyPrint(actionUsableAce, <span class="string">'Optimal policy with usable Ace'</span>, <span class="string">'Action (0 Hit, 1 Stick)'</span>)</div><div class="line">    prettyPrint(actionNoUsableAce, <span class="string">'Optimal policy with no usable Ace'</span>, <span class="string">'Action (0 Hit, 1 Stick)'</span>)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Run the code we’ll get the conceptual diagram like follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces.png" alt="mces"></p><p>Let us to see the implementation (<strong>monteCarloES</strong> method) of this algorithm. Note that, some auxiliary variables are defined earlier.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo with Exploring Starts</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloES</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    <span class="comment"># (playerSum, dealerCard, usableAce, action)</span></div><div class="line">    stateActionValues = np.zeros((<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">2</span>))</div><div class="line">    <span class="comment"># set default to 1 to avoid being divided by 0</span></div><div class="line">    stateActionPairCount = np.ones((<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">2</span>))</div><div class="line">    <span class="comment"># behavior policy is greedy</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicy</span><span class="params">(usableAce, playerSum, dealerCard)</span>:</span></div><div class="line">        usableAce = int(usableAce)</div><div class="line">        playerSum -= <span class="number">12</span></div><div class="line">        dealerCard -= <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> argmax(stateActionValues[playerSum, dealerCard, usableAce, :])</div><div class="line"></div><div class="line">    <span class="comment"># play for several episodes</span></div><div class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(nEpisodes):</div><div class="line">        print(<span class="string">'episode:'</span>, episode)</div><div class="line">        <span class="comment"># for each episode, use a randomly initialized state and action</span></div><div class="line">        initialState = [bool(np.random.choice([<span class="number">0</span>, <span class="number">1</span>])),</div><div class="line">                       np.random.choice(range(<span class="number">12</span>, <span class="number">22</span>)),</div><div class="line">                       np.random.choice(range(<span class="number">1</span>, <span class="number">11</span>))]</div><div class="line">        initialAction = np.random.choice(actions)</div><div class="line">        _, reward, trajectory = play(behaviorPolicy, initialState, initialAction)</div><div class="line">        <span class="keyword">for</span> action, (usableAce, playerSum, dealerCard) <span class="keyword">in</span> trajectory:</div><div class="line">            usableAce = int(usableAce)</div><div class="line">            playerSum -= <span class="number">12</span></div><div class="line">            dealerCard -= <span class="number">1</span></div><div class="line">            <span class="comment"># update values of state-action pairs</span></div><div class="line">            stateActionValues[playerSum, dealerCard, usableAce, action] += reward</div><div class="line">            stateActionPairCount[playerSum, dealerCard, usableAce, action] += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> stateActionValues / stateActionPairCount</div></pre></td></tr></table></figure><p>You can see we use the <strong>trajectory</strong> variable now. The difference between Monte Carlo On Policy and Monte Carlo ES is prior calculates the average return of each state and later calculates the average return of each state-action pair.</p><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_policy.png" alt="mcse_usbale_ace_optimal_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_policy.png" alt="mcse_usbale_ace_optimal_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_usable_ace_optimal_state_value.png" alt="mcse_usable_ace_optimal_state_value"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/mces_no_usable_ace_optimal_state_value.png" alt="mcse_no_usable_ace_optimal_state_value"></p><p>How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call <em>on-policy</em> (Do you remember this term?) methods and <em>off-policy</em> methods. On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a policy different from that used to generate the data.. The First-visit Monte Carlo method and the Monte Carlo ES method are an example of an on-policy method. Here we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. Off-policy methods are considered later.</p><p>The on-policy method we presented in here uses $\epsilon \text{-} greedy$ policies. The complete algorithm is given below.</p><blockquote><p><strong>On-policy first-visit MC control (for $\epsilon \text{-soft}$ policies)</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a ) \leftarrow \text{arbitrary}$</p><p>​ $Returns(s,a) \leftarrow \text{empty list}$</p><p>​ $\pi(a|s) \leftarrow \text{an arbitrary} \; \epsilon \text{-soft policy}$</p><p>Repeat forever:</p><p>​ (a) Generate an episode using $\pi$</p><p>​ (b) For each pair $s, a$ appearing in the episode:</p><p>​ $G \leftarrow $ return following the first occurrence of $s, a$</p><p>​ Append $G$ to $Returns(s,a)$</p><p>​ $Q(s, a) \leftarrow \text{average}(Returns(s,a))$</p><p>​ (c) For each s in the episode:</p><p>​ $A^{\star} \leftarrow \arg\max_{a} Q(s,a)$</p><p>​ For all $a \in \mathcal{A(s)}$:</p><p>​ $\pi(a|s) \leftarrow \left\{ \begin{array}{c} 1 - \epsilon + \epsilon/|\mathcal{A(s)}|\;\;\;\text{if} \;a=A^{\star} \\ \epsilon/|\mathcal{A(s)}|\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{if} \;a\neq A^{\star} \end{array} \right.$</p></blockquote><p>All learning control methods face a dilemma: They seek to learn action values conditional on subsequent <em>optimal</em> behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromise–it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to user two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the <em>target policy</em>, and the policy used to generate behavior is called the <em>behavior policy</em>. In this case we say that learning is from data “off” the target policy, and the overall process is termed <em>off-policy learning</em>.</p><p>We begin the study of off-policy methods by considering the <em>prediction</em> problem, in which both target and behavior policies are fixed. That is, suppose we wish to estimate $v_{\pi}$ or $q_{\pi}$, but all we have are episodes following another policy $\mu$, where $\mu \neq \pi$. In this case, $\pi$ is the target policy, $\mu$ is the behavior policy, and both policies are considered fixed and given.</p><p>In order to use episodes from $\mu$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $\mu$. That is, we require that $\pi(a|s) &gt; 0$ implies $\mu(a|s) &gt; 0$. This is called the assumption of <em>converge</em>. It follows from converge that $\mu$ must be stochastic in states where it is not identical to $\pi$. The target policy $\pi$, on the other hand, may be deterministic. In here, we consider the prediction problem, in which $\pi$ is unchanging and given.</p><p>Almost all off-policy methods utilize <em>importance sampling</em>ddd, a general technique for estimating excepted values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectory occurring under the target and behavior policies, called the <em>importance-sampling ratio</em>. Given a starting state $S_{t}$, the probability of the subsequent state-action trajectory , $A_{t}, S_{t+1}, A_{t+1, \cdots, S_{T}}$, occurring under any policy $\pi$ is<br>$$<br>\prod_{k=t}^{T-1} \pi(A_k | S_k)p(S_{k+1} | S_k, A_k),<br>$$<br>where $p$ here is the state-transition probability function. Thus, the relative probability of the trajectory under the target and behavior policies (the important-sampling ratio) is<br>$$<br>\rho_{t}^{T} \doteq \frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1} \mu(A_k|S_k)p(S_{k+1}|S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi (A_k | S_k)}{\mu (A_k | S_k)}<br>$$<br>Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy $\mu$ to estimate $v_{\pi}(s)$. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time $t=101$. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state $s$ is visited, denote $\tau(s)$. This is for an every-visit method; for a first-visit method, $\tau(s)$ would only include time steps that were first visits to $s$ within their episodes. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up though $T(t)$. Then $\{G_t\}_{t \in \tau(s)}$ are returns that pertain to state $s$, and $\{\rho_{t}^{T(t)}\}_{t \in \tau(s)}$ are the corresponding importance-sampling ratios. To estimate $v_{\pi}(s)$, we simply scale the returns by the ratios and average the results:<br>$$<br>V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{|\tau(s)|}.<br>$$<br>When importance sampling is done as a simple average in this way it is called <em>ordinary importance sampling</em>.</p><p>An important alternative is <em>weighted importance sampling</em>, which uses a weighted average, defined as<br>$$<br>V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{\sum_{t \in \tau(s)} \rho_{t}^{T(t)}},<br>$$<br>or zero if the denominator is zero.</p><p>We applied both ordinary and weighted importance-sampling methods to estimate the value of as single blackjack state from off-policy data. Recall that one of the advantages of Monte Carlo methods is that they can be used to evaluate a single state without forming estimated for any other states. In this example, we evaluated the state in which the dealer is showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three aces). The data was generate by starting in this state then choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21. The value of this state under the target policy is approximately -0.27726 (this was determined by separately generating one-hundred million episodes using the target policy and averaging their returns). Both off-policy methods closely approximated this value after 1000 off-policy episodes using the random policy. To make sure they did this reliably, we performed 100 independent runs, each starting from estimates of zero and learning for 10,000 episodes. The implementation details are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Monte Carlo Sample with Off-Policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloOffPolicy</span><span class="params">(nEpisodes)</span>:</span></div><div class="line">    initialState = [<span class="keyword">True</span>, <span class="number">13</span>, <span class="number">2</span>]</div><div class="line">    sumOfImportanceRatio = [<span class="number">0</span>]</div><div class="line">    sumOfRewards = [<span class="number">0</span>]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        _, reward, playerTrajectory = play(behaviorPolicyPlayer, initialState=initialState)</div><div class="line"></div><div class="line">        <span class="comment"># get the importance ratio</span></div><div class="line">        importanceRatioAbove = <span class="number">1.0</span></div><div class="line">        importanceRatioBelow = <span class="number">1.0</span></div><div class="line">        <span class="keyword">for</span> action, (usableAce, playerSum, dealerCard) <span class="keyword">in</span> playerTrajectory:</div><div class="line">            <span class="keyword">if</span> action == targetPolicyPlayer(usableAce, playerSum, dealerCard):</div><div class="line">                importanceRatioBelow *= <span class="number">0.5</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                importanceRatioAbove = <span class="number">0.0</span></div><div class="line">                <span class="keyword">break</span></div><div class="line">        importanceRatio = importanceRatioAbove / importanceRatioBelow</div><div class="line">        sumOfImportanceRatio.append(sumOfImportanceRatio[<span class="number">-1</span>] + importanceRatio)</div><div class="line">        sumOfRewards.append(sumOfRewards[<span class="number">-1</span>] + reward * importanceRatio)</div><div class="line">    <span class="keyword">del</span> sumOfImportanceRatio[<span class="number">0</span>]</div><div class="line">    <span class="keyword">del</span> sumOfRewards[<span class="number">0</span>]</div><div class="line"></div><div class="line">    sumOfRewards= np.asarray(sumOfRewards)</div><div class="line">    sumOfImportanceRatio= np.asarray(sumOfImportanceRatio)</div><div class="line">    ordinarySampling = sumOfRewards / np.arange(<span class="number">1</span>, nEpisodes + <span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> np.errstate(divide=<span class="string">'ignore'</span>,invalid=<span class="string">'ignore'</span>):</div><div class="line">        weightedSampling = np.where(sumOfImportanceRatio != <span class="number">0</span>, sumOfRewards / sumOfImportanceRatio, <span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> ordinarySampling, weightedSampling</div></pre></td></tr></table></figure><p>Note that the <strong>behaviorPolicyPlayer</strong> that is a function that define the behavior policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># function form of behavior policy of player</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicyPlayer</span><span class="params">(usableAcePlayer, playerSum, dealerCard)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> ACTION_STAND</div><div class="line">    <span class="keyword">return</span> ACTION_HIT</div></pre></td></tr></table></figure><p>And the calculation of the numerator and denominator of the importance-sampling are the summation operation. As the number of iterations increases, we need to accumulate the result from each episode. So we need to store the previous results. The <strong>sumOfRewards</strong> and <strong>sumOfImportanceRatio</strong> are used for this purpose.</p><p>Then we need to show the result (mean square error):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Figure 5.4</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">offPolicy</span><span class="params">()</span>:</span></div><div class="line">    trueValue = <span class="number">-0.27726</span></div><div class="line">    nEpisodes = <span class="number">10000</span></div><div class="line">    nRuns = <span class="number">100</span></div><div class="line">    ordinarySampling = np.zeros(nEpisodes)</div><div class="line">    weightedSampling = np.zeros(nEpisodes)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, nRuns):</div><div class="line">        ordinarySampling_, weightedSampling_ = monteCarloOffPolicy(nEpisodes)</div><div class="line">        <span class="comment"># get the squared error</span></div><div class="line">        ordinarySampling += np.power(ordinarySampling_ - trueValue, <span class="number">2</span>)</div><div class="line">        weightedSampling += np.power(weightedSampling_ - trueValue, <span class="number">2</span>)</div><div class="line">    ordinarySampling /= nRuns</div><div class="line">    weightedSampling /= nRuns</div><div class="line">    axisX = np.log10(np.arange(<span class="number">1</span>, nEpisodes + <span class="number">1</span>))</div><div class="line">    plt.plot(axisX, ordinarySampling, label=<span class="string">'Ordinary Importance Sampling'</span>)</div><div class="line">    plt.plot(axisX, weightedSampling, label=<span class="string">'Weighted Importance Sampling'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Episodes (10^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Mean square error'</span>)</div><div class="line">    plt.legend()</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/off_policy.png" alt="off_policy"></p><p>Formally, the difference between the two kinds of importance sampling is expressed in their biases and variances. The ordinary importance-sampling estimator is unbiased whereas the weighted importance-sampling estimator is biased (the bias converges asymptotically to zero). On the other hand, the variance of the ordinary importance-sampling estimator is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximate that we explore later.</p><p>The estimates of ordinary importance sampling will typical have infinite variance, and this can easily happen in off-policy learning when trajectories contains loops. A simple example is shown below:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance_example.png" alt="infinite_variance_example"></p><p>There is only one nonterminal state $s$ and two action, <strong>end</strong> and <strong>back</strong>. The <strong>end</strong> action causes a deterministic transition to termination, whereas the <strong>back</strong> action transitions, with probability 0.9, back to $s$ or, with probability 0.1, on to termination. The rewards are +1 on latter transition and otherwise zero. Consider the target policy that always selects back. All episodes under this policy consist of some number (possibly zero) of transitions back to $s$ followed by termination with a reward and return of +1. Thus the value of $s$ under the target policy is 1. Suppose we are estimating this value from off-policy data using the behavior policy that selects <strong>end</strong> and <strong>back</strong> with equal probability.</p><p>The implementation details are as follows. We first define the two policies:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">ACTION_BACK = <span class="number">0</span></div><div class="line">ACTION_END = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># behavior policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">behaviorPolicy</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>)</div><div class="line"></div><div class="line"><span class="comment"># target policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">targetPolicy</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> ACTION_BACK</div></pre></td></tr></table></figure><p>Then we define how an episode runs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># one turn</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">()</span>:</span></div><div class="line">    <span class="comment"># track the action for importance ratio</span></div><div class="line">    trajectory = []</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        action = behaviorPolicy()</div><div class="line">        trajectory.append(action)</div><div class="line">        <span class="keyword">if</span> action == ACTION_END:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span>, trajectory</div><div class="line">        <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.9</span>) == <span class="number">0</span>:</div><div class="line">            <span class="keyword">return</span> <span class="number">1</span>, trajectory</div></pre></td></tr></table></figure><p>Now we start our off-policy (first-visit MC) learning process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Figure 5.5</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">monteCarloSample</span><span class="params">()</span>:</span></div><div class="line">    runs = <span class="number">10</span></div><div class="line">    episodes = <span class="number">100000</span></div><div class="line">    axisX = np.log10(np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>))</div><div class="line">    <span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">        sumOfRewards = [<span class="number">0</span>]</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            reward, trajectory = play()</div><div class="line">            <span class="keyword">if</span> trajectory[<span class="number">-1</span>] == ACTION_END:</div><div class="line">                importanceRatio = <span class="number">0</span> <span class="comment"># Because it is impossible on the target policy</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                importanceRatio = <span class="number">1.0</span> / pow(<span class="number">0.5</span>, len(trajectory))</div><div class="line">            sumOfRewards.append(sumOfRewards[<span class="number">-1</span>] + importanceRatio * reward)</div><div class="line">        <span class="keyword">del</span> sumOfRewards[<span class="number">0</span>]</div><div class="line">        estimations = np.asarray(sumOfRewards) / np.arange(<span class="number">1</span>, episodes + <span class="number">1</span>)</div><div class="line">        plt.plot(axisX, estimations)</div><div class="line">    plt.xlabel(<span class="string">'Episodes (10^x)'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Ordinary Importance Sampling'</span>)</div><div class="line">    plt.show()</div><div class="line">    <span class="keyword">return</span></div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/mc/infinite_variance.png" alt="inifinite_variance"></p><p>The correct estimate here is 1, and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite, and the estimates do not convergence to this value.</p><p>At last, we proposed two fancy algorithms, that is, the <strong>Incremental off-policy every-visit MC policy evaluation</strong> and the <strong>Off-policy every-visit MC control</strong>.</p><blockquote><p><strong>Incremental off-policy every-visit MC policy evaluation</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow$ arbitrary</p><p>​ $C(s,a) \leftarrow$ 0</p><p>​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy</p><p>​ $\pi(a|s) \leftarrow$ an arbitrary target policy</p><p>Repeat forever:</p><p>​ Generate an episode using $\mu$:</p><p>​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$</p><p>​ $G \leftarrow 0$</p><p>​ $W \leftarrow 1$</p><p>​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:</p><p>​ $G \leftarrow \gamma G + R_{t+1}$</p><p>​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$</p><p>​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$</p><p>​ $W \leftarrow W\frac{\pi(A_t | S_t)}{\mu(A_t | S_t)}$</p><p>​ If $W = 0$ then ExitForLoop</p><p><strong>Off-policy every-visit MC control (returns $\pi \approx \pi_{\star}$)</strong></p><p>Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:</p><p>​ $Q(s,a) \leftarrow$ arbitrary</p><p>​ $C(s,a) \leftarrow$ 0</p><p>​ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policy</p><p>​ $\pi(s) \leftarrow$ an deterministic policy that is greedy with respect $Q$</p><p>Repeat forever:</p><p>​ Generate an episode using $\mu$:</p><p>​ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$</p><p>​ $G \leftarrow 0$</p><p>​ $W \leftarrow 1$</p><p>​ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:</p><p>​ $G \leftarrow \gamma G + R_{t+1}$</p><p>​ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$</p><p>​ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$</p><p>​ $\pi(S_t) \leftarrow \arg \max_{a}Q(S_t, a)$ (with ties broken consistently)</p><p>​ If $A_t \neq \pi(S_t)$ then ExitForLoop</p><p>​ $W \leftarrow W\frac{1}{\mu(A_t | S_t)}$</p></blockquote></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/11/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/13/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">125</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">59</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>