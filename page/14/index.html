<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/14/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/14/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/10/The-First-Course-of-C/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/10/The-First-Course-of-C/" itemprop="url">The First Course of C#</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-10T19:08:14+08:00">2017-05-10 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/10/The-First-Course-of-C/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/10/The-First-Course-of-C/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/10/The-First-Course-of-C/" class="leancloud_visitors" data-flag-title="The First Course of C#"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><h3 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子:"></a><strong>一个例子:</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">using System;</div><div class="line">namespace HelloWorldApplication</div><div class="line">&#123;</div><div class="line">    /* 类名为 HelloWorld */</div><div class="line">    class HelloWorld</div><div class="line">    &#123;</div><div class="line">        /* main函数 */</div><div class="line">        static void Main(string[] args)</div><div class="line">        &#123;</div><div class="line">            /* 我的第一个 C# 程序 */</div><div class="line">            Console.WriteLine(&quot;Hello World!&quot;);</div><div class="line">            Console.ReadKey();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="对象类型"><a href="#对象类型" class="headerlink" title="对象类型"></a><strong>对象类型</strong></h3><p>是 C# 通用类型系统（Common Type System - CTS）中所有数据类型的终极基类。Object 是 System.Object 类的别名。所以对象（Object）类型可以被分配任何其他类型（值类型、引用类型、预定义类型或用户自定义类型）的值。但是，在分配值之前，需要先进行类型转换。</p><p>当一个值类型转换为对象类型时，则被称为 <strong>装箱</strong>；另一方面，当一个对象类型转换为值类型时，则被称为 <strong>拆箱</strong>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">object obj;</div><div class="line">obj = 100; // 这是装箱</div></pre></td></tr></table></figure><h3 id="动态类型"><a href="#动态类型" class="headerlink" title="动态类型"></a><strong>动态类型</strong></h3><p>您可以存储任何类型的值在动态数据类型变量中。这些变量的类型检查是在运行时发生的。</p><p>声明动态类型的语法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dynamic &lt;variable_name&gt; = value;</div></pre></td></tr></table></figure><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dynamic d = 20;</div></pre></td></tr></table></figure><p>动态类型与对象类型相似，但是对象类型变量的类型检查是在编译时发生的，而动态类型变量的类型检查是在运行时发生的。</p><h3 id="字符串的特殊定义方式"><a href="#字符串的特殊定义方式" class="headerlink" title="字符串的特殊定义方式"></a><strong>字符串的特殊定义方式</strong></h3><p>字符串（String）类型允许您给变量分配任何字符串值。字符串（String）类型是 System.String 类的别名。它是从对象（Object）类型派生的。字符串（String）类型的值可以通过两种形式进行分配：引号和 @引号。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">String str = &quot;runoob.com&quot;;</div></pre></td></tr></table></figure><p>一个 @引号字符串：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">@&quot;runoob.com&quot;;</div></pre></td></tr></table></figure><p>C# string 字符串的前面可以加 @（称作”逐字字符串”）将转义字符（\）当作普通字符对待，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">string str = @&quot;C:\Windows&quot;;</div></pre></td></tr></table></figure><p>等价于：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">string str = &quot;C:\\Windows&quot;;</div></pre></td></tr></table></figure><p>@ 字符串中可以任意换行，换行符及缩进空格都计算在字符串长度之内。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">string str = @&quot;&lt;script type=&quot;&quot;text/javascript&quot;&quot;&gt;</div><div class="line">    &lt;!--</div><div class="line">    --&gt;</div><div class="line">&lt;/script&gt;&quot;;</div></pre></td></tr></table></figure><h3 id="显式类型转换方式"><a href="#显式类型转换方式" class="headerlink" title="显式类型转换方式"></a><strong>显式类型转换方式</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">namespace TypeConversionApplication</div><div class="line">&#123;</div><div class="line">    class StringConversion</div><div class="line">    &#123;</div><div class="line">        static void Main(string[] args)</div><div class="line">        &#123;</div><div class="line">            int i = 75;</div><div class="line">            float f = 53.005f;</div><div class="line">            double d = 2345.7652;</div><div class="line">            bool b = true;</div><div class="line"></div><div class="line">            Console.WriteLine(i.ToString());</div><div class="line">            Console.WriteLine(f.ToString());</div><div class="line">            Console.WriteLine(d.ToString());</div><div class="line">            Console.WriteLine(b.ToString());</div><div class="line">            Console.ReadKey();</div><div class="line">            </div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="命令行输入"><a href="#命令行输入" class="headerlink" title="命令行输入"></a><strong>命令行输入</strong></h3><p><code>System</code>命名空间中的<code>Console</code>类提供了一个函数 <code>ReadLine()</code>，用于接收来自用户的输入，并把它存储到一个变量中。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">int num;</div><div class="line">num = Convert.ToInt32(Console.ReadLine());</div></pre></td></tr></table></figure><p>函数 <code>Convert.ToInt32()</code>把用户输入的数据转换为<code>int</code> 数据类型，因为 <code>Console.ReadLine()</code>只接受字符串格式的数据。</p><h3 id="特殊运算符"><a href="#特殊运算符" class="headerlink" title="特殊运算符"></a><strong>特殊运算符</strong></h3><table><thead><tr><th>运算符</th><th>描述</th><th>实例</th></tr></thead><tbody><tr><td>sizeof()</td><td>返回数据类型的大小。</td><td>sizeof(int)，将返回 4.</td></tr><tr><td>typeof()</td><td>返回 class 的类型。</td><td>typeof(StreamReader);</td></tr><tr><td>&amp;</td><td>返回变量的地址。</td><td>&a; 将得到变量的实际地址。</td></tr><tr><td>*</td><td>变量的指针。</td><td>*a; 将指向一个变量。</td></tr><tr><td>? :</td><td>条件表达式</td><td>如果条件为真 ? 则为 X : 否则为 Y</td></tr><tr><td>is</td><td>判断对象是否为某一类型。</td><td>If( Ford is Car) // 检查 Ford 是否是 Car 类的一个对象。</td></tr><tr><td>as</td><td>强制转换，即使转换失败也不会抛出异常。</td><td>Object obj = new StringReader(“Hello”);StringReader r = obj as StringReader;</td></tr></tbody></table><h3 id="特殊访问修饰符"><a href="#特殊访问修饰符" class="headerlink" title="特殊访问修饰符"></a><strong>特殊访问修饰符</strong></h3><h4 id="Internal-访问修饰符"><a href="#Internal-访问修饰符" class="headerlink" title="Internal 访问修饰符"></a><em>Internal 访问修饰符</em></h4><p>Internal 访问说明符允许一个类将其成员变量和成员函数暴露给当前程序中的其他函数和对象。换句话说，带有 internal 访问修饰符的任何成员可以被定义在该成员所定义的应用程序内的任何类或方法访问。</p><p>类的默认访问标识符是 <strong>internal</strong>，成员的默认访问标识符是 <strong>private</strong>。</p><p>下面的实例说明了这点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">using System;</div><div class="line"></div><div class="line">namespace RectangleApplication</div><div class="line">&#123;</div><div class="line">    class Rectangle</div><div class="line">    &#123;</div><div class="line">        //成员变量</div><div class="line">        internal double length;</div><div class="line">        internal double width;</div><div class="line">        </div><div class="line">        double GetArea()</div><div class="line">        &#123;</div><div class="line">            return length * width;</div><div class="line">        &#125;</div><div class="line">       public void Display()</div><div class="line">        &#123;</div><div class="line">            Console.WriteLine(&quot;长度： &#123;0&#125;&quot;, length);</div><div class="line">            Console.WriteLine(&quot;宽度： &#123;0&#125;&quot;, width);</div><div class="line">            Console.WriteLine(&quot;面积： &#123;0&#125;&quot;, GetArea());</div><div class="line">        &#125;</div><div class="line">    &#125;//end class Rectangle    </div><div class="line">    class ExecuteRectangle</div><div class="line">    &#123;</div><div class="line">        static void Main(string[] args)</div><div class="line">        &#123;</div><div class="line">            Rectangle r = new Rectangle();</div><div class="line">            r.length = 4.5;</div><div class="line">            r.width = 3.5;</div><div class="line">            r.Display();</div><div class="line">            Console.ReadLine();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>当上面的代码被编译和执行时，它会产生下列结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">长度： 4.5</div><div class="line">宽度： 3.5</div><div class="line">面积： 15.75</div></pre></td></tr></table></figure><p>在上面的实例中，请注意成员函数 <em>GetArea()</em> 声明的时候不带有任何访问修饰符。如果没有指定访问修饰符，则使用类成员的默认访问修饰符，即为 <strong>private</strong>。</p><h4 id="Protected-Internal-访问修饰符"><a href="#Protected-Internal-访问修饰符" class="headerlink" title="Protected Internal 访问修饰符"></a><em>Protected Internal 访问修饰符</em></h4><p>Protected Internal 访问修饰符允许在本类,派生类或者包含该类的程序集中访问。这也被用于实现继承。</p><h3 id="按引用传递参数"><a href="#按引用传递参数" class="headerlink" title="按引用传递参数"></a><strong>按引用传递参数</strong></h3><p>引用参数是一个对变量的内存位置的引用。当按引用传递参数时，与值参数不同的是，它不会为这些参数创建一个新的存储位置。引用参数表示与提供给方法的实际参数具有相同的内存位置。</p><p>在 <code>C#</code> 中，使用 <code>ref</code> 关键字声明引用参数。下面的实例演示了这点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">using System;</div><div class="line">namespace CalculatorApplication</div><div class="line">&#123;</div><div class="line">   class NumberManipulator</div><div class="line">   &#123;</div><div class="line">      public void swap(ref int x, ref int y)</div><div class="line">      &#123;</div><div class="line">         int temp;</div><div class="line"></div><div class="line">         temp = x; /* 保存 x 的值 */</div><div class="line">         x = y;    /* 把 y 赋值给 x */</div><div class="line">         y = temp; /* 把 temp 赋值给 y */</div><div class="line">       &#125;</div><div class="line">   </div><div class="line">      static void Main(string[] args)</div><div class="line">      &#123;</div><div class="line">         NumberManipulator n = new NumberManipulator();</div><div class="line">         /* 局部变量定义 */</div><div class="line">         int a = 100;</div><div class="line">         int b = 200;</div><div class="line"></div><div class="line">         Console.WriteLine(&quot;在交换之前，a 的值： &#123;0&#125;&quot;, a);</div><div class="line">         Console.WriteLine(&quot;在交换之前，b 的值： &#123;0&#125;&quot;, b);</div><div class="line"></div><div class="line">         /* 调用函数来交换值 */</div><div class="line">         n.swap(ref a, ref b);</div><div class="line"></div><div class="line">         Console.WriteLine(&quot;在交换之后，a 的值： &#123;0&#125;&quot;, a);</div><div class="line">         Console.WriteLine(&quot;在交换之后，b 的值： &#123;0&#125;&quot;, b);</div><div class="line"> </div><div class="line">         Console.ReadLine();</div><div class="line"></div><div class="line">      &#125;</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>当上面的代码被编译和执行时，它会产生下列结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">在交换之前，a 的值：100</div><div class="line">在交换之前，b 的值：200</div><div class="line">在交换之后，a 的值：200</div><div class="line">在交换之后，b 的值：100</div></pre></td></tr></table></figure><p>结果表明，<em><code>swap</code></em> 函数内的值改变了，且这个改变可以在 <em><code>Main</code></em> 函数中反映出来。</p><div class="post-button text-center"><a class="btn" href="/2017/05/10/The-First-Course-of-C/#more" rel="contents">Read more &raquo;</a></div></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/" itemprop="url">GRADIENTS, BATCH NORMALIZATION AND LAYER NORMALIZATION</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-09T10:17:48+08:00">2017-05-09 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/" class="leancloud_visitors" data-flag-title="GRADIENTS, BATCH NORMALIZATION AND LAYER NORMALIZATION"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In this post, we will take a look at common pit falls with optimization and solutions to some of these issues. The main topics that will be covered are:</p><ul><li>Gradients</li><li>Exploding gradients</li><li>Vanishing gradients</li><li>LSTMs (pertaining to vanishing gradients)</li><li>Normalization</li></ul><p>And then we will see how to implement batch and layer normalization and apply them to our cells.</p><h2 id="GRADIENTS"><a href="#GRADIENTS" class="headerlink" title="GRADIENTS"></a>GRADIENTS</h2><p>First, we will take a closer look at gradients and backpropagation during optimization. Our example will be a simple MLP but we will extend to an RNN later on.</p><p>I want to go over what a gradient means. Let’s say we have a very simple MLP with 1 set of weights W_1 which is used to calcualte some y. We devise a very simple loss function J, and our gradient becomes dJ/dW_1 (d = partials). Sure we can take the derivative and apply chain rule and get a number, but what does this value even mean? The gradient can be thought of as several things. One is that the magnitude of the gradient represents the sensitivity or impact this weight has on determining y which determines our loss. This can be seen below:</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-10-26-at-6-20-24-pm.png?w=620" alt="Screen Shot 2016-10-26 at 6.20.24 PM.png"></p><p><strong>CS231n</strong></p><p>What the gradients (dfdx, dfdy, dfdz, dfdq, dfdz) tell us is the sensitivity of each variable on our result f. In an MLP, we will produce a result (logits) and compare it with our targets to determine the deviance in what we got and what we should have gotten. From this we can use backpropagation to determine how much adjusting needs to be made for each variable along the way, all the way to the beginning.</p><p>The gradient also holds another key piece of information. It repesents how much we need to change the weights in order to move towards our goal (minimizing the loss, maximizing some objective, etc.). With simple SGD, we get the gradient and we apply an update to the weights (W_i_new = W_i_old – alpha * gradient). If we follow the direction of the gradient, we will be maximizing the goal function. Our loss functions (NLL or cross entropy) are functions we wish to minimize, so we subtract the gradient. We use the learning parameter alpha to control how quickly we change. This is where all of the normalization techniques in this post will come in handy.</p><p>If we have an alpha that is 1 or larger, we will allow the gradient to directly impact our weights. In the beginning of training a neural net, our weight initializations are bound to be far off from the weights we actually need. This creates a large error and so, results in large gradients. If we choose to update our weights with these large gradients, we will be never reach the minimum point for our loss function. We will keep overshooting and bouncing back and forth. So, we use this alpha (small value) to control how much impact the gradient has. Eventually, the gradient will get smaller as well because of less error and we will reach our goal, but with such a small alpha, this can take a while. With techniques, such as batch normalization and layer normalization, we can afford to use large alpha because the gradients will be controlled due to controlled outputs from the neurons.</p><p>Now, even with a simple RNN structure, backpropagation can pose several issues. When we get our result, we need to backpropagate all the way back to the very first cell in order to complete our updates. The main principles to really understand are: if I multiply a number greater than 1 over and over, I will reach infinity (explosion) and vice versa, if I multiply a number less than 1 over and over, I will reach 0 (vanishing).</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-10-04-at-5-54-13-am.png?w=620" alt="screen-shot-2016-10-04-at-5-54-13-am"></p><h2 id="EXPLODING-GRADIENTS"><a href="#EXPLODING-GRADIENTS" class="headerlink" title="EXPLODING GRADIENTS"></a>EXPLODING GRADIENTS</h2><p>The first issue is that our gradients can be greater than 1. As we backpropagate the gradient through the network, we can end up with massive gradients. So far, the solution to exploding gradients is a very hacky but cheap solution; just clip the norm of the gradient at some threshold.</p><p><img src="https://qph.ec.quoracdn.net/main-qimg-3e453fb7dd33a6e6e4d82adf6165d39a?convert_to_webp=true" alt="img"></p><h2 id="VANISHING-GRADIENTS"><a href="#VANISHING-GRADIENTS" class="headerlink" title="VANISHING GRADIENTS"></a>VANISHING GRADIENTS</h2><p>We could also experience the other issue where the gradient is less than 1 to start with and as we backpropagate, the effect of the gradient weakens and it will eventually be negligible. A common scenario where this occurs is when we have saturation at the tails of the sigmoidal function (0 or 1). This is problematic because now the derivative will always be near 0. During backpropagation, we will be multiplying this near zero derivative with our error repeatedly.</p><p>Let’s look at the sigmoidal activation function. You can replicate this example for tanh too.</p><p><img src="https://qph.ec.quoracdn.net/main-qimg-45bad3db11225318bd4aa686a823181c?convert_to_webp=true" alt="img"><img src="https://qph.ec.quoracdn.net/main-qimg-4635c1521f87e2d6f5cf4fe8f39ca76d?convert_to_webp=true" alt="img"></p><p>To solve this issue, we can use rectified linear units (ReLU) which don’t suffer from this tail saturation as much. <img src="https://qph.ec.quoracdn.net/main-qimg-bb38bf7ef543aa6a0c24134f61d15ba7?convert_to_webp=true" alt="img"></p><p>The derivative is 1 if x &gt; 0, so now error signal won’t weaken as it backpropagates through the network. But we do have the problem in the negative region (x &lt;0) where the derivative is zero. This can nullify our error signal so it’s best to add a leaky factor (<a href="http://arxiv.org/abs/1502.01852" target="_blank" rel="external">http://arxiv.org/abs/1502.01852</a>) to the ReLU unit, where the negative region will have some small negative slope. This parameter can be fixed or be a randomized parameter and be fixed after training. There’s also maxout (<a href="http://arxiv.org/abs/1302.4389" target="_blank" rel="external">http://arxiv.org/abs/1302.4389</a>) but this will have twice the amount of weights as a regular ReLU unit.</p><h2 id="LSTMS-VANISHING-GRADIENTS"><a href="#LSTMS-VANISHING-GRADIENTS" class="headerlink" title="LSTMS (VANISHING GRADIENTS)"></a>LSTMS (VANISHING GRADIENTS)</h2><p>As for how LSTMs solve the vanishing gradient issue, they don’t have to worry about the error signal weakening as with a regular basic RNN cell. It’s a bit complicated but the basic idea is that they have a forget gate that determines how much previous memory is stored in the network. This architecture allows the error signal to be transferred effectively to the previous time step. This is usually referred to as the constant error carousel (CEC).</p><h2 id="NORMALIZATION"><a href="#NORMALIZATION" class="headerlink" title="NORMALIZATION"></a>NORMALIZATION</h2><p>There are several types of normalization techniques but the idea behind all of them is the same, which is shifting our inputs to a zero mean and unit variance.</p><p>Techniques like batch norm (<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">https://arxiv.org/abs/1502.03167</a>) may help with the gradient issues as a side effect but the main object is to improve overall optimization. When we first initialize our weights, we are bound to have very large deviances from the true weights. These outliers need to be compensated for by the gradients and this further delays convergence during training. Batchnorm helps us here by normalizing the gradients (reducing influence from weight deviances) on a batched implementation and allows us to train faster (can even safely use larger learning rates now).</p><p>With batch norm, the main idea is to normalize at each layer for every minibatch. We initially may normalize our inputs, but as they travel through the layers, the inputs are operated on by weights and neurons and effectively change. As this progresses, the deviances get larger and larger and our backpropagation will need to account for these large deviances. This restricts us to using a small learning rate to prevent gradient explosion/vanishing. With <strong>batch norm</strong>, we will normalize the inputs (<strong>activations</strong> coming from the previous layer) going into each layer using the mean and variance of the activations for the <strong>entire</strong> <strong>minibatch</strong>. The normalization is a bit different during training and inference but it is beyond the scope of this post. (details in paper).</p><p>Batch normalization is very nice but it is based on minibatch size and so it’s a bit difficult to use with recurrent architectures. With <strong>layer normalization</strong>, we instead compute the mean and variance using ALL of the summed inputs to the neurons in a layer for EVERY <strong>single</strong> <strong>training**</strong>case<strong>. This removes the dependency on a minibatch size. Unlike batch normalization, the normalization operation for layer norm is same for training and inference. More details can be found on Hinton’s paper </strong>here**.</p><p>######</p><h2 id="IMPLEMENTING-BATCH-NORMALIZATION"><a href="#IMPLEMENTING-BATCH-NORMALIZATION" class="headerlink" title="IMPLEMENTING BATCH NORMALIZATION"></a>IMPLEMENTING BATCH NORMALIZATION</h2><p>As stated above, the main goal of batch normalization is optimization. By normalizing the inputs to a layer to zero mean and unit variance, we can help our net learn faster by minimizing the effects from large errors (especially during initial training).</p><p>Batch norm is given by the operation below, where \epsilon is a small random noise (for stability). When we apply batch norm on a layer, we are restricting the inputs to follow a normal distribution, which ultimately will restrict the nets ability to learn. In order to fix this, we multiply by a scale parameter (\alpha) and add a shift parameter (\beta). Both of these parameters are trainable.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-08-at-8-09-28-pm.png?w=620" alt="Screen Shot 2016-11-08 at 8.09.28 PM.png"></p><p>Note that both alpha and beta are applied element wise, so there will be a scale and shift for each neuron in the subsequent layer. With batchnorm, we compute mean and variance across an entire batch and we have a value for each neuron we are feeding our normalized inputs into.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-14-at-9-00-40-pm.png?w=620" alt="Screen Shot 2016-11-14 at 9.00.40 PM.png"></p><p>So for a given layer, the mean during BN will be 1X. Each training data gets this mean subtracted from it and divided by sqrt(var + epsilon) and then shifted and scaled. To find the mean and var, we use all the examples in the training batch.</p><p>In order to accurately evaluate the effectiveness of batchnorm, we will use a simple MLP to classify MNIST digits. We will run a normal MLP and an MLP with batchnorm, both initialized with the same starting weights. Let’s take a look at both the naive and TF implementations.</p><p>First, the naive version:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Naive BN layer</span></div><div class="line">scale1 = tf.Variable(tf.ones([<span class="number">100</span>]))</div><div class="line">shift1 = tf.Variable(tf.zeros([<span class="number">100</span>]))</div><div class="line">W1_BN = tf.Variable(W1_init)</div><div class="line">b1_BN = tf.Variable(tf.zeros([<span class="number">100</span>]))</div><div class="line">z1_BN = tf.matmul(X,W1_BN)+b1_BN</div><div class="line">mean1, var1 = tf.nn.moments(z1_BN, [<span class="number">0</span>])</div><div class="line">BN1 = (z1_BN - mean1) / tf.sqrt(var1 + FLAGS.epsilon)</div><div class="line">BN1 = scale1*BN1 + shift1</div><div class="line">fc1_BN = tf.nn.relu(BN1)</div></pre></td></tr></table></figure><p>TF implementation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># TF BN layer</span></div><div class="line">scale2 = tf.Variable(tf.ones([<span class="number">100</span>]))</div><div class="line">shift2 = tf.Variable(tf.zeros([<span class="number">100</span>]))</div><div class="line">W2_BN = tf.Variable(W2_init)</div><div class="line">b2_BN = tf.Variable(tf.zeros([<span class="number">100</span>]))</div><div class="line">z2_BN = tf.matmul(fc1_BN,W2_BN)+b2_BN</div><div class="line">mean2, var2 = tf.nn.moments(z2_BN, [<span class="number">0</span>])</div><div class="line">BN2 = tf.nn.batch_normalization(z2_BN,mean2,var2,shift2,scale2,FLAGS.epsilon)</div><div class="line">fc2_BN = tf.nn.relu(BN2)</div></pre></td></tr></table></figure><p>We first need to compute the mean and variance of the inputs coming into the layer. Then normalize them and scale/shift and then apply the activation function and pass to the next layer.</p><p>Let’s compare the performance of the normal MLP and the MLP with batchnorm. We will focus of the massive impact on our cost with and without BN. Other interesting features to look at would be gradient norm, neuron inputs, etc.</p><h3 id="CROSS-ENTROPY-LOSS"><a href="#CROSS-ENTROPY-LOSS" class="headerlink" title="CROSS ENTROPY LOSS"></a>CROSS ENTROPY LOSS</h3><p>###</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-10-at-8-58-44-pm.png?w=620" alt="Screen Shot 2016-11-10 at 8.58.44 PM.png"></p><h2 id="NUANCE"><a href="#NUANCE" class="headerlink" title="NUANCE:"></a>NUANCE:</h2><p>Training is all fine and well, but what about testing. When doing BN on our test set, with the implementation from above, we will be using the mean and variance from our test set. Now think about what will happen if our test set is very small or even size 1. This will homogenize all the outputs we get since all inputs will be close to mean 0 and variance 1. The solution to this is to calculate the population mean and variance during testing and then use those values during testing.</p><p>Now there are couple ways we can try to calculate the population, even simple as taking the average of the training batch and using it for testing. This isn’t the true population measure so we will calculate the unbiased mean and variance as they do in the original <strong>paper</strong>. But first, let’s see the accuracy when we feed in test samples of size 1.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-10-at-9-40-52-pm.png?w=620" alt="Screen Shot 2016-11-10 at 9.40.52 PM.png"></p><p>Not exactly state of the art anymore. So let’s see how to calculate population mean and variance.<br><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-09-at-7-45-05-pm.png?w=620" alt="Screen Shot 2016-11-09 at 7.45.05 PM.png"></p><p>We will be updating the population mean and variance after each training batch and we will use them for inference. In fact we can simple replace the inference batchnorm process with a simple linear transformation:</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-09-at-7-50-58-pm.png?w=620" alt="Screen Shot 2016-11-09 at 7.50.58 PM.png"></p><p>Below is the tensorflow implementation for batchnorm with the exponential moving average to use during inference. Take a look <strong>here</strong> for more implementation specifications for batch_norm but the required parameters for us is the actual input that we wish to normalize and wether or not we are training. Note: TF batchnorm with inference is in <strong>batch_norm2.py</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.contrib.layers <span class="keyword">import</span> (</div><div class="line">    batch_norm</div><div class="line">)</div><div class="line">...</div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'BN_1'</span>) <span class="keyword">as</span> BN_1:</div><div class="line">    self.BN1 = tf.cond(self.is_training_ph,</div><div class="line">        <span class="keyword">lambda</span>: batch_norm(</div><div class="line">            self.z1_BN, is_training=<span class="keyword">True</span>, center=<span class="keyword">True</span>,</div><div class="line">            scale=<span class="keyword">True</span>, activation_fn=tf.nn.relu,</div><div class="line">            updates_collections=<span class="keyword">None</span>, scope=BN_1),</div><div class="line">        <span class="keyword">lambda</span>: batch_norm(</div><div class="line">            self.z1_BN, is_training=<span class="keyword">False</span>, center=<span class="keyword">True</span>,</div><div class="line">            scale=<span class="keyword">True</span>, activation_fn=tf.nn.relu,</div><div class="line">            updates_collections=<span class="keyword">None</span>, scope=BN_1, reuse=<span class="keyword">True</span>))</div></pre></td></tr></table></figure><p>Here are the inference results with the population mean and variance:</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-14-at-6-45-55-pm.png?w=620" alt="Screen Shot 2016-11-14 at 6.45.55 PM.png"></p><p>######</p><h2 id="IMPLEMENTING-LAYER-NORMALIZATION"><a href="#IMPLEMENTING-LAYER-NORMALIZATION" class="headerlink" title="IMPLEMENTING LAYER NORMALIZATION"></a>IMPLEMENTING LAYER NORMALIZATION</h2><p>Layernorm is very similar to batch normalization in many ways as you can see with the equation below but it usually reserved for use with recurrent architectures.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2017-01-19-at-6-23-36-pm.png?w=352&amp;h=105" alt="Screen Shot 2017-01-19 at 6.23.36 PM.png"></p><p>Layernorm acts on a per layer per sample basis, where the mean and variance are calculated for a specific layer for a specific training point. To understand the different between layernorm and batchnorm let’s see how these mean and variances are computed for both with figures.</p><p>With layernorm it’s a bit different from BN. We compute the mean and var for every single sample for each layer independently and then do the LN operations using those computed values.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-14-at-8-59-44-pm.png?w=620" alt="Screen Shot 2016-11-14 at 8.59.44 PM.png"></p><p>First, we will make a function that will apply batch norm given an input tensor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># LN funcition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ln</span><span class="params">(inputs, epsilon = <span class="number">1e-5</span>, scope = None)</span>:</span></div><div class="line"> </div><div class="line">    <span class="string">""" Computer LN given an input tensor. We get in an input of shape</span></div><div class="line">    [N X D] and with LN we compute the mean and var for each individual</div><div class="line">    training point across all it's hidden dimensions rather than across</div><div class="line">    the training batch as we do in BN. This gives us a mean and var of shape</div><div class="line">    [N X 1].</div><div class="line">    """</div><div class="line">    mean, var = tf.nn.moments(inputs, [<span class="number">1</span>], keep_dims=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(scope + <span class="string">'LN'</span>):</div><div class="line">        scale = tf.get_variable(<span class="string">'alpha'</span>,</div><div class="line">                                shape=[inputs.get_shape()[<span class="number">1</span>]],</div><div class="line">                                initializer=tf.constant_initializer(<span class="number">1</span>))</div><div class="line">        shift = tf.get_variable(<span class="string">'beta'</span>,</div><div class="line">                                shape=[inputs.get_shape()[<span class="number">1</span>]],</div><div class="line">                                initializer=tf.constant_initializer(<span class="number">0</span>))</div><div class="line">    LN = scale * (inputs - mean) / tf.sqrt(var + epsilon) + shift</div><div class="line"> </div><div class="line">    <span class="keyword">return</span> LN</div></pre></td></tr></table></figure><p>Now we can apply our LN function to a GRUCell class. Note that I am using tensorflow’s <strong>GRUCell class</strong> but we can apply LN to all of their other RNN variants as well (LSTM, peephole LSTM, etc.)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GRUCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">  <span class="string">"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)."""</span></div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_units, input_size=None, activation=tanh)</span>:</span></div><div class="line">    <span class="keyword">if</span> input_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">      logging.warn(<span class="string">"%s: The input_size parameter is deprecated."</span>, self)</div><div class="line">    self._num_units = num_units</div><div class="line">    self._activation = activation</div><div class="line"> </div><div class="line"><span class="meta">  @property</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> self._num_units</div><div class="line"> </div><div class="line"><span class="meta">  @property</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> self._num_units</div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></div><div class="line">    <span class="string">"""Gated recurrent unit (GRU) with nunits cells."""</span></div><div class="line">    <span class="keyword">with</span> vs.variable_scope(scope <span class="keyword">or</span> type(self).__name__):  <span class="comment"># "GRUCell"</span></div><div class="line">      <span class="keyword">with</span> vs.variable_scope(<span class="string">"Gates"</span>):  <span class="comment"># Reset gate and update gate.</span></div><div class="line">        <span class="comment"># We start with bias of 1.0 to not reset and not update.</span></div><div class="line">        r, u = array_ops.split(<span class="number">1</span>, <span class="number">2</span>, _linear([inputs, state],</div><div class="line">                                             <span class="number">2</span> * self._num_units, <span class="keyword">True</span>, <span class="number">1.0</span>))</div><div class="line"> </div><div class="line">        <span class="comment"># Apply Layer Normalization to the two gates</span></div><div class="line">        r = ln(r, scope = <span class="string">'r/'</span>)</div><div class="line">        u = ln(r, scope = <span class="string">'u/'</span>)</div><div class="line"> </div><div class="line">        r, u = sigmoid(r), sigmoid(u)</div><div class="line">      <span class="keyword">with</span> vs.variable_scope(<span class="string">"Candidate"</span>):</div><div class="line">        c = self._activation(_linear([inputs, r * state],</div><div class="line">                                     self._num_units, <span class="keyword">True</span>))</div><div class="line">      new_h = u * state + (<span class="number">1</span> - u) * c</div><div class="line">    <span class="keyword">return</span> new_h, new_h</div></pre></td></tr></table></figure><h2 id="SHAPES"><a href="#SHAPES" class="headerlink" title="SHAPES:"></a>SHAPES:</h2><p>I received quite a few PMs about some confusing aspects of BN and LN, mostly centered around what is actually the input. Let’s look at BN first. The input to a hidden layer will be [NXH]. Applying BN involves calculating the mean value for each H across all N samples. So we will have a mean of shape [1XH]. This “batch” mean will be used for BN, basically subtracting this batch mean from each sample.</p><p>Now for LN, let’s imagine a simple RNN situation. Batch major inputs are of shape [N, M, H], where N is the batch size, M is the max number of time steps and H is the number of hidden units. Before feeing to an RNN, we can reshape to time-major which becomes [M, N, H]. Now we feed in one time step at a time into the RNN, so the shape of each time-step’s input is [N,H]. Applying LN involves calculating the mean for sample across dimension [1], which means looking at all hidden states for each sample (for this particular time step). This gives us a mean of size [NX1]. We use this “layer” mean for each sample.</p><hr><p>Source page is <a href="https://theneuralperspective.com/2016/10/27/gradient-topics/" target="_blank" rel="external">HERE</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/04/What-Is-Local-Response-Normalization-In-Convolutional-Neural-Networks/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/04/What-Is-Local-Response-Normalization-In-Convolutional-Neural-Networks/" itemprop="url">What Is Local Response Normalization In Convolutional Neural Networks</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-04T11:32:10+08:00">2017-05-04 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/04/What-Is-Local-Response-Normalization-In-Convolutional-Neural-Networks/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/04/What-Is-Local-Response-Normalization-In-Convolutional-Neural-Networks/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/04/What-Is-Local-Response-Normalization-In-Convolutional-Neural-Networks/" class="leancloud_visitors" data-flag-title="What Is Local Response Normalization In Convolutional Neural Networks"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Convolutional Neural Networks (CNNs) have been doing wonders in the field of image recognition in recent times. CNN is a type of deep neural network in which the layers are connected using spatially organized patterns. This is in line with how the human visual cortex processes image data. Researchers have been working on coming up with better architectures over the last few years. In this blog post, we will discuss a particular type of layer that has been used consistently across many famous architectures. This layer is called Local Response Normalization layer and it plays an important role. What does it do? What’s the advantage of having this in our network?</p><h2 id="Why-do-we-need-normalization-layers-in-the-first-place"><a href="#Why-do-we-need-normalization-layers-in-the-first-place" class="headerlink" title="Why do we need normalization layers in the first place?"></a><strong>Why do we need normalization layers in the first place?</strong></h2><p>A typical CNN consists of the following layers: convolution, pooling, rectified linear unit (ReLU), fully connected, and loss. If the previous sentence didn’t make sense, you may want to go through a quick CNN tutorial before proceeding further. Anyway, the reason we may want to have normalization layers in our CNN is that we want to have some kind of inhibition scheme.</p><p>In neurobiology, there is a concept called “lateral inhibition”. Now what does that mean? This refers to the capacity of an excited neuron to subdue its neighbors. We basically want a significant peak so that we have a form of local maxima. This tends to create a contrast in that area, hence increasing the sensory perception. Increasing the sensory perception is a good thing! We want to have the same thing in our CNNs.</p><h2 id="What-exactly-is-Local-Response-Normalization"><a href="#What-exactly-is-Local-Response-Normalization" class="headerlink" title="What exactly is Local Response Normalization?"></a><strong>What exactly is Local Response Normalization?</strong></h2><p>Local Response Normalization (LRN) layer implements the lateral inhibition we were talking about in the previous section. This layer is useful when we are dealing with ReLU neurons. Why is that? Because ReLU neurons have unbounded activations and we need LRN to normalize that. We want to detect high frequency features with a large response. If we normalize around the local neighborhood of the excited neuron, it becomes even more sensitive as compared to its neighbors.</p><p>At the same time, it will dampen the responses that are uniformly large in any given local neighborhood. If all the values are large, then normalizing those values will diminish all of them. So basically we want to encourage some kind of inhibition and boost the neurons with relatively larger activations. This has been discussed nicely in Section 3.3 of the <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="external">original paper</a> by Krizhevsky et al.</p><h2 id="How-is-it-done-in-practice"><a href="#How-is-it-done-in-practice" class="headerlink" title="How is it done in practice?"></a><strong>How is it done in practice?</strong></h2><p>There are two types of normalizations available in Caffe. You can either normalize within the same channel or you can normalize across channels. Both these methods tend to amplify the excited neuron while dampening the surrounding neurons. When you are normalizing within the same channel, it’s just like considering a 2D neighborhood of dimension N x N, where N is the size of the normalization window. You normalize this window using the values in this neighborhood. If you are normalizing across channels, you will consider a neighborhood along the third dimension but at a single location. You need to consider an area of shape N x 1 x 1. Here 1 x 1 refers to a single value in a 2D matrix and N refers to the normalization size.</p><hr><p>Source page is <a href="https://prateekvjoshi.com/2016/04/05/what-is-local-response-normalization-in-convolutional-neural-networks/" target="_blank" rel="external">here</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/01/The-first-course-of-the-Docker/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/01/The-first-course-of-the-Docker/" itemprop="url">The first course of the Docker</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-01T16:33:38+08:00">2017-05-01 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/01/The-first-course-of-the-Docker/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/01/The-first-course-of-the-Docker/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/01/The-first-course-of-the-Docker/" class="leancloud_visitors" data-flag-title="The first course of the Docker"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Docker command:</p><ul><li><p>docker images <strong>查看本机所有镜像</strong></p></li><li><p>docker pull NAME <strong>从仓库下载镜像</strong></p></li><li><p>docker run [-d -p 8080:80] or [-P] NAME <strong>启动镜像（-d 后台运行 -p 端口映射 -P 随机映射）</strong></p></li><li><p>docker exec [-i -t] NAME bash <strong>进入容器并执行bash</strong></p></li><li><p>docker ps <strong>查看后台容器</strong></p></li><li><p>docker stop ID <strong>停止docker容器</strong></p></li><li><p>docker restart ID <strong>重启容器</strong></p><p>​</p></li></ul><p>Docker netowrk type: bridge</p><p>Docker port map: host(eth0:80) &lt;–&gt; dicker0(bridge) &lt;–&gt; docker container(eth0:80)</p><p>Build Docker</p><ul><li>Dockerfile</li><li>docker build [-t] <strong>建立Docker，指定TAG</strong></li></ul><p>A Dcokerfile example (based tomcat):</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tomcat </div><div class="line"></div><div class="line"><span class="keyword">MAINTAINER</span> ewan ewanlee@yeah.net</div><div class="line"></div><div class="line"><span class="keyword">COPY</span> jpress-web-newest.war /usr/local/tomcat/webapps</div></pre></td></tr></table></figure><h2 id="搭建第一个Web-app"><a href="#搭建第一个Web-app" class="headerlink" title="搭建第一个Web app"></a>搭建第一个Web app</h2><p>为了介绍方便，所以使用了开源的java实现的wordpress，也就是<a href="https://github.com/JpressProjects/jpress" target="_blank" rel="external">Jpress</a></p><p>[1]下载相应的<a href="https://github.com/JpressProjects/jpress/blob/master/wars/jpress-web-newest.war" target="_blank" rel="external">war包</a>，并存到工作目录下</p><p>[2]下载一个tomcat的Docker镜像<code>docker pull tomcat</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">Using default tag: latest</div><div class="line">latest: Pulling from library/tomcat</div><div class="line">cd0a524342ef: Pull complete </div><div class="line">e39c3ffe4133: Pull complete </div><div class="line">aac3320edf40: Pull complete </div><div class="line">4d9e109682f7: Pull complete </div><div class="line">0a59efcf9553: Pull complete </div><div class="line">43a404e523e0: Pull complete </div><div class="line">806f07b1dce8: Pull complete </div><div class="line">0cad96dccb4c: Pull complete </div><div class="line">04073e2a9145: Pull complete </div><div class="line">d9e4bf4be89c: Pull complete </div><div class="line">739005fdecc9: Pull complete </div><div class="line">8bd03d99f1b2: Pull complete </div><div class="line">d586afbd7622: Pull complete </div><div class="line">Digest: sha256:88483873b279aaea5ced002c98dde04555584b66de29797a4476d5e94874e6de</div><div class="line">Status: Downloaded newer image for tomcat:latest</div></pre></td></tr></table></figure><p>[3]写一个<code>Dockerfile</code>，也就是之前的example</p><p>[4]建立镜像<code>docker build -t jpress:latest .</code></p><p>结果如下：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Sending build context to Docker daemon  <span class="number">20.8</span> MB</div><div class="line">Step <span class="number">1</span> : FROM tomcat</div><div class="line"> ---&gt; d71978506e58</div><div class="line">Step <span class="number">2</span> : MAINTAINER ewan ewanlee<span class="meta">@yeah</span>.net</div><div class="line"> ---&gt; Running <span class="keyword">in</span> dfa1902d1ea4</div><div class="line"> ---&gt; <span class="number">956612</span>ba6987</div><div class="line">Removing intermediate container dfa1902d1ea4</div><div class="line">Step <span class="number">3</span> : COPY jpress-web-newest.war /usr/local/tomcat/webapps</div><div class="line"> ---&gt; dd6eecd741e7</div><div class="line">Removing intermediate container <span class="number">1</span>fe7f943071b</div><div class="line">Successfully built dd6eecd741e7</div></pre></td></tr></table></figure><p>[5]下载一个mysql的docker镜像<code>docker pull mysql</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">Using default tag: latest</div><div class="line">latest: Pulling from library/mysql</div><div class="line">cd0a524342ef: Already exists </div><div class="line">d9c95f06c17e: Pull complete </div><div class="line">46b2d578f59a: Pull complete </div><div class="line">10fbc2bcc6e9: Pull complete </div><div class="line">91b1a29c3956: Pull complete </div><div class="line">5bf9316bd602: Pull complete </div><div class="line">69bd23f08b55: Pull complete </div><div class="line">4fb778132e94: Pull complete </div><div class="line">6913628d7744: Pull complete </div><div class="line">a477f36dc2e0: Pull complete </div><div class="line">c954124ae935: Pull complete </div><div class="line">Digest: sha256:e44b9a3ae88db013a3e8571a89998678ba44676ed4ae9f54714fd31e108f8b58</div><div class="line">Status: Downloaded newer image for mysql:latest</div></pre></td></tr></table></figure><p>[6]运行mysql并创建一个数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=000000 -e MYSQL_DATABASE=jpress mysql</div></pre></td></tr></table></figure><p>[7]运行自己建立的<code>jpress</code>镜像<code>docker run -d -p 8888:8080 jpress</code></p><p>下面进行浏览器页面的配置，在浏览器输入<code>localhost:8888</code>将出现以下界面：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/docker-jpress/index_page.png" alt="index"></p><p>在地址栏后加入后缀<code>jpress-web-newest</code></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/docker-jpress/jpress_install.png" alt="install"></p><p>填写配置信息，注意服务器地址是<code>docker0</code>网卡的ip</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/docker-jpress/config.png" alt="config"></p><p>结果</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/docker-jpress/home_page.png" alt="home"></p><p>安装过程中出现了一个bug，就是在进行配置后我退出了，再次进去重新配置出错，最后发现原因是表前缀需要改一下，因为之前配置成功了，数据库中已经有了一个相同的表前缀​:P</p><p>是不是很方便，完全不用手动安装任何东西~</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="http://www.imooc.com/learn/824" target="_blank" rel="external">http://www.imooc.com/learn/824</a></li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/01/HMM-implemented-by-hmmlearn/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/01/HMM-implemented-by-hmmlearn/" itemprop="url">HMM implemented by hmmlearn</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-01T13:33:52+08:00">2017-05-01 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/01/HMM-implemented-by-hmmlearn/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/01/HMM-implemented-by-hmmlearn/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/01/HMM-implemented-by-hmmlearn/" class="leancloud_visitors" data-flag-title="HMM implemented by hmmlearn"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="Sampling-from-HMM"><a href="#Sampling-from-HMM" class="headerlink" title="Sampling from HMM"></a>Sampling from HMM</h1><p>This script shows how to sample points from a Hidden Markov Model (HMM): we use a 4-components with specified mean and covariance.</p><p>The plot show the sequence of observations generated with the transitions between them. We can see that, as specified by our transition matrix, there are no transition between component 1 and 3.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">print(__doc__)</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="keyword">from</span> hmmlearn <span class="keyword">import</span> hmm</div></pre></td></tr></table></figure><p>Prepare parameters for a 4-components HMM Initial population probability</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">startprob = np.array([<span class="number">0.6</span>, <span class="number">0.3</span>, <span class="number">0.1</span>, <span class="number">0.0</span>])</div><div class="line"><span class="comment"># The transition matrix, note that there are no transitions possible</span></div><div class="line"><span class="comment"># between component 1 and 3</span></div><div class="line">transmat = np.array([[<span class="number">0.7</span>, <span class="number">0.2</span>, <span class="number">0.0</span>, <span class="number">0.1</span>],</div><div class="line">                     [<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.0</span>],</div><div class="line">                     [<span class="number">0.0</span>, <span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.2</span>],</div><div class="line">                     [<span class="number">0.2</span>, <span class="number">0.0</span>, <span class="number">0.2</span>, <span class="number">0.6</span>]])</div><div class="line"><span class="comment"># The means of each component</span></div><div class="line">means = np.array([[<span class="number">0.0</span>,  <span class="number">0.0</span>],</div><div class="line">                  [<span class="number">0.0</span>, <span class="number">11.0</span>],</div><div class="line">                  [<span class="number">9.0</span>, <span class="number">10.0</span>],</div><div class="line">                  [<span class="number">11.0</span>, <span class="number">-1.0</span>]])</div><div class="line"><span class="comment"># The covariance of each component</span></div><div class="line">covars = <span class="number">.5</span> * np.tile(np.identity(<span class="number">2</span>), (<span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>))</div><div class="line"></div><div class="line"><span class="comment"># Build an HMM instance and set parameters</span></div><div class="line">model = hmm.GaussianHMM(n_components=<span class="number">4</span>, covariance_type=<span class="string">"full"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Instead of fitting it from the data, we directly set the estimated</span></div><div class="line"><span class="comment"># parameters, the means and covariance of the components</span></div><div class="line">model.startprob_ = startprob</div><div class="line">model.transmat_ = transmat</div><div class="line">model.means_ = means</div><div class="line">model.covars_ = covars</div><div class="line"></div><div class="line"><span class="comment"># Generate samples</span></div><div class="line">X, Z = model.sample(<span class="number">500</span>)</div><div class="line"></div><div class="line"><span class="comment"># Plot the sampled data</span></div><div class="line">plt.plot(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], <span class="string">".-"</span>, label=<span class="string">"observations"</span>, ms=<span class="number">6</span>,</div><div class="line">         mfc=<span class="string">"orange"</span>, alpha=<span class="number">0.7</span>)</div><div class="line"></div><div class="line"><span class="comment"># Indicate the component numbers</span></div><div class="line"><span class="keyword">for</span> i, m <span class="keyword">in</span> enumerate(means):</div><div class="line">    plt.text(m[<span class="number">0</span>], m[<span class="number">1</span>], <span class="string">'Component %i'</span> % (i + <span class="number">1</span>),</div><div class="line">             size=<span class="number">17</span>, horizontalalignment=<span class="string">'center'</span>,</div><div class="line">             bbox=dict(alpha=<span class="number">.7</span>, facecolor=<span class="string">'w'</span>))</div><div class="line">plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://hmmlearn.readthedocs.io/en/latest/_images/sphx_glr_plot_hmm_sampling_001.png" alt="sampling_result"></p><p><strong>Total running time of the script:</strong> ( 0 minutes 0.676 seconds)</p><h1 id="Gaussian-HMM-of-stock-data"><a href="#Gaussian-HMM-of-stock-data" class="headerlink" title="Gaussian HMM of stock data"></a>Gaussian HMM of stock data</h1><p>This script shows how to use Gaussian HMM on stock price data from Yahoo! finance. For more information on how to visualize stock prices with matplotlib, please refer to <code>date_demo1.py</code> of matplotlib.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> datetime</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm, pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> matplotlib.dates <span class="keyword">import</span> YearLocator, MonthLocator</div><div class="line"><span class="keyword">try</span>:</div><div class="line">    <span class="keyword">from</span> matplotlib.finance <span class="keyword">import</span> quotes_historical_yahoo_ochl</div><div class="line"><span class="keyword">except</span> ImportError:</div><div class="line">    <span class="comment"># For Matplotlib prior to 1.5.</span></div><div class="line">    <span class="keyword">from</span> matplotlib.finance <span class="keyword">import</span> (</div><div class="line">        quotes_historical_yahoo <span class="keyword">as</span> quotes_historical_yahoo_ochl</div><div class="line">    )</div><div class="line"></div><div class="line"><span class="keyword">from</span> hmmlearn.hmm <span class="keyword">import</span> GaussianHMM</div><div class="line"></div><div class="line"></div><div class="line">print(__doc__)</div></pre></td></tr></table></figure><p>Get quotes from Yahoo! finance</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">quotes = quotes_historical_yahoo_ochl(</div><div class="line">    <span class="string">"INTC"</span>, datetime.date(<span class="number">1995</span>, <span class="number">1</span>, <span class="number">1</span>), datetime.date(<span class="number">2012</span>, <span class="number">1</span>, <span class="number">6</span>))</div><div class="line"></div><div class="line"><span class="comment"># Unpack quotes</span></div><div class="line">dates = np.array([q[<span class="number">0</span>] <span class="keyword">for</span> q <span class="keyword">in</span> quotes], dtype=int)</div><div class="line">close_v = np.array([q[<span class="number">2</span>] <span class="keyword">for</span> q <span class="keyword">in</span> quotes])</div><div class="line">volume = np.array([q[<span class="number">5</span>] <span class="keyword">for</span> q <span class="keyword">in</span> quotes])[<span class="number">1</span>:]</div><div class="line"></div><div class="line"><span class="comment"># Take diff of close value. Note that this makes</span></div><div class="line"><span class="comment"># ``len(diff) = len(close_t) - 1``, therefore, other quantities also</span></div><div class="line"><span class="comment"># need to be shifted by 1.</span></div><div class="line">diff = np.diff(close_v)</div><div class="line">dates = dates[<span class="number">1</span>:]</div><div class="line">close_v = close_v[<span class="number">1</span>:]</div><div class="line"></div><div class="line"><span class="comment"># Pack diff and volume for training.</span></div><div class="line">X = np.column_stack([diff, volume])</div></pre></td></tr></table></figure><p>Run Gaussian HMM</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"fitting to HMM and decoding ..."</span>, end=<span class="string">""</span>)</div><div class="line"></div><div class="line"><span class="comment"># Make an HMM instance and execute fit</span></div><div class="line">model = GaussianHMM(n_components=<span class="number">4</span>, covariance_type=<span class="string">"diag"</span>, n_iter=<span class="number">1000</span>).fit(X)</div><div class="line"></div><div class="line"><span class="comment"># Predict the optimal sequence of internal hidden state</span></div><div class="line">hidden_states = model.predict(X)</div><div class="line"></div><div class="line">print(<span class="string">"done"</span>)</div></pre></td></tr></table></figure><p>Out:</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">fitting to HMM and decoding ...done</div></pre></td></tr></table></figure><p>Print trained parameters and plot</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"Transition matrix"</span>)</div><div class="line">print(model.transmat_)</div><div class="line">print()</div><div class="line"></div><div class="line">print(<span class="string">"Means and vars of each hidden state"</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(model.n_components):</div><div class="line">    print(<span class="string">"&#123;0&#125;th hidden state"</span>.format(i))</div><div class="line">    print(<span class="string">"mean = "</span>, model.means_[i])</div><div class="line">    print(<span class="string">"var = "</span>, np.diag(model.covars_[i]))</div><div class="line">    print()</div><div class="line"></div><div class="line">fig, axs = plt.subplots(model.n_components, sharex=<span class="keyword">True</span>, sharey=<span class="keyword">True</span>)</div><div class="line">colours = cm.rainbow(np.linspace(<span class="number">0</span>, <span class="number">1</span>, model.n_components))</div><div class="line"><span class="keyword">for</span> i, (ax, colour) <span class="keyword">in</span> enumerate(zip(axs, colours)):</div><div class="line">    <span class="comment"># Use fancy indexing to plot data in each state.</span></div><div class="line">    mask = hidden_states == i</div><div class="line">    ax.plot_date(dates[mask], close_v[mask], <span class="string">".-"</span>, c=colour)</div><div class="line">    ax.set_title(<span class="string">"&#123;0&#125;th hidden state"</span>.format(i))</div><div class="line"></div><div class="line">    <span class="comment"># Format the ticks.</span></div><div class="line">    ax.xaxis.set_major_locator(YearLocator())</div><div class="line">    ax.xaxis.set_minor_locator(MonthLocator())</div><div class="line"></div><div class="line">    ax.grid(<span class="keyword">True</span>)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://hmmlearn.readthedocs.io/en/latest/_images/sphx_glr_plot_hmm_stock_analysis_001.png" alt="hmm_yahoo_analysis"></p><p>Out:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">Transition matrix</div><div class="line">[[  <span class="number">9.79220773e-01</span>   <span class="number">2.57382344e-15</span>   <span class="number">2.72061945e-03</span>   <span class="number">1.80586073e-02</span>]</div><div class="line"> [  <span class="number">1.12216188e-12</span>   <span class="number">7.73561269e-01</span>   <span class="number">1.85019044e-01</span>   <span class="number">4.14196869e-02</span>]</div><div class="line"> [  <span class="number">3.25313504e-03</span>   <span class="number">1.12692615e-01</span>   <span class="number">8.83368021e-01</span>   <span class="number">6.86228435e-04</span>]</div><div class="line"> [  <span class="number">1.18741799e-01</span>   <span class="number">4.20310643e-01</span>   <span class="number">1.18670597e-18</span>   <span class="number">4.60947557e-01</span>]]</div><div class="line"></div><div class="line">Means <span class="keyword">and</span> vars of each hidden state</div><div class="line"><span class="number">0</span>th hidden state</div><div class="line">mean =  [  <span class="number">2.33331888e-02</span>   <span class="number">4.97389989e+07</span>]</div><div class="line">var =  [  <span class="number">6.97748259e-01</span>   <span class="number">2.49466578e+14</span>]</div><div class="line"></div><div class="line"><span class="number">1</span>th hidden state</div><div class="line">mean =  [  <span class="number">2.12401671e-02</span>   <span class="number">8.81882861e+07</span>]</div><div class="line">var =  [  <span class="number">1.18665023e-01</span>   <span class="number">5.64418451e+14</span>]</div><div class="line"></div><div class="line"><span class="number">2</span>th hidden state</div><div class="line">mean =  [  <span class="number">7.69658065e-03</span>   <span class="number">5.43135922e+07</span>]</div><div class="line">var =  [  <span class="number">5.02315562e-02</span>   <span class="number">1.54569357e+14</span>]</div><div class="line"></div><div class="line"><span class="number">3</span>th hidden state</div><div class="line">mean =  [ <span class="number">-3.53210673e-01</span>   <span class="number">1.53080943e+08</span>]</div><div class="line">var =  [  <span class="number">2.55544137e+00</span>   <span class="number">5.88210257e+15</span>]</div></pre></td></tr></table></figure><p><strong>Total running time of the script:</strong> ( 0 minutes 2.903 seconds)</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/13/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><span class="page-number current">14</span><a class="page-number" href="/page/15/">15</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/15/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">124</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">59</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>