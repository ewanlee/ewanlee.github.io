<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/14/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/14/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/31/Dynamic-Programming/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/31/Dynamic-Programming/" itemprop="url">Dynamic Programming</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-31T10:19:52+08:00">2017-05-31 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/31/Dynamic-Programming/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/31/Dynamic-Programming/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/31/Dynamic-Programming/" class="leancloud_visitors" data-flag-title="Dynamic Programming"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are provides an essential foundation for the understanding of the methods presented later. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.</p><p>The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. In here we show how DP can be used to compute the value functions defined in earlier. As discussed there, we can easily obtain optimal policies once we have found the optimal value functions $v_{\star}$ or $q_{\star}$ which satisfy the Bellman optimality equations:<br>$$<br>\begin{align}<br>v_{\star}(s) &amp;= \max_{a} \mathbb{E} [R_{t+1} + \gamma v_{\star}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \max_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) \left[r + \gamma v_{\star}(s^{\prime})\right]<br>\end{align}<br>$$<br>or<br>$$<br>\begin{align}<br>q_{\star}(s, a) &amp;= \mathbb{E} [R_{t+1} + \gamma \max_{a^{\prime}} q_{\star}(S_{t+1}, a^{\prime}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma \max_{a^{\prime}} q_{\star}(s^{\prime}, a^{\prime})]<br>\end{align}<br>$$<br>for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}, \; \text{and} \; s^{\prime} \in \mathcal{S^{+}}$. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.</p><p>First we consider how to compute the state-value function $v_{\pi}$ for an arbitrary policy $\pi$. This is called <em>policy evaluation</em> in the DP literature. We also refer to it as the <em>prediction problem</em>. Recall that for all $s \in \mathcal{S}$,<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots \ | \ S_{t}=s] \\<br>&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_{t}=s] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{\pi}(s^{\prime})]<br>\end{align}<br>$$<br>If the environment’s dynamics are complete known, then (7) is a system of $|\mathcal{S}|$ simultaneous linear equations in $|\mathcal{S}|$ unknowns (the $v_{\pi}(s), s \in \mathcal{S}$). In principle, its solution is a straightforward, if tedious, computation. For our purpose, iterative solution methods are most suitable. The initial approximation, $v_0$, is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation for $v_{\pi}$ as an update rule:<br>$$<br>\begin{align}<br>v_{k+1}(s) &amp;\doteq \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r|s, a) [r + \gamma v_{k} (s^{\prime})]<br>\end{align}<br>$$<br>This algorithm is called <em>iterative policy evaluation</em>.</p><blockquote><p><strong>Iterative policy evaluation</strong></p><p>Input $\pi$, the policy to be evaluated</p><p>Initialize an array $V(s) = 0$, for all $s \in \mathcal{S^{+}}$</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ for each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p><p>Output $V \approx v_{\pi}$</p></blockquote><p>We can see the algorithm used in the <a href="https://ewanlee.github.io/2017/05/29/The-GridWorld-problem/" target="_blank" rel="external">grid world problem</a> just is the <em>iterative policy evaluation</em>.</p><p>Our reason for computing the value function for a policy is to help find better policies. Once a policy $\pi$ has been improved using $v_{\pi}$ to yield a better policy $\pi^{\prime}$, we can then compute $v_{\pi^{\prime}}$and improve it again to yield an even better $\pi^{\prime\prime}$. We can thus obtain a sequence of monotonically improving policies and value functions:<br>$$<br>\pi_{0} \stackrel{E}\longrightarrow v_{\pi_{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow v_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow v_{\star},<br>$$<br>where $\stackrel{E}\longrightarrow$ denotes a policy <em>evaluation</em> and $\stackrel{I}\longrightarrow$ denotes a policy <em>improvement</em>. This way of finding an optimal policy is called <em>policy iteration</em>.</p><blockquote><p><strong>Policy iteration (using iterative policy evaluation)</strong></p><ol><li><p>Initialization</p><p>$V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A(s)}$ arbitrarily for all $s \in \mathcal{S}$</p></li><li><p>Policy Evaluation</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ For each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \sum_{s^{\prime}, r} p(s^{\prime}, r | s, \pi(s)) [r + \gamma v(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p></li><li><p>Policy Improvement</p><p><em>policy-stable</em> $\leftarrow$ <em>true</em></p><p>For each $s \in \mathcal{S}$:</p><p>​ <em>old-action</em> $\leftarrow$ $\pi_(s)$</p><p>​ $\pi (s) \leftarrow argmax_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$</p><p>​ If <em>old-action</em> $\neq \pi(s)$, then <em>policy-stable</em> $\leftarrow$ <em>false</em></p><p>If <em>policy-stable</em>, then stop and return $V \approx v_{\star} \; \text{and} \; \pi \approx \pi_{\star}$; else go to 2.</p></li></ol></blockquote><p>Let us solve a problem used by <strong>policy iteration</strong>. The problem defined as follows:</p><blockquote><p>Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and it credited \$10 by the national company. If he out of cats at that location, then the business is lost. Cars become available for renting the day after they are returned. To ensure that cars are available where they are needed, Jack ca move them between the two locations overnight, at a cost of \$2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is $n$ is $\frac{\lambda^{n}}{n!}e^{-\lambda}$, where $\lambda$ is the excepted number. Suppose $\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\lambda=0.9$ and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight.</p></blockquote><p>The excepted result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental.png" alt="car_rental"></p><p><em>Figure 1</em></p><p>The first, we define some facts of this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># maximum # of cars in each location</span></div><div class="line">MAX_CARS = <span class="number">20</span></div><div class="line"><span class="comment"># maximum # of cars to move during night</span></div><div class="line">MAX_MOVE_OF_CARS = <span class="number">5</span></div><div class="line"><span class="comment"># expectation for rental requests in first location</span></div><div class="line">RENTAL_REQUEST_FIRST_LOC = <span class="number">3</span></div><div class="line"><span class="comment"># expectation for rental requests in second location</span></div><div class="line">RENTAL_REQUEST_SECOND_LOC = <span class="number">4</span></div><div class="line"><span class="comment"># expectation for # of cars returned in first location</span></div><div class="line">RETURNS_FIRST_LOC = <span class="number">3</span></div><div class="line"><span class="comment"># expectation for # of cars returned in second location</span></div><div class="line">RETURNS_SECOND_LOC = <span class="number">2</span></div><div class="line">DISCOUNT = <span class="number">0.9</span></div><div class="line"><span class="comment"># credit earned by a car</span></div><div class="line">RENTAL_CREDIT = <span class="number">10</span></div><div class="line"><span class="comment"># cost of moving a car</span></div><div class="line">MOVE_CAR_COST = <span class="number">2</span></div></pre></td></tr></table></figure><p>From the problem definition, we know that in this MDP the states is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight. Each action is a integer that positive number represents the number of cars moving from the first location to second location and vice verse.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># current policy</span></div><div class="line">policy = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line"><span class="comment"># current state value</span></div><div class="line">stateValue = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line"><span class="comment"># all possible states</span></div><div class="line">states = []</div><div class="line"><span class="comment"># all possible actions</span></div><div class="line">actions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + <span class="number">1</span>)</div></pre></td></tr></table></figure><p>For visualization (Figure 1) convenient, we define a method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># axes for printing use</span></div><div class="line">AxisXPrint = []</div><div class="line">AxisYPrint = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, MAX_CARS + <span class="number">1</span>):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, MAX_CARS + <span class="number">1</span>):</div><div class="line">        AxisXPrint.append(i)</div><div class="line">        AxisYPrint.append(j)</div><div class="line">        states.append([i, j])</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># plot a policy/state value matrix</span></div><div class="line">figureIndex = <span class="number">0</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prettyPrint</span><span class="params">(data, labels)</span>:</span></div><div class="line">    <span class="keyword">global</span> figureIndex</div><div class="line">    fig = plt.figure(figureIndex)</div><div class="line">    figureIndex += <span class="number">1</span></div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)</div><div class="line">    AxisZ = []</div><div class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">        AxisZ.append(data[i, j])</div><div class="line">    ax.scatter(AxisXPrint, AxisYPrint, AxisZ)</div><div class="line">    ax.set_xlabel(labels[<span class="number">0</span>])</div><div class="line">    ax.set_ylabel(labels[<span class="number">1</span>])</div><div class="line">    ax.set_zlabel(labels[<span class="number">2</span>])</div></pre></td></tr></table></figure><p>Next, we define a Poisson function that return the probability:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># An up bound for poisson distribution</span></div><div class="line"><span class="comment"># If n is greater than this value, then the probability of getting n is truncated to 0</span></div><div class="line">POISSON_UP_BOUND = <span class="number">11</span></div><div class="line"></div><div class="line"><span class="comment"># Probability for poisson distribution</span></div><div class="line"><span class="comment"># @lam: lambda should be less than 10 for this function</span></div><div class="line">poissonBackup = dict()</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">poisson</span><span class="params">(n, lam)</span>:</span></div><div class="line">    <span class="keyword">global</span> poissonBackup</div><div class="line">    key = n * <span class="number">10</span> + lam</div><div class="line">    <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> poissonBackup.keys():</div><div class="line">        poissonBackup[key] = exp(-lam) * pow(lam, n) / factorial(n)</div><div class="line">    <span class="keyword">return</span> poissonBackup[key]</div></pre></td></tr></table></figure><p>Now, the preparation is done. We’ll implement the policy iteration algorithm as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">newStateValue = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line">improvePolicy = <span class="keyword">False</span></div><div class="line">policyImprovementInd = <span class="number">0</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="keyword">if</span> improvePolicy == <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># start policy improvement</span></div><div class="line">        print(<span class="string">'Policy improvement'</span>, policyImprovementInd)</div><div class="line">        policyImprovementInd += <span class="number">1</span></div><div class="line">        newPolicy = np.zeros((MAX_CARS + <span class="number">1</span>, MAX_CARS + <span class="number">1</span>))</div><div class="line">        <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">            actionReturns = []</div><div class="line">            <span class="comment"># go through all actions and select the best one</span></div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                <span class="keyword">if</span> (action &gt;= <span class="number">0</span> <span class="keyword">and</span> i &gt;= action) <span class="keyword">or</span> (action &lt; <span class="number">0</span> <span class="keyword">and</span> j &gt;= abs(action)):</div><div class="line">                    actionReturns.append(expectedReturn([i, j], action, stateValue))</div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    actionReturns.append(-float(<span class="string">'inf'</span>))</div><div class="line">            bestAction = argmax(actionReturns)</div><div class="line">            newPolicy[i, j] = actions[bestAction]</div><div class="line"></div><div class="line">        <span class="comment"># if policy is stable</span></div><div class="line">        policyChanges = np.sum(newPolicy != policy)</div><div class="line">        print(<span class="string">'Policy for'</span>, policyChanges, <span class="string">'states changed'</span>)</div><div class="line">        <span class="keyword">if</span> policyChanges == <span class="number">0</span>:</div><div class="line">            policy = newPolicy</div><div class="line">            <span class="keyword">break</span></div><div class="line">        policy = newPolicy</div><div class="line">        improvePolicy = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="comment"># start policy evaluation</span></div><div class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> states:</div><div class="line">        newStateValue[i, j] = expectedReturn([i, j], policy[i, j], stateValue)</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(newStateValue - stateValue)) &lt; <span class="number">1e-4</span>:</div><div class="line">        stateValue[:] = newStateValue</div><div class="line">        improvePolicy = <span class="keyword">True</span></div><div class="line">        <span class="keyword">continue</span></div><div class="line">    stateValue[:] = newStateValue</div></pre></td></tr></table></figure><p>We can see the logistic is the same as the pseudocode of the policy iteration algorithm. There is a core method in the code, that is, <strong>exceptedReturn()</strong> is used to calculate the reward of cars rental.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># @state: [# of cars in first location, # of cars in second location]</span></div><div class="line"><span class="comment"># @action: positive if moving cars from first location to second location,</span></div><div class="line"><span class="comment">#          negative if moving cars from second location to first location</span></div><div class="line"><span class="comment"># @stateValue: state value matrix</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">expectedReturn</span><span class="params">(state, action, stateValue)</span>:</span></div><div class="line">    <span class="comment"># initailize total return</span></div><div class="line">    returns = <span class="number">0.0</span></div><div class="line"></div><div class="line">    <span class="comment"># cost for moving cars</span></div><div class="line">    returns -= MOVE_CAR_COST * abs(action)</div><div class="line"></div><div class="line">    <span class="comment"># go through all possible rental requests</span></div><div class="line">    <span class="keyword">for</span> rentalRequestFirstLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">        <span class="keyword">for</span> rentalRequestSecondLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">            <span class="comment"># moving cars</span></div><div class="line">            numOfCarsFirstLoc = int(min(state[<span class="number">0</span>] - action, MAX_CARS))</div><div class="line">            numOfCarsSecondLoc = int(min(state[<span class="number">1</span>] + action, MAX_CARS))</div><div class="line"></div><div class="line">            <span class="comment"># valid rental requests should be less than actual # of cars</span></div><div class="line">            realRentalFirstLoc = min(numOfCarsFirstLoc, rentalRequestFirstLoc)</div><div class="line">            realRentalSecondLoc = min(numOfCarsSecondLoc, rentalRequestSecondLoc)</div><div class="line"></div><div class="line">            <span class="comment"># get credits for renting</span></div><div class="line">            reward = (realRentalFirstLoc + realRentalSecondLoc) * RENTAL_CREDIT</div><div class="line">            numOfCarsFirstLoc -= realRentalFirstLoc</div><div class="line">            numOfCarsSecondLoc -= realRentalSecondLoc</div><div class="line"></div><div class="line">            <span class="comment"># probability for current combination of rental requests</span></div><div class="line">            prob = poisson(rentalRequestFirstLoc, RENTAL_REQUEST_FIRST_LOC) * \</div><div class="line">                         poisson(rentalRequestSecondLoc, RENTAL_REQUEST_SECOND_LOC)</div><div class="line"></div><div class="line">            <span class="comment"># if set True, model is simplified such that the # of cars returned in daytime becomes constant</span></div><div class="line">            <span class="comment"># rather than a random value from poisson distribution, which will reduce calculation time</span></div><div class="line">            <span class="comment"># and leave the optimal policy/value state matrix almost the same</span></div><div class="line">            constantReturnedCars = <span class="keyword">True</span></div><div class="line">            <span class="keyword">if</span> constantReturnedCars:</div><div class="line">                <span class="comment"># get returned cars, those cars can be used for renting tomorrow</span></div><div class="line">                returnedCarsFirstLoc = RETURNS_FIRST_LOC</div><div class="line">                returnedCarsSecondLoc = RETURNS_SECOND_LOC</div><div class="line">                numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS)</div><div class="line">                numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS)</div><div class="line">                returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc])</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                numOfCarsFirstLoc_ = numOfCarsFirstLoc</div><div class="line">                numOfCarsSecondLoc_ = numOfCarsSecondLoc</div><div class="line">                prob_ = prob</div><div class="line">                <span class="keyword">for</span> returnedCarsFirstLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">                    <span class="keyword">for</span> returnedCarsSecondLoc <span class="keyword">in</span> range(<span class="number">0</span>, POISSON_UP_BOUND):</div><div class="line">                        numOfCarsFirstLoc = numOfCarsFirstLoc_</div><div class="line">                        numOfCarsSecondLoc = numOfCarsSecondLoc_</div><div class="line">                        prob = prob_</div><div class="line">                        numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS)</div><div class="line">                        numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS)</div><div class="line">                        prob = poisson(returnedCarsFirstLoc, RETURNS_FIRST_LOC) * \</div><div class="line">                               poisson(returnedCarsSecondLoc, RETURNS_SECOND_LOC) * prob</div><div class="line">                        returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc])</div><div class="line">    <span class="keyword">return</span> returns</div></pre></td></tr></table></figure><p>The comments are very clear, and we’re going to do a lot of this. Finally, let us print the result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">prettyPrint(policy, [<span class="string">'# of cars in first location'</span>, <span class="string">'# of cars in second location'</span>, <span class="string">'# of cars to move during night'</span>])</div><div class="line">prettyPrint(stateValue, [<span class="string">'# of cars in first location'</span>, <span class="string">'# of cars in second location'</span>, <span class="string">'expected returns'</span>])</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>The results are as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Policy improvement <span class="number">0</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">332</span> states changed</div><div class="line">Policy improvement <span class="number">1</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">286</span> states changed</div><div class="line">Policy improvement <span class="number">2</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">83</span> states changed</div><div class="line">Policy improvement <span class="number">3</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">19</span> states changed</div><div class="line">Policy improvement <span class="number">4</span></div><div class="line">Policy <span class="keyword">for</span> <span class="number">0</span> states changed</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_policy.png" alt="car_rental_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/car_rental_return.png" alt="car_rental_return"></p><p>One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to $v_{\pi}$ occurs only in the limit. In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This algorithm is called <em>value iteration</em>. It can be written as a particular simple backup operation that combines the policy improvement and truncated policy evaluation steps:<br>$$<br>\begin{align}<br>v_{k+1} &amp;\doteq \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\<br>&amp;= \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{k}(s^{\prime})],<br>\end{align}<br>$$<br>for all $s \in \mathcal{S}$.</p><blockquote><p><strong>Value iteration</strong></p><p>Initialize array $V$ arbitrarily (e.g. $V(s) = 0$ for all $s \in \mathcal{S^{+}}$)</p><p>Repeat</p><p>​ $\Delta \leftarrow 0$</p><p>​ For each $s \in \mathcal{S}$:</p><p>​ $v \leftarrow V(s)$</p><p>​ $V(s) \leftarrow \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$</p><p>​ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$</p><p>until $\Delta &lt; \theta$ (a small positive number)</p><p>Output a deterministic policy, $\pi \approx \pi_{\star}$, such that</p><p>​ $\pi(s) = \arg\max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$</p></blockquote><p>Let us use the value iteration algorithm to solve a Gambler’s Problem. The problem defined as follows:</p><blockquote><p>A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he staked on the flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of \$100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer number of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gambler’s capital, $s \in \{1, 2, \cdots, 99\}$ and the actions are stakes, $a \in \{0, 1, \cdots, \min(s, 100-s)\}$. The reward is zero on all transitions excepted those on which the gambler reaches his goal, when it is +1. The state-value function then gives the probability of winning from each state. A policy is mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal. Let $p_h$ denote the probability of the coin coming up heads. If $p_h$ is known, then the entire problem is known and it can be solved, for instance, by value iteration.</p></blockquote><p>OK, now let us to solve this problem by use the value iteration algorithm.</p><p>The first we defined some facts and some auxiliary data structure:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># goal</span></div><div class="line">GOAL = <span class="number">100</span></div><div class="line"><span class="comment"># all states, including state 0 and state 100</span></div><div class="line">states = np.arange(GOAL + <span class="number">1</span>)</div><div class="line"><span class="comment"># probability of head</span></div><div class="line">headProb = <span class="number">0.4</span></div><div class="line"><span class="comment"># optimal policy</span></div><div class="line">policy = np.zeros(GOAL + <span class="number">1</span>)</div><div class="line"><span class="comment"># state value</span></div><div class="line">stateValue = np.zeros(GOAL + <span class="number">1</span>)</div><div class="line">stateValue[GOAL] = <span class="number">1.0</span></div></pre></td></tr></table></figure><p>The step of value iteration:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># value iteration</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    delta = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states[<span class="number">1</span>:GOAL]:</div><div class="line">        <span class="comment"># get possilbe actions for current state</span></div><div class="line">        actions = np.arange(min(state, GOAL - state) + <span class="number">1</span>)</div><div class="line">        actionReturns = []</div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">            actionReturns.append(headProb * stateValue[state + action] + (<span class="number">1</span> - headProb) * stateValue[state - action])</div><div class="line">        newValue = np.max(actionReturns)</div><div class="line">        delta += np.abs(stateValue[state] - newValue)</div><div class="line">        <span class="comment"># update state value</span></div><div class="line">        stateValue[state] = newValue</div><div class="line">    <span class="keyword">if</span> delta &lt; <span class="number">1e-9</span>:</div><div class="line">        <span class="keyword">break</span></div></pre></td></tr></table></figure><p>Calculate the optimal policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># calculate the optimal policy</span></div><div class="line"><span class="keyword">for</span> state <span class="keyword">in</span> states[<span class="number">1</span>:GOAL]:</div><div class="line">    actions = np.arange(min(state, GOAL - state) + <span class="number">1</span>)</div><div class="line">    actionReturns = []</div><div class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">        actionReturns.append(headProb * stateValue[state + action] + (<span class="number">1</span> - headProb) * stateValue[state - action])</div><div class="line">    <span class="comment"># due to tie and precision, can't reproduce the optimal policy in book</span></div><div class="line">    policy[state] = actions[argmax(actionReturns)]</div></pre></td></tr></table></figure><p>Print the results:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">plt.figure(<span class="number">1</span>)</div><div class="line">plt.xlabel(<span class="string">'Capital'</span>)</div><div class="line">plt.ylabel(<span class="string">'Value estimates'</span>)</div><div class="line">plt.plot(stateValue)</div><div class="line">plt.figure(<span class="number">2</span>)</div><div class="line">plt.scatter(states, policy)</div><div class="line">plt.xlabel(<span class="string">'Capital'</span>)</div><div class="line">plt.ylabel(<span class="string">'Final policy (stake)'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_policy.png" alt="gambler_policy"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/dp/gambler_value.png" alt="gambler_value"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/29/The-GridWorld-problem/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/29/The-GridWorld-problem/" itemprop="url">The GridWorld problem</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-29T14:09:26+08:00">2017-05-29 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/29/The-GridWorld-problem/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/29/The-GridWorld-problem/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/29/The-GridWorld-problem/" class="leancloud_visitors" data-flag-title="The GridWorld problem"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>A reinforcement learning task that satisfied the Markov property is called <em>Markov decision process</em>, or <em>MDP</em>. If the state and action spaces are finite, then it is called a <em>finite Markov decision process</em> (<em>finite MDP</em>).</p><p>A particular finite MDP is defined by its state and action sets and by the one-step dynamics of the environment. Given any state and action <em>s</em> and <em>a</em>, the probability of each possible pair of next state and reward, <em>s’</em>, <em>r</em>, is denoted<br>$$<br>p(s^{\prime}, r | s, a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime}, R_{t+1}=r \ | \ S_{t}=s, A_{t}=a \}<br>$$<br>Given that, one can compute anything else one might want to know about the environment, such as the excepted rewards of state-action pairs,<br>$$<br>r(s,a) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a] = \sum_{r \in \mathcal{R}}r\sum_{s^{\prime} \in \mathcal{S}}p(s^{\prime},r|s, a)<br>$$<br>the <em>state-transition probabilities</em>,<br>$$<br>p(s^{\prime}|s,a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime} \ | \ S_{t}=s, A_{t}=a\} = \sum_{r \in \mathcal{R}} p(s^{\prime},r|s, a)<br>$$<br>and the excepted rewards for state-action-next-state triples,<br>$$<br>r(s, a, s^{\prime}) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a, S_{t+1}=s^{\prime}] = \frac{\sum_{r \in \mathcal{R}}rp(s^{\prime},r|s,a)}{p(s^{\prime}|s,a)}<br>$$<br>Almost all reinforcement learning algorithms involve estimating <em>value functions</em>–functions of states (or of state-action pairs) that estimate <em>how good</em> it is for the agent to be in a given state (or how good it is to perform a given action in a given state).</p><p>Recall that a policy, $\pi$, is a mapping from a each state, $s \in \mathcal{S}$, and action, $a \in \mathcal{A}(s)$, to the probability $\pi(a|s)$ of taking action <em>a</em> when in state <em>s</em>. Informally, the <em>value</em> of a state <em>s</em> under a policy $\pi$, denoted $v_{\pi}(s)$, is the excepted return when starting in <em>s</em> and following $\pi$ thereafter. For MDPs, we can define $v_{\pi}(s)$ formally as<br>$$<br>v_{\pi}(s) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right]<br>$$<br>Note that the value of the terminal state, if any, is always zero. We call the function $v_{\pi}$ the <em>state-value function for policy $\pi$.</em></p><p>Similarly, we define the value of taking action <em>a</em> in state <em>s</em> under a policy $\pi$, denoted $q_{\pi}(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:<br>$$<br>q_{\pi}(s,a) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s, A_{t}=a] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s, A_{t}=a\right]<br>$$<br>We call $q_{\pi}$ the <em>action-value function for policy $\pi$</em>.</p><p>A fundamental property of the value functions used in reinforcement learning and dynamic programming is that they satisfy particular recursive relationships. For any policy $\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \mathbb{E_{\pi}[G_{t} \ | \ S_{t}=s]} \\<br>&amp;= \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right] \\<br>&amp;= \mathbb{E_{\pi}}\left[R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} \ | \ S_{t}=s \right] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}}\sum_{r}p(s^{\prime},r|s,a) \left[ r + \gamma \mathbb{E_{\pi}} \left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} | S_{t+1}=s^{\prime} \right] \right] \\<br>&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},<br>\end{align}<br>$$<br>Equation (11) is the <em>Bellman equation for $v_{\pi}$</em>.</p><p>Figure 1 (left) shows a rectangular grid world representation of a simple finite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: <strong>north</strong>, <strong>south</strong>, <strong>east</strong>, and <strong>west</strong>, which deterministically cause the agent to move one cell in the respective direction on the grid. Action would take the agent off the grid leave its location unchanged, but also result in a reward of -1. Other actions result in a reward of 0, excepted those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to $\mathrm{A^{\prime}}$. From state B, all actions yield a reward +5 and take the agent to $\mathrm{B^{\prime}}$.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/grid_world/grid_world.png" alt="grid_world"></p><p><em>Figure 1</em></p><p>Suppose the agent selects all four actions with equal probability in all states. Figure 1 (right) shows the value function, $v_{\pi}$, for this policy, for the discounted reward case with $\gamma = 0.9$. This value function was computed by solving the system of linear equations (11).</p><p>OK, now let us solve this problem. The first, we need to define the grid world by code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">WORLD_SIZE = <span class="number">5</span></div><div class="line">A_POS = [<span class="number">0</span>, <span class="number">1</span>]</div><div class="line">A_PRIME_POS = [<span class="number">4</span>, <span class="number">1</span>]</div><div class="line">B_POS = [<span class="number">0</span>, <span class="number">3</span>]</div><div class="line">B_PRIME_POS = [<span class="number">2</span>, <span class="number">3</span>]</div><div class="line">discount = <span class="number">0.9</span></div><div class="line"></div><div class="line">world = np.zeros((WORLD_SIZE, WORLD_SIZE))</div></pre></td></tr></table></figure><p>This world has 5 by 5 cells, and there are four special cells: A, A’, B, B’. Discount represents the $\gamma $ in equation (11). We know that the agent in the world selects all four actions with equal probability in all states (cells). So we have:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># left, up, right, down</span></div><div class="line">actions = [<span class="string">'L'</span>, <span class="string">'U'</span>, <span class="string">'R'</span>, <span class="string">'D'</span>]</div><div class="line"></div><div class="line">actionProb = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">    actionProb.append([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        actionProb[i].append(dict(&#123;<span class="string">'L'</span>:<span class="number">0.25</span>, <span class="string">'U'</span>:<span class="number">0.25</span>, <span class="string">'R'</span>:<span class="number">0.25</span>, <span class="string">'D'</span>:<span class="number">0.25</span>&#125;))</div></pre></td></tr></table></figure><p>The <strong>actionProb</strong> is a list that has five items. Each item represents a row in the grid and it also is a list that has five items that represents a column in corresponding row, that is, each item in a row represents a cell in the grid. In all cells (states), there are four direction could be selected with equal probability 0.25. Then, we’ll define a undirected graph with weights. The node represented the cell in grid. If between two node has a edge then the agent could move between this two nodes (cells). The weight on the edges represents the reward do this move.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">nextState = []</div><div class="line">actionReward = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">    nextState.append([])</div><div class="line">    actionReward.append([])</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        next = dict()</div><div class="line">        reward = dict()</div><div class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</div><div class="line">            next[<span class="string">'U'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'U'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'U'</span>] = [i - <span class="number">1</span>, j]</div><div class="line">            reward[<span class="string">'U'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> i == WORLD_SIZE - <span class="number">1</span>:</div><div class="line">            next[<span class="string">'D'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'D'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'D'</span>] = [i + <span class="number">1</span>, j]</div><div class="line">            reward[<span class="string">'D'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> j == <span class="number">0</span>:</div><div class="line">            next[<span class="string">'L'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'L'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'L'</span>] = [i, j - <span class="number">1</span>]</div><div class="line">            reward[<span class="string">'L'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> j == WORLD_SIZE - <span class="number">1</span>:</div><div class="line">            next[<span class="string">'R'</span>] = [i, j]</div><div class="line">            reward[<span class="string">'R'</span>] = <span class="number">-1.0</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            next[<span class="string">'R'</span>] = [i, j + <span class="number">1</span>]</div><div class="line">            reward[<span class="string">'R'</span>] = <span class="number">0.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> [i, j] == A_POS:</div><div class="line">            next[<span class="string">'L'</span>] = next[<span class="string">'R'</span>] = next[<span class="string">'D'</span>] = next[<span class="string">'U'</span>] = A_PRIME_POS</div><div class="line">            reward[<span class="string">'L'</span>] = reward[<span class="string">'R'</span>] = reward[<span class="string">'D'</span>] = reward[<span class="string">'U'</span>] = <span class="number">10.0</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> [i, j] == B_POS:</div><div class="line">            next[<span class="string">'L'</span>] = next[<span class="string">'R'</span>] = next[<span class="string">'D'</span>] = next[<span class="string">'U'</span>] = B_PRIME_POS</div><div class="line">            reward[<span class="string">'L'</span>] = reward[<span class="string">'R'</span>] = reward[<span class="string">'D'</span>] = reward[<span class="string">'U'</span>] = <span class="number">5.0</span></div><div class="line"></div><div class="line">        nextState[i].append(next)</div><div class="line">        actionReward[i].append(reward)</div></pre></td></tr></table></figure><p>The <strong>nextState</strong> and <strong>actionReward</strong> are the same as <strong>actionProb</strong> that we explained earlier.</p><p>Now, we could solve this problem by use the equation (11):<br>$$<br>\begin{align}<br>v_{\pi}(s) &amp;\doteq \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},<br>\end{align}<br>$$<br>Let us jump into the implementation detail.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep iteration until convergence</span></div><div class="line">    newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                newPosition = nextState[i][j][action]</div><div class="line">                <span class="comment"># bellman equation</span></div><div class="line">                newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + discount * world[newPosition[<span class="number">0</span>], newPosition[<span class="number">1</span>]])</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(world - newWorld)) &lt; <span class="number">1e-4</span>:</div><div class="line">        print(<span class="string">'Random Policy'</span>)</div><div class="line">        print(newWorld)</div><div class="line">        <span class="keyword">break</span></div><div class="line">    world = newWorld</div></pre></td></tr></table></figure><p>The core code is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + 		 discount * world[newPosition[<span class="number">0</span>], newPosition[<span class="number">1</span>]])</div></pre></td></tr></table></figure><p>The <code>+=</code> represents the first sum notation in the equation (11). If we ensure the current state (cell) and action will take in this world, then the next state and reward also will be ensured. So $\sum_{s^{\prime},r} p(s^{\prime}, r | s, a)$ is equal to 1.</p><p>The result as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Random Policy</div><div class="line">[[ <span class="number">3.30902999</span>  <span class="number">8.78932551</span>  <span class="number">4.42765281</span>  <span class="number">5.3224012</span>   <span class="number">1.49221235</span>]</div><div class="line"> [ <span class="number">1.52162172</span>  <span class="number">2.9923515</span>   <span class="number">2.25017358</span>  <span class="number">1.90760531</span>  <span class="number">0.5474363</span> ]</div><div class="line"> [ <span class="number">0.05085614</span>  <span class="number">0.73820423</span>  <span class="number">0.67314689</span>  <span class="number">0.35821982</span> <span class="number">-0.40310755</span>]</div><div class="line"> [<span class="number">-0.97355865</span> <span class="number">-0.43546179</span> <span class="number">-0.35484864</span> <span class="number">-0.58557148</span> <span class="number">-1.18304148</span>]</div><div class="line"> [<span class="number">-1.8576669</span>  <span class="number">-1.34519762</span> <span class="number">-1.22923364</span> <span class="number">-1.42288454</span> <span class="number">-1.97514545</span>]]</div></pre></td></tr></table></figure><p>We can see the value of all states is the same as the Figure 1.</p><p>Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi^{\prime}$ if its excepted return is greater than or equal to that of $\pi^{\prime}$ for all states. In other words, $\pi \ge \pi^{\prime}$ if and only if $v_{\pi}(s) \ge v_{\pi^{\prime}}(s)$ for all $s \in \mathcal{S}$. There is always at least one policy that is better than or equal to all other policies. This is an <em>optimal policy</em>. Although there may be more than one, we denote all the optimal policies by $\pi_{\star}$. They share the same state-value function, called the <em>optimal state-value function</em>, denote $v_{\star}$, and defined as<br>$$<br>v_{\star}(s) \doteq \max_{\pi} v_{\pi}(s),<br>$$<br>for all $s \in \mathcal{S}$.</p><p>Optimal policies also share the same <em>optimal action-value function</em>, denoted $q_{\star}$, and defined as<br>$$<br>q_{\star}(s, a) \doteq \max_{\pi} q_{\pi}(s, a)<br>$$<br>for all $s \in \mathcal{S}$ and $a \in \mathcal{A(s)}$. For the state-action pair <em>(s, a)</em>, this function gives the excepted return for taking action <em>a</em> in state <em>s</em> and thereafter following an optimal policy. Thus, we can write $q_{\star}$ in terms of $v_{\star}$ as follows:<br>$$<br>q_{\star}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\star} \ | \ S_{t}=s, A_{t}=a]<br>$$<br>Suppose we solve the Bellman equation for $v_{\star}$ for the simple grid task introduced in earlier and shown again in Figure 2 (left). Recall that state A is followed by a reward of +10 and transition to state A’. while state B is followed by a reward of +5 and transition to state B’. Figure 2 (middle) shows the optimal value function, and Figure 2 (right) shows the corresponding optimal policies. Where there are multiple arrows in a cell, any of the corresponding actions are optimal.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/grid_world/optimal_value.png" alt="optimal_value"></p><p><em>Figure 2</em></p><p>Now, let us solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">world = np.zeros((WORLD_SIZE, WORLD_SIZE))</div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    <span class="comment"># keep iteration until convergence</span></div><div class="line">    newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, WORLD_SIZE):</div><div class="line">            values = []</div><div class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> actions:</div><div class="line">                newPosition = nextState[i][j][action]</div><div class="line">                <span class="comment"># value iteration</span></div><div class="line">                values.append(actionReward[i][j][action] + discount * world[newPosition[<span class="number">0</span>], newPosition[<span class="number">1</span>]])</div><div class="line">            newWorld[i][j] = np.max(values)</div><div class="line">    <span class="keyword">if</span> np.sum(np.abs(world - newWorld)) &lt; <span class="number">1e-4</span>:</div><div class="line">        print(<span class="string">'Optimal Policy'</span>)</div><div class="line">        print(newWorld)</div><div class="line">        <span class="keyword">break</span></div><div class="line">    world = newWorld</div></pre></td></tr></table></figure><p>We can see the core code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">newWorld[i][j] = np.max(values)</div></pre></td></tr></table></figure><p>The only difference between this code and the earlier code is the prior only uses the maximum value and the latter uses the weighted average.</p><p>The result is below:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Optimal Policy</div><div class="line">[[ <span class="number">21.97744338</span>  <span class="number">24.41938153</span>  <span class="number">21.97744338</span>  <span class="number">19.41938153</span>  <span class="number">17.47744338</span>]</div><div class="line"> [ <span class="number">19.77969904</span>  <span class="number">21.97744338</span>  <span class="number">19.77969904</span>  <span class="number">17.80172914</span>  <span class="number">16.02153504</span>]</div><div class="line"> [ <span class="number">17.80172914</span>  <span class="number">19.77969904</span>  <span class="number">17.80172914</span>  <span class="number">16.02153504</span>  <span class="number">14.41938153</span>]</div><div class="line"> [ <span class="number">16.02153504</span>  <span class="number">17.80172914</span>  <span class="number">16.02153504</span>  <span class="number">14.41938153</span>  <span class="number">12.97744338</span>]</div><div class="line"> [ <span class="number">14.41938153</span>  <span class="number">16.02153504</span>  <span class="number">14.41938153</span>  <span class="number">12.97744338</span>  <span class="number">11.67969904</span>]]</div></pre></td></tr></table></figure><p>It is not doubt that the result is the same as the Figure 2 (middle).</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/25/How-to-generate-a-unique-ID-in-a-distribute-system/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/25/How-to-generate-a-unique-ID-in-a-distribute-system/" itemprop="url">How to generate a unique ID in a distribute system</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-25T13:03:40+08:00">2017-05-25 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/25/How-to-generate-a-unique-ID-in-a-distribute-system/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/25/How-to-generate-a-unique-ID-in-a-distribute-system/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/25/How-to-generate-a-unique-ID-in-a-distribute-system/" class="leancloud_visitors" data-flag-title="How to generate a unique ID in a distribute system"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>现今所有的企业级应用都需要处理海量的数据对象，这些对象都需要一个唯一的ID与其他的对象区分开来。在关系型数据库中，我们一般是创建主键来达到这个目的。一些数据库支持内建的列类型（AUTO_INCREMENT/AUTO_NUMBER）来产生一个单调递增的64位长的数。有些人喜欢在他们的应用层中生成id，以便获得对这代人的更多控制，然后使用数据层保存记录。但是，第二种方法通过缓存最新生成的数字，并且通过某种持久性技术保存已经生成的id的轨迹来避免主键冲突。</p><p>上述两种方法本身都有各自的优点和缺点，但它们都有一个共同的缺点，即在分布式架构的情况下，这些都不具有弹性。那么需要考虑数据分片在多个数据库节点之间时，第一个技术如何确保不同节点中的表不会产生相同的auto_increment数或想象一个拓扑，在多个节点上运行应用程序，那么第二种技术如何满足所有节点的需求。</p><p>没有一种方法可以满足所有的需求，下面是在许多大型应用程序中使用的最流行的方法。</p><h2 id="1-数据库自增长序列或字段"><a href="#1-数据库自增长序列或字段" class="headerlink" title="1. 数据库自增长序列或字段"></a><strong>1. 数据库自增长序列或字段</strong></h2><p>最常见的方式。利用数据库，全数据库唯一。</p><p>优点：</p><p>1）简单，代码方便，性能可以接受。</p><p>2）数字ID天然排序，对分页或者需要排序的结果很有帮助。</p><p>缺点：</p><p>1）不同数据库语法和实现不同，数据库迁移的时候或多数据库版本支持的时候需要处理。</p><p>2）在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险。</p><p>3）在性能达不到要求的情况下，比较难于扩展。</p><p>4）如果遇见多个系统需要合并或者涉及到数据迁移会相当痛苦。</p><p>5）分表分库的时候会有麻烦。</p><p>优化方案：</p><p>1）针对主库单点，如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。比如：Master1 生成的是 1，4，7，10，Master2生成的是2,5,8,11 Master3生成的是 3,6,9,12。这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。</p><h2 id="2-UUID"><a href="#2-UUID" class="headerlink" title="2. UUID"></a><strong>2. UUID</strong></h2><p>常见的方式。可以利用数据库也可以利用程序生成，一般来说全球唯一。</p><p>优点：</p><p>1）简单，代码方便。</p><p>2）生成ID性能非常好，基本不会有性能问题。</p><p>3）全球唯一，在遇见数据迁移，系统数据合并，或者数据库变更等情况下，可以从容应对。</p><p>缺点：</p><p>1）没有排序，无法保证趋势递增。</p><p>2）UUID往往是使用字符串存储，查询的效率比较低。</p><p>3）存储空间比较大，如果是海量数据库，就需要考虑存储量的问题。</p><p>4）传输数据量大</p><p>5）不可读。</p><h2 id="3-UUID的变种"><a href="#3-UUID的变种" class="headerlink" title="3. UUID的变种"></a><strong>3. UUID的变种</strong></h2><p>1）为了解决UUID不可读，可以使用UUID to Int64的方法。及</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// &lt;summary&gt;</span></div><div class="line"><span class="comment">/// 根据GUID获取唯一数字序列</span></div><div class="line"><span class="comment">/// &lt;/summary&gt;</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">GuidToInt64</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">byte</span>[] bytes = Guid.NewGuid().ToByteArray();</div><div class="line">    <span class="keyword">return</span> BitConverter.ToInt64(bytes, <span class="number">0</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>2）为了解决UUID无序的问题，NHibernate在其主键生成方式中提供了Comb算法（combined guid/timestamp）。保留GUID的10个字节，用另6个字节表示GUID生成的时间（DateTime）。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// &lt;summary&gt; </span></div><div class="line"><span class="comment">/// Generate a new &lt;see cref="Guid"/&gt; using the comb algorithm. </span></div><div class="line"><span class="comment">/// &lt;/summary&gt; </span></div><div class="line"><span class="function"><span class="keyword">private</span> Guid <span class="title">GenerateComb</span><span class="params">()</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">byte</span>[] guidArray = Guid.NewGuid().ToByteArray();</div><div class="line"> </div><div class="line">    DateTime baseDate = <span class="keyword">new</span> DateTime(<span class="number">1900</span>, <span class="number">1</span>, <span class="number">1</span>);</div><div class="line">    DateTime now = DateTime.Now;</div><div class="line"> </div><div class="line">    <span class="comment">// Get the days and milliseconds which will be used to build    </span></div><div class="line">    <span class="comment">//the byte string    </span></div><div class="line">    TimeSpan days = <span class="keyword">new</span> TimeSpan(now.Ticks - baseDate.Ticks);</div><div class="line">    TimeSpan msecs = now.TimeOfDay;</div><div class="line"> </div><div class="line">    <span class="comment">// Convert to a byte array        </span></div><div class="line">    <span class="comment">// Note that SQL Server is accurate to 1/300th of a    </span></div><div class="line">    <span class="comment">// millisecond so we divide by 3.333333    </span></div><div class="line">    <span class="keyword">byte</span>[] daysArray = BitConverter.GetBytes(days.Days);</div><div class="line">    <span class="keyword">byte</span>[] msecsArray = BitConverter.GetBytes((<span class="keyword">long</span>)</div><div class="line">      (msecs.TotalMilliseconds / <span class="number">3.333333</span>));</div><div class="line"> </div><div class="line">    <span class="comment">// Reverse the bytes to match SQL Servers ordering    </span></div><div class="line">    Array.Reverse(daysArray);</div><div class="line">    Array.Reverse(msecsArray);</div><div class="line"> </div><div class="line">    <span class="comment">// Copy the bytes into the guid    </span></div><div class="line">    Array.Copy(daysArray, daysArray.Length - <span class="number">2</span>, guidArray,</div><div class="line">      guidArray.Length - <span class="number">6</span>, <span class="number">2</span>);</div><div class="line">    Array.Copy(msecsArray, msecsArray.Length - <span class="number">4</span>, guidArray,</div><div class="line">      guidArray.Length - <span class="number">4</span>, <span class="number">4</span>);</div><div class="line"> </div><div class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Guid(guidArray);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>用上面的算法测试一下，得到如下的结果：作为比较，前面3个是使用COMB算法得出的结果，最后12个字符串是时间序（统一毫秒生成的3个UUID），过段时间如果再次生成，则12个字符串会比图示的要大。后面3个是直接生成的GUID。</p><p><a href="http://images2015.cnblogs.com/blog/15700/201602/15700-20160227213048174-1443183768.png" target="_blank" rel="external"><img src="http://images2015.cnblogs.com/blog/15700/201602/15700-20160227213048721-177386520.png" alt="ODX}_`4N5X$F93OAS~`8Z)C"></a></p><p>如果想把时间序放在前面，可以生成后改变12个字符串的位置，也可以修改算法类的最后两个Array.Copy。</p><h2 id="4-Redis生成ID"><a href="#4-Redis生成ID" class="headerlink" title="4. Redis生成ID"></a><strong>4. Redis生成ID</strong></h2><p>当使用数据库来生成ID性能不够要求的时候，我们可以尝试使用Redis来生成ID。这主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。</p><p>可以使用Redis集群来获取更高的吞吐量。假如一个集群中有5台Redis。可以初始化每台Redis的值分别是1,2,3,4,5，然后步长都是5。各个Redis生成的ID为：</p><p>A：1,6,11,16,21</p><p>B：2,7,12,17,22</p><p>C：3,8,13,18,23</p><p>D：4,9,14,19,24</p><p>E：5,10,15,20,25</p><p>这个，随便负载到哪个机确定好，未来很难做修改。但是3-5台服务器基本能够满足器上，都可以获得不同的ID。但是步长和初始值一定需要事先需要了。使用Redis集群也可以方式单点故障的问题。</p><p>另外，比较适合使用Redis来生成每天从0开始的流水号。比如订单号=日期+当日自增长号。可以每天在Redis中生成一个Key，使用INCR进行累加。</p><p>优点：</p><p>1）不依赖于数据库，灵活方便，且性能优于数据库。</p><p>2）数字ID天然排序，对分页或者需要排序的结果很有帮助。</p><p>缺点：</p><p>1）如果系统中没有Redis，还需要引入新的组件，增加系统复杂度。</p><p>2）需要编码和配置的工作量比较大。</p><h2 id="5-Twitter的snowflake算法"><a href="#5-Twitter的snowflake算法" class="headerlink" title="5. Twitter的snowflake算法"></a><strong>5. Twitter的snowflake算法</strong></h2><p>snowflake是Twitter开源的分布式ID生成算法，结果是一个long型的ID。其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。</p><p>C#代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// &lt;summary&gt;</span></div><div class="line">    <span class="comment">/// From: https://github.com/twitter/snowflake</span></div><div class="line">    <span class="comment">/// An object that generates IDs.</span></div><div class="line">    <span class="comment">/// This is broken into a separate class in case</span></div><div class="line">    <span class="comment">/// we ever want to support multiple worker threads</span></div><div class="line">    <span class="comment">/// per process</span></div><div class="line">    <span class="comment">/// &lt;/summary&gt;</span></div><div class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IdWorker</span></span></div><div class="line">    &#123;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">long</span> workerId;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">long</span> datacenterId;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">long</span> sequence = <span class="number">0L</span>;</div><div class="line"></div><div class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> twepoch = <span class="number">1288834974657L</span>;</div><div class="line"></div><div class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> workerIdBits = <span class="number">5L</span>;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> datacenterIdBits = <span class="number">5L</span>;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> maxWorkerId = -<span class="number">1L</span> ^ (-<span class="number">1L</span> &lt;&lt; (<span class="keyword">int</span>)workerIdBits);</div><div class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> maxDatacenterId = -<span class="number">1L</span> ^ (-<span class="number">1L</span> &lt;&lt; (<span class="keyword">int</span>)datacenterIdBits);</div><div class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> sequenceBits = <span class="number">12L</span>;</div><div class="line"></div><div class="line">        <span class="keyword">private</span> <span class="keyword">long</span> workerIdShift = sequenceBits;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">long</span> datacenterIdShift = sequenceBits + workerIdBits;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">long</span> timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">long</span> sequenceMask = -<span class="number">1L</span> ^ (-<span class="number">1L</span> &lt;&lt; (<span class="keyword">int</span>)sequenceBits);</div><div class="line"></div><div class="line">        <span class="keyword">private</span> <span class="keyword">long</span> lastTimestamp = -<span class="number">1L</span>;</div><div class="line">        <span class="keyword">private</span> <span class="keyword">static</span> object syncRoot = <span class="keyword">new</span> object();</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="title">IdWorker</span><span class="params">(<span class="keyword">long</span> workerId, <span class="keyword">long</span> datacenterId)</span></span></div><div class="line">        &#123;</div><div class="line"></div><div class="line">            <span class="comment">// sanity check for workerId</span></div><div class="line">            <span class="keyword">if</span> (workerId &gt; maxWorkerId || workerId &lt; <span class="number">0</span>)</div><div class="line">            &#123;</div><div class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> ArgumentException(string.Format(<span class="string">"worker Id can't be greater than %d or less than 0"</span>, maxWorkerId));</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">if</span> (datacenterId &gt; maxDatacenterId || datacenterId &lt; <span class="number">0</span>)</div><div class="line">            &#123;</div><div class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> ArgumentException(string.Format(<span class="string">"datacenter Id can't be greater than %d or less than 0"</span>, maxDatacenterId));</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">this</span>.workerId = workerId;</div><div class="line">            <span class="keyword">this</span>.datacenterId = datacenterId;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">nextId</span><span class="params">()</span></span></div><div class="line">        &#123;</div><div class="line">            lock (syncRoot)</div><div class="line">            &#123;</div><div class="line">                <span class="keyword">long</span> timestamp = timeGen();</div><div class="line"></div><div class="line">                <span class="keyword">if</span> (timestamp &lt; lastTimestamp)</div><div class="line">                &#123;</div><div class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> ApplicationException(string.Format(<span class="string">"Clock moved backwards.  Refusing to generate id for %d milliseconds"</span>, lastTimestamp - timestamp));</div><div class="line">                &#125;</div><div class="line"></div><div class="line">                <span class="keyword">if</span> (lastTimestamp == timestamp)</div><div class="line">                &#123;</div><div class="line">                    sequence = (sequence + <span class="number">1</span>) &amp; sequenceMask;</div><div class="line">                    <span class="keyword">if</span> (sequence == <span class="number">0</span>)</div><div class="line">                    &#123;</div><div class="line">                        timestamp = tilNextMillis(lastTimestamp);</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">                <span class="keyword">else</span></div><div class="line">                &#123;</div><div class="line">                    sequence = <span class="number">0L</span>;</div><div class="line">                &#125;</div><div class="line"></div><div class="line">                lastTimestamp = timestamp;</div><div class="line"></div><div class="line">                <span class="keyword">return</span> ((timestamp - twepoch) &lt;&lt; (<span class="keyword">int</span>)timestampLeftShift) | (datacenterId &lt;&lt; (<span class="keyword">int</span>)datacenterIdShift) | (workerId &lt;&lt; (<span class="keyword">int</span>)workerIdShift) | sequence;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">long</span> <span class="title">tilNextMillis</span><span class="params">(<span class="keyword">long</span> lastTimestamp)</span></span></div><div class="line">        &#123;</div><div class="line">            <span class="keyword">long</span> timestamp = timeGen();</div><div class="line">            <span class="keyword">while</span> (timestamp &lt;= lastTimestamp)</div><div class="line">            &#123;</div><div class="line">                timestamp = timeGen();</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">return</span> timestamp;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">long</span> <span class="title">timeGen</span><span class="params">()</span></span></div><div class="line">        &#123;</div><div class="line">            <span class="keyword">return</span> (<span class="keyword">long</span>)(DateTime.UtcNow - <span class="keyword">new</span> DateTime(<span class="number">1970</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, DateTimeKind.Utc)).TotalMilliseconds;</div><div class="line">        &#125;</div><div class="line">    &#125;</div></pre></td></tr></table></figure><p>测试代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">TestIdWorker</span><span class="params">()</span></span></div><div class="line">        &#123;</div><div class="line">            HashSet&lt;<span class="keyword">long</span>&gt; set = <span class="keyword">new</span> HashSet&lt;<span class="keyword">long</span>&gt;();</div><div class="line">            IdWorker idWorker1 = <span class="keyword">new</span> IdWorker(<span class="number">0</span>, <span class="number">0</span>);</div><div class="line">            IdWorker idWorker2 = <span class="keyword">new</span> IdWorker(<span class="number">1</span>, <span class="number">0</span>);</div><div class="line">            Thread t1 = <span class="keyword">new</span> Thread(() =&gt; DoTestIdWoker(idWorker1, set));</div><div class="line">            Thread t2 = <span class="keyword">new</span> Thread(() =&gt; DoTestIdWoker(idWorker2, set));</div><div class="line">            t1.IsBackground = <span class="keyword">true</span>;</div><div class="line">            t2.IsBackground = <span class="keyword">true</span>;</div><div class="line"></div><div class="line">            t1.Start();</div><div class="line">            t2.Start();</div><div class="line">            <span class="keyword">try</span></div><div class="line">            &#123;</div><div class="line">                Thread.Sleep(<span class="number">30000</span>);</div><div class="line">                t1.Abort();</div><div class="line">                t2.Abort();</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">catch</span> (Exception e)</div><div class="line">            &#123;</div><div class="line">            &#125;</div><div class="line"></div><div class="line">            Console.WriteLine(<span class="string">"done"</span>);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">DoTestIdWoker</span><span class="params">(IdWorker idWorker, HashSet&lt;<span class="keyword">long</span>&gt; set)</span></span></div><div class="line">        &#123;</div><div class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>)</div><div class="line">            &#123;</div><div class="line">                <span class="keyword">long</span> id = idWorker.nextId();</div><div class="line">                <span class="keyword">if</span> (!set.Add(id))</div><div class="line">                &#123;</div><div class="line">                    Console.WriteLine(<span class="string">"duplicate:"</span> + id);</div><div class="line">                &#125;</div><div class="line"></div><div class="line">                Thread.Sleep(<span class="number">1</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;</div></pre></td></tr></table></figure><p>snowflake算法可以根据自身项目的需要进行一定的修改。比如估算未来的数据中心个数，每个数据中心的机器数以及统一毫秒可以能的并发数来调整在算法中所需要的bit数。</p><p>优点：</p><p>1）不依赖于数据库，灵活方便，且性能优于数据库。</p><p>2）ID按照时间在单机上是递增的。</p><p>缺点：</p><p>1）在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况。</p><p>Snowflake 的其他变种</p><p>Snowflake 有一些变种, 各个应用结合自己的实际场景对 Snowflake 做了一些改动. 这里主要介绍 3 种.</p><h3 id="5-1-Boundary-flake"><a href="#5-1-Boundary-flake" class="headerlink" title="5.1 Boundary flake"></a><strong>5.1 Boundary flake</strong></h3><p>变化:</p><ul><li>ID 长度扩展到 128 bits:</li><li>最高 64 bits 时间戳;</li><li>然后是 48 bits 的 Worker 号 (和 Mac 地址一样长);</li><li>最后是 16 bits 的 Seq Number</li><li>由于它用 48 bits 作为 Worker ID, 和 Mac 地址的长度一样, 这样启动时不需要和 Zookeeper 通讯获取 Worker ID. 做到了完全的去中心化</li><li>基于 Erlang</li></ul><p>它这样做的目的是用更多的 bits 实现更小的冲突概率, 这样就支持更多的 Worker 同时工作. 同时, 每毫秒能分配出更多的 ID</p><h3 id="5-2-Simpleflake"><a href="#5-2-Simpleflake" class="headerlink" title="5.2 Simpleflake"></a><strong>5.2 Simpleflake</strong></h3><p>Simpleflake 的思路是取消 Worker 号, 保留 41 bits 的 Timestamp, 同时把 sequence number 扩展到 22 bits;</p><p>Simpleflake 的特点:</p><ul><li>sequence number 完全靠随机产生 (这样也导致了生成的 ID 可能出现重复)</li><li>没有 Worker 号, 也就不需要和 Zookeeper 通讯, 实现了完全去中心化</li><li>Timestamp 保持和 Snowflake 一致, 今后可以无缝升级到 Snowflake</li></ul><p>Simpleflake 的问题就是 sequence number 完全随机生成, 会导致生成的 ID 重复的可能. 这个生成 ID 重复的概率随着每秒生成的 ID 数的增长而增长.</p><p>所以, Simpleflake 的限制就是每秒生成的 ID 不能太多 (最好小于 100次/秒, 如果大于 100次/秒的场景, Simpleflake 就不适用了, 建议切换回 Snowflake).</p><h3 id="5-3-instagram-的做法"><a href="#5-3-instagram-的做法" class="headerlink" title="5.3  instagram 的做法"></a><strong>5.3 instagram 的做法</strong></h3><p>先简单介绍一下 instagram 的分布式存储方案:</p><ul><li>先把每个 Table 划分为多个逻辑分片 (logic Shard), 逻辑分片的数量可以很大, 例如 2000 个逻辑分片</li><li>然后制定一个规则, 规定每个逻辑分片被存储到哪个数据库实例上面; 数据库实例不需要很多. 例如, 对有 2 个 PostgreSQL 实例的系统 (instagram 使用 PostgreSQL); 可以使用奇数逻辑分片存放到第一个数据库实例, 偶数逻辑分片存放到第二个数据库实例的规则</li><li>每个 Table 指定一个字段作为分片字段 (例如, 对用户表, 可以指定 uid 作为分片字段)</li><li>插入一个新的数据时, 先根据分片字段的值, 决定数据被分配到哪个逻辑分片 (logic Shard)</li><li>然后再根据 logic Shard 和 PostgreSQL 实例的对应关系, 确定这条数据应该被存放到哪台 PostgreSQL 实例上</li></ul><p>instagram unique ID 的组成:</p><ul><li>41 bits: Timestamp (毫秒)</li><li>13 bits: 每个 logic Shard 的代号 (最大支持 8 x 1024 个 logic Shards)</li><li>10 bits: sequence number; 每个 Shard 每毫秒最多可以生成 1024 个 ID</li></ul><p>生成 unique ID 时, 41 bits 的 Timestamp 和 Snowflake 类似, 这里就不细说了.</p><p>主要介绍一下 13 bits 的 logic Shard 代号 和 10 bits 的 sequence number 怎么生成.</p><p>logic Shard 代号:</p><ul><li>假设插入一条新的用户记录, 插入时, 根据 uid 来判断这条记录应该被插入到哪个 logic Shard 中.</li><li>假设当前要插入的记录会被插入到第 1341 号 logic Shard 中 (假设当前的这个 Table 一共有 2000 个 logic Shard)</li><li>新生成 ID 的 13 bits 段要填的就是 1341 这个数字</li></ul><p>sequence number 利用 PostgreSQL 每个 Table 上的 auto-increment sequence 来生成:</p><ul><li>如果当前表上已经有 5000 条记录, 那么这个表的下一个 auto-increment sequence 就是 5001 (直接调用 PL/PGSQL 提供的方法可以获取到)</li><li>然后把 这个 5001 对 1024 取模就得到了 10 bits 的 sequence number</li></ul><p>instagram 这个方案的优势在于:</p><ul><li>利用 logic Shard 号来替换 Snowflake 使用的 Worker 号, 就不需要到中心节点获取 Worker 号了. 做到了完全去中心化</li><li>另外一个附带的好处就是, 可以通过 ID 直接知道这条记录被存放在哪个 logic Shard 上</li></ul><p>同时, 今后做数据迁移的时候, 也是按 logic Shard 为单位做数据迁移的, 所以这种做法也不会影响到今后的数据迁移</p><h2 id="6-利用zookeeper生成唯一ID"><a href="#6-利用zookeeper生成唯一ID" class="headerlink" title="6. 利用zookeeper生成唯一ID"></a><strong>6. 利用zookeeper生成唯一ID</strong></h2><p>zookeeper主要通过其znode数据版本来生成序列号，可以生成32位和64位的数据版本号，客户端可以使用这个版本号来作为唯一的序列号。很少会使用zookeeper来生成唯一ID。主要是由于需要依赖zookeeper，并且是多步调用API，如果在竞争较大的情况下，需要考虑使用分布式锁。因此，性能在高并发的分布式环境下，也不甚理想。</p><h2 id="7-MongoDB的ObjectId"><a href="#7-MongoDB的ObjectId" class="headerlink" title="7. MongoDB的ObjectId"></a><strong>7. MongoDB的ObjectId</strong></h2><p>MongoDB的ObjectId和snowflake算法类似。它设计成轻量型的，不同的机器都能用全局唯一的同种方法方便地生成它。MongoDB 从一开始就设计用来作为分布式数据库，处理多个节点是一个核心要求。使其在分片环境中要容易生成得多。</p><p>其格式如下：</p><p><img src="http://images.blogjava.net/blogjava_net/dongbule/46046/o_111.PNG?_=5208136" alt="mogonDB"></p><p>前4 个字节是从标准纪元开始的时间戳，单位为秒。时间戳，与随后的5 个字节组合起来，提供了秒级别的唯一性。由于时间戳在前，这意味着ObjectId 大致会按照插入的顺序排列。这对于某些方面很有用，如将其作为索引提高效率。这4 个字节也隐含了文档创建的时间。绝大多数客户端类库都会公开一个方法从ObjectId 获取这个信息。<br>接下来的3 字节是所在主机的唯一标识符。通常是机器主机名的散列值。这样就可以确保不同主机生成不同的ObjectId，不产生冲突。<br>为了确保在同一台机器上并发的多个进程产生的ObjectId 是唯一的，接下来的两字节来自产生ObjectId 的进程标识符（PID）。<br>前9 字节保证了同一秒钟不同机器不同进程产生的ObjectId 是唯一的。后3 字节就是一个自动增加的计数器，确保相同进程同一秒产生的ObjectId 也是不一样的。同一秒钟最多允许每个进程拥有2563（16 777 216）个不同的ObjectId。</p><h2 id="8-Flickr-的全局主键生成方案"><a href="#8-Flickr-的全局主键生成方案" class="headerlink" title="8. Flickr 的全局主键生成方案"></a><strong>8. Flickr 的全局主键生成方案</strong></h2><p><a href="http://code.flickr.net/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-cheap/" target="_blank" rel="external">flickr</a>巧妙地使用了MySQL的自增ID，及replace into语法，十分简洁地实现了分片ID生成功能。</p><p>比如创建64位的自增id：<br>首先，创建一个表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`uid_sequence`</span> (</div><div class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">20</span>) <span class="keyword">unsigned</span> <span class="keyword">NOT</span> <span class="literal">NULL</span> auto_increment,</div><div class="line">  <span class="string">`stub`</span> <span class="built_in">char</span>(<span class="number">1</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> <span class="keyword">default</span> <span class="string">''</span>,</div><div class="line">  PRIMARY <span class="keyword">KEY</span>  (<span class="string">`id`</span>),</div><div class="line">  <span class="keyword">UNIQUE</span> <span class="keyword">KEY</span> <span class="string">`stub`</span> (<span class="string">`stub`</span>)</div><div class="line">) <span class="keyword">ENGINE</span>=MyISAM;123456123456</div></pre></td></tr></table></figure><p>SELECT * from uid_sequence 输出：<br>+——————-+——+<br>| id | stub |<br>+——————-+——+<br>| 72157623227190423 | a |</p><p>如果我需要一个全局的唯一的64位uid，则执行：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">REPLACE</span> <span class="keyword">INTO</span> uid_sequence (stub) <span class="keyword">VALUES</span> (<span class="string">'a'</span>);</div><div class="line"><span class="keyword">SELECT</span> <span class="keyword">LAST_INSERT_ID</span>();1212</div></pre></td></tr></table></figure><p>说明：</p><ul><li>用 REPLACE INTO 代替 INSERT INTO 的好处是避免表行数太大，还要另外定期清理。</li><li>stub 字段要设为唯一索引，这个 sequence 表只有一条纪录，但也可以同时为多张表生成全局主键，例如user_order_id。除非你需要表的主键是连续的，那么就另建一个 user_order_id_sequence 表。</li><li>经过实际对比测试，使用 MyISAM 比 Innodb 有更高的性能。</li></ul><p>这里flickr使用两台数据库（也可以更多）作为自增序列生成，通过这两台机器做主备和负载均衡。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">TicketServer1:</div><div class="line">auto-increment-increment = 2</div><div class="line">auto-increment-offset = 1</div><div class="line"></div><div class="line">TicketServer2:</div><div class="line">auto-increment-increment = 2</div><div class="line">auto-increment-offset = 212345671234567</div></pre></td></tr></table></figure><p>优点：</p><ul><li>简单可靠。</li></ul><p>缺点：</p><ul><li>id只是一个ID，没有带入时间，shardingId等信息。</li></ul></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/24/Reproduce-DQN-result/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/24/Reproduce-DQN-result/" itemprop="url">Reproduce DQN result</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-24T12:56:51+08:00">2017-05-24 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/24/Reproduce-DQN-result/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/24/Reproduce-DQN-result/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/24/Reproduce-DQN-result/" class="leancloud_visitors" data-flag-title="Reproduce DQN result"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>论文链接：<a href="https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html" target="_blank" rel="external">https://www.nature.com/nature/journal/v518/n7540/full/nature14236.html</a></p><p>源代码地址：<a href="https://sites.google.com/a/deepmind.com/dqn/" target="_blank" rel="external">https://sites.google.com/a/deepmind.com/dqn/</a></p><p>由于源代码中只有训练阶段，没有测试阶段，因此我才用了这个<a href="https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner" target="_blank" rel="external">项目</a>的测试脚本，并且生成游戏动图。</p><p>实验复现的步骤如下（这里引用作者原文）：</p><h2 id="DQN-3-0"><a href="#DQN-3-0" class="headerlink" title="DQN 3.0"></a>DQN 3.0</h2><p>This project contains the source code of DQN 3.0, a Lua-based deep reinforcement learning architecture, necessary to reproduce the experiments described in the paper “Human-level control through deep reinforcement learning”, Nature 518, 529–533 (26 February 2015) doi:10.1038/nature14236.</p><p>To replicate the experiment results, a number of dependencies need to be installed, namely:</p><ul><li>LuaJIT and Torch 7.0</li><li>nngraph</li><li>Xitari (fork of the Arcade Learning Environment (Bellemare et al., 2013))</li><li>AleWrap (a lua interface to Xitari) An install script for these dependencies is provided.</li></ul><p>Two run scripts are provided: run_cpu and run_gpu. As the names imply, the former trains the DQN network using regular CPUs, while the latter uses GPUs (CUDA), which typically results in a significant speed-up.</p><h2 id="Installation-instructions"><a href="#Installation-instructions" class="headerlink" title="Installation instructions"></a>Installation instructions</h2><p>The installation requires Linux with apt-get.</p><p>Note: In order to run the GPU version of DQN, you should additionally have the NVIDIA® CUDA® (version 5.5 or later) toolkit installed prior to the Torch installation below. This can be downloaded from <a href="https://developer.nvidia.com/cuda-toolkit" target="_blank" rel="external">https://developer.nvidia.com/cuda-toolkit</a> and installation instructions can be found in <a href="http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux" target="_blank" rel="external">http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux</a></p><p>To train DQN on Atari games, the following components must be installed:</p><ul><li>LuaJIT and Torch 7.0</li><li>nngraph</li><li>Xitari</li><li>AleWrap</li></ul><p>To install all of the above in a subdirectory called ‘torch’, it should be enough to run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./install_dependencies.sh</div></pre></td></tr></table></figure><p>from the base directory of the package.</p><p>Note: The above install script will install the following packages via apt-get: build-essential, gcc, g++, cmake, curl, libreadline-dev, git-core, libjpeg-dev, libpng-dev, ncurses-dev, imagemagick, unzip</p><h2 id="Training-DQN-on-Atari-games"><a href="#Training-DQN-on-Atari-games" class="headerlink" title="Training DQN on Atari games"></a>Training DQN on Atari games</h2><p>Prior to running DQN on a game, you should copy its ROM in the ‘roms’ subdirectory. It should then be sufficient to run the script</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./run_cpu &lt;game name&gt;</div></pre></td></tr></table></figure><p>Or, if GPU support is enabled,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./run_gpu &lt;game name&gt;</div></pre></td></tr></table></figure><p>Note: On a system with more than one GPU, DQN training can be launched on a specified GPU by setting the environment variable GPU_ID, e.g. by</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">GPU_ID=2 ./run_gpu &lt;game name&gt;</div></pre></td></tr></table></figure><p>If GPU_ID is not specified, the first available GPU (ID 0) will be used by default.</p><p>这之后是我采用另一个项目的测试步骤：</p><h2 id="Storing-a-gif-for-a-trained-network"><a href="#Storing-a-gif-for-a-trained-network" class="headerlink" title="Storing a .gif for a trained network"></a>Storing a .gif for a trained network</h2><p>Once you have a snapshot of a network you can run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./test_gpu &lt;game name&gt; &lt;snapshopt filename&gt;</div></pre></td></tr></table></figure><p>to make it play one game and store the .gif under <code>gifs</code>. For example</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./test_gpu breakout DQN3_0_1_breakout_FULL_Y.t7</div></pre></td></tr></table></figure><h2 id="Options"><a href="#Options" class="headerlink" title="Options"></a>Options</h2><p>Options to DQN are set within run_cpu (respectively, run_gpu). You may, for example, want to change the frequency at which information is output to stdout by setting ‘prog_freq’ to a different value.</p><p>我在实验过程中碰到了一系列的问题，实验环境为Windows 10 Vmware Workstation中运行的Ubuntu 16.04 LTS虚拟机。需要声明的是，我才用原始代码并没有运行成功，经分析应该是虚拟机的问题，但以下碰到的问题应当是具有一般性地，下一步准备采用测试项目代码运行。</p><h2 id="Some-Problem"><a href="#Some-Problem" class="headerlink" title="Some Problem"></a>Some Problem</h2><h3 id="run-cpu之后出现Segmentation-fault错误"><a href="#run-cpu之后出现Segmentation-fault错误" class="headerlink" title="./run_cpu之后出现Segmentation fault错误"></a>./run_cpu之后出现Segmentation fault错误</h3><ol><li>可能是因为其后参数名中出现大写字母</li><li>可能是因为内存不足，可以尝试换一个游戏运行</li></ol><h3 id="test-cpu之后提示找不到gd"><a href="#test-cpu之后提示找不到gd" class="headerlink" title="./test_cpu之后提示找不到gd"></a>./test_cpu之后提示找不到gd</h3><p>这个时候需要手动安装gd，具体安装方法如下：</p><p>下载地址：<a href="https://ittner.github.io/lua-gd/manual.html#download" target="_blank" rel="external">https://ittner.github.io/lua-gd/manual.html#download</a></p><p>我下载的是这个版本<a href="http://files.luaforge.net/releases/lua-gd/lua-gd/lua-gd-2.0.33r2forLua5.1/lua-gd-2.0.33r2.tar.gz" target="_blank" rel="external">http://files.luaforge.net/releases/lua-gd/lua-gd/lua-gd-2.0.33r2forLua5.1/lua-gd-2.0.33r2.tar.gz</a></p><p>下载解压后，进到对应的目录，</p><p>执行命令：</p><p><code>make</code></p><p>make成功后，执行：</p><p><code>sudo make install</code></p><p>如果中间出现错误的话，请把下面的几个包都安装上：</p><p><code>sudo apt-get install lua5.1</code><br><code>sudo apt-get install lua5.1-0-dev</code><br><code>sudo apt-get install liblua5.1-0-dev</code><br><code>sudo apt-get install libgd2-dev</code></p><p>安装成功之后会有如下提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">gcc -o gd.so `gdlib-config --features |sed -e &quot;s/GD_/-DGD_/g&quot;`</div><div class="line">`gdlib-config --cflags` `pkg-config lua5.1 --cflags` -O3 -Wall -shared</div><div class="line">`gdlib-config --ldflags` `gdlib-config --libs` `pkg-config lua5.1 --libs`</div><div class="line">-lgd luagd.c</div><div class="line">lua test_features.lua</div><div class="line">Lua-GD version: lua-gd 2.0.33r2</div><div class="line">Lua-GD features:</div><div class="line">    PNG support ..................... Enabled</div><div class="line">    GIF support ..................... Enabled</div><div class="line">    JPEG support .................... Enabled</div><div class="line">    XPM/XBM support ................. Enabled</div><div class="line">    FreeType support ................ Enabled</div><div class="line">    Fontconfig support .............. Enabled</div></pre></td></tr></table></figure><h3 id="安装gd时进行make的时候出现gd-h-No-such-file-or-directory"><a href="#安装gd时进行make的时候出现gd-h-No-such-file-or-directory" class="headerlink" title="安装gd时进行make的时候出现gd.h: No such file or directory"></a>安装gd时进行make的时候出现gd.h: No such file or directory</h3><p>Try to install this package if you are in debian : libgd2-noxpm-dev</p><h3 id="安装gd时进行make的时候出现srlua-makefile-error-lua-h-No-such-file-or-directory"><a href="#安装gd时进行make的时候出现srlua-makefile-error-lua-h-No-such-file-or-directory" class="headerlink" title="安装gd时进行make的时候出现srlua makefile error lua.h No such file or directory"></a>安装gd时进行make的时候出现srlua makefile error lua.h No such file or directory</h3><p><code>sudo apt-get install liblua5.1-0-dev</code></p><h3 id="在解决以上问题后依然通不过编译"><a href="#在解决以上问题后依然通不过编译" class="headerlink" title="在解决以上问题后依然通不过编译"></a>在解决以上问题后依然通不过编译</h3><p>这里引用了<a href="https://groups.google.com/forum/#!topic/bamboo-cn/myYzVk5XLgc" target="_blank" rel="external">https://groups.google.com/forum/#!topic/bamboo-cn/myYzVk5XLgc</a></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/13/Question-Answering-A-Very-Brief-Introduction/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/13/Question-Answering-A-Very-Brief-Introduction/" itemprop="url">Question Answering: A Very Brief Introduction</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-13T13:07:06+08:00">2017-05-13 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/13/Question-Answering-A-Very-Brief-Introduction/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/13/Question-Answering-A-Very-Brief-Introduction/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/13/Question-Answering-A-Very-Brief-Introduction/" class="leancloud_visitors" data-flag-title="Question Answering: A Very Brief Introduction"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><ul><li><p>Find answers to (natual language) questions by machine</p></li><li><p>Types of questions</p><ul><li>Factoid</li><li>Definition</li><li>Yes-No</li><li>Opinion</li><li>Comparison</li></ul></li><li><p>Multiple Intelligences in Modern in QA Systems</p><ul><li><p>Knowledge-QA</p><p>结构化的，基于知识库，其实就是一个图，结点是实体，边是语义关系。关键是能够提取中问题中的实体以及实体之间的语义关系</p></li><li><p>Document-QA</p><p>非结构化的</p></li><li><p>Social-QA</p><p>类似Quora, Zhihu, Stackoverflow等</p></li></ul></li></ul></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/13/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><span class="page-number current">14</span><a class="page-number" href="/page/15/">15</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/15/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">130</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">64</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/tomaxent" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>