<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/15/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/15/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/30/The-basic-of-Hidden-Markov-Model/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/30/The-basic-of-Hidden-Markov-Model/" itemprop="url">The Basic of Hidden Markov Model</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-30T23:04:54+08:00">2017-04-30 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/30/The-basic-of-Hidden-Markov-Model/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/30/The-basic-of-Hidden-Markov-Model/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/30/The-basic-of-Hidden-Markov-Model/" class="leancloud_visitors" data-flag-title="The Basic of Hidden Markov Model"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><div class="row"><embed src="http://o7ie0tcjk.bkt.clouddn.com/hmm-paper/tutorial%20on%20hmm%20and%20applications.pdf" width="100%" height="550" type="application/pdf"></div></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/29/The-awesome-Wasserstein-GAN/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/29/The-awesome-Wasserstein-GAN/" itemprop="url">The awesome Wasserstein GAN</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-29T16:55:06+08:00">2017-04-29 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/29/The-awesome-Wasserstein-GAN/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/29/The-awesome-Wasserstein-GAN/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/29/The-awesome-Wasserstein-GAN/" class="leancloud_visitors" data-flag-title="The awesome Wasserstein GAN"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>原帖地址：<a href="https://zhuanlan.zhihu.com/p/25071913">https://zhuanlan.zhihu.com/p/25071913</a></p><blockquote><p>本文后续：<a href="https://www.zhihu.com/question/52602529/answer/158727900">Wasserstein GAN最新进展：从weight clipping到gradient penalty，更加先进的Lipschitz限制手法</a></p></blockquote><p>在GAN的相关研究如火如荼甚至可以说是泛滥的今天，一篇新鲜出炉的arXiv论文《<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1701.07875">Wasserstein GAN</a>》却在Reddit的Machine Learning频道火了，连Goodfellow都<a href="http://link.zhihu.com/?target=https%3A//www.reddit.com/r/MachineLearning/comments/5qxoaz/r_170107875_wasserstein_gan/">在帖子里和大家热烈讨论</a>，这篇论文究竟有什么了不得的地方呢？</p><p>要知道自从<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1406.2661">2014年Ian Goodfellow提出</a>以来，GAN就存在着训练困难、生成器和判别器的loss无法指示训练进程、生成样本缺乏多样性等问题。从那时起，很多论文都在尝试解决，但是效果不尽人意，比如最有名的一个改进<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.06434">DCGAN</a>依靠的是对判别器和生成器的架构进行实验枚举，最终找到一组比较好的网络架构设置，但是实际上是治标不治本，没有彻底解决问题。而今天的主角Wasserstein GAN（下面简称WGAN）成功地做到了以下爆炸性的几点：</p><ul><li>彻底解决GAN训练不稳定的问题，不再需要小心平衡生成器和判别器的训练程度</li><li>基本解决了collapse mode的问题，确保了生成样本的多样性</li><li>训练过程中终于有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表GAN训练得越好，代表生成器产生的图像质量越高（如题图所示）</li><li>以上一切好处不需要精心设计的网络架构，最简单的多层全连接网络就可以做到</li></ul><p>那以上好处来自哪里？这就是令人拍案叫绝的部分了——实际上作者整整花了两篇论文，在第一篇《<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1701.04862">Towards Principled Methods for Training Generative Adversarial Networks</a>》里面推了一堆公式定理，从理论上分析了原始GAN的问题所在，从而针对性地给出了改进要点；在这第二篇《<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1701.07875">Wasserstein GAN</a>》里面，又再从这个改进点出发推了一堆公式定理，最终给出了改进的算法实现流程，<strong>而改进后相比原始GAN的算法实现流程却只改了四点</strong>：</p><ul><li>判别器最后一层去掉sigmoid</li><li>生成器和判别器的loss不取log</li><li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li><li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li></ul><p>算法截图如下：</p><p><img src="https://pic1.zhimg.com/v2-6be6e2ef3d15c4b10c2a943e9bf4db70_b.jpg" alt="img"></p><p>改动是如此简单，效果却惊人地好，以至于Reddit上不少人在感叹：就这样？没有别的了？ 太简单了吧！这些反应让我想起了一个颇有年头的鸡汤段子，说是一个工程师在电机外壳上用粉笔划了一条线排除了故障，要价一万美元——画一条线，1美元；知道在哪画线，9999美元。上面这四点改进就是作者Martin Arjovsky划的简简单单四条线，对于工程实现便已足够，但是知道在哪划线，背后却是精巧的数学分析，而这也是本文想要整理的内容。</p><p>本文内容分为五个部分：</p><ul><li>原始GAN究竟出了什么问题？（此部分较长）</li><li>WGAN之前的一个过渡解决方案</li><li>Wasserstein距离的优越性质</li><li>从Wasserstein距离到WGAN</li><li>总结</li></ul><p><em>理解原文的很多公式定理需要对测度论、 拓扑学等数学知识有所掌握，本文会从直观的角度对每一个重要公式进行解读，有时通过一些低维的例子帮助读者理解数学背后的思想，所以不免会失于严谨，如有引喻不当之处，欢迎在评论中指出。</em></p><p><em>以下简称《Wassertein GAN》为“WGAN本作”，简称《Towards Principled Methods for Training Generative Adversarial Networks》为“WGAN前作”。</em></p><p><em>WGAN源码实现：martinarjovsky/WassersteinGAN</em></p><h2 id="第一部分：原始GAN究竟出了什么问题？"><a href="#第一部分：原始GAN究竟出了什么问题？" class="headerlink" title="第一部分：原始GAN究竟出了什么问题？"></a>第一部分：原始GAN究竟出了什么问题？</h2><p>回顾一下，原始GAN中判别器要最小化如下损失函数，尽可能把真实样本分为正例，生成样本分为负例：</p><p>$$-\mathbb{E}_{x \sim P_r}[log D(x)] - \mathbb{E}_{x \sim P_{g}}[log(1-D(x))]$$</p><p>其中$P_r$是真实样本分布，$P_g$是由生成器产生的样本分布。对于生成器，Goodfellow一开始提出来一个损失函数，后来又提出了一个改进的损失函数，分别是</p><p>$$ \mathbb{E}_{x \sim P_g}[log(1-D(x))]$$</p><p>$$ \mathbb{E}_{x \sim P_g}[-log D(x)]$$</p><p>后者在WGAN两篇论文中称为“the - log D alternative”或“the - log D trick”。WGAN前作分别分析了这两种形式的原始GAN各自的问题所在，下面分别说明。</p><h2 id="第一种原始GAN形式的问题"><a href="#第一种原始GAN形式的问题" class="headerlink" title="第一种原始GAN形式的问题"></a>第一种原始GAN形式的问题</h2><p><strong>一句话概括：判别器越好，生成器梯度消失越严重。</strong>WGAN前作从两个角度进行了论证，第一个角度是从生成器的等价损失函数切入的。</p><p>首先从公式1可以得到，在生成器G固定参数时最优的判别器D应该是什么。对于一个具体的样本$x$，它可能来自真实分布也可能来自生成分布，它对公式1损失函数的贡献是</p><p>$$- P_r(x)logD(x) - p_g(x)log[1 - D(x)]$$</p><p>令其关于$D(x)$的导数为0，得<br>$$<br>-\frac{P_r(x)}{D(x)} + \frac{P_g(x)}{1 - D(x)} = 0<br>$$<br>化简得最优判别器为：<br>$$<br>D^{\star}(x) = \frac{P_r(x)}{P_r(x) + P_g(x)}<br>$$<br>这个结果从直观上很容易理解，就是看一个样本$x$来自真实分布和生成分布的可能性的相对比例。如果$P_r(x) = 0$且$P_g(x) \neq 0$，最优判别器就应该非常自信地给出概率0；如果$P_r(x) = P_g(x)$，说明该样本是真是假的可能性刚好一半一半，此时最优判别器也应该给出概率0.5。</p><p>然而GAN训练有一个trick，就是别把判别器训练得太好，否则在实验中生成器会完全学不动（loss降不下去），为了探究背后的原因，我们就可以看看在极端情况——判别器最优时，生成器的损失函数变成什么。给公式2加上一个不依赖于生成器的项，使之变成<br>$$<br>\mathbb{E}_{x \sim P_r}[log D(x)] - \mathbb{E}_{x \sim P_{g}}[log(1-D(x))]<br>$$</p><p>注意，最小化这个损失函数等价于最小化公式2，而且它刚好是判别器损失函数的反。代入最优判别器即公式4，再进行简单的变换可以得到</p><p>$$\mathbb{E}_{x \sim P_r} \log \frac{P_r(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} + \mathbb{E}_{x \sim P_g} \log \frac{P_g(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} - 2\log 2$$</p><p>变换成这个样子是为了引入Kullback–Leibler divergence（简称KL散度）和Jensen-Shannon divergence（简称JS散度）这两个重要的相似度衡量指标，后面的主角之一Wasserstein距离，就是要来吊打它们两个的。所以接下来介绍这两个重要的配角——KL散度和JS散度：</p><p>$$KL(P_1||P_2) = \mathbb{E}_{x \sim P_1} \log \frac{P_1}{P_2}$$</p><p>$$JS(P_1 || P_2) = \frac{1}{2}KL(P_1||\frac{P_1 + P_2}{2}) + \frac{1}{2}KL(P_2||\frac{P_1 + P_2}{2})$$</p><p>于是公式5就可以继续写成</p><p>$$2JS(P_r || P_g) - 2\log 2$$</p><p>到这里读者可以先喘一口气，看看目前得到了什么结论：<strong>根据原始GAN定义的判别器loss，我们可以得到最优判别器的形式；而在最优判别器的下，我们可以把原始GAN定义的生成器loss等价变换为最小化真实分布$P_r$与生成分布$P_g$之间的JS散度。我们越训练判别器，它就越接近最优，最小化生成器的loss也就会越近似于最小化$P_r$和$P_g$之间的JS散度。</strong></p><p>问题就出在这个JS散度上。我们会希望如果两个分布之间越接近它们的JS散度越小，我们通过优化JS散度就能将$P_g$“拉向”$P_r$，最终以假乱真。这个希望在两个分布有所重叠的时候是成立的，但是如果两个分布完全没有重叠的部分，或者它们重叠的部分可忽略（下面解释什么叫可忽略），它们的JS散度是多少呢？</p><p>答案是$\log 2$，因为对于任意一个x只有四种可能：</p><p>$$P_1(x) = 0且P_2(x) = 0$$</p><p>$$P_1(x) \neq 0且P_2(x) \neq 0$$</p><p>$$P_1(x) = 0且P_2(x) \neq 0$$</p><p>$$P_1(x) \neq 且P_2(x) = 0$$</p><p>第一种对计算JS散度无贡献，第二种情况由于重叠部分可忽略所以贡献也为0，第三种情况对公式7右边第一个项的贡献是$\log \frac{P_2}{\frac{1}{2}(P_2 + 0)} = \log 2$，第四种情况与之类似，所以最终$JS(P_1||P_2) = \log 2$。</p><p>换句话说，无论$P_r$跟$P_g$是远在天边，还是近在眼前，只要它们俩没有一点重叠或者重叠部分可忽略，JS散度就固定是常数$\log 2$，<strong>而这对于梯度下降方法意味着——梯度为0</strong>！此时对于最优判别器来说，生成器肯定是得不到一丁点梯度信息的；即使对于接近最优的判别器来说，生成器也有很大机会面临梯度消失的问题。</p><p>但是$P_r$与$P_g$不重叠或重叠部分可忽略的可能性有多大？不严谨的答案是：非常大。比较严谨的答案是：<strong>当$P_r$与$P_g$的支撑集（support）是高维空间中的低维流形（manifold）时，$P_r$与$P_g$重叠部分测度（measure）为0的概率为1。</strong></p><p>不用被奇怪的术语吓得关掉页面，虽然论文给出的是严格的数学表述，但是直观上其实很容易理解。首先简单介绍一下这几个概念：</p><ul><li>支撑集（support）其实就是函数的非零部分子集，比如ReLU函数的支撑集就是$(0, +\infty)$，一个概率分布的支撑集就是所有概率密度非零部分的集合。</li><li>流形（manifold）是高维空间中曲线、曲面概念的拓广，我们可以在低维上直观理解这个概念，比如我们说三维空间中的一个曲面是一个二维流形，因为它的本质维度（intrinsic dimension）只有2，一个点在这个二维流形上移动只有两个方向的自由度。同理，三维空间或者二维空间中的一条曲线都是一个一维流形。</li><li>测度（measure）是高维空间中长度、面积、体积概念的拓广，可以理解为“超体积”。</li></ul><p>回过头来看第一句话，“当$P_r$与$P_g$的支撑集是高维空间中的低维流形时”，基本上是成立的。原因是GAN中的生成器一般是从某个低维（比如100维）的随机分布中采样出一个编码向量，再经过一个神经网络生成出一个高维样本（比如64x64的图片就有4096维）。当生成器的参数固定时，生成样本的概率分布虽然是定义在4096维的空间上，但它本身所有可能产生的变化已经被那个100维的随机分布限定了，其本质维度就是100，再考虑到神经网络带来的映射降维，最终可能比100还小，所以生成样本分布的支撑集就在4096维空间中构成一个最多100维的低维流形，“撑不满”整个高维空间。</p><p>“撑不满”就会导致真实分布与生成分布难以“碰到面”，这很容易在二维空间中理解：一方面，二维平面中随机取两条曲线，它们之间刚好存在重叠线段的概率为0；另一方面，虽然它们很大可能会存在交叉点，但是相比于两条曲线而言，交叉点比曲线低一个维度，长度（测度）为0，可忽略。三维空间中也是类似的，随机取两个曲面，它们之间最多就是比较有可能存在交叉线，但是交叉线比曲面低一个维度，面积（测度）是0，可忽略。从低维空间拓展到高维空间，就有了如下逻辑：因为一开始生成器随机初始化，所以$P_g$几乎不可能与$P_r$有什么关联，所以它们的支撑集之间的重叠部分要么不存在，要么就比$P_r$和$P_g$的最小维度还要低至少一个维度，故而测度为0。所谓“重叠部分测度为0”，就是上文所言“不重叠或者重叠部分可忽略”的意思。</p><p>我们就得到了WGAN前作中关于生成器梯度消失的第一个论证：<strong>在（近似）最优判别器下，最小化生成器的loss等价于最小化$P_r$与$P_g$之间的JS散度，而由于$P_r$与$P_g$几乎不可能有不可忽略的重叠，所以无论它们相距多远JS散度都是常数$\log 2$，最终导致生成器的梯度（近似）为0，梯度消失。</strong></p><p>接着作者写了很多公式定理从第二个角度进行论证，但是背后的思想也可以直观地解释：</p><ul><li>首先，$P_r$与$P_g$之间几乎不可能有不可忽略的重叠，所以无论它们之间的“缝隙”多狭小，都肯定存在一个最优分割曲面把它们隔开，最多就是在那些可忽略的重叠处隔不开而已。</li><li>由于判别器作为一个神经网络可以无限拟合这个分隔曲面，所以存在一个最优判别器，对几乎所有真实样本给出概率1，对几乎所有生成样本给出概率0，而那些隔不开的部分就是难以被最优判别器分类的样本，但是它们的测度为0，可忽略。</li><li>最优判别器在真实分布和生成分布的支撑集上给出的概率都是常数（1和0），导致生成器的loss梯度为0，梯度消失。</li></ul><p>有了这些理论分析，原始GAN不稳定的原因就彻底清楚了：判别器训练得太好，生成器梯度消失，生成器loss降不下去；判别器训练得不好，生成器梯度不准，四处乱跑。只有判别器训练得不好不坏才行，但是这个火候又很难把握，甚至在同一轮训练的前后不同阶段这个火候都可能不一样，所以GAN才那么难训练。</p><p>实验辅证如下：</p><blockquote><p>WGAN前作Figure 2。先分别将DCGAN训练1，20，25个epoch，然后固定生成器不动，判别器重新随机初始化从头开始训练，对于第一种形式的生成器loss产生的梯度可以打印出其尺度的变化曲线，可以看到随着判别器的训练，生成器的梯度均迅速衰减。注意y轴是对数坐标轴。</p></blockquote><div class="post-button text-center"><a class="btn" href="/2017/04/29/The-awesome-Wasserstein-GAN/#more" rel="contents">Read more &raquo;</a></div></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/29/WGAN-implemented-by-PyTorch/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/29/WGAN-implemented-by-PyTorch/" itemprop="url">WGAN implemented by PyTorch</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-29T23:21:33+08:00">2017-04-29 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/29/WGAN-implemented-by-PyTorch/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/29/WGAN-implemented-by-PyTorch/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/29/WGAN-implemented-by-PyTorch/" class="leancloud_visitors" data-flag-title="WGAN implemented by PyTorch"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"></div><div class="line"><span class="comment"># Wasserstein Generative Adversarial Networks (WGAN) example in PyTorch.</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># Data params</span></div><div class="line">data_mean = <span class="number">4</span></div><div class="line">data_stddev = <span class="number">1.25</span></div><div class="line"></div><div class="line"><span class="comment"># Model params</span></div><div class="line">g_input_size = <span class="number">1</span>     <span class="comment"># Random noise dimension coming into generator, per output vector</span></div><div class="line">g_hidden_size = <span class="number">50</span>   <span class="comment"># Generator complexity</span></div><div class="line">g_output_size = <span class="number">1</span>    <span class="comment"># size of generated output vector</span></div><div class="line">d_input_size = <span class="number">100</span>   <span class="comment"># Minibatch size - cardinality of distributions</span></div><div class="line">d_hidden_size = <span class="number">50</span>   <span class="comment"># Discriminator complexity</span></div><div class="line">d_output_size = <span class="number">1</span>    <span class="comment"># Single dimension for 'real' vs. 'fake'</span></div><div class="line">minibatch_size = d_input_size</div><div class="line"></div><div class="line">d_learning_rate = <span class="number">2e-4</span>  <span class="comment"># 2e-4</span></div><div class="line">g_learning_rate = <span class="number">2e-4</span></div><div class="line"><span class="comment"># optim_betas = (0.9, 0.999)</span></div><div class="line">num_epochs = <span class="number">30000</span></div><div class="line">print_interval = <span class="number">200</span></div><div class="line"><span class="comment"># d_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator</span></div><div class="line">d_steps = <span class="number">5</span></div><div class="line">g_steps = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># ### Uncomment only one of these</span></div><div class="line"><span class="comment">#(name, preprocess, d_input_func) = ("Raw data", lambda data: data, lambda x: x)</span></div><div class="line">(name, preprocess, d_input_func) = (<span class="string">"Data and variances"</span>, <span class="keyword">lambda</span> data: decorate_with_diffs(data, <span class="number">2.0</span>), <span class="keyword">lambda</span> x: x * <span class="number">2</span>)</div><div class="line"></div><div class="line">print(<span class="string">"Using data [%s]"</span> % (name))</div><div class="line"></div><div class="line"><span class="comment"># ##### DATA: Target data and generator input data</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_distribution_sampler</span><span class="params">(mu, sigma)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> n: torch.Tensor(np.random.normal(mu, sigma, (<span class="number">1</span>, n)))  <span class="comment"># Gaussian</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_generator_input_sampler</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> m, n: torch.rand(m, n)  <span class="comment"># Uniform-dist data into generator, _NOT_ Gaussian</span></div><div class="line"></div><div class="line"><span class="comment"># ##### MODELS: Generator model and discriminator model</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        super(Generator, self).__init__()</div><div class="line">        self.map1 = nn.Linear(input_size, hidden_size)</div><div class="line">        self.map2 = nn.Linear(hidden_size, hidden_size)</div><div class="line">        self.map3 = nn.Linear(hidden_size, output_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.elu(self.map1(x))</div><div class="line">        x = F.sigmoid(self.map2(x))</div><div class="line">        <span class="keyword">return</span> self.map3(x)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        super(Discriminator, self).__init__()</div><div class="line">        self.map1 = nn.Linear(input_size, hidden_size)</div><div class="line">        self.map2 = nn.Linear(hidden_size, hidden_size)</div><div class="line">        self.map3 = nn.Linear(hidden_size, output_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.elu(self.map1(x))</div><div class="line">        x = F.elu(self.map2(x))</div><div class="line">        <span class="comment"># return F.sigmoid(self.map3(x))</span></div><div class="line">        <span class="keyword">return</span> self.map3(x)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">(v)</span>:</span></div><div class="line">    <span class="keyword">return</span> v.data.storage().tolist()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stats</span><span class="params">(d)</span>:</span></div><div class="line">    <span class="keyword">return</span> [np.mean(d), np.std(d)]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorate_with_diffs</span><span class="params">(data, exponent)</span>:</span></div><div class="line">    mean = torch.mean(data.data, <span class="number">1</span>)</div><div class="line">    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[<span class="number">0</span>][<span class="number">0</span>])</div><div class="line">    diffs = torch.pow(data - Variable(mean_broadcast), exponent)</div><div class="line">    <span class="keyword">return</span> torch.cat([data, diffs], <span class="number">1</span>)</div><div class="line"></div><div class="line">d_sampler = get_distribution_sampler(data_mean, data_stddev)</div><div class="line">gi_sampler = get_generator_input_sampler()</div><div class="line">G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)</div><div class="line">D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)</div><div class="line"><span class="comment"># criterion = nn.BCELoss()  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss</span></div><div class="line"><span class="comment"># d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)</span></div><div class="line"><span class="comment"># g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)</span></div><div class="line">d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)</div><div class="line">g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</div><div class="line">    <span class="keyword">for</span> d_index <span class="keyword">in</span> range(d_steps):</div><div class="line">        <span class="comment"># 1. Train D on real+fake</span></div><div class="line">        D.zero_grad()</div><div class="line"></div><div class="line">        <span class="comment">#  1A: Train D on real</span></div><div class="line">        d_real_data = Variable(d_sampler(d_input_size))</div><div class="line">        d_real_decision = D(preprocess(d_real_data))</div><div class="line">        <span class="comment"># d_real_error = criterion(d_real_decision, Variable(torch.ones(1)))  # ones = true</span></div><div class="line">        d_real_error = -torch.mean(d_real_decision)</div><div class="line">        d_real_error.backward() <span class="comment"># compute/store gradients, but don't change params</span></div><div class="line"></div><div class="line">        <span class="comment">#  1B: Train D on fake</span></div><div class="line">        d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))</div><div class="line">        d_fake_data = G(d_gen_input).detach()  <span class="comment"># detach to avoid training G on these labels</span></div><div class="line">        d_fake_decision = D(preprocess(d_fake_data.t()))</div><div class="line">        <span class="comment"># d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1)))  # zeros = fake</span></div><div class="line">        d_fake_error = torch.mean(d_fake_decision)</div><div class="line">        d_fake_error.backward()</div><div class="line">        d_optimizer.step()     <span class="comment"># Only optimizes D's parameters; changes based on stored gradients from backward()</span></div><div class="line">        <span class="comment"># Weight Clipping</span></div><div class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> D.parameters():</div><div class="line">        	p.data.clamp_(<span class="number">-0.01</span>, <span class="number">0.01</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> g_index <span class="keyword">in</span> range(g_steps):</div><div class="line">        <span class="comment"># 2. Train G on D's response (but DO NOT train D on these labels)</span></div><div class="line">        G.zero_grad()</div><div class="line"></div><div class="line">        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))</div><div class="line">        g_fake_data = G(gen_input)</div><div class="line">        dg_fake_decision = D(preprocess(g_fake_data.t()))</div><div class="line">        <span class="comment"># g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))  # we want to fool, so pretend it's all genuine</span></div><div class="line">        g_error = -torch.mean(dg_fake_decision)</div><div class="line"></div><div class="line">        g_error.backward()</div><div class="line">        g_optimizer.step()  <span class="comment"># Only optimizes G's parameters</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> epoch % print_interval == <span class="number">0</span>:</div><div class="line">        print(<span class="string">"%s: D: %s/%s G: %s (Real: %s, Fake: %s) "</span> % (epoch,</div><div class="line">                                                            extract(d_real_error)[<span class="number">0</span>],</div><div class="line">                                                            extract(d_fake_error)[<span class="number">0</span>],</div><div class="line">                                                            extract(g_error)[<span class="number">0</span>],</div><div class="line">                                                            stats(extract(d_real_data)),</div><div class="line">                                                            stats(extract(d_fake_data))))</div></pre></td></tr></table></figure><p>与<a href="https://ewanlee.github.io/2017/04/28/Generative-Adversarial-Networks-GANs-in-50-lines-of-code-PyTorch/">之前的文章</a>所做的修改仅仅只有以下几点（理论支持参考我之前转发的一篇<a href="https://ewanlee.github.io/2017/04/29/The-awesome-Wasserstein-GAN/">博文</a>）:</p><ul><li><p>判别模型最后一层直接用线型激活函数，而不是用Sigmoid函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        super(Discriminator, self).__init__()</div><div class="line">        self.map1 = nn.Linear(input_size, hidden_size)</div><div class="line">        self.map2 = nn.Linear(hidden_size, hidden_size)</div><div class="line">        self.map3 = nn.Linear(hidden_size, output_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.elu(self.map1(x))</div><div class="line">        x = F.elu(self.map2(x))</div><div class="line">        <span class="comment"># return F.sigmoid(self.map3(x))</span></div><div class="line">        <span class="keyword">return</span> self.map3(x)</div></pre></td></tr></table></figure></li><li><p>生成模型与判别模型的loss函数进行修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成模型</span></div><div class="line"><span class="comment"># d_real_error = criterion(d_real_decision, Variable(torch.ones(1)))  # ones = true</span></div><div class="line">d_real_error = -torch.mean(d_real_decision)</div><div class="line"><span class="comment"># d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1)))  # zeros = fake</span></div><div class="line">d_fake_error = torch.mean(d_fake_decision)</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 判别模型</span></div><div class="line"><span class="comment"># g_error = criterion(dg_fake_decision, Variable(torch.ones(1)))  # we want to fool, so pretend it's all genuine</span></div><div class="line">g_error = -torch.mean(dg_fake_decision)</div></pre></td></tr></table></figure></li><li><p>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c (这里取的是0.01)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Weight Clipping</span></div><div class="line"><span class="keyword">for</span> p <span class="keyword">in</span> D.parameters():</div><div class="line">    p.data.clamp_(<span class="number">-0.01</span>, <span class="number">0.01</span>)</div></pre></td></tr></table></figure></li><li><p>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)</span></div><div class="line"><span class="comment"># g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)</span></div><div class="line">d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)</div><div class="line">g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)</div></pre></td></tr></table></figure><p>​</p></li></ul><p>实验结果如下：</p><div class="post-button text-center"><a class="btn" href="/2017/04/29/WGAN-implemented-by-PyTorch/#more" rel="contents">Read more &raquo;</a></div></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/28/Generative-Adversarial-Networks-GANs-in-50-lines-of-code-PyTorch/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/28/Generative-Adversarial-Networks-GANs-in-50-lines-of-code-PyTorch/" itemprop="url">Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-28T19:46:55+08:00">2017-04-28 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/28/Generative-Adversarial-Networks-GANs-in-50-lines-of-code-PyTorch/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/28/Generative-Adversarial-Networks-GANs-in-50-lines-of-code-PyTorch/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/28/Generative-Adversarial-Networks-GANs-in-50-lines-of-code-PyTorch/" class="leancloud_visitors" data-flag-title="Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p><a href="https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f" target="_blank" rel="external">Source Blog</a></p><p>PyTorch Install: <a href="http://pytorch.org/" target="_blank" rel="external">http://pytorch.org/</a></p><p>The models play two distinct (literally, <em>adversarial</em>) roles. Given some real data set <strong>R</strong>, <strong>G</strong> is the <em>generator</em>, trying to create fake data that looks just like the genuine data, while <strong>D</strong> is the <em>discriminator</em>, getting data from either the real set or <strong>G </strong>and labeling the difference. Goodfellow’s metaphor (and a fine one it is) was that <strong>G</strong> was like a team of forgers trying to match real paintings with their output, while <strong>D</strong> was the team of detectives trying to tell the difference. (Except that in this case, the forgers <strong>G</strong> never get to see the original data — only the judgments of <strong>D</strong>. They’re like <em>blind</em> forgers.)</p><p><img src="https://cdn-images-1.medium.com/max/800/1*-gFsbymY9oJUQJ-A3GTfeg.png" alt="img"></p><p>In the ideal case, both <strong>D</strong> and <strong>G</strong> would get better over time until <strong>G</strong> had essentially become a “master forger” of the genuine article and <strong>D</strong> was at a loss, “unable to differentiate between the two distributions.”</p><p>In practice, what Goodfellow had shown was that <strong>G</strong> would be able to perform a form of <em>unsupervised learning</em> on the original dataset, finding some way of representing that data in a (possibly) much lower-dimensional manner. And as Yann LeCun famously stated, <a href="https://www.facebook.com/yann.lecun/posts/10153426023477143" target="_blank" rel="external">unsupervised learning is the “cake” of true AI</a>.</p><hr><p>This powerful technique seems like it must require a <strong>metric ton</strong> of code just to get started, right? Nope. Using <a href="http://pytorch.org/" target="_blank" rel="external">PyTorch</a>, we can actually create a very simple GAN in under 50 lines of code. There are really only 5 components to think about:</p><ul><li><strong>R</strong>: The original, genuine data set</li><li><strong>I</strong>: The random noise that goes into the generator as a source of entropy</li><li><strong>G</strong>: The generator which tries to copy/mimic the original data set</li><li><strong>D</strong>: The discriminator which tries to tell apart <strong>G</strong>’s output from <strong>R</strong></li><li>The actual ‘training’ loop where we teach <strong>G</strong> to trick <strong>D</strong> and <strong>D </strong>to <em>beware</em> <strong>G</strong>.</li></ul><p><strong>1.) R</strong>: In our case, we’ll start with the simplest possible <strong>R</strong> — a bell curve. This function takes a mean and a standard deviation and returns a function which provides the right shape of sample data from a Gaussian with those parameters. In our sample code, we’ll use a mean of 4.0 and a standard deviation of 1.25.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*xsuE-nhsJOzk9lfI3rayuw.png" alt="img"></p><p><strong>2.) I</strong>: The input into the generator is also random, but to make our job a little bit harder, let’s use a uniform distribution rather than a normal one. This means that our model <strong>G</strong> can’t simply shift/scale the input to copy <strong>R, </strong>but has to reshape the data in a non-linear way.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*wuhEVnK25V3zXQzuCwFDAg.png" alt="img"></p><p><strong>3.) G</strong>: The generator is a standard feedforward graph — two hidden layers, three linear maps. We’re using an <a href="http://pytorch.org/docs/nn.html#elu" target="_blank" rel="external">ELU (exponential linear unit)</a> because<a href="https://www.linkedin.com/pulse/exponential-linear-units-elu-deep-network-learning-martin-heusel" target="_blank" rel="external">they’re the new black, yo.</a> <strong>G</strong> is going to get the uniformly distributed data samples from <strong>I</strong> and somehow mimic the normally distributed samples from <strong>R</strong>.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*NM6wfbhZLSiVnCX33f7eBw.png" alt="img"></p><p><strong>4.) D</strong>: The discriminator code is very similar to <strong>G</strong>’s generator code; a feedforward graph with two hidden layers and three linear maps. It’s going to get samples from either <strong>R</strong> or <strong>G</strong> and will output a single scalar between 0 and 1, interpreted as ‘fake’ vs. ‘real’. This is about as milquetoast as a neural net can get.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*5x9hrP5oozp3e2pm-Mtqmw.png" alt="img"></p><p><strong>5.)</strong> Finally, the training loop alternates between two modes: first training <strong>D</strong> on real data vs. fake data, with <em>accurate</em> labels (think of this as <a href="https://en.wikipedia.org/wiki/Police_Academy_%28film%29" target="_blank" rel="external">Police Academy</a>); and then training <strong>G</strong> to fool <strong>D</strong>, with <em>inaccurate</em> labels (this is more like those preparation montages from <a href="https://en.wikipedia.org/wiki/Ocean%27s_Eleven" target="_blank" rel="external">Ocean’s Eleven</a>). It’s a fight between good and evil, people.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*MESLBNZIWxJp553TWKUADQ.png" alt="img"></p><p>Even if you haven’t seen PyTorch before, you can probably tell what’s going on. In the first (green) section, we push both types of data through <strong>D</strong> and apply a differentiable criterion to <strong>D</strong>’s guesses vs. the actual labels. That pushing is the ‘forward’ step; we then call ‘backward()’ explicitly in order to calculate gradients, which are then used to update <strong>D</strong>’s parameters in the d_optimizer step() call. <strong>G</strong> is used but isn’t trained here.</p><p>Then in the last (red) section, we do the same thing for <strong>G</strong> — note that we also run <strong>G</strong>’s output through <strong>D</strong> (we’re essentially giving the forger a detective to practice on) but we <em>do not optimize or change</em> <strong>D</strong> at this step. We don’t want the detective <strong>D</strong> to learn the wrong labels. Hence, we only call g_optimizer.step().</p><p>And…<em>that’s all</em>. There’s some other boilerplate code but the GAN-specific stuff is just those 5 components, nothing else.</p><hr><p>After a few thousand rounds of this forbidden dance between <strong>D</strong> and <strong>G</strong>, what do we get? The discriminator <strong>D</strong> gets good very quickly (while <strong>G</strong> slowly moves up), but once it gets to a certain level of power, <strong>G</strong> has a worthy adversary and begins to improve. <em>Really</em> improve.</p><p>Over 20,000 training rounds, the mean of <strong>G</strong>’s output overshoots 4.0 but then comes back in a fairly stable, correct range (left). Likewise, the standard deviation initially drops in the wrong direction but then rises up to the desired 1.25 range (right), matching <strong>R</strong>.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*2Qm33RqWBKVF3g1Vg2HnVg.png" alt="img"></p><p>Ok, so the basic stats match <strong>R</strong>, eventually. How about the higher moments? Does the shape of the distribution look right? After all, you could certainly have a uniform distribution with a mean of 4.0 and a standard deviation of 1.25, but that wouldn’t really match <strong>R</strong>. Let’s show the final distribution emitted by <strong>G</strong>.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*Ary_6gaLxIijk7j2trroBQ.png" alt="img"></p><p>Not bad. The left tail is a bit longer than the right, but the skew and kurtosis are, shall we say, <em>evocative</em> of the original Gaussian.</p><p><strong>G</strong> recovers the original distribution <strong>R</strong> nearly perfectly — and <strong>D</strong> is left cowering in the corner, mumbling to itself, unable to tell fact from fiction. This is <em>precisely</em> the behavior we want (see <a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank" rel="external">Figure 1 in Goodfellow</a>). <strong>From fewer than 50 lines of code</strong>.</p><p>Goodfellow would go on to publish many other papers on GANs, including a <a href="https://arxiv.org/pdf/1606.03498.pdf" target="_blank" rel="external">2016 gem describing some practical improvements</a>, including the minibatch discrimination method adapted here. And <a href="https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks" target="_blank" rel="external">here’s a 2-hour tutorial he presented at NIPS 2016</a>. For TensorFlow users, here’s a parallel <a href="http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/" target="_blank" rel="external">post from Aylien on GANs</a>.</p><p>Ok. Enough talk. <a href="https://github.com/devnag/pytorch-generative-adversarial-networks" target="_blank" rel="external"><strong>Go look at the code</strong></a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"></div><div class="line"><span class="comment"># Generative Adversarial Networks (GAN) example in PyTorch.</span></div><div class="line"><span class="comment"># See related blog post at https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f#.sch4xgsa9</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># Data params</span></div><div class="line">data_mean = <span class="number">4</span></div><div class="line">data_stddev = <span class="number">1.25</span></div><div class="line"></div><div class="line"><span class="comment"># Model params</span></div><div class="line">g_input_size = <span class="number">1</span>     <span class="comment"># Random noise dimension coming into generator, per output vector</span></div><div class="line">g_hidden_size = <span class="number">50</span>   <span class="comment"># Generator complexity</span></div><div class="line">g_output_size = <span class="number">1</span>    <span class="comment"># size of generated output vector</span></div><div class="line">d_input_size = <span class="number">100</span>   <span class="comment"># Minibatch size - cardinality of distributions</span></div><div class="line">d_hidden_size = <span class="number">50</span>   <span class="comment"># Discriminator complexity</span></div><div class="line">d_output_size = <span class="number">1</span>    <span class="comment"># Single dimension for 'real' vs. 'fake'</span></div><div class="line">minibatch_size = d_input_size</div><div class="line"></div><div class="line">d_learning_rate = <span class="number">2e-4</span>  <span class="comment"># 2e-4</span></div><div class="line">g_learning_rate = <span class="number">2e-4</span></div><div class="line">optim_betas = (<span class="number">0.9</span>, <span class="number">0.999</span>)</div><div class="line">num_epochs = <span class="number">30000</span></div><div class="line">print_interval = <span class="number">200</span></div><div class="line">d_steps = <span class="number">1</span>  <span class="comment"># 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator</span></div><div class="line">g_steps = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># ### Uncomment only one of these</span></div><div class="line"><span class="comment">#(name, preprocess, d_input_func) = ("Raw data", lambda data: data, lambda x: x)</span></div><div class="line">(name, preprocess, d_input_func) = (<span class="string">"Data and variances"</span>, <span class="keyword">lambda</span> data: decorate_with_diffs(data, <span class="number">2.0</span>), <span class="keyword">lambda</span> x: x * <span class="number">2</span>)</div><div class="line"></div><div class="line">print(<span class="string">"Using data [%s]"</span> % (name))</div><div class="line"></div><div class="line"><span class="comment"># ##### DATA: Target data and generator input data</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_distribution_sampler</span><span class="params">(mu, sigma)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> n: torch.Tensor(np.random.normal(mu, sigma, (<span class="number">1</span>, n)))  <span class="comment"># Gaussian</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_generator_input_sampler</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> m, n: torch.rand(m, n)  <span class="comment"># Uniform-dist data into generator, _NOT_ Gaussian</span></div><div class="line"></div><div class="line"><span class="comment"># ##### MODELS: Generator model and discriminator model</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        super(Generator, self).__init__()</div><div class="line">        self.map1 = nn.Linear(input_size, hidden_size)</div><div class="line">        self.map2 = nn.Linear(hidden_size, hidden_size)</div><div class="line">        self.map3 = nn.Linear(hidden_size, output_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.elu(self.map1(x))</div><div class="line">        x = F.sigmoid(self.map2(x))</div><div class="line">        <span class="keyword">return</span> self.map3(x)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size)</span>:</span></div><div class="line">        super(Discriminator, self).__init__()</div><div class="line">        self.map1 = nn.Linear(input_size, hidden_size)</div><div class="line">        self.map2 = nn.Linear(hidden_size, hidden_size)</div><div class="line">        self.map3 = nn.Linear(hidden_size, output_size)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = F.elu(self.map1(x))</div><div class="line">        x = F.elu(self.map2(x))</div><div class="line">        <span class="keyword">return</span> F.sigmoid(self.map3(x))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">(v)</span>:</span></div><div class="line">    <span class="keyword">return</span> v.data.storage().tolist()</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stats</span><span class="params">(d)</span>:</span></div><div class="line">    <span class="keyword">return</span> [np.mean(d), np.std(d)]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">decorate_with_diffs</span><span class="params">(data, exponent)</span>:</span></div><div class="line">    mean = torch.mean(data.data, <span class="number">1</span>)</div><div class="line">    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[<span class="number">0</span>][<span class="number">0</span>])</div><div class="line">    diffs = torch.pow(data - Variable(mean_broadcast), exponent)</div><div class="line">    <span class="keyword">return</span> torch.cat([data, diffs], <span class="number">1</span>)</div><div class="line"></div><div class="line">d_sampler = get_distribution_sampler(data_mean, data_stddev)</div><div class="line">gi_sampler = get_generator_input_sampler()</div><div class="line">G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)</div><div class="line">D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)</div><div class="line">criterion = nn.BCELoss()  <span class="comment"># Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss</span></div><div class="line">d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)</div><div class="line">g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)</div><div class="line"></div><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</div><div class="line">    <span class="keyword">for</span> d_index <span class="keyword">in</span> range(d_steps):</div><div class="line">        <span class="comment"># 1. Train D on real+fake</span></div><div class="line">        D.zero_grad()</div><div class="line"></div><div class="line">        <span class="comment">#  1A: Train D on real</span></div><div class="line">        d_real_data = Variable(d_sampler(d_input_size))</div><div class="line">        d_real_decision = D(preprocess(d_real_data))</div><div class="line">        d_real_error = criterion(d_real_decision, Variable(torch.ones(<span class="number">1</span>)))  <span class="comment"># ones = true</span></div><div class="line">        d_real_error.backward() <span class="comment"># compute/store gradients, but don't change params</span></div><div class="line"></div><div class="line">        <span class="comment">#  1B: Train D on fake</span></div><div class="line">        d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size))</div><div class="line">        d_fake_data = G(d_gen_input).detach()  <span class="comment"># detach to avoid training G on these labels</span></div><div class="line">        d_fake_decision = D(preprocess(d_fake_data.t()))</div><div class="line">        d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(<span class="number">1</span>)))  <span class="comment"># zeros = fake</span></div><div class="line">        d_fake_error.backward()</div><div class="line">        d_optimizer.step()     <span class="comment"># Only optimizes D's parameters; changes based on stored gradients from backward()</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> g_index <span class="keyword">in</span> range(g_steps):</div><div class="line">        <span class="comment"># 2. Train G on D's response (but DO NOT train D on these labels)</span></div><div class="line">        G.zero_grad()</div><div class="line"></div><div class="line">        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))</div><div class="line">        g_fake_data = G(gen_input)</div><div class="line">        dg_fake_decision = D(preprocess(g_fake_data.t()))</div><div class="line">        g_error = criterion(dg_fake_decision, Variable(torch.ones(<span class="number">1</span>)))  <span class="comment"># we want to fool, so pretend it's all genuine</span></div><div class="line"></div><div class="line">        g_error.backward()</div><div class="line">        g_optimizer.step()  <span class="comment"># Only optimizes G's parameters</span></div><div class="line"></div><div class="line">    <span class="keyword">if</span> epoch % print_interval == <span class="number">0</span>:</div><div class="line">        print(<span class="string">"%s: D: %s/%s G: %s (Real: %s, Fake: %s) "</span> % (epoch,</div><div class="line">                                                            extract(d_real_error)[<span class="number">0</span>],</div><div class="line">                                                            extract(d_fake_error)[<span class="number">0</span>],</div><div class="line">                                                            extract(g_error)[<span class="number">0</span>],</div><div class="line">                                                            stats(extract(d_real_data)),</div><div class="line">                                                            stats(extract(d_fake_data))))</div></pre></td></tr></table></figure><p>Result：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div></pre></td><td class="code"><pre><div class="line">ewan<span class="meta">@ubuntu</span>:~<span class="regexp">/Documents/g</span>an/pytorch-generative-adversarial-networks$ python gan_pytorch.py </div><div class="line">Using data [Data and variances]</div><div class="line"><span class="number">0</span>: D: <span class="number">0.636019647121</span>/<span class="number">0.687892377377</span> G: <span class="number">0.692580163479</span> (Real: [<span class="number">4.0121619534492492</span>, <span class="number">1.3228379995364423</span>], Fake: [<span class="number">0.36497069358825684</span>, <span class="number">0.0040907625909989871</span>]) </div><div class="line"><span class="number">200</span>: D: <span class="number">2.92067015835e-05</span>/<span class="number">0.474851727486</span> G: <span class="number">1.00973010063</span> (Real: [<span class="number">4.0935744738578794</span>, <span class="number">1.3016500752040552</span>], Fake: [<span class="number">-0.5716635638475418</span>, <span class="number">0.019948046232028654</span>]) </div><div class="line"><span class="number">400</span>: D: <span class="number">0.0014917049557</span>/<span class="number">0.502498149872</span> G: <span class="number">0.943185687065</span> (Real: [<span class="number">4.198446000814438</span>, <span class="number">1.1262929992527102</span>], Fake: [<span class="number">-0.21786054879426955</span>, <span class="number">0.0067362612730766476</span>]) </div><div class="line"><span class="number">600</span>: D: <span class="number">6.4969262894e-06</span>/<span class="number">0.384293109179</span> G: <span class="number">1.15257537365</span> (Real: [<span class="number">3.8602226501703263</span>, <span class="number">1.3292726136430937</span>], Fake: [<span class="number">-0.29857088595628739</span>, <span class="number">0.03924369275813562</span>]) </div><div class="line"><span class="number">800</span>: D: <span class="number">1.84774467016e-06</span>/<span class="number">0.211148008704</span> G: <span class="number">1.67116880417</span> (Real: [<span class="number">4.0269100540876392</span>, <span class="number">1.2954351206409835</span>], Fake: [<span class="number">-0.32296697288751602</span>, <span class="number">0.14901211840131676</span>]) </div><div class="line"><span class="number">1000</span>: D: <span class="number">9.02455067262e-05</span>/<span class="number">0.0219078511</span> G: <span class="number">4.19585323334</span> (Real: [<span class="number">3.9491306754946707</span>, <span class="number">1.3613105655283608</span>], Fake: [<span class="number">0.13110455054789782</span>, <span class="number">0.5252103421913964</span>]) </div><div class="line"><span class="number">1200</span>: D: <span class="number">0.00441630883142</span>/<span class="number">0.137605398893</span> G: <span class="number">2.78980493546</span> (Real: [<span class="number">4.238747425079346</span>, <span class="number">1.1837142728845262</span>], Fake: [<span class="number">2.3851456820964811</span>, <span class="number">0.69947230698573948</span>]) </div><div class="line"><span class="number">1400</span>: D: <span class="number">0.291683584452</span>/<span class="number">0.824121117592</span> G: <span class="number">0.26126781106</span> (Real: [<span class="number">3.8486315739154815</span>, <span class="number">1.2074486225815622</span>], Fake: [<span class="number">3.4868409335613251</span>, <span class="number">1.2438192602257458</span>]) </div><div class="line"><span class="number">1600</span>: D: <span class="number">0.503275632858</span>/<span class="number">1.08712184429</span> G: <span class="number">0.628099560738</span> (Real: [<span class="number">3.7856648898124696</span>, <span class="number">1.1925325100947208</span>], Fake: [<span class="number">3.9149187129735945</span>, <span class="number">1.5374543372663099</span>]) </div><div class="line"><span class="number">1800</span>: D: <span class="number">0.992162883282</span>/<span class="number">0.955306172371</span> G: <span class="number">0.215137541294</span> (Real: [<span class="number">3.9097139459848402</span>, <span class="number">1.3729001379532129</span>], Fake: [<span class="number">4.9751595187187192</span>, <span class="number">1.2850838287273094</span>]) </div><div class="line"><span class="number">2000</span>: D: <span class="number">0.701098382473</span>/<span class="number">0.634775817394</span> G: <span class="number">0.389043629169</span> (Real: [<span class="number">3.9641699814796447</span>, <span class="number">1.1512756986625183</span>], Fake: [<span class="number">5.0374661159515384</span>, <span class="number">1.5190411587235346</span>]) </div><div class="line"><span class="number">2200</span>: D: <span class="number">0.510353624821</span>/<span class="number">0.350295126438</span> G: <span class="number">1.5988701582</span> (Real: [<span class="number">4.0406568145751951</span>, <span class="number">1.3612318676859239</span>], Fake: [<span class="number">5.4763065743446351</span>, <span class="number">1.2736378899688456</span>]) </div><div class="line"><span class="number">2400</span>: D: <span class="number">0.895085930824</span>/<span class="number">0.400622785091</span> G: <span class="number">0.922062814236</span> (Real: [<span class="number">3.8292097043991089</span>, <span class="number">1.1506111704583193</span>], Fake: [<span class="number">4.5642045128345492</span>, <span class="number">1.7082890861364539</span>]) </div><div class="line"><span class="number">2600</span>: D: <span class="number">0.802581310272</span>/<span class="number">0.717123866081</span> G: <span class="number">0.572393655777</span> (Real: [<span class="number">4.0654918360710148</span>, <span class="number">1.2552944260604222</span>], Fake: [<span class="number">5.1286249160766602</span>, <span class="number">1.0479449058428656</span>]) </div><div class="line"><span class="number">2800</span>: D: <span class="number">0.51098883152</span>/<span class="number">0.489002883434</span> G: <span class="number">0.842381119728</span> (Real: [<span class="number">4.0405197954177856</span>, <span class="number">1.136660175398452</span>], Fake: [<span class="number">3.9549839448928834</span>, <span class="number">1.1751749984899784</span>]) </div><div class="line"><span class="number">3000</span>: D: <span class="number">0.496278882027</span>/<span class="number">0.97537201643</span> G: <span class="number">0.753688693047</span> (Real: [<span class="number">4.0026307255029678</span>, <span class="number">1.2446167315972034</span>], Fake: [<span class="number">3.2340782660245897</span>, <span class="number">1.2949288892421307</span>]) </div><div class="line"><span class="number">3200</span>: D: <span class="number">0.696556508541</span>/<span class="number">0.829834342003</span> G: <span class="number">0.475445389748</span> (Real: [<span class="number">3.9983750417828561</span>, <span class="number">1.2828095340103229</span>], Fake: [<span class="number">3.5434492731094362</span>, <span class="number">0.98673911467128028</span>]) </div><div class="line"><span class="number">3400</span>: D: <span class="number">0.479906737804</span>/<span class="number">0.477254271507</span> G: <span class="number">1.2421528101</span> (Real: [<span class="number">4.1585888534784319</span>, <span class="number">1.2672863214247221</span>], Fake: [<span class="number">3.3173918831348419</span>, <span class="number">1.156708995162234</span>]) </div><div class="line"><span class="number">3600</span>: D: <span class="number">1.36562228203</span>/<span class="number">0.508370876312</span> G: <span class="number">0.550418972969</span> (Real: [<span class="number">4.0406067597866056</span>, <span class="number">1.1363201759386616</span>], Fake: [<span class="number">4.4300824308395388</span>, <span class="number">1.0639278538481793</span>]) </div><div class="line"><span class="number">3800</span>: D: <span class="number">0.538426816463</span>/<span class="number">0.622343420982</span> G: <span class="number">0.786149024963</span> (Real: [<span class="number">4.0097330248355867</span>, <span class="number">1.1609232820569348</span>], Fake: [<span class="number">4.5179304122924808</span>, <span class="number">1.2347411732817635</span>]) </div><div class="line"><span class="number">4000</span>: D: <span class="number">0.350504934788</span>/<span class="number">0.361344873905</span> G: <span class="number">0.728424191475</span> (Real: [<span class="number">3.7975878280401232</span>, <span class="number">1.2378775025626094</span>], Fake: [<span class="number">4.3484812033176423</span>, <span class="number">1.4327683271077338</span>]) </div><div class="line"><span class="number">4200</span>: D: <span class="number">0.912463009357</span>/<span class="number">0.779066801071</span> G: <span class="number">0.840294659138</span> (Real: [<span class="number">3.9861780107021332</span>, <span class="number">1.2293009498211762</span>], Fake: [<span class="number">4.0718169224262235</span>, <span class="number">1.2044778720046834</span>]) </div><div class="line"><span class="number">4400</span>: D: <span class="number">0.814347147942</span>/<span class="number">0.794115483761</span> G: <span class="number">0.889387726784</span> (Real: [<span class="number">3.9556436133384705</span>, <span class="number">1.1131208050960595</span>], Fake: [<span class="number">3.6148070895671847</span>, <span class="number">1.1790021094109027</span>]) </div><div class="line"><span class="number">4600</span>: D: <span class="number">0.637132883072</span>/<span class="number">0.639598190784</span> G: <span class="number">0.835896074772</span> (Real: [<span class="number">4.0807307386398319</span>, <span class="number">1.1590112689981971</span>], Fake: [<span class="number">3.6376679444313051</span>, <span class="number">1.2540016088688517</span>]) </div><div class="line"><span class="number">4800</span>: D: <span class="number">0.816388785839</span>/<span class="number">0.629823803902</span> G: <span class="number">0.6337043643</span> (Real: [<span class="number">4.1595975148677828</span>, <span class="number">1.2996693029809485</span>], Fake: [<span class="number">4.0303308999538423</span>, <span class="number">1.3050560562935769</span>]) </div><div class="line"><span class="number">5000</span>: D: <span class="number">1.38226401806</span>/<span class="number">0.714248239994</span> G: <span class="number">1.17240273952</span> (Real: [<span class="number">3.9217003214359285</span>, <span class="number">1.3408209709046912</span>], Fake: [<span class="number">4.4204820060729979</span>, <span class="number">1.0378887480226417</span>]) </div><div class="line"><span class="number">5200</span>: D: <span class="number">0.752707779408</span>/<span class="number">0.432243227959</span> G: <span class="number">0.735915839672</span> (Real: [<span class="number">4.033863249272108</span>, <span class="number">1.417255801501303</span>], Fake: [<span class="number">3.7434970003366472</span>, <span class="number">1.4305561672741818</span>]) </div><div class="line"><span class="number">5400</span>: D: <span class="number">0.672449588776</span>/<span class="number">0.694190680981</span> G: <span class="number">0.671269893646</span> (Real: [<span class="number">3.9849637061357499</span>, <span class="number">1.3054745436415693</span>], Fake: [<span class="number">3.7987613070011137</span>, <span class="number">1.1584021967574571</span>]) </div><div class="line"><span class="number">5600</span>: D: <span class="number">0.633513212204</span>/<span class="number">0.678804934025</span> G: <span class="number">0.736048042774</span> (Real: [<span class="number">3.8742538380622862</span>, <span class="number">1.1924929483627851</span>], Fake: [<span class="number">4.0905960440635685</span>, <span class="number">1.0496450658176097</span>]) </div><div class="line"><span class="number">5800</span>: D: <span class="number">0.954816102982</span>/<span class="number">0.619474828243</span> G: <span class="number">0.847522497177</span> (Real: [<span class="number">4.0848416697978971</span>, <span class="number">1.2377045321962332</span>], Fake: [<span class="number">4.5059887909889218</span>, <span class="number">1.0769809353783582</span>]) </div><div class="line"><span class="number">6000</span>: D: <span class="number">0.634225904942</span>/<span class="number">0.653471052647</span> G: <span class="number">0.402414888144</span> (Real: [<span class="number">3.9909452509880068</span>, <span class="number">1.2152347623325401</span>], Fake: [<span class="number">3.9412865948677065</span>, <span class="number">1.2808620107297906</span>]) </div><div class="line"><span class="number">6200</span>: D: <span class="number">0.733776032925</span>/<span class="number">0.414616316557</span> G: <span class="number">0.969770550728</span> (Real: [<span class="number">4.0096452310681343</span>, <span class="number">1.2858629342885464</span>], Fake: [<span class="number">3.4776910370588303</span>, <span class="number">1.4216167469252254</span>]) </div><div class="line"><span class="number">6400</span>: D: <span class="number">0.483776688576</span>/<span class="number">0.456314682961</span> G: <span class="number">0.42595911026</span> (Real: [<span class="number">4.16927042722702</span>, <span class="number">1.2557057135387499</span>], Fake: [<span class="number">3.905275868177414</span>, <span class="number">1.3509040440658031</span>]) </div><div class="line"><span class="number">6600</span>: D: <span class="number">1.06177055836</span>/<span class="number">0.443961560726</span> G: <span class="number">0.910483181477</span> (Real: [<span class="number">4.0327691116929056</span>, <span class="number">1.1752792712434861</span>], Fake: [<span class="number">4.1322225379943847</span>, <span class="number">1.3041032842304898</span>]) </div><div class="line"><span class="number">6800</span>: D: <span class="number">0.911615252495</span>/<span class="number">0.851063728333</span> G: <span class="number">0.822307884693</span> (Real: [<span class="number">4.0429812586307525</span>, <span class="number">1.0149434426406105</span>], Fake: [<span class="number">4.181604235172272</span>, <span class="number">1.1091966315801844</span>]) </div><div class="line"><span class="number">7000</span>: D: <span class="number">0.859644412994</span>/<span class="number">0.819373309612</span> G: <span class="number">0.683367550373</span> (Real: [<span class="number">4.0413902151584624</span>, <span class="number">1.2697299173474621</span>], Fake: [<span class="number">3.6461249232292174</span>, <span class="number">1.1392232969008105</span>]) </div><div class="line"><span class="number">7200</span>: D: <span class="number">0.697537004948</span>/<span class="number">1.29639554024</span> G: <span class="number">0.567749083042</span> (Real: [<span class="number">3.9289280462265013</span>, <span class="number">1.1476723124689931</span>], Fake: [<span class="number">4.3612218284606934</span>, <span class="number">1.1698644305174593</span>]) </div><div class="line"><span class="number">7400</span>: D: <span class="number">0.892510712147</span>/<span class="number">0.93148213625</span> G: <span class="number">1.18729686737</span> (Real: [<span class="number">3.9838603484630584</span>, <span class="number">1.10640478112829</span>], Fake: [<span class="number">4.1228645443916321</span>, <span class="number">1.2695625804586594</span>]) </div><div class="line"><span class="number">7600</span>: D: <span class="number">0.855136275291</span>/<span class="number">0.683420717716</span> G: <span class="number">0.87994658947</span> (Real: [<span class="number">4.1161885654926298</span>, <span class="number">1.1923004904972447</span>], Fake: [<span class="number">3.6958885985612868</span>, <span class="number">1.3379389180110717</span>]) </div><div class="line"><span class="number">7800</span>: D: <span class="number">0.549697399139</span>/<span class="number">1.37823116779</span> G: <span class="number">0.398991644382</span> (Real: [<span class="number">4.2173074555397037</span>, <span class="number">1.2371073094023581</span>], Fake: [<span class="number">3.8741448554396629</span>, <span class="number">1.3837623378110455</span>]) </div><div class="line"><span class="number">8000</span>: D: <span class="number">1.35398185253</span>/<span class="number">0.410179078579</span> G: <span class="number">0.527717351913</span> (Real: [<span class="number">3.9588229835033415</span>, <span class="number">1.3744496473744439</span>], Fake: [<span class="number">3.9429207968711855</span>, <span class="number">1.3684983506717674</span>]) </div><div class="line"><span class="number">8200</span>: D: <span class="number">0.700774013996</span>/<span class="number">0.295857429504</span> G: <span class="number">0.803082704544</span> (Real: [<span class="number">3.8515358114242555</span>, <span class="number">1.2566173136350174</span>], Fake: [<span class="number">3.7108538401126863</span>, <span class="number">1.3342916614304938</span>]) </div><div class="line"><span class="number">8400</span>: D: <span class="number">0.689352571964</span>/<span class="number">0.590398311615</span> G: <span class="number">0.698961615562</span> (Real: [<span class="number">3.965521250963211</span>, <span class="number">1.2231963456729893</span>], Fake: [<span class="number">4.6866454958915709</span>, <span class="number">1.1286615282559416</span>]) </div><div class="line"><span class="number">8600</span>: D: <span class="number">0.19632807374</span>/<span class="number">0.604559898376</span> G: <span class="number">0.812706291676</span> (Real: [<span class="number">3.8928249645233155</span>, <span class="number">1.3264703109197318</span>], Fake: [<span class="number">3.918080286383629</span>, <span class="number">1.2016505045193488</span>]) </div><div class="line"><span class="number">8800</span>: D: <span class="number">0.595732450485</span>/<span class="number">0.572122216225</span> G: <span class="number">0.738678693771</span> (Real: [<span class="number">3.7554583859443667</span>, <span class="number">1.2011572644775179</span>], Fake: [<span class="number">3.8252914756536485</span>, <span class="number">1.1905187885079342</span>]) </div><div class="line"><span class="number">9000</span>: D: <span class="number">0.232542961836</span>/<span class="number">1.26930451393</span> G: <span class="number">0.834500789642</span> (Real: [<span class="number">3.9203160056471824</span>, <span class="number">1.2725988502730134</span>], Fake: [<span class="number">4.1613124001026156</span>, <span class="number">1.2681795442466237</span>]) </div><div class="line"><span class="number">9200</span>: D: <span class="number">1.257376194</span>/<span class="number">0.5735257864</span> G: <span class="number">0.554405272007</span> (Real: [<span class="number">3.8860677522420883</span>, <span class="number">1.1041807259307903</span>], Fake: [<span class="number">3.9102136331796644</span>, <span class="number">1.3811967247690093</span>]) </div><div class="line"><span class="number">9400</span>: D: <span class="number">0.610212028027</span>/<span class="number">0.538761377335</span> G: <span class="number">0.558459818363</span> (Real: [<span class="number">4.0015355503559116</span>, <span class="number">0.99711450973270277</span>], Fake: [<span class="number">3.8555663478374482</span>, <span class="number">1.1037480705144518</span>]) </div><div class="line"><span class="number">9600</span>: D: <span class="number">0.702151358128</span>/<span class="number">0.81621837616</span> G: <span class="number">0.706716835499</span> (Real: [<span class="number">4.0513852632045744</span>, <span class="number">1.1984303669025829</span>], Fake: [<span class="number">4.2933621263504032</span>, <span class="number">1.1478353305254103</span>]) </div><div class="line"><span class="number">9800</span>: D: <span class="number">0.511451423168</span>/<span class="number">0.670217812061</span> G: <span class="number">0.873916983604</span> (Real: [<span class="number">3.935146123766899</span>, <span class="number">1.3218541944694313</span>], Fake: [<span class="number">4.2863738107681275</span>, <span class="number">1.1362357473661524</span>]) </div><div class="line"><span class="number">10000</span>: D: <span class="number">0.587130308151</span>/<span class="number">0.764386773109</span> G: <span class="number">0.714644312859</span> (Real: [<span class="number">4.0829932641983033</span>, <span class="number">1.1844677307174318</span>], Fake: [<span class="number">4.2149634605646131</span>, <span class="number">1.1542778585504672</span>]) </div><div class="line"><span class="number">10200</span>: D: <span class="number">0.454408079386</span>/<span class="number">0.390097141266</span> G: <span class="number">0.694087386131</span> (Real: [<span class="number">3.9480907583236693</span>, <span class="number">1.2586832917742197</span>], Fake: [<span class="number">3.9525690937042235</span>, <span class="number">1.3555640918653922</span>]) </div><div class="line"><span class="number">10400</span>: D: <span class="number">0.232991695404</span>/<span class="number">0.377689123154</span> G: <span class="number">0.839949011803</span> (Real: [<span class="number">3.9636431083083155</span>, <span class="number">1.2146210496905581</span>], Fake: [<span class="number">4.0022356742620468</span>, <span class="number">1.0348462356745984</span>]) </div><div class="line"><span class="number">10600</span>: D: <span class="number">0.887756228447</span>/<span class="number">0.452646583319</span> G: <span class="number">0.776298880577</span> (Real: [<span class="number">4.1107078218460087</span>, <span class="number">1.3061081296488184</span>], Fake: [<span class="number">4.3001403945684435</span>, <span class="number">1.3191353715419794</span>]) </div><div class="line"><span class="number">10800</span>: D: <span class="number">0.988030552864</span>/<span class="number">0.472889751196</span> G: <span class="number">2.00703763962</span> (Real: [<span class="number">4.1303015506267551</span>, <span class="number">1.2646447231333668</span>], Fake: [<span class="number">4.2425211107730867</span>, <span class="number">1.2706986066792705</span>]) </div><div class="line"><span class="number">11000</span>: D: <span class="number">0.962553679943</span>/<span class="number">1.00584948063</span> G: <span class="number">0.458068579435</span> (Real: [<span class="number">4.1017441129684444</span>, <span class="number">1.1564779436003478</span>], Fake: [<span class="number">3.861787896156311</span>, <span class="number">1.2478181443952361</span>]) </div><div class="line"><span class="number">11200</span>: D: <span class="number">0.404395908117</span>/<span class="number">0.560545325279</span> G: <span class="number">0.764987766743</span> (Real: [<span class="number">3.8819530367851258</span>, <span class="number">1.1290593525971337</span>], Fake: [<span class="number">4.0393019503355028</span>, <span class="number">1.1760851438968263</span>]) </div><div class="line"><span class="number">11400</span>: D: <span class="number">1.04482722282</span>/<span class="number">0.170368790627</span> G: <span class="number">0.979512214661</span> (Real: [<span class="number">4.0775347077846531</span>, <span class="number">1.1743573984958275</span>], Fake: [<span class="number">4.4076948529481887</span>, <span class="number">1.1430737801156545</span>]) </div><div class="line"><span class="number">11600</span>: D: <span class="number">0.767144262791</span>/<span class="number">0.419019073248</span> G: <span class="number">0.804197788239</span> (Real: [<span class="number">4.1507718646526337</span>, <span class="number">1.2935215526943189</span>], Fake: [<span class="number">4.2565110635757444</span>, <span class="number">1.1195747875890809</span>]) </div><div class="line"><span class="number">11800</span>: D: <span class="number">0.328228145838</span>/<span class="number">0.192100420594</span> G: <span class="number">0.694948136806</span> (Real: [<span class="number">4.2615561389923098</span>, <span class="number">1.3187283101366121</span>], Fake: [<span class="number">3.7841238260269163</span>, <span class="number">1.2796545407667934</span>]) </div><div class="line"><span class="number">12000</span>: D: <span class="number">0.939581632614</span>/<span class="number">0.512252509594</span> G: <span class="number">0.486280798912</span> (Real: [<span class="number">4.1770594882965089</span>, <span class="number">1.2492834466325793</span>], Fake: [<span class="number">4.0997331076860428</span>, <span class="number">1.0701209918243111</span>]) </div><div class="line"><span class="number">12200</span>: D: <span class="number">0.964525461197</span>/<span class="number">0.397465586662</span> G: <span class="number">1.45534229279</span> (Real: [<span class="number">3.9129967219382524</span>, <span class="number">1.3473476671217695</span>], Fake: [<span class="number">4.3561846733093263</span>, <span class="number">1.1667221650406194</span>]) </div><div class="line"><span class="number">12400</span>: D: <span class="number">0.516430974007</span>/<span class="number">0.255626231432</span> G: <span class="number">0.753806650639</span> (Real: [<span class="number">3.9942912605404852</span>, <span class="number">1.3623400447216258</span>], Fake: [<span class="number">4.2171517282724382</span>, <span class="number">1.2046534326031684</span>]) </div><div class="line"><span class="number">12600</span>: D: <span class="number">0.050210531801</span>/<span class="number">0.567070662975</span> G: <span class="number">0.887824892998</span> (Real: [<span class="number">3.9560802054405211</span>, <span class="number">1.3569670682588555</span>], Fake: [<span class="number">3.6434229278564452</span>, <span class="number">1.2798963544271591</span>]) </div><div class="line"><span class="number">12800</span>: D: <span class="number">0.566556215286</span>/<span class="number">1.45121753216</span> G: <span class="number">2.67591071129</span> (Real: [<span class="number">4.0868541407585148</span>, <span class="number">1.1440918337515926</span>], Fake: [<span class="number">3.7308121472597122</span>, <span class="number">1.2567484994327229</span>]) </div><div class="line"><span class="number">13000</span>: D: <span class="number">0.285438686609</span>/<span class="number">1.26493763924</span> G: <span class="number">0.714931368828</span> (Real: [<span class="number">4.0406689298152925</span>, <span class="number">1.2295255598171184</span>], Fake: [<span class="number">4.1976348906755447</span>, <span class="number">1.2778464434389283</span>]) </div><div class="line"><span class="number">13200</span>: D: <span class="number">0.420082330704</span>/<span class="number">0.20268279314</span> G: <span class="number">1.13221895695</span> (Real: [<span class="number">4.0006502330303189</span>, <span class="number">1.1790149224725006</span>], Fake: [<span class="number">4.2336275362968445</span>, <span class="number">1.2803975596845565</span>]) </div><div class="line"><span class="number">13400</span>: D: <span class="number">0.219869300723</span>/<span class="number">0.733704686165</span> G: <span class="number">1.4634616375</span> (Real: [<span class="number">3.8348834168910981</span>, <span class="number">1.240605849665303</span>], Fake: [<span class="number">3.8208065938949587</span>, <span class="number">1.3042463825727604</span>]) </div><div class="line"><span class="number">13600</span>: D: <span class="number">1.35286784172</span>/<span class="number">0.161317944527</span> G: <span class="number">2.29795908928</span> (Real: [<span class="number">4.0841373348236081</span>, <span class="number">1.2295542819596996</span>], Fake: [<span class="number">4.0513113558292391</span>, <span class="number">1.2789595441318489</span>]) </div><div class="line"><span class="number">13800</span>: D: <span class="number">0.188396275043</span>/<span class="number">0.38589566946</span> G: <span class="number">1.38826131821</span> (Real: [<span class="number">4.0228236329555509</span>, <span class="number">1.3524482715610078</span>], Fake: [<span class="number">4.2307587480545044</span>, <span class="number">1.2042737228043698</span>]) </div><div class="line"><span class="number">14000</span>: D: <span class="number">0.0101562952623</span>/<span class="number">0.363918542862</span> G: <span class="number">1.24292945862</span> (Real: [<span class="number">4.0695835274457934</span>, <span class="number">1.4484548400603423</span>], Fake: [<span class="number">4.3588982570171355</span>, <span class="number">1.2305509242343933</span>]) </div><div class="line"><span class="number">14200</span>: D: <span class="number">0.308517187834</span>/<span class="number">0.687216579914</span> G: <span class="number">0.831201374531</span> (Real: [<span class="number">4.1314239382743834</span>, <span class="number">1.2039768851618762</span>], Fake: [<span class="number">4.3469831347465515</span>, <span class="number">1.1622408025070994</span>]) </div><div class="line"><span class="number">14400</span>: D: <span class="number">1.05658388138</span>/<span class="number">0.777651846409</span> G: <span class="number">0.713593065739</span> (Real: [<span class="number">3.9307258637249469</span>, <span class="number">1.3932677098843045</span>], Fake: [<span class="number">3.8781710839271546</span>, <span class="number">1.3920662615905985</span>]) </div><div class="line"><span class="number">14600</span>: D: <span class="number">0.428974717855</span>/<span class="number">0.430344074965</span> G: <span class="number">0.865560889244</span> (Real: [<span class="number">4.2443156433105464</span>, <span class="number">1.4786604488020483</span>], Fake: [<span class="number">3.9386759352684022</span>, <span class="number">1.2173706417721266</span>]) </div><div class="line"><span class="number">14800</span>: D: <span class="number">0.358524769545</span>/<span class="number">0.631785154343</span> G: <span class="number">1.72760403156</span> (Real: [<span class="number">4.0897545439004901</span>, <span class="number">1.3611061267905207</span>], Fake: [<span class="number">4.0185626268386843</span>, <span class="number">1.2011546705663261</span>]) </div><div class="line"><span class="number">15000</span>: D: <span class="number">0.451200634241</span>/<span class="number">0.451773911715</span> G: <span class="number">1.10325527191</span> (Real: [<span class="number">3.9933083570003509</span>, <span class="number">1.0881706638388742</span>], Fake: [<span class="number">3.902902855873108</span>, <span class="number">1.1771562868487595</span>]) </div><div class="line"><span class="number">15200</span>: D: <span class="number">0.756480932236</span>/<span class="number">0.419855684042</span> G: <span class="number">0.942300021648</span> (Real: [<span class="number">4.1753564620018002</span>, <span class="number">1.3629881946025171</span>], Fake: [<span class="number">3.8721090507507325</span>, <span class="number">1.189488508024922</span>]) </div><div class="line"><span class="number">15400</span>: D: <span class="number">0.219109147787</span>/<span class="number">0.190036550164</span> G: <span class="number">2.20304942131</span> (Real: [<span class="number">3.9836783826351168</span>, <span class="number">1.4838718408508595</span>], Fake: [<span class="number">3.9491609585285188</span>, <span class="number">1.1700151592543104</span>]) </div><div class="line"><span class="number">15600</span>: D: <span class="number">1.01965582371</span>/<span class="number">0.519556045532</span> G: <span class="number">1.10594069958</span> (Real: [<span class="number">4.1213941669464109</span>, <span class="number">1.2398676800048194</span>], Fake: [<span class="number">4.1908504700660707</span>, <span class="number">1.1195751576139747</span>]) </div><div class="line"><span class="number">15800</span>: D: <span class="number">0.733263611794</span>/<span class="number">0.697221815586</span> G: <span class="number">0.84056687355</span> (Real: [<span class="number">4.0593542096018789</span>, <span class="number">1.1946663317303297</span>], Fake: [<span class="number">4.3031868946552274</span>, <span class="number">1.0306412415157991</span>]) </div><div class="line"><span class="number">16000</span>: D: <span class="number">0.400649875402</span>/<span class="number">0.377974271774</span> G: <span class="number">1.2899967432</span> (Real: [<span class="number">4.0140545344352718</span>, <span class="number">1.2630515897106358</span>], Fake: [<span class="number">4.1656066524982451</span>, <span class="number">1.1779954377184654</span>]) </div><div class="line"><span class="number">16200</span>: D: <span class="number">0.34089872241</span>/<span class="number">0.265896707773</span> G: <span class="number">1.11251270771</span> (Real: [<span class="number">4.0408088731765748</span>, <span class="number">1.3839176416694203</span>], Fake: [<span class="number">4.0593357777595518</span>, <span class="number">1.2213436233279213</span>]) </div><div class="line"><span class="number">16400</span>: D: <span class="number">0.00472234329209</span>/<span class="number">0.513436615467</span> G: <span class="number">1.63225841522</span> (Real: [<span class="number">4.1417997646331788</span>, <span class="number">1.2449733327544124</span>], Fake: [<span class="number">3.7269023895263671</span>, <span class="number">1.1296458384504016</span>]) </div><div class="line"><span class="number">16600</span>: D: <span class="number">0.756382524967</span>/<span class="number">0.66779255867</span> G: <span class="number">0.536718785763</span> (Real: [<span class="number">3.9379871004819869</span>, <span class="number">1.278594816781579</span>], Fake: [<span class="number">3.8750299978256226</span>, <span class="number">1.2829775944385431</span>]) </div><div class="line"><span class="number">16800</span>: D: <span class="number">0.879319548607</span>/<span class="number">0.169020995498</span> G: <span class="number">2.33787298203</span> (Real: [<span class="number">4.2075482982397077</span>, <span class="number">1.3725696551173026</span>], Fake: [<span class="number">3.6744112837314606</span>, <span class="number">1.3225226221432227</span>]) </div><div class="line"><span class="number">17000</span>: D: <span class="number">0.0482731573284</span>/<span class="number">1.43823099136</span> G: <span class="number">1.15067052841</span> (Real: [<span class="number">4.0404629743099214</span>, <span class="number">1.218948521692204</span>], Fake: [<span class="number">4.0387165582180025</span>, <span class="number">1.2794767516999943</span>]) </div><div class="line"><span class="number">17200</span>: D: <span class="number">2.88490628009e-05</span>/<span class="number">0.57872825861</span> G: <span class="number">0.495411038399</span> (Real: [<span class="number">3.9901529085636138</span>, <span class="number">1.4349120434336065</span>], Fake: [<span class="number">4.0573103535175328</span>, <span class="number">1.1918079188127153</span>]) </div><div class="line"><span class="number">17400</span>: D: <span class="number">0.231002807617</span>/<span class="number">1.2511702776</span> G: <span class="number">1.33606302738</span> (Real: [<span class="number">3.7472488379478452</span>, <span class="number">1.1658634335870959</span>], Fake: [<span class="number">3.9354779303073881</span>, <span class="number">1.2931455406139682</span>]) </div><div class="line"><span class="number">17600</span>: D: <span class="number">0.181431129575</span>/<span class="number">0.149175107479</span> G: <span class="number">2.51311731339</span> (Real: [<span class="number">4.1270963573455814</span>, <span class="number">1.312367798822683</span>], Fake: [<span class="number">4.3470913958549495</span>, <span class="number">1.1818067904116243</span>]) </div><div class="line"><span class="number">17800</span>: D: <span class="number">0.830040276051</span>/<span class="number">0.415931969881</span> G: <span class="number">1.57710897923</span> (Real: [<span class="number">3.99146986246109</span>, <span class="number">1.0836663745208763</span>], Fake: [<span class="number">4.3325731372833252</span>, <span class="number">1.266683405420135</span>]) </div><div class="line"><span class="number">18000</span>: D: <span class="number">0.20047518611</span>/<span class="number">0.460676729679</span> G: <span class="number">2.56421780586</span> (Real: [<span class="number">4.3388666504621503</span>, <span class="number">1.3881540592894346</span>], Fake: [<span class="number">3.9820314025878907</span>, <span class="number">1.0436684747098013</span>]) </div><div class="line"><span class="number">18200</span>: D: <span class="number">0.0659740716219</span>/<span class="number">0.428199917078</span> G: <span class="number">0.931035280228</span> (Real: [<span class="number">3.8892200005054476</span>, <span class="number">1.2217018988161374</span>], Fake: [<span class="number">3.8822696304321287</span>, <span class="number">1.304586899060783</span>]) </div><div class="line"><span class="number">18400</span>: D: <span class="number">0.791511416435</span>/<span class="number">0.56503880024</span> G: <span class="number">1.98549497128</span> (Real: [<span class="number">3.7894453473389147</span>, <span class="number">1.3567878969348022</span>], Fake: [<span class="number">4.0909739780426024</span>, <span class="number">1.2361544714927677</span>]) </div><div class="line"><span class="number">18600</span>: D: <span class="number">1.15297484398</span>/<span class="number">0.102882102132</span> G: <span class="number">1.85704553127</span> (Real: [<span class="number">4.2316720616817474</span>, <span class="number">1.2603607958456993</span>], Fake: [<span class="number">3.7415710711479186</span>, <span class="number">1.311454258421634</span>]) </div><div class="line"><span class="number">18800</span>: D: <span class="number">1.06078708172</span>/<span class="number">0.366641134024</span> G: <span class="number">0.914008259773</span> (Real: [<span class="number">3.9394708669185636</span>, <span class="number">1.2924449902046702</span>], Fake: [<span class="number">3.9466111737489702</span>, <span class="number">1.137776845711856</span>]) </div><div class="line"><span class="number">19000</span>: D: <span class="number">0.374139517546</span>/<span class="number">0.448283135891</span> G: <span class="number">0.701639294624</span> (Real: [<span class="number">3.9492650532722475</span>, <span class="number">1.2348435624999976</span>], Fake: [<span class="number">3.7365686148405075</span>, <span class="number">1.215777672310739</span>]) </div><div class="line"><span class="number">19200</span>: D: <span class="number">0.209440857172</span>/<span class="number">0.522395193577</span> G: <span class="number">0.707223057747</span> (Real: [<span class="number">3.8846979635953902</span>, <span class="number">1.2146658434075039</span>], Fake: [<span class="number">4.1696245861053463</span>, <span class="number">1.2979841463522084</span>]) </div><div class="line"><span class="number">19400</span>: D: <span class="number">0.15654887259</span>/<span class="number">0.133351936936</span> G: <span class="number">1.43907415867</span> (Real: [<span class="number">4.0292040088772776</span>, <span class="number">1.2291287794070285</span>], Fake: [<span class="number">3.8498308193683624</span>, <span class="number">1.1121767482065514</span>]) </div><div class="line"><span class="number">19600</span>: D: <span class="number">0.329566717148</span>/<span class="number">0.222448319197</span> G: <span class="number">0.429250627756</span> (Real: [<span class="number">3.7978928279876709</span>, <span class="number">1.1554982239517226</span>], Fake: [<span class="number">3.5122534275054931</span>, <span class="number">1.2462801759237472</span>]) </div><div class="line"><span class="number">19800</span>: D: <span class="number">0.0176634714007</span>/<span class="number">0.480926275253</span> G: <span class="number">0.39424943924</span> (Real: [<span class="number">4.0822606313228604</span>, <span class="number">1.2484518469881001</span>], Fake: [<span class="number">4.5482089626789097</span>, <span class="number">1.1266585202489452</span>]) </div><div class="line"><span class="number">20000</span>: D: <span class="number">0.45860773325</span>/<span class="number">0.517112135887</span> G: <span class="number">0.957448124886</span> (Real: [<span class="number">4.0875282829999922</span>, <span class="number">1.2310698313795749</span>], Fake: [<span class="number">4.2767848205566406</span>, <span class="number">1.1186856033319335</span>]) </div><div class="line"><span class="number">20200</span>: D: <span class="number">1.71172118187</span>/<span class="number">0.240745082498</span> G: <span class="number">0.314642876387</span> (Real: [<span class="number">3.8525538909435273</span>, <span class="number">1.2094100771830765</span>], Fake: [<span class="number">3.6543397814035417</span>, <span class="number">1.2917598911679764</span>]) </div><div class="line"><span class="number">20400</span>: D: <span class="number">0.583434104919</span>/<span class="number">0.703361749649</span> G: <span class="number">1.45571947098</span> (Real: [<span class="number">4.0388400733470915</span>, <span class="number">1.2267253073862441</span>], Fake: [<span class="number">3.9019298100471498</span>, <span class="number">1.0292402192122965</span>]) </div><div class="line"><span class="number">20600</span>: D: <span class="number">0.176266431808</span>/<span class="number">0.55411952734</span> G: <span class="number">0.962469100952</span> (Real: [<span class="number">4.0694609802961352</span>, <span class="number">1.2276659305759301</span>], Fake: [<span class="number">3.9728190612792971</span>, <span class="number">1.1212652107309595</span>]) </div><div class="line"><span class="number">20800</span>: D: <span class="number">1.17427504063</span>/<span class="number">0.212535098195</span> G: <span class="number">0.505771696568</span> (Real: [<span class="number">3.7983859290182589</span>, <span class="number">1.3565768879920506</span>], Fake: [<span class="number">4.0766829651594163</span>, <span class="number">1.1742807548541911</span>]) </div><div class="line"><span class="number">21000</span>: D: <span class="number">0.247546881437</span>/<span class="number">0.242251947522</span> G: <span class="number">2.533826828</span> (Real: [<span class="number">4.048124186992645</span>, <span class="number">1.2074367711533176</span>], Fake: [<span class="number">3.8443934541940687</span>, <span class="number">1.0964556009967605</span>]) </div><div class="line"><span class="number">21200</span>: D: <span class="number">0.000996549613774</span>/<span class="number">1.77280521393</span> G: <span class="number">0.741032421589</span> (Real: [<span class="number">3.8826335191726686</span>, <span class="number">1.3432952882949609</span>], Fake: [<span class="number">4.0052364200353621</span>, <span class="number">1.0658632049377181</span>]) </div><div class="line"><span class="number">21400</span>: D: <span class="number">0.0162861924618</span>/<span class="number">0.202122434974</span> G: <span class="number">0.640827775002</span> (Real: [<span class="number">3.949158318042755</span>, <span class="number">1.2312223613675215</span>], Fake: [<span class="number">3.9677765011787414</span>, <span class="number">1.1984950273079937</span>]) </div><div class="line"><span class="number">21600</span>: D: <span class="number">0.494586825371</span>/<span class="number">0.368914216757</span> G: <span class="number">1.73299539089</span> (Real: [<span class="number">4.2141097390651705</span>, <span class="number">1.3170628249721785</span>], Fake: [<span class="number">3.9259325069189073</span>, <span class="number">1.2402090610341174</span>]) </div><div class="line"><span class="number">21800</span>: D: <span class="number">1.72856020927</span>/<span class="number">0.280478566885</span> G: <span class="number">0.301942139864</span> (Real: [<span class="number">3.9425574642419816</span>, <span class="number">1.3421295277895979</span>], Fake: [<span class="number">4.1370714265108113</span>, <span class="number">1.3135434962232824</span>]) </div><div class="line"><span class="number">22000</span>: D: <span class="number">0.316263616085</span>/<span class="number">0.425417006016</span> G: <span class="number">4.6092467308</span> (Real: [<span class="number">3.9253722500801085</span>, <span class="number">1.1573266813219236</span>], Fake: [<span class="number">3.7590440094470976</span>, <span class="number">1.2176312271677099</span>]) </div><div class="line"><span class="number">22200</span>: D: <span class="number">1.70313096046</span>/<span class="number">0.166758075356</span> G: <span class="number">1.76803898811</span> (Real: [<span class="number">4.1788750314712528</span>, <span class="number">1.3796412025948377</span>], Fake: [<span class="number">4.4896411395072935</span>, <span class="number">0.88890948354147137</span>]) </div><div class="line"><span class="number">22400</span>: D: <span class="number">0.00245383195579</span>/<span class="number">0.618139982224</span> G: <span class="number">0.561835348606</span> (Real: [<span class="number">4.0531666296720505</span>, <span class="number">1.3030890495946361</span>], Fake: [<span class="number">3.9800510057806968</span>, <span class="number">1.2769573713555427</span>]) </div><div class="line"><span class="number">22600</span>: D: <span class="number">0.0456999950111</span>/<span class="number">0.270536243916</span> G: <span class="number">0.719259619713</span> (Real: [<span class="number">3.8036734467744826</span>, <span class="number">1.2489490089903446</span>], Fake: [<span class="number">4.2525720745325089</span>, <span class="number">1.3061806069103183</span>]) </div><div class="line"><span class="number">22800</span>: D: <span class="number">0.0318684391677</span>/<span class="number">0.34651991725</span> G: <span class="number">1.3301807642</span> (Real: [<span class="number">4.0768313544988635</span>, <span class="number">1.2930152979365797</span>], Fake: [<span class="number">4.4993063497543337</span>, <span class="number">1.2277717696258752</span>]) </div><div class="line"><span class="number">23000</span>: D: <span class="number">1.38112533092</span>/<span class="number">0.656377196312</span> G: <span class="number">0.700986683369</span> (Real: [<span class="number">4.0261077487468722</span>, <span class="number">1.1634786009859657</span>], Fake: [<span class="number">4.1274698692560197</span>, <span class="number">1.1909195549188023</span>]) </div><div class="line"><span class="number">23200</span>: D: <span class="number">0.7532761693</span>/<span class="number">0.30048418045</span> G: <span class="number">1.24321329594</span> (Real: [<span class="number">4.0255234652757643</span>, <span class="number">1.2277433432951119</span>], Fake: [<span class="number">4.0463824319839476</span>, <span class="number">1.2493841122917879</span>]) </div><div class="line"><span class="number">23400</span>: D: <span class="number">1.54497790337</span>/<span class="number">0.524266302586</span> G: <span class="number">1.88104653358</span> (Real: [<span class="number">4.1244187545776363</span>, <span class="number">1.2126284333800423</span>], Fake: [<span class="number">4.0199511092901226</span>, <span class="number">1.4125067136876193</span>]) </div><div class="line"><span class="number">23600</span>: D: <span class="number">0.838026106358</span>/<span class="number">1.1139113903</span> G: <span class="number">2.2735543251</span> (Real: [<span class="number">4.0352903008460999</span>, <span class="number">1.1687086536829701</span>], Fake: [<span class="number">4.5685070466995237</span>, <span class="number">1.4508884769834012</span>]) </div><div class="line"><span class="number">23800</span>: D: <span class="number">0.869914472103</span>/<span class="number">0.160864800215</span> G: <span class="number">1.42444908619</span> (Real: [<span class="number">4.1635012495517731</span>, <span class="number">1.1441051019240691</span>], Fake: [<span class="number">4.1520407730340958</span>, <span class="number">1.2022442680490875</span>]) </div><div class="line"><span class="number">24000</span>: D: <span class="number">0.0401677601039</span>/<span class="number">0.240127012134</span> G: <span class="number">1.21359109879</span> (Real: [<span class="number">4.0558859372138976</span>, <span class="number">1.1263029268841764</span>], Fake: [<span class="number">3.8535136532783509</span>, <span class="number">0.99055012605544335</span>]) </div><div class="line"><span class="number">24200</span>: D: <span class="number">0.444084912539</span>/<span class="number">0.761975646019</span> G: <span class="number">1.18176090717</span> (Real: [<span class="number">4.1462872040271757</span>, <span class="number">1.1670976588949802</span>], Fake: [<span class="number">4.0291124176979061</span>, <span class="number">1.4000525541431663</span>]) </div><div class="line"><span class="number">24400</span>: D: <span class="number">0.259448975325</span>/<span class="number">0.206390738487</span> G: <span class="number">0.850725114346</span> (Real: [<span class="number">4.2600694203376772</span>, <span class="number">1.3260391555100224</span>], Fake: [<span class="number">4.7161277580261229</span>, <span class="number">1.3763624799621637</span>]) </div><div class="line"><span class="number">24600</span>: D: <span class="number">0.821855664253</span>/<span class="number">0.381440609694</span> G: <span class="number">0.898442983627</span> (Real: [<span class="number">3.9929001557826997</span>, <span class="number">1.316718033939094</span>], Fake: [<span class="number">3.659836998283863</span>, <span class="number">1.033547623133473</span>]) </div><div class="line"><span class="number">24800</span>: D: <span class="number">0.869792580605</span>/<span class="number">0.143853545189</span> G: <span class="number">1.68244981766</span> (Real: [<span class="number">3.9503055346012115</span>, <span class="number">1.1980136516743376</span>], Fake: [<span class="number">4.3753550618886949</span>, <span class="number">1.4268488751378543</span>]) </div><div class="line"><span class="number">25000</span>: D: <span class="number">0.533834278584</span>/<span class="number">0.944993913174</span> G: <span class="number">1.35653877258</span> (Real: [<span class="number">3.8403973925113677</span>, <span class="number">1.1415226099240794</span>], Fake: [<span class="number">4.3022644245624546</span>, <span class="number">1.277824404897737</span>]) </div><div class="line"><span class="number">25200</span>: D: <span class="number">0.57686984539</span>/<span class="number">1.21011674404</span> G: <span class="number">0.49785476923</span> (Real: [<span class="number">4.1094828593730925</span>, <span class="number">1.0606124114518727</span>], Fake: [<span class="number">3.8350191235542299</span>, <span class="number">1.1822398134788241</span>]) </div><div class="line"><span class="number">25400</span>: D: <span class="number">1.30570268631</span>/<span class="number">0.127069279552</span> G: <span class="number">2.14658904076</span> (Real: [<span class="number">3.8440176880359651</span>, <span class="number">1.2759016439053388</span>], Fake: [<span class="number">4.2303895175457003</span>, <span class="number">1.2478330871411345</span>]) </div><div class="line"><span class="number">25600</span>: D: <span class="number">0.163877904415</span>/<span class="number">0.356351107359</span> G: <span class="number">1.50513041019</span> (Real: [<span class="number">3.9149920016527178</span>, <span class="number">1.3322359586431274</span>], Fake: [<span class="number">4.5107577931880947</span>, <span class="number">1.37733363996175</span>]) </div><div class="line"><span class="number">25800</span>: D: <span class="number">0.0257995054126</span>/<span class="number">0.501479804516</span> G: <span class="number">0.846267580986</span> (Real: [<span class="number">4.0328698861598973</span>, <span class="number">1.0891363228332751</span>], Fake: [<span class="number">4.2062628841400143</span>, <span class="number">1.2707193105443095</span>]) </div><div class="line"><span class="number">26000</span>: D: <span class="number">0.4208984375</span>/<span class="number">0.45090213418</span> G: <span class="number">1.24405300617</span> (Real: [<span class="number">4.0495267909765245</span>, <span class="number">1.3629959211491509</span>], Fake: [<span class="number">3.881335927248001</span>, <span class="number">1.1534035700479874</span>]) </div><div class="line"><span class="number">26200</span>: D: <span class="number">1.0977101326</span>/<span class="number">0.260044932365</span> G: <span class="number">0.274282753468</span> (Real: [<span class="number">4.0526520502567287</span>, <span class="number">1.1354404896569923</span>], Fake: [<span class="number">3.7989616423845289</span>, <span class="number">1.3036229409468019</span>]) </div><div class="line"><span class="number">26400</span>: D: <span class="number">0.836492598057</span>/<span class="number">0.194570705295</span> G: <span class="number">1.25769793987</span> (Real: [<span class="number">4.2580243301391603</span>, <span class="number">1.1229754918621602</span>], Fake: [<span class="number">4.9420129108428954</span>, <span class="number">1.4595622988211396</span>]) </div><div class="line"><span class="number">26600</span>: D: <span class="number">0.0381172671914</span>/<span class="number">0.229116663337</span> G: <span class="number">3.23367476463</span> (Real: [<span class="number">3.9871047949790954</span>, <span class="number">1.2891811878363044</span>], Fake: [<span class="number">5.5130027627944944</span>, <span class="number">1.3531596753079107</span>]) </div><div class="line"><span class="number">26800</span>: D: <span class="number">0.33750808239</span>/<span class="number">0.0588937625289</span> G: <span class="number">2.76632380486</span> (Real: [<span class="number">4.0901136839389798</span>, <span class="number">1.2240984948711151</span>], Fake: [<span class="number">5.9970619964599612</span>, <span class="number">1.3296608494175821</span>]) </div><div class="line"><span class="number">27000</span>: D: <span class="number">0.403919011354</span>/<span class="number">0.025144957006</span> G: <span class="number">5.00026988983</span> (Real: [<span class="number">3.9684947764873506</span>, <span class="number">1.1928812330565042</span>], Fake: [<span class="number">5.5821900677680967</span>, <span class="number">1.5869340992569609</span>]) </div><div class="line"><span class="number">27200</span>: D: <span class="number">1.26118826866</span>/<span class="number">1.14945113659</span> G: <span class="number">0.233536079526</span> (Real: [<span class="number">4.0953157800436024</span>, <span class="number">1.2000917970554563</span>], Fake: [<span class="number">3.457775202393532</span>, <span class="number">1.2362199991432059</span>]) </div><div class="line"><span class="number">27400</span>: D: <span class="number">0.842516124249</span>/<span class="number">0.577941656113</span> G: <span class="number">0.518706798553</span> (Real: [<span class="number">3.8673747038841246</span>, <span class="number">1.1826108239366226</span>], Fake: [<span class="number">3.6999527400732042</span>, <span class="number">1.2050256827670227</span>]) </div><div class="line"><span class="number">27600</span>: D: <span class="number">0.459548681974</span>/<span class="number">0.516558885574</span> G: <span class="number">1.69328427315</span> (Real: [<span class="number">4.0379843235015871</span>, <span class="number">1.267741160236167</span>], Fake: [<span class="number">4.3069088852405546</span>, <span class="number">1.2883256614455194</span>]) </div><div class="line"><span class="number">27800</span>: D: <span class="number">0.757292568684</span>/<span class="number">0.295852422714</span> G: <span class="number">0.82683211565</span> (Real: [<span class="number">3.6750951480865477</span>, <span class="number">1.1881818498282759</span>], Fake: [<span class="number">4.3079475378990173</span>, <span class="number">1.3863961893145142</span>]) </div><div class="line"><span class="number">28000</span>: D: <span class="number">1.0311729908</span>/<span class="number">0.836829304695</span> G: <span class="number">0.54562240839</span> (Real: [<span class="number">3.8109287106990815</span>, <span class="number">1.2699445078581264</span>], Fake: [<span class="number">4.0800623488426204</span>, <span class="number">1.2420579399013889</span>]) </div><div class="line"><span class="number">28200</span>: D: <span class="number">0.662180066109</span>/<span class="number">0.698618113995</span> G: <span class="number">0.430238395929</span> (Real: [<span class="number">3.8820258617401122</span>, <span class="number">1.3192879801078357</span>], Fake: [<span class="number">3.8678512275218964</span>, <span class="number">1.2100339116659864</span>]) </div><div class="line"><span class="number">28400</span>: D: <span class="number">0.857332766056</span>/<span class="number">0.637849986553</span> G: <span class="number">0.443328052759</span> (Real: [<span class="number">4.0044168281555175</span>, <span class="number">1.2977773729964786</span>], Fake: [<span class="number">3.77621297955513</span>, <span class="number">1.10884790779666</span>]) </div><div class="line"><span class="number">28600</span>: D: <span class="number">0.518617451191</span>/<span class="number">0.676390469074</span> G: <span class="number">0.824631929398</span> (Real: [<span class="number">3.9321113193035124</span>, <span class="number">1.189980080467403</span>], Fake: [<span class="number">4.1412628889083862</span>, <span class="number">1.4110153520360829</span>]) </div><div class="line"><span class="number">28800</span>: D: <span class="number">0.924657285213</span>/<span class="number">0.57682287693</span> G: <span class="number">0.867313206196</span> (Real: [<span class="number">3.8806186806410552</span>, <span class="number">1.2663798129949515</span>], Fake: [<span class="number">3.7928846073150635</span>, <span class="number">0.96599856269415929</span>]) </div><div class="line"><span class="number">29000</span>: D: <span class="number">0.681347727776</span>/<span class="number">0.833830595016</span> G: <span class="number">0.880895376205</span> (Real: [<span class="number">4.0122552135586735</span>, <span class="number">1.3382642859979685</span>], Fake: [<span class="number">3.8699622356891634</span>, <span class="number">1.5246898233773196</span>]) </div><div class="line"><span class="number">29200</span>: D: <span class="number">0.690975308418</span>/<span class="number">0.571468651295</span> G: <span class="number">0.539677977562</span> (Real: [<span class="number">3.9422134029865266</span>, <span class="number">1.2798402813873653</span>], Fake: [<span class="number">3.4796924066543578</span>, <span class="number">1.0078584415562459</span>]) </div><div class="line"><span class="number">29400</span>: D: <span class="number">0.600927650928</span>/<span class="number">0.692537486553</span> G: <span class="number">0.785535871983</span> (Real: [<span class="number">4.0494313037395475</span>, <span class="number">1.2729051468200046</span>], Fake: [<span class="number">4.0457676327228542</span>, <span class="number">1.2121629628604733</span>]) </div><div class="line"><span class="number">29600</span>: D: <span class="number">0.662378668785</span>/<span class="number">0.552553355694</span> G: <span class="number">0.665563106537</span> (Real: [<span class="number">3.8692034566402436</span>, <span class="number">1.1988600586203602</span>], Fake: [<span class="number">4.3626180648803707</span>, <span class="number">1.3098951956607312</span>]) </div><div class="line"><span class="number">29800</span>: D: <span class="number">0.844242811203</span>/<span class="number">0.719559967518</span> G: <span class="number">0.89226102829</span> (Real: [<span class="number">3.8751950478553772</span>, <span class="number">1.1053984789259368</span>], Fake: [<span class="number">3.9671442759037019</span>, <span class="number">1.1584875699071935</span>])</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/" itemprop="url">LSTM by Example using Tensorflow (Text Generate)</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-26T21:05:01+08:00">2017-04-26 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/26/LSTM-by-Example-using-Tensorflow-Text-Generate/" class="leancloud_visitors" data-flag-title="LSTM by Example using Tensorflow (Text Generate)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In Deep Learning, Recurrent Neural Networks (RNN) are a family of neural networks that excels in learning from sequential data. A class of RNN that has found practical applications is Long Short-Term Memory (LSTM) because it is robust against the problems of long-term dependency.</p><p>What seems to be lacking is a good documentation and example on how to build an easy to understand Tensorflow application based on LSTM. This is the motivation behind this article.</p><p>Suppose we want to train a LSTM to predict the next word using a sample short story, <a href="http://www.taleswithmorals.com/" target="_blank" rel="external">Aesop’s Fables</a>:</p><blockquote><p>long ago , the mice had a general council to consider what measures they could take to outwit their common enemy , the cat . some said this , and some said that but at last a young mouse got up and said he had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in the sly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easily escape from her . i venture , therefore , to propose that a small bell be procured , and attached by a ribbon round the neck of the cat . by this means we should always know when she was about , and could easily retire while she was in the neighbourhood . this proposal met with general applause , until an old mouse got up and said that is all very well , but who is to bell the cat ? the mice looked at one another and nobody spoke . then the old mouse said it is easy to propose impossible remedies .</p></blockquote><p>If we feed a LSTM with correct sequences from the text of 3 symbols as inputs and 1 labeled symbol, eventually the neural network will learn to predict the next symbol correctly.</p><p><img src="https://cdn-images-1.medium.com/max/800/1*epcf2SBjRHBynBNFf-CpQA.png" alt="lstm"></p><p>Technically, LSTM inputs can only understand real numbers. A way to convert symbol to number is to assign a unique integer to each symbol based on frequency of occurrence. For example, there are 112 unique symbols in the text above. The function in Listing 2 builds a dictionary with the following entries [ “,” : 0 ][ “the” : 1 ], …, [ “council” : 37 ],…,[ “spoke” : 111 ]. The reverse dictionary is also generated since it will be used in decoding the output of LSTM.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words)</span>:</span></div><div class="line">    count = collections.Counter(words).most_common()</div><div class="line">    dictionary = dict()</div><div class="line">    <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</div><div class="line">        dictionary[word] = len(dictionary)</div><div class="line">    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</div><div class="line">    <span class="keyword">return</span> dictionary, reverse_dictionary</div></pre></td></tr></table></figure><p>Similarly, the prediction is a unique integer identifying the index in the reverse dictionary of the predicted symbol. For example, if the prediction is 37, the predicted symbol is actually “council”.</p><p>The generation of output may sound simple but actually LSTM produces a 112-element vector of probabilities of prediction for the next symbol normalized by the softmax() function. The index of the element with the highest probability is the predicted index of the symbol in the reverse dictionary (ie a one-hot vector).</p><p><img src="https://cdn-images-1.medium.com/max/800/1*XAJdt_EbedqDlrTT9eqWvQ.png" alt="word-gen"></p><p>There is the source code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''</span></div><div class="line">A Recurrent Neural Network (LSTM) implementation example using TensorFlow..</div><div class="line">Next word prediction after n_input words learned from text file.</div><div class="line">A story is automatically generated if the predicted word is fed back as input.</div><div class="line">Author: Rowel Atienza</div><div class="line">Project: https://github.com/roatienza/Deep-Learning-Experiments</div><div class="line">'''</div><div class="line"></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> collections</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">start_time = time.time()</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">elapsed</span><span class="params">(sec)</span>:</span></div><div class="line">    <span class="keyword">if</span> sec&lt;<span class="number">60</span>:</div><div class="line">        <span class="keyword">return</span> str(sec) + <span class="string">" sec"</span></div><div class="line">    <span class="keyword">elif</span> sec&lt;(<span class="number">60</span>*<span class="number">60</span>):</div><div class="line">        <span class="keyword">return</span> str(sec/<span class="number">60</span>) + <span class="string">" min"</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> str(sec/(<span class="number">60</span>*<span class="number">60</span>)) + <span class="string">" hr"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Target log path</span></div><div class="line">logs_path = <span class="string">'/tmp/tensorflow/rnn_words'</span></div><div class="line">writer = tf.summary.FileWriter(logs_path)</div><div class="line"></div><div class="line"><span class="comment"># Text file containing words for training</span></div><div class="line">training_file = <span class="string">'belling_the_cat.txt'</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(fname)</span>:</span></div><div class="line">    <span class="keyword">with</span> open(fname) <span class="keyword">as</span> f:</div><div class="line">        content = f.readlines()</div><div class="line">    content = [x.strip() <span class="keyword">for</span> x <span class="keyword">in</span> content]</div><div class="line">    content = [content[i].split() <span class="keyword">for</span> i <span class="keyword">in</span> range(len(content))]</div><div class="line">    content = np.array(content)</div><div class="line">    content = np.reshape(content, [<span class="number">-1</span>, ])</div><div class="line">    <span class="keyword">return</span> content</div><div class="line"></div><div class="line">training_data = read_data(training_file)</div><div class="line">print(<span class="string">"Loaded training data..."</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset</span><span class="params">(words)</span>:</span></div><div class="line">    count = collections.Counter(words).most_common()</div><div class="line">    dictionary = dict()</div><div class="line">    <span class="keyword">for</span> word, _ <span class="keyword">in</span> count:</div><div class="line">        dictionary[word] = len(dictionary)</div><div class="line">    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))</div><div class="line">    <span class="keyword">return</span> dictionary, reverse_dictionary</div><div class="line"></div><div class="line">dictionary, reverse_dictionary = build_dataset(training_data)</div><div class="line">vocab_size = len(dictionary)</div><div class="line"></div><div class="line"><span class="comment"># Parameters</span></div><div class="line">learning_rate = <span class="number">0.001</span></div><div class="line">training_iters = <span class="number">50000</span></div><div class="line">display_step = <span class="number">1000</span></div><div class="line">n_input = <span class="number">3</span></div><div class="line"></div><div class="line"><span class="comment"># number of units in RNN cell</span></div><div class="line">n_hidden = <span class="number">512</span></div><div class="line"></div><div class="line"><span class="comment"># tf Graph input</span></div><div class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_input, <span class="number">1</span>])</div><div class="line">y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, vocab_size])</div><div class="line"></div><div class="line"><span class="comment"># RNN output node weights and biases</span></div><div class="line">weights = &#123;</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden, vocab_size]))</div><div class="line">&#125;</div><div class="line">biases = &#123;</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([vocab_size]))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">RNN</span><span class="params">(x, weights, biases)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># reshape to [1, n_input]</span></div><div class="line">    x = tf.reshape(x, [<span class="number">-1</span>, n_input])</div><div class="line"></div><div class="line">    <span class="comment"># Generate a n_input-element sequence of inputs</span></div><div class="line">    <span class="comment"># (eg. [had] [a] [general] -&gt; [20] [6] [33])</span></div><div class="line">    x = tf.split(x,n_input,<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 2-layer LSTM, each layer has n_hidden units.</span></div><div class="line">    <span class="comment"># Average Accuracy= 95.20% at 50k iter</span></div><div class="line">    </div><div class="line">    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])</div><div class="line"></div><div class="line">    <span class="comment"># 1-layer LSTM with n_hidden units but with lower accuracy.</span></div><div class="line">    <span class="comment"># Average Accuracy= 90.60% 50k iter</span></div><div class="line">    <span class="comment"># Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above</span></div><div class="line">    <span class="comment"># rnn_cell = rnn.BasicLSTMCell(n_hidden)</span></div><div class="line"></div><div class="line">    <span class="comment"># generate prediction</span></div><div class="line">    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)</div><div class="line"></div><div class="line">    <span class="comment"># there are n_input outputs but</span></div><div class="line">    <span class="comment"># we only want the last output</span></div><div class="line">    <span class="keyword">return</span> tf.matmul(outputs[<span class="number">-1</span>], weights[<span class="string">'out'</span>]) + biases[<span class="string">'out'</span>]</div><div class="line"></div><div class="line">pred = RNN(x, weights, biases)</div><div class="line"></div><div class="line"><span class="comment"># Loss and optimizer</span></div><div class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))</div><div class="line">optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)</div><div class="line"></div><div class="line"><span class="comment"># Model evaluation</span></div><div class="line">correct_pred = tf.equal(tf.argmax(pred,<span class="number">1</span>), tf.argmax(y,<span class="number">1</span>))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</div><div class="line"></div><div class="line"><span class="comment"># Initializing the variables</span></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line"><span class="comment"># Launch the graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</div><div class="line">    session.run(init)</div><div class="line">    step = <span class="number">0</span></div><div class="line">    offset = random.randint(<span class="number">0</span>,n_input+<span class="number">1</span>)</div><div class="line">    end_offset = n_input + <span class="number">1</span></div><div class="line">    acc_total = <span class="number">0</span></div><div class="line">    loss_total = <span class="number">0</span></div><div class="line"></div><div class="line">    writer.add_graph(session.graph)</div><div class="line"></div><div class="line">    <span class="keyword">while</span> step &lt; training_iters:</div><div class="line">        <span class="comment"># Generate a minibatch. Add some randomness on selection process.</span></div><div class="line">        <span class="keyword">if</span> offset &gt; (len(training_data)-end_offset):</div><div class="line">            offset = random.randint(<span class="number">0</span>, n_input+<span class="number">1</span>)</div><div class="line"></div><div class="line">        symbols_in_keys = [ [dictionary[ str(training_data[i])]] <span class="keyword">for</span> i <span class="keyword">in</span> range(offset, offset+n_input) ]</div><div class="line">        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [<span class="number">-1</span>, n_input, <span class="number">1</span>])</div><div class="line"></div><div class="line">        symbols_out_onehot = np.zeros([vocab_size], dtype=float)</div><div class="line">        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = <span class="number">1.0</span></div><div class="line">        symbols_out_onehot = np.reshape(symbols_out_onehot,[<span class="number">1</span>,<span class="number">-1</span>])</div><div class="line"></div><div class="line">        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \</div><div class="line">                                                feed_dict=&#123;x: symbols_in_keys, y: symbols_out_onehot&#125;)</div><div class="line">        loss_total += loss</div><div class="line">        acc_total += acc</div><div class="line">        <span class="keyword">if</span> (step+<span class="number">1</span>) % display_step == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"Iter= "</span> + str(step+<span class="number">1</span>) + <span class="string">", Average Loss= "</span> + \</div><div class="line">                  <span class="string">"&#123;:.6f&#125;"</span>.format(loss_total/display_step) + <span class="string">", Average Accuracy= "</span> + \</div><div class="line">                  <span class="string">"&#123;:.2f&#125;%"</span>.format(<span class="number">100</span>*acc_total/display_step))</div><div class="line">            acc_total = <span class="number">0</span></div><div class="line">            loss_total = <span class="number">0</span></div><div class="line">            symbols_in = [training_data[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(offset, offset + n_input)]</div><div class="line">            symbols_out = training_data[offset + n_input]</div><div class="line">            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, <span class="number">1</span>).eval())]</div><div class="line">            print(<span class="string">"%s - [%s] vs [%s]"</span> % (symbols_in,symbols_out,symbols_out_pred))</div><div class="line">        step += <span class="number">1</span></div><div class="line">        offset += (n_input+<span class="number">1</span>)</div><div class="line">    print(<span class="string">"Optimization Finished!"</span>)</div><div class="line">    print(<span class="string">"Elapsed time: "</span>, elapsed(time.time() - start_time))</div><div class="line">    print(<span class="string">"Run on command line."</span>)</div><div class="line">    print(<span class="string">"\ttensorboard --logdir=%s"</span> % (logs_path))</div><div class="line">    print(<span class="string">"Point your web browser to: http://localhost:6006/"</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        prompt = <span class="string">"%s words: "</span> % n_input</div><div class="line">        sentence = input(prompt)</div><div class="line">        sentence = sentence.strip()</div><div class="line">        words = sentence.split(<span class="string">' '</span>)</div><div class="line">        <span class="keyword">if</span> len(words) != n_input:</div><div class="line">            <span class="keyword">continue</span></div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            symbols_in_keys = [dictionary[str(words[i])] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(words))]</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</div><div class="line">                keys = np.reshape(np.array(symbols_in_keys), [<span class="number">-1</span>, n_input, <span class="number">1</span>])</div><div class="line">                onehot_pred = session.run(pred, feed_dict=&#123;x: keys&#125;)</div><div class="line">                onehot_pred_index = int(tf.argmax(onehot_pred, <span class="number">1</span>).eval())</div><div class="line">                sentence = <span class="string">"%s %s"</span> % (sentence,reverse_dictionary[onehot_pred_index])</div><div class="line">                symbols_in_keys = symbols_in_keys[<span class="number">1</span>:]</div><div class="line">                symbols_in_keys.append(onehot_pred_index)</div><div class="line">            print(sentence)</div><div class="line">        <span class="keyword">except</span>:</div><div class="line">            print(<span class="string">"Word not in dictionary"</span>)</div></pre></td></tr></table></figure><p><strong>source blog:</strong> <a href="https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537" target="_blank" rel="external">https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537</a></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/14/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/16/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">124</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">59</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>