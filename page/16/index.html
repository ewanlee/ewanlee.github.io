<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/16/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/16/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/23/Solution-for-Bracket-in-markdown-link-address/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/23/Solution-for-Bracket-in-markdown-link-address/" itemprop="url">Solution for Bracket in markdown link address</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-23T19:56:40+08:00">2017-04-23 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/23/Solution-for-Bracket-in-markdown-link-address/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/23/Solution-for-Bracket-in-markdown-link-address/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/23/Solution-for-Bracket-in-markdown-link-address/" class="leancloud_visitors" data-flag-title="Solution for Bracket in markdown link address"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Markdown创造一个链接或者图片是使用 <code>[title](link)</code> 和 <code>![title](link)</code>.</p><p>我们可以避免<code>[]</code>内出现中括号, 或者使用转义.</p><p>但是在小括号的链接里面就可能会出问题. 有些网址上面会具有小括号. 例如,</p><p><a href="https://github.com/CjTouzi/Learning-RSpark/blob/master/Zaharia%20M.%2C%20et%20al.%20Learning%20Spark%20%28O%27Reilly%2C%202015%29%28274s%29.pdf" target="_blank" rel="external">https://github.com/CjTouzi/Learning-RSpark/blob/master/Zaharia%20M.%2C%20et%20al.%20Learning%20Spark%20%28O%27Reilly%2C%202015%29%28274s%29.pdf</a></p><p>解决方法:</p><p><code>%28</code> 代替<code>(</code>, <code>%29</code>代替<code>)</code> 主要是后者会歧义链接部分的结束. 这是使用url符号码去代替ascii的符号. 能够解决这个问题</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/23/The-first-course-of-the-Spark/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/23/The-first-course-of-the-Spark/" itemprop="url">The first course of the Spark</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-23T17:37:04+08:00">2017-04-23 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/23/The-first-course-of-the-Spark/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/23/The-first-course-of-the-Spark/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/23/The-first-course-of-the-Spark/" class="leancloud_visitors" data-flag-title="The first course of the Spark"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><ul><li>扩充了MapReduce计算模型</li><li>基于内存的计算</li><li>能够进行批处理、迭代式计算、交互查询和流处理<ul><li>降低里维护成本</li></ul></li><li>提供了Python、Java、Scala、SQL的API和丰富的内置库</li><li>可以与Hadoop、Kafka等整合</li></ul><h1 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h1><p><img src="http://o7ie0tcjk.bkt.clouddn.com/spark/first_course/spark_components.png" alt="components"></p><h2 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h2><p>Spark Core contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. Spark Core is also home to the API that defines <em>resilient distributed datasets</em> (RDDs), which are Spark’s main programming abstraction. RDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.</p><h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><p>Spark SQL is Spark’s package for working with structured data. It allows querying data via SQL as well as the Apache Hive variant of SQL—called the Hive Query Language (HQL)—and it supports many sources of data, including Hive tables, Parquet, and JSON. Beyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java, and Scala, all within a single application, thus combining SQL with complex analytics. This tight integration with the rich computing environment provided by Spark makes Spark SQL unlike any other open source data warehouse tool. Spark SQL was added to Spark in version 1.0.</p><p>Shark was an older SQL-on-Spark project out of the University of California, Berkeley, that modified Apache Hive to run on Spark. It has now been replaced by Spark SQL to provide better integration with the Spark engine and language APIs.</p><h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><p>Spark Streaming is a Spark component that enables processing of live streams of data. Examples of data streams include logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service. Spark Streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API, making it easy for programmers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. Underneath its API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.</p><h2 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h2><p>Spark comes with a library containing common machine learning (ML) functionality, called MLlib. MLlib provides multiple types of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as supporting functionality such as model evaluation and data import. It also provides some lower-level ML primitives, including a generic gradient descent optimization algorithm. All of these methods are designed to scale out across a cluster.</p><h2 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h2><p>GraphX is a library for manipulating graphs (e.g., a social network’s friend graph) and performing graph-parallel computations. Like Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. GraphX also provides various operators for manipulating graphs (e.g., <code>subgraph</code> and <code>mapVertices</code>) and a library of common graph algorithms (e.g., PageRank and triangle counting).</p><h2 id="Cluster-Managers"><a href="#Cluster-Managers" class="headerlink" title="Cluster Managers"></a>Cluster Managers</h2><p>Under the hood, Spark is designed to efficiently scale up from one to many thousands of compute nodes. To achieve this while maximizing flexibility, Spark can run over a variety of <em>cluster managers</em>, including Hadoop YARN, Apache Mesos, and a simple cluster manager included in Spark itself called the Standalone Scheduler. If you are just installing Spark on an empty set of machines, the Standalone Scheduler provides an easy way to get started; if you already have a Hadoop YARN or Mesos cluster, however, Spark’s support for these cluster managers allows your applications to also run on them.</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ul><li>Spark由Scala编写，运行于JVM上，运行环境为Java 7+</li><li>如果使用Python API，需要安装Python 2.6+ 或者Python 3.4+</li><li>Spark 1.6.2 – Scala 2.10 / Spark 2.0.0 – Scala 2.11</li></ul><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">http://spark.apache.org/downloads.html</a></p><p>不需要Hadoop集群；如果已经搭建好Hadoop集群，可下载相应版本</p><h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>README.md<ul><li>Contains short instructions for getting started with Spark.</li></ul></li><li>bin<ul><li>Contains executable files that can be used to interact with Spark in various ways (e.g., the Spark shell, which we will cover later in this chapter).</li></ul></li><li>core, streaming, python, …<ul><li>Contains the source code of major components of the Spark project.</li></ul></li><li>examples<ul><li>Contains some helpful Spark standalone jobs that you can look at and run tolearn about the Spark API.</li></ul></li></ul><h2 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h2><ul><li>Python Shell<ul><li><code>bin/pyspark</code></li></ul></li><li>Scala Shell<ul><li><code>bin/spark-shell</code></li></ul></li></ul><h1 id="开发环境搭建"><a href="#开发环境搭建" class="headerlink" title="开发环境搭建"></a>开发环境搭建</h1><h2 id="Scala安装"><a href="#Scala安装" class="headerlink" title="Scala安装"></a>Scala安装</h2><p><a href="https://www.scala-lang.org/download/" target="_blank" rel="external">https://www.scala-lang.org/download/</a></p><p>注意版本对应</p><h2 id="IntelliJ-IDEA安装"><a href="#IntelliJ-IDEA安装" class="headerlink" title="IntelliJ IDEA安装"></a>IntelliJ IDEA安装</h2><p><a href="https://www.jetbrains.com/idea/#chooseYourEdition" target="_blank" rel="external">https://www.jetbrains.com/idea/#chooseYourEdition</a></p><p>可以申请教育账号</p><h3 id="插件安装"><a href="#插件安装" class="headerlink" title="插件安装"></a>插件安装</h3><p><code>File-Settings-Plugins</code> 搜索Scala，安装</p><h3 id="项目创建"><a href="#项目创建" class="headerlink" title="项目创建"></a>项目创建</h3><p><code>File-New-Project-Scala-SBT</code></p><p>同样注意版本匹配（这里用的是Spark 2.1.0, Scala 2.11.11）</p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>需要定义使用的Spark版本</p><p><code>build.sbt</code>追加</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">libraryDependencies ++= <span class="type">Seq</span>(</div><div class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-core"</span> % <span class="string">"2.1.0"</span></div><div class="line">)</div></pre></td></tr></table></figure><p>重建项目即可</p><h3 id="源程序编写"><a href="#源程序编写" class="headerlink" title="源程序编写"></a>源程序编写</h3><p><code>New-Scala Class-Class to Object</code></p><p><code>WordCount.scala</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</div><div class="line"><span class="comment">/**</span></div><div class="line">  * Created by root on 4/23/17.</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"wordcount"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> input = sc.textFile(<span class="string">"/home/hduser/Anaconda2-4.3.1-Linux-x86_64.sh"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> lines = input.flatMap(line =&gt; line.split(<span class="string">" "</span>))</div><div class="line">    <span class="keyword">val</span> count = lines.map(word =&gt; (word, <span class="number">1</span>)).reduceByKey&#123;<span class="keyword">case</span> (x, y) =&gt; x + y&#125;</div><div class="line">    <span class="keyword">val</span> output = count.saveAsTextFile(<span class="string">"/home/hduser/scala_wordcount_demo_output"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h3><p><code>File-Project Structure-Project Setting-Artifacts-Add-JAR-From modules with dependencies</code></p><p><code>Build-Build Artifacts-Build</code></p><h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><ul><li>启动master<ul><li><code>sbin/start-master.sh</code></li></ul></li><li>启动worker<ul><li><code>bin/spark-class org.apache.spark.deploy.worker.Worker spark://Ubuntu:7077</code></li><li>注意这里的spark服务器地址可以通过浏览器输入<code>localhost:8080</code>来查看</li></ul></li><li>提交作业<ul><li><code>bin/spark-submit --master spark://Ubuntu:7077 --class WordCount /home/hduser/scala_demo.jar</code></li><li>注意这里的<code>scala_demo.jar</code>文件为打包阶段生成</li></ul></li></ul><h1 id="TODO-RDDs"><a href="#TODO-RDDs" class="headerlink" title="TODO: RDDs"></a>TODO: RDDs</h1><p><a href="https://github.com/CjTouzi/Learning-RSpark/blob/master/Zaharia%20M.%2C%20et%20al.%20Learning%20Spark%20%28O%27Reilly%2C%202015%29%28274s%29.pdf" target="_blank" rel="external">Zaharia M., et al. Learning Spark (O’Reilly, 2015)(274s).pdf</a></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>慕课网 <a href="http://www.imooc.com/learn/814" target="_blank" rel="external">http://www.imooc.com/learn/814</a></li><li>Zaharia M., et al. Learning Spark (O’Reilly, 2015)(274s).pdf</li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/21/Implement-k-means-on-the-hadoop-platform/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/21/Implement-k-means-on-the-hadoop-platform/" itemprop="url">Implement k-means on the hadoop platform</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-21T13:44:53+08:00">2017-04-21 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/21/Implement-k-means-on-the-hadoop-platform/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/21/Implement-k-means-on-the-hadoop-platform/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/21/Implement-k-means-on-the-hadoop-platform/" class="leancloud_visitors" data-flag-title="Implement k-means on the hadoop platform"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>首先在单机上搭一个伪分布式环境，主要是对<code>*-site.xml</code>配置文件进行修改，具体修改如下：</p><p><code>core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0"?&gt;</div><div class="line"><span class="comment">&lt;!-- core-site.xml --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0"?&gt;</div><div class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0"?&gt;</div><div class="line"><span class="comment">&lt;!-- yarn-site.xml --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0"?&gt;</div><div class="line"><span class="comment">&lt;!-- mapred-site.xml --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p>然后启动<code>hadoop</code>，启动的流程如下：</p><ol><li><code>start-dfs.sh</code></li><li><code>start-yarn.sh</code></li><li><code>mr-jobhistory-daemon.sh start historyserver</code></li></ol><p><strong>注意，以上命令能够得到正确执行的前提是已经将<code>hadoop</code>的安装目录下的<code>bin</code>目录加到环境变量中</strong></p><p>由于很久没有使用<code>Java</code>，所以采用<code>Python</code>实现，这里需要用到一个<code>package</code>，也就是<code>mrjob</code></p><p>首先计划一下实现步骤：</p><p>[ Mapper ]</p><p><strong>Accepts</strong></p><ul><li>data</li><li>global constant representing the list of centers</li></ul><p><strong>Computes</strong></p><ul><li>the nearest center for each data instance</li></ul><p><strong>Emits</strong></p><ul><li>nearest centers (<strong>key</strong>) and points (<strong>value</strong>).</li></ul><hr><p>[ Reducer ]</p><p><strong>Accepts</strong></p><ul><li>center instance / coordinate (<strong>key</strong>)</li><li>points (<strong>value</strong>)</li></ul><p><strong>Computes</strong></p><ul><li>the new centers based on clusters</li></ul><p><strong>Emits</strong></p><ul><li>new centers</li></ul><p>You will provide the next epoch of K-Means with:</p><ol><li>the same data from your initial epoch</li><li>the centers emitted from the reducer as global constants</li></ol><p>Repeat until your stopping criteria are met.</p><p>如果要用<code>Python</code>进行相关的<code>Hadoop</code>操作的话，肯定是要使用<code>hadoop streaming</code>的，但是存在一个问题，也就是<code>streaming</code>流程只能跑一遍，但是很显然，作为一个<code>machine learning</code>算法，<code>k-means</code>是类似于<code>EM</code>算法要经过多步迭代的，那么最容易想到的就是使用shell脚本多次调用相关命令，但是这样显得十分<code>ugly</code>，因此可以采用<code>mrjob</code>包来帮助我们完成这个工作。</p><p>从上面看来，我们需要两个文件，一个是<code>python</code>实现的<code>map-reduce</code>，另一个是<code>mrjob</code>的<code>job</code>文件，相当于<code>master</code>，下面列出这两个文件，因为实现比较简单，因此不作过多解释.</p><p><code>kmeans.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</div><div class="line"><span class="keyword">import</span> mrjob</div><div class="line"><span class="comment"># MRJob is a python class which will be overloaded</span></div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MRKMeans</span><span class="params">(MRJob)</span>:</span></div><div class="line"></div><div class="line">    SORT_VALUES = <span class="keyword">True</span></div><div class="line">    OUTPUT_PROTOCOL = mrjob.protocol.RawProtocol</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dist_vec</span><span class="params">(self, v1, v2)</span>:</span></div><div class="line">        <span class="comment"># calculate the ditance between two vectors (in two dimensions)</span></div><div class="line">        <span class="keyword">return</span> sqrt((v2[<span class="number">0</span>] - v1[<span class="number">0</span>]) * (v2[<span class="number">0</span>] - v1[<span class="number">0</span>]) + (v2[<span class="number">1</span>] - v1[<span class="number">1</span>]) * (v2[<span class="number">1</span>] - v1[<span class="number">1</span>]))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">configure_options</span><span class="params">(self)</span>:</span></div><div class="line">        super(MRKMeans, self).configure_options()</div><div class="line">        <span class="comment"># the line below define that the file folowing the --c option is the</span></div><div class="line">        <span class="comment"># centroid and is loadable</span></div><div class="line">        self.add_file_option(<span class="string">'--c'</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_centroids</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Definition : extracts centroids from the centroids file define afetr --c flag</div><div class="line">        Out : Return the list of centroids</div><div class="line">        """</div><div class="line">        <span class="comment"># self.options.c is the name of the file following --c option</span></div><div class="line">        f = open(self.options.c, <span class="string">'r'</span>)</div><div class="line">        centroids = []</div><div class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.read().split(<span class="string">'\n'</span>):</div><div class="line">            <span class="keyword">if</span> line:</div><div class="line">                x, y = line.split(<span class="string">', '</span>)</div><div class="line">                centroids.append([float(x), float(y)])</div><div class="line">        f.close()</div><div class="line">        <span class="keyword">return</span> centroids</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, _, lines)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Definition : Mapper take centroids extract form get_centroids()</div><div class="line">        and the point cloud and for each point, calculate the distance</div><div class="line">        to the centroids, find the mininum of it</div><div class="line">        Out : yield the point with it's class</div><div class="line">        """</div><div class="line">        centroids = self.get_centroids()</div><div class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> lines.split(<span class="string">'\n'</span>):</div><div class="line">            x, y = l.split(<span class="string">', '</span>)</div><div class="line">            point = [float(x), float(y)]</div><div class="line">            min_dist = <span class="number">100000000.0</span></div><div class="line">            classe = <span class="number">0</span></div><div class="line">            <span class="comment"># iterate over the centroids (Here we know that we are doing a 3means)</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">                dist = self.dist_vec(point, centroids[i])</div><div class="line">                <span class="keyword">if</span> dist &lt; min_dist:</div><div class="line">                    min_dist = dist</div><div class="line">                    classe = i</div><div class="line">            <span class="keyword">yield</span> classe, point</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combiner</span><span class="params">(self, k, v)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Definition : Calculate for each class, at the end of the mapper,</div><div class="line">        before reducer, the medium point of each class</div><div class="line">        Out: return for each class, the centroids for each mapper</div><div class="line">        """</div><div class="line">        count = <span class="number">0</span></div><div class="line">        moy_x = moy_y = <span class="number">0.0</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> v:</div><div class="line">            count += <span class="number">1</span></div><div class="line">            moy_x += t[<span class="number">0</span>]</div><div class="line">            moy_y += t[<span class="number">1</span>]</div><div class="line">        <span class="keyword">yield</span> k, (moy_x / count, moy_y / count)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, k, v)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Definition : for each class, get all the tmp centroids from each</div><div class="line">        combiner and calculate the new centroids.</div><div class="line">        """</div><div class="line">        <span class="comment"># k is class and v are medium points linked to the class</span></div><div class="line">        count = <span class="number">0</span></div><div class="line">        moy_x = moy_y = <span class="number">0.0</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> v:</div><div class="line">            count += <span class="number">1</span></div><div class="line">            moy_x += t[<span class="number">0</span>]</div><div class="line">            moy_y += t[<span class="number">1</span>]</div><div class="line">        <span class="keyword">print</span> str(k) + <span class="string">", "</span> + str(moy_x / count) + <span class="string">", "</span> + str(moy_y / count)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="comment"># just run mapreduce !</span></div><div class="line">    MRKMeans.run()</div></pre></td></tr></table></figure><p><code>main.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</div><div class="line"><span class="keyword">from</span> kmeans <span class="keyword">import</span> MRKMeans</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> os.path</div><div class="line"><span class="keyword">import</span> shutil</div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">input_c = <span class="string">"centroids"</span></div><div class="line"></div><div class="line">CENTROIDS_FILE = <span class="string">"/home/hduser/tmp/centroid"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_c</span><span class="params">(job, runner)</span>:</span></div><div class="line">    c = []</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> runner.stream_output():</div><div class="line">        <span class="comment"># print "stream_output: ", line</span></div><div class="line">        key, value = job.parse_output_line(line)</div><div class="line">        c.append(key)</div><div class="line">    <span class="keyword">return</span> c</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_first_c</span><span class="params">(fname)</span>:</span></div><div class="line">    f = open(fname, <span class="string">'r'</span>)</div><div class="line">    centroids = []</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.read().split(<span class="string">'\n'</span>):</div><div class="line">        <span class="keyword">if</span> line:</div><div class="line">            x, y = line.split(<span class="string">', '</span>)</div><div class="line">            centroids.append([float(x), float(y)])</div><div class="line">    f.close()</div><div class="line">    <span class="keyword">return</span> centroids</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_c</span><span class="params">(centroids)</span>:</span></div><div class="line">    f = open(CENTROIDS_FILE, <span class="string">"w"</span>)</div><div class="line">    centroids.sort()</div><div class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> centroids:</div><div class="line">        k, cx, cy = c.split(<span class="string">', '</span>)</div><div class="line">        <span class="comment"># print c</span></div><div class="line">        f.write(<span class="string">"%s, %s\n"</span> % (cx, cy))</div><div class="line">    f.close()</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist_vec</span><span class="params">(v1, v2)</span>:</span></div><div class="line">    <span class="keyword">return</span> sqrt((v2[<span class="number">0</span>] - v1[<span class="number">0</span>]) * (v2[<span class="number">0</span>] - v1[<span class="number">0</span>]) + (v2[<span class="number">1</span>] - v1[<span class="number">1</span>]) * (v2[<span class="number">1</span>] - v1[<span class="number">1</span>]))</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">diff</span><span class="params">(cs1, cs2)</span>:</span></div><div class="line">    max_dist = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">        dist = dist_vec(cs1[i], cs2[i])</div><div class="line">        <span class="keyword">if</span> dist &gt; max_dist:</div><div class="line">            max_dist = dist</div><div class="line"></div><div class="line">    <span class="keyword">return</span> max_dist</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"></div><div class="line">    args = sys.argv[<span class="number">1</span>:]</div><div class="line"></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(CENTROIDS_FILE):</div><div class="line">        shutil.copy(input_c, CENTROIDS_FILE)</div><div class="line"></div><div class="line">    old_c = get_first_c(input_c)</div><div class="line"></div><div class="line">    i = <span class="number">1</span></div><div class="line">    start = time.time()</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"Iteration #%i"</span> % i</div><div class="line">        mr_job = MRKMeans(args=args + [<span class="string">'--c='</span> + CENTROIDS_FILE])</div><div class="line">        <span class="comment"># print "start runner.."</span></div><div class="line"></div><div class="line">        <span class="keyword">with</span> mr_job.make_runner() <span class="keyword">as</span> runner:</div><div class="line">            runner.run()</div><div class="line">            centroids = get_c(mr_job, runner)</div><div class="line"></div><div class="line">        <span class="comment"># print "mr result: ", centroids</span></div><div class="line">        write_c(centroids)</div><div class="line">        n_c = get_first_c(CENTROIDS_FILE)</div><div class="line">        <span class="comment"># print "old_c", old_c</span></div><div class="line">        <span class="comment"># print "n_c", n_c </span></div><div class="line">        max_d = diff(n_c, old_c)</div><div class="line">        <span class="comment"># print "dist max = "+str(max_d)</span></div><div class="line">        <span class="keyword">if</span> max_d &lt; <span class="number">0.01</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            old_c = n_c</div><div class="line">            i = i + <span class="number">1</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"used time: "</span>, time.time() - start, <span class="string">'s'</span></div></pre></td></tr></table></figure><p>根据上面写的实现步骤可以看出，我们需要两个文件，一个存储输入数据，另一个存储<code>centroids</code>，由于只是一个demo，因此在这里我简化了具体问题。设所有的数据都是二维数据点，并且聚类个数为3。当然，如果真的是在大数据上进行工业级的处理的话，还是推荐使用<code>Spark</code>。下面列出这两个文件：</p><p><code>kmeans_data</code></p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="number">1</span>, <span class="number">2</span></div><div class="line"><span class="number">2</span>, <span class="number">3</span></div><div class="line"><span class="number">1</span>, <span class="number">3.5</span></div><div class="line"><span class="number">4</span>, <span class="number">3.5</span></div><div class="line"><span class="number">3</span>, <span class="number">4.2</span></div><div class="line"><span class="number">2</span>, <span class="number">1.6</span></div><div class="line"><span class="number">5</span>, <span class="number">2.3</span></div><div class="line"><span class="number">1.5</span>, <span class="number">2.3</span></div><div class="line"><span class="number">3</span>, <span class="number">5.2</span></div><div class="line"><span class="number">2</span>, <span class="number">3</span></div><div class="line"><span class="number">1</span>, <span class="number">3.5</span></div><div class="line"><span class="number">4</span>, <span class="number">3.5</span></div><div class="line"><span class="number">3</span>, <span class="number">4.2</span></div><div class="line"><span class="number">2</span>, <span class="number">1.6</span></div><div class="line"><span class="number">5</span>, <span class="number">2.3</span></div><div class="line"><span class="number">1.5</span>, <span class="number">2.3</span></div><div class="line"><span class="number">3</span>, <span class="number">5</span></div></pre></td></tr></table></figure><p><code>centroids</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">1, 2</div><div class="line">2, 3</div><div class="line">1, 3.5</div></pre></td></tr></table></figure><p>按照以下方式运行：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python main.py kmeans_data -r hadoop</div></pre></td></tr></table></figure><p>结果显示如下：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Iteration #<span class="number">1</span></div><div class="line">No handlers could be found <span class="keyword">for</span> logger <span class="string">"mrjob.hadoop"</span></div><div class="line">old_c [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">n_c [[<span class="number">1.625</span>, <span class="number">1.95833333333</span>], [<span class="number">3.4</span>, <span class="number">3.62</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">Iteration #<span class="number">2</span></div><div class="line">old_c [[<span class="number">1.625</span>, <span class="number">1.95833333333</span>], [<span class="number">3.4</span>, <span class="number">3.62</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">n_c [[<span class="number">1.72916666667</span>, <span class="number">2.2625</span>], [<span class="number">3.75</span>, <span class="number">3.775</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">Iteration #<span class="number">3</span></div><div class="line">old_c [[<span class="number">1.72916666667</span>, <span class="number">2.2625</span>], [<span class="number">3.75</span>, <span class="number">3.775</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">n_c [[<span class="number">1.72916666667</span>, <span class="number">2.2625</span>], [<span class="number">3.75</span>, <span class="number">3.775</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">time:  <span class="number">148.277868032</span></div></pre></td></tr></table></figure><p>最后生成结果文件：</p><p><code>centroid</code></p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">1.72916666667</span>, <span class="number">2.2625</span></div><div class="line"><span class="number">3.75</span>, <span class="number">3.775</span></div><div class="line"><span class="number">1.0</span>, <span class="number">3.5</span></div></pre></td></tr></table></figure><p>根据以上可以看出，对于小数据集，效率反而会比较低，因为整个程序运行过程中大部分的时间都没有花在实际的算法运行上。</p><p><strong>TODO：</strong></p><ol><li>用常规方法实现，作为baseline</li><li>在大数据集上继续实验，观察结果</li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/20/Something-about-cloudera/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/20/Something-about-cloudera/" itemprop="url">Something about cloudera</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-20T09:55:41+08:00">2017-04-20 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/20/Something-about-cloudera/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/20/Something-about-cloudera/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/20/Something-about-cloudera/" class="leancloud_visitors" data-flag-title="Something about cloudera"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>最近发现了一个神器，cloudera</p><p><a href="https://www.cloudera.com/" target="_blank" rel="external">https://www.cloudera.com/</a></p><p>它其实是一个集成了Hadoop生态系统的CentOS 6.7的VM，可以跑在Docker、Virtual Box或者VMware上</p><p><a href="https://www.cloudera.com/downloads/quickstart_vms/5-10.html" target="_blank" rel="external">https://www.cloudera.com/downloads/quickstart_vms/5-10.html</a></p><p>虚拟机配置的时候需要分配至少8G的RAM以及2个Cores。</p><p>另外，第一次启动会有些慢，请耐心等待。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/18/Hadoop-Distributed-Filesystem-notes/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/18/Hadoop-Distributed-Filesystem-notes/" itemprop="url">Hadoop Distributed Filesystem notes</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-18T11:53:51+08:00">2017-04-18 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/18/Hadoop-Distributed-Filesystem-notes/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/18/Hadoop-Distributed-Filesystem-notes/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/18/Hadoop-Distributed-Filesystem-notes/" class="leancloud_visitors" data-flag-title="Hadoop Distributed Filesystem notes"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="HDFS-Concepts"><a href="#HDFS-Concepts" class="headerlink" title="HDFS Concepts"></a>HDFS Concepts</h2><h3 id="Blocks"><a href="#Blocks" class="headerlink" title="Blocks"></a>Blocks</h3><ul><li>128 MB by default<ul><li>HDFS blocks are large compared to disk blocks, and the reason is to minimize the cost<br>of seeks.</li></ul></li><li>Having a block abstraction for a distributed filesystem brings several benefits<ul><li>A file can be larger than any single disk in the network.</li><li>Simplifies the storage subsystem.</li><li>Providing fault tolerance and availability.</li></ul></li><li><code>% hdfs fsck / -files -blocks</code></li></ul><h3 id="Namenodes-and-Datanodes"><a href="#Namenodes-and-Datanodes" class="headerlink" title="Namenodes and Datanodes"></a>Namenodes and Datanodes</h3><ul><li>An HDFS cluster has two types of nodes operating in a master−worker pattern<ul><li>namenode (the master)</li><li>datanodes (workers)</li></ul></li><li>Without the namenode, the filesystem cannot be used<ul><li>For this reason, it is important to make the namenode resilient to failure</li><li>The first way is to back up the files that make up the persistent state of the filesystem<br>metadata.</li><li>It is also possible to run a secondary namenode</li></ul></li></ul><h3 id="Block-Caching"><a href="#Block-Caching" class="headerlink" title="Block Caching"></a>Block Caching</h3><ul><li>For frequently accessed files the blocks may be explicitly cached in the datanode’s memory, in an off-heap block cache. By default, a block is cached in only one datanode’s memory.</li></ul><h3 id="HDFS-Federation"><a href="#HDFS-Federation" class="headerlink" title="HDFS Federation"></a>HDFS Federation</h3><ul><li>one namenode might manage all the files rooted under /user, say, and a second name‐<br>node might handle files under /share.<ul><li>namespace volume</li><li>block pool</li></ul></li><li>namenodes do not communicate with one another</li></ul><h3 id="HDFS-High-Avalibility"><a href="#HDFS-High-Avalibility" class="headerlink" title="HDFS High Avalibility"></a>HDFS High Avalibility</h3><ul><li>The new namenode is not able to serve requests until it has<ul><li>loaded its namespace image into memory</li><li>replayed its edit log</li><li>received enough block reports from the datanodes to leave safe mode.</li></ul></li><li>On large clusters with many files and blocks, the time it takes for a namenode to start from cold can be 30 minutes or more.</li><li>Hadoop 2 remedied this situation by adding support for HDFS high availability (HA).<ul><li>there are a pair of namenodes in an active-standby configuration. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption.</li></ul></li><li>There are two choices for the highly available shared storage<ul><li>NFS filer</li><li>quorum journal manager (QJM)</li><li>The actual observed failover time will be longer in practice (around a minute or so)</li></ul></li><li>The transition from the active namenode to the standby is managed by a new entity in<br>the system called the failover controller<ul><li>default implementation uses ZooKeeper to ensure that only one namenode is active.</li><li>The QJM only allows one namenode to write to the edit log at one time</li></ul></li></ul><h2 id="The-Command-Line-Interface"><a href="#The-Command-Line-Interface" class="headerlink" title="The Command-Line Interface"></a>The Command-Line Interface</h2><h3 id="Basic-Filesystem-Operations"><a href="#Basic-Filesystem-Operations" class="headerlink" title="Basic Filesystem Operations"></a>Basic Filesystem Operations</h3><ul><li><p>copying a file from the local filesystem to HDFS</p><ul><li><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">% hadoop fs -copyFromLocal input/docs/quangle.txt \</div><div class="line"> hdfs:<span class="comment">//localhost/user/tom/quangle.txt</span></div></pre></td></tr></table></figure></li></ul></li><li><p>copy the file back to the local filesystem and check whether it’s the same</p><ul><li><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">% hadoop fs -copyToLocal quangle.txt quangle.copy.txt</div><div class="line">% md5 input/docs/quangle.txt quangle.copy.txt</div><div class="line">e7891a2627cf263a079fb0f18256ffb2 input/docs/quangle.txt</div><div class="line">MD5 (quangle.copy.txt) = e7891a2627cf263a079fb0f18256ffb2</div></pre></td></tr></table></figure></li></ul></li><li><p>create a directory first just to see how it is displayed in the listing</p><ul><li><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">% hadoop fs -mkdir books</div><div class="line">% hadoop fs -ls .</div><div class="line">Found <span class="number">2</span> items</div><div class="line">drwxr-xr-x - tom supergroup <span class="number">0</span> <span class="number">2014</span><span class="number">-10</span><span class="number">-04</span> <span class="number">13</span>:<span class="number">22</span> books</div><div class="line">-rw-r--r-- <span class="number">1</span> tom supergroup <span class="number">119</span> <span class="number">2014</span><span class="number">-10</span><span class="number">-04</span> <span class="number">13</span>:<span class="number">21</span> quangle.txt</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="The-Java-Interface"><a href="#The-Java-Interface" class="headerlink" title="The Java Interface"></a>The Java Interface</h2><h3 id="Reading-Data-from-a-Hadoop-URL"><a href="#Reading-Data-from-a-Hadoop-URL" class="headerlink" title="Reading Data from a Hadoop URL"></a>Reading Data from a Hadoop URL</h3><p><em>Example. Displaying files from a Hadoop filesystem on standard output using a URLStreamHandler</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// cc URLCat Displays files from a Hadoop filesystem on standard output using a URLStreamHandler</span></div><div class="line"><span class="keyword">import</span> java.io.InputStream;</div><div class="line"><span class="keyword">import</span> java.net.URL;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FsUrlStreamHandlerFactory;</div><div class="line"><span class="keyword">import</span> org.a</div><div class="line">  pache.hadoop.io.IOUtils;</div><div class="line"></div><div class="line"><span class="comment">// vv URLCat</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">URLCat</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="keyword">static</span> &#123;</div><div class="line">    URL.setURLStreamHandlerFactory(<span class="keyword">new</span> FsUrlStreamHandlerFactory());</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">    InputStream in = <span class="keyword">null</span>;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      in = <span class="keyword">new</span> URL(args[<span class="number">0</span>]).openStream();</div><div class="line">      IOUtils.copyBytes(in, System.out, <span class="number">4096</span>, <span class="keyword">false</span>);</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      IOUtils.closeStream(in);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// ^^ URLCat</span></div></pre></td></tr></table></figure><p><strong>There’s a little bit more work required to make Java recognize Hadoop’s hdfs URL scheme. This is achieved by calling the setURLStreamHandlerFactory() method on URL with an instance of FsUrlStreamHandlerFactory. This method can be called only once per JVM, so it is typically executed in a static block.</strong></p><p>Here’s a sample run:</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">% <span class="keyword">export</span> HADOOP_CLASSPATH=hadoop-examples.jar</div><div class="line">% hadoop URLCat hdfs:<span class="comment">//localhost/user/tom/quangle.txt</span></div><div class="line">On the top of the Crumpetty Tree</div><div class="line">The Quangle Wangle sat,</div><div class="line">But his face you could not see,</div><div class="line">On account of his Beaver Hat.</div></pre></td></tr></table></figure><h3 id="Reading-Data-Using-the-FileSystem-API"><a href="#Reading-Data-Using-the-FileSystem-API" class="headerlink" title="Reading Data Using the FileSystem API"></a>Reading Data Using the FileSystem API</h3><p><em>Example. Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// cc FileSystemCat Displays files from a Hadoop filesystem on standard output by using the FileSystem directly</span></div><div class="line"><span class="keyword">import</span> java.io.InputStream;</div><div class="line"><span class="keyword">import</span> java.net.URI;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</div><div class="line"></div><div class="line"><span class="comment">// vv FileSystemCat</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileSystemCat</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">    String uri = args[<span class="number">0</span>];</div><div class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">    FileSystem fs = FileSystem.get(URI.create(uri), conf);</div><div class="line">    InputStream in = <span class="keyword">null</span>;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      in = fs.open(<span class="keyword">new</span> Path(uri));</div><div class="line">      IOUtils.copyBytes(in, System.out, <span class="number">4096</span>, <span class="keyword">false</span>);</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      IOUtils.closeStream(in);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// ^^ FileSystemCat</span></div></pre></td></tr></table></figure><p>The program runs as follows:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">% hadoop FileSystemCat hdfs:<span class="comment">//localhost/user/tom/quangle.txt</span></div><div class="line">On the top of the Crumpetty Tree</div><div class="line">The Quangle Wangle sat,</div><div class="line">But his face you could not see,</div><div class="line">On account of his Beaver Hat.</div></pre></td></tr></table></figure><h4 id="FSDataInputStream"><a href="#FSDataInputStream" class="headerlink" title="FSDataInputStream"></a>FSDataInputStream</h4><p><em>Example. Displaying files from a Hadoop filesystem on standard output twice, by using seek()</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// cc FileSystemDoubleCat Displays files from a Hadoop filesystem on standard output twice, by using seek</span></div><div class="line"><span class="keyword">import</span> java.net.URI;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</div><div class="line"></div><div class="line"><span class="comment">// vv FileSystemDoubleCat</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileSystemDoubleCat</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">    String uri = args[<span class="number">0</span>];</div><div class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">    FileSystem fs = FileSystem.get(URI.create(uri), conf);</div><div class="line">    FSDataInputStream in = <span class="keyword">null</span>;</div><div class="line">    <span class="keyword">try</span> &#123;</div><div class="line">      in = fs.open(<span class="keyword">new</span> Path(uri));</div><div class="line">      IOUtils.copyBytes(in, System.out, <span class="number">4096</span>, <span class="keyword">false</span>);</div><div class="line">      in.seek(<span class="number">0</span>); <span class="comment">// go back to the start of the file</span></div><div class="line">      IOUtils.copyBytes(in, System.out, <span class="number">4096</span>, <span class="keyword">false</span>);</div><div class="line">    &#125; <span class="keyword">finally</span> &#123;</div><div class="line">      IOUtils.closeStream(in);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// ^^ FileSystemDoubleCat</span></div></pre></td></tr></table></figure><p>Here’s the result of running it on a small file:</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">% hadoop FileSystemDoubleCat hdfs:<span class="comment">//localhost/user/tom/quangle.txt</span></div><div class="line">On the top of the Crumpetty Tree</div><div class="line">The Quangle Wangle sat,</div><div class="line">But his face you could not see,</div><div class="line">On account of his Beaver Hat.</div><div class="line">On the top of the Crumpetty Tree</div><div class="line">The Quangle Wangle sat,</div><div class="line">But his face you could not see,</div><div class="line">On account of his Beaver Hat.</div></pre></td></tr></table></figure><h3 id="Writing-Data"><a href="#Writing-Data" class="headerlink" title="Writing Data"></a>Writing Data</h3><p><em>Example. Copying a local file to a Hadoop filesystem</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// cc FileCopyWithProgress Copies a local file to a Hadoop filesystem, and shows progress</span></div><div class="line"><span class="keyword">import</span> java.io.BufferedInputStream;</div><div class="line"><span class="keyword">import</span> java.io.FileInputStream;</div><div class="line"><span class="keyword">import</span> java.io.InputStream;</div><div class="line"><span class="keyword">import</span> java.io.OutputStream;</div><div class="line"><span class="keyword">import</span> java.net.URI;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.util.Progressable;</div><div class="line"></div><div class="line"><span class="comment">// vv FileCopyWithProgress</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileCopyWithProgress</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">    String localSrc = args[<span class="number">0</span>];</div><div class="line">    String dst = args[<span class="number">1</span>];</div><div class="line">    </div><div class="line">    InputStream in = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(localSrc));</div><div class="line">    </div><div class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">    FileSystem fs = FileSystem.get(URI.create(dst), conf);</div><div class="line">    OutputStream out = fs.create(<span class="keyword">new</span> Path(dst), <span class="keyword">new</span> Progressable() &#123;</div><div class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progress</span><span class="params">()</span> </span>&#123;</div><div class="line">        System.out.print(<span class="string">"."</span>);</div><div class="line">      &#125;</div><div class="line">    &#125;);</div><div class="line">    </div><div class="line">    IOUtils.copyBytes(in, out, <span class="number">4096</span>, <span class="keyword">true</span>);</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// ^^ FileCopyWithProgress</span></div></pre></td></tr></table></figure><p>Typical usage:</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">% hadoop FileCopyWithProgress input/docs/<span class="number">1400</span><span class="number">-8.</span>txt</div><div class="line">hdfs:<span class="comment">//localhost/user/tom/1400-8.txt</span></div><div class="line">.................</div></pre></td></tr></table></figure><h3 id="Querying-the-Filesystem"><a href="#Querying-the-Filesystem" class="headerlink" title="Querying the Filesystem"></a>Querying the Filesystem</h3><p>The <code>FileStatus</code> class encapsulates filesystem metadata for files and directories, including file length, block size, replication, modification time, ownership, and permission information.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShowFileStatusTest</span> </span>&#123;</div><div class="line">  <span class="keyword">private</span> MiniDFSCluster cluster; <span class="comment">// use an in-process HDFS cluster for testing</span></div><div class="line">  <span class="keyword">private</span> FileSystem fs;</div><div class="line"></div><div class="line">  <span class="meta">@Before</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUp</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">    <span class="keyword">if</span> (System.getProperty(<span class="string">"test.build.data"</span>) == <span class="keyword">null</span>) &#123;</div><div class="line">      System.setProperty(<span class="string">"test.build.data"</span>, <span class="string">"/tmp"</span>);</div><div class="line">    &#125;</div><div class="line">    cluster = <span class="keyword">new</span> MiniDFSCluster.Builder(conf).build();</div><div class="line">    fs = cluster.getFileSystem();</div><div class="line">    OutputStream out = fs.create(<span class="keyword">new</span> Path(<span class="string">"/dir/file"</span>));</div><div class="line">    out.write(<span class="string">"content"</span>.getBytes(<span class="string">"UTF-8"</span>));</div><div class="line">    out.close();</div><div class="line">  &#125;</div><div class="line"> </div><div class="line">  <span class="meta">@After</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">tearDown</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    <span class="keyword">if</span> (fs != <span class="keyword">null</span>) &#123; fs.close(); &#125;</div><div class="line">    <span class="keyword">if</span> (cluster != <span class="keyword">null</span>) &#123; cluster.shutdown(); &#125;</div><div class="line">  &#125;</div><div class="line"> </div><div class="line">  <span class="meta">@Test</span>(expected = FileNotFoundException.class)</div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">throwsFileNotFoundForNonExistentFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    fs.getFileStatus(<span class="keyword">new</span> Path(<span class="string">"no-such-file"</span>));</div><div class="line">  &#125;</div><div class="line"> </div><div class="line">  <span class="meta">@Test</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">fileStatusForFile</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    Path file = <span class="keyword">new</span> Path(<span class="string">"/dir/file"</span>);</div><div class="line">    FileStatus stat = fs.getFileStatus(file);</div><div class="line">    assertThat(stat.getPath().toUri().getPath(), is(<span class="string">"/dir/file"</span>));</div><div class="line">    assertThat(stat.isDirectory(), is(<span class="keyword">false</span>));</div><div class="line">    assertThat(stat.getLen(), is(<span class="number">7L</span>));</div><div class="line">    assertThat(stat.getModificationTime(),</div><div class="line">    is(lessThanOrEqualTo(System.currentTimeMillis())));</div><div class="line">    assertThat(stat.getReplication(), is((<span class="keyword">short</span>) <span class="number">1</span>));</div><div class="line">    assertThat(stat.getBlockSize(), is(<span class="number">128</span> * <span class="number">1024</span> * <span class="number">1024L</span>));</div><div class="line">    assertThat(stat.getOwner(), is(System.getProperty(<span class="string">"user.name"</span>)));</div><div class="line">    assertThat(stat.getGroup(), is(<span class="string">"supergroup"</span>));</div><div class="line">    assertThat(stat.getPermission().toString(), is(<span class="string">"rw-r--r--"</span>));</div><div class="line"> &#125;</div><div class="line"> </div><div class="line">  <span class="meta">@Test</span></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">fileStatusForDirectory</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">    Path dir = <span class="keyword">new</span> Path(<span class="string">"/dir"</span>);</div><div class="line">    FileStatus stat = fs.getFileStatus(dir);</div><div class="line">    assertThat(stat.getPath().toUri().getPath(), is(<span class="string">"/dir"</span>));</div><div class="line">    assertThat(stat.isDirectory(), is(<span class="keyword">true</span>));</div><div class="line">    assertThat(stat.getLen(), is(<span class="number">0L</span>));</div><div class="line">    assertThat(stat.getModificationTime(),</div><div class="line">    is(lessThanOrEqualTo(System.currentTimeMillis())));</div><div class="line">    assertThat(stat.getReplication(), is((<span class="keyword">short</span>) <span class="number">0</span>));</div><div class="line">    assertThat(stat.getBlockSize(), is(<span class="number">0L</span>));</div><div class="line">    assertThat(stat.getOwner(), is(System.getProperty(<span class="string">"user.name"</span>)));</div><div class="line">    assertThat(stat.getGroup(), is(<span class="string">"supergroup"</span>));</div><div class="line">    assertThat(stat.getPermission().toString(), is(<span class="string">"rwxr-xr-x"</span>));</div><div class="line">  &#125; </div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="Listing-files"><a href="#Listing-files" class="headerlink" title="Listing files"></a>Listing files</h4><p><em>Example. Showing the file statuses for a collection of paths in a Hadoop filesystem</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// cc ListStatus Shows the file statuses for a collection of paths in a Hadoop filesystem </span></div><div class="line"><span class="keyword">import</span> java.net.URI;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileUtil;</div><div class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</div><div class="line"></div><div class="line"><span class="comment">// vv ListStatus</span></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ListStatus</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">    String uri = args[<span class="number">0</span>];</div><div class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">    FileSystem fs = FileSystem.get(URI.create(uri), conf);</div><div class="line">    </div><div class="line">    Path[] paths = <span class="keyword">new</span> Path[args.length];</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; paths.length; i++) &#123;</div><div class="line">      paths[i] = <span class="keyword">new</span> Path(args[i]);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    FileStatus[] status = fs.listStatus(paths);</div><div class="line">    Path[] listedPaths = FileUtil.stat2Paths(status);</div><div class="line">    <span class="keyword">for</span> (Path p : listedPaths) &#123;</div><div class="line">      System.out.println(p);</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// ^^ ListStatus</span></div></pre></td></tr></table></figure><p>We can use this program to find the union of directory listings for a collection of paths:</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">% hadoop ListStatus hdfs:<span class="comment">//localhost/ hdfs://localhost/user/tom</span></div><div class="line">hdfs:<span class="comment">//localhost/user</span></div><div class="line">hdfs:<span class="comment">//localhost/user/tom/books</span></div><div class="line">hdfs:<span class="comment">//localhost/user/tom/quangle.txt</span></div></pre></td></tr></table></figure><h2 id="DataFlow"><a href="#DataFlow" class="headerlink" title="DataFlow"></a>DataFlow</h2><h3 id="Anatomy-of-a-File-Read"><a href="#Anatomy-of-a-File-Read" class="headerlink" title="Anatomy of a File Read"></a>Anatomy of a File Read</h3><p><img src="http://o7ie0tcjk.bkt.clouddn.com/hadoop/ch03/hdfs_dataflow_read.png" alt="read"></p><h4 id="Network-Topology-and-Hadoop"><a href="#Network-Topology-and-Hadoop" class="headerlink" title="Network Topology and Hadoop"></a>Network Topology and Hadoop</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/hadoop/ch03/hdfs_dataflow_distance.png" alt="distance"></p><p><strong>Mathematically inclined readers will notice that this is an example of a distance metric.</strong></p><h3 id="Anatomy-of-a-File-Write"><a href="#Anatomy-of-a-File-Write" class="headerlink" title="Anatomy of a File Write"></a>Anatomy of a File Write</h3><p><img src="http://o7ie0tcjk.bkt.clouddn.com/hadoop/ch03/hdfs_dataflow_write.png" alt="write"></p><p>A typical replica pipeline:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/hadoop/ch03/hdfs_dataflow_replica.png" alt="replica"></p><h2 id="Coherency-Model"><a href="#Coherency-Model" class="headerlink" title="Coherency Model"></a>Coherency Model</h2><p>After creating a file, it is visible in the filesystem namespace, as expected:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Path p = <span class="keyword">new</span> Path(<span class="string">"p"</span>);</div><div class="line">fs.create(p);</div><div class="line">assertThat(fs.exists(p), is(<span class="keyword">true</span>));</div></pre></td></tr></table></figure><p>However, any content written to the file is not guaranteed to be visible, even if the stream is flushed. So, the file appears to have a length of zero:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Path p = <span class="keyword">new</span> Path(<span class="string">"p"</span>);</div><div class="line">OutputStream out = fs.create(p);</div><div class="line">out.write(<span class="string">"content"</span>.getBytes(<span class="string">"UTF-8"</span>));</div><div class="line">out.flush();</div><div class="line">assertThat(fs.getFileStatus(p).getLen(), is(<span class="number">0L</span>));</div></pre></td></tr></table></figure><p>HDFS provides a way to force all buffers to be flushed to the datanodes via the hflush() method on FSDataOutputStream. After a successful return from hflush(), HDFS guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Path p = <span class="keyword">new</span> Path(<span class="string">"p"</span>);</div><div class="line">FSDataOutputStream out = fs.create(p);</div><div class="line">out.write(<span class="string">"content"</span>.getBytes(<span class="string">"UTF-8"</span>));</div><div class="line">out.hflush();</div><div class="line">assertThat(fs.getFileStatus(p).getLen(), is(((<span class="keyword">long</span>) <span class="string">"content"</span>.length())));</div></pre></td></tr></table></figure><p>Note that hflush() does not guarantee that the datanodes have written the data to disk, only that it’s in the datanodes’ memory (so in the event of a data center power outage, for example, data could be lost). For this stronger guarantee, use hsync() instead.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">FileOutputStream out = <span class="keyword">new</span> FileOutputStream(localFile);</div><div class="line">out.write(<span class="string">"content"</span>.getBytes(<span class="string">"UTF-8"</span>));</div><div class="line">out.flush(); <span class="comment">// flush to operating system</span></div><div class="line">out.getFD().sync(); <span class="comment">// sync to disk</span></div><div class="line">assertThat(localFile.length(), is(((<span class="keyword">long</span>) <span class="string">"content"</span>.length())));</div></pre></td></tr></table></figure><p>Closing a file in HDFS performs an implicit hflush(), too:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Path p = <span class="keyword">new</span> Path(<span class="string">"p"</span>);</div><div class="line">OutputStream out = fs.create(p);</div><div class="line">out.write(<span class="string">"content"</span>.getBytes(<span class="string">"UTF-8"</span>));</div><div class="line">out.close();</div><div class="line">assertThat(fs.getFileStatus(p).getLen(), is(((<span class="keyword">long</span>) <span class="string">"content"</span>.length())));</div></pre></td></tr></table></figure><h2 id="Parallel-Copying-with-distcp"><a href="#Parallel-Copying-with-distcp" class="headerlink" title="Parallel Copying with distcp"></a>Parallel Copying with distcp</h2><p>One use for distcp is as an efficient replacement for hadoop fs -cp. For example, you can copy one file to another with:</p><p><code>% hadoop distcp file1 file2</code></p><p>You can also copy directories:</p><p><code>% hadoop distcp dir1 dir2</code></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/15/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><span class="page-number current">16</span><a class="page-number" href="/page/17/">17</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/17/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">121</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">58</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>