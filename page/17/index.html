<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/17/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/17/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/26/Movie-Recommendation-with-MLlib/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/26/Movie-Recommendation-with-MLlib/" itemprop="url">Movie Recommendation with MLlib</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-26T14:24:57+08:00">2017-04-26 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/26/Movie-Recommendation-with-MLlib/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/26/Movie-Recommendation-with-MLlib/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/26/Movie-Recommendation-with-MLlib/" class="leancloud_visitors" data-flag-title="Movie Recommendation with MLlib"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Spark-Summit-2014"><a href="#Spark-Summit-2014" class="headerlink" title="Spark Summit 2014"></a>Spark Summit 2014</h2><p><a href="https://databricks-training.s3.amazonaws.com/index.html" target="_blank" rel="external">https://databricks-training.s3.amazonaws.com/index.html</a></p><p>we will use MLlib to make personalized movie recommendations tailored <em>for you</em>. We will work with 10 million ratings from 72,000 users on 10,000 movies, collected by <a href="http://movielens.umn.edu/" target="_blank" rel="external">MovieLens</a>. This dataset is pre-loaded in your USB drive under <code>data/movielens/large</code>. For quick testing of your code, you may want to use a smaller dataset under <code>data/movielens/medium</code>, which contains 1 million ratings from 6000 users on 4000 movies.</p><h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>We will use two files from this MovieLens dataset: “<code>ratings.dat</code>” and “<code>movies.dat</code>”. All ratings are contained in the file “<code>ratings.dat</code>” and are in the following format:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">UserID::MovieID::Rating::Timestamp</div></pre></td></tr></table></figure><p>Movie information is in the file “<code>movies.dat</code>” and is in the following format:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">MovieID::Title::Genres</div></pre></td></tr></table></figure><h2 id="Collaborative-filtering"><a href="#Collaborative-filtering" class="headerlink" title="Collaborative filtering"></a>Collaborative filtering</h2><p>Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix, in our case, the user-movie rating matrix. MLlib currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. In particular, we implement the alternating least squares (ALS) algorithm to learn these latent factors.</p><p><img src="https://databricks-training.s3.amazonaws.com/img/matrix_factorization.png" alt="cf"></p><h2 id="Create-training-examples"><a href="#Create-training-examples" class="headerlink" title="Create training examples"></a>Create training examples</h2><p><a href="https://github.com/ewanlee/spark-training" target="_blank" rel="external">https://github.com/ewanlee/spark-training</a></p><p>To make recommendation <em>for you</em>, we are going to learn your taste by asking you to rate a few movies. We have selected a small set of movies that have received the most ratings from users in the MovieLens dataset. You can rate those movies by running <code>bin/rateMovies</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python bin/rateMovies</div></pre></td></tr></table></figure><p>When you run the script, you should see prompt similar to the following:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Please rate the following movie (1-5 (best), or 0 if not seen):</div><div class="line">Toy Story (1995):</div></pre></td></tr></table></figure><p>After you’re done rating the movies, we save your ratings in <code>personalRatings.txt</code> in the MovieLens format, where a special user id <code>0</code> is assigned to you.</p><p><code>rateMovies</code> allows you to re-rate the movies if you’d like to see how your ratings affect your recommendations.</p><p>If you don’t have python installed, please copy <code>personalRatings.txt.template</code> to <code>personalRatings.txt</code> and replace <code>?</code>s with your ratings.</p><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><p>We will be using a standalone project template for this exercise.</p><ul><li><p>In the training USB drive, this has been setup in</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">machine-learning/python/</div></pre></td></tr></table></figure></li><li><p>You should find the following items in the directory:</p></li><li><p><code>MovieLensALS.py</code>: Main Python program that you are going to edit, compile and run</p></li><li><p><code>solution</code>: Directory containing the solution code</p></li></ul><p><code>MovieLensALS.py</code> should look as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</div><div class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</div><div class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join, isfile, dirname</div><div class="line"></div><div class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</div><div class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseRating</span><span class="params">(line)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Parses a rating record in MovieLens format userId::movieId::rating::timestamp .</div><div class="line">    """</div><div class="line">    <span class="comment"># ...</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseMovie</span><span class="params">(line)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Parses a movie record in MovieLens format movieId::movieTitle .</div><div class="line">    """</div><div class="line">    <span class="comment"># ...</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadRatings</span><span class="params">(ratingsFile)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Load ratings from file.</div><div class="line">    """</div><div class="line">    <span class="comment"># ...</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeRmse</span><span class="params">(model, data, n)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Compute RMSE (Root Mean Squared Error).</div><div class="line">    """</div><div class="line">    <span class="comment"># ...</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    <span class="keyword">if</span> (len(sys.argv) != <span class="number">3</span>):</div><div class="line">        <span class="keyword">print</span> <span class="string">"Usage: [usb root directory]/spark/bin/spark-submit --driver-memory 2g "</span> + \</div><div class="line">          <span class="string">"MovieLensALS.py movieLensDataDir personalRatingsFile"</span></div><div class="line">        sys.exit(<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># set up environment</span></div><div class="line">    conf = SparkConf() \</div><div class="line">      .setAppName(<span class="string">"MovieLensALS"</span>) \</div><div class="line">      .set(<span class="string">"spark.executor.memory"</span>, <span class="string">"2g"</span>)</div><div class="line">    sc = SparkContext(conf=conf)</div><div class="line"></div><div class="line">    <span class="comment"># load personal ratings</span></div><div class="line">    myRatings = loadRatings(sys.argv[<span class="number">2</span>])</div><div class="line">    myRatingsRDD = sc.parallelize(myRatings, <span class="number">1</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># load ratings and movie titles</span></div><div class="line"></div><div class="line">    movieLensHomeDir = sys.argv[<span class="number">1</span>]</div><div class="line"></div><div class="line">    <span class="comment"># ratings is an RDD of (last digit of timestamp, (userId, movieId, rating))</span></div><div class="line">    ratings = sc.textFile(join(movieLensHomeDir, <span class="string">"ratings.dat"</span>)).map(parseRating)</div><div class="line"></div><div class="line">    <span class="comment"># movies is an RDD of (movieId, movieTitle)</span></div><div class="line">    movies = dict(sc.textFile(join(movieLensHomeDir, <span class="string">"movies.dat"</span>)).map(parseMovie).collect())</div><div class="line"></div><div class="line">    <span class="comment"># your code here</span></div><div class="line">    </div><div class="line">    <span class="comment"># clean up</span></div><div class="line">    sc.stop()</div></pre></td></tr></table></figure><p>Let’s first take a closer look at our template code in a text editor, then we’ll start adding code to the template. Locate the<code>MovieLensALS</code> class and open it with a text editor.</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">usb/$ cd machine-learning/python</div><div class="line">vim MovieLensALS.py  # Or your editor of choice</div></pre></td></tr></table></figure><p>For any Spark computation, we first create a SparkConf object and use it to create a SparkContext object. Since we will be using spark-submit to execute the programs in this tutorial (more on spark-submit in the next section), we only need to configure the executor memory allocation and give the program a name, e.g. “MovieLensALS”, to identify it in Spark’s web UI. In local mode, the web UI can be access at <a href="http://localhost:4040/" target="_blank" rel="external"><code>localhost:4040</code></a> during the execution of a program.</p><p>This is what it looks like in our template code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">conf = SparkConf() \</div><div class="line">     .setAppName(<span class="string">"MovieLensALS"</span>) \</div><div class="line">     .set(<span class="string">"spark.executor.memory"</span>, <span class="string">"2g"</span>)</div><div class="line">   sc = SparkContext(conf=conf)</div></pre></td></tr></table></figure><p>Next, the code uses the SparkContext to read in ratings. Recall that the rating file is a text file with “<code>::</code>” as the delimiter. The code parses each line to create a RDD for ratings that contains <code>(Int, Rating)</code> pairs. We only keep the last digit of the timestamp as a random key. The <code>Rating</code> class is a wrapper around the tuple <code>(user: Int, product: Int, rating: Double)</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">movieLensHomeDir = sys.argv[<span class="number">1</span>]</div><div class="line"></div><div class="line"><span class="comment"># ratings is an RDD of (last digit of timestamp, (userId, movieId, rating))</span></div><div class="line">ratings = sc.textFile(join(movieLensHomeDir, <span class="string">"ratings.dat"</span>)).map(parseRating)</div></pre></td></tr></table></figure><p>Next, the code read in movie ids and titles, collect them into a movie id to title map.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseMovie</span><span class="params">(line)</span>:</span></div><div class="line">      fields = line.split(<span class="string">"::"</span>)</div><div class="line">      <span class="keyword">return</span> int(fields[<span class="number">0</span>]), fields[<span class="number">1</span>]</div><div class="line"></div><div class="line">    movies = dict(sc.textFile(join(movieLensHomeDir, <span class="string">"movies.dat"</span>)).map(parseMovie).collect())</div></pre></td></tr></table></figure><p>Now, let’s make our first edit to add code to get a summary of the ratings.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">numRatings = ratings.count()</div><div class="line">numUsers = ratings.values().map(<span class="keyword">lambda</span> r: r[<span class="number">0</span>]).distinct().count()</div><div class="line">numMovies = ratings.values().map(<span class="keyword">lambda</span> r: r[<span class="number">1</span>]).distinct().count()</div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">"Got %d ratings from %d users on %d movies."</span> % (numRatings, numUsers, numMovies)</div></pre></td></tr></table></figure><h2 id="Running-the-program"><a href="#Running-the-program" class="headerlink" title="Running the program"></a>Running the program</h2><p>Before we compute movie recommendations, here is a quick reminder on how you can run the program at any point during this exercise. As mentioned above, we will use <code>spark-submit</code> to execute your program in local mode for this tutorial.</p><p>Starting with Spark 1.0, <code>spark-submit</code> is the recommended way for running Spark applications, both on clusters and locally in standalone mode.</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">usb/$ cd machine-learning/python</div><div class="line"></div><div class="line"># change the folder name from <span class="string">"medium"</span> to <span class="string">"large"</span> to run on the large data <span class="keyword">set</span></div><div class="line">[usb root directory]/spark/bin/spark-submit MovieLensALS.py [usb root directory]/data/movielens/medium/ ../personalRatings.txt</div></pre></td></tr></table></figure><p>You should see output similar to the following on your screen:</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Got <span class="number">1000209</span> ratings from <span class="number">6040</span> users on <span class="number">3706</span> movies.</div></pre></td></tr></table></figure><h2 id="Splitting-training-data"><a href="#Splitting-training-data" class="headerlink" title="Splitting training data"></a>Splitting training data</h2><p>We will use MLlib’s <code>ALS</code> to train a <code>MatrixFactorizationModel</code>, which takes a <code>RDD[Rating]</code> object as input in Scala and <code>RDD[(user, product, rating)]</code> in Python. ALS has training parameters such as rank for matrix factors and regularization constants. To determine a good combination of the training parameters, we split the data into three non-overlapping subsets, named training, test, and validation, based on the last digit of the timestamp, and cache them. We will train multiple models based on the training set, select the best model on the validation set based on RMSE (Root Mean Squared Error), and finally evaluate the best model on the test set. We also add your ratings to the training set to make recommendations for you. We hold the training, validation, and test sets in memory by calling <code>cache</code> because we need to visit them multiple times.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">numPartitions = <span class="number">4</span></div><div class="line">training = ratings.filter(<span class="keyword">lambda</span> x: x[<span class="number">0</span>] &lt; <span class="number">6</span>) \</div><div class="line">  .values() \</div><div class="line">  .union(myRatingsRDD) \</div><div class="line">  .repartition(numPartitions) \</div><div class="line">  .cache()</div><div class="line"></div><div class="line">validation = ratings.filter(<span class="keyword">lambda</span> x: x[<span class="number">0</span>] &gt;= <span class="number">6</span> <span class="keyword">and</span> x[<span class="number">0</span>] &lt; <span class="number">8</span>) \</div><div class="line">  .values() \</div><div class="line">  .repartition(numPartitions) \</div><div class="line">  .cache()</div><div class="line"></div><div class="line">test = ratings.filter(<span class="keyword">lambda</span> x: x[<span class="number">0</span>] &gt;= <span class="number">8</span>).values().cache()</div><div class="line"></div><div class="line">numTraining = training.count()</div><div class="line">numValidation = validation.count()</div><div class="line">numTest = test.count()</div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">"Training: %d, validation: %d, test: %d"</span> % (numTraining, numValidation, numTest)</div></pre></td></tr></table></figure><p>After the split, you should see</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Training: <span class="number">602251</span>, validation: <span class="number">198919</span>, test: <span class="number">199049.</span></div></pre></td></tr></table></figure><h2 id="Training-using-ALS"><a href="#Training-using-ALS" class="headerlink" title="Training using ALS"></a>Training using ALS</h2><p>In this section, we will use <code>ALS.train</code> to train a bunch of models, and select and evaluate the best. Among the training paramters of ALS, the most important ones are rank, lambda (regularization constant), and number of iterations. The <code>train</code>method of ALS we are going to use is defined as the following:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ALS</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(cls, ratings, rank, iterations=<span class="number">5</span>, lambda_=<span class="number">0.01</span>, blocks=<span class="number">-1</span>)</span>:</span></div><div class="line">    <span class="comment"># ...</span></div><div class="line">    <span class="keyword">return</span> MatrixFactorizationModel(sc, mod)</div></pre></td></tr></table></figure><p>deally, we want to try a large number of combinations of them in order to find the best one. Due to time constraint, we will test only 8 combinations resulting from the cross product of 2 different ranks (8 and 12), 2 different lambdas (1.0 and 10.0), and two different numbers of iterations (10 and 20). We use the provided method <code>computeRmse</code> to compute the RMSE on the validation set for each model. The model with the smallest RMSE on the validation set becomes the one selected and its RMSE on the test set is used as the final metric.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">ranks = [<span class="number">8</span>, <span class="number">12</span>]</div><div class="line">lambdas = [<span class="number">1.0</span>, <span class="number">10.0</span>]</div><div class="line">numIters = [<span class="number">10</span>, <span class="number">20</span>]</div><div class="line">bestModel = <span class="keyword">None</span></div><div class="line">bestValidationRmse = float(<span class="string">"inf"</span>)</div><div class="line">bestRank = <span class="number">0</span></div><div class="line">bestLambda = <span class="number">-1.0</span></div><div class="line">bestNumIter = <span class="number">-1</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> rank, lmbda, numIter <span class="keyword">in</span> itertools.product(ranks, lambdas, numIters):</div><div class="line">    model = ALS.train(training, rank, numIter, lmbda)</div><div class="line">    validationRmse = computeRmse(model, validation, numValidation)</div><div class="line">    <span class="keyword">print</span> <span class="string">"RMSE (validation) = %f for the model trained with "</span> % validationRmse + \</div><div class="line">          <span class="string">"rank = %d, lambda = %.1f, and numIter = %d."</span> % (rank, lmbda, numIter)</div><div class="line">    <span class="keyword">if</span> (validationRmse &lt; bestValidationRmse):</div><div class="line">        bestModel = model</div><div class="line">        bestValidationRmse = validationRmse</div><div class="line">        bestRank = rank</div><div class="line">        bestLambda = lmbda</div><div class="line">        bestNumIter = numIter</div><div class="line"></div><div class="line">testRmse = computeRmse(bestModel, test, numTest)</div><div class="line"></div><div class="line"><span class="comment"># evaluate the best model on the test set</span></div><div class="line"><span class="keyword">print</span> <span class="string">"The best model was trained with rank = %d and lambda = %.1f, "</span> % (bestRank, bestLambda) \</div><div class="line">  + <span class="string">"and numIter = %d, and its RMSE on the test set is %f."</span> % (bestNumIter, testRmse)</div></pre></td></tr></table></figure><p>Spark might take a minute or two to train the models. You should see the following on the screen:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">The best model was trained using rank <span class="number">8</span> <span class="keyword">and</span> <span class="keyword">lambda</span> <span class="number">10.0</span>, <span class="keyword">and</span> its RMSE on test <span class="keyword">is</span> <span class="number">0.8808492431998702</span>.</div></pre></td></tr></table></figure><h2 id="Recommending-movies-for-you"><a href="#Recommending-movies-for-you" class="headerlink" title="Recommending movies for you"></a>Recommending movies for you</h2><p>As the last part of our tutorial, let’s take a look at what movies our model recommends for you. This is done by generating <code>(0, movieId)</code> pairs for all movies you haven’t rated and calling the model’s <code>predict</code> method to get predictions. <code>0</code> is the special user id assigned to you.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MatrixFactorizationModel</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predictAll</span><span class="params">(self, usersProducts)</span>:</span></div><div class="line">        <span class="comment"># ...</span></div><div class="line">        <span class="keyword">return</span> RDD(self._java_model.predict(usersProductsJRDD._jrdd),</div><div class="line">                   self._context, RatingDeserializer())</div></pre></td></tr></table></figure><p>After we get all predictions, let us list the top 50 recommendations and see whether they look good to you.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">myRatedMovieIds = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> myRatings])</div><div class="line">candidates = sc.parallelize([m <span class="keyword">for</span> m <span class="keyword">in</span> movies <span class="keyword">if</span> m <span class="keyword">not</span> <span class="keyword">in</span> myRatedMovieIds])</div><div class="line">predictions = bestModel.predictAll(candidates.map(<span class="keyword">lambda</span> x: (<span class="number">0</span>, x))).collect()</div><div class="line">recommendations = sorted(predictions, key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>], reverse=<span class="keyword">True</span>)[:<span class="number">50</span>]</div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">"Movies recommended for you:"</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(recommendations)):</div><div class="line">    <span class="keyword">print</span> (<span class="string">"%2d: %s"</span> % (i + <span class="number">1</span>, movies[recommendations[i][<span class="number">1</span>]])).encode(<span class="string">'ascii'</span>, <span class="string">'ignore'</span>)</div></pre></td></tr></table></figure><p>The output should be similar to</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Movies recommended for you:</div><div class="line"> 1: Silence of the Lambs, The (1991)</div><div class="line"> 2: Saving Private Ryan (1998)</div><div class="line"> 3: Godfather, The (1972)</div><div class="line"> 4: Star Wars: Episode IV - A New Hope (1977)</div><div class="line"> 5: Braveheart (1995)</div><div class="line"> 6: Schindler's List (1993)</div><div class="line"> 7: Shawshank Redemption, The (1994)</div><div class="line"> 8: Star Wars: Episode V - The Empire Strikes Back (1980)</div><div class="line"> 9: Pulp Fiction (1994)</div><div class="line">10: Alien (1979)</div><div class="line">...</div></pre></td></tr></table></figure><h2 id="Comparing-to-a-naive-baseline"><a href="#Comparing-to-a-naive-baseline" class="headerlink" title="Comparing to a naive baseline"></a>Comparing to a naive baseline</h2><p>Does ALS output a non-trivial model? We can compare the evaluation result with a naive baseline model that only outputs the average rating (or you may try one that outputs the average rating per movie). Computing the baseline’s RMSE is straightforward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">meanRating = training.union(validation).map(<span class="keyword">lambda</span> x: x[<span class="number">2</span>]).mean()</div><div class="line">baselineRmse = sqrt(test.map(<span class="keyword">lambda</span> x: (meanRating - x[<span class="number">2</span>]) ** <span class="number">2</span>).reduce(add) / numTest)</div><div class="line">improvement = (baselineRmse - testRmse) / baselineRmse * <span class="number">100</span></div><div class="line"><span class="keyword">print</span> <span class="string">"The best model improves the baseline by %.2f"</span> % (improvement) + <span class="string">"%."</span></div></pre></td></tr></table></figure><p>The output should be similar to</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">The best model improves the baseline by <span class="number">20.96</span>%.</div></pre></td></tr></table></figure><p>It seems obvious that the trained model would outperform the naive baseline. However, a bad combination of training parameters would lead to a model worse than this naive baseline. Choosing the right set of parameters is quite important for this task.</p><h2 id="Solution-code"><a href="#Solution-code" class="headerlink" title="Solution code"></a>Solution code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/env python</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</div><div class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> add</div><div class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join, isfile, dirname</div><div class="line"></div><div class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</div><div class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseRating</span><span class="params">(line)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Parses a rating record in MovieLens format userId::movieId::rating::timestamp .</div><div class="line">    """</div><div class="line">    fields = line.strip().split(<span class="string">"::"</span>)</div><div class="line">    <span class="keyword">return</span> long(fields[<span class="number">3</span>]) % <span class="number">10</span>, (int(fields[<span class="number">0</span>]), int(fields[<span class="number">1</span>]), float(fields[<span class="number">2</span>]))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parseMovie</span><span class="params">(line)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Parses a movie record in MovieLens format movieId::movieTitle .</div><div class="line">    """</div><div class="line">    fields = line.strip().split(<span class="string">"::"</span>)</div><div class="line">    <span class="keyword">return</span> int(fields[<span class="number">0</span>]), fields[<span class="number">1</span>]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadRatings</span><span class="params">(ratingsFile)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Load ratings from file.</div><div class="line">    """</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isfile(ratingsFile):</div><div class="line">        <span class="keyword">print</span> <span class="string">"File %s does not exist."</span> % ratingsFile</div><div class="line">        sys.exit(<span class="number">1</span>)</div><div class="line">    f = open(ratingsFile, <span class="string">'r'</span>)</div><div class="line">    ratings = filter(<span class="keyword">lambda</span> r: r[<span class="number">2</span>] &gt; <span class="number">0</span>, [parseRating(line)[<span class="number">1</span>] <span class="keyword">for</span> line <span class="keyword">in</span> f])</div><div class="line">    f.close()</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ratings:</div><div class="line">        <span class="keyword">print</span> <span class="string">"No ratings provided."</span></div><div class="line">        sys.exit(<span class="number">1</span>)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> ratings</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeRmse</span><span class="params">(model, data, n)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Compute RMSE (Root Mean Squared Error).</div><div class="line">    """</div><div class="line">    predictions = model.predictAll(data.map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>], x[<span class="number">1</span>])))</div><div class="line">    predictionsAndRatings = predictions.map(<span class="keyword">lambda</span> x: ((x[<span class="number">0</span>], x[<span class="number">1</span>]), x[<span class="number">2</span>])) \</div><div class="line">      .join(data.map(<span class="keyword">lambda</span> x: ((x[<span class="number">0</span>], x[<span class="number">1</span>]), x[<span class="number">2</span>]))) \</div><div class="line">      .values()</div><div class="line">    <span class="keyword">return</span> sqrt(predictionsAndRatings.map(<span class="keyword">lambda</span> x: (x[<span class="number">0</span>] - x[<span class="number">1</span>]) ** <span class="number">2</span>).reduce(add) / float(n))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    <span class="keyword">if</span> (len(sys.argv) != <span class="number">3</span>):</div><div class="line">        <span class="keyword">print</span> <span class="string">"Usage: /path/to/spark/bin/spark-submit --driver-memory 2g "</span> + \</div><div class="line">          <span class="string">"MovieLensALS.py movieLensDataDir personalRatingsFile"</span></div><div class="line">        sys.exit(<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="comment"># set up environment</span></div><div class="line">    conf = SparkConf() \</div><div class="line">      .setAppName(<span class="string">"MovieLensALS"</span>) \</div><div class="line">      .set(<span class="string">"spark.executor.memory"</span>, <span class="string">"2g"</span>)</div><div class="line">    sc = SparkContext(conf=conf)</div><div class="line"></div><div class="line">    <span class="comment"># load personal ratings</span></div><div class="line">    myRatings = loadRatings(sys.argv[<span class="number">2</span>])</div><div class="line">    myRatingsRDD = sc.parallelize(myRatings, <span class="number">1</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># load ratings and movie titles</span></div><div class="line"></div><div class="line">    movieLensHomeDir = sys.argv[<span class="number">1</span>]</div><div class="line"></div><div class="line">    <span class="comment"># ratings is an RDD of (last digit of timestamp, (userId, movieId, rating))</span></div><div class="line">    ratings = sc.textFile(join(movieLensHomeDir, <span class="string">"ratings.dat"</span>)).map(parseRating)</div><div class="line"></div><div class="line">    <span class="comment"># movies is an RDD of (movieId, movieTitle)</span></div><div class="line">    movies = dict(sc.textFile(join(movieLensHomeDir, <span class="string">"movies.dat"</span>)).map(parseMovie).collect())</div><div class="line"></div><div class="line">    numRatings = ratings.count()</div><div class="line">    numUsers = ratings.values().map(<span class="keyword">lambda</span> r: r[<span class="number">0</span>]).distinct().count()</div><div class="line">    numMovies = ratings.values().map(<span class="keyword">lambda</span> r: r[<span class="number">1</span>]).distinct().count()</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Got %d ratings from %d users on %d movies."</span> % (numRatings, numUsers, numMovies)</div><div class="line"></div><div class="line">    <span class="comment"># split ratings into train (60%), validation (20%), and test (20%) based on the </span></div><div class="line">    <span class="comment"># last digit of the timestamp, add myRatings to train, and cache them</span></div><div class="line"></div><div class="line">    <span class="comment"># training, validation, test are all RDDs of (userId, movieId, rating)</span></div><div class="line"></div><div class="line">    numPartitions = <span class="number">4</span></div><div class="line">    training = ratings.filter(<span class="keyword">lambda</span> x: x[<span class="number">0</span>] &lt; <span class="number">6</span>) \</div><div class="line">      .values() \</div><div class="line">      .union(myRatingsRDD) \</div><div class="line">      .repartition(numPartitions) \</div><div class="line">      .cache()</div><div class="line"></div><div class="line">    validation = ratings.filter(<span class="keyword">lambda</span> x: x[<span class="number">0</span>] &gt;= <span class="number">6</span> <span class="keyword">and</span> x[<span class="number">0</span>] &lt; <span class="number">8</span>) \</div><div class="line">      .values() \</div><div class="line">      .repartition(numPartitions) \</div><div class="line">      .cache()</div><div class="line"></div><div class="line">    test = ratings.filter(<span class="keyword">lambda</span> x: x[<span class="number">0</span>] &gt;= <span class="number">8</span>).values().cache()</div><div class="line"></div><div class="line">    numTraining = training.count()</div><div class="line">    numValidation = validation.count()</div><div class="line">    numTest = test.count()</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Training: %d, validation: %d, test: %d"</span> % (numTraining, numValidation, numTest)</div><div class="line"></div><div class="line">    <span class="comment"># train models and evaluate them on the validation set</span></div><div class="line"></div><div class="line">    ranks = [<span class="number">8</span>, <span class="number">12</span>]</div><div class="line">    lambdas = [<span class="number">0.1</span>, <span class="number">10.0</span>]</div><div class="line">    numIters = [<span class="number">10</span>, <span class="number">20</span>]</div><div class="line">    bestModel = <span class="keyword">None</span></div><div class="line">    bestValidationRmse = float(<span class="string">"inf"</span>)</div><div class="line">    bestRank = <span class="number">0</span></div><div class="line">    bestLambda = <span class="number">-1.0</span></div><div class="line">    bestNumIter = <span class="number">-1</span></div><div class="line"></div><div class="line">    <span class="keyword">for</span> rank, lmbda, numIter <span class="keyword">in</span> itertools.product(ranks, lambdas, numIters):</div><div class="line">        model = ALS.train(training, rank, numIter, lmbda)</div><div class="line">        validationRmse = computeRmse(model, validation, numValidation)</div><div class="line">        <span class="keyword">print</span> <span class="string">"RMSE (validation) = %f for the model trained with "</span> % validationRmse + \</div><div class="line">              <span class="string">"rank = %d, lambda = %.1f, and numIter = %d."</span> % (rank, lmbda, numIter)</div><div class="line">        <span class="keyword">if</span> (validationRmse &lt; bestValidationRmse):</div><div class="line">            bestModel = model</div><div class="line">            bestValidationRmse = validationRmse</div><div class="line">            bestRank = rank</div><div class="line">            bestLambda = lmbda</div><div class="line">            bestNumIter = numIter</div><div class="line"></div><div class="line">    testRmse = computeRmse(bestModel, test, numTest)</div><div class="line"></div><div class="line">    <span class="comment"># evaluate the best model on the test set</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"The best model was trained with rank = %d and lambda = %.1f, "</span> % (bestRank, bestLambda) \</div><div class="line">      + <span class="string">"and numIter = %d, and its RMSE on the test set is %f."</span> % (bestNumIter, testRmse)</div><div class="line"></div><div class="line">    <span class="comment"># compare the best model with a naive baseline that always returns the mean rating</span></div><div class="line">    meanRating = training.union(validation).map(<span class="keyword">lambda</span> x: x[<span class="number">2</span>]).mean()</div><div class="line">    baselineRmse = sqrt(test.map(<span class="keyword">lambda</span> x: (meanRating - x[<span class="number">2</span>]) ** <span class="number">2</span>).reduce(add) / numTest)</div><div class="line">    improvement = (baselineRmse - testRmse) / baselineRmse * <span class="number">100</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"The best model improves the baseline by %.2f"</span> % (improvement) + <span class="string">"%."</span></div><div class="line"></div><div class="line">    <span class="comment"># make personalized recommendations</span></div><div class="line"></div><div class="line">    myRatedMovieIds = set([x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> myRatings])</div><div class="line">    candidates = sc.parallelize([m <span class="keyword">for</span> m <span class="keyword">in</span> movies <span class="keyword">if</span> m <span class="keyword">not</span> <span class="keyword">in</span> myRatedMovieIds])</div><div class="line">    predictions = bestModel.predictAll(candidates.map(<span class="keyword">lambda</span> x: (<span class="number">0</span>, x))).collect()</div><div class="line">    recommendations = sorted(predictions, key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>], reverse=<span class="keyword">True</span>)[:<span class="number">50</span>]</div><div class="line"></div><div class="line">    <span class="keyword">print</span> <span class="string">"Movies recommended for you:"</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(recommendations)):</div><div class="line">        <span class="keyword">print</span> (<span class="string">"%2d: %s"</span> % (i + <span class="number">1</span>, movies[recommendations[i][<span class="number">1</span>]])).encode(<span class="string">'ascii'</span>, <span class="string">'ignore'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># clean up</span></div><div class="line">    sc.stop()</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/25/Machine-Learning-with-MLlib-of-Spark/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/25/Machine-Learning-with-MLlib-of-Spark/" itemprop="url">Machine Learning with MLlib of Spark</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-25T19:13:07+08:00">2017-04-25 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/25/Machine-Learning-with-MLlib-of-Spark/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/25/Machine-Learning-with-MLlib-of-Spark/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/25/Machine-Learning-with-MLlib-of-Spark/" class="leancloud_visitors" data-flag-title="Machine Learning with MLlib of Spark"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Example-Spam-Classification"><a href="#Example-Spam-Classification" class="headerlink" title="Example: Spam Classification"></a>Example: Spam Classification</h2><p>This program uses two MLlib algorithms: <code>HashingTF</code>, which builds term frequency feature vectors from text data, and LogisticRegressionWithSGD, which implements the logistic regression procedure using stochastic gradient descent (<code>SGD</code>). We assume that we start with two files, spam.txt an normal.txt, each of which contains examples of spam and non-spam emails, one per line. We then turn the text in each file into a feature vector with <code>TF</code>, and train a logistic regression model to separate the two types of messages.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Licensed to the Apache Software Foundation (ASF) under one or more</span></div><div class="line"><span class="comment"># contributor license agreements.  See the NOTICE file distributed with</span></div><div class="line"><span class="comment"># this work for additional information regarding copyright ownership.</span></div><div class="line"><span class="comment"># The ASF licenses this file to You under the Apache License, Version 2.0</span></div><div class="line"><span class="comment"># (the "License"); you may not use this file except in compliance with</span></div><div class="line"><span class="comment"># the License.  You may obtain a copy of the License at</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment">#    http://www.apache.org/licenses/LICENSE-2.0</span></div><div class="line"><span class="comment">#</span></div><div class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></div><div class="line"><span class="comment"># distributed under the License is distributed on an "AS IS" BASIS,</span></div><div class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></div><div class="line"><span class="comment"># See the License for the specific language governing permissions and</span></div><div class="line"><span class="comment"># limitations under the License.</span></div><div class="line"><span class="comment">#</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> LogisticRegressionWithSGD</div><div class="line"><span class="keyword">from</span> pyspark.mllib.feature <span class="keyword">import</span> HashingTF</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    sc = SparkContext(appName=<span class="string">"PythonBookExample"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Load 2 types of emails from text files: spam and ham (non-spam).</span></div><div class="line">    <span class="comment"># Each line has text from one email.</span></div><div class="line">    spam = sc.textFile(<span class="string">"file:///home/hduser/learning-spark/files/spam.txt"</span>)</div><div class="line">    ham = sc.textFile(<span class="string">"file:///home/hduser/learning-spark/files/ham.txt"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Create a HashingTF instance to map email text to vectors of 100 features.</span></div><div class="line">    tf = HashingTF(numFeatures = <span class="number">100</span>)</div><div class="line">    <span class="comment"># Each email is split into words, and each word is mapped to one feature.</span></div><div class="line">    spamFeatures = spam.map(<span class="keyword">lambda</span> email: tf.transform(email.split(<span class="string">" "</span>)))</div><div class="line">    hamFeatures = ham.map(<span class="keyword">lambda</span> email: tf.transform(email.split(<span class="string">" "</span>)))</div><div class="line"></div><div class="line">    <span class="comment"># Create LabeledPoint datasets for positive (spam) and negative (ham) examples.</span></div><div class="line">    positiveExamples = spamFeatures.map(<span class="keyword">lambda</span> features: LabeledPoint(<span class="number">1</span>, features))</div><div class="line">    negativeExamples = hamFeatures.map(<span class="keyword">lambda</span> features: LabeledPoint(<span class="number">0</span>, features))</div><div class="line">    training_data = positiveExamples.union(negativeExamples)</div><div class="line">    training_data.cache() <span class="comment"># Cache data since Logistic Regression is an iterative algorithm.</span></div><div class="line"></div><div class="line">    <span class="comment"># Run Logistic Regression using the SGD optimizer.</span></div><div class="line">    <span class="comment"># regParam is model regularization, which can make models more robust.</span></div><div class="line">    model = LogisticRegressionWithSGD.train(training_data)</div><div class="line"></div><div class="line">    <span class="comment"># Test on a positive example (spam) and a negative one (ham).</span></div><div class="line">    <span class="comment"># First apply the same HashingTF feature transformation used on the training data.</span></div><div class="line">    posTestExample = tf.transform(<span class="string">"O M G GET cheap stuff by sending money to ..."</span>.split(<span class="string">" "</span>))</div><div class="line">    negTestExample = tf.transform(<span class="string">"Hi Dad, I started studying Spark the other ..."</span>.split(<span class="string">" "</span>))</div><div class="line"></div><div class="line">    <span class="comment"># Now use the learned model to predict spam/ham for new emails.</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"Prediction for positive test example: %g"</span> % model.predict(posTestExample)</div><div class="line">    <span class="keyword">print</span> <span class="string">"Prediction for negative test example: %g"</span> % model.predict(negTestExample)</div><div class="line"></div><div class="line">    sc.stop()</div></pre></td></tr></table></figure><h2 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h2><p><strong><em>Here only has some usual APIs.</em></strong></p><h3 id="Feature-Extraction"><a href="#Feature-Extraction" class="headerlink" title="Feature Extraction"></a>Feature Extraction</h3><h4 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h4><p>Most machine learning algorithms consider the magnitude of each element in the feature vector, and thus work best when the features are scaled so they weigh equally (e.g., all features have a mean of 0 and standard deviation of 1). Once you have built feature vectors, you can use the StandardScaler class in MLlib to do this scaling, both for the mean and the standard deviation. You create a StandardScaler, call fit() on a dataset to obtain a StandardScalerModel (i.e., compute the mean and variance of each column), and then call transform() on the model to scale a dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.feature <span class="keyword">import</span> StandardScaler</div><div class="line"></div><div class="line">vectors = [Vectors.dense([<span class="number">-2.0</span>, <span class="number">5.0</span>, <span class="number">1.0</span>]), Vectors.dense([<span class="number">2.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>])]</div><div class="line">dataset = sc.parallelize(vectors)</div><div class="line">scaler = StandardScaler(withMean=<span class="keyword">True</span>, withStd=<span class="keyword">True</span>)</div><div class="line">model = scaler.fit(dataset)</div><div class="line">result = model.transform(dataset)</div><div class="line"></div><div class="line"><span class="comment"># Result: &#123;[-0.7071, 0.7071, 0.0], [0.7071, -0.7071, 0.0]&#125;</span></div></pre></td></tr></table></figure><h4 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h4><p>Simply use <code>Normalizer().transform(rdd)</code>. By default Normalizer uses the L 2 norm (i.e, Euclidean length), but you can also pass a power <code>p</code>to Normalizer to use the L p norm.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.feature <span class="keyword">import</span> Normalizer</div><div class="line"><span class="keyword">from</span> pyspark.mllib.util <span class="keyword">import</span> MLUtils</div><div class="line"></div><div class="line">data = MLUtils.loadLibSVMFile(sc, <span class="string">"data/mllib/sample_libsvm_data.txt"</span>)</div><div class="line">labels = data.map(<span class="keyword">lambda</span> x: x.label)</div><div class="line">features = data.map(<span class="keyword">lambda</span> x: x.features)</div><div class="line"></div><div class="line">normalizer1 = Normalizer()</div><div class="line">normalizer2 = Normalizer(p=float(<span class="string">"inf"</span>))</div><div class="line"></div><div class="line"><span class="comment"># Each sample in data1 will be normalized using $L^2$ norm.</span></div><div class="line">data1 = labels.zip(normalizer1.transform(features))</div><div class="line"></div><div class="line"><span class="comment"># Each sample in data2 will be normalized using $L^\infty$ norm.</span></div><div class="line">data2 = labels.zip(normalizer2.transform(features))</div></pre></td></tr></table></figure><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>Once you have trained the model (withWord2Vec.fit(rdd)), you will receive a Word2VecModel that can be used to transform() each word into a vector. Note that the size of the models in Word2Vec will be equal to the number of words in your vocabulary times the size of a vector (by default, 100). You may wish to filter out words that are not in a standard dictionary to limit the size. In general, a good size for the vocabulary is 100,000 words.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.feature <span class="keyword">import</span> Word2Vec</div><div class="line"></div><div class="line">inp = sc.textFile(<span class="string">"data/mllib/sample_lda_data.txt"</span>).map(<span class="keyword">lambda</span> row: row.split(<span class="string">" "</span>))</div><div class="line"></div><div class="line">word2vec = Word2Vec()</div><div class="line">model = word2vec.fit(inp)</div><div class="line"></div><div class="line">synonyms = model.findSynonyms(<span class="string">'1'</span>, <span class="number">5</span>)</div><div class="line"></div><div class="line"><span class="keyword">for</span> word, cosine_distance <span class="keyword">in</span> synonyms:</div><div class="line">    print(<span class="string">"&#123;&#125;: &#123;&#125;"</span>.format(word, cosine_distance))</div></pre></td></tr></table></figure><h3 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h3><p><strong><em>Statistics.colStats(rdd)</em></strong><br>Computes a statistical summary of an RDD of vectors, which stores the min, max, mean, and variance for each column in the set of vectors. This can be used to obtain a wide variety of statistics in one pass.</p><p><strong><em>Statistics.corr(rdd, method)</em></strong><br>Computes the correlation matrix between columns in an RDD of vectors, using either the Pearson or Spearman correlation (method must be one of pearson and spearman).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.stat <span class="keyword">import</span> Statistics</div><div class="line"></div><div class="line">seriesX = sc.parallelize([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">3.0</span>, <span class="number">5.0</span>])  <span class="comment"># a series</span></div><div class="line"><span class="comment"># seriesY must have the same number of partitions and cardinality as seriesX</span></div><div class="line">seriesY = sc.parallelize([<span class="number">11.0</span>, <span class="number">22.0</span>, <span class="number">33.0</span>, <span class="number">33.0</span>, <span class="number">555.0</span>])</div><div class="line"></div><div class="line"><span class="comment"># Compute the correlation using Pearson's method. Enter "spearman" for Spearman's method.</span></div><div class="line"><span class="comment"># If a method is not specified, Pearson's method will be used by default.</span></div><div class="line">print(<span class="string">"Correlation is: "</span> + str(Statistics.corr(seriesX, seriesY, method=<span class="string">"pearson"</span>)))</div><div class="line"></div><div class="line">data = sc.parallelize(</div><div class="line">    [np.array([<span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>]), np.array([<span class="number">2.0</span>, <span class="number">20.0</span>, <span class="number">200.0</span>]), np.array([<span class="number">5.0</span>, <span class="number">33.0</span>, <span class="number">366.0</span>])]</div><div class="line">)  <span class="comment"># an RDD of Vectors</span></div><div class="line"></div><div class="line"><span class="comment"># calculate the correlation matrix using Pearson's method. Use "spearman" for Spearman's method.</span></div><div class="line"><span class="comment"># If a method is not specified, Pearson's method will be used by default.</span></div><div class="line">print(Statistics.corr(data, method=<span class="string">"pearson"</span>))</div></pre></td></tr></table></figure><p><strong><em>Statistics.corr(rdd1, rdd2, method)</em></strong><br>Computes the correlation between two RDDs of floating-point values, using either the Pearson or Spearman correlation (method must be one of pearson and spearman).</p><p><strong><em>Statistics.chiSqTest(rdd)</em></strong><br>Computes Pearson’s independence test for every feature with the label on an RDD of LabeledPoint objects. Returns an array of ChiSqTestResult objects that capture the p-value, test statistic, and degrees of freedom for each feature. Label and feature values must be categorical (i.e., discrete values).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.linalg <span class="keyword">import</span> Matrices, Vectors</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"><span class="keyword">from</span> pyspark.mllib.stat <span class="keyword">import</span> Statistics</div><div class="line"></div><div class="line">vec = Vectors.dense(<span class="number">0.1</span>, <span class="number">0.15</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.25</span>)  <span class="comment"># a vector composed of the frequencies of events</span></div><div class="line"></div><div class="line"><span class="comment"># compute the goodness of fit. If a second vector to test against</span></div><div class="line"><span class="comment"># is not supplied as a parameter, the test runs against a uniform distribution.</span></div><div class="line">goodnessOfFitTestResult = Statistics.chiSqTest(vec)</div><div class="line"></div><div class="line"><span class="comment"># summary of the test including the p-value, degrees of freedom,</span></div><div class="line"><span class="comment"># test statistic, the method used, and the null hypothesis.</span></div><div class="line">print(<span class="string">"%s\n"</span> % goodnessOfFitTestResult)</div><div class="line"></div><div class="line">mat = Matrices.dense(<span class="number">3</span>, <span class="number">2</span>, [<span class="number">1.0</span>, <span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>])  <span class="comment"># a contingency matrix</span></div><div class="line"></div><div class="line"><span class="comment"># conduct Pearson's independence test on the input contingency matrix</span></div><div class="line">independenceTestResult = Statistics.chiSqTest(mat)</div><div class="line"></div><div class="line"><span class="comment"># summary of the test including the p-value, degrees of freedom,</span></div><div class="line"><span class="comment"># test statistic, the method used, and the null hypothesis.</span></div><div class="line">print(<span class="string">"%s\n"</span> % independenceTestResult)</div><div class="line"></div><div class="line">obs = sc.parallelize(</div><div class="line">    [LabeledPoint(<span class="number">1.0</span>, [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>]),</div><div class="line">     LabeledPoint(<span class="number">1.0</span>, [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">0.0</span>]),</div><div class="line">     LabeledPoint(<span class="number">1.0</span>, [<span class="number">-1.0</span>, <span class="number">0.0</span>, <span class="number">-0.5</span>])]</div><div class="line">)  <span class="comment"># LabeledPoint(feature, label)</span></div><div class="line"></div><div class="line"><span class="comment"># The contingency table is constructed from an RDD of LabeledPoint and used to conduct</span></div><div class="line"><span class="comment"># the independence test. Returns an array containing the ChiSquaredTestResult for every feature</span></div><div class="line"><span class="comment"># against the label.</span></div><div class="line">featureTestResults = Statistics.chiSqTest(obs)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i, result <span class="keyword">in</span> enumerate(featureTestResults):</div><div class="line">    print(<span class="string">"Column %d:\n%s"</span> % (i + <span class="number">1</span>, result))</div></pre></td></tr></table></figure><h3 id="Classification-and-Regression"><a href="#Classification-and-Regression" class="headerlink" title="Classification and Regression"></a>Classification and Regression</h3><p>MLlib includes a variety of methods for classification and regression, including simple linear methods and decision trees and forests.</p><h4 id="Linear-regression"><a href="#Linear-regression" class="headerlink" title="Linear regression"></a>Linear regression</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LinearRegressionWithSGD</div><div class="line"></div><div class="line">points = <span class="comment"># (create RDD of LabeledPoint)</span></div><div class="line">model = LinearRegressionWithSGD.train(points, iterations=<span class="number">200</span>, intercept=<span class="keyword">True</span>)</div><div class="line"><span class="keyword">print</span> <span class="string">"weights: %s, intercept: %s"</span> % (model.weights, model.intercept)</div></pre></td></tr></table></figure><h4 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h4><p>The logistic regression algorithm has a very similar API to linear regression, covered in the previous section. One difference is that there are two algorithms available for solving it: <code>SGD</code> and <code>LBFGS</code>. <code>LBFGS</code> is generally the best choice, but is not available in some earlier versions of <code>MLlib</code> (before <code>Spark 1.2</code>). These algorithms are available in the <code>mllib.classification.LogisticRegressionWithLBFGS</code> and <code>WithSGD</code> classes, which have interfaces similar to <code>LinearRegressionWithSGD</code>. They take all the same parameters as linear regression.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> LogisticRegressionWithLBFGS, LogisticRegressionModel</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsePoint</span><span class="params">(line)</span>:</span></div><div class="line">    values = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">' '</span>)]</div><div class="line">    <span class="keyword">return</span> LabeledPoint(values[<span class="number">0</span>], values[<span class="number">1</span>:])</div><div class="line"></div><div class="line">data = sc.textFile(<span class="string">"data/mllib/sample_svm_data.txt"</span>)</div><div class="line">parsedData = data.map(parsePoint)</div><div class="line"></div><div class="line"><span class="comment"># Build the model</span></div><div class="line">model = LogisticRegressionWithLBFGS.train(parsedData)</div><div class="line"></div><div class="line"><span class="comment"># Evaluating the model on training data</span></div><div class="line">labelsAndPreds = parsedData.map(<span class="keyword">lambda</span> p: (p.label, model.predict(p.features)))</div><div class="line">trainErr = labelsAndPreds.filter(<span class="keyword">lambda</span> (v, p): v != p).count() / float(parsedData.count())</div><div class="line">print(<span class="string">"Training Error = "</span> + str(trainErr))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">model.save(sc, <span class="string">"target/tmp/pythonLogisticRegressionWithLBFGSModel"</span>)</div><div class="line">sameModel = LogisticRegressionModel.load(sc,</div><div class="line">                                         <span class="string">"target/tmp/pythonLogisticRegressionWithLBFGSModel"</span>)</div></pre></td></tr></table></figure><h4 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h4><p>They are available through the <code>SVMWithSGD</code> class, with similar parameters to linear and logisitic regression. The returned <code>SVMModel</code> uses a threshold for prediction like <code>LogisticRegressionModel</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> SVMWithSGD, SVMModel</div><div class="line"><span class="keyword">from</span> pyspark.mllib.regression <span class="keyword">import</span> LabeledPoint</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">parsePoint</span><span class="params">(line)</span>:</span></div><div class="line">    values = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">' '</span>)]</div><div class="line">    <span class="keyword">return</span> LabeledPoint(values[<span class="number">0</span>], values[<span class="number">1</span>:])</div><div class="line"></div><div class="line">data = sc.textFile(<span class="string">"data/mllib/sample_svm_data.txt"</span>)</div><div class="line">parsedData = data.map(parsePoint)</div><div class="line"></div><div class="line"><span class="comment"># Build the model</span></div><div class="line">model = SVMWithSGD.train(parsedData, iterations=<span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># Evaluating the model on training data</span></div><div class="line">labelsAndPreds = parsedData.map(<span class="keyword">lambda</span> p: (p.label, model.predict(p.features)))</div><div class="line">trainErr = labelsAndPreds.filter(<span class="keyword">lambda</span> (v, p): v != p).count() / float(parsedData.count())</div><div class="line">print(<span class="string">"Training Error = "</span> + str(trainErr))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">model.save(sc, <span class="string">"target/tmp/pythonSVMWithSGDModel"</span>)</div><div class="line">sameModel = SVMModel.load(sc, <span class="string">"target/tmp/pythonSVMWithSGDModel"</span>)</div></pre></td></tr></table></figure><h4 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h4><p>In <code>MLlib</code>, you can use <code>Naive Bayes</code> through the<code>mllib.classification.NaiveBayes</code> class. It supports one parameter, <code>lambda</code> (or <code>lambda_</code> in Python), used for smoothing. You can call it on an <code>RDD</code> of <code>LabeledPoints</code>, where the labels are between 0 and C–1 for C classes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.classification <span class="keyword">import</span> NaiveBayes, NaiveBayesModel</div><div class="line"><span class="keyword">from</span> pyspark.mllib.util <span class="keyword">import</span> MLUtils</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data file.</span></div><div class="line">data = MLUtils.loadLibSVMFile(sc, <span class="string">"data/mllib/sample_libsvm_data.txt"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Split data approximately into training (60%) and test (40%)</span></div><div class="line">training, test = data.randomSplit([<span class="number">0.6</span>, <span class="number">0.4</span>])</div><div class="line"></div><div class="line"><span class="comment"># Train a naive Bayes model.</span></div><div class="line">model = NaiveBayes.train(training, <span class="number">1.0</span>)</div><div class="line"></div><div class="line"><span class="comment"># Make prediction and test accuracy.</span></div><div class="line">predictionAndLabel = test.map(<span class="keyword">lambda</span> p: (model.predict(p.features), p.label))</div><div class="line">accuracy = <span class="number">1.0</span> * predictionAndLabel.filter(<span class="keyword">lambda</span> (x, v): x == v).count() / test.count()</div><div class="line">print(<span class="string">'model accuracy &#123;&#125;'</span>.format(accuracy))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">output_dir = <span class="string">'target/tmp/myNaiveBayesModel'</span></div><div class="line">shutil.rmtree(output_dir, ignore_errors=<span class="keyword">True</span>)</div><div class="line">model.save(sc, output_dir)</div><div class="line">sameModel = NaiveBayesModel.load(sc, output_dir)</div><div class="line">predictionAndLabel = test.map(<span class="keyword">lambda</span> p: (sameModel.predict(p.features), p.label))</div><div class="line">accuracy = <span class="number">1.0</span> * predictionAndLabel.filter(<span class="keyword">lambda</span> (x, v): x == v).count() / test.count()</div><div class="line">print(<span class="string">'sameModel accuracy &#123;&#125;'</span>.format(accuracy))</div></pre></td></tr></table></figure><h4 id="Decision-trees-and-random-forests"><a href="#Decision-trees-and-random-forests" class="headerlink" title="Decision trees and random forests"></a>Decision trees and random forests</h4><p>In <code>MLlib</code>, you can train trees using the <code>mllib.tree.DecisionTree</code> class, through the static methods <code>trainClassifier()</code> and <code>trainRegressor()</code>. Unlike in some of the other algorithms, the Java and Scala APIs also use static methods instead of a <code>DecisionTree</code> object with setters.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.tree <span class="keyword">import</span> DecisionTree, DecisionTreeModel</div><div class="line"><span class="keyword">from</span> pyspark.mllib.util <span class="keyword">import</span> MLUtils</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data file into an RDD of LabeledPoint.</span></div><div class="line">data = MLUtils.loadLibSVMFile(sc, <span class="string">'data/mllib/sample_libsvm_data.txt'</span>)</div><div class="line"><span class="comment"># Split the data into training and test sets (30% held out for testing)</span></div><div class="line">(trainingData, testData) = data.randomSplit([<span class="number">0.7</span>, <span class="number">0.3</span>])</div><div class="line"></div><div class="line"><span class="comment"># Train a DecisionTree model.</span></div><div class="line"><span class="comment">#  Empty categoricalFeaturesInfo indicates all features are continuous.</span></div><div class="line">model = DecisionTree.trainClassifier(trainingData, numClasses=<span class="number">2</span>, categoricalFeaturesInfo=&#123;&#125;,</div><div class="line">                                     impurity=<span class="string">'gini'</span>, maxDepth=<span class="number">5</span>, maxBins=<span class="number">32</span>)</div><div class="line"></div><div class="line"><span class="comment"># Evaluate model on test instances and compute test error</span></div><div class="line">predictions = model.predict(testData.map(<span class="keyword">lambda</span> x: x.features))</div><div class="line">labelsAndPredictions = testData.map(<span class="keyword">lambda</span> lp: lp.label).zip(predictions)</div><div class="line">testErr = labelsAndPredictions.filter(<span class="keyword">lambda</span> (v, p): v != p).count() / float(testData.count())</div><div class="line">print(<span class="string">'Test Error = '</span> + str(testErr))</div><div class="line">print(<span class="string">'Learned classification tree model:'</span>)</div><div class="line">print(model.toDebugString())</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">model.save(sc, <span class="string">"target/tmp/myDecisionTreeClassificationModel"</span>)</div><div class="line">sameModel = DecisionTreeModel.load(sc, <span class="string">"target/tmp/myDecisionTreeClassificationModel"</span>)</div></pre></td></tr></table></figure><h3 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h3><h4 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> array</div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</div><div class="line"></div><div class="line"><span class="keyword">from</span> pyspark.mllib.clustering <span class="keyword">import</span> KMeans, KMeansModel</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data</span></div><div class="line">data = sc.textFile(<span class="string">"data/mllib/kmeans_data.txt"</span>)</div><div class="line">parsedData = data.map(<span class="keyword">lambda</span> line: array([float(x) <span class="keyword">for</span> x <span class="keyword">in</span> line.split(<span class="string">' '</span>)]))</div><div class="line"></div><div class="line"><span class="comment"># Build the model (cluster the data)</span></div><div class="line">clusters = KMeans.train(parsedData, <span class="number">2</span>, maxIterations=<span class="number">10</span>, initializationMode=<span class="string">"random"</span>)</div><div class="line"></div><div class="line"><span class="comment"># Evaluate clustering by computing Within Set Sum of Squared Errors</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">error</span><span class="params">(point)</span>:</span></div><div class="line">    center = clusters.centers[clusters.predict(point)]</div><div class="line">    <span class="keyword">return</span> sqrt(sum([x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> (point - center)]))</div><div class="line"></div><div class="line">WSSSE = parsedData.map(<span class="keyword">lambda</span> point: error(point)).reduce(<span class="keyword">lambda</span> x, y: x + y)</div><div class="line">print(<span class="string">"Within Set Sum of Squared Error = "</span> + str(WSSSE))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">clusters.save(sc, <span class="string">"target/org/apache/spark/PythonKMeansExample/KMeansModel"</span>)</div><div class="line">sameModel = KMeansModel.load(sc, <span class="string">"target/org/apache/spark/PythonKMeansExample/KMeansModel"</span>)</div></pre></td></tr></table></figure><h3 id="Collaborative-Filtering-and-Recommendation"><a href="#Collaborative-Filtering-and-Recommendation" class="headerlink" title="Collaborative Filtering and Recommendation"></a>Collaborative Filtering and Recommendation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> pyspark.mllib.recommendation <span class="keyword">import</span> ALS, MatrixFactorizationModel, Rating</div><div class="line"></div><div class="line"><span class="comment"># Load and parse the data</span></div><div class="line">data = sc.textFile(<span class="string">"data/mllib/als/test.data"</span>)</div><div class="line">ratings = data.map(<span class="keyword">lambda</span> l: l.split(<span class="string">','</span>))\</div><div class="line">    .map(<span class="keyword">lambda</span> l: Rating(int(l[<span class="number">0</span>]), int(l[<span class="number">1</span>]), float(l[<span class="number">2</span>])))</div><div class="line"></div><div class="line"><span class="comment"># Build the recommendation model using Alternating Least Squares</span></div><div class="line">rank = <span class="number">10</span></div><div class="line">numIterations = <span class="number">10</span></div><div class="line">model = ALS.train(ratings, rank, numIterations)</div><div class="line"></div><div class="line"><span class="comment"># Evaluate the model on training data</span></div><div class="line">testdata = ratings.map(<span class="keyword">lambda</span> p: (p[<span class="number">0</span>], p[<span class="number">1</span>]))</div><div class="line">predictions = model.predictAll(testdata).map(<span class="keyword">lambda</span> r: ((r[<span class="number">0</span>], r[<span class="number">1</span>]), r[<span class="number">2</span>]))</div><div class="line">ratesAndPreds = ratings.map(<span class="keyword">lambda</span> r: ((r[<span class="number">0</span>], r[<span class="number">1</span>]), r[<span class="number">2</span>])).join(predictions)</div><div class="line">MSE = ratesAndPreds.map(<span class="keyword">lambda</span> r: (r[<span class="number">1</span>][<span class="number">0</span>] - r[<span class="number">1</span>][<span class="number">1</span>])**<span class="number">2</span>).mean()</div><div class="line">print(<span class="string">"Mean Squared Error = "</span> + str(MSE))</div><div class="line"></div><div class="line"><span class="comment"># Save and load model</span></div><div class="line">model.save(sc, <span class="string">"target/tmp/myCollaborativeFilter"</span>)</div><div class="line">sameModel = MatrixFactorizationModel.load(sc, <span class="string">"target/tmp/myCollaborativeFilter"</span>)</div></pre></td></tr></table></figure><p>The <a href="https://databricks-training.s3.amazonaws.com/index.html" target="_blank" rel="external">training exercises</a> from the Spark Summit 2014 include a hands-on tutorial for <a href="https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html" target="_blank" rel="external">personalized movie recommendation with <code>spark.mllib</code></a>.</p><h3 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h3><h4 id="Principal-component-analysis"><a href="#Principal-component-analysis" class="headerlink" title="Principal component analysis"></a>Principal component analysis</h4><p>PCA in Scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.<span class="type">Matrix</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.distributed.<span class="type">RowMatrix</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> points: <span class="type">RDD</span>[<span class="type">Vector</span>] = <span class="comment">// ...</span></div><div class="line"><span class="keyword">val</span> mat: <span class="type">RowMatrix</span> = <span class="keyword">new</span> <span class="type">RowMatrix</span>(points)</div><div class="line"><span class="keyword">val</span> pc: <span class="type">Matrix</span> = mat.computePrincipalComponents(<span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment">// Project points to low-dimensional space</span></div><div class="line"><span class="keyword">val</span> projected = mat.multiply(pc).rows</div><div class="line"></div><div class="line"><span class="comment">// Train a k-means model on the projected 2-dimensional data</span></div><div class="line"><span class="keyword">val</span> model = <span class="type">KMeans</span>.train(projected, <span class="number">10</span>)</div></pre></td></tr></table></figure><h4 id="Singular-value-decomposition"><a href="#Singular-value-decomposition" class="headerlink" title="Singular value decomposition"></a>Singular value decomposition</h4><p>SVD in Scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Compute the top 20 singular values of a RowMatrix mat and their singular vectors.</span></div><div class="line"><span class="keyword">val</span> svd: <span class="type">SingularValueDecomposition</span>[<span class="type">RowMatrix</span>, <span class="type">Matrix</span>] =</div><div class="line"> mat.computeSVD(<span class="number">20</span>, computeU=<span class="literal">true</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> <span class="type">U</span>: <span class="type">RowMatrix</span> = svd.<span class="type">U</span> <span class="comment">// U is a distributed RowMatrix.</span></div><div class="line"><span class="keyword">val</span> s: <span class="type">Vector</span> = svd.s <span class="comment">// Singular values are a local dense vector.</span></div><div class="line"><span class="keyword">val</span> <span class="type">V</span>: <span class="type">Matrix</span> = svd.<span class="type">V</span> <span class="comment">// V is a local dense matrix.</span></div></pre></td></tr></table></figure><h3 id="Pipeline-API"><a href="#Pipeline-API" class="headerlink" title="Pipeline API"></a>Pipeline API</h3><p>Pipeline API version of spam classification in Scala</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SQLContext</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.<span class="type">Pipeline</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.<span class="type">LogisticRegression</span></div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.&#123;<span class="type">HashingTF</span>, <span class="type">Tokenizer</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.tuning.&#123;<span class="type">CrossValidator</span>, <span class="type">ParamGridBuilder</span>&#125;</div><div class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">BinaryClassificationEvaluator</span></div><div class="line"></div><div class="line"><span class="comment">// A class to represent documents -- will be turned into a SchemaRDD</span></div><div class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">LabeledDocument</span>(<span class="params">id: <span class="type">Long</span>, text: <span class="type">String</span>, label: <span class="type">Double</span></span>)</span></div><div class="line"><span class="keyword">val</span> documents = <span class="comment">// (load RDD of LabeledDocument)</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</div><div class="line"><span class="keyword">import</span> sqlContext._</div><div class="line"></div><div class="line"><span class="comment">// Configure an ML pipeline with three stages: tokenizer, tf, and lr; each stage</span></div><div class="line"><span class="comment">// outputs a column in a SchemaRDD and feeds it to the next stage's input column</span></div><div class="line"><span class="keyword">val</span> tokenizer = <span class="keyword">new</span> <span class="type">Tokenizer</span>() <span class="comment">// Splits each email into words</span></div><div class="line"> .setInputCol(<span class="string">"text"</span>)</div><div class="line"> .setOutputCol(<span class="string">"words"</span>)</div><div class="line"><span class="keyword">val</span> tf = <span class="keyword">new</span> <span class="type">HashingTF</span>() <span class="comment">// Maps email words to vectors of 10000 features</span></div><div class="line"> .setNumFeatures(<span class="number">10000</span>)</div><div class="line"> .setInputCol(tokenizer.getOutputCol)</div><div class="line"> .setOutputCol(<span class="string">"features"</span>)</div><div class="line"><span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>() <span class="comment">// Uses "features" as inputCol by default</span></div><div class="line"><span class="keyword">val</span> pipeline = <span class="keyword">new</span> <span class="type">Pipeline</span>().setStages(<span class="type">Array</span>(tokenizer, tf, lr))</div><div class="line"></div><div class="line"><span class="comment">// Fit the pipeline to the training documents</span></div><div class="line"><span class="keyword">val</span> model = pipeline.fit(documents)</div><div class="line"></div><div class="line"><span class="comment">// Alternatively, instead of fitting once with the parameters above, we can do a</span></div><div class="line"><span class="comment">// grid search over some parameters and pick the best model via cross-validation</span></div><div class="line"><span class="keyword">val</span> paramMaps = <span class="keyword">new</span> <span class="type">ParamGridBuilder</span>()</div><div class="line"> .addGrid(tf.numFeatures, <span class="type">Array</span>(<span class="number">10000</span>, <span class="number">20000</span>))</div><div class="line"> .addGrid(lr.maxIter, <span class="type">Array</span>(<span class="number">100</span>, <span class="number">200</span>))</div><div class="line"> .build() <span class="comment">// Builds all combinations of parameters</span></div><div class="line"><span class="keyword">val</span> eval = <span class="keyword">new</span> <span class="type">BinaryClassificationEvaluator</span>()</div><div class="line"><span class="keyword">val</span> cv = <span class="keyword">new</span> <span class="type">CrossValidator</span>()</div><div class="line"> .setEstimator(lr)</div><div class="line"> .setEstimatorParamMaps(paramMaps)</div><div class="line"> .setEvaluator(eval)</div><div class="line"><span class="keyword">val</span> bestModel = cv.fit(documents)</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/23/Solution-for-Bracket-in-markdown-link-address/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/23/Solution-for-Bracket-in-markdown-link-address/" itemprop="url">Solution for Bracket in markdown link address</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-23T19:56:40+08:00">2017-04-23 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/23/Solution-for-Bracket-in-markdown-link-address/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/23/Solution-for-Bracket-in-markdown-link-address/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/23/Solution-for-Bracket-in-markdown-link-address/" class="leancloud_visitors" data-flag-title="Solution for Bracket in markdown link address"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Markdown创造一个链接或者图片是使用 <code>[title](link)</code> 和 <code>![title](link)</code>.</p><p>我们可以避免<code>[]</code>内出现中括号, 或者使用转义.</p><p>但是在小括号的链接里面就可能会出问题. 有些网址上面会具有小括号. 例如,</p><p><a href="https://github.com/CjTouzi/Learning-RSpark/blob/master/Zaharia%20M.%2C%20et%20al.%20Learning%20Spark%20%28O%27Reilly%2C%202015%29%28274s%29.pdf" target="_blank" rel="external">https://github.com/CjTouzi/Learning-RSpark/blob/master/Zaharia%20M.%2C%20et%20al.%20Learning%20Spark%20%28O%27Reilly%2C%202015%29%28274s%29.pdf</a></p><p>解决方法:</p><p><code>%28</code> 代替<code>(</code>, <code>%29</code>代替<code>)</code> 主要是后者会歧义链接部分的结束. 这是使用url符号码去代替ascii的符号. 能够解决这个问题</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/23/The-first-course-of-the-Spark/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/23/The-first-course-of-the-Spark/" itemprop="url">The first course of the Spark</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-23T17:37:04+08:00">2017-04-23 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/23/The-first-course-of-the-Spark/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/23/The-first-course-of-the-Spark/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/23/The-first-course-of-the-Spark/" class="leancloud_visitors" data-flag-title="The first course of the Spark"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><ul><li>扩充了MapReduce计算模型</li><li>基于内存的计算</li><li>能够进行批处理、迭代式计算、交互查询和流处理<ul><li>降低里维护成本</li></ul></li><li>提供了Python、Java、Scala、SQL的API和丰富的内置库</li><li>可以与Hadoop、Kafka等整合</li></ul><h1 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h1><p><img src="http://o7ie0tcjk.bkt.clouddn.com/spark/first_course/spark_components.png" alt="components"></p><h2 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h2><p>Spark Core contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. Spark Core is also home to the API that defines <em>resilient distributed datasets</em> (RDDs), which are Spark’s main programming abstraction. RDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.</p><h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><p>Spark SQL is Spark’s package for working with structured data. It allows querying data via SQL as well as the Apache Hive variant of SQL—called the Hive Query Language (HQL)—and it supports many sources of data, including Hive tables, Parquet, and JSON. Beyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java, and Scala, all within a single application, thus combining SQL with complex analytics. This tight integration with the rich computing environment provided by Spark makes Spark SQL unlike any other open source data warehouse tool. Spark SQL was added to Spark in version 1.0.</p><p>Shark was an older SQL-on-Spark project out of the University of California, Berkeley, that modified Apache Hive to run on Spark. It has now been replaced by Spark SQL to provide better integration with the Spark engine and language APIs.</p><h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><p>Spark Streaming is a Spark component that enables processing of live streams of data. Examples of data streams include logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service. Spark Streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API, making it easy for programmers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. Underneath its API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.</p><h2 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h2><p>Spark comes with a library containing common machine learning (ML) functionality, called MLlib. MLlib provides multiple types of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as supporting functionality such as model evaluation and data import. It also provides some lower-level ML primitives, including a generic gradient descent optimization algorithm. All of these methods are designed to scale out across a cluster.</p><h2 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h2><p>GraphX is a library for manipulating graphs (e.g., a social network’s friend graph) and performing graph-parallel computations. Like Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. GraphX also provides various operators for manipulating graphs (e.g., <code>subgraph</code> and <code>mapVertices</code>) and a library of common graph algorithms (e.g., PageRank and triangle counting).</p><h2 id="Cluster-Managers"><a href="#Cluster-Managers" class="headerlink" title="Cluster Managers"></a>Cluster Managers</h2><p>Under the hood, Spark is designed to efficiently scale up from one to many thousands of compute nodes. To achieve this while maximizing flexibility, Spark can run over a variety of <em>cluster managers</em>, including Hadoop YARN, Apache Mesos, and a simple cluster manager included in Spark itself called the Standalone Scheduler. If you are just installing Spark on an empty set of machines, the Standalone Scheduler provides an easy way to get started; if you already have a Hadoop YARN or Mesos cluster, however, Spark’s support for these cluster managers allows your applications to also run on them.</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ul><li>Spark由Scala编写，运行于JVM上，运行环境为Java 7+</li><li>如果使用Python API，需要安装Python 2.6+ 或者Python 3.4+</li><li>Spark 1.6.2 – Scala 2.10 / Spark 2.0.0 – Scala 2.11</li></ul><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">http://spark.apache.org/downloads.html</a></p><p>不需要Hadoop集群；如果已经搭建好Hadoop集群，可下载相应版本</p><h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>README.md<ul><li>Contains short instructions for getting started with Spark.</li></ul></li><li>bin<ul><li>Contains executable files that can be used to interact with Spark in various ways (e.g., the Spark shell, which we will cover later in this chapter).</li></ul></li><li>core, streaming, python, …<ul><li>Contains the source code of major components of the Spark project.</li></ul></li><li>examples<ul><li>Contains some helpful Spark standalone jobs that you can look at and run tolearn about the Spark API.</li></ul></li></ul><h2 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h2><ul><li>Python Shell<ul><li><code>bin/pyspark</code></li></ul></li><li>Scala Shell<ul><li><code>bin/spark-shell</code></li></ul></li></ul><h1 id="开发环境搭建"><a href="#开发环境搭建" class="headerlink" title="开发环境搭建"></a>开发环境搭建</h1><h2 id="Scala安装"><a href="#Scala安装" class="headerlink" title="Scala安装"></a>Scala安装</h2><p><a href="https://www.scala-lang.org/download/" target="_blank" rel="external">https://www.scala-lang.org/download/</a></p><p>注意版本对应</p><h2 id="IntelliJ-IDEA安装"><a href="#IntelliJ-IDEA安装" class="headerlink" title="IntelliJ IDEA安装"></a>IntelliJ IDEA安装</h2><p><a href="https://www.jetbrains.com/idea/#chooseYourEdition" target="_blank" rel="external">https://www.jetbrains.com/idea/#chooseYourEdition</a></p><p>可以申请教育账号</p><h3 id="插件安装"><a href="#插件安装" class="headerlink" title="插件安装"></a>插件安装</h3><p><code>File-Settings-Plugins</code> 搜索Scala，安装</p><h3 id="项目创建"><a href="#项目创建" class="headerlink" title="项目创建"></a>项目创建</h3><p><code>File-New-Project-Scala-SBT</code></p><p>同样注意版本匹配（这里用的是Spark 2.1.0, Scala 2.11.11）</p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>需要定义使用的Spark版本</p><p><code>build.sbt</code>追加</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">libraryDependencies ++= <span class="type">Seq</span>(</div><div class="line">  <span class="string">"org.apache.spark"</span> %% <span class="string">"spark-core"</span> % <span class="string">"2.1.0"</span></div><div class="line">)</div></pre></td></tr></table></figure><p>重建项目即可</p><h3 id="源程序编写"><a href="#源程序编写" class="headerlink" title="源程序编写"></a>源程序编写</h3><p><code>New-Scala Class-Class to Object</code></p><p><code>WordCount.scala</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</div><div class="line"><span class="comment">/**</span></div><div class="line">  * Created by root on 4/23/17.</div><div class="line">  */</div><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</div><div class="line"></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"wordcount"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> input = sc.textFile(<span class="string">"/home/hduser/Anaconda2-4.3.1-Linux-x86_64.sh"</span>)</div><div class="line"></div><div class="line">    <span class="keyword">val</span> lines = input.flatMap(line =&gt; line.split(<span class="string">" "</span>))</div><div class="line">    <span class="keyword">val</span> count = lines.map(word =&gt; (word, <span class="number">1</span>)).reduceByKey&#123;<span class="keyword">case</span> (x, y) =&gt; x + y&#125;</div><div class="line">    <span class="keyword">val</span> output = count.saveAsTextFile(<span class="string">"/home/hduser/scala_wordcount_demo_output"</span>)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="打包"><a href="#打包" class="headerlink" title="打包"></a>打包</h3><p><code>File-Project Structure-Project Setting-Artifacts-Add-JAR-From modules with dependencies</code></p><p><code>Build-Build Artifacts-Build</code></p><h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><ul><li>启动master<ul><li><code>sbin/start-master.sh</code></li></ul></li><li>启动worker<ul><li><code>bin/spark-class org.apache.spark.deploy.worker.Worker spark://Ubuntu:7077</code></li><li>注意这里的spark服务器地址可以通过浏览器输入<code>localhost:8080</code>来查看</li></ul></li><li>提交作业<ul><li><code>bin/spark-submit --master spark://Ubuntu:7077 --class WordCount /home/hduser/scala_demo.jar</code></li><li>注意这里的<code>scala_demo.jar</code>文件为打包阶段生成</li></ul></li></ul><h1 id="TODO-RDDs"><a href="#TODO-RDDs" class="headerlink" title="TODO: RDDs"></a>TODO: RDDs</h1><p><a href="https://github.com/CjTouzi/Learning-RSpark/blob/master/Zaharia%20M.%2C%20et%20al.%20Learning%20Spark%20%28O%27Reilly%2C%202015%29%28274s%29.pdf" target="_blank" rel="external">Zaharia M., et al. Learning Spark (O’Reilly, 2015)(274s).pdf</a></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>慕课网 <a href="http://www.imooc.com/learn/814" target="_blank" rel="external">http://www.imooc.com/learn/814</a></li><li>Zaharia M., et al. Learning Spark (O’Reilly, 2015)(274s).pdf</li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/21/Implement-k-means-on-the-hadoop-platform/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/04/21/Implement-k-means-on-the-hadoop-platform/" itemprop="url">Implement k-means on the hadoop platform</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-21T13:44:53+08:00">2017-04-21 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/04/21/Implement-k-means-on-the-hadoop-platform/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/04/21/Implement-k-means-on-the-hadoop-platform/" itemprop="commentsCount"></span> </a></span><span id="/2017/04/21/Implement-k-means-on-the-hadoop-platform/" class="leancloud_visitors" data-flag-title="Implement k-means on the hadoop platform"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>首先在单机上搭一个伪分布式环境，主要是对<code>*-site.xml</code>配置文件进行修改，具体修改如下：</p><p><code>core-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0"?&gt;</div><div class="line"><span class="comment">&lt;!-- core-site.xml --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>hdfs-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0"?&gt;</div><div class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>yarn-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0"?&gt;</div><div class="line"><span class="comment">&lt;!-- yarn-site.xml --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p><code>mapred-site.xml</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version="1.0"?&gt;</div><div class="line"><span class="comment">&lt;!-- mapred-site.xml --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure><p>然后启动<code>hadoop</code>，启动的流程如下：</p><ol><li><code>start-dfs.sh</code></li><li><code>start-yarn.sh</code></li><li><code>mr-jobhistory-daemon.sh start historyserver</code></li></ol><p><strong>注意，以上命令能够得到正确执行的前提是已经将<code>hadoop</code>的安装目录下的<code>bin</code>目录加到环境变量中</strong></p><p>由于很久没有使用<code>Java</code>，所以采用<code>Python</code>实现，这里需要用到一个<code>package</code>，也就是<code>mrjob</code></p><p>首先计划一下实现步骤：</p><p>[ Mapper ]</p><p><strong>Accepts</strong></p><ul><li>data</li><li>global constant representing the list of centers</li></ul><p><strong>Computes</strong></p><ul><li>the nearest center for each data instance</li></ul><p><strong>Emits</strong></p><ul><li>nearest centers (<strong>key</strong>) and points (<strong>value</strong>).</li></ul><hr><p>[ Reducer ]</p><p><strong>Accepts</strong></p><ul><li>center instance / coordinate (<strong>key</strong>)</li><li>points (<strong>value</strong>)</li></ul><p><strong>Computes</strong></p><ul><li>the new centers based on clusters</li></ul><p><strong>Emits</strong></p><ul><li>new centers</li></ul><p>You will provide the next epoch of K-Means with:</p><ol><li>the same data from your initial epoch</li><li>the centers emitted from the reducer as global constants</li></ol><p>Repeat until your stopping criteria are met.</p><p>如果要用<code>Python</code>进行相关的<code>Hadoop</code>操作的话，肯定是要使用<code>hadoop streaming</code>的，但是存在一个问题，也就是<code>streaming</code>流程只能跑一遍，但是很显然，作为一个<code>machine learning</code>算法，<code>k-means</code>是类似于<code>EM</code>算法要经过多步迭代的，那么最容易想到的就是使用shell脚本多次调用相关命令，但是这样显得十分<code>ugly</code>，因此可以采用<code>mrjob</code>包来帮助我们完成这个工作。</p><p>从上面看来，我们需要两个文件，一个是<code>python</code>实现的<code>map-reduce</code>，另一个是<code>mrjob</code>的<code>job</code>文件，相当于<code>master</code>，下面列出这两个文件，因为实现比较简单，因此不作过多解释.</p><p><code>kmeans.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</div><div class="line"><span class="keyword">import</span> mrjob</div><div class="line"><span class="comment"># MRJob is a python class which will be overloaded</span></div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MRKMeans</span><span class="params">(MRJob)</span>:</span></div><div class="line"></div><div class="line">    SORT_VALUES = <span class="keyword">True</span></div><div class="line">    OUTPUT_PROTOCOL = mrjob.protocol.RawProtocol</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dist_vec</span><span class="params">(self, v1, v2)</span>:</span></div><div class="line">        <span class="comment"># calculate the ditance between two vectors (in two dimensions)</span></div><div class="line">        <span class="keyword">return</span> sqrt((v2[<span class="number">0</span>] - v1[<span class="number">0</span>]) * (v2[<span class="number">0</span>] - v1[<span class="number">0</span>]) + (v2[<span class="number">1</span>] - v1[<span class="number">1</span>]) * (v2[<span class="number">1</span>] - v1[<span class="number">1</span>]))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">configure_options</span><span class="params">(self)</span>:</span></div><div class="line">        super(MRKMeans, self).configure_options()</div><div class="line">        <span class="comment"># the line below define that the file folowing the --c option is the</span></div><div class="line">        <span class="comment"># centroid and is loadable</span></div><div class="line">        self.add_file_option(<span class="string">'--c'</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_centroids</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Definition : extracts centroids from the centroids file define afetr --c flag</div><div class="line">        Out : Return the list of centroids</div><div class="line">        """</div><div class="line">        <span class="comment"># self.options.c is the name of the file following --c option</span></div><div class="line">        f = open(self.options.c, <span class="string">'r'</span>)</div><div class="line">        centroids = []</div><div class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.read().split(<span class="string">'\n'</span>):</div><div class="line">            <span class="keyword">if</span> line:</div><div class="line">                x, y = line.split(<span class="string">', '</span>)</div><div class="line">                centroids.append([float(x), float(y)])</div><div class="line">        f.close()</div><div class="line">        <span class="keyword">return</span> centroids</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, _, lines)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Definition : Mapper take centroids extract form get_centroids()</div><div class="line">        and the point cloud and for each point, calculate the distance</div><div class="line">        to the centroids, find the mininum of it</div><div class="line">        Out : yield the point with it's class</div><div class="line">        """</div><div class="line">        centroids = self.get_centroids()</div><div class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> lines.split(<span class="string">'\n'</span>):</div><div class="line">            x, y = l.split(<span class="string">', '</span>)</div><div class="line">            point = [float(x), float(y)]</div><div class="line">            min_dist = <span class="number">100000000.0</span></div><div class="line">            classe = <span class="number">0</span></div><div class="line">            <span class="comment"># iterate over the centroids (Here we know that we are doing a 3means)</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">                dist = self.dist_vec(point, centroids[i])</div><div class="line">                <span class="keyword">if</span> dist &lt; min_dist:</div><div class="line">                    min_dist = dist</div><div class="line">                    classe = i</div><div class="line">            <span class="keyword">yield</span> classe, point</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combiner</span><span class="params">(self, k, v)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Definition : Calculate for each class, at the end of the mapper,</div><div class="line">        before reducer, the medium point of each class</div><div class="line">        Out: return for each class, the centroids for each mapper</div><div class="line">        """</div><div class="line">        count = <span class="number">0</span></div><div class="line">        moy_x = moy_y = <span class="number">0.0</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> v:</div><div class="line">            count += <span class="number">1</span></div><div class="line">            moy_x += t[<span class="number">0</span>]</div><div class="line">            moy_y += t[<span class="number">1</span>]</div><div class="line">        <span class="keyword">yield</span> k, (moy_x / count, moy_y / count)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, k, v)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Definition : for each class, get all the tmp centroids from each</div><div class="line">        combiner and calculate the new centroids.</div><div class="line">        """</div><div class="line">        <span class="comment"># k is class and v are medium points linked to the class</span></div><div class="line">        count = <span class="number">0</span></div><div class="line">        moy_x = moy_y = <span class="number">0.0</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> v:</div><div class="line">            count += <span class="number">1</span></div><div class="line">            moy_x += t[<span class="number">0</span>]</div><div class="line">            moy_y += t[<span class="number">1</span>]</div><div class="line">        <span class="keyword">print</span> str(k) + <span class="string">", "</span> + str(moy_x / count) + <span class="string">", "</span> + str(moy_y / count)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="comment"># just run mapreduce !</span></div><div class="line">    MRKMeans.run()</div></pre></td></tr></table></figure><p><code>main.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</div><div class="line"><span class="keyword">from</span> kmeans <span class="keyword">import</span> MRKMeans</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> os.path</div><div class="line"><span class="keyword">import</span> shutil</div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</div><div class="line"><span class="keyword">import</span> time</div><div class="line"></div><div class="line">input_c = <span class="string">"centroids"</span></div><div class="line"></div><div class="line">CENTROIDS_FILE = <span class="string">"/home/hduser/tmp/centroid"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_c</span><span class="params">(job, runner)</span>:</span></div><div class="line">    c = []</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> runner.stream_output():</div><div class="line">        <span class="comment"># print "stream_output: ", line</span></div><div class="line">        key, value = job.parse_output_line(line)</div><div class="line">        c.append(key)</div><div class="line">    <span class="keyword">return</span> c</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_first_c</span><span class="params">(fname)</span>:</span></div><div class="line">    f = open(fname, <span class="string">'r'</span>)</div><div class="line">    centroids = []</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f.read().split(<span class="string">'\n'</span>):</div><div class="line">        <span class="keyword">if</span> line:</div><div class="line">            x, y = line.split(<span class="string">', '</span>)</div><div class="line">            centroids.append([float(x), float(y)])</div><div class="line">    f.close()</div><div class="line">    <span class="keyword">return</span> centroids</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_c</span><span class="params">(centroids)</span>:</span></div><div class="line">    f = open(CENTROIDS_FILE, <span class="string">"w"</span>)</div><div class="line">    centroids.sort()</div><div class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> centroids:</div><div class="line">        k, cx, cy = c.split(<span class="string">', '</span>)</div><div class="line">        <span class="comment"># print c</span></div><div class="line">        f.write(<span class="string">"%s, %s\n"</span> % (cx, cy))</div><div class="line">    f.close()</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dist_vec</span><span class="params">(v1, v2)</span>:</span></div><div class="line">    <span class="keyword">return</span> sqrt((v2[<span class="number">0</span>] - v1[<span class="number">0</span>]) * (v2[<span class="number">0</span>] - v1[<span class="number">0</span>]) + (v2[<span class="number">1</span>] - v1[<span class="number">1</span>]) * (v2[<span class="number">1</span>] - v1[<span class="number">1</span>]))</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">diff</span><span class="params">(cs1, cs2)</span>:</span></div><div class="line">    max_dist = <span class="number">0.0</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</div><div class="line">        dist = dist_vec(cs1[i], cs2[i])</div><div class="line">        <span class="keyword">if</span> dist &gt; max_dist:</div><div class="line">            max_dist = dist</div><div class="line"></div><div class="line">    <span class="keyword">return</span> max_dist</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line"></div><div class="line">    args = sys.argv[<span class="number">1</span>:]</div><div class="line"></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(CENTROIDS_FILE):</div><div class="line">        shutil.copy(input_c, CENTROIDS_FILE)</div><div class="line"></div><div class="line">    old_c = get_first_c(input_c)</div><div class="line"></div><div class="line">    i = <span class="number">1</span></div><div class="line">    start = time.time()</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"Iteration #%i"</span> % i</div><div class="line">        mr_job = MRKMeans(args=args + [<span class="string">'--c='</span> + CENTROIDS_FILE])</div><div class="line">        <span class="comment"># print "start runner.."</span></div><div class="line"></div><div class="line">        <span class="keyword">with</span> mr_job.make_runner() <span class="keyword">as</span> runner:</div><div class="line">            runner.run()</div><div class="line">            centroids = get_c(mr_job, runner)</div><div class="line"></div><div class="line">        <span class="comment"># print "mr result: ", centroids</span></div><div class="line">        write_c(centroids)</div><div class="line">        n_c = get_first_c(CENTROIDS_FILE)</div><div class="line">        <span class="comment"># print "old_c", old_c</span></div><div class="line">        <span class="comment"># print "n_c", n_c </span></div><div class="line">        max_d = diff(n_c, old_c)</div><div class="line">        <span class="comment"># print "dist max = "+str(max_d)</span></div><div class="line">        <span class="keyword">if</span> max_d &lt; <span class="number">0.01</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            old_c = n_c</div><div class="line">            i = i + <span class="number">1</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"used time: "</span>, time.time() - start, <span class="string">'s'</span></div></pre></td></tr></table></figure><p>根据上面写的实现步骤可以看出，我们需要两个文件，一个存储输入数据，另一个存储<code>centroids</code>，由于只是一个demo，因此在这里我简化了具体问题。设所有的数据都是二维数据点，并且聚类个数为3。当然，如果真的是在大数据上进行工业级的处理的话，还是推荐使用<code>Spark</code>。下面列出这两个文件：</p><p><code>kmeans_data</code></p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="number">1</span>, <span class="number">2</span></div><div class="line"><span class="number">2</span>, <span class="number">3</span></div><div class="line"><span class="number">1</span>, <span class="number">3.5</span></div><div class="line"><span class="number">4</span>, <span class="number">3.5</span></div><div class="line"><span class="number">3</span>, <span class="number">4.2</span></div><div class="line"><span class="number">2</span>, <span class="number">1.6</span></div><div class="line"><span class="number">5</span>, <span class="number">2.3</span></div><div class="line"><span class="number">1.5</span>, <span class="number">2.3</span></div><div class="line"><span class="number">3</span>, <span class="number">5.2</span></div><div class="line"><span class="number">2</span>, <span class="number">3</span></div><div class="line"><span class="number">1</span>, <span class="number">3.5</span></div><div class="line"><span class="number">4</span>, <span class="number">3.5</span></div><div class="line"><span class="number">3</span>, <span class="number">4.2</span></div><div class="line"><span class="number">2</span>, <span class="number">1.6</span></div><div class="line"><span class="number">5</span>, <span class="number">2.3</span></div><div class="line"><span class="number">1.5</span>, <span class="number">2.3</span></div><div class="line"><span class="number">3</span>, <span class="number">5</span></div></pre></td></tr></table></figure><p><code>centroids</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">1, 2</div><div class="line">2, 3</div><div class="line">1, 3.5</div></pre></td></tr></table></figure><p>按照以下方式运行：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python main.py kmeans_data -r hadoop</div></pre></td></tr></table></figure><p>结果显示如下：</p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">Iteration #<span class="number">1</span></div><div class="line">No handlers could be found <span class="keyword">for</span> logger <span class="string">"mrjob.hadoop"</span></div><div class="line">old_c [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">n_c [[<span class="number">1.625</span>, <span class="number">1.95833333333</span>], [<span class="number">3.4</span>, <span class="number">3.62</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">Iteration #<span class="number">2</span></div><div class="line">old_c [[<span class="number">1.625</span>, <span class="number">1.95833333333</span>], [<span class="number">3.4</span>, <span class="number">3.62</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">n_c [[<span class="number">1.72916666667</span>, <span class="number">2.2625</span>], [<span class="number">3.75</span>, <span class="number">3.775</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">Iteration #<span class="number">3</span></div><div class="line">old_c [[<span class="number">1.72916666667</span>, <span class="number">2.2625</span>], [<span class="number">3.75</span>, <span class="number">3.775</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">n_c [[<span class="number">1.72916666667</span>, <span class="number">2.2625</span>], [<span class="number">3.75</span>, <span class="number">3.775</span>], [<span class="number">1.0</span>, <span class="number">3.5</span>]]</div><div class="line">time:  <span class="number">148.277868032</span></div></pre></td></tr></table></figure><p>最后生成结果文件：</p><p><code>centroid</code></p><figure class="highlight typescript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="number">1.72916666667</span>, <span class="number">2.2625</span></div><div class="line"><span class="number">3.75</span>, <span class="number">3.775</span></div><div class="line"><span class="number">1.0</span>, <span class="number">3.5</span></div></pre></td></tr></table></figure><p>根据以上可以看出，对于小数据集，效率反而会比较低，因为整个程序运行过程中大部分的时间都没有花在实际的算法运行上。</p><p><strong>TODO：</strong></p><ol><li>用常规方法实现，作为baseline</li><li>在大数据集上继续实验，观察结果</li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/16/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><span class="page-number current">17</span><a class="page-number" href="/page/18/">18</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/18/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">128</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">63</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/tomaxent" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>