<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/30/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/30/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/05/On-policy-Prediction-with-Approximation/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/05/On-policy-Prediction-with-Approximation/" itemprop="url">On-policy Prediction with Approximation</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-05T15:29:22+08:00">2017-07-05 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/05/On-policy-Prediction-with-Approximation/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/05/On-policy-Prediction-with-Approximation/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/05/On-policy-Prediction-with-Approximation/" class="leancloud_visitors" data-flag-title="On-policy Prediction with Approximation"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>The novelty in this post is that the approximate value function is represented not as a table but as a parameterized functional form with weight vector $\mathbf{w} \in \mathbb{R}^d$. We will write $\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)$ for the approximated value of state $s$ given weight vector $\mathbf{w}$. For example, $\hat{v}$ might be a linear function in features of the state, with $\mathbf{w}$ the vector of feature weights. Consequently, when a single state is updated, the change generalizes from that state to affect the values of many other states.</p><h3 id="The-prediction-Objective-MSVE"><a href="#The-prediction-Objective-MSVE" class="headerlink" title="The prediction Objective (MSVE)"></a>The prediction Objective (MSVE)</h3><p>In the tabular case a continuous measure of prediction quality was not necessary because the learned value function could come to equal the true value function exactly. But with genuine approximation, an update at one state aﬀects many others, and it is not possible to get all states exactly correct. By assumption we have far more states than weights, so making one state’s estimate more accurate invariably means making others’ less accurate.</p><p>By the error in a state $s$ we mean the square of the diﬀerence between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_{\pi}(s)$. Weighting this over the state space by the distribution $\mu$, we obtain a natural objective function, the <strong>Mean Squared Value Error</strong>, or <strong>MSVE</strong>:<br>$$<br>\text{MSVE}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s) \Big[v_{\pi}(s) - \hat{v}(s, \mathbf{w}) \Big]^2.<br>$$<br>The square root of this measure, the root MSVE or RMSVE, gives a rough measure of how much the approximate values diﬀer from the true values and is often used in plots. Typically one chooses $\mu(s)$ to be the fraction of time spent in $s$ under the target policy $\pi$. This is called the <em>on-policy distribution</em>.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/on_policy_dist.png" alt="on-policy-distribution"></p><h3 id="Stochastic-gradient-Methods"><a href="#Stochastic-gradient-Methods" class="headerlink" title="Stochastic-gradient Methods"></a>Stochastic-gradient Methods</h3><p>We assume that states appear in examples with the same distribution, µ, over which we are trying to minimize the MSVE. A good strategy in this case is to try to minimize error on the observed examples. Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example by a small amount in the direction that would most reduce the error on that example:<br>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t - \frac{1}{2} \alpha \nabla \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big]^2 \\<br>&amp;= \mathbf{w}_t + \alpha \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).<br>\end{align}<br>$$<br>And<br>$$<br>\nabla f(\mathbf{w}) \doteq \Big( \frac{\partial f(\mathbf{w})}{\partial w_1}, \frac{\partial f(\mathbf{w})}{\partial w_2}, \cdots, \frac{\partial f(\mathbf{w})}{\partial w_d}\Big)^{\top}.<br>$$<br>Although $v_{\pi}(S_t)$ is unknown, but we can approximate it by substituting $U_t$ (the $t$th training example) in place of $v_{\pi}(S_t)$. This yields the following general SGD method for state-value prediction:<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).<br>$$<br>If $U_t$ is an <strong>unbiased</strong> estimate, that is, if $\mathbb{E}[U_t]=v_{\pi}(S_t)$, for each $t$, then $\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic approximation conditions for decreasing $\alpha$.</p><p>For example, suppose the states in the examples are the states generated by interaction (or simulated interaction) with the environment using policy $\pi$. Because the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t \doteq G_t$ is by deﬁnition an unbiased estimate of $v_{\pi}(S_t)$. Thus, the gradient-descent version of Monte Carlo state-value prediction is guaranteed to ﬁnd a locally optimal solution. Pseudocode for a complete algorithm<br>is shown in the box.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/gradient_mc.png" alt="gradient_mc"></p><h4 id="Example-State-Aggregation-on-the-1000-state-Random-Walk"><a href="#Example-State-Aggregation-on-the-1000-state-Random-Walk" class="headerlink" title="Example: State Aggregation on the 1000-state Random Walk"></a>Example: State Aggregation on the 1000-state Random Walk</h4><p>State aggregation is a simple form of generalizing function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group. The value of a state is estimated as its group’s component, and when the state is updated, that component alone is updated. State aggregation is a special case of SGD in which the gradient, $\nabla \hat{v}(S_t, \mathbf{w}_t)$, is <strong>1</strong> for $S_t$’s group’s component and <strong>0</strong> for the other components.</p><p>Consider a 1000-state version of the random walk task. The states are numbered from 1 to 1000, left to right, and all episodes begin near the center, in state 500. State transitions are from the current state to one of the 100 neighboring states to its left, or to one of the 100 neighboring states to its right, all with equal probability. Of course, if the current state is near an edge, then there may be fewer than 100 neighbors on that side of it. In this case, all the probability that would have gone into those missing neighbors goes into the probability of terminating on that side (thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right). As usual, termination on the left produces a reward of −1, and termination on the right produces a reward of +1. All other transitions have a reward of zero.</p><p>Now, let us solve this problem by gradient Monte Carlo algorithm. First of all, we define the environment of this problem.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># # of states except for terminal states</span></div><div class="line">N_STATES = <span class="number">1000</span></div><div class="line"></div><div class="line"><span class="comment"># true state values, just a promising guess</span></div><div class="line">trueStateValues = np.arange(<span class="number">-1001</span>, <span class="number">1003</span>, <span class="number">2</span>) / <span class="number">1001.0</span></div><div class="line"></div><div class="line"><span class="comment"># all states</span></div><div class="line">states = np.arange(<span class="number">1</span>, N_STATES + <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># start from a central state</span></div><div class="line">START_STATE = <span class="number">500</span></div><div class="line"></div><div class="line"><span class="comment"># terminal states</span></div><div class="line">END_STATES = [<span class="number">0</span>, N_STATES + <span class="number">1</span>]</div><div class="line"></div><div class="line"><span class="comment"># possible actions</span></div><div class="line">ACTION_LEFT = <span class="number">-1</span></div><div class="line">ACTION_RIGHT = <span class="number">1</span></div><div class="line">ACTIONS = [ACTION_LEFT, ACTION_RIGHT]</div><div class="line"></div><div class="line"><span class="comment"># maximum stride for an action</span></div><div class="line">STEP_RANGE = <span class="number">100</span></div></pre></td></tr></table></figure><p>We need a true value of each state, thus use the dynamic programming to get these value:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Dynamic programming to find the true state values, based on the promising guess above</span></div><div class="line"><span class="comment"># Assume all rewards are 0, given that we have already given value -1 and 1 to terminal states</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">    oldTrueStateValues = np.copy(trueStateValues)</div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states:</div><div class="line">        trueStateValues[state] = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, STEP_RANGE + <span class="number">1</span>):</div><div class="line">                step *= action</div><div class="line">                newState = state + step</div><div class="line">                newState = max(min(newState, N_STATES + <span class="number">1</span>), <span class="number">0</span>)</div><div class="line">                <span class="comment"># asynchronous update for faster convergence</span></div><div class="line">                trueStateValues[state] += <span class="number">1.0</span> / (<span class="number">2</span> * STEP_RANGE) * trueStateValues[newState]</div><div class="line">    error = np.sum(np.abs(oldTrueStateValues - trueStateValues))</div><div class="line">    print(error)</div><div class="line">    <span class="keyword">if</span> error &lt; <span class="number">1e-2</span>:</div><div class="line">        <span class="keyword">break</span></div><div class="line"><span class="comment"># correct the state value for terminal states to 0</span></div><div class="line">trueStateValues[<span class="number">0</span>] = trueStateValues[<span class="number">-1</span>] = <span class="number">0</span></div></pre></td></tr></table></figure><p>The policy of episodes generation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take an @action at @state, return new state and reward for this transition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(state, action)</span>:</span></div><div class="line">    step = np.random.randint(<span class="number">1</span>, STEP_RANGE + <span class="number">1</span>)</div><div class="line">    step *= action</div><div class="line">    state += step</div><div class="line">    state = max(min(state, N_STATES + <span class="number">1</span>), <span class="number">0</span>)</div><div class="line">    <span class="keyword">if</span> state == <span class="number">0</span>:</div><div class="line">        reward = <span class="number">-1</span></div><div class="line">    <span class="keyword">elif</span> state == N_STATES + <span class="number">1</span>:</div><div class="line">        reward = <span class="number">1</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        reward = <span class="number">0</span></div><div class="line">    <span class="keyword">return</span> state, reward</div></pre></td></tr></table></figure><p>The reward after take an action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get an action, following random policy</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> <span class="number">-1</span></div></pre></td></tr></table></figure><p>And we have a special value function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># a wrapper class for aggregation value function</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValueFunction</span>:</span></div><div class="line">    <span class="comment"># @numOfGroups: # of aggregations</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, numOfGroups)</span>:</span></div><div class="line">        self.numOfGroups = numOfGroups</div><div class="line">        self.groupSize = N_STATES // numOfGroups</div><div class="line"></div><div class="line">        <span class="comment"># thetas</span></div><div class="line">        self.params = np.zeros(numOfGroups)</div><div class="line"></div><div class="line">    <span class="comment"># get the value of @state</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="keyword">if</span> state <span class="keyword">in</span> END_STATES:</div><div class="line">            <span class="keyword">return</span> <span class="number">0</span></div><div class="line">        groupIndex = (state - <span class="number">1</span>) // self.groupSize</div><div class="line">        <span class="keyword">return</span> self.params[groupIndex]</div><div class="line"></div><div class="line">    <span class="comment"># update parameters</span></div><div class="line">    <span class="comment"># @delta: step size * (target - old estimation)</span></div><div class="line">    <span class="comment"># @state: state of current sample</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, delta, state)</span>:</span></div><div class="line">        groupIndex = (state - <span class="number">1</span>) // self.groupSize</div><div class="line">        self.params[groupIndex] += delta</div></pre></td></tr></table></figure><p>And the gradient MC algorithm:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># gradient Monte Carlo algorithm</span></div><div class="line"><span class="comment"># @valueFunction: an instance of class ValueFunction</span></div><div class="line"><span class="comment"># @alpha: step size</span></div><div class="line"><span class="comment"># @distribution: array to store the distribution statistics</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientMonteCarlo</span><span class="params">(valueFunction, alpha, distribution=None)</span>:</span></div><div class="line">    currentState = START_STATE</div><div class="line">    trajectory = [currentState]</div><div class="line"></div><div class="line">    <span class="comment"># We assume gamma = 1, so return is just the same as the latest reward</span></div><div class="line">    reward = <span class="number">0.0</span></div><div class="line">    <span class="keyword">while</span> currentState <span class="keyword">not</span> <span class="keyword">in</span> END_STATES:</div><div class="line">        action = getAction()</div><div class="line">        newState, reward = takeAction(currentState, action)</div><div class="line">        trajectory.append(newState)</div><div class="line">        currentState = newState</div><div class="line"></div><div class="line">    <span class="comment"># Gradient update for each state in this trajectory</span></div><div class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> trajectory[:<span class="number">-1</span>]:</div><div class="line">        delta = alpha * (reward - valueFunction.value(state))</div><div class="line">        valueFunction.update(delta, state)</div><div class="line">        <span class="keyword">if</span> distribution <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            distribution[state] += <span class="number">1</span></div></pre></td></tr></table></figure><p>Finally. let us solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">nEpisodes = int(<span class="number">1e5</span>)</div><div class="line">alpha = <span class="number">2e-5</span></div><div class="line"></div><div class="line"><span class="comment"># we have 10 aggregations in this example, each has 100 states</span></div><div class="line">valueFunction = ValueFunction(<span class="number">10</span>)</div><div class="line">distribution = np.zeros(N_STATES + <span class="number">2</span>)</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">    print(<span class="string">'episode:'</span>, episode)</div><div class="line">    gradientMonteCarlo(valueFunction, alpha, distribution)</div><div class="line"></div><div class="line">distribution /= np.sum(distribution)</div><div class="line">stateValues = [valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states]</div><div class="line"></div><div class="line">plt.figure(<span class="number">0</span>)</div><div class="line">plt.plot(states, stateValues, label=<span class="string">'Approximate MC value'</span>)</div><div class="line">plt.plot(states, trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'True value'</span>)</div><div class="line">plt.xlabel(<span class="string">'State'</span>)</div><div class="line">plt.ylabel(<span class="string">'Value'</span>)</div><div class="line">plt.legend()</div><div class="line"></div><div class="line">plt.figure(<span class="number">1</span>)</div><div class="line">plt.plot(states, distribution[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'State distribution'</span>)</div><div class="line">plt.xlabel(<span class="string">'State'</span>)</div><div class="line">plt.ylabel(<span class="string">'Distribution'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_1_1.png" alt="distribution"></p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_1_2.png" alt="state_value"></p><h3 id="Semi-gradient-Methods"><a href="#Semi-gradient-Methods" class="headerlink" title="Semi-gradient Methods"></a>Semi-gradient Methods</h3><p>Bootstrapping target such as n-step returns $G_{t:t+n}$ or the DP target $\sum_{a, s^{\prime}, r} \pi(a|S_t)p(s^{\prime}, r|S_t, a)[r + \gamma \hat{v}(s^{\prime}, \mathbf{w}_t)]$ all depend on the current value of the weight vector $\mathbf{w}_t$, which implies that they will be biased and that they will not produce a true gradient=descent method. We call them <em>semi-gradient methods</em>.</p><p>Although semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear case. Moreover, they oﬀer important advantages which makes them often clearly preferred. One reason for this is that they are typically signiﬁcantly faster to learn. Another is that they enable learning to be continual and online, without waiting for the end of an episode. This enables them to be used on continuing problems and provides computational advantages. A prototypical semi-gradient method is semi-gradient TD(0), which uses $U_t \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w})$ as its target. Complete pseudocode for this method is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/semi_grad_td.png" alt="semi_grad_td"></p><h3 id="Linear-Methods"><a href="#Linear-Methods" class="headerlink" title="Linear Methods"></a>Linear Methods</h3><p>One of the most important special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot, \mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$. Corresponding to every state $s$, there is a real-valued vector of features $\mathbf{x}(s) \doteq (x_1(s),x_2(s),\cdots,x_d(s))^{\top}$, with the same number of components as $\mathbf{w}$. However the features are constructed, the approximate state-value function is given by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:<br>$$<br>\hat{v}(s, \mathbf{w}) \doteq \mathbf{w^{\top}x}(s) \doteq \sum_{i=1}^d w_i x_i(s).<br>$$<br>The individual functions $x_i:\mathcal{S} \rightarrow \mathbb{E}$ are called <strong>basis functions</strong>. The gradient of the approximate value function with respect to $\mathbf{w}$ in this case is<br>$$<br>\nabla \hat{v} (s, \mathbf{w}) = \mathbf{x}(s).<br>$$<br>The update at each time $t$ is</p><p>$$<br>\begin{align}<br>\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t + \alpha \Big( R_{t+1} + \gamma \mathbf{w}_t^{\top}\mathbf{x}_{t+1} - \mathbf{w}_t^{\top}\mathbf{x}_{t}\Big)\mathbf{x}_t \\<br>&amp;= \mathbf{w}_t + \alpha \Big( R_{t+1}\mathbf{x}_t - \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \mathbf{w}_t\Big),<br>\end{align}<br>$$<br>where here we have used the notational shorthand $\mathbf{x}_t = \mathbf{x}(S_t)$. If the system converges, it must converge to the weight vector $\mathbf{w}_{TD}$ at which<br>$$<br>\mathbf{w}_{TD} \doteq \mathbf{A^{-1}b},<br>$$<br>where<br>$$<br>\mathbf{b} \doteq \mathbb{E}[R_{t+1}\mathbf{x}_t] \in \mathbb{R}^d \;\; \text{and} \;\; \mathbf{A} \doteq \mathbb{E}\big[ \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \big] \in \mathbb{R}^d \times \mathbb{R}^d.<br>$$<br>This quantity is called the TD <strong>fixedpoint</strong>. At this point we have:<br>$$<br>\text{MSVE}(\mathbf{w}_{TD}) \leq \frac{1}{1 - \gamma} \min_{\mathbf{w}} \text{MSVE}(\mathbf{w}).<br>$$<br>Now we use the state aggregation example again, but use the semi-gradient TD method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># semi-gradient n-step TD algorithm</span></div><div class="line"><span class="comment"># @valueFunction: an instance of class ValueFunction</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="comment"># @alpha: step size</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">semiGradientTemporalDifference</span><span class="params">(valueFunction, n, alpha)</span>:</span></div><div class="line">    <span class="comment"># initial starting state</span></div><div class="line">    currentState = START_STATE</div><div class="line"></div><div class="line">    <span class="comment"># arrays to store states and rewards for an episode</span></div><div class="line">    <span class="comment"># space isn't a major consideration, so I didn't use the mod trick</span></div><div class="line">    states = [currentState]</div><div class="line">    rewards = [<span class="number">0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># choose an action randomly</span></div><div class="line">            action = getAction()</div><div class="line">            newState, reward = takeAction(currentState, action)</div><div class="line"></div><div class="line">            <span class="comment"># store new state and new reward</span></div><div class="line">            states.append(newState)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newState <span class="keyword">in</span> END_STATES:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += rewards[t]</div><div class="line">            <span class="comment"># add state value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += valueFunction.value(states[updateTime + n])</div><div class="line">            stateToUpdate = states[updateTime]</div><div class="line">            <span class="comment"># update the value function</span></div><div class="line">            <span class="keyword">if</span> <span class="keyword">not</span> stateToUpdate <span class="keyword">in</span> END_STATES:</div><div class="line">                delta = alpha * (returns - valueFunction.value(stateToUpdate))</div><div class="line">                valueFunction.update(delta, stateToUpdate)</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentState = newState</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">nEpisodes = int(<span class="number">1e5</span>)</div><div class="line">    alpha = <span class="number">2e-4</span></div><div class="line">    valueFunction = ValueFunction(<span class="number">10</span>)</div><div class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, nEpisodes):</div><div class="line">        print(<span class="string">'episode:'</span>, episode)</div><div class="line">        semiGradientTemporalDifference(valueFunction, <span class="number">1</span>, alpha)</div><div class="line"></div><div class="line">    stateValues = [valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states]</div><div class="line">    plt.figure(<span class="number">2</span>)</div><div class="line">    plt.plot(states, stateValues, label=<span class="string">'Approximate TD value'</span>)</div><div class="line">    plt.plot(states, trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], label=<span class="string">'True value'</span>)</div><div class="line">    plt.xlabel(<span class="string">'State'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Value'</span>)</div><div class="line">    plt.legend()</div></pre></td></tr></table></figure><p>Results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_2_1.png" alt="semi_gradient_td"></p><p>We also could use the <a href="https://ewanlee.github.io/2017/07/04/n-step-TD/" target="_blank" rel="external">n-step semi-gradient TD method</a>. To obtain such quantitatively similar results we switched the state aggregation to 20 groups of 50 states each. The 20 groups are then quantitatively close to the 19 states of the tabular problem.</p><p>The semi-gradient n-step TD algorithm we used in this example is the natural extension of the tabular n-step TD algorithm. The key equation is<br>$$<br>\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \Big[ G_{t:t+n} - \hat{v}(S_t, \mathbf{w}_{t+n-1})\Big] \nabla \hat{v}(S_t, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T,<br>$$<br>where<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{v}(S_{t+n}, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T-n.<br>$$<br>Pseudocode for the complete algorithm is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/n_step_semi_gradient_td.png" alt="n_step_semi_gradient_td"></p><p>Now let us show the performance of different value of n:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># truncate value for better display</span></div><div class="line">truncateValue = <span class="number">0.55</span></div><div class="line"></div><div class="line"><span class="comment"># all possible steps</span></div><div class="line">steps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">10</span>))</div><div class="line"></div><div class="line"><span class="comment"># all possible alphas</span></div><div class="line">alphas = np.arange(<span class="number">0</span>, <span class="number">1.1</span>, <span class="number">0.1</span>)</div><div class="line"></div><div class="line"><span class="comment"># each run has 10 episodes</span></div><div class="line">episodes = <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># perform 100 independent runs</span></div><div class="line">runs = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="comment"># track the errors for each (step, alpha) combination</span></div><div class="line">errors = np.zeros((len(steps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> stepInd, step <span class="keyword">in</span> zip(range(len(steps)), steps):</div><div class="line">        <span class="keyword">for</span> alphaInd, alpha <span class="keyword">in</span> zip(range(len(alphas)), alphas):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'step:'</span>, step, <span class="string">'alpha:'</span>, alpha)</div><div class="line">            <span class="comment"># we have 20 aggregations in this example</span></div><div class="line">            valueFunction = ValueFunction(<span class="number">20</span>)</div><div class="line">            <span class="keyword">for</span> ep <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                semiGradientTemporalDifference(valueFunction, step, alpha)</div><div class="line">                <span class="comment"># calculate the RMS error</span></div><div class="line">                currentStateValues = np.asarray([valueFunction.value(i) <span class="keyword">for</span> i <span class="keyword">in</span> states])</div><div class="line">                errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - trueStateValues[<span class="number">1</span>: <span class="number">-1</span>], <span class="number">2</span>)) / N_STATES)</div><div class="line"><span class="comment"># take average</span></div><div class="line">errors /= episodes * runs</div><div class="line"><span class="comment"># truncate the error</span></div><div class="line">errors[errors &gt; truncateValue] = truncateValue</div><div class="line">plt.figure(<span class="number">3</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(steps)):</div><div class="line">    plt.plot(alphas, errors[i, :], label=<span class="string">'n = '</span> + str(steps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMS error'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The results are as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/9_2_2.png" alt="n_step_semi_gradient_td_compare"></p><h3 id="Feature-Construction-for-Linear-Methods"><a href="#Feature-Construction-for-Linear-Methods" class="headerlink" title="Feature Construction for Linear Methods"></a>Feature Construction for Linear Methods</h3><p>Choosing features appropriate to the task is an important way of adding prior domain knowledge to reinforcement learning systems. Intuitively, the features should correspond to the natural features of the task, those along which generalization is most appropriate. If we are valuing geometric objects, for example, we might want to have features for each possible shape, color, size, or function. If we are valuing states of a mobile robot, then we might want to have features for locations, degrees of remaining battery power, recent sonar readings, and so on.</p><h4 id="Polynomials"><a href="#Polynomials" class="headerlink" title="Polynomials"></a>Polynomials</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/poly_feat.png" alt="poly"></p><h4 id="Fourier-Basis"><a href="#Fourier-Basis" class="headerlink" title="Fourier Basis"></a>Fourier Basis</h4><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/fourier.png" alt="fourier"></p><p>Konidaris et al. (2011) found that when using Fourier cosine basis functions with a learning algorithm such as semi-gradient TD(0), or semi-gradient Sarsa, it is helpful to use a diﬀerent step-size parameter for each basis function. If $\alpha$ is the basic step-size parameter, they suggest setting the step-size parameter for basis function $x_i$ to $a_i=\alpha/\sqrt{(c_1^i)^2 + \cdots + (c_d^i)^2}$ (except when each $c_j^i=0$, in which case $\alpha_i=\alpha$).</p><p>Now, let us we compare the Fourier and polynomial bases on the 1000-state random walk example. <strong>In general, we do not recommend using the polynomial basis for online learning.</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># a wrapper class for polynomial / Fourier -based value function</span></div><div class="line">POLYNOMIAL_BASES = <span class="number">0</span></div><div class="line">FOURIER_BASES = <span class="number">1</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasesValueFunction</span>:</span></div><div class="line">    <span class="comment"># @order: # of bases, each function also has one more constant parameter (called bias in machine learning)</span></div><div class="line">    <span class="comment"># @type: polynomial bases or Fourier bases</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, order, type)</span>:</span></div><div class="line">        self.order = order</div><div class="line">        self.weights = np.zeros(order + <span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># set up bases function</span></div><div class="line">        self.bases = []</div><div class="line">        <span class="keyword">if</span> type == POLYNOMIAL_BASES:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, order + <span class="number">1</span>):</div><div class="line">                self.bases.append(<span class="keyword">lambda</span> s, i=i: pow(s, i))</div><div class="line">        <span class="keyword">elif</span> type == FOURIER_BASES:</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, order + <span class="number">1</span>):</div><div class="line">                self.bases.append(<span class="keyword">lambda</span> s, i=i: np.cos(i * np.pi * s))</div><div class="line"></div><div class="line">    <span class="comment"># get the value of @state</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="comment"># map the state space into [0, 1]</span></div><div class="line">        state /= float(N_STATES)</div><div class="line">        <span class="comment"># get the feature vector</span></div><div class="line">        feature = np.asarray([func(state) <span class="keyword">for</span> func <span class="keyword">in</span> self.bases])</div><div class="line">        <span class="keyword">return</span> np.dot(self.weights, feature)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, delta, state)</span>:</span></div><div class="line">        <span class="comment"># map the state space into [0, 1]</span></div><div class="line">        state /= float(N_STATES)</div><div class="line">        <span class="comment"># get derivative value</span></div><div class="line">        derivativeValue = np.asarray([func(state) <span class="keyword">for</span> func <span class="keyword">in</span> self.bases])</div><div class="line">        self.weights += delta * derivativeValue</div></pre></td></tr></table></figure><p>The function upper is used to construction the features of states (map states to features).</p><p>Next, we will compare different super-parameters’ (order) performance:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">1</span></div><div class="line"></div><div class="line">episodes = <span class="number">5000</span></div><div class="line"></div><div class="line"><span class="comment"># # of bases</span></div><div class="line">orders = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>]</div><div class="line"></div><div class="line">alphas = [<span class="number">1e-4</span>, <span class="number">5e-5</span>]</div><div class="line">labels = [[<span class="string">'polynomial basis'</span>] * <span class="number">3</span>, [<span class="string">'fourier basis'</span>] * <span class="number">3</span>]</div><div class="line"></div><div class="line"><span class="comment"># track errors for each episode</span></div><div class="line">errors = np.zeros((len(alphas), len(orders), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(orders)):</div><div class="line">        valueFunctions = [BasesValueFunction(orders[i], POLYNOMIAL_BASES), BasesValueFunction(orders[i], FOURIER_BASES)]</div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">            <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'order:'</span>, orders[i], labels[j][i], <span class="string">'episode:'</span>, episode)</div><div class="line"></div><div class="line">                <span class="comment"># gradient Monte Carlo algorithm</span></div><div class="line">                gradientMonteCarlo(valueFunctions[j], alphas[j])</div><div class="line"></div><div class="line">                <span class="comment"># get state values under current value function</span></div><div class="line">                stateValues = [valueFunctions[j].value(state) <span class="keyword">for</span> state <span class="keyword">in</span> states]</div><div class="line"></div><div class="line">                <span class="comment"># get the root-mean-squared error</span></div><div class="line">                errors[j, i, episode] += np.sqrt(np.mean(np.power(trueStateValues[<span class="number">1</span>: <span class="number">-1</span>] - stateValues, <span class="number">2</span>)))</div><div class="line"></div><div class="line"><span class="comment"># average over independent runs</span></div><div class="line">errors /= runs</div><div class="line"></div><div class="line">plt.figure(<span class="number">5</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, len(orders)):</div><div class="line">        plt.plot(errors[i, j, :], label=labels[i][j]+<span class="string">' order = '</span> + str(orders[j]))</div><div class="line">plt.xlabel(<span class="string">'Episodes'</span>)</div><div class="line">plt.ylabel(<span class="string">'RMSVE'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>Results:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr/poly_vs_four.png" alt="poly_vs_four"></p><h3 id="TODO-TILE-CODING"><a href="#TODO-TILE-CODING" class="headerlink" title="TODO: TILE CODING"></a>TODO: TILE CODING</h3></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/29/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><span class="page-number current">30</span><a class="page-number" href="/page/31/">31</a><span class="space">&hellip;</span><a class="page-number" href="/page/103/">103</a><a class="extend next" rel="next" href="/page/31/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">103</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">55</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>