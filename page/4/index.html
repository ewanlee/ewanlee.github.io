<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/4/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/4/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/15/About-Feature-Scaling-and-Normalization/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/10/15/About-Feature-Scaling-and-Normalization/" itemprop="url">About Feature Scaling and Normalization</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-15T21:48:20+08:00">2018-10-15 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/10/15/About-Feature-Scaling-and-Normalization/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/10/15/About-Feature-Scaling-and-Normalization/" itemprop="commentsCount"></span> </a></span><span id="/2018/10/15/About-Feature-Scaling-and-Normalization/" class="leancloud_visitors" data-flag-title="About Feature Scaling and Normalization"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="About-standardization"><a href="#About-standardization" class="headerlink" title="About standardization"></a>About standardization</h2><p>The result of <strong>standardization</strong> (or <strong>Z-score normalization</strong>) is that the features will be rescaled so that they’ll have the properties of a standard normal distribution with $\mu=0$ and $\sigma=1$</p><p>where $\mu$ is the mean (average) and $\sigma$ is the standard deviation from the mean; standard scores (also called <strong>z</strong> scores) of the samples are calculated as follows:<br>$$<br>z = \frac{x - \mu}{\sigma}<br>$$<br>Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. Intuitively, we can think of gradient descent as a prominent example (an optimization algorithm often used in logistic regression, SVMs, perceptrons, neural networks etc.); with features being on different scales, certain weights may update faster than others since the feature values $x_j$ play a role in the weight updates<br>$$<br>\Delta w_j = - \eta \frac{\partial J}{\partial w_j} = \eta \sum_i (t^{(i)} - o^{(i)}) x_j^{(i)},<br>$$<br>so that</p><p>$w_j := w_j + \Delta w_j$, where $\eta$ is the learning rate, $t$ the target class label, and $o$ the actual output. Other intuitive examples include K-Nearest Neighbor algorithms and clustering algorithms that use, for example, Euclidean distance measures – in fact, tree-based classifier are probably the only classifiers where feature scaling doesn’t make a difference.</p><p>In fact, the only family of algorithms that I could think of being scale-invariant are tree-based methods. Let’s take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as “is feature x_i &gt;= some_val?” Intuitively, we can see that it really doesn’t matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale – it really doesn’t matter).</p><p>Some examples of algorithms where feature scaling matters are:</p><ul><li>k-nearest neighbors with an Euclidean distance measure if want all features to contribute equally</li><li>k-means (see k-nearest neighbors)</li><li>logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than others</li><li>linear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since you’d emphasize variables on “larger measurement scales” more. There are many more cases than I can possibly list here … I always recommend you to think about the algorithm and what it’s doing, and then it typically becomes obvious whether we want to scale your features or not.</li></ul><p>In addition, we’d also want to think about whether we want to “standardize” or “normalize” (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights “equally.” As a rule of thumb I’d say: When in doubt, just standardize the data, it shouldn’t hurt.</p><h2 id="About-Min-Max-scaling"><a href="#About-Min-Max-scaling" class="headerlink" title="About Min-Max scaling"></a>About Min-Max scaling</h2><p>An alternative approach to Z-score normalization (or standardization) is the so-called <strong>Min-Max scaling</strong>(often also simply called “normalization” - a common cause for ambiguities).<br>In this approach, the data is scaled to a fixed range - usually 0 to 1.<br>The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.</p><p>A Min-Max scaling is typically done via the following equation:<br>$$<br>X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}<br>$$</p><h2 id="Z-score-standardization-or-Min-Max-scaling"><a href="#Z-score-standardization-or-Min-Max-scaling" class="headerlink" title="Z-score standardization or Min-Max scaling?"></a>Z-score standardization or Min-Max scaling?</h2><p><em>“Standardization or Min-Max scaling?”</em> - There is no obvious answer to this question: it really depends on the application.</p><p>For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; <a href="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html" target="_blank" rel="external">but more about PCA in my previous article</a>).</p><p>However, this doesn’t mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale.</p><h2 id="Standardizing-and-normalizing-how-it-can-be-done-using-scikit-learn"><a href="#Standardizing-and-normalizing-how-it-can-be-done-using-scikit-learn" class="headerlink" title="Standardizing and normalizing - how it can be done using scikit-learn"></a>Standardizing and normalizing - how it can be done using scikit-learn</h2><p>Of course, we could make use of NumPy’s vectorization capabilities to calculate the z-scores for standardization and to normalize the data using the equations that were mentioned in the previous sections. However, there is an even more convenient approach using the preprocessing module from one of Python’s open-source machine learning library <a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn</a>.</p><p>For the following examples and discussion, we will have a look at the free “Wine” Dataset that is deposited on the UCI machine learning repository<br>(<a href="http://archive.ics.uci.edu/ml/datasets/Wine" target="_blank" rel="external">http://archive.ics.uci.edu/ml/datasets/Wine</a>).</p><blockquote><p>Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.</p><p>Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository [<a href="http://archive.ics.uci.edu/ml" target="_blank" rel="external">http://archive.ics.uci.edu/ml</a>]. Irvine, CA: University of California, School of Information and Computer Science.</p></blockquote><p>The Wine dataset consists of 3 different classes where each row correspond to a particular wine sample.</p><p>The class labels (1, 2, 3) are listed in the first column, and the columns 2-14 correspond to 13 different attributes (features):</p><p>1) Alcohol<br>2) Malic acid<br>…</p><h4 id="Loading-the-wine-dataset"><a href="#Loading-the-wine-dataset" class="headerlink" title="Loading the wine dataset"></a>Loading the wine dataset</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">df = pd.io.parsers.read_csv(</div><div class="line">    <span class="string">'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv'</span>,</div><div class="line">     header=<span class="keyword">None</span>,</div><div class="line">     usecols=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</div><div class="line">    )</div><div class="line"></div><div class="line">df.columns=[<span class="string">'Class label'</span>, <span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]</div><div class="line"></div><div class="line">df.head()</div></pre></td></tr></table></figure><table><thead><tr><th></th><th>Class label</th><th>Alcohol</th><th>Malic acid</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>14.23</td><td>1.71</td></tr><tr><td>1</td><td>1</td><td>13.20</td><td>1.78</td></tr><tr><td>2</td><td>1</td><td>13.16</td><td>2.36</td></tr><tr><td>3</td><td>1</td><td>14.37</td><td>1.95</td></tr><tr><td>4</td><td>1</td><td>13.24</td><td>2.59</td></tr></tbody></table><p>As we can see in the table above, the features <strong>Alcohol</strong> (percent/volumne) and <strong>Malic acid</strong> (g/l) are measured on different scales, so that <strong>Feature Scaling</strong> is necessary important prior to any comparison or combination of these data.</p><h4 id="Standardization-and-Min-Max-scaling"><a href="#Standardization-and-Min-Max-scaling" class="headerlink" title="Standardization and Min-Max scaling"></a>Standardization and Min-Max scaling</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line">std_scale = preprocessing.StandardScaler().fit(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line">df_std = std_scale.transform(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line"></div><div class="line">minmax_scale = preprocessing.MinMaxScaler().fit(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div><div class="line">df_minmax = minmax_scale.transform(df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]])</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'Mean after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_std[:,<span class="number">0</span>].mean(), df_std[:,<span class="number">1</span>].mean()))</div><div class="line">print(<span class="string">'\nStandard deviation after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_std[:,<span class="number">0</span>].std(), df_std[:,<span class="number">1</span>].std()))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Mean after standardization:</div><div class="line">Alcohol=<span class="number">0.00</span>, Malic acid=<span class="number">0.00</span></div><div class="line"></div><div class="line">Standard deviation after standardization:</div><div class="line">Alcohol=<span class="number">1.00</span>, Malic acid=<span class="number">1.00</span></div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">'Min-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_minmax[:,<span class="number">0</span>].min(), df_minmax[:,<span class="number">1</span>].min()))</div><div class="line">print(<span class="string">'\nMax-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;'</span></div><div class="line">      .format(df_minmax[:,<span class="number">0</span>].max(), df_minmax[:,<span class="number">1</span>].max()))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Min-value after min-max scaling:</div><div class="line">Alcohol=<span class="number">0.00</span>, Malic acid=<span class="number">0.00</span></div><div class="line"></div><div class="line">Max-value after min-max scaling:</div><div class="line">Alcohol=<span class="number">1.00</span>, Malic acid=<span class="number">1.00</span></div></pre></td></tr></table></figure><h4 id="Plotting"><a href="#Plotting" class="headerlink" title="Plotting"></a>Plotting</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">()</span>:</span></div><div class="line">    plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</div><div class="line"></div><div class="line">    plt.scatter(df[<span class="string">'Alcohol'</span>], df[<span class="string">'Malic acid'</span>],</div><div class="line">            color=<span class="string">'green'</span>, label=<span class="string">'input scale'</span>, alpha=<span class="number">0.5</span>)</div><div class="line"></div><div class="line">    plt.scatter(df_std[:,<span class="number">0</span>], df_std[:,<span class="number">1</span>], color=<span class="string">'red'</span>,</div><div class="line">            label=<span class="string">'Standardized [$$N  (\mu=0, \; \sigma=1)$$]'</span>, alpha=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">    plt.scatter(df_minmax[:,<span class="number">0</span>], df_minmax[:,<span class="number">1</span>],</div><div class="line">            color=<span class="string">'blue'</span>, label=<span class="string">'min-max scaled [min=0, max=1]'</span>, alpha=<span class="number">0.3</span>)</div><div class="line"></div><div class="line">    plt.title(<span class="string">'Alcohol and Malic Acid content of the wine dataset'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Alcohol'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Malic Acid'</span>)</div><div class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</div><div class="line">    plt.grid()</div><div class="line"></div><div class="line">    plt.tight_layout()</div><div class="line"></div><div class="line">plot()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_44_0.png" alt="png"></p><p>The plot above includes the wine datapoints on all three different scales: the input scale where the alcohol content was measured in volume-percent (green), the standardized features (red), and the normalized features (blue). In the following plot, we will zoom in into the three different axis-scales.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">fig, ax = plt.subplots(<span class="number">3</span>, figsize=(<span class="number">6</span>,<span class="number">14</span>))</div><div class="line"></div><div class="line"><span class="keyword">for</span> a,d,l <span class="keyword">in</span> zip(range(len(ax)),</div><div class="line">               (df[[<span class="string">'Alcohol'</span>, <span class="string">'Malic acid'</span>]].values, df_std, df_minmax),</div><div class="line">               (<span class="string">'Input scale'</span>,</div><div class="line">                <span class="string">'Standardized [$$N  (\mu=0, \; \sigma=1)$$]'</span>,</div><div class="line">                <span class="string">'min-max scaled [min=0, max=1]'</span>)</div><div class="line">                ):</div><div class="line">    <span class="keyword">for</span> i,c <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'green'</span>)):</div><div class="line">        ax[a].scatter(d[df[<span class="string">'Class label'</span>].values == i, <span class="number">0</span>],</div><div class="line">                  d[df[<span class="string">'Class label'</span>].values == i, <span class="number">1</span>],</div><div class="line">                  alpha=<span class="number">0.5</span>,</div><div class="line">                  color=c,</div><div class="line">                  label=<span class="string">'Class %s'</span> %i</div><div class="line">                  )</div><div class="line">    ax[a].set_title(l)</div><div class="line">    ax[a].set_xlabel(<span class="string">'Alcohol'</span>)</div><div class="line">    ax[a].set_ylabel(<span class="string">'Malic Acid'</span>)</div><div class="line">    ax[a].legend(loc=<span class="string">'upper left'</span>)</div><div class="line">    ax[a].grid()</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_48_0.png" alt="png"></p><h2 id="Bottom-up-approaches"><a href="#Bottom-up-approaches" class="headerlink" title="Bottom-up approaches"></a>Bottom-up approaches</h2><p>Of course, we can also code the equations for standardization and 0-1 Min-Max scaling “manually”. However, the scikit-learn methods are still useful if you are working with test and training data sets and want to scale them equally.</p><p>E.g.,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">std_scale = preprocessing.StandardScaler().fit(X_train)</div><div class="line">X_train = std_scale.transform(X_train)</div><div class="line">X_test = std_scale.transform(X_test)</div></pre></td></tr></table></figure><p>Below, we will perform the calculations using “pure” Python code, and an more convenient NumPy solution, which is especially useful if we attempt to transform a whole matrix.</p><h3 id="Vanilla-Python"><a href="#Vanilla-Python" class="headerlink" title="Vanilla Python"></a>Vanilla Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Standardization</span></div><div class="line"></div><div class="line">x = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">3</span>]</div><div class="line">mean = sum(x)/len(x)</div><div class="line">std_dev = (<span class="number">1</span>/len(x) * sum([ (x_i - mean)**<span class="number">2</span> <span class="keyword">for</span> x_i <span class="keyword">in</span> x]))**<span class="number">0.5</span></div><div class="line"></div><div class="line">z_scores = [(x_i - mean)/std_dev <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</div><div class="line"></div><div class="line"><span class="comment"># Min-Max scaling</span></div><div class="line"></div><div class="line">minmax = [(x_i - min(x)) / (max(x) - min(x)) <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</div></pre></td></tr></table></figure><h3 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># Standardization</span></div><div class="line"></div><div class="line">x_np = np.asarray(x)</div><div class="line">z_scores_np = (x_np - x_np.mean()) / x_np.std()</div><div class="line"></div><div class="line"><span class="comment"># Min-Max scaling</span></div><div class="line"></div><div class="line">np_minmax = (x_np - x_np.min()) / (x_np.max() - x_np.min())</div></pre></td></tr></table></figure><h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><p>Just to make sure that our code works correctly, let us plot the results via matplotlib.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=<span class="number">2</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">5</span>))</div><div class="line"></div><div class="line">y_pos = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x))]</div><div class="line"></div><div class="line">ax1.scatter(z_scores, y_pos, color=<span class="string">'g'</span>)</div><div class="line">ax1.set_title(<span class="string">'Python standardization'</span>, color=<span class="string">'g'</span>)</div><div class="line"></div><div class="line">ax2.scatter(minmax, y_pos, color=<span class="string">'g'</span>)</div><div class="line">ax2.set_title(<span class="string">'Python Min-Max scaling'</span>, color=<span class="string">'g'</span>)</div><div class="line"></div><div class="line">ax3.scatter(z_scores_np, y_pos, color=<span class="string">'b'</span>)</div><div class="line">ax3.set_title(<span class="string">'Python NumPy standardization'</span>, color=<span class="string">'b'</span>)</div><div class="line"></div><div class="line">ax4.scatter(np_minmax, y_pos, color=<span class="string">'b'</span>)</div><div class="line">ax4.set_title(<span class="string">'Python NumPy Min-Max scaling'</span>, color=<span class="string">'b'</span>)</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> (ax1, ax2, ax3, ax4):</div><div class="line">    ax.get_yaxis().set_visible(<span class="keyword">False</span>)</div><div class="line">    ax.grid()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_64_0.png" alt="png"></p><h2 id="The-effect-of-standardization-on-PCA-in-a-pattern-classification-task"><a href="#The-effect-of-standardization-on-PCA-in-a-pattern-classification-task" class="headerlink" title="The effect of standardization on PCA in a pattern classification task"></a>The effect of standardization on PCA in a pattern classification task</h2><p>Earlier, I mentioned the Principal Component Analysis (PCA) as an example where standardization is crucial, since it is “analyzing” the variances of the different features.<br>Now, let us see how the standardization affects PCA and a following supervised classification on the whole wine dataset.</p><p>In the following section, we will go through the following steps:</p><ul><li>Reading in the dataset</li><li>Dividing the dataset into a separate training and test dataset</li><li>Standardization of the features</li><li>Principal Component Analysis (PCA) to reduce the dimensionality</li><li>Training a naive Bayes classifier</li><li>Evaluating the classification accuracy with and without standardization</li></ul><h3 id="Reading-in-the-dataset"><a href="#Reading-in-the-dataset" class="headerlink" title="Reading in the dataset"></a>Reading in the dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"></div><div class="line">df = pd.io.parsers.read_csv(</div><div class="line">    <span class="string">'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv'</span>,</div><div class="line">    header=<span class="keyword">None</span>,</div><div class="line">    )</div></pre></td></tr></table></figure><h3 id="Dividing-the-dataset-into-a-separate-training-and-test-dataset"><a href="#Dividing-the-dataset-into-a-separate-training-and-test-dataset" class="headerlink" title="Dividing the dataset into a separate training and test dataset"></a>Dividing the dataset into a separate training and test dataset</h3><p>In this step, we will randomly divide the wine dataset into a training dataset and a test dataset where the training dataset will contain 70% of the samples and the test dataset will contain 30%, respectively.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line">X_wine = df.values[:,<span class="number">1</span>:]</div><div class="line">y_wine = df.values[:,<span class="number">0</span>]</div><div class="line"></div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine,</div><div class="line">    test_size=<span class="number">0.30</span>, random_state=<span class="number">12345</span>)</div></pre></td></tr></table></figure><h3 id="Feature-Scaling-Standardization"><a href="#Feature-Scaling-Standardization" class="headerlink" title="Feature Scaling - Standardization"></a>Feature Scaling - Standardization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</div><div class="line"></div><div class="line">std_scale = preprocessing.StandardScaler().fit(X_train)</div><div class="line">X_train_std = std_scale.transform(X_train)</div><div class="line">X_test_std = std_scale.transform(X_test)</div></pre></td></tr></table></figure><h3 id="Dimensionality-reduction-via-Principal-Component-Analysis-PCA"><a href="#Dimensionality-reduction-via-Principal-Component-Analysis-PCA" class="headerlink" title="Dimensionality reduction via Principal Component Analysis (PCA)"></a>Dimensionality reduction via Principal Component Analysis (PCA)</h3><p>Now, we perform a PCA on the standardized and the non-standardized datasets to transform the dataset onto a 2-dimensional feature subspace.<br>In a real application, a procedure like cross-validation would be done in order to find out what choice of features would yield a optimal balance between “preserving information” and “overfitting” for different classifiers. However, we will omit this step since we don’t want to train a perfect classifier here, but merely compare the effects of standardization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</div><div class="line"></div><div class="line"><span class="comment"># on non-standardized data</span></div><div class="line">pca = PCA(n_components=<span class="number">2</span>).fit(X_train)</div><div class="line">X_train = pca.transform(X_train)</div><div class="line">X_test = pca.transform(X_test)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># om standardized data</span></div><div class="line">pca_std = PCA(n_components=<span class="number">2</span>).fit(X_train_std)</div><div class="line">X_train_std = pca_std.transform(X_train_std)</div><div class="line">X_test_std = pca_std.transform(X_test_std)</div></pre></td></tr></table></figure><p>Let us quickly visualize how our new feature subspace looks like (note that class labels are not considered in a PCA - in contrast to a Linear Discriminant Analysis - but I will add them in the plot for clarity).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">fig, (ax1, ax2) = plt.subplots(ncols=<span class="number">2</span>, figsize=(<span class="number">10</span>,<span class="number">4</span>))</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">for</span> l,c,m <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'blue'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>), (<span class="string">'^'</span>, <span class="string">'s'</span>, <span class="string">'o'</span>)):</div><div class="line">    ax1.scatter(X_train[y_train==l, <span class="number">0</span>], X_train[y_train==l, <span class="number">1</span>],</div><div class="line">        color=c,</div><div class="line">        label=<span class="string">'class %s'</span> %l,</div><div class="line">        alpha=<span class="number">0.5</span>,</div><div class="line">        marker=m</div><div class="line">        )</div><div class="line"></div><div class="line"><span class="keyword">for</span> l,c,m <span class="keyword">in</span> zip(range(<span class="number">1</span>,<span class="number">4</span>), (<span class="string">'blue'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>), (<span class="string">'^'</span>, <span class="string">'s'</span>, <span class="string">'o'</span>)):</div><div class="line">    ax2.scatter(X_train_std[y_train==l, <span class="number">0</span>], X_train_std[y_train==l, <span class="number">1</span>],</div><div class="line">        color=c,</div><div class="line">        label=<span class="string">'class %s'</span> %l,</div><div class="line">        alpha=<span class="number">0.5</span>,</div><div class="line">        marker=m</div><div class="line">        )</div><div class="line"></div><div class="line">ax1.set_title(<span class="string">'Transformed NON-standardized training dataset after PCA'</span>)    </div><div class="line">ax2.set_title(<span class="string">'Transformed standardized training dataset after PCA'</span>)    </div><div class="line"></div><div class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> (ax1, ax2):</div><div class="line"></div><div class="line">    ax.set_xlabel(<span class="string">'1st principal component'</span>)</div><div class="line">    ax.set_ylabel(<span class="string">'2nd principal component'</span>)</div><div class="line">    ax.legend(loc=<span class="string">'upper right'</span>)</div><div class="line">    ax.grid()</div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure><p><img src="http://sebastianraschka.com/images/blog/2014/about_standardization_normalization/about_standardization_normalization_89_0.png" alt="png"></p><h3 id="Training-a-naive-Bayes-classifier"><a href="#Training-a-naive-Bayes-classifier" class="headerlink" title="Training a naive Bayes classifier"></a>Training a naive Bayes classifier</h3><p>We will use a naive Bayes classifier for the classification task. If you are not familiar with it, the term “naive” comes from the assumption that all features are “independent”.<br>All in all, it is a simple but robust classifier based on Bayes’ rule</p><p>I don’t want to get into more detail about Bayes’ rule in this article, but if you are interested in a more detailed collection of examples, please have a look at the <a href="https://github.com/rasbt/pattern_classification#statistical-pattern-recognition-examples" target="_blank" rel="external">Statistical Patter Classification</a> in my pattern classification repository.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"></div><div class="line"><span class="comment"># on non-standardized data</span></div><div class="line">gnb = GaussianNB()</div><div class="line">fit = gnb.fit(X_train, y_train)</div><div class="line"></div><div class="line"><span class="comment"># on standardized data</span></div><div class="line">gnb_std = GaussianNB()</div><div class="line">fit_std = gnb_std.fit(X_train_std, y_train)</div></pre></td></tr></table></figure><h3 id="Evaluating-the-classification-accuracy-with-and-without-standardization"><a href="#Evaluating-the-classification-accuracy-with-and-without-standardization" class="headerlink" title="Evaluating the classification accuracy with and without standardization"></a>Evaluating the classification accuracy with and without standardization</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"></div><div class="line">pred_train = gnb.predict(X_train)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the training dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;'</span>.format(metrics.accuracy_score(y_train, pred_train)))</div><div class="line"></div><div class="line">pred_test = gnb.predict(X_test)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the test dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;\n'</span>.format(metrics.accuracy_score(y_test, pred_test)))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Prediction accuracy <span class="keyword">for</span> the training dataset</div><div class="line"><span class="number">81.45</span>%</div><div class="line"></div><div class="line">Prediction accuracy <span class="keyword">for</span> the test dataset</div><div class="line"><span class="number">64.81</span>%</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">pred_train_std = gnb_std.predict(X_train_std)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the training dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;'</span>.format(metrics.accuracy_score(y_train, pred_train_std)))</div><div class="line"></div><div class="line">pred_test_std = gnb_std.predict(X_test_std)</div><div class="line"></div><div class="line">print(<span class="string">'\nPrediction accuracy for the test dataset'</span>)</div><div class="line">print(<span class="string">'&#123;:.2%&#125;\n'</span>.format(metrics.accuracy_score(y_test, pred_test_std)))</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Prediction accuracy <span class="keyword">for</span> the training dataset</div><div class="line"><span class="number">96.77</span>%</div><div class="line"></div><div class="line">Prediction accuracy <span class="keyword">for</span> the test dataset</div><div class="line"><span class="number">98.15</span>%</div></pre></td></tr></table></figure><p>As we can see, the standardization prior to the PCA definitely led to an decrease in the empirical error rate on classifying samples from test dataset.</p><h2 id="Appendix-A-The-effect-of-scaling-and-mean-centering-of-variables-prior-to-PCA"><a href="#Appendix-A-The-effect-of-scaling-and-mean-centering-of-variables-prior-to-PCA" class="headerlink" title="Appendix A: The effect of scaling and mean centering of variables prior to PCA"></a>Appendix A: The effect of scaling and mean centering of variables prior to PCA</h2><p>Let us think about whether it matters or not if the variables are centered for applications such as Principal Component Analysis (PCA) if the PCA is calculated from the covariance matrix (i.e., the kkprincipal components are the eigenvectors of the covariance matrix that correspond to the kk largest eigenvalues.</p><h3 id="1-Mean-centering-does-not-affect-the-covariance-matrix"><a href="#1-Mean-centering-does-not-affect-the-covariance-matrix" class="headerlink" title="1. Mean centering does not affect the covariance matrix"></a>1. Mean centering does not affect the covariance matrix</h3><p>Here, the rational is: If the covariance is the same whether the variables are centered or not, the result of the PCA will be the same.</p><p>Let’s assume we have the 2 variables $x$ and $y$ Then the covariance between the attributes is calculated as<br>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>Let us write the centered variables as<br>$$<br>x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}<br>$$<br>The centered covariance would then be calculated as follows:<br>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i’ - \bar{y}’)<br>$$<br>But since after centering, $\bar{x}’ = 0$ and $\bar{y}’ = 0$ we have</p><p>$\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} x_i’ y_i’$ which is our original covariance matrix if we resubstitute back the terms $x’ = x - \bar{x} \text{ and } y’ = y - \bar{y}$.</p><p>Even centering only one variable, e.g., xx wouldn’t affect the covariance:</p><p>$$<br>\sigma_{\text{xy}} = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - \bar{x}’)(y_i - \bar{y})<br>$$</p><h3 id="2-Scaling-of-variables-does-affect-the-covariance-matrix"><a href="#2-Scaling-of-variables-does-affect-the-covariance-matrix" class="headerlink" title="2. Scaling of variables does affect the covariance matrix"></a>2. Scaling of variables does affect the covariance matrix</h3><p>If one variable is scaled, e.g, from pounds into kilogram (1 pound = 0.453592 kg), it does affect the covariance and therefore influences the results of a PCA.</p><p>Let cc be the scaling factor for $x$</p><p>Given that the “original” covariance is calculated as</p><p>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>the covariance after scaling would be calculated as:</p><p>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (c \cdot x_i - c \cdot \bar{x})(y_i - \bar{y}) = \frac{c}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y}) \Rightarrow \sigma_{xy}’ = c \cdot \sigma_{xy}<br>$$<br>Therefore, the covariance after scaling one attribute by the constant $c$ will result in a rescaled covariance $c \sigma_{xy}$ So if we’d scaled $x$ from pounds to kilograms, the covariance between $x$ and $y$ will be 0.453592 times smaller.</p><h3 id="3-Standardizing-affects-the-covariance"><a href="#3-Standardizing-affects-the-covariance" class="headerlink" title="3. Standardizing affects the covariance"></a>3. Standardizing affects the covariance</h3><p>Standardization of features will have an effect on the outcome of a PCA (assuming that the variables are originally not standardized). This is because we are scaling the covariance between every pair of variables by the product of the standard deviations of each pair of variables.</p><p>The equation for standardization of a variable is written as</p><p>$$<br>z = \frac{x_i - \bar{x}}{\sigma}<br>$$<br>The “original” covariance matrix:</p><p>$$<br>\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$<br>And after standardizing both variables:</p><p>$$<br>x’ = \frac{x - \bar{x}}{\sigma_x} \text{ and } y’ =\frac{y - \bar{y}}{\sigma_y}<br>$$</p><p>$$<br>\sigma_{xy}’ = \frac{1}{n-1} \sum_{i}^{n} (x_i’ - 0)(y_i’ - 0) = \frac{1}{n-1} \sum_{i}^{n} \bigg(\frac{x - \bar{x}}{\sigma_x}\bigg)\bigg(\frac{y - \bar{y}}{\sigma_y}\bigg) = \frac{1}{(n-1) \cdot \sigma_x \sigma_y} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})<br>$$</p><p>$$<br>\Rightarrow \sigma_{xy}’ = \frac{\sigma_{xy}}{\sigma_x \sigma_y}<br>$$</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/07/Install-SerpentAI-on-Windows-10/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/06/07/Install-SerpentAI-on-Windows-10/" itemprop="url">Install SerpentAI on Windows 10</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-07T21:50:00+08:00">2018-06-07 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/06/07/Install-SerpentAI-on-Windows-10/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/06/07/Install-SerpentAI-on-Windows-10/" itemprop="commentsCount"></span> </a></span><span id="/2018/06/07/Install-SerpentAI-on-Windows-10/" class="leancloud_visitors" data-flag-title="Install SerpentAI on Windows 10"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Python-Environment"><a href="#Python-Environment" class="headerlink" title="Python Environment"></a>Python Environment</h2><h3 id="Python-3-6-with-Anaconda"><a href="#Python-3-6-with-Anaconda" class="headerlink" title="Python 3.6+ (with Anaconda)"></a>Python 3.6+ (with Anaconda)</h3><p>Serpent.AI was developed taking full advantage of Python 3.6 so it is only natural that the Python requirement be for versions 3.6 and up.</p><p>Installing regular Python 3.6+ isn’t exactly difficult but Serpent.AI relies on a good amount of scientific computing libraries that are extremely difficult / impossible to compile on your own on Windows. Thankfully, the <a href="https://www.anaconda.com/distribution" target="_blank" rel="external">Anaconda Distribution</a> exists and takes this huge weight off our collective shoulders.</p><h4 id="Installing-Anaconda-5-2-0-Python-3-6"><a href="#Installing-Anaconda-5-2-0-Python-3-6" class="headerlink" title="Installing Anaconda 5.2.0 (Python 3.6)"></a>Installing Anaconda 5.2.0 (Python 3.6)</h4><p><a href="https://www.anaconda.com/download/" target="_blank" rel="external">Download</a> the Python 3.6 version of Anaconda 5.2.0 and run the graphical installer.</p><p>The following commands are to be performed in an <em>Anaconda Prompt</em> with elevated privileges (Right click and <strong>Run as Administrator</strong>). It is recommended to create a shortcut to this prompt because every Python and Serpent command will have to be performed from there starting now.</p><h4 id="Creating-a-Conda-Env-for-Serpent-AI"><a href="#Creating-a-Conda-Env-for-Serpent-AI" class="headerlink" title="Creating a Conda Env for Serpent.AI"></a>Creating a Conda Env for Serpent.AI</h4><p><code>conda create --name serpent python=3.6</code> (‘serpent’ can be replaced with another name)</p><h4 id="Creating-a-directory-for-your-Serpent-AI-projects"><a href="#Creating-a-directory-for-your-Serpent-AI-projects" class="headerlink" title="Creating a directory for your Serpent.AI projects"></a>Creating a directory for your Serpent.AI projects</h4><p><code>mkdir SerpentAI &amp;&amp; cd SerpentAI</code></p><h4 id="Activating-the-Conda-Env"><a href="#Activating-the-Conda-Env" class="headerlink" title="Activating the Conda Env"></a>Activating the Conda Env</h4><p><code>conda activate serpent</code></p><h2 id="3rd-Party-Dependencies"><a href="#3rd-Party-Dependencies" class="headerlink" title="3rd-Party Dependencies"></a>3rd-Party Dependencies</h2><h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><p>Redis is used in the framework as the in-memory store for the captured frame buffers as well as the temporary storage of analytics events. It is not meant to be compatible with Windows! Microsoft used to <a href="https://github.com/MicrosoftArchive/redis" target="_blank" rel="external">maintain a port</a> but it’s been abandoned since. This being said, that Redis version is sufficient and it outperforms stuff like running it in WSL on Windows 10. It will install as a Windows Service. Make sure you set it to start automatically.</p><h4 id="Install-Windows-Subsystem-for-Linux-WSL"><a href="#Install-Windows-Subsystem-for-Linux-WSL" class="headerlink" title="Install Windows Subsystem for Linux (WSL)"></a><a href="https://msdn.microsoft.com/en-us/commandline/wsl/install_guide" target="_blank" rel="external">Install Windows Subsystem for Linux (WSL)</a></h4><ol><li>From Start, search for <strong>Turn Windows features on or off</strong> (type <code>turn</code>)</li><li><strong>Select Windows Subsystem for Linux (beta)</strong></li></ol><p><a href="https://raw.githubusercontent.com/ServiceStack/Assets/master/img/redis/install-wsl.png" target="_blank" rel="external"><img src="https://raw.githubusercontent.com/ServiceStack/Assets/master/img/redis/install-wsl.png" alt="img"></a></p><p>Once installed you can run bash on Ubuntu by typing <strong>bash</strong> from a Windows Command Prompt. To install the latest version of Redis we’ll need to use a repository that maintains up-to-date packages for Ubuntu and Debian servers like <a href="https://www.dotdeb.org/" target="_blank" rel="external">https://www.dotdeb.org</a> which you can add to Ubuntu’s apt-get sources with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ echo deb http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list</div><div class="line">$ echo deb-src http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list</div><div class="line">$ sudo mv dotdeb.org.list /etc/apt/sources.list.d</div><div class="line">$ wget -q -O - http://www.dotdeb.org/dotdeb.gpg | sudo apt-key add -</div></pre></td></tr></table></figure><p>Then after updating our APT cache we can install Redis with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install redis-server</div></pre></td></tr></table></figure><p>You’ll then be able to launch redis with:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ redis-server --daemonize yes</div></pre></td></tr></table></figure><p>Which will run redis in the background freeing your shell so you can play with it using the redis client:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ redis-cli</div><div class="line">$ 127.0.0.1:6379&gt; SET foo bar</div><div class="line">OK</div><div class="line">$ 127.0.0.1:6379&gt; GET foo</div><div class="line">&quot;bar&quot;</div></pre></td></tr></table></figure><p>Which you can connect to from within bash or from your Windows desktop using the <a href="https://github.com/ServiceStack/redis-windows#option-3-running-microsofts-native-port-of-redis" target="_blank" rel="external">redis-cli native Windows binary from MSOpenTech</a>.</p><h3 id="Build-Tools-for-Visual-Studio-2017"><a href="#Build-Tools-for-Visual-Studio-2017" class="headerlink" title="Build Tools for Visual Studio 2017"></a>Build Tools for Visual Studio 2017</h3><p>Some of the packages that will be installed alongside Serpent.AI are not pre-compiled binaries and will be need to be built from source. This is a little more problematic for Windows but with the correct C++ Build Tools for Visual Studio it all goes down smoothly.</p><p>You can get the proper installer by visiting <a href="https://www.visualstudio.com/downloads/" target="_blank" rel="external">https://www.visualstudio.com/downloads/</a> and scrolling down to the <em>Build Tools for Visual Studio 2017</em> download. Download, run, select the <em>Visual C++ build tools</em> section and make sure the following components are checked (VSs are not installed):</p><ul><li>Visual C++ Build Tools core features</li><li>VC++ 2017 version 15.7 v14.14 latest v141 tools</li><li>Visual C++ 2017 Redistributable Update</li><li>VC++ 2015.3 v14.00 (v140) toolset for desktop</li><li>Windows 10 SDK (10.0.17134.0)</li><li>Windows Universal CRT SDK</li></ul><h2 id="Installing-Serpent-AI"><a href="#Installing-Serpent-AI" class="headerlink" title="Installing Serpent.AI"></a>Installing Serpent.AI</h2><p>Once all of the above had been installed and set up, you are ready to install the framework. Remember that PATH changes in Windows are not reflected in your command prompts that were opened while you made the changes. Open a fresh Anaconda prompt before continuing to avoid installation issues.</p><p>Go back to the directory you created earlier for your Serpent.AI projects. Make sure you are scoped in your Conda Env.</p><p>Run <code>pip install SerpentAI</code></p><p>Then run <code>serpent setup</code> to install the remaining dependencies automatically.</p><h2 id="Installing-Optional-Modules"><a href="#Installing-Optional-Modules" class="headerlink" title="Installing Optional Modules"></a>Installing Optional Modules</h2><p>In the spirit of keeping the initial installation on the light side, some specialized / niche components with extra dependencies have been isolated from the core. It is recommended to only focus on installing them once you reach a point where you actually need them. The framework will provide a warning when a feature you are trying to use requires one of those modules.</p><h3 id="OCR"><a href="#OCR" class="headerlink" title="OCR"></a>OCR</h3><p>A module to provide OCR functionality in your game agents.</p><h4 id="Tesseract"><a href="#Tesseract" class="headerlink" title="Tesseract"></a>Tesseract</h4><p>Serpent.AI leverages Tesseract for its OCR functionality. You can install Tesseract for Windows by following these steps:</p><ol><li>Visit <a href="https://github.com/UB-Mannheim/tesseract/wiki" target="_blank" rel="external">https://github.com/UB-Mannheim/tesseract/wiki</a></li><li>Download the .exe for version 3</li><li>Run the graphical installer (Remember the install path!)</li><li>Add the path to <em>tesseract.exe</em> to your %PATH% environment variable</li></ol><p>You can test your Tesseract installation by opening an Anaconda Prompt and executing <code>tesseract --list-langs</code>.</p><h4 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h4><p>Once you’ve validated that Tesseract has been properly set up, you can install the module with <code>serpent setup ocr</code></p><h3 id="GUI"><a href="#GUI" class="headerlink" title="GUI"></a>GUI</h3><p>A module to allow Serpent.AI desktop app to run.</p><h4 id="Kivy"><a href="#Kivy" class="headerlink" title="Kivy"></a>Kivy</h4><p>Kivy is the GUI framework used in the framework.</p><p>Once you are ready to test your Kivy, you can install the module with <code>serpent setup gui</code> and try to run <code>serpent visual_debugger</code></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/Matching-Networks-for-One-Shot-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/06/02/Matching-Networks-for-One-Shot-Learning/" itemprop="url">Matching Networks for One Shot Learning</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-02T22:35:08+08:00">2018-06-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/06/02/Matching-Networks-for-One-Shot-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/06/02/Matching-Networks-for-One-Shot-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2018/06/02/Matching-Networks-for-One-Shot-Learning/" class="leancloud_visitors" data-flag-title="Matching Networks for One Shot Learning"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>By DeepMind crew: <strong>Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra</strong></p><p>This is a paper on <strong>one-shot</strong> learning, where we’d like to learn a class based on very few (or indeed, 1) training examples. E.g. it suffices to show a child a single giraffe, not a few hundred thousands before it can recognize more giraffes.</p><p>This paper falls into a category of <em>“duh of course”</em> kind of paper, something very interesting, powerful, but somehow obvious only in retrospect. I like it.</p><p>Suppose you’re given a single example of some class and would like to label it in test images.</p><ul><li><strong>Observation 1</strong>: a standard approach might be to train an Exemplar SVM for this one (or few) examples vs. all the other training examples - i.e. a linear classifier. But this requires optimization.</li><li><strong>Observation 2:</strong> known non-parameteric alternatives (e.g. k-Nearest Neighbor) don’t suffer from this problem. E.g. I could immediately use a Nearest Neighbor to classify the new class without having to do any optimization whatsoever. However, NN is gross because it depends on an (arbitrarily-chosen) metric, e.g. L2 distance. Ew.</li><li><strong>Core idea</strong>: lets train a fully end-to-end nearest neighbor classifer!<img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2010.08.44%20PM.png" alt="Screen Shot 2016-08-07 at 10.08.44 PM"></li></ul><h2 id="The-training-protocol"><a href="#The-training-protocol" class="headerlink" title="The training protocol"></a>The training protocol</h2><p>As the authors amusingly point out in the conclusion (and this is the <em>duh of course</em> part), <em>“one-shot learning is much easier if you train the network to do one-shot learning”</em>. Therefore, we want the test-time protocol (given N novel classes with only k examples each (e.g. k = 1 or 5), predict new instances to one of N classes) to exactly match the training time protocol.</p><p>To create each “episode” of training from a dataset of examples then:</p><ol><li>Sample a task T from the training data, e.g. select 5 labels, and up to 5 examples per label (i.e. 5-25 examples).</li><li>To form one episode sample a label set L (e.g. {cats, dogs}) and then use L to sample the support set S and a batch B of examples to evaluate loss on.</li></ol><p>The idea on high level is clear but the writing here is a bit unclear on details, of exactly how the sampling is done.</p><h2 id="The-model"><a href="#The-model" class="headerlink" title="The model"></a>The model</h2><p>I find the paper’s model description slightly wordy and unclear, but basically we’re building a <strong>differentiable nearest neighbor++</strong>. The output \hat{y} for a test example \hat{x} is computed very similar to what you might see in Nearest Neighbors:<img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.14.26%20PM.png" alt="Screen Shot 2016-08-07 at 11.14.26 PM"><br>where <strong>a</strong> acts as a kernel, computing the extent to which \hat{x} is similar to a training example x_i, and then the labels from the training examples (y_i) are weight-blended together accordingly. The paper doesn’t mention this but I assume for classification y_i would presumbly be one-hot vectors.</p><p>Now, we’re going to embed both the training examples x_i and the test example \hat{x}, and we’ll interpret their inner products (or here a cosine similarity) as the “match”, and pass that through a softmax to get normalized mixing weights so they add up to 1. No surprises here, this is quite natural:</p><p><img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.20.29%20PM.png" alt="Screen Shot 2016-08-07 at 11.20.29 PM"><br>Here <strong>c()</strong> is cosine distance, which I presume is implemented by normalizing the two input vectors to have unit L2 norm and taking a dot product. I assume the authors tried skipping the normalization too and it did worse? Anyway, now all that’s left to define is the function <strong>f</strong> (i.e. how do we embed the test example into a vector) and the function <strong>g</strong> (i.e. how do we embed each training example into a vector?).</p><p><strong>Embedding the training examples.</strong> This (the function <strong>g</strong>) is a bidirectional LSTM over the examples:</p><p><img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-07%20at%2011.57.10%20PM.png" alt="Screen Shot 2016-08-07 at 11.57.10 PM"></p><p>i.e. encoding of i’th example x_i is a function of its “raw” embedding g’(x_i) and the embedding of its friends, communicated through the bidirectional network’s hidden states. i.e. each training example is a function of not just itself but all of its friends in the set. This is part of the ++ above, because in a normal nearest neighbor you wouldn’t change the representation of an example as a function of the other data points in the training set.</p><p>It’s odd that the <strong>order</strong> is not mentioned, I assume it’s random? This is a bit gross because order matters to a bidirectional LSTM; you’d get different embeddings if you permute the examples.</p><p><strong>Embedding the test example.</strong> This (the function <strong>f</strong>) is a an LSTM that processes for a fixed amount (K time steps) and at each point also <em>attends</em> over the examples in the training set. The encoding is the last hidden state of the LSTM. Again, this way we’re allowing the network to change its encoding of the test example as a function of the training examples. Nifty: <img src="https://raw.githubusercontent.com/karpathy/paper-notes/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2012.11.15%20AM.png" alt="Screen Shot 2016-08-08 at 12.11.15 AM"></p><p>That looks scary at first but it’s really just a vanilla LSTM with attention where the input at each time step is constant (f’(\hat{x}), an encoding of the test example all by itself) and the hidden state is a function of previous hidden state but also a concatenated readout vector <strong>r</strong>, which we obtain by attending over the encoded training examples (encoded with <strong>g</strong> from above).</p><p>Oh and I assume there is a typo in equation (5), it should say r_k = … without the -1 on LHS.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>Task</strong>: N-way k-shot learning task. i.e. we’re given k (e.g. 1 or 5) labelled examples for N classes that we have not previously trained on and asked to classify new instances into he N classes.</p><p><strong>Baselines:</strong> an “obvious” strategy of using a pretrained ConvNet and doing nearest neighbor based on the codes. An option of finetuning the network on the new examples as well (requires training and careful and strong regularization!).</p><p><strong>MANN</strong> of Santoro et al. [21]: Also a DeepMind paper, a fun NTM-like Meta-Learning approach that is fed a sequence of examples and asked to predict their labels.</p><p><strong>Siamese network</strong> of Koch et al. [11]: A siamese network that takes two examples and predicts whether they are from the same class or not with logistic regression. A test example is labeled with a nearest neighbor: with the class it matches best according to the siamese net (requires iteration over all training examples one by one). Also, this approach is less end-to-end than the one here because it requires the ad-hoc nearest neighbor matching, while here the <em>exact</em> end task is optimized for. It’s beautiful.</p><h3 id="Omniglot-experiments"><a href="#Omniglot-experiments" class="headerlink" title="Omniglot experiments"></a>Omniglot experiments</h3><h3><a href="#" class="headerlink"></a><img src="https://github.com/karpathy/paper-notes/raw/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2010.21.45%20AM.png" alt="Screen Shot 2016-08-08 at 10.21.45 AM"></h3><p>Omniglot of <a href="http://www.cs.toronto.edu/~rsalakhu/papers/LakeEtAl2015Science.pdf" target="_blank" rel="external">Lake et al. [14]</a> is a MNIST-like scribbles dataset with 1623 characters with 20 examples each.</p><p>Image encoder is a CNN with 4 modules of [3x3 CONV 64 filters, batchnorm, ReLU, 2x2 max pool]. The original image is claimed to be so resized from original 28x28 to 1x1x64, which doesn’t make sense because factor of 2 downsampling 4 times is reduction of 16, and 28/16 is a non-integer &gt;1. I’m assuming they use VALID convs?</p><p>Results: <img src="https://github.com/karpathy/paper-notes/raw/master/img/matching_networks/Screen%20Shot%202016-08-08%20at%2010.27.46%20AM.png" alt="Screen Shot 2016-08-08 at 10.27.46 AM"></p><p>Matching nets do best. Fully Conditional Embeddings (FCE) by which I mean they the “Full Context Embeddings” of Section 2.1.2 instead are not used here, mentioned to not work much better. Finetuning helps a bit on baselines but not with Matching nets (weird).</p><p>The comparisons in this table are somewhat confusing:</p><ul><li>I can’t find the MANN numbers of 82.8% and 94.9% in their paper [21]; not clear where they come from. E.g. for 5 classes and 5-shot they seem to report 88.4% not 94.9% as seen here. I must be missing something.</li><li>I also can’t find the numbers reported here in the Siamese Net [11] paper. As far as I can tell in their Table 2 they report one-shot accuracy, 20-way classification to be 92.0, while here it is listed as 88.1%?</li><li>The results of Lake et al. [14] who proposed Omniglot are also missing from the table. If I’m understanding this correctly they report 95.2% on 1-shot 20-way, while matching nets here show 93.8%, and humans are estimated at 95.5%. That is, the results here appear weaker than those of Lake et al., but one should keep in mind that the method here is significantly more generic and does not make any assumptions about the existence of strokes, etc., and it’s a simple, single fully-differentiable blob of neural stuff.</li></ul><p>(skipping ImageNet/LM experiments as there are few surprises)</p><h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>Good paper, effectively develops a differentiable nearest neighbor trained end-to-end. It’s something new, I like it!</p><p>A few concerns:</p><ul><li><p>A bidirectional LSTMs (not order-invariant compute) is applied over sets of training examples to encode them. The authors don’t talk about the order actually used, which presumably is random, or mention this potentially unsatisfying feature. This can be solved by using a recurrent attentional mechanism instead, as the authors are certainly aware of and as has been discussed at length in <a href="https://arxiv.org/abs/1511.06391" target="_blank" rel="external">ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS</a>, where Oriol is also the first author. I wish there was a comment on this point in the paper somewhere.</p></li><li><p>The approach also gets quite a bit slower as the number of training examples grow, but once this number is large one would presumable switch over to a parameteric approach.</p></li><li><p>It’s also potentially concerning that during training the method uses a specific number of examples, e.g. 5-25, so this is the number of that must also be used at test time. What happens if we want the size of our training set to grow online? It appears that we need to retrain the network because the encoder LSTM for the training data is not “used to” seeing inputs of more examples? That is unless you fall back to iteratively subsampling the training data, doing multiple inference passes and averaging, or something like that. If we don’t use FCE it can still be that the attention mechanism LSTM can still not be “used to” attending over many more examples, but it’s not clear how much this matters. An interesting experiment would be to not use FCE and try to use 100 or 1000 training examples, while only training on up to 25 (with and fithout FCE). Discussion surrounding this point would be interesting.</p></li><li><p>Not clear what happened with the Omniglot experiments, with incorrect numbers for [11], [21], and the exclusion of Lake et al. [14] comparison.</p></li><li><p>A baseline that is missing would in my opinion also include training of an <a href="https://www.cs.cmu.edu/~tmalisie/projects/iccv11/" target="_blank" rel="external">Exemplar SVM</a>, which is a much more powerful approach than encode-with-a-cnn-and-nearest-neighbor.</p><p>​</p></li></ul></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/One-Shot-Learning-and-Siamese-Networks-in-Keras/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/06/02/One-Shot-Learning-and-Siamese-Networks-in-Keras/" itemprop="url">One Shot Learning and Siamese Networks in Keras</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-02T22:21:29+08:00">2018-06-02 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/06/02/One-Shot-Learning-and-Siamese-Networks-in-Keras/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/06/02/One-Shot-Learning-and-Siamese-Networks-in-Keras/" itemprop="commentsCount"></span> </a></span><span id="/2018/06/02/One-Shot-Learning-and-Siamese-Networks-in-Keras/" class="leancloud_visitors" data-flag-title="One Shot Learning and Siamese Networks in Keras"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Background"><a href="#Background" class="headerlink" title="Background:"></a>Background:</h2><p>Conventional wisdom says that deep neural networks are really good at learning from high dimensional data like images or spoken language, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of <em>one-shot learning</em> - if you take a human who’s never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with astoundingly high precision.</p><p><a href="https://sorenbouma.github.io/images/spatula.jpg" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/spatula.jpg" alt="image"></a></p><p>Never been inside a kitchen before? Now’s your chance to test your one shot learning ability! which of the images on the right is of the same type as the big image? Email me for the correct answer.</p><p>..Yet another one of the <a href="https://dspace.mit.edu/handle/1721.1/6125" target="_blank" rel="external">things</a> humans can do that seemed trivial to us right up until we tried to make an algorithm do it.</p><p>This ability to rapidly learn from very little data seems like it’s obviously desirable for machine learning systems to have because collecting and labelling data is expensive. I also think this is an important step on the long road towards general intelligence.</p><p>Recently there have been <a href="https://arxiv.org/abs/1703.07326" target="_blank" rel="external">many</a> <a href="https://sorenbouma.github.io/blog/oneshot/%22%22" target="_blank" rel="external">interesting</a> <a href="https://sorenbouma.github.io/blog/oneshot/%22%22" target="_blank" rel="external">papers</a> about one-shot learning with neural nets and they’ve gotten some good results. This is a new area that really excites me, so I wanted to make a gentle introduction to make it more accessible to fellow newcomers to deep learning.</p><p>In this post, I want to:</p><ul><li>Introduce and formulate the problem of one-shot learning</li><li>Describe benchmarks for one-shot classification and give a baseline for performance</li><li>Give an example of deep one-shot learning by partially reimplementing the model in <a href="http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" target="_blank" rel="external">this paper</a> with keras.</li><li>Hopefully point out some small insights that aren’t obvious to everyone</li></ul><h2 id="Formulating-the-Problem-N-way-One-Shot-Learning"><a href="#Formulating-the-Problem-N-way-One-Shot-Learning" class="headerlink" title="Formulating the Problem - N-way One-Shot Learning"></a>Formulating the Problem - N-way One-Shot Learning</h2><p>Before we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:</p><p>Our model is given a tiny labelled training set SS, which has N examples, each vectors of the same dimension with a distinct label yy.<br>$$<br>S={(x_1,y_1),…,(x_N,y_N)}<br>$$<br>It is also given $\hat{x}$, the test example it has to classify. Since exactly one example in the support set has the right class, the aim is to correctly predict which $y \in S$ is the same as $\hat{x}$ ‘s label, $\hat{y}$.</p><p>There are fancier ways of defining the problem, but this one is ours. Here are some things to make note of:</p><ul><li>Real world problems might not always have the constraint that exactly one image has the correct class</li><li>It’s easy to generalize this to k-shot learning by having there be k examples for each yiyirather than just one.</li><li>When N is higher, there are more possible classes that $\hat{x}$ can belong to, so it’s harder to predict the correct one.</li><li>Random guessing will average $\frac{100}{n}\%$ accuracy.</li></ul><p>Here are some examples of one-shot learning tasks on the Omniglot dataset, which I’ll describe in the next section.</p><p><a href="https://sorenbouma.github.io/images/task_9.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/task_9.png" alt="image"></a><a href="https://sorenbouma.github.io/images/task_25.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/task_25.png" alt="image"></a><a href="hhttps://sorenbouma.github.io/images/task_36.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/task_36.png" alt="image"></a>9, 25 and 36 way one-shot learnng tasks.</p><h2 id="About-the-data-Omniglot"><a href="#About-the-data-Omniglot" class="headerlink" title="About the data - Omniglot! :"></a>About the data - Omniglot! :</h2><p>The <a href="https://github.com/brendenlake/omniglot" target="_blank" rel="external">Omniglot dataset</a> is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are just 20 examples, each drawn by a different person at resolution 105x105.</p><p><a href="https://sorenbouma.github.io/images/alphabets/Braille.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Braille.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Bengali.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Bengali.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Sanskrit.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Sanskrit.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Greek.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Greek.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Futurama.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Futurama.png" alt="image"></a><a href="https://sorenbouma.github.io/images/alphabets/Hebrew.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/alphabets/Hebrew.png" alt="image"></a>A few of the alphabets from the omniglot dataset. As you can see, there’s a huge variety of different symbols.</p><p>If you like machine learning, you’ve probably heard of the <a href="https://en.wikipedia.org/wiki/MNIST_database" target="_blank" rel="external">MNIST dataset</a>. Omniglot is sometimes referred to as the <em>transpose</em> of mnist, since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits. There is also data about the strokes used to create each character, but we won’t be using that. Usually, it’s split into 30 training alphabets and 20 evaluation alphabets. All those different characters make for lots of possible one-shot tasks, so it’s a really good benchmark for one-shot learning algorithms.</p><h4 id="A-One-Shot-Learning-Baseline-1-Nearest-Neighbour"><a href="#A-One-Shot-Learning-Baseline-1-Nearest-Neighbour" class="headerlink" title="A One-Shot Learning Baseline / 1 Nearest Neighbour"></a>A One-Shot Learning Baseline / 1 Nearest Neighbour</h4><p>The simplest way of doing classification is with k-nearest neighbours, but since there is only one example per class we have to do 1 nearest neighbour. This is very simple, just calculate the Euclidean distance of the test example from each training example and pick the closest one:<br>$$<br>C(\hat{x})={\arg \min}_{c\in S}||\hat{x} − x_c||<br>$$<br>According to Koch et al, 1-nn gets ~28% accuracy in 20 way one shot classification on omniglot. 28% doesn’t sound great, but it’s nearly six times more accurate than random guessing(5%). This is a good baseline or “sanity check” to compare future one-shot algorithms with.</p><p><a href="http://cims.nyu.edu/~brenden/LakeEtAlNips2013.pdf" target="_blank" rel="external">Hierarchical Bayesian Program Learning</a> from Lake et al gets 95.2% - very impressive! The ~30% of this paper which I understood was very interesting. Comparing it with deep learning results that train on raw pixels is kind of “apples and oranges” though, because:</p><ol><li>HBPL used data about the strokes, not just the raw pixels</li><li>HBPL on omniglot involved learning a generative model for strokes. The algorithm requires data with more complicated annotation, so unlike deep learning it can’t easily be tweaked to one-shot learn from raw pixels of dogs/trucks/brain scans/spatulas and other objects that aren’t made up of brushstrokes.</li></ol><p>Lake et al also says that humans get 95.5% accuracy in 20 way classification on omniglot, only beating HBPL by a tiny margin. In the spirit of nullius in verba, I tried testing myself on the 20 way tasks and managed to average 97.2%. I wasn’t always doing true one-shot learning though - I saw several symbols I recognised, since I’m familiar with the greek alphabet, hiragana and katakana. I removed those alphabets and tried again but still managed 96.7%. My hypothesis is that having to read my own terrible handwriting has endowed me with superhuman symbol recognition ability.</p><h4 id="Ways-to-use-deep-networks-for-one-shot-learning"><a href="#Ways-to-use-deep-networks-for-one-shot-learning" class="headerlink" title="Ways to use deep networks for one shot learning?!"></a>Ways to use deep networks for one shot learning?!</h4><p>If we naively train a neural network on a one-shot as a vanilla cross-entropy-loss softmax classifier, it will <em>severely</em> overfit. Heck, even if it was a <em>hundred</em> shot learning a modern neural net would still probably overfit. Big neural networks have millions of parameters to adjust to their data and so they can learn a huge space of possible functions. (More formally, they have a high <a href="https://en.wikipedia.org/wiki/VC_dimension" target="_blank" rel="external">VC dimension</a>, which is part of why they do so well at learning from complex data with high dimensionality.) Unfortunately this strength also appears to be their undoing for one-shot learning. When there are millions of parameters to gradient descend upon, and a staggeringly huge number of possible mappings that can be learned, how can we make a network learn one that generalizes when there’s just a single example to learn from?</p><p>It’s easier for humans to one-shot learn the concept of a spatula or the letter ΘΘ because they have spent a lifetime observing and learning from similar objects. It’s not really fair to compare the performance of a human who’s spent a lifetime having to classify objects and symbols with that of a randomly initialized neural net, which imposes a very weak prior about the structure of the mapping to be learned from the data. This is why most of the one-shot learning papers I’ve seen take the approach of <em>knowledge transfer</em> from other tasks.</p><p>Neural nets are really good at extracting useful features from structurally complex/high dimensional data, such as images. If a neural network is given training data that is similar to (but not the same as) that in the one-shot task, it might be able to learn useful features which can be used in a simple learning algorithm that doesn’t require adjusting these parameters. It still counts as one-shot learning as long as the training examples are of different classes to the examples used for one-shot testing.</p><p>(NOTE: Here a <em>feature</em> means a “transformation of the data that is useful for learning”.)</p><p>So now an interesting problem is <em>how do we get a neural network to learn the features?</em> The most obvious way of doing this (if there’s labelled data) is just vanilla transfer learning - train a softmax classifier on the training set, then fine-tune the weights of the last layer on the support set of the one-shot task. In practice, neural net classifiers don’t work too well for data like omniglot where there are few examples per class, and even fine tuning only the weights in the last layer is enough to overfit the support set. Still works quite a lot better than L2 distance nearest neighbour though! (See <a href="https://arxiv.org/pdf/1606.04080" target="_blank" rel="external">Matching Networks for One Shot learning</a> for a comparison table of various deep one-shot learning methods and their accuracy.)</p><p>There’s a better way of doing it though! Remember 1 nearest neighbour? This simple, non-parametric one-shot learner just classifies the test example with the same class of whatever support example is the closest in L2 distance. This works ok, but L2 Distance suffers from the ominous sounding <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="external">curse of dimensionality</a> and so won’t work well for data with thousands of dimensions like omniglot. Also, if you have two nearly identical images and move one over a few pixels to the right the L2 distance can go from being almost zero to being really high. L2 distance is a metric that is just woefully inadequate for this task. Deep learning to the rescue? We can use a deep convolutional network to learn some kind of similarity function that a non-parametric classifer like nearest neighbor can use.</p><h3 id="Siamese-networks"><a href="#Siamese-networks" class="headerlink" title="Siamese networks"></a>Siamese networks</h3><p><a href="https://sorenbouma.github.io/images/cats.jpg" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/cats.jpg" alt="image"></a>I originally planned to have craniopagus conjoined twins as the accompanying image for this section but ultimately decided that siamese cats would go over better..</p><p><a href="http://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" target="_blank" rel="external">This wonderful paper</a> is what I will be implementing in this tutorial. Koch et al’s approach to getting a neural net to do one-shot classification is to give it two images and train it to guess whether they have the same category. Then when doing a one-shot classification task described above, the network can compare the test image to each image in the support set, and pick which one it thinks is most likely to be of the same category. So we want a neural net architecture that takes two images as input and outputs the probability they share the same class.</p><p>Say x1 and x2 are two images in our dataset, and let x1∘x2 mean “x1 and x2 are images with the same class”. Note that x1∘x2 is the same as x2∘x1 - this means that if we reverse the order of the inputs to the neural network, the output should be the same - p(x1∘x2) should equal p(x2∘x1). This property is called <em>symmetry</em> and siamese nets are designed around having it.</p><p>Symmetry is important because it’s required for learning a distance metric - the distance from x1 to x2 should equal the distance x2 to x1.</p><p>If we just concatenate two examples together and use them as a single input to a neural net, each example will be matrix multiplied(or convolved) with a different set of weights, which breaks symmetry. Sure it’s possible it will eventually manage to learn the exact same weights for each input, but it would be much easier to learn a single set of weights applied to both inputs. So we could propagate both inputs through identical twin neural nets with shared parameters, then use the absolute difference as the input to a linear classifier - this is essentially what a siamese net is. Two identical twins, joined at the head, hence the name.</p><h4 id="Network-architecture"><a href="#Network-architecture" class="headerlink" title="Network architecture"></a>Network architecture</h4><p><em>Unfortunately, properly explaining how and why a convolutional neural net work would make this post twice as long. If you want to understand convnets work, I suggest checking out cs231n and then colah. For any non-dl people who are reading this, the best summary I can give of a CNN is this: An image is a 3D array of pixels. A convolutional layer is where you have a neuron connected to a tiny subgrid of pixels or neurons, and use copies of that neuron across all parts of the image/block to make another 3d array of neuron activations. A max pooling layer makes a block of activations spatially smaller. Lots of these stacked on top of one another can be trained with gradient descent and are really good at learning from images.</em></p><p>I’m going to describe the architecture pretty briefly because it’s not the important part of the paper. Koch et al uses a <em>convolutional</em> siamese network to classify pairs of omniglot images, so the twin networks are both convolutional neural nets(CNNs). The twins each have the following architecture: convolution with 64 10x10 filters, relu -&gt; max pool -&gt; convolution with 128 7x7 filters, relu -&gt; max pool -&gt; convolution with 128 4x4 filters, relu -&gt; max pool -&gt; convolution with 256 4x4 filters. The twin networks reduce their inputs down to smaller and smaller 3d tensors, finally their is a fully connected layer with 4096 units. The absolute difference between the two vectors is used as input to a linear classifier. All up, the network has 38,951,745 parameters - 96% of which belong to the fully connected layer. This is quite a lot, so the network has high capacity to overfit, but as I show below, pairwse training means the dataset size is huge so this won’t be a problem.</p><p><a href="https://sorenbouma.github.io/images/Siamese_diagram_2.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/Siamese_diagram_2.png" alt="image"></a>Hastily made architecture diagram.</p><p>The output is squashed into [0,1] with a sigmoid function to make it a probability. We use the target t=1t=1 when the images have the same class and t=0t=0 for a different class. It’s trained with logistic regression. This means the loss function should be binary cross entropy between the predictions and targets. There is also a L2 weight decay term in the loss to encourage the network to learn smaller/less noisy weights and possibly improve generalization:<br>$$<br>L(x_1,x_2,t)=t⋅\log(p(x_1∘x_2))+(1−t)⋅\log(1−p(x_1∘x_2))+λ⋅||w||^2<br>$$<br>When it does a one-shot task, the siamese net simply classifies the test image as whatever image in the support set it thinks is most similar to the test image:<br>$$<br>C(\hat{x},S) = {\arg\max}_c P(\hat{x}∘x_c),x_c∈S<br>$$<br>This uses an argmax unlike nearest neighbour which uses an argmin, because a <em>metric</em> like L2 is higher the more “different” the examples are, but this models outputs p(x1∘x2), so we want the highest. This approach has one flaw that’s obvious to me: for any xaxa in the support set,the probability $\hat{x}∘x_a$ is independent of every other example in the support set! This means the probabilities won’t sum to 1, ignores important information, namely that the test image will be the same type as exactly <em>one</em> x∈S…</p><h4 id="Observation-effective-dataset-size-in-pairwise-training"><a href="#Observation-effective-dataset-size-in-pairwise-training" class="headerlink" title="Observation: effective dataset size in pairwise training"></a>Observation: effective dataset size in pairwise training</h4><p>EDIT: After discussing this with a PhD student at UoA, I think this bit might be overstated or even just wrong. Emperically, my implementation <em>did</em> overfit, even though it wasn’t trained for enough iterations to sample every possible pair, which kind of contradicts this section. I’m leaving it up in the spirit of being wrong loudly.</p><p>One cool thing I noticed about training on pairs is that there are quadratically many possible pairs of images to train the model on, making it hard to overfit. Say we have CC examples each of EE classes. Since there are C⋅EC⋅E images total, the total number of possible pairs is given by<br>$$<br>Npairs=(C⋅E 2)=(C⋅E)!/2!(C⋅E−2)!<br>$$<br>For omniglot with its 20 examples of 964 training classes, this leads to 185,849,560 possible pairs, which is huge! However, the siamese network needs examples of both same and different class pairs. There are E examples per class, so there will be (E 2) pairs for every class, which means there are Nsame=(E 2)⋅C possible pairs with the same class - 183,160 pairs for omniglot. Even though 183,160 example pairs is plenty, it’s only a thousandth of the possible pairs, and the number of same-class pairs increases quadratically with E but only linearly with C. This is important because the siamese network should be given a 1:1 ratio of same-class and different-class pairs to train on - perhaps it implies that pairwise training is easier on datasets with lots of examples per class.</p><h3 id="The-Code"><a href="#The-Code" class="headerlink" title="The Code:"></a>The Code:</h3><p><a href="https://github.com/sorenbouma/keras-oneshot" target="_blank" rel="external">Prefer to just play with a jupyter notebook? I got you fam</a></p><p>Here is the model definition, it should be pretty easy to follow if you’ve seen keras before. I only define the twin network’s architecture once as a Sequential() model and then call it with respect to each of two input layers, this way the same parameters are used for both inputs. Then merge them together with absolute distance and add an output layer, and compile the model with binary cross entropy loss.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, Sequential</div><div class="line"><span class="keyword">from</span> keras.regularizers <span class="keyword">import</span> l2</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> SGD,Adam</div><div class="line"><span class="keyword">from</span> keras.losses <span class="keyword">import</span> binary_crossentropy</div><div class="line"><span class="keyword">import</span> numpy.random <span class="keyword">as</span> rng</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> dill <span class="keyword">as</span> pickle</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">W_init</span><span class="params">(shape,name=None)</span>:</span></div><div class="line">    <span class="string">"""Initialize weights as in paper"""</span></div><div class="line">    values = rng.normal(loc=<span class="number">0</span>,scale=<span class="number">1e-2</span>,size=shape)</div><div class="line">    <span class="keyword">return</span> K.variable(values,name=name)</div><div class="line"><span class="comment">#//<span class="doctag">TODO:</span> figure out how to initialize layer biases in keras.</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">b_init</span><span class="params">(shape,name=None)</span>:</span></div><div class="line">    <span class="string">"""Initialize bias as in paper"""</span></div><div class="line">    values=rng.normal(loc=<span class="number">0.5</span>,scale=<span class="number">1e-2</span>,size=shape)</div><div class="line">    <span class="keyword">return</span> K.variable(values,name=name)</div><div class="line"></div><div class="line">input_shape = (<span class="number">105</span>, <span class="number">105</span>, <span class="number">1</span>)</div><div class="line">left_input = Input(input_shape)</div><div class="line">right_input = Input(input_shape)</div><div class="line"><span class="comment">#build convnet to use in each siamese 'leg'</span></div><div class="line">convnet = Sequential()</div><div class="line">convnet.add(Conv2D(<span class="number">64</span>,(<span class="number">10</span>,<span class="number">10</span>),activation=<span class="string">'relu'</span>,input_shape=input_shape,</div><div class="line">                   kernel_initializer=W_init,kernel_regularizer=l2(<span class="number">2e-4</span>)))</div><div class="line">convnet.add(MaxPooling2D())</div><div class="line">convnet.add(Conv2D(<span class="number">128</span>,(<span class="number">7</span>,<span class="number">7</span>),activation=<span class="string">'relu'</span>,</div><div class="line">                   kernel_regularizer=l2(<span class="number">2e-4</span>),kernel_initializer=W_init,bias_initializer=b_init))</div><div class="line">convnet.add(MaxPooling2D())</div><div class="line">convnet.add(Conv2D(<span class="number">128</span>,(<span class="number">4</span>,<span class="number">4</span>),activation=<span class="string">'relu'</span>,kernel_initializer=W_init,kernel_regularizer=l2(<span class="number">2e-4</span>),bias_initializer=b_init))</div><div class="line">convnet.add(MaxPooling2D())</div><div class="line">convnet.add(Conv2D(<span class="number">256</span>,(<span class="number">4</span>,<span class="number">4</span>),activation=<span class="string">'relu'</span>,kernel_initializer=W_init,kernel_regularizer=l2(<span class="number">2e-4</span>),bias_initializer=b_init))</div><div class="line">convnet.add(Flatten())</div><div class="line">convnet.add(Dense(<span class="number">4096</span>,activation=<span class="string">"sigmoid"</span>,kernel_regularizer=l2(<span class="number">1e-3</span>),kernel_initializer=W_init,bias_initializer=b_init))</div><div class="line"><span class="comment">#encode each of the two inputs into a vector with the convnet</span></div><div class="line">encoded_l = convnet(left_input)</div><div class="line">encoded_r = convnet(right_input)</div><div class="line"><span class="comment">#merge two encoded inputs with the l1 distance between them</span></div><div class="line">L1_distance = <span class="keyword">lambda</span> x: K.abs(x[<span class="number">0</span>]-x[<span class="number">1</span>])</div><div class="line">both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</div><div class="line">prediction = Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>,bias_initializer=b_init)(both)</div><div class="line">siamese_net = Model(input=[left_input,right_input],output=prediction)</div><div class="line"><span class="comment">#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)</span></div><div class="line"></div><div class="line">optimizer = Adam(<span class="number">0.00006</span>)</div><div class="line"><span class="comment">#//<span class="doctag">TODO:</span> get layerwise learning rates and momentum annealing scheme described in paperworking</span></div><div class="line">siamese_net.compile(loss=<span class="string">"binary_crossentropy"</span>,optimizer=optimizer)</div><div class="line"></div><div class="line">siamese_net.count_params()</div></pre></td></tr></table></figure><p>The original paper used layerwise learning rates and momentum - I skipped this because it; was kind of messy to implement in keras and the hyperparameters aren’t the interesting part of the paper. Koch et al adds examples to the dataset by distorting the images and runs experiments with a fixed training set of up to 150,000 pairs. Since that won’t fit in my computers memory, I decided to just randomly sample pairs. Loading image pairs was probably the hardest part of this to implement. Since there were 20 examples for every class, I reshaped the data into N_classes x 20 x 105 x 105 arrays, to make it easier to index by category.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Siamese_Loader</span>:</span></div><div class="line">    <span class="string">"""For loading batches and testing tasks to a siamese net"""</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,Xtrain,Xval)</span>:</span></div><div class="line">        self.Xval = Xval</div><div class="line">        self.Xtrain = Xtrain</div><div class="line">        self.n_classes,self.n_examples,self.w,self.h = Xtrain.shape</div><div class="line">        self.n_val,self.n_ex_val,_,_ = Xval.shape</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_batch</span><span class="params">(self,n)</span>:</span></div><div class="line">        <span class="string">"""Create batch of n pairs, half same class, half different class"""</span></div><div class="line">        categories = rng.choice(self.n_classes,size=(n,),replace=<span class="keyword">False</span>)</div><div class="line">        pairs=[np.zeros((n, self.h, self.w,<span class="number">1</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>)]</div><div class="line">        targets=np.zeros((n,))</div><div class="line">        targets[n//<span class="number">2</span>:] = <span class="number">1</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</div><div class="line">            category = categories[i]</div><div class="line">            idx_1 = rng.randint(<span class="number">0</span>,self.n_examples)</div><div class="line">            pairs[<span class="number">0</span>][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,<span class="number">1</span>)</div><div class="line">            idx_2 = rng.randint(<span class="number">0</span>,self.n_examples)</div><div class="line">            <span class="comment">#pick images of same class for 1st half, different for 2nd</span></div><div class="line">            category_2 = category <span class="keyword">if</span> i &gt;= n//<span class="number">2</span> <span class="keyword">else</span> (category + rng.randint(<span class="number">1</span>,self.n_classes)) % self.n_classes</div><div class="line">            pairs[<span class="number">1</span>][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,<span class="number">1</span>)</div><div class="line">        <span class="keyword">return</span> pairs, targets</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_oneshot_task</span><span class="params">(self,N)</span>:</span></div><div class="line">        <span class="string">"""Create pairs of test image, support set for testing N way one-shot learning. """</span></div><div class="line">        categories = rng.choice(self.n_val,size=(N,),replace=<span class="keyword">False</span>)</div><div class="line">        indices = rng.randint(<span class="number">0</span>,self.n_ex_val,size=(N,))</div><div class="line">        true_category = categories[<span class="number">0</span>]</div><div class="line">        ex1, ex2 = rng.choice(self.n_examples,replace=<span class="keyword">False</span>,size=(<span class="number">2</span>,))</div><div class="line">        test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,<span class="number">1</span>)</div><div class="line">        support_set = self.Xval[categories,indices,:,:]</div><div class="line">        support_set[<span class="number">0</span>,:,:] = self.Xval[true_category,ex2]</div><div class="line">        support_set = support_set.reshape(N,self.w,self.h,<span class="number">1</span>)</div><div class="line">        pairs = [test_image,support_set]</div><div class="line">        targets = np.zeros((N,))</div><div class="line">        targets[<span class="number">0</span>] = <span class="number">1</span></div><div class="line">        <span class="keyword">return</span> pairs, targets</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test_oneshot</span><span class="params">(self,model,N,k,verbose=<span class="number">0</span>)</span>:</span></div><div class="line">        <span class="string">"""Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks"""</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line">        n_correct = <span class="number">0</span></div><div class="line">        <span class="keyword">if</span> verbose:</div><div class="line">            print(<span class="string">"Evaluating model on &#123;&#125; unique &#123;&#125; way one-shot learning tasks ..."</span>.format(k,N))</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">            inputs, targets = self.make_oneshot_task(N)</div><div class="line">            probs = model.predict(inputs)</div><div class="line">            <span class="keyword">if</span> np.argmax(probs) == <span class="number">0</span>:</div><div class="line">                n_correct+=<span class="number">1</span></div><div class="line">        percent_correct = (<span class="number">100.0</span>*n_correct / k)</div><div class="line">        <span class="keyword">if</span> verbose:</div><div class="line">            print(<span class="string">"Got an average of &#123;&#125;% &#123;&#125; way one-shot learning accuracy"</span>.format(percent_correct,N))</div><div class="line">        <span class="keyword">return</span> percent_correct</div></pre></td></tr></table></figure><p>..And now the training loop. Nothing unusual here, except for that I monitor one-shot tasks validation accuracy to test performance, rather than loss on the validation set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">evaluate_every = <span class="number">7000</span></div><div class="line">loss_every=<span class="number">300</span></div><div class="line">batch_size = <span class="number">32</span></div><div class="line">N_way = <span class="number">20</span></div><div class="line">n_val = <span class="number">550</span></div><div class="line">siamese_net.load_weights(<span class="string">"PATH"</span>)</div><div class="line">best = <span class="number">76.0</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">900000</span>):</div><div class="line">    (inputs,targets)=loader.get_batch(batch_size)</div><div class="line">    loss=siamese_net.train_on_batch(inputs,targets)</div><div class="line">    <span class="keyword">if</span> i % evaluate_every == <span class="number">0</span>:</div><div class="line">        val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">if</span> val_acc &gt;= best:</div><div class="line">            print(<span class="string">"saving"</span>)</div><div class="line">            siamese_net.save(<span class="string">'PATH'</span>)</div><div class="line">            best=val_acc</div><div class="line"></div><div class="line">    <span class="keyword">if</span> i % loss_every == <span class="number">0</span>:</div><div class="line">        print(<span class="string">"iteration &#123;&#125;, training loss: &#123;:.2f&#125;,"</span>.format(i,loss))</div></pre></td></tr></table></figure><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>Once the learning curve flattened out, I used the weights which got the best validation 20 way accuracy for testing. My network averaged ~83% accuracy for tasks from the evaluation set, compared to 93% in the original paper. Probably this difference is because I didn’t implement many of the performance enhancing tricks from the original paper, like layerwise learning rates/momentum, data augmentation with distortions, bayesian hyperparemeter optimization and I also probably trained for less epochs. I’m not too worried about this because this tutorial was more about introducing one-shot learning in general, than squeezing the last few % performance out of a classifier. There is no shortage of resources on that!</p><p>I was curious to see how accuracy varied over different values of “N” in N way one shot learning, so I plotted it, with comparisons to 1 nearest neighbours, random guessing and training set performance.</p><p><a href="https://sorenbouma.github.io/images/results1.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/results1.png" alt="image"></a>results.</p><p>As you can see, it performs worse on tasks from the validaiton set than the train set, especially for high values of N, so there must be overfitting. It would be interesting to see how well traditional regularization methods like dropout work when the validation set is made of completely different classes to the training set. It works better than I expected for large N, still averaging above 65% accuracy for 50-60 way tasks.</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><p>We’ve just trained a neural network trained to do same-different pairwise classification on symbols. More importantly, we’ve shown that it can then get reasonable accuracy in 20 way one-shot learning on symbols from unseen alphabets. Of course, this is not the only way to use deep networks for one-shot learning.</p><p>As I touched on earlier, I think a major flaw of this siamese approach is that it only compares the test image to every support image individualy, when it should be comparing it to the support set as a whole. When the network compares the test image to any image x1, p(x^∘x1) is the same no matter what else is the support set. This is silly. Say you’re doing a one-shot task and you see an image that looks similar to the test image. You should be much less confident they have the same class if there is another image in the support set that also looks similar to the test image. The training objective is different to the test objective. It might work better to have a model that can compare the test image to the support set as a whole and use the constraint that only one support image has the same class.</p><p><a href="https://arxiv.org/pdf/1606.04080.pdf" target="_blank" rel="external">Matching Networks for One Shot learning</a> does exactly that. Rather than learning a similarity function, they have a deep model learn a full nearest neighbour classifier end to end, training directly on oneshot tasks rather than on image pairs. <a href="https://github.com/karpathy/paper-notes/blob/master/matching_networks.md" target="_blank" rel="external">Andrej Karpathy’s notes</a> explain it much better than I can. Since you are learning a machine classifier, this can be seen as a kind of <em>meta-learning</em>. <a href="https://arxiv.org/pdf/1605.06065.pdf" target="_blank" rel="external">One-shot Learning with Memory-Augmented Neural Networks</a> explores the connection between one-shot learning and meta learning and trains a memory augmented network on omniglot, though I confess I had trouble understanding this paper.</p><h3 id="What-next"><a href="#What-next" class="headerlink" title="What next?"></a>What next?</h3><p>The omniglot dataset has been around since 2015, and already there are scalable ML algorithms getting within the ballpark of human level performance on certain one-shot learning tasks. Hopefully one day it will be seen as a mere “sanity check” for one-shot classification algorithms much like MNIST is for supervised learning now.</p><p>Image classification is cool but I don’t think it’s the most interesting problem in machine learning. Now that we know deep one-shot learning can work pretty good, I think it would be cool to see attempts at one-shot learning for other, more exotic tasks.</p><p>Ideas from one-shot learning could be used for more sample efficient reinforcement learning, especially for problems like OpenAI’s Universe, where there are lots of MDPs/environments that have similar visual features and dynamics. - It would be cool to have an RL agent that could efficiently explore a new environment after learning in similar MDPs.</p><p><a href="https://sorenbouma.github.io/images/worldofbits.png" target="_blank" rel="external"><img src="https://sorenbouma.github.io/images/worldofbits.jpg" alt="image"></a>OpenAI’s world of bits environments.</p><p><a href="https://arxiv.org/abs/1703.07326" target="_blank" rel="external">One-shot Imitation learning</a> is one of my favourite one-shot learning papers. The goal is to have an agent learn a robust policy for solving a task from a single human demonstration of that task.This is done by:</p><ol><li>Having a neural net map from the current state and a sequence of states(the human demonstration) to an action</li><li>Training it on pairs of human demonstrations on slightly different variants of the same task, with the goal of reproducing the second demonstration based on the first.</li></ol><p>This strikes me as a really promising path to one day having broadly applicable, learning based robots!</p><p>Bringing one-shot learning to NLP tasks is a cool idea too. <em>Matching Networks for One-Shot learning</em> has an attempt at one-shot language modeling, filling a missing word in a test sentence given a small set of support sentences, and it seems to work pretty well. Exciting!</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>Anyway, thanks for reading! I hope you’ve managed to one-shot learn the concept of one-shot learning :) If not, I’d love to hear feedback or answer any questions you have!</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/17/Anaconda-uses-socket-proxy-on-Windows-10/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/05/17/Anaconda-uses-socket-proxy-on-Windows-10/" itemprop="url">Anaconda uses socket proxy on Windows 10</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-17T17:54:53+08:00">2018-05-17 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/05/17/Anaconda-uses-socket-proxy-on-Windows-10/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/05/17/Anaconda-uses-socket-proxy-on-Windows-10/" itemprop="commentsCount"></span> </a></span><span id="/2018/05/17/Anaconda-uses-socket-proxy-on-Windows-10/" class="leancloud_visitors" data-flag-title="Anaconda uses socket proxy on Windows 10"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>you need to create a <strong>.condarc</strong> file in you Windows user area:</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">C:\Users\&lt;username&gt;\</div></pre></td></tr></table></figure><p>The file should contain (if you are using shadowsocks):</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">channels:</div><div class="line">- defaults</div><div class="line"></div><div class="line"><span class="comment"># Show channel URLs when displaying what is going to be downloaded and</span></div><div class="line"><span class="comment"># in 'conda list'. The default is False.</span></div><div class="line">show_channel_urls: True</div><div class="line">allow_other_channels: True</div><div class="line"></div><div class="line">proxy_servers:</div><div class="line">    http: socks5://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">1080</span></div><div class="line">    https: socks5://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">1080</span></div><div class="line"></div><div class="line">ssl_verify: False</div></pre></td></tr></table></figure><p>Noticed that you cannot create a file that begins with a dot in Windows directly.</p><p>To <strong>create/rename on windows explorer</strong>, just rename to <code>.name.</code> - The additional dot at the end is necessary, and will be removed by Windows Explorer.</p><p>To create a new file begins with a dot, on command prompt:</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo testing &gt; .name</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">131</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">64</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/tomaxent" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>