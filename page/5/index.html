<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/5/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/5/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/03/Paper-Note-Selective-Search-for-Object-Recognition/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/05/03/Paper-Note-Selective-Search-for-Object-Recognition/" itemprop="url">Paper Note: Selective Search for Object Recognition</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-03T13:59:00+08:00">2018-05-03 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/05/03/Paper-Note-Selective-Search-for-Object-Recognition/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/05/03/Paper-Note-Selective-Search-for-Object-Recognition/" itemprop="commentsCount"></span> </a></span><span id="/2018/05/03/Paper-Note-Selective-Search-for-Object-Recognition/" class="leancloud_visitors" data-flag-title="Paper Note: Selective Search for Object Recognition"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>与 Selective Search 初次见面是在著名的物体检测论文 <code>Rich feature hierarchies for accurate object detection and semantic segmentation</code> ，因此，这篇论文算是阅读 R-CNN 的准备。</p><p>这篇论文的标题虽然也提到了 Object Recognition ，但就创新点而言，其实在 Selective Search 。所以，这里只简单介绍 Selective Search 的思想和算法过程，对于 Object Recognition 则不再赘述。</p><h3 id="什么是-Selective-Search"><a href="#什么是-Selective-Search" class="headerlink" title="什么是 Selective Search"></a>什么是 Selective Search</h3><p>Selective Search，说的简单点，就是从图片中找出物体可能存在的区域。</p><p><a href="http://jermmy.xyz/images/2017-5-4/result.png" target="_blank" rel="external"><img src="http://jermmy.xyz/images/2017-5-4/result.png" alt="result"></a>result</p><p>上面这幅宇航员的图片中，那些红色的框就是 Selective Search 找出来的可能存在物体的区域。</p><p>在进一步探讨它的原理之前，我们分析一下，如何判别哪些 region 属于一个物体？</p><p><a href="http://jermmy.xyz/images/2017-5-4/image%20seg.png" target="_blank" rel="external"><img src="http://jermmy.xyz/images/2017-5-4/image%20seg.png" alt="image seg"></a>image seg</p><p>作者在论文中用以上四幅图，分别描述了四种可能的情况：</p><ol><li>图 a ，物体之间可能存在层级关系，比如：碗里有个勺；</li><li>图 b，我们可以用颜色来分开两只猫，却没法用纹理来区分；</li><li>图 c，我们可以用纹理来区分变色龙，却没法用颜色来区分；</li><li>图 d，轮胎是车的一部分，不是因为它们颜色相近、纹理相近，而是因为轮胎包含在车上。</li></ol><p>所以，我们没法用单一的特征来定位物体，需要综合考虑多种策略，这一点是 Selective Search 精要所在。</p><h3 id="需要考虑的问题"><a href="#需要考虑的问题" class="headerlink" title="需要考虑的问题"></a>需要考虑的问题</h3><p>在学习 Selective Search 算法之前，我曾在计算机视觉课上学到过关于物体（主要是人脸）检测的方法。通常来说，最常规也是最简单粗暴的方法，就是用不同尺寸的矩形框，一行一行地扫描整张图像，通过提取矩形框内的特征判断是否是待检测物体。这种方法的复杂度极高，所以又被称为 <strong>exhaustive search</strong>。在人脸识别中，由于使用了 Haar 特征，因此可以借助 <strong>Paul Viola</strong> 和 <strong>Michael Jones</strong> 两位大牛提出的积分图，使检测在常规时间内完成。但并不是每种特征都适用于积分图，尤其在神经网络中，积分图这种动态规划的思路就没什么作用了。</p><p>针对传统方法的不足，Selective Search 从三个角度提出了改进：</p><ol><li>我们没法事先得知物体的大小，在传统方法中需要用不同尺寸的矩形框检测物体，防止遗漏。而 Selective Search 采用了一种具备层次结构的算法来解决这个问题；</li><li>检测的时间复杂度可能会很高。Selective Search 遵循简单即是美的原则，只负责快速地生成可能是物体的区域，而不做具体的检测；</li><li>另外，结合上一节提出的，采用多种先验知识来对各个区域进行简单的判别，避免一些无用的搜索，提高速度和精度。</li></ol><h3 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h3><p><a href="http://jermmy.xyz/images/2017-5-4/algorithm.png" target="_blank" rel="external"><img src="http://jermmy.xyz/images/2017-5-4/algorithm.png" alt="algorithm"></a>algorithm</p><p>论文中给出的这个算法框架还是很详细的，这里再简单翻译一下。</p><ul><li>输入：彩色图片。</li><li>输出：物体可能的位置，实际上是很多的矩形坐标。</li><li>首先，我们使用这篇<a href="http://cs.brown.edu/~pff/segment/" target="_blank" rel="external">论文</a>的方法将图片初始化为很多小区域 $R=r_i, \cdots, r_n$。由于我们的重点是 Selective Search，因此我直接将该论文的算法当成一个黑盒子。</li><li>初始化一个相似集合为空集： $S=∅$。</li><li>计算所有相邻区域之间的相似度（相似度函数之后会重点分析），放入集合 S 中，集合 S 保存的其实是一个<strong>区域对</strong>以及它们之间的相似度。</li><li>找出 S 中相似度最高的区域对，将它们合并，并从 S 中删除与它们相关的所有相似度和区域对。重新计算这个新区域与周围区域的相似度，放入集合 S 中，并将这个新合并的区域放入集合 R 中。重复这个步骤直到 S 为空。</li><li>从 R 中找出所有区域的 bounding box（即包围该区域的最小矩形框），这些 box 就是物体可能的区域。</li></ul><p>另外，为了提高速度，新合并区域的 feature 可以通过之前的两个区域获得，而不必重新遍历新区域的像素点进行计算。这个 feature 会被用于计算相似度。</p><h3 id="相似度计算方法"><a href="#相似度计算方法" class="headerlink" title="相似度计算方法"></a>相似度计算方法</h3><p>相似度计算方法将直接影响合并区域的顺序，进而影响到检测结果的好坏。</p><p>论文中比较了八种颜色空间的特点，在实际操作中，只选择一个颜色空间（比如：RGB 空间）进行计算。</p><p>正如一开始提出的那样，我们需要综合多种信息来判断。作者将相似度度量公式分为四个子公式，称为<strong>互补相似度测量(Complementary Similarity Measures)</strong> 。这四个子公式的值都被归一化到区间 [0, 1] 内。</p><h4 id="1-颜色相似度scolor-ri-rj-scolor-ri-rj"><a href="#1-颜色相似度scolor-ri-rj-scolor-ri-rj" class="headerlink" title="1. 颜色相似度scolor (ri,rj)scolor (ri,rj)"></a>1. 颜色相似度scolor (ri,rj)scolor (ri,rj)</h4><p>正如本文一开始提到的，颜色是一个很重要的区分物体的因素。论文中将每个 region 的像素按不同颜色通道统计成直方图，其中，每个颜色通道的直方图为 25 bins （比如，对于 0 ～ 255 的颜色通道来说，就每隔 9(255/25=9) 个数值统计像素数量）。这样，三个通道可以得到一个 75 维的直方图向量 $C_i={c_{i}^{1}, …, c_{i}^{n}}$，其中 n = 75。之后，我们用 <strong>L1 范数</strong>（绝对值之和）对直方图进行归一化。由直方图我们就可以计算两个区域的颜色相似度：<br>$$<br>s_{color}(r_i, r_j) =\sum_{k=1}^{n}{min(c_{i}^{k}, c_{j}^{k})}<br>$$<br>这个颜色直方图可以在合并区域的时候，很方便地传递给下一级区域。即它们合并后的区域的直方图向量为：<br>$$<br>C_t=\frac{size(r_i)<em>C_i+size(r_j)</em>C_j}{size(r_i)+size(r_j)}<br>$$<br>，其中$size(r_i)$ 表示区域 $r_i$ 的面积，合并后的区域为 $size(r_t)=size(r_i)+size(r_j)$。</p><h4 id="2-纹理相似度-s-texture-r-i-r-j"><a href="#2-纹理相似度-s-texture-r-i-r-j" class="headerlink" title="2. 纹理相似度$s_{texture}(r_i,r_j)$"></a>2. 纹理相似度$s_{texture}(r_i,r_j)$</h4><p>另一个需要考虑的因素是纹理，即图像的梯度信息。</p><p>论文中对纹理的计算采用了 SIFT-like 特征，该特征借鉴了 SIFT 的计算思路，对每个颜色通道的像素点，沿周围 8 个方向计算高斯一阶导数(σ=1σ=1)，每个方向统计一个直方图（bin = 10），这样，一个颜色通道统计得到的直方图向量为 80 维，三个通道就是 240 维：$T_i={t_i^{(1)}, …, t_i^{(n)}}$，其中 n = 240。注意这个直方图要用 <strong>L1 范数</strong>归一化。然后，我们按照颜色相似度的计算思路计算两个区域的纹理相似度：<br>$$<br>s_{texture}(r_i, r_j) =\sum_{k=1}^{n}{min(t_{i}^{k}, t_{j}^{k})}<br>$$</p><h4 id="3-尺寸相似度-s-size-r-i-r-j"><a href="#3-尺寸相似度-s-size-r-i-r-j" class="headerlink" title="3. 尺寸相似度$s_{size} (r_i,r_j)$"></a>3. 尺寸相似度$s_{size} (r_i,r_j)$</h4><p>在合并区域的时候，论文优先考虑小区域的合并，这种做法可以在一定程度上保证每次合并的区域面积都比较相似，防止大区域对小区域的逐步蚕食。这么做的理由也很简单，我们要均匀地在图片的每个角落生成不同尺寸的区域，作用相当于 <strong>exhaustive search</strong> 中用不同尺寸的矩形扫描图片。具体的相似度计算公式为：<br>$$<br>s_{size}(r_i, r_j)=1-\frac{size(r_i) + size(r_j)}{size(im)}<br>$$<br>其中，$size(im)$ 表示原图片的像素数量。</p><h4 id="4-填充相似度-s-fill-r-i-r-j"><a href="#4-填充相似度-s-fill-r-i-r-j" class="headerlink" title="4. 填充相似度$s_{fill}(r_i,r_j)$"></a>4. 填充相似度$s_{fill}(r_i,r_j)$</h4><p>填充相似度主要用来测量两个区域之间 fit 的程度，个人觉得这一点是要解决文章最开始提出的物体之间的包含关系（比如：轮胎包含在汽车上）。在给出填充相似度的公式前，我们需要定义一个矩形区域 $BB_{ij}$，它表示包含 $r_i$ 和 $r_j$ 的最小的 bounding box。基于此，我们给出相似度计算公式为：<br>$$<br>s_{fill}(r_i, r_j)=1-\frac{size(BB_{ij})-size(r_i)-size(r_j)}{size(im)}<br>$$<br>为了高效地计算 $BB_{ij}$，我们可以在计算每个 region 的时候，都保存它们的 bounding box 的位置，这样，$BB_{ij}$ 就可以很快地由两个区域的 bounding box 推出来。</p><h4 id="5-相似度计算公式"><a href="#5-相似度计算公式" class="headerlink" title="5. 相似度计算公式"></a>5. 相似度计算公式</h4><p>综合上面四个子公式，我们可以得到计算相似度的最终公式：<br>$$<br>s(r_i, r_j) = a_1 s_{color}(r_i, r_j) +a_2s_{texture}(r_i, r_j) \\\\ +a_3s_{size}(r_i, r_j)+a_4s_{fill}(r_i, r_j)<br>$$<br>其中，$a_i$的取值为 0 或 1，表示某个相似度是否被采纳。</p><h3 id="Combining-Locations"><a href="#Combining-Locations" class="headerlink" title="Combining Locations"></a>Combining Locations</h3><p>前面我们基本完成了 Selective Search 的流程，从图片中提取出了物体可能的位置。现在，我们想完善最后一个问题，那就是给这些位置排个序。因为提取出来的矩形框数量巨大，而用户可能只需要其中的几个，这个时候我们就很有必要对这些矩形框赋予优先级，按照优先级高低返回给用户。原文中作者称这一步为 <strong>Combining Locations</strong>，我找不出合适的翻译，就姑且保留英文原文。</p><p>这个排序的方法也很简单。作者先给各个 region 一个序号，前面说了，Selective Search 是一个逐步合并的层级结构，因此，我们将覆盖整个区域的 region 的序号标记为 1，合成这个区域的两个子区域的序号为 2，以此类推。但如果仅按序号排序，会存在一个漏洞，那就是区域面积大的会排在前面，为了避免这个漏洞，作者又在每个序号前乘上一个随机数 $RND∈[0,1]$，通过这个新计算出来的数值，按从小到大的顺序得出 region 最终的排序结果。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="http://blog.csdn.net/langb2014/article/details/52575507" target="_blank" rel="external">Selective Search for Object Recognition(阅读)</a></li><li><a href="http://cs.brown.edu/~pff/segment/" target="_blank" rel="external">Efficient Graph-Based Image Segmentation</a></li></ul><blockquote><p><strong>本文作者：</strong> Jermmy</p><p><strong>本文链接：</strong> <a href="https://jermmy.github.io/2017/05/04/2017-5-4-paper-notes-selective-search/" target="_blank" rel="external">https://jermmy.github.io/2017/05/04/2017-5-4-paper-notes-selective-search/</a></p><p><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank" rel="external">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</p></blockquote></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/" itemprop="url">Micro- and Macro-average of Precision, Recall and F-Score</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-27T14:36:15+08:00">2018-04-27 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/" itemprop="commentsCount"></span> </a></span><span id="/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/" class="leancloud_visitors" data-flag-title="Micro- and Macro-average of Precision, Recall and F-Score"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="Micro-average-Method"><a href="#Micro-average-Method" class="headerlink" title="Micro-average Method"></a><strong>Micro-average Method</strong></h3><p>In Micro-average method, you sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics. For example, for a set of data, the system’s</p><p>True positive (TP1)= 12<br>False positive (FP1)=9<br>False negative (FN1)=3</p><p>Then precision (P1) and recall (R1) will be 57.14 and 80</p><p>and for a different set of data, the system’s</p><p>True positive (TP2)= 50<br>False positive (FP2)=23<br>False negative (FN2)=9</p><p>Then precision (P2) and recall (R2) will be 68.49 and 84.75</p><p>Now, the average precision and recall of the system using the Micro-average method is</p><p>Micro-average of precision = (TP1+TP2)/(TP1+TP2+FP1+FP2) = (12+50)/(12+50+9+23) = 65.96<br>Micro-average of recall = (TP1+TP2)/(TP1+TP2+FN1+FN2) = (12+50)/(12+50+3+9) = 83.78</p><p>The Micro-average F-Score will be simply the harmonic mean of these two figures.</p><h3 id="Macro-average-Method"><a href="#Macro-average-Method" class="headerlink" title="Macro-average Method"></a><strong>Macro-average Method</strong></h3><p>The method is straight forward. Just take the average of the precision and recall of the system on different sets. For example, the macro-average precision and recall of the system for the given example is</p><p>Macro-average precision = (P1+P2)/2 = (57.14+68.49)/2 = 62.82<br>Macro-average recall = (R1+R2)/2 = (80+84.75)/2 = 82.25</p><p>The Macro-average F-Score will be simply the harmonic mean of these two figures.</p><p>Suitability<br>Macro-average method can be used when you want to know how the system performs overall across the sets of data. You should not come up with any specific decision with this average.</p><p>On the other hand, micro-average can be a useful measure when your dataset varies in size.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/11/Get-financial-data-from-Tushare/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/04/11/Get-financial-data-from-Tushare/" itemprop="url">Get financial data from Tushare</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-11T19:42:46+08:00">2018-04-11 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/04/11/Get-financial-data-from-Tushare/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/04/11/Get-financial-data-from-Tushare/" itemprop="commentsCount"></span> </a></span><span id="/2018/04/11/Get-financial-data-from-Tushare/" class="leancloud_visitors" data-flag-title="Get financial data from Tushare"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>TuShare is a famous free, open source python financial data interface package. Its official home page is: <a href="http://tushare.waditu.com/" target="_blank" rel="external">TuShare - financial data interface package</a>. The interface package now provides a large amount of financial data covering a wide range of data such as stocks, fundamentals, macros, news, etc. (Please check the official website for details) and keep updating. At present, the length of the stock’s data is three years. Although it is a bit short, it can basically meet the needs of quantitative beginners for testing.</p><h1 id="Tutorial"><a href="#Tutorial" class="headerlink" title="Tutorial"></a>Tutorial</h1><h2 id="Install-and-Import"><a href="#Install-and-Import" class="headerlink" title="Install and Import"></a>Install and Import</h2><p><strong>You need to install first:</strong></p><ul><li>Pandas</li><li>lxml</li></ul><p><strong>Two way to install tushare:</strong></p><ol><li><code>pip install tushare</code></li><li>visit <a href="https://pypi.python.org/pypi/Tushare/" target="_blank" rel="external">https://pypi.python.org/pypi/Tushare/</a>, download and install</li></ol><p><strong>How to update:</strong></p><p><code>pip install tushare --upgrade</code></p><p><strong>Import package and view package version:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tushare</div><div class="line"></div><div class="line">print(tushare.__version__)</div></pre></td></tr></table></figure><h2 id="Use-some-simple-function"><a href="#Use-some-simple-function" class="headerlink" title="Use some simple function"></a>Use some simple function</h2><h3 id="Stock-data"><a href="#Stock-data" class="headerlink" title="Stock data"></a>Stock data</h3><p><strong>update：Many of the quotes returned by the <code>get_hist_data</code> function are wrong, but both <code>get_h_data</code> and <code>get_k_data</code> can be used</strong></p><p>We should still master how to use <code>tushare</code> to obtain stock market data, using the <code>ts.get_hist_data()</code> function whose <strong>input parameters</strong> are:</p><ul><li><strong>code: </strong>Stock code, ie 6-digit code, or index code (sh = Shanghai index sz = Shenzhen index hs300 = CSI 300 index sz50 = SSE 50 zxb = small and medium board cyb = board)</li><li><strong>start: </strong>Start date, format YYYY-MM-DD</li><li><strong>end: </strong>End date, format YYYY-MM-DD</li><li><strong>ktype: </strong>Data type, D = day k line W = week M = month 5 = 5 minutes 15 = 15 minutes 30 = 30 minutes 60 = 60 minutes, the default is D</li><li><strong>retry_count: </strong>The number of retries after the network is abnormal. The default is 3</li><li><strong>pause: </strong>Pause seconds when retrying, default is 0</li></ul><p><strong>Return values:</strong></p><ul><li><strong>date</strong>：date</li><li><strong>open</strong>：Opening price</li><li><strong>high</strong>：Highest price</li><li><strong>close</strong>：Closing price</li><li><strong>low</strong>：Lowest price</li><li><strong>volume</strong>：Volume</li><li><strong>price_change</strong>：price fluncuation</li><li><strong>p_change</strong>：Quote change</li><li><strong>ma5</strong>：5-day average price</li><li><strong>ma10</strong>：10-day average price</li><li><strong>ma20</strong>: 20-day average price</li><li><strong>v_ma5</strong>: 5-day average volume</li><li><strong>v_ma10</strong>: 10-day average volume</li><li><strong>v_ma20</strong>: 20-day average volume</li><li><strong>turnover</strong>: Change in hand rate [Note: Index does not have this item]</li></ul><h4 id="Specific-examples"><a href="#Specific-examples" class="headerlink" title="Specific examples:"></a>Specific examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">ts.get_hist_data(<span class="string">'600848'</span>)</div><div class="line"></div><div class="line"> date       open    high   close     low     volume    p_change   ma5    </div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-11</span>   <span class="number">6.880</span>   <span class="number">7.380</span>   <span class="number">7.060</span>   <span class="number">6.880</span>   <span class="number">14129.96</span>     <span class="number">2.62</span>   <span class="number">7.060</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-12</span>   <span class="number">7.050</span>   <span class="number">7.100</span>   <span class="number">6.980</span>   <span class="number">6.900</span>    <span class="number">7895.19</span>    <span class="number">-1.13</span>   <span class="number">7.020</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-13</span>   <span class="number">6.950</span>   <span class="number">7.000</span>   <span class="number">6.700</span>   <span class="number">6.690</span>    <span class="number">6611.87</span>    <span class="number">-4.01</span>   <span class="number">6.913</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-16</span>   <span class="number">6.680</span>   <span class="number">6.750</span>   <span class="number">6.510</span>   <span class="number">6.480</span>    <span class="number">2941.63</span>    <span class="number">-2.84</span>   <span class="number">6.813</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-17</span>   <span class="number">6.660</span>   <span class="number">6.880</span>   <span class="number">6.860</span>   <span class="number">6.460</span>    <span class="number">8642.57</span>     <span class="number">5.38</span>   <span class="number">6.822</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-18</span>   <span class="number">7.000</span>   <span class="number">7.300</span>   <span class="number">6.890</span>   <span class="number">6.880</span>   <span class="number">13075.40</span>     <span class="number">0.44</span>   <span class="number">6.788</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-19</span>   <span class="number">6.690</span>   <span class="number">6.950</span>   <span class="number">6.890</span>   <span class="number">6.680</span>    <span class="number">6117.32</span>     <span class="number">0.00</span>   <span class="number">6.770</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-20</span>   <span class="number">6.870</span>   <span class="number">7.080</span>   <span class="number">7.010</span>   <span class="number">6.870</span>    <span class="number">6813.09</span>     <span class="number">1.74</span>   <span class="number">6.832</span></div><div class="line"></div><div class="line">date         ma10    ma20      v_ma5     v_ma10     v_ma20     turnover</div><div class="line"></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-11</span>   <span class="number">7.060</span>   <span class="number">7.060</span>   <span class="number">14129.96</span>   <span class="number">14129.96</span>   <span class="number">14129.96</span>     <span class="number">0.48</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-12</span>   <span class="number">7.020</span>   <span class="number">7.020</span>   <span class="number">11012.58</span>   <span class="number">11012.58</span>   <span class="number">11012.58</span>     <span class="number">0.27</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-13</span>   <span class="number">6.913</span>   <span class="number">6.913</span>    <span class="number">9545.67</span>    <span class="number">9545.67</span>    <span class="number">9545.67</span>     <span class="number">0.23</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-16</span>   <span class="number">6.813</span>   <span class="number">6.813</span>    <span class="number">7894.66</span>    <span class="number">7894.66</span>    <span class="number">7894.66</span>     <span class="number">0.10</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-17</span>   <span class="number">6.822</span>   <span class="number">6.822</span>    <span class="number">8044.24</span>    <span class="number">8044.24</span>    <span class="number">8044.24</span>     <span class="number">0.30</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-18</span>   <span class="number">6.833</span>   <span class="number">6.833</span>    <span class="number">7833.33</span>    <span class="number">8882.77</span>    <span class="number">8882.77</span>     <span class="number">0.45</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-19</span>   <span class="number">6.841</span>   <span class="number">6.841</span>    <span class="number">7477.76</span>    <span class="number">8487.71</span>    <span class="number">8487.71</span>     <span class="number">0.21</span></div><div class="line"><span class="number">2012</span><span class="number">-01</span><span class="number">-20</span>   <span class="number">6.863</span>   <span class="number">6.863</span>    <span class="number">7518.00</span>    <span class="number">8278.38</span>    <span class="number">8278.38</span>     <span class="number">0.23</span></div></pre></td></tr></table></figure><p>You can also set the start time and end time of historical data:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">ts.get_hist_data(<span class="string">'600848'</span>,start=<span class="string">'2015-01-05'</span>,end=<span class="string">'2015-01-09'</span>)</div><div class="line"></div><div class="line"> date       open    high   close     low    volume   p_change   ma5    ma10</div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-05</span>  <span class="number">11.160</span>  <span class="number">11.390</span>  <span class="number">11.260</span>  <span class="number">10.890</span>  <span class="number">46383.57</span>     <span class="number">1.26</span>  <span class="number">11.156</span>  <span class="number">11.212</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-06</span>  <span class="number">11.130</span>  <span class="number">11.660</span>  <span class="number">11.610</span>  <span class="number">11.030</span>  <span class="number">59199.93</span>     <span class="number">3.11</span>  <span class="number">11.182</span>  <span class="number">11.155</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-07</span>  <span class="number">11.580</span>  <span class="number">11.990</span>  <span class="number">11.920</span>  <span class="number">11.480</span>  <span class="number">86681.38</span>     <span class="number">2.67</span>  <span class="number">11.366</span>  <span class="number">11.251</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-08</span>  <span class="number">11.700</span>  <span class="number">11.920</span>  <span class="number">11.670</span>  <span class="number">11.640</span>  <span class="number">56845.71</span>    <span class="number">-2.10</span>  <span class="number">11.516</span>  <span class="number">11.349</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-09</span>  <span class="number">11.680</span>  <span class="number">11.710</span>  <span class="number">11.230</span>  <span class="number">11.190</span>  <span class="number">44851.56</span>    <span class="number">-3.77</span>  <span class="number">11.538</span>  <span class="number">11.363</span></div><div class="line"> date        ma20     v_ma5    v_ma10     v_ma20      turnover</div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-05</span>  <span class="number">11.198</span>  <span class="number">58648.75</span>  <span class="number">68429.87</span>   <span class="number">97141.81</span>     <span class="number">1.59</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-06</span>  <span class="number">11.382</span>  <span class="number">54854.38</span>  <span class="number">63401.05</span>   <span class="number">98686.98</span>     <span class="number">2.03</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-07</span>  <span class="number">11.543</span>  <span class="number">55049.74</span>  <span class="number">61628.07</span>  <span class="number">103010.58</span>     <span class="number">2.97</span></div><div class="line"><span class="number">2015</span><span class="number">-01</span><span class="number">-08</span>  <span class="number">11.647</span>  <span class="number">57268.99</span>  <span class="number">61376.00</span>  <span class="number">105823.50</span>     <span class="number">1.95</span></div></pre></td></tr></table></figure><p>Others:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'W'</span>) <span class="comment"># Get weekly k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'M'</span>) <span class="comment"># Get monthly k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'5'</span>) <span class="comment"># Get 5 minutes k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'15'</span>) <span class="comment"># Get 15 minutes k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'30'</span>) <span class="comment"># Get 30 minutes k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'600848'</span>, ktype=<span class="string">'60'</span>) <span class="comment"># Get 60 minutes k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'sh'</span>）<span class="comment"># Get data on the Shanghai index k-line, other parameters consistent with the stocks, the same below</span></div><div class="line">ts.get_hist_data(<span class="string">'sz'</span>）<span class="comment"># Get Shenzhen Chengzhi k line data</span></div><div class="line">ts.get_hist_data(<span class="string">'hs300'</span>）<span class="comment"># Get the CSI 300 k line data</span></div><div class="line">ts.get_hist_data(<span class="string">'sz50'</span>）<span class="comment"># Get SSE 50 Index k-line data</span></div><div class="line">ts.get_hist_data(<span class="string">'zxb'</span>）<span class="comment"># Get the k-line data of small and medium board indices</span></div><div class="line">ts.get_hist_data(<span class="string">'cyb'</span>）<span class="comment"># Get GEM Index k-line data</span></div></pre></td></tr></table></figure><h3 id="Fundamental-data"><a href="#Fundamental-data" class="headerlink" title="Fundamental data"></a>Fundamental data</h3><p>With <code>tushare</code> we can also get fundamental data through <code>ts.get_stock_basics()</code> (shown in the results section):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">ts.get_stock_basics()</div><div class="line"></div><div class="line">code    name      industry  area     pe     outstanding    totals   totalAssets                                                                   </div><div class="line"><span class="number">300563</span>    N神宇  通信设备   江苏    <span class="number">26.73</span>      <span class="number">2000.00</span>     <span class="number">8000.00</span>  <span class="number">4.216000e+04</span>   </div><div class="line"><span class="number">601882</span>   海天精工     机床制造   浙江    <span class="number">26.83</span>      <span class="number">5220.00</span>    <span class="number">52200.00</span>  <span class="number">1.877284e+05</span> </div><div class="line"><span class="number">601880</span>    大连港       港口   辽宁    <span class="number">76.40</span>    <span class="number">773582.00</span>  <span class="number">1289453.63</span>  <span class="number">3.263012e+06</span>   </div><div class="line"><span class="number">300556</span>   丝路视觉     软件服务   深圳   <span class="number">101.38</span>      <span class="number">2780.00</span>    <span class="number">11113.33</span>  <span class="number">4.448248e+04</span> </div><div class="line"><span class="number">600528</span>   中铁二局     建筑施工   四川   <span class="number">149.34</span>    <span class="number">145920.00</span>   <span class="number">145920.00</span>  <span class="number">5.709568e+06</span> </div><div class="line"><span class="number">002495</span>   佳隆股份       食品   广东   <span class="number">202.12</span>     <span class="number">66611.13</span>    <span class="number">93562.56</span>  <span class="number">1.169174e+05</span> </div><div class="line"><span class="number">600917</span>   重庆燃气     供气供热   重庆    <span class="number">76.87</span>     <span class="number">15600.00</span>   <span class="number">155600.00</span>  <span class="number">8.444600e+05</span> </div><div class="line"><span class="number">002752</span>   昇兴股份     广告包装   福建    <span class="number">75.14</span>     <span class="number">12306.83</span>    <span class="number">63000.00</span>  <span class="number">2.387493e+05</span> </div><div class="line"><span class="number">002346</span>   柘中股份     电气设备   上海   <span class="number">643.97</span>      <span class="number">7980.00</span>    <span class="number">44157.53</span>  <span class="number">2.263010e+05</span> </div><div class="line"><span class="number">000680</span>   山推股份     工程机械   山东     <span class="number">0.00</span>    <span class="number">105694.97</span>   <span class="number">124078.75</span>  <span class="number">9.050701e+05</span></div><div class="line">...</div></pre></td></tr></table></figure><h3 id="Macro-data"><a href="#Macro-data" class="headerlink" title="Macro data"></a>Macro data</h3><p>We use the resident consumer index as an example, which can be obtained through the <code>ts.get_cpi()</code> function (it will get 322 items at a time, some of them will be displayed):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> ts.get_cpi()</div><div class="line"></div><div class="line">       month     cpi</div><div class="line"><span class="number">0</span>    <span class="number">2016.10</span>  <span class="number">102.10</span></div><div class="line"><span class="number">1</span>     <span class="number">2016.9</span>  <span class="number">101.90</span></div><div class="line"><span class="number">2</span>     <span class="number">2016.8</span>  <span class="number">101.34</span></div><div class="line"><span class="number">3</span>     <span class="number">2016.7</span>  <span class="number">101.77</span></div><div class="line"><span class="number">4</span>     <span class="number">2016.6</span>  <span class="number">101.88</span></div><div class="line"><span class="number">5</span>     <span class="number">2016.5</span>  <span class="number">102.04</span></div><div class="line"><span class="number">6</span>     <span class="number">2016.4</span>  <span class="number">102.33</span></div><div class="line"><span class="number">7</span>     <span class="number">2016.3</span>  <span class="number">102.30</span></div><div class="line"><span class="number">8</span>     <span class="number">2016.2</span>  <span class="number">102.28</span></div><div class="line"><span class="number">9</span>     <span class="number">2016.1</span>  <span class="number">101.75</span></div><div class="line"><span class="number">10</span>   <span class="number">2015.12</span>  <span class="number">101.64</span></div><div class="line">...</div></pre></td></tr></table></figure><h3 id="Recent-news"><a href="#Recent-news" class="headerlink" title="Recent news"></a>Recent news</h3><p>The <code>tushare</code> package can use the <code>ts.get_latest_news()</code> function to view the latest news, and it will return 80. For reasons of space, we only show the first 15 here. We can see that it is all Sina Finance’s news data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> ts.get_latest_news();</div><div class="line"></div><div class="line">   classify                         title         time  \</div><div class="line"><span class="number">0</span>        美股            “特朗普通胀”预期升温 美国国债下挫  <span class="number">11</span><span class="number">-14</span> <span class="number">23</span>:<span class="number">10</span>   </div><div class="line"><span class="number">1</span>        美股          特朗普：脸书、推特等社交媒体助我入主白宫  <span class="number">11</span><span class="number">-14</span> <span class="number">23</span>:<span class="number">10</span>   </div><div class="line"><span class="number">2</span>        证券                <span class="number">11</span>月<span class="number">14</span>日晚增减持每日速览  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">54</span>   </div><div class="line"><span class="number">3</span>        美股          财经观察：日本为何急于推动TPP批准程序  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">54</span>   </div><div class="line"><span class="number">4</span>        美股              新总统谜题：特朗普会连续加息吗？  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">52</span>   </div><div class="line"><span class="number">5</span>        证券      神州专车财报遭质疑 增发<span class="number">100</span>亿股东退出需<span class="number">50</span>年  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">41</span>   </div><div class="line"><span class="number">6</span>        证券           恒大闪电杀回马枪锁仓半年 戒短炒了吗？  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">38</span>   </div><div class="line"><span class="number">7</span>      国内财经         楼继伟力推改革做派 或加快国有资本划拨社保  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">36</span>   </div><div class="line"><span class="number">8</span>        美股            开盘：美股周一小幅高开 延续上周涨势  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">32</span>   </div><div class="line"><span class="number">9</span>        美股            喜达屋创始人：当好总统就要走中庸之道  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">24</span>   </div><div class="line"><span class="number">10</span>       证券              北京高华：将乐视网评级下调至中性  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">09</span>   </div><div class="line"><span class="number">11</span>       美股             <span class="number">11</span>月<span class="number">14</span>日<span class="number">22</span>点交易员正关注要闻  <span class="number">11</span><span class="number">-14</span> <span class="number">22</span>:<span class="number">02</span>   </div><div class="line"><span class="number">12</span>       美股           摩根大通：新兴市场股市、货币的前景悲观  <span class="number">11</span><span class="number">-14</span> <span class="number">21</span>:<span class="number">55</span>   </div><div class="line"><span class="number">13</span>     国内财经        人民日报刊文谈全面深化改革这三年：啃下硬骨头  <span class="number">11</span><span class="number">-14</span> <span class="number">21</span>:<span class="number">46</span>   </div><div class="line"><span class="number">14</span>       证券       泽平宏观：经济L型延续 地产销量回落投资超预期  <span class="number">11</span><span class="number">-14</span> <span class="number">21</span>:<span class="number">43</span>   </div><div class="line"><span class="number">15</span>       证券       黄燕铭等五大券商大佬告诉你 <span class="number">2017</span>年买点啥？  <span class="number">11</span><span class="number">-14</span> <span class="number">21</span>:<span class="number">41</span>   </div><div class="line"></div><div class="line">url  </div><div class="line"><span class="number">0</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">1</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">2</span>   http://finance.sina.com.cn/stock/y/<span class="number">2016</span><span class="number">-11</span><span class="number">-14</span>/...  </div><div class="line"><span class="number">3</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">4</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">5</span>   http://finance.sina.com.cn/stock/marketresearc...  </div><div class="line"><span class="number">6</span>   http://finance.sina.com.cn/stock/marketresearc...  </div><div class="line"><span class="number">7</span>   http://finance.sina.com.cn/china/gncj/<span class="number">2016</span><span class="number">-11</span>-...  </div><div class="line"><span class="number">8</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">9</span>   http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">10</span>  http://finance.sina.com.cn/stock/s/<span class="number">2016</span><span class="number">-11</span><span class="number">-14</span>/...  </div><div class="line"><span class="number">11</span>  http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">12</span>  http://finance.sina.com.cn/stock/usstock/c/<span class="number">201.</span>..  </div><div class="line"><span class="number">13</span>  http://finance.sina.com.cn/china/gncj/<span class="number">2016</span><span class="number">-11</span>-...  </div><div class="line"><span class="number">14</span>  http://finance.sina.com.cn/stock/marketresearc...  </div><div class="line"><span class="number">15</span>  http://finance.sina.com.cn/stock/marketresearc...</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/10/A-Brief-History-of-CNNs-in-Image-Segmentation-From-R-CNN-to-Mask-R-CNN/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/04/10/A-Brief-History-of-CNNs-in-Image-Segmentation-From-R-CNN-to-Mask-R-CNN/" itemprop="url">A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-10T16:34:47+08:00">2018-04-10 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/04/10/A-Brief-History-of-CNNs-in-Image-Segmentation-From-R-CNN-to-Mask-R-CNN/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/04/10/A-Brief-History-of-CNNs-in-Image-Segmentation-From-R-CNN-to-Mask-R-CNN/" itemprop="commentsCount"></span> </a></span><span id="/2018/04/10/A-Brief-History-of-CNNs-in-Image-Segmentation-From-R-CNN-to-Mask-R-CNN/" class="leancloud_visitors" data-flag-title="A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Ever since <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">Alex Krizhevsky, Geoff Hinton, and Ilya Sutskever won ImageNet in 2012</a>, Convolutional Neural Networks(CNNs) have become the gold standard for image classification. In fact, since then, CNNs have improved to the point where they now outperform humans on the ImageNet challenge!</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*bGTawFxQwzc5yV1_szDrwQ.png" alt="img"></p><p>CNNs now outperform humans on the ImageNet challenge. The y-axis in the above graph is the error rate on ImageNet.</p><p>While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*8GVucX9yhnL21KCtcyFDRQ.png" alt="img"></p><p>An example of an image used in the classification challenge. Note how the image is well framed and has just one object.</p><p>In classification, there’s generally an image with a single object as the focus and the task is to say what that image is (see above). But when we look at the world around us, we carry out far more complex tasks.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*eJjj2TVUVZDiVSTcnzh7fA.png" alt="img"></p><p>Sights in real life are often composed of a multitude of different, overlapping objects, backgrounds, and actions.</p><p>We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*NdwfHMrW3rpj5SW_VQtWVw.png" alt="img"></p><p>In image segmentation, our goal is to classify the different objects in the image, and identify their boundaries. Source: Mask R-CNN paper.</p><p>Can CNNs help us with such complex tasks? Namely, given a more complicated image, can we use CNNs to identify the different objects in the image, and their boundaries? As has been shown by Ross Girshick and his peers over the last few years, the answer is conclusively yes.</p><h4 id="Goals-of-this-Post"><a href="#Goals-of-this-Post" class="headerlink" title="Goals of this Post"></a>Goals of this Post</h4><p>Through this post, we’ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they’ve evolved from one implementation to the next. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Finally, we’ll cover Mask R-CNN, a paper released recently by Facebook Research that extends such object detection techniques to provide pixel level segmentation. Here are the papers referenced in this post:</p><ol><li>R-CNN: <a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">https://arxiv.org/abs/1311.2524</a></li><li>Fast R-CNN: <a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="external">https://arxiv.org/abs/1504.08083</a></li><li>Faster R-CNN: <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a></li><li>Mask R-CNN: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a></li></ol><hr><h4 id="2014-R-CNN-An-Early-Application-of-CNNs-to-Object-Detection"><a href="#2014-R-CNN-An-Early-Application-of-CNNs-to-Object-Detection" class="headerlink" title="2014: R-CNN - An Early Application of CNNs to Object Detection"></a>2014: R-CNN - An Early Application of CNNs to Object Detection</h4><p><img src="https://cdn-images-1.medium.com/max/1000/1*r9ELExnk1B1zHnRReDW9Ow.png" alt="img"></p><p>Object detection algorithms such as R-CNN take in an image and identify the locations and classifications of the main objects in the image. Source: <a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">https://arxiv.org/abs/1311.2524</a>.</p><p>Inspired by the research of Hinton’s lab at the University of Toronto, a small team at UC Berkeley, led by Professor Jitendra Malik, asked themselves what today seems like an inevitable question:</p><blockquote><p>To what extent do [Krizhevsky et. al’s results] generalize to object detection?</p></blockquote><p>Object detection is the task of finding the different objects in an image and classifying them (as seen in the image above). The team, comprised of Ross Girshick (a name we’ll see again), Jeff Donahue, and Trevor Darrel found that this problem can be solved with Krizhevsky’s results by testing on the PASCAL VOC Challenge, a popular object detection challenge akin to ImageNet. They write,</p><blockquote><p>This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features.</p></blockquote><p>Let’s now take a moment to understand how their architecture, Regions With CNNs (R-CNN) works.</p><p><strong>Understanding R-CNN</strong></p><p>The goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image.</p><ul><li><strong>Inputs</strong>: Image</li><li><strong>Outputs</strong>: Bounding boxes + labels for each object in the image.</li></ul><p>But how do we find out where these bounding boxes are? R-CNN does what we might intuitively do as well - <strong>propose</strong> <strong>a bunch of boxes in the image and see if any of them actually correspond to an object</strong>.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*ZQ03Ib84bYioFKoho5HnKg.png" alt="img"></p><p>Selective Search looks through windows of multiple scales and looks for adjacent pixels that share textures, colors, or intensities. Image source: <a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_blank" rel="external">https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf</a></p><p>R-CNN creates these bounding boxes, or region proposals, using a process called Selective Search which you can read about <a href="http://www.cs.cornell.edu/courses/cs7670/2014sp/slides/VisionSeminar14.pdf" target="_blank" rel="external">here</a>. At a high level, Selective Search (shown in the image above) looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.</p><p><img src="https://cdn-images-1.medium.com/max/1000/0*Sdj6sKDRQyZpO6oH." alt="img"></p><p>After creating a set of region proposals, R-CNN passes the image through a modified version of AlexNet to determine whether or not it is a valid region. Source: <a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">https://arxiv.org/abs/1311.2524</a>.</p><p>Once the proposals are created, R-CNN warps the region to a standard square size and passes it through to a modified version of AlexNet (the winning submission to ImageNet 2012 that inspired R-CNN), as shown above.</p><p>On the final layer of the CNN, R-CNN adds a Support Vector Machine (SVM) that simply classifies whether this is an object, and if so what object. This is step 4 in the image above.</p><p><strong>Improving the Bounding Boxes</strong></p><p>Now, having found the object in the box, can we tighten the box to fit the true dimensions of the object? We can, and this is the final step of R-CNN. R-CNN runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result. Here are the inputs and outputs of this regression model:</p><ul><li><strong>Inputs</strong>: sub-regions of the image corresponding to objects.</li><li><strong>Outputs</strong>: New bounding box coordinates for the object in the sub-region.</li></ul><p>So, to summarize, R-CNN is just the following steps:</p><ol><li>Generate a set of proposals for bounding boxes.</li><li>Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is.</li><li>Run the box through a linear regression model to output tighter coordinates for the box once the object has been classified.</li></ol><hr><h4 id="2015-Fast-R-CNN-Speeding-up-and-Simplifying-R-CNN"><a href="#2015-Fast-R-CNN-Speeding-up-and-Simplifying-R-CNN" class="headerlink" title="2015: Fast R-CNN - Speeding up and Simplifying R-CNN"></a>2015: Fast R-CNN - Speeding up and Simplifying R-CNN</h4><p><img src="https://cdn-images-1.medium.com/max/1000/1*3xnXHBEAz6FGzb-EehXtkA.png" alt="img"></p><p>Ross Girshick wrote both R-CNN and Fast R-CNN. He continues to push the boundaries of Computer Vision at Facebook Research.</p><p>R-CNN works really well, but is really quite slow for a few simple reasons:</p><ol><li>It requires a forward pass of the CNN (AlexNet) for every single region proposal for every single image (that’s around 2000 forward passes per image!).</li><li>It has to train three different models separately - the CNN to generate image features, the classifier that predicts the class, and the regression model to tighten the bounding boxes. This makes the pipeline extremely hard to train.</li></ol><p>In 2015, Ross Girshick, the first author of R-CNN, solved both these problems, leading to the second algorithm in our short history - Fast R-CNN. Let’s now go over its main insights.</p><p><strong>Fast R-CNN Insight 1: RoI (Region of Interest) Pooling</strong></p><p>For the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). His insight was simple — <strong>Why not run the CNN just once per image and then find a way to share that computation across the ~2000 proposals?</strong></p><p><img src="https://cdn-images-1.medium.com/max/1000/1*4K_Bq1AhAsTe9vlT0wsdXQ.png" alt="img"></p><p>In RoIPool, a full forward pass of the image is created and the conv features for each region of interest are extracted from the resulting forward pass. Source: Stanford’s CS231N slides by Fei Fei Li, Andrei Karpathy, and Justin Johnson.</p><p>This is exactly what Fast R-CNN does using a technique known as RoIPool (Region of Interest Pooling). At its core, RoIPool shares the forward pass of a CNN for an image across its subregions. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. Then, the features in each region are pooled (usually using max pooling). So all it takes us is one pass of the original image as opposed to ~2000!</p><p><strong>Fast R-CNN Insight 2: Combine All Models into One Network</strong></p><p><img src="https://cdn-images-1.medium.com/max/1000/1*E_P1vAEbGT4HNYjqMtIz4g.png" alt="img"></p><p>Fast R-CNN combined the CNN, classifier, and bounding box regressor into one, single network. Source: <a href="https://www.slideshare.net/simplyinsimple/detection-52781995" target="_blank" rel="external">https://www.slideshare.net/simplyinsimple/detection-52781995</a>.</p><p>The second insight of Fast R-CNN is to jointly train the CNN, classifier, and bounding box regressor in a single model. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), <strong>Fast R-CNN instead used a single network to compute all three.</strong></p><p>You can see how this was done in the image above. Fast R-CNN replaced the SVM classifier with a softmax layer on top of the CNN to output a classification. It also added a linear regression layer parallel to the softmax layer to output bounding box coordinates. In this way, all the outputs needed came from one single network! Here are the inputs and outputs to this overall model:</p><ul><li><strong>Inputs</strong>: Images with region proposals.</li><li><strong>Outputs</strong>: Object classifications of each region along with tighter bounding boxes.</li></ul><hr><h4 id="2016-Faster-R-CNN-Speeding-Up-Region-Proposal"><a href="#2016-Faster-R-CNN-Speeding-Up-Region-Proposal" class="headerlink" title="2016: Faster R-CNN - Speeding Up Region Proposal"></a>2016: Faster R-CNN - Speeding Up Region Proposal</h4><p>Even with all these advancements, there was still one remaining bottleneck in the Fast R-CNN process — the region proposer. As we saw, the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test. In Fast R-CNN, these proposals were created using <strong>Selective Search</strong>, a fairly slow process that was found to be the bottleneck of the overall process.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*xY9rmw06KZWQlNIPk6ItqA.png" alt="img"></p><p>Jian Sun, a principal researcher at Microsoft Research, led the team behind Faster R-CNN. Source: <a href="https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdp" target="_blank" rel="external">https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdp</a></p><p>In the middle 2015, a team at Microsoft Research composed of Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, found a way to make the region proposal step almost cost free through an architecture they (creatively) named Faster R-CNN.</p><p>The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). <strong>So why not reuse those same CNN results for region proposals instead of running a separate selective search algorithm?</strong></p><p><img src="https://cdn-images-1.medium.com/max/1000/0*_nNI03ESXm2P6YXO." alt="img"></p><p>In Faster R-CNN, a single CNN is used for region proposals, and classifications. Source: <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a>.</p><p>Indeed, this is just what the Faster R-CNN team achieved. In the image above, you can see how a single CNN is used to both carry out region proposals and classification. This way, <strong>only one CNN needs to be trained</strong> and we get region proposals almost for free! The authors write:</p><blockquote><p>Our observation is that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region proposals [thus enabling nearly cost-free region proposals].</p></blockquote><p>Here are the inputs and outputs of their model:</p><ul><li><strong>Inputs</strong>: Images (Notice how region proposals are not needed).</li><li><strong>Outputs</strong>: Classifications and bounding box coordinates of objects in the images.</li></ul><p><strong>How the Regions are Generated</strong></p><p>Let’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Faster R-CNN adds a Fully Convolutional Network on top of the features of the CNN creating what’s known as the <strong>Region Proposal Network</strong>.</p><p><img src="https://cdn-images-1.medium.com/max/1000/0*n6pZEyvW47nlcdQz." alt="img"></p><p>The Region Proposal Network slides a window over the features of the CNN. At each window location, the network outputs a score and a bounding box per anchor (hence 4k box coordinates where k is the number of anchors). Source: <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="external">https://arxiv.org/abs/1506.01497</a>.</p><p>The Region Proposal Network works by passing a sliding window over the CNN feature map and at each window, outputting <strong>k </strong>potential bounding boxes and scores for how good each of those boxes is expected to be. What do these <strong>k </strong>boxes represent?</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*pJ3OTVXjtp9vWfBOPsnWIw.png" alt="img"></p><p>We know that the bounding boxes for people tend to be rectangular and vertical. We can use this intuition to guide our Region Proposal networks through creating an anchor of such dimensions. Image Source: <a href="http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignments/assignment1/bbox0062.jpg" target="_blank" rel="external">http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignments/assignment1/bbox0062.jpg</a>.</p><p>Intuitively, we know that objects in an image should fit certain common aspect ratios and sizes. For instance, we know that we want some rectangular boxes that resemble the shapes of humans. Likewise, we know we won’t see many boxes that are very very thin. In such a way, we create <strong>k</strong> such common aspect ratios we call <strong>anchor boxes</strong>. For each such anchor box, we output one bounding box and score per position in the image.</p><p>With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:</p><ul><li><strong>Inputs</strong>: CNN Feature Map.</li><li><strong>Outputs</strong>: A bounding box per anchor. A score representing how likely the image in that bounding box will be an object.</li></ul><p>We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes.</p><hr><h4 id="2017-Mask-R-CNN-Extending-Faster-R-CNN-for-Pixel-Level-Segmentation"><a href="#2017-Mask-R-CNN-Extending-Faster-R-CNN-for-Pixel-Level-Segmentation" class="headerlink" title="2017: Mask R-CNN - Extending Faster R-CNN for Pixel Level Segmentation"></a>2017: Mask R-CNN - Extending Faster R-CNN for Pixel Level Segmentation</h4><p><img src="https://cdn-images-1.medium.com/max/1000/1*E_5qBTrotLzclyaxsekBmQ.png" alt="img"></p><p>The goal of image instance segmentation is to identify, at a pixel level, what the different objets in a scene are. Source: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a>.</p><p>So far, we’ve seen how we’ve been able to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes.</p><p>Can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes? This problem, known as image segmentation, is what Kaiming He and a team of researchers, including Girshick, explored at Facebook AI using an architecture known as <strong>Mask R-CNN</strong>.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*cYW3EdKx75Stl1EreATdfw.png" alt="img"></p><p>Kaiming He, a researcher at Facebook AI, is lead author of Mask R-CNN and also a coauthor of Faster R-CNN.</p><p>Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward. Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel level segmentation?</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*BiRpf-ogjxARQf5LxI17Jw.png" alt="img"></p><p>In Mask R-CNN, a Fully Convolutional Network (FCN) is added on top of the CNN features of Faster R-CNN to generate a mask (segmentation output). Notice how this is in parallel to the classification and bounding box regression network of Faster R-CNN. Source: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a>.</p><p>Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch (in white in the above image), as before, is just a Fully Convolutional Network on top of a CNN based feature map. Here are its inputs and outputs:</p><ul><li><strong>Inputs</strong>: CNN Feature Map.</li><li><strong>Outputs</strong>: Matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a <a href="https://en.wikipedia.org/wiki/Mask_%28computing%29" target="_blank" rel="external">binary mask</a>).</li></ul><p>But the Mask R-CNN authors had to make one small adjustment to make this pipeline work as expected.</p><p><strong>RoiAlign - Realigning RoIPool to be More Accurate</strong></p><p><img src="https://cdn-images-1.medium.com/max/1000/0*KtaZfpUErYqwH4RX." alt="img"></p><p>Instead of RoIPool, the image gets passed through RoIAlign so that the regions of the feature map selected by RoIPool correspond more precisely to the regions of the original image. This is needed because pixel level segmentation requires more fine-grained alignment than bounding boxes. Source: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a>.</p><p>When run without modifications on the original Faster R-CNN architecture, the Mask R-CNN authors realized that the regions of the feature map selected by RoIPool were slightly misaligned from the regions of the original image. Since image segmentation requires pixel level specificity, unlike bounding boxes, this naturally led to inaccuracies.</p><p>The authors were able to solve this problem by cleverly adjusting RoIPool to be more precisely aligned using a method known as RoIAlign.</p><p><img src="https://cdn-images-1.medium.com/max/1000/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg" alt="img"></p><p>How do we accurately map a region of interest from the original image onto the feature map?</p><p>Imagine we have an image of size <strong>128x128</strong> and a feature map of size <strong>25x25</strong>. Let’s imagine we want features the region corresponding to the top-left <strong>15x15</strong>pixels in the original image (see above). How might we select these pixels from the feature map?</p><p>We know each pixel in the original image corresponds to ~ 25/128 pixels in the feature map. To select 15 pixels from the original image, we just select 15 <em>25/128 ~= <em>*2.93</em></em> pixels.</p><p>In RoIPool, we would round this down and select 2 pixels causing a slight misalignment. However, in RoIAlign, <strong>we avoid such rounding.</strong> Instead, we use <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation" target="_blank" rel="external">bilinear interpolation</a> to get a precise idea of what would be at pixel 2.93. This, at a high level, is what allows us to avoid the misalignments caused by RoIPool.</p><p>Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:</p><p><img src="https://cdn-images-1.medium.com/max/1250/1*6CClgIKH8zhZjmcftfNoEQ.png" alt="img"></p><p>Mask R-CNN is able to segment as well as classify the objects in an image. Source: <a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="external">https://arxiv.org/abs/1703.06870</a>.</p><hr><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p>If you’re interested in trying out these algorithms yourselves, here are relevant repositories:</p><p><strong>Faster R-CNN</strong></p><ul><li>Caffe: <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external">https://github.com/rbgirshick/py-faster-rcnn</a></li><li>PyTorch: <a href="https://github.com/longcw/faster_rcnn_pytorch" target="_blank" rel="external">https://github.com/longcw/faster_rcnn_pytorch</a></li><li>MatLab: <a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="external">https://github.com/ShaoqingRen/faster_rcnn</a></li></ul><p><strong>Mask R-CNN</strong></p><ul><li>PyTorch: <a href="https://github.com/felixgwu/mask_rcnn_pytorch" target="_blank" rel="external">https://github.com/felixgwu/mask_rcnn_pytorch</a></li><li>TensorFlow: <a href="https://github.com/CharlesShang/FastMaskRCNN" target="_blank" rel="external">https://github.com/CharlesShang/FastMaskRCNN</a></li></ul><p>Reblog from <a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" target="_blank" rel="external">here</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/" itemprop="url">Some Paper Summaries of Semantic Segmentation with Deep Learning</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-10T20:01:58+08:00">2018-04-10 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2018/04/10/Some-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning/" class="leancloud_visitors" data-flag-title="Some Paper Summaries of Semantic Segmentation with Deep Learning"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h3 id="What-exactly-is-semantic-segmentation"><a href="#What-exactly-is-semantic-segmentation" class="headerlink" title="What exactly is semantic segmentation?"></a>What exactly is semantic segmentation?</h3><p>Semantic segmentation is understanding an image at pixel level i.e, we want to assign each pixel in the image an object class. For example, check out the following images.</p><p><img src="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21.jpg" alt="biker"> <img src="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/images/21_class.png" alt="biker"><br><em>Left</em>: Input image. <em>Right</em>: It’s semantic segmentation. <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/segexamples/index.html" target="_blank" rel="external">Source.</a></p><p>Apart from recognizing the bike and the person riding it, we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models.</p><p><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" target="_blank" rel="external">VOC2012</a> and <a href="http://mscoco.org/explore/" target="_blank" rel="external">MSCOCO</a> are the most important datasets for semantic segmentation.</p><h3 id="What-are-the-different-approaches"><a href="#What-are-the-different-approaches" class="headerlink" title="What are the different approaches?"></a>What are the different approaches?</h3><p>Before deep learning took over computer vision, people used approaches like <a href="http://mi.eng.cam.ac.uk/~cipolla/publications/inproceedings/2008-CVPR-semantic-texton-forests.pdf" target="_blank" rel="external">TextonForest</a> and <a href="http://www.cse.chalmers.se/edu/year/2011/course/TDA361/Advanced%20Computer%20Graphics/BodyPartRecognition.pdf" target="_blank" rel="external">Random Forest based classifiers</a> for semantic segmentation. As with image classification, convolutional neural networks (CNN) have had enormous success on segmentation problems.</p><p>One of the popular initial deep learning approaches was <a href="http://people.idsia.ch/~juergen/nips2012.pdf" target="_blank" rel="external">patch classification</a> where each pixel was separately classified into classes using a patch of image around it. Main reason to use patches was that classification networks usually have full connected layers and therefore required fixed size images.</p><p>In 2014, <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#fcn" target="_blank" rel="external">Fully Convolutional Networks (FCN)</a> by Long et al. from Berkeley, popularized CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for image of any size and was also much faster compared to the patch classification approach. Almost all the subsequent state of the art approaches on semantic segmentation adopted this paradigm.</p><p>Apart from fully connected layers, one of the main problems with using CNNs for segmentation is <em>pooling layers</em>. Pooling layers increase the field of view and are able to aggregate the context while discarding the ‘where’ information. However, semantic segmentation requires the exact alignment of class maps and thus, needs the ‘where’ information to be preserved. Two different classes of architectures evolved in the literature to tackle this issue.</p><p>First one is encoder-decoder architecture. Encoder gradually reduces the spatial dimension with pooling layers and decoder gradually recovers the object details and spatial dimension. There are usually shortcut connections from encoder to decoder to help decoder recover the object details better. <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="external">U-Net</a> is a popular architecture from this class.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/unet.png" alt="U-Net architecture"><br>U-Net: An encoder-decoder architecture. <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="external">Source</a>.</p><p>Architectures in the second class use what are called as <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated/atrous convolutions</a>and do away with pooling layers.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/dilated_conv.png" alt="Dilated/atrous convolutions"><br>Dilated/atrous convolutions. rate=1 is same as normal convolutions. <a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="external">Source</a>.</p><p><a href="https://arxiv.org/abs/1210.5644" target="_blank" rel="external">Conditional Random Field (CRF) postprocessing</a> are usually used to improve the segmentation. CRFs are graphical models which ‘smooth’ segmentation based on the underlying image intensities. They work based on the observation that similar intensity pixels tend to be labeled as the same class. CRFs can boost scores by 1-2%.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/crf.png" alt="CRF"><br>CRF illustration. (b) Unary classifiers is the segmentation input to the CRF. (c, d, e) are variants of CRF with (e) being the widely used one. <a href="https://arxiv.org/abs/1210.5644" target="_blank" rel="external">Source</a>.</p><p>In the next section, I’ll summarize a few papers that represent the evolution of segmentation architectures starting from FCN. All these architectures are benchmarked on <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php" target="_blank" rel="external">VOC2012 evaluation server</a>.</p><h3 id="Summaries"><a href="#Summaries" class="headerlink" title="Summaries"></a>Summaries</h3><p>Following papers are summarized (in chronological order):</p><ol><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#fcn" target="_blank" rel="external">FCN</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#segnet" target="_blank" rel="external">SegNet</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">Dilated Convolutions</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLab (v1 &amp; v2)</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#refinenet" target="_blank" rel="external">RefineNet</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#pspnet" target="_blank" rel="external">PSPNet</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#large-kernel" target="_blank" rel="external">Large Kernel Matters</a></li><li><a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplabv3" target="_blank" rel="external">DeepLab v3</a></li></ol><p>For each of these papers, I list down their key contributions and explain them. I also show their benchmark scores (mean IOU) on VOC2012 test dataset.</p><h4 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h4><ul><li>Fully Convolutional Networks for Semantic Segmentation</li><li>Submitted on 14 Nov 2014</li><li><a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Popularize the use of end to end convolutional networks for semantic segmentation</li><li>Re-purpose imagenet pretrained networks for segmentation</li><li>Upsample using <em>deconvolutional</em> layers</li><li>Introduce skip connections to improve over the coarseness of upsampling</li></ul><p><em>Explanation</em>:</p><p>Key observation is that fully connected layers in classification networks can be viewed as convolutions with kernels that cover their entire input regions. This is equivalent to evaluating the original classification network on overlapping input patches but is much more efficient because computation is shared over the overlapping regions of patches. Although this observation is not unique to this paper (see <a href="https://arxiv.org/abs/1312.6229" target="_blank" rel="external">overfeat</a>, <a href="https://plus.google.com/+PierreSermanet/posts/VngsFR3tug9" target="_blank" rel="external">this post</a>), it improved the state of the art on VOC2012 significantly.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/FCN%20-%20illustration.png" alt="FCN architecture"><br>Fully connected layers as a convolution. <a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="external">Source</a>.</p><p>After convolutionalizing fully connected layers in a imagenet pretrained network like VGG, feature maps still need to be upsampled because of pooling operations in CNNs. Instead of using simple bilinear interpolation, <em>deconvolutional layers</em> can learn the interpolation. This layer is also known as upconvolution, full convolution, transposed convolution or fractionally-strided convolution.</p><p>However, upsampling (even with deconvolutional layers) produces coarse segmentation maps because of loss of information during pooling. Therefore, shortcut/skip connections are introduced from higher resolution feature maps.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>62.2</td><td>-</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6&amp;submid=6103#KEY_FCN-8s" target="_blank" rel="external">leaderboard</a></td></tr><tr><td>67.2</td><td>More momentum. Not described in paper</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6&amp;submid=6103#KEY_FCN-8s-heavy" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><p><em>My Comments</em>:</p><ul><li>This was an important contribution but state of the art has improved a lot by now though.</li></ul><h4 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h4><ul><li>SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</li><li>Submitted on 2 Nov 2015</li><li><a href="https://arxiv.org/abs/1511.00561" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Maxpooling indices transferred to decoder to improve the segmentation resolution.</li></ul><p><em>Explanation</em>:</p><p>FCN, despite upconvolutional layers and a few shortcut connections produces coarse segmentation maps. Therefore, more shortcut connections are introduced. However, instead of copying the encoder features as in FCN, indices from maxpooling are copied. This makes SegNet more memory efficient than FCN.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/segnet_architecture.png" alt="SegNet Architecture"><br>Segnet Architecture. <a href="https://arxiv.org/abs/1511.00561" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>59.9</td><td>-</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_SegNet" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><p><em>My comments</em>:</p><ul><li>FCN and SegNet are one of the first encoder-decoder architectures.</li><li>Benchmarks for SegNet are not good enough to be used anymore.</li></ul><h4 id="Dilated-Convolutions"><a href="#Dilated-Convolutions" class="headerlink" title="Dilated Convolutions"></a>Dilated Convolutions</h4><ul><li>Multi-Scale Context Aggregation by Dilated Convolutions</li><li>Submitted on 23 Nov 2015</li><li><a href="https://arxiv.org/abs/1511.07122" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Use dilated convolutions, a convolutional layer for dense predictions.</li><li>Propose ‘context module’ which uses dilated convolutions for multi scale aggregation.</li></ul><p><em>Explanation</em>:</p><p>Pooling helps in classification networks because receptive field increases. But this is not the best thing to do for segmentation because pooling decreases the resolution. Therefore, authors use <em>dilated convolution</em> layer which works like this:</p><p><img src="https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif" alt="Dilated/Atrous Convolutions"><br>Dilated/Atrous Convolutions. <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank" rel="external">Source</a></p><p>Dilated convolutional layer (also called as atrous convolution in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLab</a>) allows for exponential increase in field of view without decrease of spatial dimensions.</p><p>Last two pooling layers from pretrained classification network (here, VGG) are removed and subsequent convolutional layers are replaced with dilated convolutions. In particular, convolutions between the pool-3 and pool-4 have dilation 2 and convolutions after pool-4 have dilation 4. With this module (called <em>frontend module</em> in the paper), dense predictions are obtained without any increase in number of parameters.</p><p>A module (called <em>context module</em> in the paper) is trained separately with the outputs of frontend module as inputs. This module is a cascade of dilated convolutions of different dilations so that multi scale context is aggregated and predictions from frontend are improved.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>71.3</td><td>frontend</td><td>reported in the paper</td></tr><tr><td>73.5</td><td>frontend + context</td><td>reported in the paper</td></tr><tr><td>74.7</td><td>frontend + context + CRF</td><td>reported in the paper</td></tr><tr><td>75.3</td><td>frontend + context + CRF-RNN</td><td>reported in the paper</td></tr></tbody></table><p><em>My comments</em>:</p><ul><li>Note that predicted segmentation map’s size is 1/8th of that of the image. This is the case with almost all the approaches. They are interpolated to get the final segmentation map.</li></ul><h4 id="DeepLab-v1-amp-v2"><a href="#DeepLab-v1-amp-v2" class="headerlink" title="DeepLab (v1 &amp; v2)"></a>DeepLab (v1 &amp; v2)</h4><ul><li><strong>v1</strong> : Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</li><li>Submitted on 22 Dec 2014</li><li><a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="external">Arxiv Link</a></li><li></li><li><strong>v2</strong> : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</li><li>Submitted on 2 Jun 2016</li><li><a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Use atrous/dilated convolutions.</li><li>Propose atrous spatial pyramid pooling (ASPP)</li><li>Use Fully connected CRF</li></ul><p><em>Explanation</em>:</p><p>Atrous/Dilated convolutions increase the field of view without increasing the number of parameters. Net is modified like in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions paper</a>.</p><p>Multiscale processing is achieved either by passing multiple rescaled versions of original images to parallel CNN branches (Image pyramid) and/or by using multiple parallel atrous convolutional layers with different sampling rates (ASPP).</p><p>Structured prediction is done by fully connected CRF. CRF is trained/tuned separately as a post processing step.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/deeplabv2.png" alt="DeepLab2 Pipeline"><br>DeepLab2 Pipeline. <a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>79.7</td><td>ResNet-101 + atrous Convolutions + ASPP + CRF</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?cls=mean&amp;challengeid=11&amp;compid=6&amp;submid=6103#KEY_DeepLabv2-CRF" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><h4 id="RefineNet"><a href="#RefineNet" class="headerlink" title="RefineNet"></a>RefineNet</h4><ul><li>RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation</li><li>Submitted on 20 Nov 2016</li><li><a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Encoder-Decoder architecture with well thought-out decoder blocks</li><li>All the components follow residual connection design</li></ul><p><em>Explanation</em>:</p><p>Approach of using dilated/atrous convolutions are not without downsides. Dilated convolutions are computationally expensive and take a lot of memory because they have to be applied on large number of high resolution feature maps. This hampers the computation of high-res predictions. <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLab’s</a> predictions, for example are 1/8th the size of original input.</p><p>So, the paper proposes to use encoder-decoder architecture. Encoder part is ResNet-101 blocks. Decoder has RefineNet blocks which concatenate/fuse high resolution features from encoder and low resolution features from previous RefineNet block.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/refinenet%20-%20architecture.png" alt="RefineNet Architecture"><br>RefineNet Architecture. <a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">Source</a>.</p><p>Each RefineNet block has a component to fuse the multi resolution features by upsampling the lower resolution features and a component to capture context based on repeated 5 x 5 <em>stride 1</em> pool layers. Each of these components employ the residual connection design following the identity map mindset.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/refinenet%20-%20block.png" alt="RefineNet Block"><br>RefineNet Block. <a href="https://arxiv.org/abs/1611.06612" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>84.2</td><td>Uses CRF, Multiscale inputs, COCO pretraining</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_Multipath-RefineNet" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><h4 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h4><ul><li>Pyramid Scene Parsing Network</li><li>Submitted on 4 Dec 2016</li><li><a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Propose pyramid pooling module to aggregate the context.</li><li>Use auxiliary loss</li></ul><p><em>Explanation</em>:</p><p>Global scene categories matter because it provides clues on the distribution of the segmentation classes. Pyramid pooling module captures this information by applying large kernel pooling layers.</p><p>Dilated convolutions are used as in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions paper</a> to modify Resnet and a pyramid pooling module is added to it. This module concatenates the feature maps from ResNet with upsampled output of parallel pooling layers with kernels covering whole, half of and small portions of image.</p><p>An auxiliary loss, additional to the loss on main branch, is applied after the fourth stage of ResNet (i.e input to pyramid pooling module). This idea was also called as intermediate supervision elsewhere.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/pspnet.png" alt="PSPNet Architecture"><br>PSPNet Architecture. <a href="https://arxiv.org/abs/1612.01105" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>85.4</td><td>MSCOCO pretraining, multi scale input, no CRF</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_PSPNet" target="_blank" rel="external">leaderboard</a></td></tr><tr><td>82.6</td><td>no MSCOCO pretraining, multi scale input, no CRF</td><td>reported in the paper</td></tr></tbody></table><h4 id="Large-Kernel-Matters"><a href="#Large-Kernel-Matters" class="headerlink" title="Large Kernel Matters"></a>Large Kernel Matters</h4><ul><li>Large Kernel Matters – Improve Semantic Segmentation by Global Convolutional Network</li><li>Submitted on 8 Mar 2017</li><li><a href="https://arxiv.org/abs/1703.02719" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Propose a encoder-decoder architecture with very large kernels convolutions</li></ul><p><em>Explanation</em>:</p><p>Semantic segmentation requires both segmentation and classification of the segmented objects. Since fully connected layers cannot be present in a segmentation architecture, convolutions with very large kernels are adopted instead.</p><p>Another reason to adopt large kernels is that although deeper networks like ResNet have very large receptive field, <a href="https://arxiv.org/abs/1412.6856" target="_blank" rel="external">studies</a> show that the network tends to gather information from a much smaller region (valid receptive filed).</p><p>Larger kernels are computationally expensive and have a lot of parameters. Therefore, k x k convolution is approximated with sum of 1 x k + k x 1 and k x 1 and 1 x k convolutions. This module is called as <em>Global Convolutional Network</em> (GCN) in the paper.</p><p>Coming to architecture, ResNet(without any dilated convolutions) forms encoder part of the architecture while GCNs and deconvolutions form decoder. A simple residual block called <em>Boundary Refinement</em> (BR) is also used.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/large_kernel_matter.png" alt="GCN Architecture"><br>GCN Architecture. <a href="https://arxiv.org/abs/1703.02719" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>82.2</td><td>-</td><td>reported in the paper</td></tr><tr><td>83.6</td><td>Improved training, not described in the paper</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_Large_Kernel_Matters" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><h4 id="DeepLab-v3"><a href="#DeepLab-v3" class="headerlink" title="DeepLab v3"></a>DeepLab v3</h4><ul><li>Rethinking Atrous Convolution for Semantic Image Segmentation</li><li>Submitted on 17 Jun 2017</li><li><a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="external">Arxiv Link</a></li></ul><p><em>Key Contributions</em>:</p><ul><li>Improved atrous spatial pyramid pooling (ASPP)</li><li>Module which employ atrous convolutions in cascade</li></ul><p><em>Explanation</em>:</p><p>ResNet model is modified to use dilated/atrous convolutions as in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLabv2</a> and <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions</a>. Improved ASPP involves concatenation of image-level features, a 1x1 convolution and three 3x3 atrous convolutions with different rates. Batch normalization is used after each of the parallel convolutional layers.</p><p>Cascaded module is a resnet block except that component convolution layers are made atrous with different rates. This module is similar to context module used in <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#dilation" target="_blank" rel="external">dilated convolutions paper</a> but this is applied directly on intermediate feature maps instead of belief maps (belief maps are final CNN feature maps with channels equal to number of classes).</p><p>Both the proposed models are evaluated independently and attempt to combine the both did not improve the performance. Both of them performed very similarly on val set with ASPP performing slightly better. CRF is not used.</p><p>Both these models outperform the best model from <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#deeplab" target="_blank" rel="external">DeepLabv2</a>. Authors note that the improvement comes from the batch normalization and better way to encode multi scale context.</p><p><img src="http://blog.qure.ai/assets/images/segmentation-review/deeplabv3.png" alt="DeepLabv3 ASPP"><br>DeepLabv3 ASPP (used for submission). <a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="external">Source</a>.</p><p><em>Benchmarks (VOC2012)</em>:</p><table><thead><tr><th>Score</th><th>Comment</th><th>Source</th></tr></thead><tbody><tr><td>85.7</td><td>used ASPP (no cascaded modules)</td><td><a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=6#KEY_DeepLabv3" target="_blank" rel="external">leaderboard</a></td></tr></tbody></table><p>Reblog from <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review#sec-2" target="_blank" rel="external">here</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">131</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">64</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://twitter.com/tomaxent" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i> Twitter</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>