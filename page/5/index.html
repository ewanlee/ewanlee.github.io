<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/5/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/5/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/19/Python多核编程mpi4py实践/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/03/19/Python多核编程mpi4py实践/" itemprop="url">Python多核编程mpi4py实践</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-19T13:25:36+08:00">2018-03-19 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/03/19/Python多核编程mpi4py实践/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/03/19/Python多核编程mpi4py实践/" itemprop="commentsCount"></span> </a></span><span id="/2018/03/19/Python多核编程mpi4py实践/" class="leancloud_visitors" data-flag-title="Python多核编程mpi4py实践"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>转载自<a href="http://blog.csdn.net/ztf312/article/details/74997939" target="_blank" rel="external">这篇博文</a>.</p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>​ CPU从三十多年前的8086，到十年前的奔腾，再到当下的多核i7。一开始，以单核cpu的主频为目标，架构的改良和集成电路工艺的进步使得cpu的性能高速上升，单核cpu的主频从老爷车的MHz阶段一度接近4GHz高地。然而，也因为工艺和功耗等的限制，单核cpu遇到了人生的天花板，急需转换思维，以满足无止境的性能需求。多核cpu在此登上历史舞台。给你的老爷车多加两个引擎，让你有法拉利的感觉。现时代，连手机都到处叫嚣自己有4核8核处理器的时代，PC就更不用说了。</p><p>​ 扯远了，anyway，对于俺们程序员来说，如何利用如此强大的引擎完成我们的任务才是我们要考虑的。随着大规模数据处理、大规模问题和复杂系统求解需求的增加，以前的单核编程已经有心无力了。如果程序一跑就得几个小时，甚至一天，想想都无法原谅自己。那如何让自己更快的过度到高大上的多核并行编程中去呢？哈哈，广大人民的力量！</p><p>​ 目前工作中我所接触到的并行处理框架主要有MPI、OpenMP和MapReduce(Hadoop)三个（CUDA属于GPU并行编程，这里不提及）。MPI和Hadoop都可以在集群中运行，而OpenMP因为共享存储结构的关系，不能在集群上运行，只能单机。另外，MPI可以让数据保留在内存中，可以为节点间的通信和数据交互保存上下文，所以能执行迭代算法，而Hadoop却不具有这个特性。因此，需要迭代的机器学习算法大多使用MPI来实现。当然了，部分机器学习算法也是可以通过设计使用Hadoop来完成的。（浅见，如果错误，希望各位不吝指出，谢谢）。</p><p>​ 本文主要介绍Python环境下MPI编程的实践基础。</p><h1 id="MPI与mpi4py"><a href="#MPI与mpi4py" class="headerlink" title="MPI与mpi4py"></a>MPI与mpi4py</h1><p>​ MPI是Message Passing Interface的简称，也就是消息传递。消息传递指的是并行执行的各个进程具有自己独立的堆栈和代码段，作为互不相关的多个程序独立执行，进程之间的信息交互完全通过显示地调用通信函数来完成。</p><p>​ Mpi4py是构建在mpi之上的python库，使得python的数据结构可以在进程（或者多个cpu）之间进行传递。</p><h2 id="MPI的工作方式"><a href="#MPI的工作方式" class="headerlink" title="MPI的工作方式"></a>MPI的工作方式</h2><p>​ 很简单，就是你启动了一组MPI进程，每个进程都是执行同样的代码！然后每个进程都有一个ID，也就是rank来标记我是谁。什么意思呢？假设一个CPU是你请的一个工人，共有10个工人。你有100块砖头要搬，然后很公平，让每个工人搬10块。这时候，你把任务写到一个任务卡里面，让10个工人都执行这个任务卡中的任务，也就是搬砖！这个任务卡中的“搬砖”就是你写的代码。然后10个CPU执行同一段代码。需要注意的是，代码里面的所有变量都是每个进程独有的，虽然名字相同。</p><p>​ 例如，一个脚本test.py，里面包含以下代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from mpi4py import MPI  </div><div class="line">print("hello world'')  </div><div class="line">print("my rank is: %d" %MPI.rank)</div></pre></td></tr></table></figure><p>​ 然后我们在命令行通过以下方式运行：</p><p>​ <code>mpirun –np 5 python test.py</code></p><p>​ <code>-np5</code> 指定启动5个mpi进程来执行后面的程序。相当于对脚本拷贝了5份，每个进程运行一份，互不干扰。在运行的时候代码里面唯一的不同，就是各自的rank也就是ID不一样。所以这个代码就会打印5个hello world和5个不同的rank值，从0到4.</p><h2 id="点对点通信"><a href="#点对点通信" class="headerlink" title="点对点通信"></a>点对点通信</h2><p>​ 点对点通信（Point-to-PointCommunication）的能力是信息传递系统最基本的要求。意思就是让两个进程直接可以传输数据，也就是一个发送数据，另一个接收数据。接口就两个，send和recv，来个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="comment"># point to point communication  </span></div><div class="line">data_send = [comm_rank]*<span class="number">5</span>  </div><div class="line">comm.send(data_send,dest=(comm_rank+<span class="number">1</span>)%comm_size)  </div><div class="line">data_recv =comm.recv(source=(comm_rank<span class="number">-1</span>)%comm_size)  </div><div class="line">print(<span class="string">"my rank is %d, and Ireceived:"</span> % comm_rank)  </div><div class="line"><span class="keyword">print</span> data_recv</div></pre></td></tr></table></figure><p>​ 启动5个进程运行以上代码，结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">my rank <span class="keyword">is</span> <span class="number">0</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]  </div><div class="line">my rank <span class="keyword">is</span> <span class="number">1</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]  </div><div class="line">my rank <span class="keyword">is</span> <span class="number">2</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]  </div><div class="line">my rank <span class="keyword">is</span> <span class="number">3</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]  </div><div class="line">my rank <span class="keyword">is</span> <span class="number">4</span>, <span class="keyword">and</span> I received:  </div><div class="line">[<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]</div></pre></td></tr></table></figure><p>​ 可以看到，每个进程都创建了一个数组，然后把它传递给下一个进程，最后的那个进程传递给第一个进程。<code>comm_size</code>就是mpi的进程个数，也就是<code>-np</code>指定的那个数。<code>MPI.COMM_WORLD</code>表示进程所在的通信组。</p><p>​ 但这里面有个需要注意的问题，如果我们要发送的数据比较小的话，mpi会缓存我们的数据，也就是说执行到<code>send</code>这个代码的时候，会缓存被send的数据，然后继续执行后面的指令，而不会等待对方进程执行<code>recv</code>指令接收完这个数据。但是，如果要发送的数据很大，那么进程就是挂起等待，直到接收进程执行了<code>recv</code>指令接收了这个数据，进程才继续往下执行。所以上述的代码发送[rank]<em>5没啥问题，如果发送[rank]</em>500程序就会半死不活的样子了。因为所有的进程都会卡在发送这条指令，等待下一个进程发起接收的这个指令，但是进程是执行完发送的指令才能执行接收的指令，这就和死锁差不多了。所以一般，我们将其修改成以下的方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line">data_send = [comm_rank]*<span class="number">5</span>  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   comm.send(data_send, dest=(comm_rank+<span class="number">1</span>)%comm_size)  </div><div class="line"><span class="keyword">if</span> comm_rank &gt; <span class="number">0</span>:  </div><div class="line">   data_recv = comm.recv(source=(comm_rank<span class="number">-1</span>)%comm_size)  </div><div class="line">   comm.send(data_send, dest=(comm_rank+<span class="number">1</span>)%comm_size)  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data_recv = comm.recv(source=(comm_rank<span class="number">-1</span>)%comm_size)  </div><div class="line">print(<span class="string">"my rank is %d, and Ireceived:"</span> % comm_rank)  </div><div class="line"><span class="keyword">print</span> data_recv</div></pre></td></tr></table></figure><p>​ 第一个进程一开始就发送数据，其他进程一开始都是在等待接收数据，这时候进程1接收了进程0的数据，然后发送进程1的数据，进程2接收了，再发送进程2的数据……知道最后进程0接收最后一个进程的数据，从而避免了上述问题。</p><p>​ 一个比较常用的方法是封一个组长，也就是一个主进程，一般是进程0作为主进程leader。主进程将数据发送给其他的进程，其他的进程处理数据，然后返回结果给进程0。换句话说，就是进程0来控制整个数据处理流程。</p><h2 id="群体通信"><a href="#群体通信" class="headerlink" title="群体通信"></a>群体通信</h2><p>​ 点对点通信是A发送给B，一个人将自己的秘密告诉另一个人，群体通信（Collective Communications）像是拿个大喇叭，一次性告诉所有的人。前者是一对一，后者是一对多。但是，群体通信是以更有效的方式工作的。它的原则就一个：尽量把所有的进程在所有的时刻都使用上！我们在下面的bcast小节讲述。</p><p>​ 群体通信还是发送和接收两类，一个是一次性把数据发给所有人，另一个是一次性从所有人那里回收结果。</p><h3 id="广播bcast"><a href="#广播bcast" class="headerlink" title="广播bcast"></a>广播bcast</h3><p>​ 将一份数据发送给所有的进程。例如我有200份数据，有10个进程，那么每个进程都会得到这200份数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data = range(comm_size)  </div><div class="line">data = comm.bcast(data <span class="keyword">if</span> comm_rank == <span class="number">0</span><span class="keyword">else</span> <span class="keyword">None</span>, root=<span class="number">0</span>)  </div><div class="line"><span class="keyword">print</span> <span class="string">'rank %d, got:'</span> % (comm_rank)  </div><div class="line"><span class="keyword">print</span> data</div></pre></td></tr></table></figure><p>​ 结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">rank <span class="number">0</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">1</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">2</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">3</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">4</span>, got:  </div><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</div></pre></td></tr></table></figure><p>​ Root进程自己建了一个列表，然后广播给所有的进程。这样所有的进程都拥有了这个列表。然后爱干嘛就干嘛了。</p><p>​ 对广播最直观的观点是某个特定进程将数据一一发送给每个进程。假设有n个进程，那么假设我们的数据在0进程，那么0进程就需要将数据发送给剩下的n-1个进程，这是非常低效的，复杂度是O(n)。那有没有高效的方式？一个最常用也是非常高效的手段是规约树广播：收到广播数据的所有进程都参与到数据广播的过程中。首先只有一个进程有数据，然后它把它发送给第一个进程，此时有两个进程有数据；然后这两个进程都参与到下一次的广播中，这时就会有4个进程有数据，……，以此类推，每次都会有2的次方个进程有数据。通过这种规约树的广播方法，广播的复杂度降为O(log n)。这就是上面说的群体通信的高效原则：充分利用所有的进程来实现数据的发送和接收。</p><h3 id="散播scatter"><a href="#散播scatter" class="headerlink" title="散播scatter"></a>散播scatter</h3><p>​ 将一份数据平分给所有的进程。例如我有200份数据，有10个进程，那么每个进程会分别得到20份数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data = range(comm_size)  </div><div class="line">   <span class="keyword">print</span> data  </div><div class="line"><span class="keyword">else</span>:  </div><div class="line">   data = <span class="keyword">None</span>  </div><div class="line">local_data = comm.scatter(data, root=<span class="number">0</span>)  </div><div class="line"><span class="keyword">print</span> <span class="string">'rank %d, got:'</span> % comm_rank  </div><div class="line"><span class="keyword">print</span> local_data</div></pre></td></tr></table></figure><p>​ 结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">0</span>, got:  </div><div class="line"><span class="number">0</span>  </div><div class="line">rank <span class="number">1</span>, got:  </div><div class="line"><span class="number">1</span>  </div><div class="line">rank <span class="number">2</span>, got:  </div><div class="line"><span class="number">2</span>  </div><div class="line">rank <span class="number">3</span>, got:  </div><div class="line"><span class="number">3</span>  </div><div class="line">rank <span class="number">4</span>, got:  </div><div class="line"><span class="number">4</span></div></pre></td></tr></table></figure><p>​ 这里root进程创建了一个list，然后将它散播给所有的进程，相当于对这个list做了划分，每个进程获得等分的数据，这里就是list的每一个数。（主要根据list的索引来划分，list索引为第i份的数据就发送给第i个进程）。如果是矩阵，那么就等分的划分行，每个进程获得相同的行数进行处理。</p><p>​ 需要注意的是，MPI的工作方式是每个进程都会执行所有的代码，所以每个进程都会执行scatter这个指令，但只有root执行它的时候，它才兼备发送者和接收者的身份（root也会得到属于自己的数据），对于其他进程来说，他们都只是接收者而已。</p><h3 id="收集gather"><a href="#收集gather" class="headerlink" title="收集gather"></a>收集gather</h3><p>​ 那有发送，就有一起回收的函数。Gather是将所有进程的数据收集回来，合并成一个列表。下面联合scatter和gather组成一个完成的分发和收回过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data = range(comm_size)  </div><div class="line">   <span class="keyword">print</span> data  </div><div class="line"><span class="keyword">else</span>:  </div><div class="line">   data = <span class="keyword">None</span>  </div><div class="line">local_data = comm.scatter(data, root=<span class="number">0</span>)  </div><div class="line">local_data = local_data * <span class="number">2</span>  </div><div class="line"><span class="keyword">print</span> <span class="string">'rank %d, got and do:'</span> % comm_rank  </div><div class="line"><span class="keyword">print</span> local_data  </div><div class="line">combine_data = comm.gather(local_data,root=<span class="number">0</span>)  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">printcombine_data</div></pre></td></tr></table></figure><p>​ 结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">0</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">0</span>  </div><div class="line">rank <span class="number">1</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">2</span>  </div><div class="line">rank <span class="number">2</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">4</span>  </div><div class="line">rank <span class="number">4</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">8</span>  </div><div class="line">rank <span class="number">3</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">6</span>  </div><div class="line">[<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>]</div></pre></td></tr></table></figure><p>​ Root进程将数据通过scatter等分发给所有的进程，等待所有的进程都处理完后（这里只是简单的乘以2），root进程再通过gather回收他们的结果，和分发的原则一样，组成一个list。Gather还有一个变体就是allgather，可以理解为它在gather的基础上将gather的结果再bcast了一次。啥意思？意思是root进程将所有进程的结果都回收统计完后，再把整个统计结果告诉大家。这样，不仅root可以访问combine_data，所有的进程都可以访问combine_data了。</p><h3 id="规约reduce"><a href="#规约reduce" class="headerlink" title="规约reduce"></a>规约reduce</h3><p>​ 规约是指不但将所有的数据收集回来，收集回来的过程中还进行了简单的计算，例如求和，求最大值等等。为什么要有这个呢？我们不是可以直接用gather全部收集回来了，再对列表求个sum或者max就可以了吗？这样不是累死组长吗？为什么不充分使用每个工人呢？规约实际上是使用规约树来实现的。例如求max，完成可以让工人两两pk后，再返回两两pk的最大值，然后再对第二层的最大值两两pk，直到返回一个最终的max给组长。组长就非常聪明的将工作分配下工人高效的完成了。这是O(n)的复杂度，下降到O(log n)（底数为2）的复杂度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line">comm_size = comm.Get_size()  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">   data = range(comm_size)  </div><div class="line">   <span class="keyword">print</span> data  </div><div class="line"><span class="keyword">else</span>:  </div><div class="line">   data = <span class="keyword">None</span>  </div><div class="line">local_data = comm.scatter(data, root=<span class="number">0</span>)  </div><div class="line">local_data = local_data * <span class="number">2</span>  </div><div class="line"><span class="keyword">print</span> <span class="string">'rank %d, got and do:'</span> % comm_rank  </div><div class="line"><span class="keyword">print</span> local_data  </div><div class="line">all_sum = comm.reduce(local_data, root=<span class="number">0</span>,op=MPI.SUM)  </div><div class="line"><span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line"><span class="keyword">print</span> <span class="string">'sumis:%d'</span> % all_sum</div></pre></td></tr></table></figure><p>​ 结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]  </div><div class="line">rank <span class="number">0</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">0</span>  </div><div class="line">rank <span class="number">1</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">2</span>  </div><div class="line">rank <span class="number">2</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">4</span>  </div><div class="line">rank <span class="number">3</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">6</span>  </div><div class="line">rank <span class="number">4</span>, got <span class="keyword">and</span> do:  </div><div class="line"><span class="number">8</span>  </div><div class="line">sum <span class="keyword">is</span>:<span class="number">20</span></div></pre></td></tr></table></figure><p>​ 可以看到，最后可以得到一个sum值。</p><h1 id="常见用法"><a href="#常见用法" class="headerlink" title="常见用法"></a>常见用法</h1><h2 id="对一个文件的多个行并行处理"><a href="#对一个文件的多个行并行处理" class="headerlink" title="对一个文件的多个行并行处理"></a>对一个文件的多个行并行处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!usr/bin/env python  </span></div><div class="line"><span class="comment">#-*- coding: utf-8 -*-  </span></div><div class="line">   </div><div class="line"><span class="keyword">import</span> sys  </div><div class="line"><span class="keyword">import</span> os  </div><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </div><div class="line">   </div><div class="line">   </div><div class="line"><span class="comment">#  </span></div><div class="line"><span class="comment">#  Global variables for MPI  </span></div><div class="line"><span class="comment">#  </span></div><div class="line">   </div><div class="line"><span class="comment"># instance for invoking MPI relatedfunctions  </span></div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line"><span class="comment"># the node rank in the whole community  </span></div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line"><span class="comment"># the size of the whole community, i.e.,the total number of working nodes in the MPI cluster  </span></div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line">   </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </div><div class="line">   <span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">       sys.stderr.write(<span class="string">"processor root starts reading data...\n"</span>)  </div><div class="line">       all_lines = sys.stdin.readlines()  </div><div class="line">   all_lines = comm.bcast(all_lines <span class="keyword">if</span> comm_rank == <span class="number">0</span> <span class="keyword">else</span> <span class="keyword">None</span>, root = <span class="number">0</span>)  </div><div class="line">   num_lines = len(all_lines)  </div><div class="line">   local_lines_offset = np.linspace(<span class="number">0</span>, num_lines, comm_size +<span class="number">1</span>).astype(<span class="string">'int'</span>)  </div><div class="line">   local_lines = all_lines[local_lines_offset[comm_rank] :local_lines_offset[comm_rank + <span class="number">1</span>]]  </div><div class="line">   sys.stderr.write(<span class="string">"%d/%d processor gets %d/%d data \n"</span> %(comm_rank, comm_size, len(local_lines), num_lines))  </div><div class="line">   cnt = <span class="number">0</span>  </div><div class="line">   <span class="keyword">for</span> line <span class="keyword">in</span> local_lines:  </div><div class="line">       fields = line.strip().split(<span class="string">'\t'</span>)  </div><div class="line">       cnt += <span class="number">1</span>  </div><div class="line">       <span class="keyword">if</span> cnt % <span class="number">100</span> == <span class="number">0</span>:  </div><div class="line">           sys.stderr.write(<span class="string">"processor %d has processed %d/%d lines \n"</span> %(comm_rank, cnt, len(local_lines)))  </div><div class="line">       output = line.strip() + <span class="string">' process every line here'</span>  </div><div class="line">       <span class="keyword">print</span> output</div></pre></td></tr></table></figure><h2 id="对多个文件并行处理"><a href="#对多个文件并行处理" class="headerlink" title="对多个文件并行处理"></a>对多个文件并行处理</h2><p>​ 如果我们的文件太大，例如几千万行，那么mpi是没办法将这么大的数据bcast给所有的进程的，所以我们可以先把大的文件split成小的文件，再让每个进程处理少数的文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!usr/bin/env python  </span></div><div class="line"><span class="comment">#-*- coding: utf-8 -*-  </span></div><div class="line">   </div><div class="line"><span class="keyword">import</span> sys  </div><div class="line"><span class="keyword">import</span> os  </div><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </div><div class="line">   </div><div class="line"><span class="comment">#  </span></div><div class="line"><span class="comment">#  Global variables for MPI  </span></div><div class="line"><span class="comment">#  </span></div><div class="line">   </div><div class="line"><span class="comment"># instance for invoking MPI relatedfunctions  </span></div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line"><span class="comment"># the node rank in the whole community  </span></div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line"><span class="comment"># the size of the whole community, i.e.,the total number of working nodes in the MPI cluster  </span></div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line">   </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </div><div class="line">   <span class="keyword">if</span> len(sys.argv) != <span class="number">2</span>:  </div><div class="line">       sys.stderr.write(<span class="string">"Usage: python *.py directoty_with_files\n"</span>)  </div><div class="line">       sys.exit(<span class="number">1</span>)  </div><div class="line">   path = sys.argv[<span class="number">1</span>]  </div><div class="line">   <span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">       file_list = os.listdir(path)  </div><div class="line">       sys.stderr.write(<span class="string">"%d files\n"</span> % len(file_list))  </div><div class="line">   file_list = comm.bcast(file_list <span class="keyword">if</span> comm_rank == <span class="number">0</span> <span class="keyword">else</span> <span class="keyword">None</span>, root = <span class="number">0</span>)  </div><div class="line">   num_files = len(file_list)  </div><div class="line">   local_files_offset = np.linspace(<span class="number">0</span>, num_files, comm_size +<span class="number">1</span>).astype(<span class="string">'int'</span>)  </div><div class="line">   local_files = file_list[local_files_offset[comm_rank] :local_files_offset[comm_rank + <span class="number">1</span>]]  </div><div class="line">   sys.stderr.write(<span class="string">"%d/%d processor gets %d/%d data \n"</span> %(comm_rank, comm_size, len(local_files), num_files))  </div><div class="line">    cnt = <span class="number">0</span>  </div><div class="line">   <span class="keyword">for</span> file_name <span class="keyword">in</span> local_files:  </div><div class="line">       hd = open(os.path.join(path, file_name))  </div><div class="line">       <span class="keyword">for</span> line <span class="keyword">in</span> hd:  </div><div class="line">           output = line.strip() + <span class="string">' process every line here'</span>  </div><div class="line">           <span class="keyword">print</span> output  </div><div class="line">       cnt += <span class="number">1</span>  </div><div class="line">       sys.stderr.write(<span class="string">"processor %d has processed %d/%d files \n"</span> %(comm_rank, cnt, len(local_files)))  </div><div class="line">       hd.close()</div></pre></td></tr></table></figure><h2 id="联合numpy对矩阵的多个行或者多列并行处理"><a href="#联合numpy对矩阵的多个行或者多列并行处理" class="headerlink" title="联合numpy对矩阵的多个行或者多列并行处理"></a>联合numpy对矩阵的多个行或者多列并行处理</h2><p>​ Mpi4py一个非常优秀的特性是完美支持numpy！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os, sys, time  </div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </div><div class="line"><span class="keyword">import</span> mpi4py.MPI <span class="keyword">as</span> MPI  </div><div class="line">   </div><div class="line">   </div><div class="line"><span class="comment">#  </span></div><div class="line"><span class="comment">#  Global variables for MPI  </span></div><div class="line"><span class="comment">#  </span></div><div class="line">   </div><div class="line">   </div><div class="line"><span class="comment"># instance for invoking MPI relatedfunctions  </span></div><div class="line">comm = MPI.COMM_WORLD  </div><div class="line"><span class="comment"># the node rank in the whole community  </span></div><div class="line">comm_rank = comm.Get_rank()  </div><div class="line"><span class="comment"># the size of the whole community, i.e.,the total number of working nodes in the MPI cluster  </span></div><div class="line">comm_size = comm.Get_size()  </div><div class="line">   </div><div class="line"><span class="comment"># test MPI  </span></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:  </div><div class="line">    <span class="comment">#create a matrix  </span></div><div class="line">   <span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">       all_data = np.arange(<span class="number">20</span>).reshape(<span class="number">4</span>, <span class="number">5</span>)  </div><div class="line">       <span class="keyword">print</span> <span class="string">"************ data ******************"</span>  </div><div class="line">       <span class="keyword">print</span> all_data  </div><div class="line">     </div><div class="line">    <span class="comment">#broadcast the data to all processors  </span></div><div class="line">   all_data = comm.bcast(all_data <span class="keyword">if</span> comm_rank == <span class="number">0</span> <span class="keyword">else</span> <span class="keyword">None</span>, root = <span class="number">0</span>)  </div><div class="line">     </div><div class="line">    <span class="comment">#divide the data to each processor  </span></div><div class="line">   num_samples = all_data.shape[<span class="number">0</span>]  </div><div class="line">   local_data_offset = np.linspace(<span class="number">0</span>, num_samples, comm_size + <span class="number">1</span>).astype(<span class="string">'int'</span>)  </div><div class="line">     </div><div class="line">    <span class="comment">#get the local data which will be processed in this processor  </span></div><div class="line">   local_data = all_data[local_data_offset[comm_rank] :local_data_offset[comm_rank + <span class="number">1</span>]]  </div><div class="line">   <span class="keyword">print</span> <span class="string">"****** %d/%d processor gets local data ****"</span> %(comm_rank, comm_size)  </div><div class="line">   <span class="keyword">print</span> local_data  </div><div class="line">     </div><div class="line">    <span class="comment">#reduce to get sum of elements  </span></div><div class="line">   local_sum = local_data.sum()  </div><div class="line">   all_sum = comm.reduce(local_sum, root = <span class="number">0</span>, op = MPI.SUM)  </div><div class="line">     </div><div class="line">    <span class="comment">#process in local  </span></div><div class="line">   local_result = local_data ** <span class="number">2</span>  </div><div class="line">     </div><div class="line">    <span class="comment">#gather the result from all processors and broadcast it  </span></div><div class="line">   result = comm.allgather(local_result)  </div><div class="line">   result = np.vstack(result)  </div><div class="line">     </div><div class="line">   <span class="keyword">if</span> comm_rank == <span class="number">0</span>:  </div><div class="line">       <span class="keyword">print</span> <span class="string">"*** sum: "</span>, all_sum  </div><div class="line">       <span class="keyword">print</span> <span class="string">"************ result ******************"</span>  </div><div class="line">       <span class="keyword">print</span> result</div></pre></td></tr></table></figure><h1 id="MPI和mpi4py的环境搭建"><a href="#MPI和mpi4py的环境搭建" class="headerlink" title="MPI和mpi4py的环境搭建"></a>MPI和mpi4py的环境搭建</h1><p>​ 这章放到这里是作为一个附录。我们的环境是linux，需要安装的包有python、openmpi、numpy、cpython和mpi4py，过程如下：</p><h2 id="安装Python"><a href="#安装Python" class="headerlink" title="安装Python"></a>安装Python</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">tar xzvf Python-2.7.tgz  </div><div class="line">cd Python-2.7  </div><div class="line">./configure--prefix=/home/work/vis/zouxiaoyi/my_tools  </div><div class="line">make  </div><div class="line">make install</div></pre></td></tr></table></figure><p>​ 先将Python放到环境变量里面，还有Python的插件库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">exportPATH=/home/work/vis/zouxiaoyi/my_tools/bin:$PATH  </div><div class="line">exportLD_LIBRARY_PATH=/home/work/vis/zouxiaoyi/my_tools/lib:$LD_LIBRARY_PATH</div></pre></td></tr></table></figure><p>​ 执行<code>python</code>，如果看到可爱的&gt;&gt;&gt;出来，就表示成功了。按<code>crtl+d</code>退出</p><h2 id="安装openmpi"><a href="#安装openmpi" class="headerlink" title="安装openmpi"></a>安装openmpi</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">wget http://www.open-mpi.org/software/ompi/v1.4/downloads/openmpi-1.4.1.tar.gz  </div><div class="line">tar xzvf openmpi-1.4.1.tar.gz  </div><div class="line">cd openmpi-1.4.1  </div><div class="line">./configure--prefix=/home/work/vis/zouxiaoyi/my_tools  </div><div class="line">make -j 8  </div><div class="line">make install</div></pre></td></tr></table></figure><p>​ 然后把bin路径加到环境变量里面：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">exportPATH=/home/work/vis/zouxiaoyi/my_tools/bin:$PATH  </div><div class="line">exportLD_LIBRARY_PATH=/home/work/vis/zouxiaoyi/my_tools/lib:$LD_LIBRARY_PATH</div></pre></td></tr></table></figure><p>​ 执行<code>mpirun</code>，如果有帮助信息打印出来，就表示安装好了。需要注意的是，我安装了几个版本都没有成功，最后安装了1.4.1这个版本才能成功，因此就看你的人品了。</p><h2 id="安装numpy和Cython"><a href="#安装numpy和Cython" class="headerlink" title="安装numpy和Cython"></a>安装numpy和Cython</h2><p>​ 安装python库的方法可以参考<a href="http://blog.csdn.net/zouxy09/article/details/48903179" target="_blank" rel="external">之前的博客</a>。过程一般如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tar –xgvf Cython-0.20.2.tar.gz  </div><div class="line">cd Cython-0.20.2  </div><div class="line">python setup.py install</div></pre></td></tr></table></figure><p>​ 打开Python，import Cython，如果没有报错，就表示安装成功了</p><h2 id="安装mpi4py"><a href="#安装mpi4py" class="headerlink" title="安装mpi4py"></a>安装mpi4py</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tar –xgvf mpi4py_1.3.1.tar.gz  </div><div class="line">cd mpi4py  </div><div class="line">vi mpi.cfg</div></pre></td></tr></table></figure><p>​ 在68行，<code>[openmpi]</code>下面，将刚才已经安装好的openmpi的目录给改上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">mpi_dir = /home/work/vis/zouxiaoyi/my_tools  </div><div class="line">python setup.py install</div></pre></td></tr></table></figure><p>​ 打开Python，<code>import mpi4py as MPI</code>，如果没有报错，就表示安装成功了</p><p>​ 下面就可以开始属于你的并行之旅了，勇敢探索多核的乐趣吧。</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/29/A-Visual-Guide-to-Evolution-Strategies/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/29/A-Visual-Guide-to-Evolution-Strategies/" itemprop="url">A Visual Guide to Evolution Strategies</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-29T21:23:43+08:00">2018-01-29 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/29/A-Visual-Guide-to-Evolution-Strategies/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/29/A-Visual-Guide-to-Evolution-Strategies/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/29/A-Visual-Guide-to-Evolution-Strategies/" class="leancloud_visitors" data-flag-title="A Visual Guide to Evolution Strategies"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Source post is <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/" target="_blank" rel="external">here</a>.</p><p><img src="http://blog.otoro.net/assets/20171031/es_bear.jpeg" alt="img"><br><em>Survival of the fittest.</em></p><p>In this post I explain how evolution strategies (ES) work with the aid of a few visual examples. I try to keep the equations light, and I provide links to original articles if the reader wishes to understand more details. This is the first post in a series of articles, where I plan to show how to apply these algorithms to a range of tasks from MNIST, OpenAI Gym, Roboschool to PyBullet environments.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Neural network models are highly expressive and flexible, and if we are able to find a suitable set of model parameters, we can use neural nets to solve many challenging problems. Deep learning’s success largely comes from the ability to use the backpropagation algorithm to efficiently calculate the gradient of an objective function over each model parameter. With these gradients, we can efficiently search over the parameter space to find a solution that is often good enough for our neural net to accomplish difficult tasks.</p><p>However, there are many problems where the backpropagation algorithm cannot be used. For example, in reinforcement learning (RL) problems, we can also train a neural network to make decisions to perform a sequence of actions to accomplish some task in an environment. However, it is not trivial to estimate the gradient of reward signals given to the agent in the future to an action performed by the agent right now, especially if the reward is realised many timesteps in the future. Even if we are able to calculate accurate gradients, there is also the issue of being stuck in a local optimum, which exists many for RL tasks.</p><p><img src="http://blog.otoro.net/assets/20171031/biped/biped_local_optima.gif" alt="img"></p><p><em>Stuck in a local optimum.</em></p><p>A whole area within RL is devoted to studying this credit-assignment problem, and great progress has been made in recent years. However, credit assignment is still difficult when the reward signals are sparse. In the real world, rewards can be sparse and noisy. Sometimes we are given just a single reward, like a bonus check at the end of the year, and depending on our employer, it may be difficult to figure out exactly why it is so low. For these problems, rather than rely on a very noisy and possibly meaningless gradient estimate of the future to our policy, we might as well just ignore any gradient information, and attempt to use black-box optimisation techniques such as genetic algorithms (GA) or ES.</p><p>OpenAI published a paper called <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a> where they showed that evolution strategies, while being less data efficient than RL, offer many benefits. The ability to abandon gradient calculation allows such algorithms to be evaluated more efficiently. It is also easy to distribute the computation for an ES algorithm to thousands of machines for parallel computation. By running the algorithm from scratch many times, they also showed that policies discovered using ES tend to be more diverse compared to policies discovered by RL algorithms.</p><p>I would like to point out that even for the problem of identifying a machine learning model, such as designing a neural net’s architecture, is one where we cannot directly compute gradients. While <a href="https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html" target="_blank" rel="external">RL</a>, <a href="https://arxiv.org/abs/1703.00548" target="_blank" rel="external">Evolution</a>, <a href="http://blog.otoro.net/2016/05/07/backprop-neat/" target="_blank" rel="external">GA</a> etc., can be applied to search in the space of model architectures, in this post, I will focus only on applying these algorithms to search for parameters of a pre-defined model.</p><h2 id="What-is-an-Evolution-Strategy"><a href="#What-is-an-Evolution-Strategy" class="headerlink" title="What is an Evolution Strategy?"></a>What is an Evolution Strategy?</h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/8/8b/Rastrigin_function.png" alt="img"><br><em>Two-dimensional Rastrigin function has many local optima (Source: Wikipedia</em>).</p><p>The diagrams below are top-down plots of <em>shifted</em> 2D <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization" target="_blank" rel="external">Schaffer and Rastrigin</a> functions, two of several simple toy problems used for testing continuous black-box optimisation algorithms. Lighter regions of the plots represent higher values of $F(x,y)$. As you can see, there are many local optimums in this function. Our job is to find a set of <em>model parameters</em> $(x, y)$, such that $F(x,y)$ is as close as possible to the global maximum.</p><p><em>Schaffer-2D Function</em><br><img src="http://blog.otoro.net/assets/20171031/schaffer/schaffer_label.png" alt="img"></p><p><em>Rastrigin-2D Function</em><br><img src="http://blog.otoro.net/assets/20171031/rastrigin/rastrigin_label.png" alt="img"></p><p>Although there are many definitions of evolution strategies, we can define an evolution strategy as an algorithm that provides the user a set of candidate solutions to evaluate a problem. The evaluation is based on an <em>objective function</em> that takes a given solution and returns a single <em>fitness</em> value. Based on the fitness results of the current solutions, the algorithm will then produce the next generation of candidate solutions that is more likely to produce even better results than the current generation. The iterative process will stop once the best known solution is satisfactory for the user.</p><p>Given an evolution strategy algorithm called <code>EvolutionStrategy</code>, we can use in the following way:</p><hr><p><code>solver = EvolutionStrategy()</code></p><p><code>while True:</code></p><p><code># ask the ES to give us a set of candidate solutions</code><br><code>solutions = solver.ask()</code></p><p><code># create an array to hold the fitness results.</code><br><code>fitness_list = np.zeros(solver.popsize)</code></p><p><code># evaluate the fitness for each given solution.</code><br><code>for i in range(solver.popsize):</code><br><code>fitness_list[i] = evaluate(solutions[i])</code></p><p><code># give list of fitness results back to ES</code><br><code>solver.tell(fitness_list)</code></p><p><code># get best parameter, fitness from ES</code><br><code>best_solution, best_fitness = solver.result()</code></p><p><code>if best_fitness &gt; MY_REQUIRED_FITNESS:</code><br><code>break</code></p><hr><p>Although the size of the population is usually held constant for each generation, they don’t need to be. The ES can generate as many candidate solutions as we want, because the solutions produced by an ES are <em>sampled</em> from a distribution whose parameters are being updated by the ES at each generation. I will explain this sampling process with an example of a simple evolution strategy.</p><h2 id="Simple-Evolution-Strategy"><a href="#Simple-Evolution-Strategy" class="headerlink" title="Simple Evolution Strategy"></a>Simple Evolution Strategy</h2><p>One of the simplest evolution strategy we can imagine will just sample a set of solutions from a Normal distribution, with a mean \muμand a fixed standard deviation \sigmaσ. In our 2D problem, \mu = (\mu_x, \mu_y)μ=(μx,μy) and \sigma = (\sigma_x, \sigma_y)σ=(σx,σy). Initially, \muμ is set at the origin. After the fitness results are evaluated, we set \muμ to the best solution in the population, and sample the next generation of solutions around this new mean. This is how the algorithm behaves over 20 generations on the two problems mentioned earlier:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/simplees.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/simplees.gif" alt="img"></p><p>In the visualisation above, the green dot indicates the mean of the distribution at each generation, the blue dots are the sampled solutions, and the red dot is the best solution found so far by our algorithm.</p><p>This simple algorithm will generally only work for simple problems. Given its greedy nature, it throws away all but the best solution, and can be prone to be stuck at a local optimum for more complicated problems. It would be beneficial to sample the next generation from a probability distribution that represents a more diverse set of ideas, rather than just from the best solution from the current generation.</p><h2 id="Simple-Genetic-Algorithm"><a href="#Simple-Genetic-Algorithm" class="headerlink" title="Simple Genetic Algorithm"></a>Simple Genetic Algorithm</h2><p>One of the oldest black-box optimisation algorithms is the genetic algorithm. There are many variations with many degrees of sophistication, but I will illustrate the simplest version here.</p><p>The idea is quite simple: keep only 10% of the best performing solutions in the current generation, and let the rest of the population die. In the next generation, to sample a new solution is to randomly select two solutions from the survivors of the previous generation, and recombine their parameters to form a new solution. This <em>crossover</em> recombination process uses a coin toss to determine which parent to take each parameter from. In the case of our 2D toy function, our new solution might inherit xx or yy from either parents with 50% chance. Gaussian noise with a fixed standard deviation will also be injected into each new solution after this recombination process.</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/simplega.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/simplega.gif" alt="img"></p><p>The figure above illustrates how the simple genetic algorithm works. The green dots represent members of the elite population from the previous generation, the blue dots are the offsprings to form the set of candidate solutions, and the red dot is the best solution.</p><p>Genetic algorithms help diversity by keeping track of a diverse set of candidate solutions to reproduce the next generation. However, in practice, most of the solutions in the elite surviving population tend to converge to a local optimum over time. There are more sophisticated variations of GA out there, such as <a href="http://people.idsia.ch/~juergen/gomez08a.pdf" target="_blank" rel="external">CoSyNe</a>, <a href="http://blog.otoro.net/2015/03/10/esp-algorithm-for-double-pendulum/" target="_blank" rel="external">ESP</a>, and <a href="http://blog.otoro.net/2016/05/07/backprop-neat/" target="_blank" rel="external">NEAT</a>, where the idea is to cluster similar solutions in the population together into different species, to maintain better diversity over time.</p><h2 id="Covariance-Matrix-Adaptation-Evolution-Strategy-CMA-ES"><a href="#Covariance-Matrix-Adaptation-Evolution-Strategy-CMA-ES" class="headerlink" title="Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)"></a>Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)</h2><p>A shortcoming of both the Simple ES and Simple GA is that our standard deviation noise parameter is fixed. There are times when we want to explore more and increase the standard deviation of our search space, and there are times when we are confident we are close to a good optima and just want to fine tune the solution. We basically want our search process to behave like this:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/cmaes.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes.gif" alt="img"></p><p>Amazing isn’it it? The search process shown in the figure above is produced by <a href="https://en.wikipedia.org/wiki/CMA-ES" target="_blank" rel="external">Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)</a>. CMA-ES an algorithm that can take the results of each generation, and adaptively increase or decrease the search space for the next generation. It will not only adapt for the mean $\mu$ and sigma $\sigma$ parameters, but will calculate the entire covariance matrix of the parameter space. At each generation, CMA-ES provides the parameters of a multi-variate normal distribution to sample solutions from. So how does it know how to increase or decrease the search space?</p><p>Before we discuss its methodology, let’s review how to estimate a <a href="https://en.wikipedia.org/wiki/Covariance_matrix" target="_blank" rel="external">covariance matrix</a>. This will be important to understand CMA-ES’s methodology later on. If we want to estimate the covariance matrix of our entire sampled population of size of $N$, we can do so using the set of equations below to calculate the maximum likelihood estimate of a covariance matrix $C$. We first calculate the means of each of the $x_i$ and $y_i$ in our population:<br>$$<br>\mu_x = \frac{1}{N} \sum_{i=1}^{N}x_i,<br>$$</p><p>$$<br>\mu_y = \frac{1}{N} \sum_{i=1}^{N}y_i.<br>$$</p><p>The terms of the 2x2 covariance matrix $C$ will be:<br>$$<br>\begin{align}<br>\sigma_x^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)^2, \\<br>\sigma_y^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(y_i - \mu_y)^2, \\<br>\sigma_{xy} &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y).<br>\end{align}<br>$$<br>Of course, these resulting mean estimates $\mu_x$ and $\mu_y$, and covariance terms $\sigma_x$, $\sigma_y$, $\sigma_{xy}$ will just be an estimate to the actual covariance matrix that we originally sampled from, and not particularly useful to us.</p><p>CMA-ES modifies the above covariance calculation formula in a clever way to make it adapt well to an optimisation problem. I will go over how it does this step-by-step. Firstly, it focuses on the best $N_{best}$ solutions in the current generation. For simplicity let’s set $N_{best}$ to be the best 25% of solutions. After sorting the solutions based on fitness, we calculate the mean $\mu^{(g+1)}$ of the next generation $(g+1)$ as the average of only the best 25% of the solutions in current population $(g)$, i.e.:<br>$$<br>\begin{align}<br>\mu_x^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}x_i, \\<br>\mu_y^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}y_i.<br>\end{align}<br>$$<br>Next, we use only the best 25% of the solutions to estimate the covariance matrix $C^{(g+1)}$ of the next generation, but the clever <em>hack</em> here is that it uses the <em>current</em> generation’s $\mu^{(g)}$, rather than the updated $\mu^{(g+1)}$ parameters that we had just calculated, in the calculation:<br>$$<br>\begin{align}<br>\sigma_x^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})^2, \\<br>\sigma_y^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(y_i - \mu_y^{(g)})^2, \\<br>\sigma_{xy}^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})(y_i - \mu_y^{(g)}).<br>\end{align}<br>$$<br>Armed with a set of $\mu_x$, $\mu_y$, $\sigma_x$, $\sigma_y$, and $\sigma_{xy}$ parameters for the next generation $(g+1)$, we can now sample the next generation of candidate solutions.</p><p>Below is a set of figures to visually illustrate how it uses the results from the current generation $(g)$ to construct the solutions in the next generation $(g+1)$:</p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step1.png" alt="img"></p><p><em>Step 1</em></p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step2.png" alt="img"></p><p><em>Step 2</em></p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step3.png" alt="img"></p><p><em>Step 3</em></p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes_step4.png" alt="img"></p><p><em>Step 4</em></p><ol><li>Calculate the fitness score of each candidate solution in generation $(g)$.</li><li>Isolates the best 25% of the population in generation $(g)$, in purple.</li><li>Using only the best solutions, along with the mean $\mu^{(g)}$ of the current generation (the green dot), calculate the covariance matrix $C^{(g+1)}$ of the next generation.</li><li>Sample a new set of candidate solutions using the updated mean $\mu^{(g+1)}$ and covariance matrix $C^{(g+1)}$.</li></ol><p>Let’s visualise the scheme one more time, on the entire search process on both problems:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/cmaes2.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/cmaes2.gif" alt="img"></p><p>Because CMA-ES can adapt both its mean and covariance matrix using information from the best solutions, it can decide to cast a wider net when the best solutions are far away, or narrow the search space when the best solutions are close by. My description of the CMA-ES algorithm for a 2D toy problem is highly simplified to get the idea across. For more details, I suggest reading the <a href="https://arxiv.org/abs/1604.00772" target="_blank" rel="external">CMA-ES Tutorial</a> prepared by Nikolaus Hansen, the author of CMA-ES.</p><p>This algorithm is one of the most popular gradient-free optimisation algorithms out there, and has been the algorithm of choice for many researchers and practitioners alike. The only real drawback is the performance if the number of model parameters we need to solve for is large, as the covariance calculation is $O(N^2)$, although recently there has been approximations to make it $O(N)$. CMA-ES is my algorithm of choice when the search space is less than a thousand parameters. I found it still usable up to ~ 10K parameters if I’m willing to be patient.</p><h2 id="Natural-Evolution-Strategies"><a href="#Natural-Evolution-Strategies" class="headerlink" title="Natural Evolution Strategies"></a>Natural Evolution Strategies</h2><hr><p><em>Imagine if you had built an artificial life simulator, and you sample a different neural network to control the behavior of each ant inside an ant colony. Using the Simple Evolution Strategy for this task will optimise for traits and behaviours that benefit individual ants, and with each successive generation, our population will be full of alpha ants who only care about their own well-being.</em></p><p><em>Instead of using a rule that is based on the survival of the fittest ants, what if you take an alternative approach where you take the sum of all fitness values of the entire ant population, and optimise for this sum instead to maximise the well-being of the entire ant population over successive generations? Well, you would end up creating a Marxist utopia.</em></p><hr><p>A perceived weakness of the algorithms mentioned so far is that they discard the majority of the solutions and only keep the best solutions. Weak solutions contain information about what <em>not</em> to do, and this is valuable information to calculate a better estimate for the next generation.</p><p>Many people who studied RL are familiar with the <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> paper. In this 1992 paper, Williams outlined an approach to estimate the gradient of the expected rewards with respect to the model parameters of a policy neural network. This paper also proposed using REINFORCE as an Evolution Strategy, in Section 6 of the paper. This special case of <em>REINFORCE-ES</em> was expanded later on in <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Parameter-Exploring Policy Gradients</a> (PEPG, 2009) and <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a> (NES, 2014).</p><p>In this approach, we want to use all of the information from each member of the population, good or bad, for estimating a gradient signal that can move the entire population to a better direction in the next generation. Since we are estimating a gradient, we can also use this gradient in a standard SGD update rule typically used for deep learning. We can even use this estimated gradient with Momentum SGD, RMSProp, or Adam if we want to.</p><p>The idea is to maximise the <em>expected value</em> of the fitness score of a sampled solution. If the expected result is good enough, then the best performing member within a sampled population will be even better, so optimising for the expectation might be a sensible approach. Maximising the expected fitness score of a sampled solution is almost the same as maximising the total fitness score of the entire population.</p><p>If $z$ is a solution vector sampled from a probability distribution function $\pi(z, \theta)$, we can define the expected value of the objective function $F$ as:<br>$$<br>J(\theta) = E_{\theta}[F(z)] = \int F(z) \; \pi(z, \theta) \; dz,<br>$$<br>where $\theta$ are the parameters of the probability distribution function. For example, if $\pi$ is a normal distribution, then $\theta$ would be \muμand $\sigma$. For our simple 2D toy problems, each ensemble $z$ is a 2D vector $(x, y)$.</p><p>The <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">NES paper</a> contains a nice derivation of the gradient of $J(\theta)$ with respect to $\theta$. Using the same <em>log-likelihood trick</em> as in the REINFORCE algorithm allows us to calculate the gradient of $J(\theta)$:<br>$$<br>\nabla_{\theta} J(\theta) = E_{\theta}[ \; F(z) \; \nabla_{\theta} \log \pi(z, \theta) \; ].<br>$$<br>In a population size of $N$, where we have solutions $z^1, z^2, … z^N$, we can estimate this gradient as a summation:<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).<br>$$<br>With this gradient $\nabla_{\theta} J(\theta)$, we can use a learning rate parameter \alphaα (such as 0.01) and start optimising the $\theta$ parameters of pdf $\pi$ so that our sampled solutions will likely get higher fitness scores on the objective function $F$. Using SGD (or Adam), we can update $\theta$ for the next generation:<br>$$<br>\theta \rightarrow \theta + \alpha \nabla_{\theta} J(\theta),<br>$$<br>and sample a new set of candidate solutions $z$ from this updated pdf, and continue until we arrive at a satisfactory solution.</p><p>In Section 6 of the <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> paper, Williams derived closed-form formulas of the gradient $\nabla_{\theta} \log \pi(z^i, \theta)$, for the special case where $ \pi(z, \theta)$ is a factored multi-variate normal distribution (i.e., the correlation parameters are zero). In this special case, $\theta$ are the $\mu$ and $\sigma$ vectors. Therefore, each element of a solution can be sampled from a univariate normal distribution $z_j \sim N(\mu_j, \sigma_j)$.</p><p>The closed-form formulas for $\nabla_{\theta} \log N(z^i, \theta)$, for each individual element of vector $\theta$ on each solution $i$ in the population can be derived as:<br>$$<br>\nabla_{\mu_{j}} \log N(z^i, \mu, \sigma) = \frac{z_j^i - \mu_j}{\sigma_j},<br>$$</p><p>$$<br>\nabla_{\sigma_{j}} \log N(z^i, \mu, \sigma) = \frac{(z_j^i - \mu_j)^2 - \sigma_j^2}{\sigma_j^3}.<br>$$</p><p>For clarity, I use the index of jj, to count across parameter space, and this is not to be confused with superscript $i$, used to count across each sampled member of the population. For our 2D problems, $z_1 = x, z_2 = y, \mu_1 = \mu_x, \mu_2 = \mu_y, \sigma_1 = \sigma_x, \sigma_2 = \sigma_y$ in this context.</p><p>These two formulas can be plugged back into the approximate gradient formula to derive explicit update rules for \muμ and \sigmaσ. In the papers mentioned above, they derived more explicit update rules, incorporated a <em>baseline</em>, and introduced other tricks such as antithetic sampling in PEPG, which is what my implementation is based on. NES proposed incorporating the inverse of the Fisher Information Matrix into the gradient update rule. But the concept is basically the same as other ES algorithms, where we update the mean and standard deviation of a multi-variate normal distribution at each new generation, and sample a new set of solutions from the updated distribution. Below is a visualization of this algorithm in action, following the formulas described above:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/pepg.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/pepg.gif" alt="img"></p><p>We see that this algorithm is able to dynamically change the $\sigma$’s to explore or fine tune the solution space as needed. Unlike CMA-ES, there is no correlation structure in our implementation, so we don’t get the diagonal ellipse samples, only the vertical or horizontal ones, although in principle we can derive update rules to incorporate the entire covariance matrix if we needed to, at the expense of computational efficiency.</p><p>I like this algorithm because like CMA-ES, the $\sigma$’s can adapt so our search space can be expanded or narrowed over time. Because the correlation parameter is not used in this implementation, the efficiency of the algorithm is $O(N)$ so I use PEPG if the performance of CMA-ES becomes an issue. I usually use PEPG when the number of model parameters exceed several thousand.</p><h2 id="OpenAI-Evolution-Strategy"><a href="#OpenAI-Evolution-Strategy" class="headerlink" title="OpenAI Evolution Strategy"></a>OpenAI Evolution Strategy</h2><p>In OpenAI’s <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">paper</a>, they implement an evolution strategy that is a special case of the REINFORCE-ES algorithm outlined earlier. In particular, \sigmaσ is fixed to a constant number, and only the \muμ parameter is updated at each generation. Below is how this strategy looks like, with a constant \sigmaσ parameter:</p><p><img src="http://blog.otoro.net/assets/20171031/schaffer/openes.gif" alt="img"><img src="http://blog.otoro.net/assets/20171031/rastrigin/oes.gif" alt="img"></p><p>In addition to the simplification, this paper also proposed a modification of the update rule that is suitable for parallel computation across different worker machines. In their update rule, a large grid of random numbers have been pre-computed using a fixed seed. By doing this, each worker can reproduce the parameters of every other worker over time, and each worker needs only to communicate a single number, the final fitness result, to all of the other workers. This is important if we want to scale evolution strategies to thousands or even a million workers located on different machines, since while it may not be feasible to transmit an entire solution vector a million times at each generation update, it may be feasible to transmit only the final fitness results. In the paper, they showed that by using 1440 workers on Amazon EC2 they were able to solve the Mujoco Humanoid walking task in ~ 10 minutes.</p><p>I think in principle, this parallel update rule should work with the original algorithm where they can also adapt $\sigma$, but perhaps in practice, they wanted to keep the number of moving parts to a minimum for large-scale parallel computing experiments. This inspiring paper also discussed many other practical aspects of deploying ES for RL-style tasks, and I highly recommend going through it to learn more.</p><h2 id="Fitness-Shaping"><a href="#Fitness-Shaping" class="headerlink" title="Fitness Shaping"></a>Fitness Shaping</h2><p>Most of the algorithms above are usually combined with a <em>fitness shaping</em> method, such as the rank-based fitness shaping method I will discuss here. Fitness shaping allows us to avoid outliers in the population from dominating the approximate gradient calculation mentioned earlier:<br>$$<br>\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).<br>$$<br>If a particular $F(z^m)$ is much larger than other $F(z^i)$ in the population, then the gradient might become dominated by this outliers and increase the chance of the algorithm being stuck in a local optimum. To mitigate this, one can apply a rank transformation of the fitness. Rather than use the actual fitness function, we would rank the results and use an augmented fitness function which is proportional to the solution’s rank in the population. Below is a comparison of what the original set of fitness may look like, and what the ranked fitness looks like:</p><p><img src="http://blog.otoro.net/assets/20171031/ranked_fitness.svg" alt="img"></p><p>What this means is supposed we have a population size of 101. We would evaluate each population to the actual fitness function, and then sort the solutions based by their fitness. We will assign an augmented fitness value of -0.50 to the worse performer, -0.49 to the second worse solution, …, 0.49 to the second best solution, and finally a fitness value of 0.50 to the best solution. This augmented set of fitness values will be used to calculate the gradient update, instead of the actual fitness values. In a way, it is a similar to just applying Batch Normalization to the results, but more direct. There are alternative methods for fitness shaping but they all basically give similar results in the end.</p><p>I find fitness shaping to be very useful for RL tasks if the objective function is non-deterministic for a given policy network, which is often the cases on RL environments where maps are randomly generated and various opponents have random policies. It is less useful for optimising for well-behaved functions that are deterministic, and the use of fitness shaping can sometimes slow down the time it takes to find a good solution.</p><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><p>Although ES might be a way to search for more novel solutions that are difficult for gradient-based methods to find, it still vastly underperforms gradient-based methods on many problems where we can calculate high quality gradients. For instance, only an idiot would attempt to use a genetic algorithm for image classification. But sometimes <a href="https://blog.openai.com/nonlinear-computation-in-linear-networks/" target="_blank" rel="external">such people</a> do exist in the world, and sometimes these explorations can be fruitful!</p><p>Since all ML algorithms should be tested on MNIST, I also tried to apply these various ES algorithms to find weights for a small, simple 2-layer convnet used to classify MNIST, just to see where we stand compared to SGD. The convnet only has ~ 11k parameters so we can accommodate the slower CMA-ES algorithm. The code and the experiments are available <a href="https://github.com/hardmaru/pytorch_notebooks/tree/master/mnist_es" target="_blank" rel="external">here</a>.</p><p>Below are the results for various ES methods, using a population size of 101, over 300 epochs. We keep track of the model parameters that performed best on the entire training set at the end of each epoch, and evaluate this model once on the test set after 300 epochs. It is interesting how sometimes the test set’s accuracy is higher than the training set for the models that have lower scores.</p><table><thead><tr><th>Method</th><th>Train Set</th><th>Test Set</th></tr></thead><tbody><tr><td>Adam (BackProp) Baseline</td><td>99.8</td><td>98.9</td></tr><tr><td>Simple GA</td><td>82.1</td><td>82.4</td></tr><tr><td>CMA-ES</td><td>98.4</td><td>98.1</td></tr><tr><td>OpenAI-ES</td><td>96.0</td><td>96.2</td></tr><tr><td>PEPG</td><td>98.5</td><td>98.0</td></tr></tbody></table><p><img src="http://blog.otoro.net/assets/20171031/mnist_results.svg" alt="img"></p><p>We should take these results with a grain of salt, since they are based on a single run, rather than the average of 5-10 runs. The results based on a single-run seem to indicate that CMA-ES is the best at the MNIST task, but the PEPG algorithm is not that far off. Both of these algorithms achieved ~ 98% test accuracy, 1% lower than the SGD/ADAM baseline. Perhaps the ability to dynamically alter its covariance matrix, and standard deviation parameters over each generation allowed it to fine-tune its weights better than OpenAI’s simpler variation.</p><h2 id="Try-It-Yourself"><a href="#Try-It-Yourself" class="headerlink" title="Try It Yourself"></a>Try It Yourself</h2><p>There are probably open source implementations of all of the algorithms described in this article. The author of CMA-ES, Nikolaus Hansen, has been maintaining a numpy-based implementation of <a href="https://github.com/CMA-ES/pycma" target="_blank" rel="external">CMA-ES</a> with lots of bells and whistles. His python implementation introduced me to the training loop interface described earlier. Since this interface is quite easy to use, I also implemented the other algorithms such as Simple Genetic Algorithm, PEPG, and OpenAI’s ES using the same interface, and put it in a small python file called <code>es.py</code>, and also wrapped the original CMA-ES library in this small library. This way, I can quickly compare different ES algorithms by just changing one line:</p><hr><p><code>import es</code></p><p><code>#solver = es.SimpleGA(...)</code><br><code>#solver = es.PEPG(...)</code><br><code>#solver = es.OpenES(...)</code><br><code>solver = es.CMAES(...)</code></p><p><code>while True:</code></p><p><code>solutions = solver.ask()</code></p><p><code>fitness_list = np.zeros(solver.popsize)</code></p><p><code>for i in range(solver.popsize):</code><br><code>fitness_list[i] = evaluate(solutions[i])</code></p><p><code>solver.tell(fitness_list)</code></p><p><code>result = solver.result()</code></p><p><code>if result[1] &gt; MY_REQUIRED_FITNESS:</code><br><code>break</code></p><hr><p>You can look at <code>es.py</code> on <a href="https://github.com/hardmaru/estool/blob/master/es.py" target="_blank" rel="external">GitHub</a> and the IPython notebook <a href="https://github.com/hardmaru/estool/blob/master/simple_es_example.ipynb" target="_blank" rel="external">examples</a> using the various ES algorithms.</p><p>In this <a href="https://github.com/hardmaru/estool/blob/master/simple_es_example.ipynb" target="_blank" rel="external">IPython notebook</a> that accompanies <code>es.py</code>, I show how to use the ES solvers in <code>es.py</code> to solve a 100-Dimensional version of the Rastrigin function with even more local optimum points. The 100-D version is somewhat more challenging than the trivial 2D version used to produce the visualizations in this article. Below is a comparison of the performance for various algorithms discussed:</p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin10d.svg" alt="img"></p><p>On this 100-D Rastrigin problem, none of the optimisers got to the global optimum solution, although CMA-ES comes close. CMA-ES blows everything else away. PEPG is in 2nd place, and OpenAI-ES / Genetic Algorithm falls behind. I had to use an annealing schedule to gradually lower \sigmaσ for OpenAI-ES to make it perform better for this task.</p><p><img src="http://blog.otoro.net/assets/20171031/rastrigin_cma_solution.png" alt="img"></p><p><em>Final solution that CMA-ES discovered for 100-D Rastrigin function.Global optimal solution is a 100-dimensional vector of exactly 10.</em></p><h2 id="References-and-Other-Links"><a href="#References-and-Other-Links" class="headerlink" title="References and Other Links"></a>References and Other Links</h2><p>Below are a few links to information related to evolutionary computing which I found useful or inspiring.</p><p>Image Credits of <a href="https://www.reddit.com/r/CryptoMarkets/comments/6qpla3/investing_in_icos_results_may_vary/" target="_blank" rel="external">Lemmings Jumping off a Cliff</a>. Your results may vary when investing in ICOs.</p><p>CMA-ES: <a href="https://github.com/CMA-ES" target="_blank" rel="external">Official Reference Implementation</a> on GitHub, <a href="https://arxiv.org/abs/1604.00772" target="_blank" rel="external">Tutorial</a>, Original CMA-ES <a href="http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmaartic.pdf" target="_blank" rel="external">Paper</a> from 2001, Overview <a href="https://www.slideshare.net/OsamaSalaheldin2/cmaes-presentation" target="_blank" rel="external">Slides</a></p><p><a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</a> (REINFORCE), 1992.</p><p><a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=A64D1AE8313A364B814998E9E245B40A?doi=10.1.1.180.7104&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Parameter-Exploring Policy Gradients</a>, 2009.</p><p><a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a>, 2014.</p><p><a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a>, OpenAI, 2017.</p><p>Risto Miikkulainen’s <a href="http://nn.cs.utexas.edu/downloads/slides/miikkulainen.ijcnn13.pdf" target="_blank" rel="external">Slides</a> on Neuroevolution.</p><p>A Neuroevolution Approach to <a href="http://www.cs.utexas.edu/~ai-lab/?atari" target="_blank" rel="external">General Atari Game Playing</a>, 2013.</p><p>Kenneth Stanley’s Talk on <a href="https://youtu.be/dXQPL9GooyI" target="_blank" rel="external">Why Greatness Cannot Be Planned: The Myth of the Objective</a>, 2015.</p><p><a href="https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning" target="_blank" rel="external">Neuroevolution</a>: A Different Kind of Deep Learning. The quest to evolve neural networks through evolutionary algorithms.</p><p><a href="http://people.idsia.ch/~juergen/compressednetworksearch.html" target="_blank" rel="external">Compressed Network Search</a> Finds Complex Neural Controllers with a Million Weights.</p><p>Karl Sims <a href="https://youtu.be/JBgG_VSP7f8" target="_blank" rel="external">Evolved Virtual Creatures</a>, 1994.</p><p>Evolved <a href="https://youtu.be/euFvRfQRbLI" target="_blank" rel="external">Step Climbing</a> Creatures.</p><p>Super Mario World Agent <a href="https://youtu.be/qv6UVOQ0F44" target="_blank" rel="external">Mario I/O</a>, Mario Kart 64 <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/(https://github.com/nicknlsn/MarioKart64NEAT" target="_blank" rel="external">Controller using</a>) using <a href="https://www.cs.ucf.edu/~kstanley/neat.html" target="_blank" rel="external">NEAT Algorithm</a>.</p><p><a href="http://www.bionik.tu-berlin.de/institut/xstart.htm" target="_blank" rel="external">Ingo Rechenberg</a>, the inventor of Evolution Strategies.</p><p>A Tutorial on <a href="https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/" target="_blank" rel="external">Differential Evolution</a> with Python.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/23/Introduction-to-Genetic-Algorithm/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/23/Introduction-to-Genetic-Algorithm/" itemprop="url">Introduction to Genetic Algorithm</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-23T15:13:09+08:00">2018-01-23 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/23/Introduction-to-Genetic-Algorithm/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/23/Introduction-to-Genetic-Algorithm/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/23/Introduction-to-Genetic-Algorithm/" class="leancloud_visitors" data-flag-title="Introduction to Genetic Algorithm"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-Intuition-behind-Genetic-Algorithms"><a href="#1-Intuition-behind-Genetic-Algorithms" class="headerlink" title="1. Intuition behind Genetic Algorithms"></a>1. Intuition behind Genetic Algorithms</h2><p>Let’s start with the famous quote by Charles Darwin:</p><blockquote><p><em>It is not the strongest of the species that survives, nor the most intelligent , but the one most responsive to change.</em></p></blockquote><p>You must be thinking what has this quote got to do with genetic algorithm? Actually, the entire concept of a genetic algorithm is based on the above line.</p><p>Let us understand with a basic example:</p><p>Let’s take a hypothetical situation where, you are head of a country, and in order to keep your city safe from bad things, you implement a policy like this.</p><ul><li>You select all the good people, and ask them to extend their generation by having their children.</li><li>This repeats for a few generations.</li><li>You will notice that now you have an entire population of good people.</li></ul><p>Now, that may not be entirely possible, but this example was just to help you understand the concept. So the basic idea was that we changed the input (i.e. population) such that we get better output (i.e. better country).</p><p>Now, I suppose you have got some intuition that the concept of a genetic algorithm is somewhat related to biology. So let’s us quickly grasp some little concepts, so that we can draw a parallel line between them.</p><h2 id="2-Biological-Inspiration"><a href="#2-Biological-Inspiration" class="headerlink" title="2. Biological Inspiration"></a>2. Biological Inspiration</h2><p>I am sure you would remember:</p><p><em>Cells are the basic building block of all living things.</em></p><p>Therefore in each cell, there is the same set of chromosomes. Chromosome are basically the strings of DNA.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153917/dna-300x194.png" alt="img"></p><p>Traditionally, these chromosomes are represented in binary as strings of 0’s and 1’s.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153928/gene.png" alt="img"></p><p>Source : <a href="https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_fundamentals.htm" target="_blank" rel="external">link</a></p><p>A chromosome consists of genes, commonly referred as blocks of DNA, where each gene encodes a specific trait, for example hair color or eye color.</p><p>I wanted you to recall these basics concept of biology before going further. Let’s get back and understand what actually is a genetic algorithm?</p><h2 id="3-What-is-a-Genetic-Algorithm"><a href="#3-What-is-a-Genetic-Algorithm" class="headerlink" title="3. What is a Genetic Algorithm?"></a>3. What is a Genetic Algorithm?</h2><p>Let’s get back to the example we discussed above and summarize what we did.</p><ol><li>Firstly, we defined our initial population as our countrymen.</li><li>We defined a function to classify whether is a person is good or bad.</li><li>Then we selected good people for mating to produce their off-springs.</li><li>And finally, these off-springs replace the bad people from the population and this process repeats.</li></ol><p>This is how genetic algorithm actually works, which basically tries to mimic the human evolution to some extent.</p><p>So to formalize a definition of a genetic algorithm, we can say that it is an optimization technique, which tries to find out such values of input so that we get the best output values or results.</p><p>The working of a genetic algorithm is also derived from biology, which is as shown in the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154007/steps-210x300.png" alt="img"></p><p>Source: <a href="https://www.neuraldesigner.com/blog/genetic_algorithms_for_feature_selection" target="_blank" rel="external">link</a></p><p>So, let us try to understand the steps one by one.</p><h2 id="4-Steps-Involved-in-Genetic-Algorithm"><a href="#4-Steps-Involved-in-Genetic-Algorithm" class="headerlink" title="4. Steps Involved in Genetic Algorithm"></a><strong>4. Steps Involved in Genetic Algorithm</strong></h2><p>Here, to make things easier, let us understand it by the famous <a href="https://en.wikipedia.org/wiki/Knapsack_problem" target="_blank" rel="external">Knapsack problem</a>.</p><p>If you haven’t come across this problem, let me introduce my version of this problem.</p><p>Let’s say, you are going to spend a month in the wilderness. Only thing you are carrying is the backpack which can hold a maximum weight of <strong>30 kg</strong>. Now you have different survival items, each having its own “Survival Points” (which are given for each item in the table). So, your objective is maximise the survival points.</p><p>Here is the table giving details about each item.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154400/table1-300x99.png" alt="img"></p><h3 id="4-1-Initialisation"><a href="#4-1-Initialisation" class="headerlink" title="4.1 Initialisation"></a>4.1 Initialisation</h3><p>To solve this problem using genetic algorithm, our first step would be defining our population. So our population will contain individuals, each having their own set of chromosomes.</p><p>We know that, chromosomes are binary strings, where for this problem 1 would mean that the following item is taken and 0 meaning that it is dropped.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22170200/Capture1-300x186.png" alt="img"></p><p>This set of chromosome is considered as our initial population.</p><h3 id="4-2-Fitness-Function"><a href="#4-2-Fitness-Function" class="headerlink" title="4.2 Fitness Function"></a>4.2 Fitness Function</h3><p>Let us calculate fitness points for our first two chromosomes.</p><p>For A1 chromosome [100110],</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/24105541/table-300x74.png" alt="img"></p><p>Similarly for A2 chromosome [001110],</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154417/table3-300x74.png" alt="img"></p><p>So, for this problem, our chromosome will be considered as more fit when it contains more survival points.</p><p>Therefore chromosome 1 is more fit than chromosome 2.</p><h3 id="4-3-Selection"><a href="#4-3-Selection" class="headerlink" title="4.3 Selection"></a>4.3 Selection</h3><p>Now, we can select fit chromosomes from our population which can mate and create their off-springs.</p><p>General thought is that we should select the fit chromosomes and allow them to produce off-springs. But that would lead to chromosomes that are more close to one another in a few next generation, and therefore less diversity.</p><p>Therefore, we generally use Roulette Wheel Selection method.</p><p>Don’t be afraid of name, just take a look at the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153953/roulette-wheel-300x188.jpg" alt="img"></p><p>I suppose we all have seen this, either in real or in movies. So, let’s build our roulette wheel.</p><p>Consider a wheel, and let’s divide that into m divisions, where m is the number of chromosomes in our populations. The area occupied by each chromosome will be proportional to its fitness value.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22155317/table4-300x72.png" alt="img"></p><p>Based on these values, let us create our roulette wheel.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22171149/roulette-300x204.png" alt="img"></p><p>So, now this wheel is rotated and the region of wheel which comes in front of the fixed point is chosen as the parent. For the second parent, the same process is repeated.</p><p>Sometimes we mark two fixed point as shown in the figure below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22171418/stio-300x194.png" alt="img"></p><p>So, in this method we can get both our parents in one go. This method is known as Stochastic Universal Selection method.</p><h3 id="4-4-Crossover"><a href="#4-4-Crossover" class="headerlink" title="4.4 Crossover"></a>4.4 Crossover</h3><p>So in this previous step, we have selected parent chromosomes that will produce off-springs. So in biological terms, crossover is nothing but reproduction.</p><p>So let us find the crossover of chromosome 1 and 4, which were selected in the previous step. Take a look at the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22173337/one-point-300x93.png" alt="img"></p><p>This is the most basic form of crossover, known as one point crossover. Here we select a random crossover point and the tails of both the chromosomes are swapped to produce a new off-springs.</p><p>If you take two crossover point, then it will called as multi point crossover which is as shown below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22174145/multi-point-300x101.png" alt="img"></p><h3 id="4-5-Mutation"><a href="#4-5-Mutation" class="headerlink" title="4.5 Mutation"></a>4.5 Mutation</h3><p>Now if you think in the biological sense, are the children produced have the same traits as their parents? The answer is NO. During their growth, there is some change in the genes of children which makes them different from its parents.</p><p>This process is known as mutation, which may be defined as a random tweak in the chromosome, which also promotes the idea of diversity in the population.</p><p>A simple method of mutation is shown in the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22174928/mutation-300x56.png" alt="img"></p><p>So the entire process is summarise as shown in the figure.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22175311/gadiagram-300x196.png" alt="img"></p><p>Source : <a href="http://www.jade-cheng.com/au/coalhmm/optimization/" target="_blank" rel="external">link</a></p><p>The off-springs thus produced are again validated using our fitness function, and if considered fit then will replace the less fit chromosomes from the population.</p><p>But the question is how we will get to know that we have reached our best possible solution?</p><p>So basically there are different termination conditions, which are listed below:</p><ol><li>There is no improvement in the population for over x iterations.</li><li>We have already predefined an absolute number of generation for our algorithm.</li><li>When our fitness function has reached a predefined value.</li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/" itemprop="url">Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-22T16:49:43+08:00">2018-01-22 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/" class="leancloud_visitors" data-flag-title="Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Source blog is <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">here</a>.</p><hr><p>We’ve <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">discovered</a> that <strong>evolution strategies (ES)</strong>, an optimization technique that’s been known for decades, rivals the performance of standard <strong>reinforcement learning (RL)</strong>techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL’s inconveniences.</p><p>In particular, ES is simpler to implement (there is no need for <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">backpropagation</a>), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer <a href="https://www.quora.com/What-are-hyperparameters-in-machine-learning" target="_blank" rel="external">hyperparameters</a>. This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on <a href="https://en.wikipedia.org/wiki/Finite_difference" target="_blank" rel="external">finite differences</a> along a few random directions at each step.</p><p>Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">“AlexNet” paper</a> showed how to design, scale and train convolutional neural networks (CNNs) to achieve extremely strong results on image recognition tasks, at a time when most researchers thought that CNNs were not a promising approach to computer vision. Similarly, in 2013, the <a href="https://arxiv.org/abs/1312.5602" target="_blank" rel="external">Deep Q-Learning paper</a> showed how to combine Q-Learning with CNNs to successfully solve Atari games, reinvigorating RL as a research field with exciting experimental (rather than theoretical) results. Likewise, our work demonstrates that ES achieves strong performance on RL benchmarks, dispelling the common belief that ES methods are impossible to apply to high dimensional problems.</p><p>ES is easy to implement and scale. Running on a computing cluster of 80 machines and 1,440 CPU cores, our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes about 10 hours). Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour.</p><p>In what follows, we’ll first briefly describe the conventional RL approach, contrast that with our ES approach, discuss the tradeoffs between ES and RL, and finally highlight some of our experiments.</p><h4 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h4><p>Let’s briefly look at how RL works. Suppose we are given some environment (e.g. a game) that we’d like to train an agent on. To describe the behavior of the agent, we define a policy function (the brain of the agent), which computes how the agent should act in any given situation. In practice, the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions. A typical policy function might have about 1,000,000 parameters, so our task comes down to finding the precise setting of these parameters such that the policy plays well (i.e. wins a lot of games).</p><p><img src="https://blog.openai.com/content/images/2017/03/first-graphic-1.png" alt="img"></p><p><em>Above: In the game of Pong, the policy could take the pixels of the screen and compute the probability of moving the player’s paddle (in green, on right) Up, Down, or neither.</em></p><p>The training process for the policy works as follows. Starting from a random initialization, we let the agent interact with the environment for a while and collect episodes of interaction (e.g. each episode is one game of Pong). We thus obtain a complete recording of what happened: what sequence of states we encountered, what actions we took in each state, and what the reward was at each step. As an example, below is a diagram of three episodes that each took 10 time steps in a hypothetical environment. Each rectangle is a state, and rectangles are colored green if the reward was positive (e.g. we just got the ball past our opponent) and red if the reward was negative (e.g. we missed the ball):</p><p><img src="https://blog.openai.com/content/images/2017/03/second-graphic-1.png" alt="img"></p><p>This diagram suggests a recipe for how we can improve the policy; whatever we happened to do leading up to the green states was good, and whatever we happened to do in the states leading up to the red areas was bad. We can then use backpropagation to compute a small update on the network’s parameters that would make the green actions more likely in those states in the future, and the red actions less likely in those states in the future. We expect that the updated policy works a bit better as a result. We then iterate the process: collect another batch of episodes, do another update, etc.</p><p><strong>Exploration by injecting noise in the actions.</strong> The policies we usually use in RL are stochastic, in that they only compute probabilities of taking any action. This way, during the course of training, the agent may find itself in a particular state many times, and at different times it will take different actions due to the sampling. This provides the signal needed for learning; some of those actions will lead to good outcomes, and get encouraged, and some of them will not work out, and get discouraged. We therefore say that we introduce exploration into the learning process by injecting noise into the agent’s actions, which we do by sampling from the action distribution at each time step. This will be in contrast to ES, which we describe next.</p><h4 id="Evolution-Strategies"><a href="#Evolution-Strategies" class="headerlink" title="Evolution Strategies"></a>Evolution Strategies</h4><p><strong>On “Evolution”.</strong> Before we dive into the ES approach, it is important to note that despite the word “evolution”, ES has very little to do with biological evolution. Early versions of these techniques may have been inspired by biological evolution and the approach can, on an abstract level, be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations. However, the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black-box stochastic optimization techniques.</p><p><strong>Black-box optimization.</strong> In ES, we forget entirely that there is an agent, an environment, that there are neural networks involved, or that interactions take place over time, etc. The whole setup is that 1,000,000 numbers (which happen to describe the parameters of the policy network) go in, 1 number comes out (the total reward), and we want to find the best setting of the 1,000,000 numbers. Mathematically, we would say that we are optimizing a function <code>f(w)</code> with respect to the input vector <code>w</code>(the parameters / weights of the network), but we make no assumptions about the structure of <code>f</code>, except that we can evaluate it (hence “black box”).</p><p><strong>The ES algorithm.</strong> Intuitively, the optimization is a “guess and check” process, where we start with some random parameters and then repeatedly 1) tweak the guess a bit randomly, and 2) move our guess slightly towards whatever tweaks worked better. Concretely, at each step we take a parameter vector <code>w</code> and generate a population of, say, 100 slightly different parameter vectors <code>w1 ... w100</code> by jittering <code>w</code> with gaussian noise. We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while, and add up all the rewards in each case. The updated parameter vector then becomes the weighted sum of the 100 vectors, where each weight is proportional to the total reward (i.e. we want the more successful candidates to have a higher weight). Mathematically, you’ll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences, except we only do it along 100 random directions. Yet another way to see it is that we’re still doing RL (Policy Gradients, or <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> specifically), where the agent’s actions are to emit entire parameter vectors using a gaussian policy.</p><p><img src="https://blog.openai.com/content/images/2017/03/evo.png" alt="img"></p><p><em>Above: ES optimization process, in a setting with only two parameters and a reward function (red = high, blue = low). At each iteration we show the current parameter value (in white), a population of jittered samples (in black), and the estimated gradient (white arrow). We keep moving the parameters to the top of the arrow until we converge to a local optimum. You can reproduce this figure with this notebook.</em></p><p><strong>Code sample.</strong> To make the core algorithm concrete and to highlight its simplicity, here is a short example of optimizing a quadratic function using ES (or see this <a href="https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d" target="_blank" rel="external">longer version</a> with more comments):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># simple example: minimize a quadratic around some solution point</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">solution = np.array([<span class="number">0.5</span>, <span class="number">0.1</span>, <span class="number">-0.3</span>])</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(w)</span>:</span> <span class="keyword">return</span> -np.sum((w - solution)**<span class="number">2</span>)</div><div class="line"></div><div class="line">npop = <span class="number">50</span>      <span class="comment"># population size</span></div><div class="line">sigma = <span class="number">0.1</span>    <span class="comment"># noise standard deviation</span></div><div class="line">alpha = <span class="number">0.001</span>  <span class="comment"># learning rate</span></div><div class="line">w = np.random.randn(<span class="number">3</span>) <span class="comment"># initial guess</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</div><div class="line">  N = np.random.randn(npop, <span class="number">3</span>)</div><div class="line">  R = np.zeros(npop)</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(npop):</div><div class="line">    w_try = w + sigma*N[j]</div><div class="line">    R[j] = f(w_try)</div><div class="line">  A = (R - np.mean(R)) / np.std(R)</div><div class="line">  w = w + alpha/(npop*sigma) * np.dot(N.T, A)</div></pre></td></tr></table></figure><p><strong>Injecting noise in the parameters.</strong> Notice that the objective is identical to the one that RL optimizes: the expected reward. However, RL injects noise in the action space and uses backpropagation to compute the parameter updates, while ES injects noise directly in the parameter space. Another way to describe this is that RL is a “guess and check” on actions, while ES is a “guess and check” on parameters. Since we’re injecting noise in the parameters, it is possible to use deterministic policies (and we do, in our experiments). It is also possible to add noise in both actions and parameters to potentially combine the two approaches.</p><h4 id="Tradeoffs-between-ES-and-RL"><a href="#Tradeoffs-between-ES-and-RL" class="headerlink" title="Tradeoffs between ES and RL"></a>Tradeoffs between ES and RL</h4><p>ES enjoys multiple advantages over RL algorithms (some of them are a little technical):</p><ul><li><strong>No need for backpropagation</strong>. ES only requires the forward pass of the policy and does not require backpropagation (or value function estimation), which makes the code shorter and between 2-3 times faster in practice. On memory-constrained systems, it is also not necessary to keep a record of the episodes for a later update. There is also no need to worry about exploding gradients in RNNs. Lastly, we can explore a much larger function class of policies, including networks that are not differentiable (such as in binary networks), or ones that include complex modules (e.g. pathfinding, or various optimization layers).</li><li><strong>Highly parallelizable.</strong> ES only requires workers to communicate a few scalars between each other, while in RL it is necessary to synchronize entire parameter vectors (which can be millions of numbers). Intuitively, this is because we control the random seeds on each worker, so each worker can locally reconstruct the perturbations of the other workers. Thus, all that we need to communicate between workers is the reward of each perturbation. As a result, we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization.</li><li><strong>Higher robustness.</strong> Several hyperparameters that are difficult to set in RL implementations are side-stepped in ES. For example, RL is not “scale-free”, so one can achieve very different learning outcomes (including a complete failure) with different settings of the frame-skip hyperparameter in Atari. As we show in our work, ES works about equally well with any frame-skip.</li><li><strong>Structured exploration.</strong> Some RL algorithms (especially policy gradients) initialize with random policies, which often manifests as random jitter on spot for a long time. This effect is mitigated in Q-Learning due to epsilon-greedy policies, where the max operation can cause the agents to perform some consistent action for a while (e.g. holding down a left arrow). This is more likely to do something in a game than if the agent jitters on spot, as is the case with policy gradients. Similar to Q-learning, ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration.</li><li><strong>Credit assignment over long time scales.</strong> By studying both ES and RL gradient estimators mathematically we can see that ES is an attractive choice especially when the number of time steps in an episode is long, where actions have longlasting effects, or if no good value function estimates are available.</li></ul><p>Conversely, we also found some challenges to applying ES in practice. One core problem is that in order for ES to work, adding noise in parameters must lead to different outcomes to obtain some gradient signal. As we elaborate on in our paper, we found that the use of virtual batchnorm can help alleviate this problem, but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary. As an example of a related difficulty, we found that in Montezuma’s Revenge, one is very unlikely to get the key in the first level with a random network, while this is occasionally possible with random actions.</p><h4 id="ES-is-competitive-with-RL"><a href="#ES-is-competitive-with-RL" class="headerlink" title="ES is competitive with RL"></a>ES is competitive with RL</h4><p>We compared the performance of ES and RL on two standard RL benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo task (see examples below) contains a physically-simulated articulated figure, where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward. Below are some example agents trained on three MuJoCo control tasks, where the objective is to move forward:</p><p><img src="https://blog.openai.com/content/images/2017/03/out.gif" alt="img"></p><p>We usually compare the performance of algorithms by looking at their efficiency of learning from data; as a function of how many states we’ve seen, what is our average reward? Here are the example learning curves that we obtain, in comparison to RL (the <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="external">TRPO</a> algorithm in this case):</p><p><img src="https://blog.openai.com/content/images/2017/03/es_vs_trpo_full.png" alt="img"></p><p><strong>Data efficiency comparison</strong>. The comparisons above show that ES (orange) can reach a comparable performance to TRPO (blue), although it doesn’t quite match or surpass it in all cases. Moreover, by scanning horizontally we can see that ES is less efficient, but no worse than about a factor of 10 (note the x-axis is in log scale).</p><p><strong>Wall clock comparison</strong>. Instead of looking at the raw number of states seen, one can argue that the most important metric to look at is the wall clock time: how long (in number of seconds) does it take to solve a given problem? This quantity ultimately dictates the achievable speed of iteration for a researcher. Since ES requires negligible communication between workers, we were able to solve one of the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 machines in only 10 minutes. As a comparison, in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours. It is also possible that the performance of RL could also improve with more algorithmic and engineering effort, but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements.</p><p>Below are a few videos of 3D humanoid walkers trained with ES. As we can see, the results have quite a bit of variety, based on which local minimum the optimization ends up converging into.</p><p><img src="https://blog.openai.com/content/images/2017/03/out-1.gif" alt="img"></p><p>On Atari, ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day. Below are some result snippets on Pong, Seaquest and Beamrider. These videos show the preprocessed frames, which is exactly what the agent sees when it is playing:</p><p><img src="https://blog.openai.com/content/images/2017/03/atari.gif" alt="img"></p><p>In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels.</p><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p>ES is an algorithm from the neuroevolution literature, which has a long history in AI and a complete literature review is beyond the scope of this post. However, we encourage an interested reader to look at <a href="https://en.wikipedia.org/wiki/Neuroevolution" target="_blank" rel="external">Wikipedia</a>, <a href="http://www.scholarpedia.org/article/Neuroevolution" target="_blank" rel="external">Scholarpedia</a>, and Jürgen Schmidhuber’s <a href="https://arxiv.org/abs/1404.7828" target="_blank" rel="external">review article (Section 6.6)</a>. The work that most closely informed our approach is <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a> by Wierstra et al. 2014. Compared to this work and much of the work it has inspired, our focus is specifically on scaling these algorithms to large-scale, distributed settings, finding components that make the algorithms work better with deep neural networks (e.g. <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="external">virtual batch norm</a>), and evaluating them on modern RL benchmarks.</p><p>It is also worth noting that neuroevolution-related approaches have seen some recent resurgence in the machine learning literature, for example with <a href="https://arxiv.org/abs/1609.09106" target="_blank" rel="external">HyperNetworks</a>, <a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="external">“Large-Scale Evolution of Image Classifiers”</a> and <a href="https://arxiv.org/abs/1606.02580" target="_blank" rel="external">“Convolution by Evolution”</a>.</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Our work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent-environment benchmarks, while offering significant benefits related to code complexity and ease of scaling to large-scale distributed settings. We also expect that more exciting work can be done by revisiting other ideas from this line of work, such as indirect encoding methods, or evolving the network structure in addition to the parameters.</p><p><strong>Note on supervised learning</strong>. It is also important to note that supervised learning problems (e.g. image classification, speech recognition, or most other tasks in the industry), where one can compute the exact gradient of the loss function with backpropagation, are not directly impacted by these findings. For example, in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1,000 times slower than using backpropagation. It is only in RL settings, where one has to estimate the gradient of the expected reward by sampling, where ES becomes competitive.</p><p><strong>Code release</strong>. Finally, if you’d like to try running ES yourself, we encourage you to dive into the full details by reading <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">our paper</a> or looking at our code on this <a href="https://github.com/openai/evolution-strategies-starter" target="_blank" rel="external">Github repo</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" itemprop="url">Seq2Seq with Attention and Beam Search [Repost]</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-21T17:35:50+08:00">2018-01-21 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" class="leancloud_visitors" data-flag-title="Seq2Seq with Attention and Beam Search [Repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Source Post is <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">here</a></p><h2 id="Sequence-to-Sequence-basics"><a href="#Sequence-to-Sequence-basics" class="headerlink" title="Sequence to Sequence basics"></a>Sequence to Sequence basics</h2><p>Let’s explain the sequence to sequence framework as we’ll rely on it for our model. Let’s start with the simplest version on the translation task.</p><blockquote><p>As an example, let’s translate <code>how are you</code> in French <code>comment vas tu</code>.</p></blockquote><h3 id="Vanilla-Seq2Seq"><a href="#Vanilla-Seq2Seq" class="headerlink" title="Vanilla Seq2Seq"></a>Vanilla Seq2Seq</h3><p>The Seq2Seq framework relies on the <strong>encoder-decoder</strong> paradigm. The <strong>encoder</strong> <em>encodes</em> the input sequence, while the <strong>decoder</strong> <em>produces</em> the target sequence</p><p><strong>Encoder</strong></p><p>Our input sequence is <code>how are you</code>. Each word from the input sequence is associated to a vector $w \in \mathbb{R}^d$ (via a lookup table). In our case, we have 3 words, thus our input will be transformed into $[w_0, w_1, w_2] \in \mathbb{R}^{d \times 3}$. Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation $e$. Let’s write the hidden states $[e_0,e_1,e_2]$ (and thus $e=e_2$)</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg" alt="Vanilla Encoder"></p><p><em>Vanilla Encoder</em></p><p><strong>Decoder</strong></p><p>Now that we have a vector ee that captures the meaning of the input sequence, we’ll use it to generate the target sequence word by word. Feed to another LSTM cell: ee as hidden state and a special <em>start of sentence</em> vector $w_{\text{sos}}$ as input. The LSTM computes the next hidden state $h_0 \in \mathbb{R}^{h}$. Then, we apply some function $g : \mathbb{R}^h \mapsto \mathbb{R}^V$ so that $s_0 := g(h_0) \in \mathbb{R}^V$ is a vector of the same size as the vocabulary.<br>$$<br>\begin{align}<br>h_0 &amp;= \operatorname{LSTM}\left(e, w_{sos} \right)\\<br>s_0 &amp;= g(h_0)\\<br>p_0 &amp;= \operatorname{softmax}(s_0)\\<br>i_0 &amp;= \operatorname{argmax}(p_0)\\<br>\end{align}<br>$$<br>Then, apply a softmax to $s_0$ to normalize it into a vector of probabilities $p_0 \in \mathbb{R}^V$ . Now, each entry of $p_0$ will measure how likely is each word in the vocabulary. Let’s say that the word <em>“comment”</em> has the highest probability (and thus $i_0 = \operatorname{argmax}(p_0)$) corresponds to the index of <em>“comment”</em>). Get a corresponding vector and $w_{i_0} = w_{comment}$ repeat the procedure: the LSTM will take $h_0$ as hidden state and $w_{comment}$ as input and will output a probability vector $p_1$ over the second word, etc.<br>$$<br>\begin{align}<br>h_1 &amp;= \operatorname{LSTM}\left(h_0, w_{i_0} \right)\\<br>s_1 &amp;= g(h_1)\\<br>p_1 &amp;= \operatorname{softmax}(s_1)\\<br>i_1 &amp;= \operatorname{argmax}(p_1)<br>\end{align}<br>$$<br>The decoding stops when the predicted word is a special <em>end of sentence</em> token.</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg" alt="Vanilla Decoder"></p><p><em>Vanilla Decoder</em></p><blockquote><p>Intuitively, the hidden vector represents the “amount of meaning” that has not been decoded yet.</p></blockquote><p>The above method aims at modelling the distribution of the next word conditionned on the beginning of the sentence<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_1, \dots, y_{t}, x_0, \dots, x_n \right]<br>$$<br>by writing<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_t, h_{t}, e \right]<br>$$</p><h3 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h3><p>The previous model has been refined over the past few years and greatly benefited from what is known as <strong>attention</strong>. Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM. One way of performing attention is explained by <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Bahdanau et al.</a>. We slightly modify the reccurrence formula that we defined above by adding a new vector $c_t$ to the input of the LSTM<br>$$<br>\begin{align}<br>h_{t} &amp;= \operatorname{LSTM}\left(h_{t-1}, [w_{i_{t-1}}, c_t] \right)\\<br>s_t &amp;= g(h_t)\\<br>p_t &amp;= \operatorname{softmax}(s_t)\\<br>i_t &amp;= \operatorname{argmax}(p_t)<br>\end{align}<br>$$<br>The vector ctct is the attention (or <strong>context</strong>) vector. We compute a new context vector at each decoding step. First, with a function $f (h_{t-1}, e_{t’}) \mapsto \alpha_{t’} \in \mathbb{R}$, compute a score for each hidden state $e_{t^{\prime}}$ of the encoder. Then, normalize the sequence of using a softmax and compute $c_t$ as the weighted average of the $e_{t^{\prime}}$.<br>$$<br>\begin{align}<br>\alpha_{t’} &amp;= f(h_{t-1}, e_{t’}) \in \mathbb{R} &amp; \text{for all } t’\\<br>\bar{\alpha} &amp;= \operatorname{softmax} (\alpha)\\<br>c_t &amp;= \sum_{t’=0}^n \bar{\alpha}_{t’} e_{t’}<br>\end{align}<br>$$<br><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg" alt="Attention Mechanism"></p><p><em>Attention Mechanism</em></p><p>The choice of the function ff varies, but is usually one of the following<br>$$<br>f(h_{t-1}, e_{t’}) =<br>\begin{cases}<br>h_{t-1}^T e_{t’} &amp; \text{dot}\\<br>h_{t-1}^T W e_{t’} &amp; \text{general}\\<br>v^T \tanh \left(W [h_{t-1}, e_{t’}]\right) &amp; \text{concat}\\<br>\end{cases}<br>$$<br>It turns out that the attention weighs $\bar{\alpha}$ can be easily interpreted. When generating the word <code>vas</code>(corresponding to <code>are</code> in English), we expect $\bar{\alpha} _ {\text{are}}$ are to be close to $1$ while $\bar{\alpha} _ {\text{how}}$ and $\bar{\alpha} _ {\text{you}}$ to be close to $0$. Intuitively, the context vector $c$ will be roughly equal to the hidden vector of <code>are</code> and it will help to generate the French word <code>vas</code>.</p><p>By putting the attention weights into a matrix (rows = input sequence, columns = output sequence), we would have access to the <strong>alignment</strong> between the words from the English and French sentences… (see <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="external">page 6</a>) There is still a lot of things to say about sequence to sequence models (for instance, it works better if the encoder processes the input sequence <em>backwards</em>…).</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><blockquote><p>What happens if the first time step is not sure about wether it should generate <code>comment</code> or <code>vas</code> (most likely case at the beginning of the training)? Then it would mess up the entire sequence, and the model will hardly learn anything…</p></blockquote><p>If we use the predicted token as input to the next step during training (as explained above), errors would accumulate and the model would rarely be exposed to the correct distribution of inputs, making training slow or impossible. To speedup things, one trick is to feed the actual output sequence (<code>&lt;sos&gt;</code> <code>comment</code> <code>vas</code> <code>tu</code>) into the decoder’s LSTM and predict the next token at every position (<code>comment</code> <code>vas</code> <code>tu</code> <code>&lt;eos&gt;</code>).</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_training.svg" alt="Training"></p><p><em>Training</em></p><p>The decoder outputs vectors of probability over the vocabulary $p_i \in \mathbb{R}^V$ for each time step. Then, for a given target sequence $y_1, \dots, y_n$, we can compute its probability as the product of the probabilities of each token being produced at each relevant time step:<br>$$<br>\mathbb{P}\left(y_1, \dots, y_m \right) = \prod_{i=1}^m p_i [y_i]<br>$$<br>where $p_i [y_i]$ means that we extract the $y_i$-th entry of the probability vector $p_i$ from the $i$-th decoding step. In particular, we can compute the probability of the actual target sequence. A perfect system would give a probabilty of 1 to this target sequence, so we are going to train our network to maximize the probability of the target sequence, which is the same as minimizing<br>$$<br>\begin{align}<br>-\log \mathbb{P} \left(y_1, \dots, y_m \right) &amp;= - \log \prod_{i=1}^m p_i [y_i]\\<br>&amp;= - \sum_{i=1}^n \log p_i [y_i]\\<br>\end{align}<br>$$<br>in our example, this is equal to<br>$$</p><ul><li>\log p_1[\text{comment}] - \log p_2[\text{vas}] - \log p_3[\text{tu}] - \log p_4[\text{<eos>}]<br>$$<br>and you recognize the standard cross entropy: we actually are minimizing the cross entropy between the target distribution (all one-hot vectors) and the predicted distribution outputed by our model (our vectors $p_i$).</eos></li></ul><h2 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h2><p>The main takeaway from the discussion above is that for the same model, we can define different behaviors. In particular, we defined a specific behavior that speeds up training.</p><blockquote><p>What about inference/testing time then? Is there an other way to decode a sentence?</p></blockquote><p>There indeed are 2 main ways of performing decoding at testing time (translating a sentence for which we don’t have a translation). The first of these methods is the one covered at the beginning of the article: <strong>greedy decoding</strong>. It is the most natural way and it consists in feeding to the next step the most likely word predicted at the previous step.</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg" alt="Greedy Decoder - feeds the best token to the next step"></p><p><em>Greedy Decoder - feeds the best token to the next step</em></p><blockquote><p>But didn’t we say that this behavior is likely to accumulate errors?</p></blockquote><p>Even after having trained the model, it can happen that the model makes a small error (and gives a small advantage to <code>vas</code> over <code>comment</code> for the first step of the decoding). This would mess up the entire decoding…</p><p>There is a better way of performing decoding, called <strong>Beam Search</strong>. Instead of only predicting the token with the best score, we keep track of $k$ hypotheses (for example $k=5$, we refer to kk as the <strong>beam size</strong>). At each new time step, for these 5 hypotheses we have $V$ new possible tokens. It makes a total of $5V$ new hypotheses. Then, only keep the $5$ best ones, and so on… Formally, define $\mathcal{H}_t$ the set of hypotheses decoded at time step $t$.<br>$$<br>\mathcal{H}_ t := \{ (w^1_1, \dots, w^1_t), \dots, (w^k_1, \dots, w^k_t) \}<br>$$<br>For instance if $k=2$, one possible $\mathcal{H}_2$ would be<br>$$<br>\mathcal{H}_ 2 := \{ (\text{comment vas}), (\text{comment tu}) \}<br>$$<br>Now we consider all the possible candidates $\mathcal{C}_{t+1}$, produced from $\mathcal{H}_t$ by adding all possible new tokens<br>$$<br>\mathcal{C}_ {t+1} := \bigcup_{i=1}^k \{ (w^i_1, \dots, w^i_t, 1), \dots, (w^i_1, \dots, w^i_t, V) \}<br>$$<br>and keep the $k$ highest scores (probability of the sequence). If we keep our example<br>$$<br>\begin{align}<br>\mathcal{C}_ 3 =&amp; \{ (\text{comment vas comment}), (\text{comment vas vas}), (\text{comment vas tu})\} \\<br>\cup &amp; \{ (\text{comment tu comment}), \ \ (\text{comment tu vas}), \ \ (\text{comment tu tu}) \}<br>\end{align}<br>$$<br>and for instance we can imagine that the 2 best ones would be<br>$$<br>\mathcal{H}_ 3 := \{ (\text{comment vas tu}), (\text{comment tu vas}) \}<br>$$<br>Once every hypothesis reached the <code>&lt;eos&gt;</code> token, we return the hypothesis with the highest score.</p><blockquote><p>If we use <strong>beam search</strong>, a small error at the first step might be rectified at the next step, as we keep the gold hypthesis in the beam!</p></blockquote><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this article we covered the seq2seq concepts. We showed that training is different than decoding. We covered two methods for decoding: <strong>greedy</strong> and <strong>beam search</strong>. While beam search generally achieves better results, it is not perfect and still suffers from <strong>exposure bias</strong>. During training, the model is never exposed to its errors! It also suffers from <strong>Loss-Evaluation Mismatch</strong>. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</p><p>Now, let’s apply Seq2Seq for LaTeX generation from images!</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_task.svg" alt="Producing LaTeX code from an image"></p><p><em>Producing LaTeX code from an image</em></p><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>Previous part covered the concepts of <strong>sequence-to-sequence</strong> applied to Machine Translation. The same framework can be applied to our LaTeX generation problem. The input sequence would just be replaced by an image, preprocessed with some convolutional model adapted to OCR (in a sense, if we <em>unfold</em> the pixels of an image into a sequence, this is exactly the same problem). This idea proved to be efficient for image captioning (see the reference paper <a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Show, Attend and Tell</a>). Building on some <a href="https://arxiv.org/pdf/1609.04938v1.pdf" target="_blank" rel="external">great work</a> from the Harvard NLP group, my teammate <a href="https://www.linkedin.com/in/romain-sauvestre-241171a2" target="_blank" rel="external">Romain</a> and I chose to follow a similar approach.</p><blockquote><p>Keep the seq2seq framework but replace the encoder by a convolutional network over the image!</p></blockquote><p>Good Tensorflow implementations of such models were hard to find. Together with this post, I am releasing the <a href="https://github.com/guillaumegenthial/im2latex" target="_blank" rel="external">code</a> and hope some will find it useful. You can use it to train your own image captioning model or adapt it for a more advanced use. <a href="https://github.com/guillaumegenthial/im2latex" target="_blank" rel="external">The code</a> does <strong>not</strong> rely on the <a href="https://www.tensorflow.org/versions/master/api_guides/python/contrib.seq2seq" target="_blank" rel="external">Tensorflow Seq2Seq library</a> as it was not entirely ready at the time of the project and I also wanted more flexibility (but adopts a similar interface).</p><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>To train our model, we’ll need labeled examples: images of formulas along with the LaTeX code used to generate the images. A good source of LaTeX code is <a href="https://arxiv.org/" target="_blank" rel="external">arXiv</a>, that has thousands of articles under the <code>.tex</code> format. After applying some heuristics to find equations in the <code>.tex</code> files, keeping only the ones that actually compile, the <a href="https://zenodo.org/record/56198#.WflVu0yZPLZ" target="_blank" rel="external">Harvard NLP group</a> extracted $\sim 100,000$ formulas.</p><blockquote><p>Wait… Don’t you have a problem as different LaTeX codes can give the same image?</p></blockquote><p>Good point: <code>(x^2 + 1)</code> and <code>\left( x^{2} + 1 \right)</code> indeed give the same output. That’s why Harvard’s paper found out that normalizing the data using a parser (<a href="https://khan.github.io/KaTeX/" target="_blank" rel="external">KaTeX</a>) improved performance. It forces adoption of some conventions, like writing <code>x ^ { 2 }</code> instead of <code>x^2</code>, etc. After normalization, they end up with a <code>.txt</code> file containing one formula per line that looks like</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">\alpha + \beta</div><div class="line">\frac &#123; 1 &#125; &#123; 2 &#125;</div><div class="line">\frac &#123; \alpha &#125; &#123; \beta &#125;</div><div class="line">1 + 2</div></pre></td></tr></table></figure><p>From this file, we’ll produce images <code>0.png</code>, <code>1.png</code>, etc. and a matching file mapping the image files to the indices (=line numbers) of the formulas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">0.png 0</div><div class="line">1.png 1</div><div class="line">2.png 2</div><div class="line">3.png 3</div></pre></td></tr></table></figure><p>The reason why we use this format is that it is flexible and allows you to use the pre-built <a href="https://zenodo.org/record/56198#.WflVu0yZPLZ" target="_blank" rel="external">dataset from Harvard</a> (You may need to use the preprocessing scripts as explained <a href="https://github.com/harvardnlp/im2markup" target="_blank" rel="external">here</a>). You’ll also need to have <code>pdflatex</code> and <code>ImageMagick</code> installed.</p><p>We also build a vocabulary, to map LaTeX tokens to indices that will be given as input to our model. If we keep the same data as above, our vocabulary will look like</p><p><code>+</code> <code>1</code> <code>2</code> <code>\alpha</code> <code>\beta</code> <code>\frac</code> <code>{</code> <code>}</code></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>Our model is going to rely on a variation of the Seq2Seq model, adapted to images. First, let’s define the input of our graph. Not surprisingly we get as input a batch of black-and-white images of shape $[H,W]$ and a batch of formulas (ids of the LaTeX tokens):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># batch of images, shape = (batch size, height, width, 1)</span></div><div class="line">img = tf.placeholder(tf.uint8, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'img'</span>)</div><div class="line"><span class="comment"># batch of formulas, shape = (batch size, length of the formula)</span></div><div class="line">formula = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>), name=<span class="string">'formula'</span>)</div><div class="line"><span class="comment"># for padding</span></div><div class="line">formula_length = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, ), name=<span class="string">'formula_length'</span>)</div></pre></td></tr></table></figure><blockquote><p>A special note on the type of the image input. You may have noticed that we use <code>tf.uint8</code>. This is because our image is encoded in grey-levels (integers from <code>0</code> to <code>255</code> - and $2^8=256$). Even if we could give a <code>tf.float32</code> Tensor as input to Tensorflow, this would be 4 times more expensive in terms of memory bandwith. As data starvation is one of the main bottlenecks of GPUs, this simple trick can save us some training time. For further improvement of the data pipeline, have a look at <a href="https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/data" target="_blank" rel="external">the new Tensorflow data pipeline</a>.</p></blockquote><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p><strong>High-level idea</strong> Apply some convolutional network on top of the image an flatten the output into a sequence of vectors $[e_1, \cdots, e_n]$, each of those corresponding to a region of the input image. These vectors will correspond to the hidden vectors of the LSTM that we used for translation.</p><blockquote><p>Once our image is transformed into a sequence, we can use the seq2seq model!</p></blockquote><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_encoder.svg" alt="Convolutional Encoder - produces a sequence of vectors"></p><p><em>Convolutional Encoder - produces a sequence of vectors</em></p><p>We need to extract features from our image, and for this, nothing has (<a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="external">yet</a>) been proven more effective than convolutions. Here, there is nothing much to say except that we pick some architecture that has been proven to be effective for Optical Character Recognition (OCR), which stacks convolutional layers and max-pooling to produce a Tensor of shape $[H^{\prime},W^{\prime},512]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># casting the image back to float32 on the GPU</span></div><div class="line">img = tf.cast(img, tf.float32) / <span class="number">255.</span></div><div class="line"></div><div class="line">out = tf.layers.conv2d(img, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line"><span class="comment"># encoder representation, shape = (batch size, height', width', 512)</span></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"VALID"</span>, activation=tf.nn.relu)</div></pre></td></tr></table></figure><p>Now that we have extracted some features from the image, let’s <strong>unfold</strong> the image to get a sequence so that we can use our sequence to sequence framework. We end up with a sequence of length $[H^{\prime},W^{\prime}]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">H, W = tf.shape(out)[<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">seq = tf.reshape(out, shape=[<span class="number">-1</span>, H*W, <span class="number">512</span>])</div></pre></td></tr></table></figure><blockquote><p>Don’t you loose a lot of structural information by reshaping? I’m afraid that when performing attention over the image, my decoder won’t be able to understand the location of each feature vector in the original image!</p></blockquote><p>It turns out that the model manages to work despite this issue, but that’s not completely satisfying. In the case of translation, the hidden states of the LSTM contained some positional information that was computed by the LSTM (after all, LSTM are by essence sequential). Can we fix this issue?</p><p><strong>Positional Embeddings</strong> I decided to follow the idea from <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external">Attention is All you Need</a> that adds <em>positional embeddings</em> to the image representation (<code>out</code>), and has the huge advantage of not adding any new trainable parameter to our model. The idea is that for each position of the image, we compute a vector of size $512$ such that its components are coscos or sinsin. More formally, the (2i)-th and (2i+1)-th entries of my positional embedding $v$ at position $p$ will be<br>$$<br>\begin{align}<br>v_{2i} &amp;= \sin\left(p / f^{2i}\right)\\<br>v_{2i+1} &amp;= \cos\left(p / f^{2i}\right)\\<br>\end{align}<br>$$<br>where $f$ is some frequency parameter. Intuitively, because $\sin(a+b)$ and $\cos⁡(a+b)$ can be expressed in terms of $\sin⁡(b)$ , $\sin(a)$ , $\cos⁡(b)$ and $\cos⁡(a)$, there will be linear dependencies between the components of distant embeddings, authorizing the model to extract relative positioning information. Good news: the tensorflow code for this technique is available in the library <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="external">tensor2tensor</a>, so we just need to reuse the same function and transform our <code>out</code> with the following call</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">out = add_timing_signal_nd(out)</div></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Now that we have a sequence of vectors $[e_1, \dots, e_n]$ that represents our input image, let’s decode it! First, let’s explain what variant of the Seq2Seq framework we are going to use.</p><p><strong>First hidden vector of the decoder’s LSTM</strong> In the seq2seq framework, this is usually just the last hidden vector of the encoder’s LSTM. Here, we don’t have such a vector, so a good choice would be to learn to compute it with a matrix $W$ and a vector $b$<br>$$<br>h_0 = \tanh\left( W \cdot \left( \frac{1}{n} \sum_{i=1}^n e_i\right) + b \right)<br>$$<br>This can be done in Tensorflow with the following logic</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">img_mean = tf.reduce_mean(seq, axis=<span class="number">1</span>)</div><div class="line">W = tf.get_variable(<span class="string">"W"</span>, shape=[<span class="number">512</span>, <span class="number">512</span>])</div><div class="line">b = tf.get_variable(<span class="string">"b"</span>, shape=[<span class="number">512</span>])</div><div class="line">h = tf.tanh(tf.matmul(img_mean, W) + b)</div></pre></td></tr></table></figure><p><strong>Attention Mechanism</strong> We first need to compute a score $\alpha_{t^{\prime}}$ for each vector $e_{t^{\prime}}$ of the sequence. We use the following method<br>$$<br>\begin{align}<br>\alpha_{t’} &amp;= \beta^T \tanh\left( W_1 \cdot e_{t’} + W_2 \cdot h_{t} \right)\\<br>\bar{\alpha} &amp;= \operatorname{softmax}\left(\alpha\right)\\<br>c_t &amp;= \sum_{i=1}^n \bar{\alpha}_{t’} e_{t’}\\<br>\end{align}<br>$$<br>This can be done in Tensorflow with the follwing code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># over the image, shape = (batch size, n, 512)</span></div><div class="line">W1_e = tf.layers.dense(inputs=seq, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># over the hidden vector, shape = (batch size, 512)</span></div><div class="line">W2_h = tf.layers.dense(inputs=h, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># sums the two contributions</span></div><div class="line">a = tf.tanh(W1_e + tf.expand_dims(W2_h, axis=<span class="number">1</span>))</div><div class="line">beta = tf.get_variable(<span class="string">"beta"</span>, shape=[<span class="number">512</span>, <span class="number">1</span>], dtype=tf.float32)</div><div class="line">a_flat = tf.reshape(a, shape=[<span class="number">-1</span>, <span class="number">512</span>])</div><div class="line">a_flat = tf.matmul(a_flat, beta)</div><div class="line">a = tf.reshape(a, shape=[<span class="number">-1</span>, n])</div><div class="line"></div><div class="line"><span class="comment"># compute weights</span></div><div class="line">a = tf.nn.softmax(a)</div><div class="line">a = tf.expand_dims(a, axis=<span class="number">-1</span>)</div><div class="line">c = tf.reduce_sum(a * seq, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Note that the line <code>W1_e = tf.layers.dense(inputs=seq, units=512, use_bias=False)</code> is common to every decoder time step, so we can just compute it once and for all. The dense layer with no bias is just a matrix multiplication.</p></blockquote><p>Now that we have our attention vector, let’s just add a small modification and compute an other vector $o_{t-1}$ (as in <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="external">Luong, Pham and Manning</a>) that we will use to make our final prediction and that we will feed as input to the LSTM at the next step. Here $w_{t−1}$ denotes the embedding of the token generated at the previous step.</p><blockquote><p>$o_t$ passes some information about the distribution from the previous time step as well as the confidence it had for the predicted token</p></blockquote><p>$$<br>\begin{align}<br>h_t &amp;= \operatorname{LSTM}\left( h_{t-1}, [w_{t-1}, o_{t-1}] \right)\\<br>c_t &amp;= \operatorname{Attention}([e_1, \dots, e_n], h_t)\\<br>o_{t} &amp;= \tanh\left(W_3 \cdot [h_{t}, c_t] \right)\\<br>p_t &amp;= \operatorname{softmax}\left(W_4 \cdot o_{t} \right)\\<br>\end{align}<br>$$</p><p>and now the code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute o</span></div><div class="line">W3_o = tf.layers.dense(inputs=tf.concat([h, c], axis=<span class="number">-1</span>), units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line">o = tf.tanh(W3_o)</div><div class="line"></div><div class="line"><span class="comment"># compute the logits scores (before softmax)</span></div><div class="line">logits = tf.layers.dense(inputs=o, units=vocab_size, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># the softmax will be computed in the loss or somewhere else</span></div></pre></td></tr></table></figure><blockquote><p>If I read carefully, I notice that for the first step of the decoding process, we need to compute an $o_0$ too, right?</p></blockquote><p>This is a good point, and we just use the same technique that we used to generate $h_0$ but with different weights.</p><h2 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h2><blockquote><p>We’ll need to create 2 different outputs in the Tensorflow graph: one for training (that uses the <code>formula</code>and feeds the ground truth at each time step, see <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">part I</a>) and one for test time (that ignores everything about the actual <code>formula</code> and uses the prediction from the previous step).</p></blockquote><h3 id="AttentionCell"><a href="#AttentionCell" class="headerlink" title="AttentionCell"></a>AttentionCell</h3><p>We’ll need to encapsulate the reccurent logic into a custom cell that inherits <code>RNNCell</code>. Our custom cell will be able to call the LSTM cell (initialized in the <code>__init__</code>). It also has a special recurrent state that combines the LSTM state and the vector oo (as we need to pass it through). An elegant way is to define a namedtuple for this recurrent state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">AttentionState = collections.namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.lstm_cell = LSTMCell(<span class="number">512</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Args:</div><div class="line">            inputs: shape = (batch_size, dim_embeddings) embeddings from previous time step</div><div class="line">            cell_state: (AttentionState) state from previous time step</div><div class="line">        """</div><div class="line">        lstm_state, o = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([inputs, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># apply previous logic</span></div><div class="line">        c = ...</div><div class="line">        new_o  = ...</div><div class="line">        logits = ...</div><div class="line"></div><div class="line">        new_state = AttentionState(new_lstm_state, new_o)</div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><p>Then, to compute our output sequence, we just need to call the previous cell on the sequence of LaTeX tokens. We first produce the sequence of token embeddings to which we concatenate the special <code>&lt;sos&gt;</code> token. Then, we call <code>dynamic_rnn</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. get token embeddings</span></div><div class="line">E = tf.get_variable(<span class="string">"E"</span>, shape=[vocab_size, <span class="number">80</span>], dtype=tf.float32)</div><div class="line"><span class="comment"># special &lt;sos&gt; token</span></div><div class="line">start_token = tf.get_variable(<span class="string">"start_token"</span>, dtype=tf.float32, shape=[<span class="number">80</span>])</div><div class="line">tok_embeddings = tf.nn.embedding_lookup(E, formula)</div><div class="line"></div><div class="line"><span class="comment"># 2. add the special &lt;sos&gt; token embedding at the beggining of every formula</span></div><div class="line">start_token_ = tf.reshape(start_token, [<span class="number">1</span>, <span class="number">1</span>, dim])</div><div class="line">start_tokens = tf.tile(start_token_, multiples=[batch_size, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line"><span class="comment"># remove the &lt;eos&gt; that won't be used because we reached the end</span></div><div class="line">tok_embeddings = tf.concat([start_tokens, tok_embeddings[:, :<span class="number">-1</span>, :]], axis=<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 3. decode</span></div><div class="line">attn_cell = AttentionCell()</div><div class="line">seq_logits, _ = tf.nn.dynamic_rnn(attn_cell, tok_embeddings, initial_state=AttentionState(h_0, o_0))</div></pre></td></tr></table></figure><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>Code speaks for itself</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute - log(p_i[y_i]) for each time step, shape = (batch_size, formula length)</span></div><div class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=seq_logits, labels=formula)</div><div class="line"><span class="comment"># masking the losses</span></div><div class="line">mask = tf.sequence_mask(formula_length)</div><div class="line">losses = tf.boolean_mask(losses, mask)</div><div class="line"><span class="comment"># averaging the loss over the batch</span></div><div class="line">loss = tf.reduce_mean(losses)</div><div class="line"><span class="comment"># building the train op</span></div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate)</div><div class="line">train_op = optimizer.minimize(loss)</div></pre></td></tr></table></figure><p>and when iterating over the batches during training, <code>train_op</code> will be given to the <code>tf.Session</code> along with a <code>feed_dict</code> containing the data for the placeholders.</p><h2 id="Decoding-in-Tensorflow"><a href="#Decoding-in-Tensorflow" class="headerlink" title="Decoding in Tensorflow"></a>Decoding in Tensorflow</h2><blockquote><p>Let’s have a look at the Tensorflow implementation of the Greedy Method before dealing with Beam Search</p></blockquote><h3 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h3><p>While greedy decoding is easy to conceptualize, implementing it in Tensorflow is not straightforward, as you need to use the previous prediction and can’t use <code>dynamic_rnn</code> on the <code>formula</code>. There are basically <strong>2 ways of approaching the problem</strong></p><ol><li><p>Modify our <code>AttentionCell</code> and <code>AttentionState</code> so that <code>AttentionState</code> also contains the embedding of the predicted word at the previous time step,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">AttentionState = namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>, <span class="string">"embedding"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        lstm_state, o, embbeding = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([embedding, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># usual logic</span></div><div class="line">        logits = ...</div><div class="line">        <span class="comment"># compute new embeddding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        new_state = AttentionState(new_lstm_state, new_o, new_embedding)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><blockquote><p>This technique has a few downsides. It <strong>doesn’t use inputs</strong> (which used to be the embedding of the gold token from the <code>formula</code> and thus we would have to call <code>dynamic_rnn</code> on a “fake” sequence). Also, how do you know when to stop decoding, once you’ve reached the <code>&lt;eos&gt;</code> token?</p></blockquote></li><li><p>Implement a variant of <code>dynamic_rnn</code> that would not run on a sequence but feed the prediction from the previous time step to the cell, while having a maximum number of decoding steps. This would involve delving deeper into Tensorflow, using <code>tf.while_loop</code>. That’s the method we’re going to use as it fixes all the problems of the first technique. We eventually want something that looks like</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">attn_cell = AttentionCell(...)</div><div class="line"><span class="comment"># wrap the attention cell for decoding</span></div><div class="line">decoder_cell = GreedyDecoderCell(attn_cell)</div><div class="line"><span class="comment"># call a special dynamic_decode primitive</span></div><div class="line">test_outputs, _ = dynamic_decode(decoder_cell, max_length_formula+<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Much better isn’t it? Now let’s see what <code>GreedyDecoderCell</code> and <code>dynamic_decode</code> look like.</p></blockquote></li></ol><h3 id="Greedy-Decoder-Cell"><a href="#Greedy-Decoder-Cell" class="headerlink" title="Greedy Decoder Cell"></a>Greedy Decoder Cell</h3><p>We first wrap the attention cell in a <code>GreedyDecoderCell</code> that takes care of the greedy logic for us, without having to modify the <code>AttentionCell</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderOutput</span><span class="params">(collections.namedtuple<span class="params">(<span class="string">"DecoderOutput"</span>, <span class="params">(<span class="string">"logits"</span>, <span class="string">"ids"</span>)</span>)</span>)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GreedyDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># next step of attention cell</span></div><div class="line">        logits, new_state = self._attention_cell.step(embedding, state)</div><div class="line">        <span class="comment"># get ids of words predicted and get embedding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        <span class="comment"># create new state of decoder</span></div><div class="line">        new_output = DecoderOutput(logits, new_ids)</div><div class="line">        new_finished = tf.logical_or(finished, tf.equal(new_ids,</div><div class="line">                self._end_token))</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (new_output, new_state, new_embedding, new_finished)</div></pre></td></tr></table></figure><h3 id="Dynamic-Decode-primitive"><a href="#Dynamic-Decode-primitive" class="headerlink" title="Dynamic Decode primitive"></a>Dynamic Decode primitive</h3><p>We need to implement a function <code>dynamic_decode</code> that will recursively call the above <code>step</code> function. We do this with a <code>tf.while_loop</code> that stops when all the hypotheses reached <code>&lt;eos&gt;</code> or <code>time</code> is greater than the max number of iterations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamic_decode</span><span class="params">(decoder_cell, maximum_iterations)</span>:</span></div><div class="line">    <span class="comment"># initialize variables (details on github)</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">condition</span><span class="params">(time, unused_outputs_ta, unused_state, unused_inputs, finished)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.logical_not(tf.reduce_all(finished))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(time, outputs_ta, state, inputs, finished)</span>:</span></div><div class="line">        new_output, new_state, new_inputs, new_finished = decoder_cell.step(</div><div class="line">            time, state, inputs, finished)</div><div class="line">        <span class="comment"># store the outputs in TensorArrays (details on github)</span></div><div class="line">        new_finished = tf.logical_or(tf.greater_equal(time, maximum_iterations), new_finished)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (time + <span class="number">1</span>, outputs_ta, new_state, new_inputs, new_finished)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"rnn"</span>):</div><div class="line">        res = tf.while_loop(</div><div class="line">            condition,</div><div class="line">            body,</div><div class="line">            loop_vars=[initial_time, initial_outputs_ta, initial_state, initial_inputs, initial_finished])</div><div class="line"></div><div class="line">    <span class="comment"># return the final outputs (details on github)</span></div></pre></td></tr></table></figure><blockquote><p>Some details using <code>TensorArrays</code> or <code>nest.map_structure</code> have been omitted for clarity but may be found on <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/dynamic_decode.py" target="_blank" rel="external">github</a></p><p>Notice that we place the <code>tf.while_loop</code> inside a scope named <code>rnn</code>. This is because <code>dynamic_rnn</code>does the same thing and thus the weights of our LSTM are defined in that scope.</p></blockquote><h3 id="Beam-Search-Decoder-Cell"><a href="#Beam-Search-Decoder-Cell" class="headerlink" title="Beam Search Decoder Cell"></a>Beam Search Decoder Cell</h3><blockquote><p>We can follow the same approach as in the greedy method and use <code>dynamic_decode</code></p></blockquote><p>Let’s create a new wrapper for <code>AttentionCell</code> in the same way we did for <code>GreedyDecoderCell</code>. This time, the code is going to be more complicated and the following is just for intuition. Note that when selecting the top kk hypotheses from the set of candidates, we must know which “beginning” they used (=parent hypothesis).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BeamSearchDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># notice the same arguments as for GreedyDecoderCell</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># compute new logits</span></div><div class="line">        logits, new_cell_state = self._attention_cell.step(embedding, state.cell_state)</div><div class="line"></div><div class="line">        <span class="comment"># compute log probs of the step (- log p(w) for all words w)</span></div><div class="line">        <span class="comment"># shape = [batch_size, beam_size, vocab_size]</span></div><div class="line">        step_log_probs = tf.nn.log_softmax(new_logits)</div><div class="line"></div><div class="line">        <span class="comment"># compute scores for the (beam_size * vocabulary_size) new hypotheses</span></div><div class="line">        log_probs = state.log_probs + step_log_probs</div><div class="line"></div><div class="line">        <span class="comment"># get top k hypotheses</span></div><div class="line">        new_probs, indices = tf.nn.top_k(log_probs, self._beam_size)</div><div class="line"></div><div class="line">        <span class="comment"># get ids of next token along with the parent hypothesis</span></div><div class="line">        new_ids = ...</div><div class="line">        new_parents = ...</div><div class="line"></div><div class="line">        <span class="comment"># compute new embeddings, new_finished, new_cell state...</span></div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div></pre></td></tr></table></figure><blockquote><p>Look at <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/beam_search_decoder_cell.py" target="_blank" rel="external">github</a> for the details. The main idea is that we add a beam dimension to every tensor, but when feeding it into <code>AttentionCell</code> we merge the beam dimension with the batch dimension. There is also some trickery involved to compute the parents and the new ids using modulos.</p></blockquote><h2 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I hope that you learned something with this post, either about the technique or Tensorflow. While the model achieves impressive performance (at least on short formulas with roughly 85% of the LaTeX being reconstructed), it still raises some questions that I list here:</p><p><em>How do we evaluate the performance of our model?</em>. We can use standard metrics from Machine Translation like <a href="https://en.wikipedia.org/wiki/BLEU" target="_blank" rel="external">BLEU</a> to evaluate how good the decoded LaTeX is compared to the reference. We can also choose to compile the predicted LaTeX sequence to get the image of the formula, and then compare this image to the orignal. As a formula is a sequence, computing the pixel-wise distance wouldn’t really make sense. A good idea is proposed by <a href="http://lstm.seas.harvard.edu/latex" target="_blank" rel="external">Harvard’s paper</a>. First, slice the image vertically. Then, compare the edit distance between these slices…</p><p><em>How to fix exposure bias?</em> While beam search generally achieves better results, it is not perfect and still suffers from <strong>exposure bias</strong>. During training, the model is never exposed to its errors! It also suffers from <strong>Loss-Evaluation Mismatch</strong>. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/ref.png" alt=""></p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/pred.png" alt=""></p><p><em>An Example of LaTeX generation - which one is the reference?</em></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">125</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">59</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>