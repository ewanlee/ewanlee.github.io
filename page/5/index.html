<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/5/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/5/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/" itemprop="url">Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-22T16:49:43+08:00">2018-01-22 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/" class="leancloud_visitors" data-flag-title="Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Source blog is <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">here</a>.</p><hr><p>We’ve <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">discovered</a> that <strong>evolution strategies (ES)</strong>, an optimization technique that’s been known for decades, rivals the performance of standard <strong>reinforcement learning (RL)</strong>techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL’s inconveniences.</p><p>In particular, ES is simpler to implement (there is no need for <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">backpropagation</a>), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer <a href="https://www.quora.com/What-are-hyperparameters-in-machine-learning" target="_blank" rel="external">hyperparameters</a>. This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on <a href="https://en.wikipedia.org/wiki/Finite_difference" target="_blank" rel="external">finite differences</a> along a few random directions at each step.</p><p>Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">“AlexNet” paper</a> showed how to design, scale and train convolutional neural networks (CNNs) to achieve extremely strong results on image recognition tasks, at a time when most researchers thought that CNNs were not a promising approach to computer vision. Similarly, in 2013, the <a href="https://arxiv.org/abs/1312.5602" target="_blank" rel="external">Deep Q-Learning paper</a> showed how to combine Q-Learning with CNNs to successfully solve Atari games, reinvigorating RL as a research field with exciting experimental (rather than theoretical) results. Likewise, our work demonstrates that ES achieves strong performance on RL benchmarks, dispelling the common belief that ES methods are impossible to apply to high dimensional problems.</p><p>ES is easy to implement and scale. Running on a computing cluster of 80 machines and 1,440 CPU cores, our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes about 10 hours). Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour.</p><p>In what follows, we’ll first briefly describe the conventional RL approach, contrast that with our ES approach, discuss the tradeoffs between ES and RL, and finally highlight some of our experiments.</p><h4 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h4><p>Let’s briefly look at how RL works. Suppose we are given some environment (e.g. a game) that we’d like to train an agent on. To describe the behavior of the agent, we define a policy function (the brain of the agent), which computes how the agent should act in any given situation. In practice, the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions. A typical policy function might have about 1,000,000 parameters, so our task comes down to finding the precise setting of these parameters such that the policy plays well (i.e. wins a lot of games).</p><p><img src="https://blog.openai.com/content/images/2017/03/first-graphic-1.png" alt="img"></p><p><em>Above: In the game of Pong, the policy could take the pixels of the screen and compute the probability of moving the player’s paddle (in green, on right) Up, Down, or neither.</em></p><p>The training process for the policy works as follows. Starting from a random initialization, we let the agent interact with the environment for a while and collect episodes of interaction (e.g. each episode is one game of Pong). We thus obtain a complete recording of what happened: what sequence of states we encountered, what actions we took in each state, and what the reward was at each step. As an example, below is a diagram of three episodes that each took 10 time steps in a hypothetical environment. Each rectangle is a state, and rectangles are colored green if the reward was positive (e.g. we just got the ball past our opponent) and red if the reward was negative (e.g. we missed the ball):</p><p><img src="https://blog.openai.com/content/images/2017/03/second-graphic-1.png" alt="img"></p><p>This diagram suggests a recipe for how we can improve the policy; whatever we happened to do leading up to the green states was good, and whatever we happened to do in the states leading up to the red areas was bad. We can then use backpropagation to compute a small update on the network’s parameters that would make the green actions more likely in those states in the future, and the red actions less likely in those states in the future. We expect that the updated policy works a bit better as a result. We then iterate the process: collect another batch of episodes, do another update, etc.</p><p><strong>Exploration by injecting noise in the actions.</strong> The policies we usually use in RL are stochastic, in that they only compute probabilities of taking any action. This way, during the course of training, the agent may find itself in a particular state many times, and at different times it will take different actions due to the sampling. This provides the signal needed for learning; some of those actions will lead to good outcomes, and get encouraged, and some of them will not work out, and get discouraged. We therefore say that we introduce exploration into the learning process by injecting noise into the agent’s actions, which we do by sampling from the action distribution at each time step. This will be in contrast to ES, which we describe next.</p><h4 id="Evolution-Strategies"><a href="#Evolution-Strategies" class="headerlink" title="Evolution Strategies"></a>Evolution Strategies</h4><p><strong>On “Evolution”.</strong> Before we dive into the ES approach, it is important to note that despite the word “evolution”, ES has very little to do with biological evolution. Early versions of these techniques may have been inspired by biological evolution and the approach can, on an abstract level, be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations. However, the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black-box stochastic optimization techniques.</p><p><strong>Black-box optimization.</strong> In ES, we forget entirely that there is an agent, an environment, that there are neural networks involved, or that interactions take place over time, etc. The whole setup is that 1,000,000 numbers (which happen to describe the parameters of the policy network) go in, 1 number comes out (the total reward), and we want to find the best setting of the 1,000,000 numbers. Mathematically, we would say that we are optimizing a function <code>f(w)</code> with respect to the input vector <code>w</code>(the parameters / weights of the network), but we make no assumptions about the structure of <code>f</code>, except that we can evaluate it (hence “black box”).</p><p><strong>The ES algorithm.</strong> Intuitively, the optimization is a “guess and check” process, where we start with some random parameters and then repeatedly 1) tweak the guess a bit randomly, and 2) move our guess slightly towards whatever tweaks worked better. Concretely, at each step we take a parameter vector <code>w</code> and generate a population of, say, 100 slightly different parameter vectors <code>w1 ... w100</code> by jittering <code>w</code> with gaussian noise. We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while, and add up all the rewards in each case. The updated parameter vector then becomes the weighted sum of the 100 vectors, where each weight is proportional to the total reward (i.e. we want the more successful candidates to have a higher weight). Mathematically, you’ll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences, except we only do it along 100 random directions. Yet another way to see it is that we’re still doing RL (Policy Gradients, or <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> specifically), where the agent’s actions are to emit entire parameter vectors using a gaussian policy.</p><p><img src="https://blog.openai.com/content/images/2017/03/evo.png" alt="img"></p><p><em>Above: ES optimization process, in a setting with only two parameters and a reward function (red = high, blue = low). At each iteration we show the current parameter value (in white), a population of jittered samples (in black), and the estimated gradient (white arrow). We keep moving the parameters to the top of the arrow until we converge to a local optimum. You can reproduce this figure with this notebook.</em></p><p><strong>Code sample.</strong> To make the core algorithm concrete and to highlight its simplicity, here is a short example of optimizing a quadratic function using ES (or see this <a href="https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d" target="_blank" rel="external">longer version</a> with more comments):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># simple example: minimize a quadratic around some solution point</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">solution = np.array([<span class="number">0.5</span>, <span class="number">0.1</span>, <span class="number">-0.3</span>])</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(w)</span>:</span> <span class="keyword">return</span> -np.sum((w - solution)**<span class="number">2</span>)</div><div class="line"></div><div class="line">npop = <span class="number">50</span>      <span class="comment"># population size</span></div><div class="line">sigma = <span class="number">0.1</span>    <span class="comment"># noise standard deviation</span></div><div class="line">alpha = <span class="number">0.001</span>  <span class="comment"># learning rate</span></div><div class="line">w = np.random.randn(<span class="number">3</span>) <span class="comment"># initial guess</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</div><div class="line">  N = np.random.randn(npop, <span class="number">3</span>)</div><div class="line">  R = np.zeros(npop)</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(npop):</div><div class="line">    w_try = w + sigma*N[j]</div><div class="line">    R[j] = f(w_try)</div><div class="line">  A = (R - np.mean(R)) / np.std(R)</div><div class="line">  w = w + alpha/(npop*sigma) * np.dot(N.T, A)</div></pre></td></tr></table></figure><p><strong>Injecting noise in the parameters.</strong> Notice that the objective is identical to the one that RL optimizes: the expected reward. However, RL injects noise in the action space and uses backpropagation to compute the parameter updates, while ES injects noise directly in the parameter space. Another way to describe this is that RL is a “guess and check” on actions, while ES is a “guess and check” on parameters. Since we’re injecting noise in the parameters, it is possible to use deterministic policies (and we do, in our experiments). It is also possible to add noise in both actions and parameters to potentially combine the two approaches.</p><h4 id="Tradeoffs-between-ES-and-RL"><a href="#Tradeoffs-between-ES-and-RL" class="headerlink" title="Tradeoffs between ES and RL"></a>Tradeoffs between ES and RL</h4><p>ES enjoys multiple advantages over RL algorithms (some of them are a little technical):</p><ul><li><strong>No need for backpropagation</strong>. ES only requires the forward pass of the policy and does not require backpropagation (or value function estimation), which makes the code shorter and between 2-3 times faster in practice. On memory-constrained systems, it is also not necessary to keep a record of the episodes for a later update. There is also no need to worry about exploding gradients in RNNs. Lastly, we can explore a much larger function class of policies, including networks that are not differentiable (such as in binary networks), or ones that include complex modules (e.g. pathfinding, or various optimization layers).</li><li><strong>Highly parallelizable.</strong> ES only requires workers to communicate a few scalars between each other, while in RL it is necessary to synchronize entire parameter vectors (which can be millions of numbers). Intuitively, this is because we control the random seeds on each worker, so each worker can locally reconstruct the perturbations of the other workers. Thus, all that we need to communicate between workers is the reward of each perturbation. As a result, we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization.</li><li><strong>Higher robustness.</strong> Several hyperparameters that are difficult to set in RL implementations are side-stepped in ES. For example, RL is not “scale-free”, so one can achieve very different learning outcomes (including a complete failure) with different settings of the frame-skip hyperparameter in Atari. As we show in our work, ES works about equally well with any frame-skip.</li><li><strong>Structured exploration.</strong> Some RL algorithms (especially policy gradients) initialize with random policies, which often manifests as random jitter on spot for a long time. This effect is mitigated in Q-Learning due to epsilon-greedy policies, where the max operation can cause the agents to perform some consistent action for a while (e.g. holding down a left arrow). This is more likely to do something in a game than if the agent jitters on spot, as is the case with policy gradients. Similar to Q-learning, ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration.</li><li><strong>Credit assignment over long time scales.</strong> By studying both ES and RL gradient estimators mathematically we can see that ES is an attractive choice especially when the number of time steps in an episode is long, where actions have longlasting effects, or if no good value function estimates are available.</li></ul><p>Conversely, we also found some challenges to applying ES in practice. One core problem is that in order for ES to work, adding noise in parameters must lead to different outcomes to obtain some gradient signal. As we elaborate on in our paper, we found that the use of virtual batchnorm can help alleviate this problem, but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary. As an example of a related difficulty, we found that in Montezuma’s Revenge, one is very unlikely to get the key in the first level with a random network, while this is occasionally possible with random actions.</p><h4 id="ES-is-competitive-with-RL"><a href="#ES-is-competitive-with-RL" class="headerlink" title="ES is competitive with RL"></a>ES is competitive with RL</h4><p>We compared the performance of ES and RL on two standard RL benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo task (see examples below) contains a physically-simulated articulated figure, where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward. Below are some example agents trained on three MuJoCo control tasks, where the objective is to move forward:</p><p><img src="https://blog.openai.com/content/images/2017/03/out.gif" alt="img"></p><p>We usually compare the performance of algorithms by looking at their efficiency of learning from data; as a function of how many states we’ve seen, what is our average reward? Here are the example learning curves that we obtain, in comparison to RL (the <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="external">TRPO</a> algorithm in this case):</p><p><img src="https://blog.openai.com/content/images/2017/03/es_vs_trpo_full.png" alt="img"></p><p><strong>Data efficiency comparison</strong>. The comparisons above show that ES (orange) can reach a comparable performance to TRPO (blue), although it doesn’t quite match or surpass it in all cases. Moreover, by scanning horizontally we can see that ES is less efficient, but no worse than about a factor of 10 (note the x-axis is in log scale).</p><p><strong>Wall clock comparison</strong>. Instead of looking at the raw number of states seen, one can argue that the most important metric to look at is the wall clock time: how long (in number of seconds) does it take to solve a given problem? This quantity ultimately dictates the achievable speed of iteration for a researcher. Since ES requires negligible communication between workers, we were able to solve one of the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 machines in only 10 minutes. As a comparison, in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours. It is also possible that the performance of RL could also improve with more algorithmic and engineering effort, but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements.</p><p>Below are a few videos of 3D humanoid walkers trained with ES. As we can see, the results have quite a bit of variety, based on which local minimum the optimization ends up converging into.</p><p><img src="https://blog.openai.com/content/images/2017/03/out-1.gif" alt="img"></p><p>On Atari, ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day. Below are some result snippets on Pong, Seaquest and Beamrider. These videos show the preprocessed frames, which is exactly what the agent sees when it is playing:</p><p><img src="https://blog.openai.com/content/images/2017/03/atari.gif" alt="img"></p><p>In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels.</p><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p>ES is an algorithm from the neuroevolution literature, which has a long history in AI and a complete literature review is beyond the scope of this post. However, we encourage an interested reader to look at <a href="https://en.wikipedia.org/wiki/Neuroevolution" target="_blank" rel="external">Wikipedia</a>, <a href="http://www.scholarpedia.org/article/Neuroevolution" target="_blank" rel="external">Scholarpedia</a>, and Jürgen Schmidhuber’s <a href="https://arxiv.org/abs/1404.7828" target="_blank" rel="external">review article (Section 6.6)</a>. The work that most closely informed our approach is <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a> by Wierstra et al. 2014. Compared to this work and much of the work it has inspired, our focus is specifically on scaling these algorithms to large-scale, distributed settings, finding components that make the algorithms work better with deep neural networks (e.g. <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="external">virtual batch norm</a>), and evaluating them on modern RL benchmarks.</p><p>It is also worth noting that neuroevolution-related approaches have seen some recent resurgence in the machine learning literature, for example with <a href="https://arxiv.org/abs/1609.09106" target="_blank" rel="external">HyperNetworks</a>, <a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="external">“Large-Scale Evolution of Image Classifiers”</a> and <a href="https://arxiv.org/abs/1606.02580" target="_blank" rel="external">“Convolution by Evolution”</a>.</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Our work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent-environment benchmarks, while offering significant benefits related to code complexity and ease of scaling to large-scale distributed settings. We also expect that more exciting work can be done by revisiting other ideas from this line of work, such as indirect encoding methods, or evolving the network structure in addition to the parameters.</p><p><strong>Note on supervised learning</strong>. It is also important to note that supervised learning problems (e.g. image classification, speech recognition, or most other tasks in the industry), where one can compute the exact gradient of the loss function with backpropagation, are not directly impacted by these findings. For example, in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1,000 times slower than using backpropagation. It is only in RL settings, where one has to estimate the gradient of the expected reward by sampling, where ES becomes competitive.</p><p><strong>Code release</strong>. Finally, if you’d like to try running ES yourself, we encourage you to dive into the full details by reading <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">our paper</a> or looking at our code on this <a href="https://github.com/openai/evolution-strategies-starter" target="_blank" rel="external">Github repo</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" itemprop="url">Seq2Seq with Attention and Beam Search [Repost]</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-21T17:35:50+08:00">2018-01-21 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" class="leancloud_visitors" data-flag-title="Seq2Seq with Attention and Beam Search [Repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Source Post is <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">here</a></p><h2 id="Sequence-to-Sequence-basics"><a href="#Sequence-to-Sequence-basics" class="headerlink" title="Sequence to Sequence basics"></a>Sequence to Sequence basics</h2><p>Let’s explain the sequence to sequence framework as we’ll rely on it for our model. Let’s start with the simplest version on the translation task.</p><blockquote><p>As an example, let’s translate <code>how are you</code> in French <code>comment vas tu</code>.</p></blockquote><h3 id="Vanilla-Seq2Seq"><a href="#Vanilla-Seq2Seq" class="headerlink" title="Vanilla Seq2Seq"></a>Vanilla Seq2Seq</h3><p>The Seq2Seq framework relies on the <strong>encoder-decoder</strong> paradigm. The <strong>encoder</strong> <em>encodes</em> the input sequence, while the <strong>decoder</strong> <em>produces</em> the target sequence</p><p><strong>Encoder</strong></p><p>Our input sequence is <code>how are you</code>. Each word from the input sequence is associated to a vector $w \in \mathbb{R}^d$ (via a lookup table). In our case, we have 3 words, thus our input will be transformed into $[w_0, w_1, w_2] \in \mathbb{R}^{d \times 3}$. Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation $e$. Let’s write the hidden states $[e_0,e_1,e_2]$ (and thus $e=e_2$)</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg" alt="Vanilla Encoder"></p><p><em>Vanilla Encoder</em></p><p><strong>Decoder</strong></p><p>Now that we have a vector ee that captures the meaning of the input sequence, we’ll use it to generate the target sequence word by word. Feed to another LSTM cell: ee as hidden state and a special <em>start of sentence</em> vector $w_{\text{sos}}$ as input. The LSTM computes the next hidden state $h_0 \in \mathbb{R}^{h}$. Then, we apply some function $g : \mathbb{R}^h \mapsto \mathbb{R}^V$ so that $s_0 := g(h_0) \in \mathbb{R}^V$ is a vector of the same size as the vocabulary.<br>$$<br>\begin{align}<br>h_0 &amp;= \operatorname{LSTM}\left(e, w_{sos} \right)\\<br>s_0 &amp;= g(h_0)\\<br>p_0 &amp;= \operatorname{softmax}(s_0)\\<br>i_0 &amp;= \operatorname{argmax}(p_0)\\<br>\end{align}<br>$$<br>Then, apply a softmax to $s_0$ to normalize it into a vector of probabilities $p_0 \in \mathbb{R}^V$ . Now, each entry of $p_0$ will measure how likely is each word in the vocabulary. Let’s say that the word <em>“comment”</em> has the highest probability (and thus $i_0 = \operatorname{argmax}(p_0)$) corresponds to the index of <em>“comment”</em>). Get a corresponding vector and $w_{i_0} = w_{comment}$ repeat the procedure: the LSTM will take $h_0$ as hidden state and $w_{comment}$ as input and will output a probability vector $p_1$ over the second word, etc.<br>$$<br>\begin{align}<br>h_1 &amp;= \operatorname{LSTM}\left(h_0, w_{i_0} \right)\\<br>s_1 &amp;= g(h_1)\\<br>p_1 &amp;= \operatorname{softmax}(s_1)\\<br>i_1 &amp;= \operatorname{argmax}(p_1)<br>\end{align}<br>$$<br>The decoding stops when the predicted word is a special <em>end of sentence</em> token.</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg" alt="Vanilla Decoder"></p><p><em>Vanilla Decoder</em></p><blockquote><p>Intuitively, the hidden vector represents the “amount of meaning” that has not been decoded yet.</p></blockquote><p>The above method aims at modelling the distribution of the next word conditionned on the beginning of the sentence<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_1, \dots, y_{t}, x_0, \dots, x_n \right]<br>$$<br>by writing<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_t, h_{t}, e \right]<br>$$</p><h3 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h3><p>The previous model has been refined over the past few years and greatly benefited from what is known as <strong>attention</strong>. Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM. One way of performing attention is explained by <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Bahdanau et al.</a>. We slightly modify the reccurrence formula that we defined above by adding a new vector $c_t$ to the input of the LSTM<br>$$<br>\begin{align}<br>h_{t} &amp;= \operatorname{LSTM}\left(h_{t-1}, [w_{i_{t-1}}, c_t] \right)\\<br>s_t &amp;= g(h_t)\\<br>p_t &amp;= \operatorname{softmax}(s_t)\\<br>i_t &amp;= \operatorname{argmax}(p_t)<br>\end{align}<br>$$<br>The vector ctct is the attention (or <strong>context</strong>) vector. We compute a new context vector at each decoding step. First, with a function $f (h_{t-1}, e_{t’}) \mapsto \alpha_{t’} \in \mathbb{R}$, compute a score for each hidden state $e_{t^{\prime}}$ of the encoder. Then, normalize the sequence of using a softmax and compute $c_t$ as the weighted average of the $e_{t^{\prime}}$.<br>$$<br>\begin{align}<br>\alpha_{t’} &amp;= f(h_{t-1}, e_{t’}) \in \mathbb{R} &amp; \text{for all } t’\\<br>\bar{\alpha} &amp;= \operatorname{softmax} (\alpha)\\<br>c_t &amp;= \sum_{t’=0}^n \bar{\alpha}_{t’} e_{t’}<br>\end{align}<br>$$<br><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg" alt="Attention Mechanism"></p><p><em>Attention Mechanism</em></p><p>The choice of the function ff varies, but is usually one of the following<br>$$<br>f(h_{t-1}, e_{t’}) =<br>\begin{cases}<br>h_{t-1}^T e_{t’} &amp; \text{dot}\\<br>h_{t-1}^T W e_{t’} &amp; \text{general}\\<br>v^T \tanh \left(W [h_{t-1}, e_{t’}]\right) &amp; \text{concat}\\<br>\end{cases}<br>$$<br>It turns out that the attention weighs $\bar{\alpha}$ can be easily interpreted. When generating the word <code>vas</code>(corresponding to <code>are</code> in English), we expect $\bar{\alpha} _ {\text{are}}$ are to be close to $1$ while $\bar{\alpha} _ {\text{how}}$ and $\bar{\alpha} _ {\text{you}}$ to be close to $0$. Intuitively, the context vector $c$ will be roughly equal to the hidden vector of <code>are</code> and it will help to generate the French word <code>vas</code>.</p><p>By putting the attention weights into a matrix (rows = input sequence, columns = output sequence), we would have access to the <strong>alignment</strong> between the words from the English and French sentences… (see <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="external">page 6</a>) There is still a lot of things to say about sequence to sequence models (for instance, it works better if the encoder processes the input sequence <em>backwards</em>…).</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><blockquote><p>What happens if the first time step is not sure about wether it should generate <code>comment</code> or <code>vas</code> (most likely case at the beginning of the training)? Then it would mess up the entire sequence, and the model will hardly learn anything…</p></blockquote><p>If we use the predicted token as input to the next step during training (as explained above), errors would accumulate and the model would rarely be exposed to the correct distribution of inputs, making training slow or impossible. To speedup things, one trick is to feed the actual output sequence (<code>&lt;sos&gt;</code> <code>comment</code> <code>vas</code> <code>tu</code>) into the decoder’s LSTM and predict the next token at every position (<code>comment</code> <code>vas</code> <code>tu</code> <code>&lt;eos&gt;</code>).</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_training.svg" alt="Training"></p><p><em>Training</em></p><p>The decoder outputs vectors of probability over the vocabulary $p_i \in \mathbb{R}^V$ for each time step. Then, for a given target sequence $y_1, \dots, y_n$, we can compute its probability as the product of the probabilities of each token being produced at each relevant time step:<br>$$<br>\mathbb{P}\left(y_1, \dots, y_m \right) = \prod_{i=1}^m p_i [y_i]<br>$$<br>where $p_i [y_i]$ means that we extract the $y_i$-th entry of the probability vector $p_i$ from the $i$-th decoding step. In particular, we can compute the probability of the actual target sequence. A perfect system would give a probabilty of 1 to this target sequence, so we are going to train our network to maximize the probability of the target sequence, which is the same as minimizing<br>$$<br>\begin{align}<br>-\log \mathbb{P} \left(y_1, \dots, y_m \right) &amp;= - \log \prod_{i=1}^m p_i [y_i]\\<br>&amp;= - \sum_{i=1}^n \log p_i [y_i]\\<br>\end{align}<br>$$<br>in our example, this is equal to<br>$$</p><ul><li>\log p_1[\text{comment}] - \log p_2[\text{vas}] - \log p_3[\text{tu}] - \log p_4[\text{<eos>}]<br>$$<br>and you recognize the standard cross entropy: we actually are minimizing the cross entropy between the target distribution (all one-hot vectors) and the predicted distribution outputed by our model (our vectors $p_i$).</eos></li></ul><h2 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h2><p>The main takeaway from the discussion above is that for the same model, we can define different behaviors. In particular, we defined a specific behavior that speeds up training.</p><blockquote><p>What about inference/testing time then? Is there an other way to decode a sentence?</p></blockquote><p>There indeed are 2 main ways of performing decoding at testing time (translating a sentence for which we don’t have a translation). The first of these methods is the one covered at the beginning of the article: <strong>greedy decoding</strong>. It is the most natural way and it consists in feeding to the next step the most likely word predicted at the previous step.</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg" alt="Greedy Decoder - feeds the best token to the next step"></p><p><em>Greedy Decoder - feeds the best token to the next step</em></p><blockquote><p>But didn’t we say that this behavior is likely to accumulate errors?</p></blockquote><p>Even after having trained the model, it can happen that the model makes a small error (and gives a small advantage to <code>vas</code> over <code>comment</code> for the first step of the decoding). This would mess up the entire decoding…</p><p>There is a better way of performing decoding, called <strong>Beam Search</strong>. Instead of only predicting the token with the best score, we keep track of $k$ hypotheses (for example $k=5$, we refer to kk as the <strong>beam size</strong>). At each new time step, for these 5 hypotheses we have $V$ new possible tokens. It makes a total of $5V$ new hypotheses. Then, only keep the $5$ best ones, and so on… Formally, define $\mathcal{H}_t$ the set of hypotheses decoded at time step $t$.<br>$$<br>\mathcal{H}_ t := \{ (w^1_1, \dots, w^1_t), \dots, (w^k_1, \dots, w^k_t) \}<br>$$<br>For instance if $k=2$, one possible $\mathcal{H}_2$ would be<br>$$<br>\mathcal{H}_ 2 := \{ (\text{comment vas}), (\text{comment tu}) \}<br>$$<br>Now we consider all the possible candidates $\mathcal{C}_{t+1}$, produced from $\mathcal{H}_t$ by adding all possible new tokens<br>$$<br>\mathcal{C}_ {t+1} := \bigcup_{i=1}^k \{ (w^i_1, \dots, w^i_t, 1), \dots, (w^i_1, \dots, w^i_t, V) \}<br>$$<br>and keep the $k$ highest scores (probability of the sequence). If we keep our example<br>$$<br>\begin{align}<br>\mathcal{C}_ 3 =&amp; \{ (\text{comment vas comment}), (\text{comment vas vas}), (\text{comment vas tu})\} \\<br>\cup &amp; \{ (\text{comment tu comment}), \ \ (\text{comment tu vas}), \ \ (\text{comment tu tu}) \}<br>\end{align}<br>$$<br>and for instance we can imagine that the 2 best ones would be<br>$$<br>\mathcal{H}_ 3 := \{ (\text{comment vas tu}), (\text{comment tu vas}) \}<br>$$<br>Once every hypothesis reached the <code>&lt;eos&gt;</code> token, we return the hypothesis with the highest score.</p><blockquote><p>If we use <strong>beam search</strong>, a small error at the first step might be rectified at the next step, as we keep the gold hypthesis in the beam!</p></blockquote><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this article we covered the seq2seq concepts. We showed that training is different than decoding. We covered two methods for decoding: <strong>greedy</strong> and <strong>beam search</strong>. While beam search generally achieves better results, it is not perfect and still suffers from <strong>exposure bias</strong>. During training, the model is never exposed to its errors! It also suffers from <strong>Loss-Evaluation Mismatch</strong>. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</p><p>Now, let’s apply Seq2Seq for LaTeX generation from images!</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_task.svg" alt="Producing LaTeX code from an image"></p><p><em>Producing LaTeX code from an image</em></p><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>Previous part covered the concepts of <strong>sequence-to-sequence</strong> applied to Machine Translation. The same framework can be applied to our LaTeX generation problem. The input sequence would just be replaced by an image, preprocessed with some convolutional model adapted to OCR (in a sense, if we <em>unfold</em> the pixels of an image into a sequence, this is exactly the same problem). This idea proved to be efficient for image captioning (see the reference paper <a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Show, Attend and Tell</a>). Building on some <a href="https://arxiv.org/pdf/1609.04938v1.pdf" target="_blank" rel="external">great work</a> from the Harvard NLP group, my teammate <a href="https://www.linkedin.com/in/romain-sauvestre-241171a2" target="_blank" rel="external">Romain</a> and I chose to follow a similar approach.</p><blockquote><p>Keep the seq2seq framework but replace the encoder by a convolutional network over the image!</p></blockquote><p>Good Tensorflow implementations of such models were hard to find. Together with this post, I am releasing the <a href="https://github.com/guillaumegenthial/im2latex" target="_blank" rel="external">code</a> and hope some will find it useful. You can use it to train your own image captioning model or adapt it for a more advanced use. <a href="https://github.com/guillaumegenthial/im2latex" target="_blank" rel="external">The code</a> does <strong>not</strong> rely on the <a href="https://www.tensorflow.org/versions/master/api_guides/python/contrib.seq2seq" target="_blank" rel="external">Tensorflow Seq2Seq library</a> as it was not entirely ready at the time of the project and I also wanted more flexibility (but adopts a similar interface).</p><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>To train our model, we’ll need labeled examples: images of formulas along with the LaTeX code used to generate the images. A good source of LaTeX code is <a href="https://arxiv.org/" target="_blank" rel="external">arXiv</a>, that has thousands of articles under the <code>.tex</code> format. After applying some heuristics to find equations in the <code>.tex</code> files, keeping only the ones that actually compile, the <a href="https://zenodo.org/record/56198#.WflVu0yZPLZ" target="_blank" rel="external">Harvard NLP group</a> extracted $\sim 100,000$ formulas.</p><blockquote><p>Wait… Don’t you have a problem as different LaTeX codes can give the same image?</p></blockquote><p>Good point: <code>(x^2 + 1)</code> and <code>\left( x^{2} + 1 \right)</code> indeed give the same output. That’s why Harvard’s paper found out that normalizing the data using a parser (<a href="https://khan.github.io/KaTeX/" target="_blank" rel="external">KaTeX</a>) improved performance. It forces adoption of some conventions, like writing <code>x ^ { 2 }</code> instead of <code>x^2</code>, etc. After normalization, they end up with a <code>.txt</code> file containing one formula per line that looks like</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">\alpha + \beta</div><div class="line">\frac &#123; 1 &#125; &#123; 2 &#125;</div><div class="line">\frac &#123; \alpha &#125; &#123; \beta &#125;</div><div class="line">1 + 2</div></pre></td></tr></table></figure><p>From this file, we’ll produce images <code>0.png</code>, <code>1.png</code>, etc. and a matching file mapping the image files to the indices (=line numbers) of the formulas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">0.png 0</div><div class="line">1.png 1</div><div class="line">2.png 2</div><div class="line">3.png 3</div></pre></td></tr></table></figure><p>The reason why we use this format is that it is flexible and allows you to use the pre-built <a href="https://zenodo.org/record/56198#.WflVu0yZPLZ" target="_blank" rel="external">dataset from Harvard</a> (You may need to use the preprocessing scripts as explained <a href="https://github.com/harvardnlp/im2markup" target="_blank" rel="external">here</a>). You’ll also need to have <code>pdflatex</code> and <code>ImageMagick</code> installed.</p><p>We also build a vocabulary, to map LaTeX tokens to indices that will be given as input to our model. If we keep the same data as above, our vocabulary will look like</p><p><code>+</code> <code>1</code> <code>2</code> <code>\alpha</code> <code>\beta</code> <code>\frac</code> <code>{</code> <code>}</code></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>Our model is going to rely on a variation of the Seq2Seq model, adapted to images. First, let’s define the input of our graph. Not surprisingly we get as input a batch of black-and-white images of shape $[H,W]$ and a batch of formulas (ids of the LaTeX tokens):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># batch of images, shape = (batch size, height, width, 1)</span></div><div class="line">img = tf.placeholder(tf.uint8, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'img'</span>)</div><div class="line"><span class="comment"># batch of formulas, shape = (batch size, length of the formula)</span></div><div class="line">formula = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>), name=<span class="string">'formula'</span>)</div><div class="line"><span class="comment"># for padding</span></div><div class="line">formula_length = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, ), name=<span class="string">'formula_length'</span>)</div></pre></td></tr></table></figure><blockquote><p>A special note on the type of the image input. You may have noticed that we use <code>tf.uint8</code>. This is because our image is encoded in grey-levels (integers from <code>0</code> to <code>255</code> - and $2^8=256$). Even if we could give a <code>tf.float32</code> Tensor as input to Tensorflow, this would be 4 times more expensive in terms of memory bandwith. As data starvation is one of the main bottlenecks of GPUs, this simple trick can save us some training time. For further improvement of the data pipeline, have a look at <a href="https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/data" target="_blank" rel="external">the new Tensorflow data pipeline</a>.</p></blockquote><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p><strong>High-level idea</strong> Apply some convolutional network on top of the image an flatten the output into a sequence of vectors $[e_1, \cdots, e_n]$, each of those corresponding to a region of the input image. These vectors will correspond to the hidden vectors of the LSTM that we used for translation.</p><blockquote><p>Once our image is transformed into a sequence, we can use the seq2seq model!</p></blockquote><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_encoder.svg" alt="Convolutional Encoder - produces a sequence of vectors"></p><p><em>Convolutional Encoder - produces a sequence of vectors</em></p><p>We need to extract features from our image, and for this, nothing has (<a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="external">yet</a>) been proven more effective than convolutions. Here, there is nothing much to say except that we pick some architecture that has been proven to be effective for Optical Character Recognition (OCR), which stacks convolutional layers and max-pooling to produce a Tensor of shape $[H^{\prime},W^{\prime},512]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># casting the image back to float32 on the GPU</span></div><div class="line">img = tf.cast(img, tf.float32) / <span class="number">255.</span></div><div class="line"></div><div class="line">out = tf.layers.conv2d(img, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line"><span class="comment"># encoder representation, shape = (batch size, height', width', 512)</span></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"VALID"</span>, activation=tf.nn.relu)</div></pre></td></tr></table></figure><p>Now that we have extracted some features from the image, let’s <strong>unfold</strong> the image to get a sequence so that we can use our sequence to sequence framework. We end up with a sequence of length $[H^{\prime},W^{\prime}]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">H, W = tf.shape(out)[<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">seq = tf.reshape(out, shape=[<span class="number">-1</span>, H*W, <span class="number">512</span>])</div></pre></td></tr></table></figure><blockquote><p>Don’t you loose a lot of structural information by reshaping? I’m afraid that when performing attention over the image, my decoder won’t be able to understand the location of each feature vector in the original image!</p></blockquote><p>It turns out that the model manages to work despite this issue, but that’s not completely satisfying. In the case of translation, the hidden states of the LSTM contained some positional information that was computed by the LSTM (after all, LSTM are by essence sequential). Can we fix this issue?</p><p><strong>Positional Embeddings</strong> I decided to follow the idea from <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external">Attention is All you Need</a> that adds <em>positional embeddings</em> to the image representation (<code>out</code>), and has the huge advantage of not adding any new trainable parameter to our model. The idea is that for each position of the image, we compute a vector of size $512$ such that its components are coscos or sinsin. More formally, the (2i)-th and (2i+1)-th entries of my positional embedding $v$ at position $p$ will be<br>$$<br>\begin{align}<br>v_{2i} &amp;= \sin\left(p / f^{2i}\right)\\<br>v_{2i+1} &amp;= \cos\left(p / f^{2i}\right)\\<br>\end{align}<br>$$<br>where $f$ is some frequency parameter. Intuitively, because $\sin(a+b)$ and $\cos⁡(a+b)$ can be expressed in terms of $\sin⁡(b)$ , $\sin(a)$ , $\cos⁡(b)$ and $\cos⁡(a)$, there will be linear dependencies between the components of distant embeddings, authorizing the model to extract relative positioning information. Good news: the tensorflow code for this technique is available in the library <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="external">tensor2tensor</a>, so we just need to reuse the same function and transform our <code>out</code> with the following call</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">out = add_timing_signal_nd(out)</div></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Now that we have a sequence of vectors $[e_1, \dots, e_n]$ that represents our input image, let’s decode it! First, let’s explain what variant of the Seq2Seq framework we are going to use.</p><p><strong>First hidden vector of the decoder’s LSTM</strong> In the seq2seq framework, this is usually just the last hidden vector of the encoder’s LSTM. Here, we don’t have such a vector, so a good choice would be to learn to compute it with a matrix $W$ and a vector $b$<br>$$<br>h_0 = \tanh\left( W \cdot \left( \frac{1}{n} \sum_{i=1}^n e_i\right) + b \right)<br>$$<br>This can be done in Tensorflow with the following logic</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">img_mean = tf.reduce_mean(seq, axis=<span class="number">1</span>)</div><div class="line">W = tf.get_variable(<span class="string">"W"</span>, shape=[<span class="number">512</span>, <span class="number">512</span>])</div><div class="line">b = tf.get_variable(<span class="string">"b"</span>, shape=[<span class="number">512</span>])</div><div class="line">h = tf.tanh(tf.matmul(img_mean, W) + b)</div></pre></td></tr></table></figure><p><strong>Attention Mechanism</strong> We first need to compute a score $\alpha_{t^{\prime}}$ for each vector $e_{t^{\prime}}$ of the sequence. We use the following method<br>$$<br>\begin{align}<br>\alpha_{t’} &amp;= \beta^T \tanh\left( W_1 \cdot e_{t’} + W_2 \cdot h_{t} \right)\\<br>\bar{\alpha} &amp;= \operatorname{softmax}\left(\alpha\right)\\<br>c_t &amp;= \sum_{i=1}^n \bar{\alpha}_{t’} e_{t’}\\<br>\end{align}<br>$$<br>This can be done in Tensorflow with the follwing code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># over the image, shape = (batch size, n, 512)</span></div><div class="line">W1_e = tf.layers.dense(inputs=seq, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># over the hidden vector, shape = (batch size, 512)</span></div><div class="line">W2_h = tf.layers.dense(inputs=h, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># sums the two contributions</span></div><div class="line">a = tf.tanh(W1_e + tf.expand_dims(W2_h, axis=<span class="number">1</span>))</div><div class="line">beta = tf.get_variable(<span class="string">"beta"</span>, shape=[<span class="number">512</span>, <span class="number">1</span>], dtype=tf.float32)</div><div class="line">a_flat = tf.reshape(a, shape=[<span class="number">-1</span>, <span class="number">512</span>])</div><div class="line">a_flat = tf.matmul(a_flat, beta)</div><div class="line">a = tf.reshape(a, shape=[<span class="number">-1</span>, n])</div><div class="line"></div><div class="line"><span class="comment"># compute weights</span></div><div class="line">a = tf.nn.softmax(a)</div><div class="line">a = tf.expand_dims(a, axis=<span class="number">-1</span>)</div><div class="line">c = tf.reduce_sum(a * seq, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Note that the line <code>W1_e = tf.layers.dense(inputs=seq, units=512, use_bias=False)</code> is common to every decoder time step, so we can just compute it once and for all. The dense layer with no bias is just a matrix multiplication.</p></blockquote><p>Now that we have our attention vector, let’s just add a small modification and compute an other vector $o_{t-1}$ (as in <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="external">Luong, Pham and Manning</a>) that we will use to make our final prediction and that we will feed as input to the LSTM at the next step. Here $w_{t−1}$ denotes the embedding of the token generated at the previous step.</p><blockquote><p>$o_t$ passes some information about the distribution from the previous time step as well as the confidence it had for the predicted token</p></blockquote><p>$$<br>\begin{align}<br>h_t &amp;= \operatorname{LSTM}\left( h_{t-1}, [w_{t-1}, o_{t-1}] \right)\\<br>c_t &amp;= \operatorname{Attention}([e_1, \dots, e_n], h_t)\\<br>o_{t} &amp;= \tanh\left(W_3 \cdot [h_{t}, c_t] \right)\\<br>p_t &amp;= \operatorname{softmax}\left(W_4 \cdot o_{t} \right)\\<br>\end{align}<br>$$</p><p>and now the code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute o</span></div><div class="line">W3_o = tf.layers.dense(inputs=tf.concat([h, c], axis=<span class="number">-1</span>), units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line">o = tf.tanh(W3_o)</div><div class="line"></div><div class="line"><span class="comment"># compute the logits scores (before softmax)</span></div><div class="line">logits = tf.layers.dense(inputs=o, units=vocab_size, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># the softmax will be computed in the loss or somewhere else</span></div></pre></td></tr></table></figure><blockquote><p>If I read carefully, I notice that for the first step of the decoding process, we need to compute an $o_0$ too, right?</p></blockquote><p>This is a good point, and we just use the same technique that we used to generate $h_0$ but with different weights.</p><h2 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h2><blockquote><p>We’ll need to create 2 different outputs in the Tensorflow graph: one for training (that uses the <code>formula</code>and feeds the ground truth at each time step, see <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">part I</a>) and one for test time (that ignores everything about the actual <code>formula</code> and uses the prediction from the previous step).</p></blockquote><h3 id="AttentionCell"><a href="#AttentionCell" class="headerlink" title="AttentionCell"></a>AttentionCell</h3><p>We’ll need to encapsulate the reccurent logic into a custom cell that inherits <code>RNNCell</code>. Our custom cell will be able to call the LSTM cell (initialized in the <code>__init__</code>). It also has a special recurrent state that combines the LSTM state and the vector oo (as we need to pass it through). An elegant way is to define a namedtuple for this recurrent state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">AttentionState = collections.namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.lstm_cell = LSTMCell(<span class="number">512</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Args:</div><div class="line">            inputs: shape = (batch_size, dim_embeddings) embeddings from previous time step</div><div class="line">            cell_state: (AttentionState) state from previous time step</div><div class="line">        """</div><div class="line">        lstm_state, o = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([inputs, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># apply previous logic</span></div><div class="line">        c = ...</div><div class="line">        new_o  = ...</div><div class="line">        logits = ...</div><div class="line"></div><div class="line">        new_state = AttentionState(new_lstm_state, new_o)</div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><p>Then, to compute our output sequence, we just need to call the previous cell on the sequence of LaTeX tokens. We first produce the sequence of token embeddings to which we concatenate the special <code>&lt;sos&gt;</code> token. Then, we call <code>dynamic_rnn</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. get token embeddings</span></div><div class="line">E = tf.get_variable(<span class="string">"E"</span>, shape=[vocab_size, <span class="number">80</span>], dtype=tf.float32)</div><div class="line"><span class="comment"># special &lt;sos&gt; token</span></div><div class="line">start_token = tf.get_variable(<span class="string">"start_token"</span>, dtype=tf.float32, shape=[<span class="number">80</span>])</div><div class="line">tok_embeddings = tf.nn.embedding_lookup(E, formula)</div><div class="line"></div><div class="line"><span class="comment"># 2. add the special &lt;sos&gt; token embedding at the beggining of every formula</span></div><div class="line">start_token_ = tf.reshape(start_token, [<span class="number">1</span>, <span class="number">1</span>, dim])</div><div class="line">start_tokens = tf.tile(start_token_, multiples=[batch_size, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line"><span class="comment"># remove the &lt;eos&gt; that won't be used because we reached the end</span></div><div class="line">tok_embeddings = tf.concat([start_tokens, tok_embeddings[:, :<span class="number">-1</span>, :]], axis=<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 3. decode</span></div><div class="line">attn_cell = AttentionCell()</div><div class="line">seq_logits, _ = tf.nn.dynamic_rnn(attn_cell, tok_embeddings, initial_state=AttentionState(h_0, o_0))</div></pre></td></tr></table></figure><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>Code speaks for itself</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute - log(p_i[y_i]) for each time step, shape = (batch_size, formula length)</span></div><div class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=seq_logits, labels=formula)</div><div class="line"><span class="comment"># masking the losses</span></div><div class="line">mask = tf.sequence_mask(formula_length)</div><div class="line">losses = tf.boolean_mask(losses, mask)</div><div class="line"><span class="comment"># averaging the loss over the batch</span></div><div class="line">loss = tf.reduce_mean(losses)</div><div class="line"><span class="comment"># building the train op</span></div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate)</div><div class="line">train_op = optimizer.minimize(loss)</div></pre></td></tr></table></figure><p>and when iterating over the batches during training, <code>train_op</code> will be given to the <code>tf.Session</code> along with a <code>feed_dict</code> containing the data for the placeholders.</p><h2 id="Decoding-in-Tensorflow"><a href="#Decoding-in-Tensorflow" class="headerlink" title="Decoding in Tensorflow"></a>Decoding in Tensorflow</h2><blockquote><p>Let’s have a look at the Tensorflow implementation of the Greedy Method before dealing with Beam Search</p></blockquote><h3 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h3><p>While greedy decoding is easy to conceptualize, implementing it in Tensorflow is not straightforward, as you need to use the previous prediction and can’t use <code>dynamic_rnn</code> on the <code>formula</code>. There are basically <strong>2 ways of approaching the problem</strong></p><ol><li><p>Modify our <code>AttentionCell</code> and <code>AttentionState</code> so that <code>AttentionState</code> also contains the embedding of the predicted word at the previous time step,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">AttentionState = namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>, <span class="string">"embedding"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        lstm_state, o, embbeding = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([embedding, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># usual logic</span></div><div class="line">        logits = ...</div><div class="line">        <span class="comment"># compute new embeddding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        new_state = AttentionState(new_lstm_state, new_o, new_embedding)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><blockquote><p>This technique has a few downsides. It <strong>doesn’t use inputs</strong> (which used to be the embedding of the gold token from the <code>formula</code> and thus we would have to call <code>dynamic_rnn</code> on a “fake” sequence). Also, how do you know when to stop decoding, once you’ve reached the <code>&lt;eos&gt;</code> token?</p></blockquote></li><li><p>Implement a variant of <code>dynamic_rnn</code> that would not run on a sequence but feed the prediction from the previous time step to the cell, while having a maximum number of decoding steps. This would involve delving deeper into Tensorflow, using <code>tf.while_loop</code>. That’s the method we’re going to use as it fixes all the problems of the first technique. We eventually want something that looks like</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">attn_cell = AttentionCell(...)</div><div class="line"><span class="comment"># wrap the attention cell for decoding</span></div><div class="line">decoder_cell = GreedyDecoderCell(attn_cell)</div><div class="line"><span class="comment"># call a special dynamic_decode primitive</span></div><div class="line">test_outputs, _ = dynamic_decode(decoder_cell, max_length_formula+<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Much better isn’t it? Now let’s see what <code>GreedyDecoderCell</code> and <code>dynamic_decode</code> look like.</p></blockquote></li></ol><h3 id="Greedy-Decoder-Cell"><a href="#Greedy-Decoder-Cell" class="headerlink" title="Greedy Decoder Cell"></a>Greedy Decoder Cell</h3><p>We first wrap the attention cell in a <code>GreedyDecoderCell</code> that takes care of the greedy logic for us, without having to modify the <code>AttentionCell</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderOutput</span><span class="params">(collections.namedtuple<span class="params">(<span class="string">"DecoderOutput"</span>, <span class="params">(<span class="string">"logits"</span>, <span class="string">"ids"</span>)</span>)</span>)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GreedyDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># next step of attention cell</span></div><div class="line">        logits, new_state = self._attention_cell.step(embedding, state)</div><div class="line">        <span class="comment"># get ids of words predicted and get embedding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        <span class="comment"># create new state of decoder</span></div><div class="line">        new_output = DecoderOutput(logits, new_ids)</div><div class="line">        new_finished = tf.logical_or(finished, tf.equal(new_ids,</div><div class="line">                self._end_token))</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (new_output, new_state, new_embedding, new_finished)</div></pre></td></tr></table></figure><h3 id="Dynamic-Decode-primitive"><a href="#Dynamic-Decode-primitive" class="headerlink" title="Dynamic Decode primitive"></a>Dynamic Decode primitive</h3><p>We need to implement a function <code>dynamic_decode</code> that will recursively call the above <code>step</code> function. We do this with a <code>tf.while_loop</code> that stops when all the hypotheses reached <code>&lt;eos&gt;</code> or <code>time</code> is greater than the max number of iterations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamic_decode</span><span class="params">(decoder_cell, maximum_iterations)</span>:</span></div><div class="line">    <span class="comment"># initialize variables (details on github)</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">condition</span><span class="params">(time, unused_outputs_ta, unused_state, unused_inputs, finished)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.logical_not(tf.reduce_all(finished))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(time, outputs_ta, state, inputs, finished)</span>:</span></div><div class="line">        new_output, new_state, new_inputs, new_finished = decoder_cell.step(</div><div class="line">            time, state, inputs, finished)</div><div class="line">        <span class="comment"># store the outputs in TensorArrays (details on github)</span></div><div class="line">        new_finished = tf.logical_or(tf.greater_equal(time, maximum_iterations), new_finished)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (time + <span class="number">1</span>, outputs_ta, new_state, new_inputs, new_finished)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"rnn"</span>):</div><div class="line">        res = tf.while_loop(</div><div class="line">            condition,</div><div class="line">            body,</div><div class="line">            loop_vars=[initial_time, initial_outputs_ta, initial_state, initial_inputs, initial_finished])</div><div class="line"></div><div class="line">    <span class="comment"># return the final outputs (details on github)</span></div></pre></td></tr></table></figure><blockquote><p>Some details using <code>TensorArrays</code> or <code>nest.map_structure</code> have been omitted for clarity but may be found on <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/dynamic_decode.py" target="_blank" rel="external">github</a></p><p>Notice that we place the <code>tf.while_loop</code> inside a scope named <code>rnn</code>. This is because <code>dynamic_rnn</code>does the same thing and thus the weights of our LSTM are defined in that scope.</p></blockquote><h3 id="Beam-Search-Decoder-Cell"><a href="#Beam-Search-Decoder-Cell" class="headerlink" title="Beam Search Decoder Cell"></a>Beam Search Decoder Cell</h3><blockquote><p>We can follow the same approach as in the greedy method and use <code>dynamic_decode</code></p></blockquote><p>Let’s create a new wrapper for <code>AttentionCell</code> in the same way we did for <code>GreedyDecoderCell</code>. This time, the code is going to be more complicated and the following is just for intuition. Note that when selecting the top kk hypotheses from the set of candidates, we must know which “beginning” they used (=parent hypothesis).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BeamSearchDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># notice the same arguments as for GreedyDecoderCell</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># compute new logits</span></div><div class="line">        logits, new_cell_state = self._attention_cell.step(embedding, state.cell_state)</div><div class="line"></div><div class="line">        <span class="comment"># compute log probs of the step (- log p(w) for all words w)</span></div><div class="line">        <span class="comment"># shape = [batch_size, beam_size, vocab_size]</span></div><div class="line">        step_log_probs = tf.nn.log_softmax(new_logits)</div><div class="line"></div><div class="line">        <span class="comment"># compute scores for the (beam_size * vocabulary_size) new hypotheses</span></div><div class="line">        log_probs = state.log_probs + step_log_probs</div><div class="line"></div><div class="line">        <span class="comment"># get top k hypotheses</span></div><div class="line">        new_probs, indices = tf.nn.top_k(log_probs, self._beam_size)</div><div class="line"></div><div class="line">        <span class="comment"># get ids of next token along with the parent hypothesis</span></div><div class="line">        new_ids = ...</div><div class="line">        new_parents = ...</div><div class="line"></div><div class="line">        <span class="comment"># compute new embeddings, new_finished, new_cell state...</span></div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div></pre></td></tr></table></figure><blockquote><p>Look at <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/beam_search_decoder_cell.py" target="_blank" rel="external">github</a> for the details. The main idea is that we add a beam dimension to every tensor, but when feeding it into <code>AttentionCell</code> we merge the beam dimension with the batch dimension. There is also some trickery involved to compute the parents and the new ids using modulos.</p></blockquote><h2 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I hope that you learned something with this post, either about the technique or Tensorflow. While the model achieves impressive performance (at least on short formulas with roughly 85% of the LaTeX being reconstructed), it still raises some questions that I list here:</p><p><em>How do we evaluate the performance of our model?</em>. We can use standard metrics from Machine Translation like <a href="https://en.wikipedia.org/wiki/BLEU" target="_blank" rel="external">BLEU</a> to evaluate how good the decoded LaTeX is compared to the reference. We can also choose to compile the predicted LaTeX sequence to get the image of the formula, and then compare this image to the orignal. As a formula is a sequence, computing the pixel-wise distance wouldn’t really make sense. A good idea is proposed by <a href="http://lstm.seas.harvard.edu/latex" target="_blank" rel="external">Harvard’s paper</a>. First, slice the image vertically. Then, compare the edit distance between these slices…</p><p><em>How to fix exposure bias?</em> While beam search generally achieves better results, it is not perfect and still suffers from <strong>exposure bias</strong>. During training, the model is never exposed to its errors! It also suffers from <strong>Loss-Evaluation Mismatch</strong>. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/ref.png" alt=""></p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/pred.png" alt=""></p><p><em>An Example of LaTeX generation - which one is the reference?</em></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/18/Understanding-Convolutions/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/18/Understanding-Convolutions/" itemprop="url">Understanding Convolutions</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-18T17:01:29+08:00">2018-01-18 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/18/Understanding-Convolutions/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/18/Understanding-Convolutions/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/18/Understanding-Convolutions/" class="leancloud_visitors" data-flag-title="Understanding Convolutions"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Lessons-from-a-Dropped-Ball"><a href="#Lessons-from-a-Dropped-Ball" class="headerlink" title="Lessons from a Dropped Ball"></a>Lessons from a Dropped Ball</h2><p>Imagine we drop a ball from some height onto the ground, where it only has one dimension of motion. <em>How likely is it that a ball will go a distance $c$ if you drop it and then drop it again from above the point at which it landed?</em></p><p>Let’s break this down. After the first drop, it will land aa units away from the starting point with probability $f(a)$, where $f$ is the probability distribution.</p><p>Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed. The probability of the ball rolling $b$ units away from the new starting point is $g(b)$, where $g$ may be a different probability distribution if it’s dropped from a different height.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-fagb.png" alt="img"></p><p>If we fix the result of the first drop so we know the ball went distance aa, for the ball to go a total distance cc, the distance traveled in the second drop is also fixed at bb, where $a+b=c$. So the probability of this happening is simply $f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn1" target="_blank" rel="external">1</a></p><p>Let’s think about this with a specific discrete example. We want the total distance $c$ to be 3. If the first time it rolls, $a=2$, the second time it must roll $b=1$ in order to reach our total distance $a+b=3$. The probability of this is $f(2)⋅g(1)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-split-21.png" alt="img"></p><p>However, this isn’t the only way we could get to a total distance of 3. The ball could roll 1 units the first time, and 2 the second. Or 0 units the first time and all 3 the second. It could go any $a$ and $b$, as long as they add to 3.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-splits-12-03.png" alt="img"></p><p>The probabilities are $f(1)⋅g(2)$ and $f(0)⋅g(3)$, respectively.</p><p>In order to find the <em>total likelihood</em> of the ball reaching a total distance of $c$, we can’t consider only one possible way of reaching $c$. Instead, we consider <em>all the possible ways</em> of partitioning $c$ into two drops $a$ and $b$ and sum over the <em>probability of each way</em>.<br>$$<br>…\;\; f(0)\cdot g(3) ~+~ f(1)\cdot g(2) ~+~ f(2)\cdot g(1)\;\;…<br>$$<br>We already know that the probability for each case of $a+b=c$ is simply $f(a)⋅g(b)$. So, summing over every solution to $a+b=c$, we can denote the total likelihood as:<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Turns out, we’re doing a convolution! In particular, the convolution of $f$ and $g$, evluated at $c$ is defined:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)~~~~<br>$$<br>If we substitute $b=c−a$, we get:<br>$$<br>(f\ast g)(c) = \sum_a f(a) \cdot g(c-a)<br>$$<br>This is the standard definition<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn2" target="_blank" rel="external">2</a> of convolution.</p><p>To make this a bit more concrete, we can think about this in terms of positions the ball might land. After the first drop, it will land at an intermediate position aa with probability $f(a)$. If it lands at $a$, it has probability $g(c−a)$ of landing at a position $c$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-OnePath.png" alt="img"></p><p>To get the convolution, we consider all intermediate positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-SumPaths.png" alt="img"></p><h2 id="Visualizing-Convolutions"><a href="#Visualizing-Convolutions" class="headerlink" title="Visualizing Convolutions"></a>Visualizing Convolutions</h2><p>There’s a very nice trick that helps one think about convolutions more easily.</p><p>First, an observation. Suppose the probability that a ball lands a certain distance $x$ from where it started is $f(x)$. Then, afterwards, the probability that it started a distance $x$ from where it landed is $f(−x)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Reverse.png" alt="img"></p><p>If we know the ball lands at a position $c$ after the second drop, what is the probability that the previous position was $a$?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-BackProb.png" alt="img"></p><p>So the probability that the previous position was $a$ is $g(−(a−c))=g(c−a)$.</p><p>Now, consider the probability each intermediate position contributes to the ball finally landing at $c$. We know the probability of the first drop putting the ball into the intermediate position a is $f(a)$. We also know that the probability of it having been in $a$, if it lands at $c$ is $g(c−a)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate.png" alt="img"></p><p>Summing over the $a$s, we get the convolution.</p><p>The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value $c$ in a single picture. By shifting the bottom half around, we can evaluate the convolution at other values of $c$. This allows us to understand the convolution as a whole.</p><p>For example, we can see that it peaks when the distributions align.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Align.png" alt="img"></p><p>And shrinks as the intersection between the distributions gets smaller.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Sep.png" alt="img"></p><p>By using this trick in an animation, it really becomes possible to visually understand convolutions.</p><p>Below, we’re able to visualize the convolution of two box functions:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Wiki-BoxConvAnim.gif" alt="Wiki-BoxConvAnim"></p><p><em>From Wikipedia</em></p><p>Armed with this perspective, a lot of things become more intuitive.</p><p>Let’s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy.</p><h2 id="Higher-Dimensional-Convolutions"><a href="#Higher-Dimensional-Convolutions" class="headerlink" title="Higher Dimensional Convolutions"></a>Higher Dimensional Convolutions</h2><p>Convolutions are an extremely general idea. We can also use them in a higher number of dimensions.</p><p>Let’s consider our example of a falling ball again. Now, as it falls, it’s position shifts not only in one dimension, but in two.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-TwoDim.png" alt="img"></p><p>Convolution is the same as before:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Except, now $a$, $b$ and $c$ are vectors. To be more explicit,<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{\begin{array}{c}a_1+b_1=c_1\\a_2+b_2=c_2\end{array}} f(a_1,a_2) \cdot g(b_1,b_2)<br>$$<br>Or in the standard definition:<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{a_1, a_2} f(a_1, a_2) \cdot g(c_1-a_1,~ c_2-a_2)<br>$$<br>Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding.</p><p>One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a “kernel.”</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/RiverTrain-ImageConvDiagram.png" alt="RiverTrain-ImageConvDiagram"></p><p><em>From the <a href="http://intellabs.github.io/RiverTrail/tutorial/" target="_blank" rel="external">River Trail documentation</a></em></p><p>The kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over.</p><p>For example, by averaging a 3x3 box of pixels, we can blur an image. To do this, our kernel takes the value $\dfrac{1}{9}$ on each pixel in the box,</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Blur.png" alt="Gimp-Blur"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>We can also detect edges by taking the values −1−1 and 11 on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Edge.png" alt="Gimp-Edge"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>The gimp documentation has <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">many other examples</a>.</p><h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><p>So, how does convolution relate to convolutional neural networks?</p><p>Consider a 1-dimensional convolutional layer with inputs $\{x_n\}$ and outputs $\{y_n\}$, like we discussed in the <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" target="_blank" rel="external">previous post</a>:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY.png" alt="img"></p><p>As we observed, we can describe the outputs in terms of the inputs:<br>$$<br>y_n = A(x_{n}, x_{n+1}, …)<br>$$<br>Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.</p><p>Recall that a typical neuron in a neural network is described by:<br>$$<br>\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~…~ + b)<br>$$<br>Where $x_0$, $x_1$… are the inputs. The weights, $w_0$, $w_1$, … describe how the neuron connects to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn3" target="_blank" rel="external">3</a> Saying that multiple neurons are identical is the same thing as saying that the weights are the same.</p><p>It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.</p><p>Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:<br>$$<br>y = \sigma(Wx + b)<br>$$<br>For example, we get:<br>$$<br>y_0 = \sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 …)<br>$$</p><p>$$<br>y_1 = \sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 …)<br>$$</p><p>Each row of the matrix describes the weights connecting a neuron to its inputs.</p><p>Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY-W.png" alt="img"></p><p>Which corresponds to the equations:<br>$$<br>y_0 = \sigma(W_0x_0 + W_1x_1 -b)<br>$$</p><p>$$<br>y_1 = \sigma(W_0x_1 + W_1x_2 -b)<br>$$</p><p>So while, normally, a weight matrix connects every input to every neuron with different weights:<br>$$<br>W = \left[\begin{array}{ccccc}<br>W_{0,0} &amp; W_{0,1} &amp; W_{0,2} &amp; W_{0,3} &amp; …\\<br>W_{1,0} &amp; W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; …\\<br>W_{2,0} &amp; W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; …\\<br>W_{3,0} &amp; W_{3,1} &amp; W_{3,2} &amp; W_{3,3} &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don’t connect to many possible inputs, there’s lots of zeros.<br>$$<br>W = \left[\begin{array}{ccccc}<br>w_0 &amp; w_1 &amp; 0 &amp; 0 &amp; …\\<br>0 &amp; w_0 &amp; w_1 &amp; 0 &amp; …\\<br>0 &amp; 0 &amp; w_0 &amp; w_1 &amp; …\\<br>0 &amp; 0 &amp; 0 &amp; w_0 &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>Multiplying by the above matrix is the same thing as convolving with $[…0, w_1, w_0, 0…]$. The function sliding to different positions corresponds to having neurons at those positions.</p><p>What about two-dimensional convolutional layers?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv2-5x5-Conv2-XY.png" alt="img"></p><p>The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.</p><p>Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image.</p><ol><li><p>We want the probability of the ball rolling aa units the first time and also rolling bb units the second time. The distributions $P(A)=f(a)$ and $P(b)=g(b)$ are independent, with both distributions centered at 0. So $P(a,b)=P(a)∗P(b)=f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref1" target="_blank" rel="external">↩</a></p></li><li><p>The non-standard definition, which I haven’t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious.</p><p>For example, convolution is a commutative operation. That is, $f∗g=g∗f$. Why?</p><p>​<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b) ~~=~ \sum_{b+a=c} g(b) \cdot f(a)<br>$$<br>Convolution is also associative. That is, $(f∗g)∗h=f∗(g∗h)$. Why?</p><p>​<br>$$<br>\sum_{(a+b)+c=d} (f(a) \cdot g(b)) \cdot h(c) ~~=~ \sum_{a+(b+c)=d} f(a) \cdot (g(b) \cdot h(c))<br>$$<br>↩</p><p>​</p></li><li><p>There’s also the bias, which is the “threshold” for whether the neuron fires, but it’s much simpler and I don’t want to clutter this section talking about it.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref3" target="_blank" rel="external">↩</a></p></li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/17/PCA-With-Tensorflow/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/17/PCA-With-Tensorflow/" itemprop="url">PCA With Tensorflow</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-17T17:04:36+08:00">2018-01-17 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/17/PCA-With-Tensorflow/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/17/PCA-With-Tensorflow/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/17/PCA-With-Tensorflow/" class="leancloud_visitors" data-flag-title="PCA With Tensorflow"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>PCA (<strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis) is probably the oldest trick in the book.</p><p>PCA is well studied and there are numerous ways to get to the same solution, we will talk about two of them here, Eigen decomposition and Singular Value Decomposition (SVD) and then we will implement the SVD way in TensorFlow.</p><p>From now on, X will be our data matrix, of shape (n, p) where n is the number of examples, and p are the dimensions.</p><p>So given X, both methods will try to find, in their own way, a way to manipulate and decompose X in a manner that later on we could multiply the decomposed results to represent maximum information in less dimensions. I know I know, sounds horrible but I will spare you most of the math but keep the parts that contribute to the understanding of the method pros and cons.</p><p>So Eigen decomposition and SVD are both ways to decompose matrices, lets see how they help us in PCA and how they are connected.</p><p>Take a glance at the flow chart below and I will explain right after.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*xnomew0zpnxftxutG8xoFw.png" alt="img"></p><p>Figure 1 PCA workflow</p><p>So why should you care about this? Well there is something very fundamental about the two procedures that tells us a lot about PCA.</p><p>As you can see both methods are pure linear algebra, that basically tells us that using PCA is looking at the real data, from a different angle — this is unique to PCA since the other methods start with random representation of lower dimensional data and try to get it to behave like the high dimensional data.</p><p>Some other notable things are that all operations are linear and with SVD are super-super fast.</p><p>Also given the same data PCA will always give the same answer (which is not true about the other two methods).</p><p>Notice how in SVD we choose the r (r is the number of dimensions we want to reduce to) left most values of Σ to lower dimensionality?</p><p>Well there is something special about Σ .</p><p>Σ is a diagonal matrix, there are p (number of dimensions) diagonal values (called singular values) and their magnitude indicates how significant they are to preserving the information.</p><p>So we can choose to reduce dimensionality, to the number of dimensions that will preserve approx. given amount of percentage of the data and I will demonstrate that in the code (e.g. gives us the ability to reduce dimensionality with a constraint of losing a max of 15% of the data).</p><p>As you will see, coding this in TensorFlow is pretty simple — what we are are going to code is a class that has <code>fit</code> method and a <code>reduce</code> method which we will supply the dimensions to.</p><h3 id="CODE-PCA"><a href="#CODE-PCA" class="headerlink" title="CODE (PCA)"></a><strong>CODE (PCA)</strong></h3><p>Lets see how the <code>fit</code> method looks like, given <code>self.X</code> contains the data and <code>self.dtype=tf.float32</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></div><div class="line">    self.graph = tf.Graph()</div><div class="line">    <span class="keyword">with</span> self.graph.as_default():</div><div class="line">        self.X = tf.placeholder(self.dtype, shape=self.data.shape)</div><div class="line"></div><div class="line">        <span class="comment"># Perform SVD</span></div><div class="line">        singular_values, u, _ = tf.svd(self.X)</div><div class="line"></div><div class="line">        <span class="comment"># Create sigma matrix</span></div><div class="line">        sigma = tf.diag(singular_values)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session(graph=self.graph) <span class="keyword">as</span> session:</div><div class="line">        self.u, self.singular_values, self.sigma = session.run([u, singular_values, sigma],</div><div class="line">                                                               feed_dict=&#123;self.X: self.data&#125;)</div></pre></td></tr></table></figure><p>So the goal of <code>fit</code> is to create our Σ and U for later use.</p><p>We’ll start with the line <code>tf.svd</code> which gives us the singular values, which are the diagonal values of what was denoted as Σ in Figure 1, and the matrices U and V.</p><p>Then <code>tf.diag</code> is TensorFlow’s way of converting a 1D vector, to a diagonal matrix, which in our case will result in Σ.</p><p>At the end of the <code>fit</code> call we will have the singular values, Σ and U.</p><p>Now lets lets implement <code>reduce</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span><span class="params">(self, n_dimensions=None, keep_info=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> keep_info:</div><div class="line">        <span class="comment"># Normalize singular values</span></div><div class="line">        normalized_singular_values = self.singular_values / sum(self.singular_values)</div><div class="line"></div><div class="line">        <span class="comment"># Create the aggregated ladder of kept information per dimension</span></div><div class="line">        ladder = np.cumsum(normalized_singular_values)</div><div class="line"></div><div class="line">        <span class="comment"># Get the first index which is above the given information threshold</span></div><div class="line">        index = next(idx <span class="keyword">for</span> idx, value <span class="keyword">in</span> enumerate(ladder) <span class="keyword">if</span> value &gt;= keep_info) + <span class="number">1</span></div><div class="line">        n_dimensions = index</div><div class="line"></div><div class="line">    <span class="keyword">with</span> self.graph.as_default():</div><div class="line">        <span class="comment"># Cut out the relevant part from sigma</span></div><div class="line">        sigma = tf.slice(self.sigma, [<span class="number">0</span>, <span class="number">0</span>], [self.data.shape[<span class="number">1</span>], n_dimensions])</div><div class="line"></div><div class="line">        <span class="comment"># PCA</span></div><div class="line">        pca = tf.matmul(self.u, sigma)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session(graph=self.graph) <span class="keyword">as</span> session:</div><div class="line">        <span class="keyword">return</span> session.run(pca, feed_dict=&#123;self.X: self.data&#125;)</div></pre></td></tr></table></figure><p>So as you can see <code>reduce</code> gets either <code>keep_info</code> or <code>n_dimensions</code> (I didn’t implement the input check where <strong>only one must be supplied</strong>).</p><p>If we supply <code>n_dimensions</code> it will simply reduce to that number, but if we supply <code>keep_info</code> which should be a float between 0 and 1, we will preserve that much information from the original data (0.9 — preserve 90% of the data).</p><p>In the first ‘if’, we normalize and check how many singular values are needed, basically figuring out <code>n_dimensions</code> out of <code>keep_info</code>.</p><p>In the graph, we just slice the Σ (sigma) matrix for as much data as we need and perform the matrix multiplication.</p><p>So lets try it out on the iris dataset, which is (150, 4) dataset of 3 species of iris flowers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"></div><div class="line">tf_pca = TF_PCA(iris_dataset.data, iris_dataset.target)</div><div class="line">tf_pca.fit()</div><div class="line">pca = tf_pca.reduce(keep_info=<span class="number">0.9</span>)  <span class="comment"># Results in 2 dimensions</span></div><div class="line"></div><div class="line">color_mapping = &#123;<span class="number">0</span>: sns.xkcd_rgb[<span class="string">'bright purple'</span>], <span class="number">1</span>: sns.xkcd_rgb[<span class="string">'lime'</span>], <span class="number">2</span>: sns.xkcd_rgb[<span class="string">'ochre'</span>]&#125;</div><div class="line">colors = list(map(<span class="keyword">lambda</span> x: color_mapping[x], tf_pca.target))</div><div class="line"></div><div class="line">plt.scatter(pca[:, <span class="number">0</span>], pca[:, <span class="number">1</span>], c=colors)</div></pre></td></tr></table></figure><p><img src="https://cdn-images-1.medium.com/max/2000/1*-am5UfbZoJkUA4C8z5d0vQ.png" alt="img"></p><p>Figure 2 Iris dataset PCA 2 dimensional plot</p><p>Not so bad huh?</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/16/Word-Embedding-Approximating-the-Softmax-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/16/Word-Embedding-Approximating-the-Softmax-Repost/" itemprop="url">Word Embedding - Approximating the Softmax [Repost]</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-16T19:09:29+08:00">2018-01-16 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/16/Word-Embedding-Approximating-the-Softmax-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/16/Word-Embedding-Approximating-the-Softmax-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/16/Word-Embedding-Approximating-the-Softmax-Repost/" class="leancloud_visitors" data-flag-title="Word Embedding - Approximating the Softmax [Repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>This is the second post in a series on word embeddings and representation learning. In the <a href="http://sebastianruder.com/word-embeddings-1/index.html" target="_blank" rel="external">previous post</a>, we gave an overview of word embedding models and introduced the classic neural language model by Bengio et al. (2003), the C&amp;W model by Collobert and Weston (2008), and the word2vec model by Mikolov et al. (2013). We observed that mitigating the complexity of computing the final softmax layer has been one of the main challenges in devising better word embedding models, a commonality with machine translation (MT) (Jean et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:10" target="_blank" rel="external">10</a>]) and language modelling (Jozefowicz et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:6" target="_blank" rel="external">6</a>]).</p><p>In this post, we will thus focus on giving an overview of various approximations to the softmax layer that have been proposed over the last years, some of which have so far only been employed in the context of language modelling or MT. We will postpone the discussion of additional hyperparameters to the subsequent post.</p><p>Let us know partially re-introduce the previous post’s notation both for consistency and to facilitate comparison as well as introduce some new notation: We assume a training corpus containing a sequence of $T$ training words $w_1,w_2,w_3, \cdots ,w_T$ that belong to a vocabulary $V$ whose size is $|V|$. Our models generally consider a context $c$ of $n$ words. We associate every word with an input embedding $v_w$ (the eponymous word embedding in the Embedding Layer) with $d$ dimensions and an output embedding $v_{w^{\prime}}$ (the representation of the word in the weight matrix of the softmax layer). We finally optimize an objective function $J_{\theta}$ with regard to our model parameters $\theta$.</p><p>Recall that the softmax calculates the probability of a word $w$ given its context $c$ and can be computed using the following equation:<br>$$<br>p(w|c) = \frac{\exp(h^{\text{T}} v_{w^{\prime}})}{\sum_{w_i \in V} \exp(h^{\text{T}}v_{w_i}^{\prime})}<br>$$<br>where $h$ is the output vector of the penultimate network layer. Note that we use $c$ for the context as mentioned above and drop the index $t$ of the target word $w_t$ for simplicity. Computing the softmax is expensive as the inner product between $h$ and the output embedding of every word $w_i$ in the vocabulary $V$ needs to be computed as part of the sum in the denominator in order to obtain the normalized probability of the target word $w$ given its context $c$.</p><p>In the following we will discuss different strategies that have been proposed to approximate the softmax. These approaches can be grouped into softmax-based and sampling-based approaches. Softmax-based approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency. Sampling-based approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax.</p><h1 id="Softmax-based-Approaches"><a href="#Softmax-based-Approaches" class="headerlink" title="Softmax-based Approaches"></a>Softmax-based Approaches</h1><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><p>Hierarchical softmax (H-Softmax) is an approximation inspired by binary trees that was proposed by Morin and Bengio [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:3" target="_blank" rel="external">3</a>]. H-Softmax essentially replaces the flat softmax layer with a hierarchical layer that has the words as leaves, as can be seen in Figure 1.</p><p>This allows us to decompose calculating the probability of one word into a sequence of probability calculations, which saves us from having to calculate the expensive normalization over all words. Replacing a softmax layer with H-Softmax can yield speedups for word prediction tasks of at least $50 \times $ and is thus critical for low-latency tasks such as real-time communication in <a href="http://googleresearch.blogspot.ie/2016/05/chat-smarter-with-allo.html" target="_blank" rel="external">Google’s new messenger app Allo</a>.</p><p><img src="http://ruder.io/content/images/2016/06/hierarchical_softmax_example.png" alt="Hierarchical softmax"></p><p>Figure 1: Hierarchical softmax (<a href="https://www.quora.com/Word2vec-How-can-hierarchical-soft-max-training-method-of-CBOW-guarantee-its-self-consistence" target="_blank" rel="external">Quora</a>)</p><p>We can think of the regular softmax as a tree of depth 11, with each word in $V$ as a leaf node. Computing the softmax probability of one word then requires normalizing over the probabilities of all $|V|$ leaves. If we instead structure the softmax as a binary tree, with the words as leaf nodes, then we only need to follow the path to the leaf node of that word, without having to consider any of the other nodes.</p><p>Since a balanced binary tree has a depth of $\log_2(|V|)$ we only need to evaluate at most $\log_2(|V|)$ nodes to obtain the final probability of a word. Note that this probability is already normalized, as the probabilities of all leaves in a binary tree sum to 11 and thus form a probability distribution. To informally verify this, we can reason that at a tree’s root node (Node 0) in Figure 1), the probabilities of branching decisions must sum to 11. At each subsequent node, the probability mass is then split among its children, until it eventually ends up at the leaf nodes, i.e. the words. Since no probability is lost along the way and since all words are leaves, the probabilities of all words must necessarily sum to 11 and hence the hierarchical softmax defines a normalized probability distribution over all words in $V$.</p><p>To get a bit more concrete, as we go through the tree, we have to be able to calculate the probability of taking the left or right branch at every junction. For this reason, we assign a representation to every node. In contrast to the regular softmax, we thus no longer have output embeddings $v^{\prime}_w$ for every word $w$ – instead, we have embeddings $v^{\prime}_n$ for every node $n$. As we have $|V|−1$ nodes and each one possesses a unique representation, the number of parameters of H-Softmax is almost the same as for the regular softmax. We can now calculate the probability of going right (or left) at a given node $n$ given the context $c$ the following way:<br>$$<br>p(\text{right}|n,c) = \sigma(h^{\text{T}}v^{\prime}_n).<br>$$<br>This is almost the same as the computations in the regular softmax; now instead of computing the dot product between $h$ and the output word embedding $v^{\prime}_w$, we compute the dot product between $h$ and the embedding $v^{\prime}_w$ of each node in the tree; additionally, instead of computing a probability distribution over the entire vocabulary words, we output just one probability, the probability of going right at node $n$ in this case, with the sigmoid function. Conversely, the probability of turning left is simply $1−p(\text{right} | n,c)$.</p><p><img src="http://ruder.io/content/images/2016/05/hierarchical_softmax.png" alt="Hierarchical softmax"></p><p>Figure 2: Hierarchical softmax computations (<a href="https://www.youtube.com/watch?v=B95LTf2rVWM" target="_blank" rel="external">Hugo Lachorelle’s Youtube lectures</a>)</p><p>The probability of a word ww given its context cc is then simply the product of the probabilities of taking right and left turns respectively that lead to its leaf node. To illustrate this, given the context “the”, “dog”, “and”, “the”, the probability of the word “cat” in Figure 2 can be computed as the product of the probability of turning left at node 1, turning right at node 2, and turning right at node 5. Hugo Lachorelle gives a more detailed account in his excellent <a href="https://www.youtube.com/watch?v=B95LTf2rVWM" target="_blank" rel="external">lecture video</a>. Rong [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:7" target="_blank" rel="external">7</a>] also does a good job of explaining these concepts and also derives the derivatives of H-Softmax.</p><p>Obviously, the structure of the tree is of significance. Intuitively, we should be able to achieve better performance, if we make it easier for the model to learn the binary predictors at every node, e.g. by enabling it to assign similar probabilities to similar paths. Based on this idea, Morin and Bengio use the synsets in WordNet as clusters for the tree. However, they still report inferior performance to the regular softmax. Mnih and Hinton [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:8" target="_blank" rel="external">8</a>] learn the tree structure with a clustering algorithm that recursively partitions the words in two clusters and allows them to achieve the same performance as the regular softmax at a fraction of the computation.</p><p>Notably, we are only able to obtain this speed-up during training, when we know the word we want to predict (and consequently its path) in advance. During testing, when we need to find the most likely prediction, we still need to calculate the probability of all words, although narrowing down the choices in advance helps here.</p><p>In practice, instead of using “right” and “left” in order to designate nodes, we can index every node with a bit vector that corresponds to the path it takes to reach that node. In Figure 2, if we assume a <code>0</code> bit for turning left and a <code>1</code> bit for turning right, we can thus represent the path to “cat” as <code>011</code>.</p><p>Recall that the path length in a balanced binary tree is $\log_2|V|$. If we set $|V|=10000$, this amounts to an average path length of about $13.3$. Analogously, we can represent every word by the bit vector of its path that is on average $13.3$ bits long. In information theory, this is referred to as an information content of $13.3$ bits per word.</p><h3 id="A-note-on-the-information-content-of-words"><a href="#A-note-on-the-information-content-of-words" class="headerlink" title="A note on the information content of words"></a>A note on the information content of words</h3><p>Recall that the information content $I(w)$ of a word $w$ is the negative logarithm of its probability $p(w)$:<br>$$<br>I(w) = − \log_2p(w)<br>$$<br>The entropy $H$ of all words in a corpus is then the expectation of the information content of all words in the vocabulary:<br>$$<br>H= \sum_{i \in V} p(w_i) I(w_i)<br>$$<br>We can also conceive of the entropy of a data source as the average number of bits needed to encode it. For a fair coin flip, we need $1$ bit per flip, whereas we need $0$ bits for a data source that always emits the same symbol. For a balanced binary tree, where we treat every word equally, the word entropy $H$ equals the information content $I(w)$ of every word $w$, as each word has the same probability. The average word entropy $H$ in a balanced binary tree with $|V|=10000$ thus coincides with its average path length:<br>$$<br>H = − \sum_{i \in V}\frac{1}{10000} \log_2 \frac{⁡1}{10000} = 13.3.<br>$$<br>We saw before that the structure of the tree is important. Notably, we can leverage the tree structure not only to gain better performance, but also to speed up computation: If we manage to encode more information into the tree, we can get away with taking shorter paths for less informative words. Morin and Bengio point out that leveraging word probabilities should work even better; as some words are more likely to occur than others, they can be encoded using less information. They note that the word entropy of their corpus (with $|V|=10,000$) is about $9.16$.</p><p>Thus, by taking into account frequencies, we can reduce the average number of bits per word in the corpus from $13.3$ to $9.16$ in this case, which amounts to a speed-up of 31%. A <a href="https://en.wikipedia.org/wiki/Huffman_coding" target="_blank" rel="external">Huffman tree</a>, which is used by Mikolov et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:1" target="_blank" rel="external">1</a>] for their hierarchical softmax, generates such a coding by assigning fewer bits to more common symbols. For instance, “the”, the most common word in the English language, would be assigned the shortest bit code in the tree, the second most frequent word would be assigned the second-shortest bit code, and so on. While we still need the same number of codes to designate all words, when we predict the words in a corpus, short codes appear now a lot more often, and we consequently need fewer bits to represent each word on average.</p><p>A coding such as Huffman coding is also known as entropy encoding, as the length of each codeword is approximately proportional to the entropy of each symbol as we have observed. Shannon [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:5" target="_blank" rel="external">5</a>] establishes in his experiments that the lower bound on the information rate in English is between $0.6$ to $1.3$ bits per character; given an average word length of $4.5$, this amounts to $2.7$ - $5.85$ bits per word.</p><p>To tie this back to language modelling (which we already talked about in the previous post): perplexity, the evaluation measure of language modelling, is $2^H$ where $H$ is the entropy. A unigram entropy of $9.16$ thus entails a still very high perplexity of $2^{9.16}=572.0$. We can render this value more tangible by observing that a model with a perplexity of $572$ is as confused by the data as if it had to choose among $572$ possibilities for each word uniformly and independently.</p><p>To put this into context: The state-of-the-art language model by Jozefowicz et al. (2016) achieves a perplexity of $24.2$ per word on the 1B Word Benchmark. Such a model would thus require an average of around 4.604.60 bits to encode each word, as $2^{4.60}=24.2$, which is incredibly close to the experimental lower bounds documented by Shannon. If and how we could use such a model to construct a better hierarchical softmax layer is still left to be explored.</p><h2 id="Differentiated-Softmax"><a href="#Differentiated-Softmax" class="headerlink" title="Differentiated Softmax"></a>Differentiated Softmax</h2><p>Chen et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:9" target="_blank" rel="external">9</a>] introduce a variation on the traditional softmax layer, the Differentiated Softmax (D-Softmax). D-Softmax is based on the intuition that not all words require the same number of parameters: Many occurrences of frequent words allow us to fit many parameters to them, while extremely rare words might only allow to fit a few.</p><p>In order to do this, instead of the dense matrix of the regular softmax layer of size $d×|V|$ containing the output word embeddings $v^{\prime}_w \in \mathbb{R}^d$, they use a sparse matrix. They then arrange $v′w$ in blocks sorted by frequency, with the embeddings in each block being of a certain dimensionality $d_k$. The number of blocks and their embedding sizes are hyperparameters that can be tuned.</p><p><img src="http://ruder.io/content/images/2016/05/differentiated_softmax_1.png" alt="Differentiated softmax"></p><p>Figure 3: Differentiated softmax (Chen et al. (2015))</p><p>In Figure 3, embeddings in partition $A$ are of dimensionality $d_A$ (these are embeddings of frequent words, as they are allocated more parameters), while embeddings in partitions $B$ and $C$ have $d_B$ and $d_C$ dimensions respectively. Note that all areas not part of any partition, i.e. the non-shaded areas in Figure 1, are set to $0$.</p><p>The output of the previous hidden layer $h$ is treated as a concatenation of features corresponding to each partition of the dimensionality of that partition, e.g. $h$ in Figure 3 is made up of partitions of size $d_A$, $d_B$, and $d_B$ respectively. Instead of computing the matrix-vector product between the entire output embedding matrix and $h$ as in the regular softmax, D-Softmax then computes the product of each partition and its corresponding section in $h$.</p><p>As many words will only require comparatively few parameters, the complexity of computing the softmax is reduced, which speeds up training. In contrast to H-Softmax, this speed-up persists during testing. Chen et al. (2015) observe that D-Softmax is the fastest method when testing, while being one of the most accurate. However, as it assigns fewer parameters to rare words, D-Softmax does a worse job at modelling them.</p><h2 id="CNN-Softmax"><a href="#CNN-Softmax" class="headerlink" title="CNN-Softmax"></a>CNN-Softmax</h2><p>Another modification to the traditional softmax layer is inspired by recent work by Kim et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:13" target="_blank" rel="external">13</a>] who produce input word embeddings $v_w$ via a character-level CNN. Jozefowicz et al. (2016) in turn suggest to do the same thing for the output word embeddings $v^{\prime}_w$ via a character-level CNN – and refer to this as CNN-Softmax. Note that if we have a CNN at the input and at the output as in Figure 4, the CNN generating the output word embeddings $v^{\prime}_w$ is necessarily different from the CNN generating the input word embeddings $v_w$, just as the input and output word embedding matrices would be different.</p><p><img src="http://ruder.io/content/images/2016/05/cnn-softmax_1.png" alt="CNN-Softmax"></p><p>Figure 4: CNN-Softmax (Jozefowicz et al. (2016))</p><p>While this still requires computing the regular softmax normalization, this approach drastically reduces the number of parameters of the model: Instead of storing an embedding matrix of $d \times |V|$, we now only need to keep track of the parameters of the CNN. During testing, the output word embeddings $v^{\prime}_w$ can be pre-computed, so that there is no loss in performance.</p><p>However, as characters are represented in a continuous space and as the resulting model tends to learn a smooth function mapping characters to a word embedding, character-based models often find it difficult to differentiate between similarly spelled words with different meanings. To mitigate this, the authors add a correction factor that is learned per word, which significantly reduces the performance gap between regular and CNN-softmax. By adjusting the dimensionality of the correction term, the authors are able to trade-off model size versus performance.</p><p>The authors also note that instead of using a CNN-softmax, the output of the previous layer hh can be fed to a character-level LSTM, which predicts the output word one character at a time. Instead of a softmax over words, a softmax outputting a probability distribution over characters would thus be used at every time step. They, however, fail to achieve competitive performance with this layer. Ling et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:14" target="_blank" rel="external">14</a>] use a similar layer for machine translation and achieve competitive results.</p><h1 id="Sampling-based-Approaches"><a href="#Sampling-based-Approaches" class="headerlink" title="Sampling-based Approaches"></a>Sampling-based Approaches</h1><p>While the approaches discussed so far still maintain the overall structure of the softmax, sampling-based approaches on the other hand completely do away with the softmax layer. They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. However, sampling-based approaches are only useful at training time – during inference, the full softmax still needs to be computed to obtain a normalised probability.</p><p>In order to gain some intuitions about the softmax denominator’s impact on the loss, we will derive the gradient of our loss function $J_{\theta}$ w.r.t. the parameters of our model $\theta$.<br>During training, we aim to minimize the cross-entropy loss of our model for every word $w$ in the training set. This is simply the negative logarithm of the output of our softmax. If you are unsure of this connection, have a look at <a href="http://cs231n.github.io/linear-classify/#softmax-classifier" target="_blank" rel="external">Karpathy’s explanation</a> to gain some more intuitions about the connection between softmax and cross-entropy. The loss of our model is then the following:<br>$$<br>J_{\theta} = − \log \frac{\exp(h^{\text{T}} v^{\prime}_w)}{\sum_{w_i \in V} \exp(h^{\text{T}} v^{\prime}_{w_i})}.<br>$$<br>Note that in practice $J_{\theta}$ would be the average of all negative log-probabilities over the whole corpus. To facilitate the derivation, we decompose $J_{\theta}$ into a sum as $\log \frac{x}{y} = \log x − \log y$:<br>$$<br>J_\theta = - \: h^\top v^{\prime}_{w} + \text{log} \sum_{w_i \in V} \text{exp}(h^\top v’_{w_i})<br>$$<br>For brevity and to conform with the notation of Bengio and Senécal [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:4" target="_blank" rel="external">4</a>, <a href="http://ruder.io/word-embeddings-softmax/index.html#fn:15" target="_blank" rel="external">15</a>] (note that in the first paper, they compute the gradient of the <em>positive</em> logarithm), we replace the dot product $h^\top v’_{w}$ with $- \mathcal{E}(w)$. Our loss then looks like the following:<br>$$<br>J_\theta = \: \mathcal{E}(w) + \text{log} \sum_{w_i \in V} \text{exp}( - \mathcal{E}(w_i))<br>$$<br>For back-propagation, we can now compute the gradient $\nabla$i of $J_{\theta}$ w.r.t. our model’s parameters $\theta$:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \nabla_\theta \text{log} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))<br>$$<br>As the gradient of $\log x$ is $\dfrac{1}{x}$, an application of the chain rule yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)<br>$$<br>We can now move the gradient inside the sum:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \nabla_\theta \: \text{exp}(- \mathcal{E}(w_i))<br>$$<br>As the gradient of $\exp(x)​$ is just $\exp(x)​$, another application of the chain rule yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)) \nabla_\theta (- \mathcal{E}(w_i))<br>$$<br>We can rewrite this as:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} \dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta (- \mathcal{E}(w_i))<br>$$<br>Note that $\dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))}$ is just the softmax probability $P(w_i)$ of $w_i$ (we omit the dependence on the context cc here for brevity). Replacing it yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} P(w_i) \nabla_\theta (- \mathcal{E}(w_i))<br>$$<br>Finally, repositioning the negative coefficient in front of the sum yields:<br>$$<br>\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) - \sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i)<br>$$<br>Bengio and Senécal (2003) note that the gradient essentially has two parts: a positive reinforcement for the target word $w$ (the first term in the above equation) and a negative reinforcement for all other words $w_i$, which is weighted by their probability (the second term). As we can see, this negative reinforcement is just the expectation $\mathbb{E}_{w_i \sim P}$ of the gradient of $\mathcal{E}$ for all words $w_i$ in $V$:<br>$$<br>\sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i) = \mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)]<br>$$<br>The crux of most sampling-based approach now is to approximate this negative reinforcement in some way to make it easier to compute, since we don’t want to sum over the probabilities for all words in $V$.</p><h2 id="Importance-Sampling"><a href="#Importance-Sampling" class="headerlink" title="Importance Sampling"></a>Importance Sampling</h2><p>We can approximate the expected value $E$ of any probability distribution using the Monte Carlo method, i.e. by taking the mean of random samples of the probability distribution. If we knew the network’s distribution, i.e. $P(w)$, we could thus directly sample mm words $w_1, \cdots ,w_m$ from it and approximate the above expectation with:<br>$$<br>\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{m} \sum\limits^m_{i=1} \nabla_\theta \mathcal{E}(w_i)<br>$$<br>However, in order to sample from the probability distribution $P$, we need to compute $P$, which is just what we wanted to avoid in the first place. We therefore have find some other distribution $Q$ (we call this the proposal distribution), from which it is cheap to sample and which can be used as the basis of Monte-Carlo sampling. Preferably, $Q$ should also be similar to $P$, since we want our approximated expectation to be as accurate as possible. A straightforward choice in the case of language modelling is to simply use the unigram distribution of the training set for $Q$.</p><p>This is essentially what classical Importance Sampling (IS) does: It uses Monte-Carlo sampling to approximate a target distribution $P$ via a proposal distribution $Q$. However, this still requires computing $P(w)$ for every word ww that is sampled. To avoid this, Bengio and Senécal (2003) use a biased estimator that was first proposed by Liu [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:16" target="_blank" rel="external">16</a>]. This estimator can be used when $P(w)$ is computed as a product, which is the case here, since every division can be transformed into a multiplication.</p><p>Essentially, instead of weighting the gradient $\nabla_\theta \mathcal{E}(w_i)$ with the expensive to compute probability $P_{w_i}$, we weight it with a factor that leverages the proposal distribution $Q$. For biased IS, this factor is $\dfrac{1}{R}r(w_i)$ where $r(w) = \dfrac{\text{exp}(- \mathcal{E}(w))}{Q(w)}$ and $R = \sum^m_{j=1} r(w_j)$.</p><p>Note that we use $r$ and $R$ instead of $w$ and $W$ as in Bengio and Senécal (2003, 2008) to avoid name clashes. As we can see, we still compute the numerator of the softmax, but replace the normalisation in the denominator with the proposal distribution $Q$. Our biased estimator that approximates the expectation thus looks like the following:<br>$$<br>\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{R} \sum\limits^m_{i=1} r(w_i) \nabla_\theta \: \mathcal{E}(w_i)<br>$$<br>Note that the fewer samples we use, the worse is our approximation. We additionally need to adjust our sample size during training, as the network’s distribution $P$ might diverge from the unigram distribution $Q$ during training, which leads to divergence of the model, if the sample size that is used is too small. Consequently, Bengio and Senécal introduce a measure to calculate the effective sample size in order to protect against possible divergence. Finally, the authors report a speed-up factor of $19$ over the regular softmax for this method.</p><h2 id="Adaptive-Importance-Sampling"><a href="#Adaptive-Importance-Sampling" class="headerlink" title="Adaptive Importance Sampling"></a>Adaptive Importance Sampling</h2><p>Bengio and Senécal (2008) note that for Importance Sampling, substituting more complex distributions, e.g. bigram and trigram distributions, later in training to combat the divergence of the unigram distribution $Q$ from the model’s true distribution $P$ does not help, as n-gram distributions seem to be quite different from the distribution of trained neural language models. As an alternative, they propose an n-gram distribution that is adapted during training to follow the target distribution $P$ more closely. To this end, they interpolate a bigram distribution and a unigram distribution according to some mixture function, whose parameters they train with SGD for different frequency bins to minimize the Kullback-Leibler divergence between the distribution $Q$ and the target distribution $P$. For experiments, they report a speed-up factor of about $100$.</p><h2 id="Target-Sampling"><a href="#Target-Sampling" class="headerlink" title="Target Sampling"></a>Target Sampling</h2><p>Jean et al. (2015) propose to use Adaptive Importance Sampling for machine translation. In order to make the method more suitable for processing on a GPU with limited memory, they limit the number of target words that need to be sampled from. They do this by partitioning the training set and including only a fixed number of sample words in every partition, which form a subset $V^{\prime}$ of the vocabulary.</p><p>This essentially means that a separate proposal distribution $Q_i$ can be used for every partition ii of the training set, which assigns equal probability to all words included in the vocabulary subset $V’_i$ and zero probability to all other words.</p><h2 id="Noise-Contrastive-Estimation"><a href="#Noise-Contrastive-Estimation" class="headerlink" title="Noise Contrastive Estimation"></a>Noise Contrastive Estimation</h2><p>Noise Contrastive Estimation (NCE) (Gutmann and Hyvärinen) [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:17" target="_blank" rel="external">17</a>] is proposed by Mnih and Teh [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:18" target="_blank" rel="external">18</a>] as a more stable sampling method than Importance Sampling (IS), as we have seen that IS poses the risk of having the proposal distribution $Q$ diverge from the distribution $P$ that should be optimized. In contrast to the former, NCE does not try to estimate the probability of a word directly. Instead, it uses an auxiliary loss that also optimises the goal of maximizing the probability of correct words.</p><p>Recall the pairwise-ranking criterion of Collobert and Weston (2008) that ranks positive windows higher than “corrupted” windows, which we discussed in the <a href="http://sebastianruder.com/word-embeddings-1/index.html" target="_blank" rel="external">previous post</a>. NCE does a similar thing: We train a model to differentiate the target word from noise. We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive, genuine data from noise samples, as can be seen in Figure 4 below.</p><p><img src="http://ruder.io/content/images/2016/06/negative_sampling.png" alt="Noise Contrastive Estimation"></p><p>Figure 4: Noise Contrastive Estimation (Stephan Gouws’ PhD dissertation [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:24" target="_blank" rel="external">24</a>])</p><p>For every word $w_i$ given its context $c_i$ of $n$ previous words $w_{t-1} , \cdots , w_{t-n+1}$ in the training set, we thus generate $k$ noise samples $w~ik$ from a noise distribution $Q$. As in IS, we can sample from the unigram distribution of the training set. As we need labels to perform our binary classification task, we designate all correct words $w_i$ given their context $c_i$ as true ($y=1$) and all noise samples $\tilde{w}_{ik}$ as false ($y=0$).</p><p>We can now use logistic regression to minimize the negative log-likelihood, i.e. cross-entropy of our training examples against the noise (conversely, we could also maximize the <em>positive</em> log-likelihood as some papers do):<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \mathbb{E}_{\tilde{w}_{ik} \sim Q} [ \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]]<br>$$<br>Instead of computing the expectation $\mathbb{E}_{\tilde{w}_{ik} \sim Q}$ of our noise samples, which would still require summing over all words in $V$ to predict the normalised probability of a negative label, we can again take the mean with the Monte Carlo approximation:<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \sum_{j=1}^k \dfrac{1}{k} \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]<br>$$<br>which reduces to:<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + \: \sum_{j=1}^k \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]<br>$$<br>By generating $k$ noise samples for every genuine word $w_i$ given its context $c$, we are effectively sampling words from two different distributions: Correct words are sampled from the empirical distribution of the training set $P_{\text{train}}$ and depend on their context $c$, whereas noise samples come from the noise distribution $Q$. We can thus represent the probability of sampling either a positive or a noise sample as a mixture of those two distributions, which are weighted based on the number of samples that come from each:<br>$$<br>P(y, w \: | \: c) = \dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)<br>$$<br>Given this mixture, we can now calculate the probability that a sample came from the training $P_{\text{train}}$ distribution as a conditional probability of $y$ given $w$ and $c$:<br>$$<br>P(y=1\:|\:w,c)= \dfrac{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)}{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)}<br>$$<br>which can be simplified to:<br>$$<br>P(y=1\:|\:w,c)= \dfrac{P_{\text{train}}(w \: | \: c)}{P_{\text{train}}(w \: | \: c) + k \: Q(w)}<br>$$<br>As we don’t know $P_{\text{train}}$ (which is what we would like to calculate), we replace $P_{\text{train}}$ with the probability of our model $P$:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{P(w \: | \: c)}{P(w \: | \: c) + k \: Q(w)}<br>$$<br>The probability of predicting a noise sample ($y=0$) is then simply $P(y=0\:|\:w,c) = 1 - P(y=1\:|\:w,c)$. Note that computing $P(w|c)$, i.e. the probability of a word $w$ given its context $c$ is essentially the definition of our softmax:<br>$$<br>P(w \: | \: c) = \dfrac{\text{exp}({h^\top v’_{w}})}{\sum_{w_i \in V} \text{exp}({h^\top v’_{w_i}})}<br>$$<br>For notational brevity and unambiguity, let us designate the denominator of the softmax with $Z(c)$, since the denominator only depends on $h$, which is generated from $c$ (assuming a fixed $V$). The softmax then looks like this:</p><p>$$<br>P(w \: | \: c) = \dfrac{\text{exp}({h^\top v’_{w}})}{Z(c)}<br>$$<br>Having to compute $P(w|c)$ means that – again – we need to compute $Z(c)$, which requires us to sum over the probabilities of all words in $V$. In the case of NCE, there exists a neat trick to circumvent this issue: We can treat the normalisation denominator $Z(c)$ as a parameter that the model can learn.</p><p>Mnih and Teh (2012) and Vaswani et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:20" target="_blank" rel="external">20</a>] actually keep $Z(c)$ fixed at $1$, which they report does not affect the model’s performance. This assumption has the nice side-effect of reducing the model’s parameters, while ensuring that the model self-normalises by not depending on the explicit normalisation in $Z(c)$. Indeed, Zoph et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:19" target="_blank" rel="external">19</a>] find that even when learned, $Z(c)$ is close to $1$ and has low variance.</p><p>If we thus set $Z(c)$ to $1$ in the above softmax equation, we are left with the following probability of word $w$ given a context $c$:</p><p>$$<br>P(w \: | \: c) = \text{exp}({h^\top v’_{w}})<br>$$<br>We can now insert this term in the above equation to compute $P(y=1|w,c)$:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + k \: Q(w)}<br>$$<br>Inserting this term in turn in our logistic regression objective finally yields the full NCE loss:</p><p>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{\text{exp}({h^\top v’_{w_i}})}{\text{exp}({h^\top v’_{w_i}}) + k \: Q(w_i)} + \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{\text{exp}({h^\top v’_{\tilde{w}_{ij}}})}{\text{exp}({h^\top v’_{\tilde{w}_{ij}}}) + k \: Q(\tilde{w}_{ij})})]<br>$$<br>Note that NCE has nice theoretical guarantees: It can be shown that as we increase the number of noise samples $k$, the NCE derivative tends towards the gradient of the softmax function. Mnih and Teh (2012) report that $25$ noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about $45$. For more information on NCE, Chris Dyer has published some excellent notes [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:21" target="_blank" rel="external">21</a>].</p><p>One caveat of NCE is that as typically different noise samples are sampled for every training word ww, the noise samples and their gradients cannot be stored in dense matrices, which reduces the benefit of using NCE with GPUs, as it cannot benefit from fast dense matrix multiplications. Jozefowicz et al. (2016) and Zoph et al. (2016) independently propose to share noise samples across all training words in a mini-batch, so that NCE gradients can be computed with dense matrix operations, which are more efficient on GPUs.</p><h3 id="Similarity-between-NCE-and-IS"><a href="#Similarity-between-NCE-and-IS" class="headerlink" title="Similarity between NCE and IS"></a>Similarity between NCE and IS</h3><p>Jozefowicz et al. (2016) show that NCE and IS are not only similar as both are sampling-based approaches, but are strongly connected. While NCE uses a binary classification task, they show that IS can be described similarly using a surrogate loss function: Instead of performing binary classification with a logistic loss function like NCE, IS then optimises a multi-class classification problem with a softmax and cross-entropy loss function. They observe that as IS performs multi-class classification, it may be a better choice for language modelling, as the loss leads to tied updates between the data and noise samples rather than independent updates as with NCE. Indeed, Jozefowicz et al. (2016) use IS for language modelling and obtain state-of-the-art performance (as mentioned above) on the 1B Word benchmark.</p><h2 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h2><p>Negative Sampling (NEG), the objective that has been popularised by Mikolov et al. (2013), can be seen as an approximation to NCE. As we have mentioned above, NCE can be shown to approximate the loss of the softmax as the number of samples kk increases. NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modelling.</p><p>NEG also uses a logistic loss function to minimise the negative log-likelihood of words in the training set. Recall that NCE calculated the probability that a word $w$ comes from the empirical training distribution $P_{\text{train}}$ given a context $c$ as follows:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + k \: Q(w)}<br>$$<br>The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible. For this reason, it sets the most expensive term, $kQ(w)$ to $1$, which leaves us with:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top v’_{w}})}{\text{exp}({h^\top v’_{w}}) + 1}<br>$$<br>$kQ(w)=1$ is exactly then true, when $k=|V|$ and $Q$ is a uniform distribution. In this case, NEG is equivalent to NCE. The reason we set $kQ(w)=1$ and not to some other constant can be seen by rewriting the equation, as $P(y=1|w,c)$ can be transformed into the sigmoid function:</p><p>$$<br>P(y=1\:|\:w,c)= \dfrac{1}{1 + \text{exp}({-h^\top v’_{w}})}<br>$$<br>If we now insert this back into the logistic regression loss from before, we get:</p><p>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v’_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{1}{1 + \text{exp}({-h^\top v’_{\tilde{w}_{ij}}})}]<br>$$<br>By simplifying slightly, we obtain:</p><p>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top v’_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (\dfrac{1}{1 + \text{exp}({h^\top v’_{\tilde{w}_{ij}}})}]<br>$$<br>Setting $\sigma(x) = \dfrac{1}{1 + \text{exp}({-x})}$ finally yields the NEG loss:<br>$$<br>J_\theta = - \sum_{w_i \in V} [ \text{log} \: \sigma(h^\top v’_{w_i}) + \: \sum_{j=1}^k \:\text{log} \: \sigma(-h^\top v’_{\tilde{w}_{ij}})]<br>$$<br>To conform with the notation of Mikolov et al. (2013), $h$ must be replaced with $v_{w_{I}}$, v′wivwi′ with v′wOvwO′ and vw~ijvw~ij with v′wivwi′. Also, in contrast to Mikolov’s NEG objective, we a) optimise the objective over the whole corpus, b) minimise negative log-likelihood instead of maximising positive log-likelihood (as mentioned before), and c) have already replaced the expectation Ew~ik∼QEw~ik∼Q with its Monte Carlo approximation. For more insights on the derivation of NEG, have a look at Goldberg and Levy’s notes [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:22" target="_blank" rel="external">22</a>].</p><p>We have seen that NEG is only equivalent to NCE when $k=|V|$ and $Q$ is uniform. In all other cases, NEG only approximates NCE, which means that it will not directly optimise the likelihood of correct words, which is key for language modelling. While NEG may thus be useful for learning word embeddings, its lack of asymptotic consistency guarantees makes it inappropriate for language modelling.</p><h2 id="Self-Normalisation"><a href="#Self-Normalisation" class="headerlink" title="Self-Normalisation"></a>Self-Normalisation</h2><p>Even though the self-normalisation technique proposed by Devlin et al. <a href="http://ruder.io/word-embeddings-softmax/index.html#fn:23" target="_blank" rel="external">23</a> is not a sampling-based approach, it provides further intuitions on self-normalisation of language models, which we briefly touched upon. We previously mentioned in passing that by setting the denominator $Z(c)$ of the NCE loss to $1$, the model essentially self-normalises. This is a useful property as it allows us to skip computing the expensive normalisation in $Z(c)$.</p><p>Recall that our loss function $J_{\theta}$ minimises the negative log-likelihood of all words $w_i$ in our training data:</p><p>$$<br>J_\theta = - \sum\limits_i [\text{log} \: \dfrac{\text{exp}({h^\top v’_{w_i}})}{Z(c)}]<br>$$<br>We can decompose the softmax into a sum as we did before:</p><p>$$<br>J_\theta \: P(w \: | \: c) = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c)]<br>$$<br>If we are able to constrain our model so that it sets $Z(c)=1$ or similarly $\log Z(c)=0$, then we can avoid computing the normalisation in $Z(c)$ altogether. Devlin et al. (2014) thus propose to add a squared error penalty term to the loss function that encourages the model to keep $\log Z(c)$ as close as possible to $0$:</p><p>$$<br>J_\theta = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c) - \alpha \:(\text{log}(Z(c)) - 0)^2]<br>$$<br>which can be rewritten as:</p><p>$$<br>J_\theta = - \sum\limits_i [h^\top v’_{w_i} + \text{log} \: Z(c) - \alpha \: \text{log}^2 Z(c)]<br>$$<br>where αα allows us to trade-off between model accuracy and mean self-normalisation. By doing this, we can essentially guarantee that $Z(c)$ will be as close to $1$ as we want. At decoding time in their MT system, Devlin et al. (2014) then set the denominator of the softmax to $1$ and only use the numerator for computing P(w|c)P(w|c) together with their penalty term:</p><p>$$<br>J_\theta = - \sum\limits_i [h^\top v’_{w_i} - \alpha \: \text{log}^2 Z(c)]<br>$$<br>They report that self-normalisation achieves a speed-up factor of about $15$, while only resulting in a small degradation of BLEU scores compared to a regular non-self-normalizing neural language model.</p><h2 id="Infrequent-Normalisation"><a href="#Infrequent-Normalisation" class="headerlink" title="Infrequent Normalisation"></a>Infrequent Normalisation</h2><p>Andreas and Klein [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:11" target="_blank" rel="external">11</a>] suggest that it should even be sufficient to only normalise a fraction of the training examples and still obtain approximate self-normalising behaviour. They thus propose Infrequent Normalisation (IN), which down-samples the penalty term, making this a sampling-based approach.</p><p>Let us first decompose the sum of the previous loss $J_{\theta}$ into two separate sums:</p><p>$$<br>J_\theta = - \sum\limits_i h^\top v’_{w_i} + \alpha \sum\limits_i \text{log}^2 Z(c)<br>$$<br>We can now down-sample the second term by only computing the normalisation for a subset $C$ of words $w_j$ and thus of contexts $c_j$ (as $Z(c)$ only depends on the context $c$) in the training data:</p><p>$$<br>J_\theta = - \sum\limits_i h^\top v’_{w_i} + \dfrac{\alpha}{\gamma} \sum\limits_{c_j \in C} \text{log}^2 Z(c_j)<br>$$<br>where $\gamma$ controls the size of the subset $C$. Andreas and Klein (2015) suggest that IF combines the strengths of NCE and self-normalisation as it does not require computing the normalisation for all training examples (which NCE avoids entirely), but like self-normalisation allows trading-off between the accuracy of the model and how well normalisation is approximated. They observe a speed-up factor of $10$ when normalising only a tenth of the training set, with no noticeable performance penalty.</p><h3 id="Other-Approaches"><a href="#Other-Approaches" class="headerlink" title="Other Approaches"></a>Other Approaches</h3><p>So far, we have focused exclusively on approximating or even entirely avoiding the computation of the softmax denominator $Z(c)$, as it is the most expensive term in the computation. We have thus not paid particular attention to $h^\top v’_{w}$, i.e. the dot-product between the penultimate layer representation hh and output word embedding $v^{\prime}_w$. Vijayanarasimhan et al. [<a href="http://ruder.io/word-embeddings-softmax/index.html#fn:12" target="_blank" rel="external">12</a>] propose fast locality-sensitive hashing to approximate $h^\top v^{\prime}_{w}$. However, while this technique accelerates the model at test time, during training, these speed-ups virtually vanish as embeddings must be re-indexed and the batch size increases.</p><h1 id="Which-Approach-to-Choose"><a href="#Which-Approach-to-Choose" class="headerlink" title="Which Approach to Choose?"></a>Which Approach to Choose?</h1><p>Having reviewed the most popular softmax-based and sampling-based approaches, we have shown that there are plenty of alternatives to the good ol’ softmax and almost all of them promise a significant speed-up and equivalent or at most marginally deteriorated performance. This naturally poses the question which approach is the best for a particular task.</p><table><thead><tr><th>Approach</th><th>Speed-upfactor</th><th>Duringtraining?</th><th>Duringtesting?</th><th>Performance(small vocab)</th><th>Performance(large vocab)</th><th>Proportion ofparameters</th></tr></thead><tbody><tr><td>Softmax</td><td>1x</td><td>-</td><td>-</td><td>very good</td><td>very poor</td><td>100%</td></tr><tr><td>Hierarchical Softmax</td><td>25x (50-100x)</td><td>X</td><td>-</td><td>very poor</td><td>very good</td><td>100%</td></tr><tr><td>Differentiated Softmax</td><td>2x</td><td>X</td><td>X</td><td>very good</td><td>very good</td><td>&lt; 100%</td></tr><tr><td>CNN-Softmax</td><td>-</td><td>X</td><td>-</td><td>-</td><td>bad - good</td><td>30%</td></tr><tr><td>Importance Sampling</td><td>(19x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>AdaptiveImportance Sampling</td><td>(100x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>Target Sampling</td><td>2x</td><td>X</td><td>-</td><td>good</td><td>bad</td><td>100%</td></tr><tr><td>Noise ContrastiveEstimation</td><td>8x (45x)</td><td>X</td><td>-</td><td>very bad</td><td>very bad</td><td>100%</td></tr><tr><td>Negative Sampling</td><td>(50-100x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>Self-Normalisation</td><td>(15x)</td><td>X</td><td>-</td><td>-</td><td>-</td><td>100%</td></tr><tr><td>InfrequentNormalisation</td><td>6x (10x)</td><td>X</td><td>-</td><td>very good</td><td>good</td><td>100%</td></tr></tbody></table><p>Table 1: Comparison of approaches to approximate the softmax for language modelling.</p><p>We compare the performance of the approaches we discussed in this post for language modelling in Table 1. Speed-up factors and performance are based on the experiments by Chen et al. (2015), while we show speed-up factors reported by the authors of the original papers in brackets. The third and fourth columns indicate if the speed-up is achieved during training and testing respectively. Note that divergence of speed-up factors might be due to unoptimised implementations or the fact that the original authors might not have had access to GPUs, which benefit the regular softmax more than some of the other approaches. Performance for approaches where no comparison is available should largely be analogous to similar approaches, i.e. Self-Normalisation should achieve similar performance as Infrequent Normalisation and Importance Sampling and Adaptive Importance Sampling should achieve similar performance as Target Sampling. The performance of CNN-Softmax is as reported by Jozefowicz et al. (2016) and ranges from bad to good depending on the size of the correction. Of all approaches, only CNN-Softmax achieves a substantial reduction in parameters as the other approaches still require storing output embeddings. Differentiated Softmax reduces parameters by being able to store a sparse weight matrix.</p><p>As it always is, there is no clear winner that beats all other approaches on all datasets or tasks. For language modelling, the regular softmax still achieves very good performance on small vocabulary datasets, such as the Penn Treebank, and even performs well on medium datasets, such as Gigaword, but does very poorly on large vocabulary datasets, e.g. the 1B Word Benchmark. Target Sampling, Hierarchical Softmax, and Infrequent Normalisation in turn do better with large vocabularies.</p><p>Differentiated Softmax generally does well for both small and large vocabularies and is the only approach that ensures a speed-up at test time. Interestingly, Hierarchical Softmax (HS) performs very poorly with small vocabularies. However, of all methods, HS is the fastest and processes most training examples in a given time frame. While NCE performs well with large vocabularies, it is generally worse than the other methods. Negative Sampling does not work well for language modelling, but it is generally superior for learning word representations, as attested by word2vec’s success. Note that all results should be taken with a grain of salt: Chen et al. (2015) report having difficulties using Noise Contrastive Estimation in practice; Kim et al. (2016) use Hierarchical Softmax to achieve state-of-the-art with a small vocabulary, while Importance Sampling is used by the state-of-the-art language model by Jozefowicz et al. (2016) on a dataset with a large vocabulary.</p><p>Finally, if you are looking to actually use the described methods, TensorFlow has <a href="https://www.tensorflow.org/versions/master/api_docs/python/nn.html#candidate-sampling" target="_blank" rel="external">implementations</a> for a few sampling-based approaches and also explains the differences between some of them <a href="https://www.tensorflow.org/extras/candidate_sampling.pdf" target="_blank" rel="external">here</a>.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>This overview of different methods to approximate the softmax attempted to provide you with intuitions that can not only be applied to improve and speed-up learning word representations, but are also relevant for language modelling and machine translation. As we have seen, most of these approaches are closely related and are driven by one uniting factor: the necessity to approximate the expensive normalisation in the denominator of the softmax. With these approaches in mind, I hope you feel now better equipped to train and understand your models and that you might even feel ready to work on learning better word representations yourself.</p><p>As we have seen, learning word representations is a vast field and many factors are relevant for success. In the previous blog post, we looked at the architectures of popular models and in this blog post, we investigated more closely a key component, the softmax layer. In the next one, we will introduce GloVe, a method that relies on matrix factorisation rather than language modelling, and turn our attention to other hyperparameters that are essential for successfully learning word embeddings.</p><p><strong>As always, let me know about any mistakes I made and approaches I missed in the comments below.</strong></p><h1 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h1><p>If you found this blog post helpful, please consider citing it as:</p><p><em>Sebastian Ruder. On word embeddings - Part 2: Approximating the Softmax. <a href="http://ruder.io/word-embeddings-softmax" target="_blank" rel="external">http://ruder.io/word-embeddings-softmax</a>, 2016.</em></p><h1 id="Other-blog-posts-on-word-embeddings"><a href="#Other-blog-posts-on-word-embeddings" class="headerlink" title="Other blog posts on word embeddings"></a>Other blog posts on word embeddings</h1><p>If you want to learn more about word embeddings, these other blog posts on word embeddings are also available:</p><ul><li><a href="http://sebastianruder.com/word-embeddings-1/index.html" target="_blank" rel="external">On word embeddings - Part 1</a></li><li><a href="http://sebastianruder.com/secret-word2vec/index.html" target="_blank" rel="external">On word embeddings - Part 3: The secret ingredients of word2vec</a></li><li><a href="http://sebastianruder.com/cross-lingual-embeddings/index.html" target="_blank" rel="external">Unofficial Part 4: A survey of cross-lingual embedding models</a></li><li><a href="http://ruder.io/word-embeddings-2017/index.html" target="_blank" rel="external">Unofficial Part 5: Word embeddings in 2017 - Trends and future directions</a></li></ul><h1 id="Translations"><a href="#Translations" class="headerlink" title="Translations"></a>Translations</h1><p>This blog post has been translated into the following languages:</p><ul><li><a href="http://geek.csdn.net/news/detail/135736" target="_blank" rel="external">Chinese</a></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol><li>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1–9. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:1" target="_blank" rel="external"></a></li><li>Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1–12. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:2" target="_blank" rel="external"></a></li><li>Morin, F., &amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats, 5. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:3" target="_blank" rel="external"></a></li><li>Bengio, Y., &amp; Senécal, J.-S. (2003). Quick Training of Probabilistic Neural Nets by Importance Sampling. AISTATS. <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/submit_aistats2003.pdf" target="_blank" rel="external">http://www.iro.umontreal.ca/~lisa/pointeurs/submit_aistats2003.pdf</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:4" target="_blank" rel="external"></a></li><li>Shannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. <a href="http://doi.org/10.1002/j.1538-7305.1951.tb01366.x" target="_blank" rel="external">http://doi.org/10.1002/j.1538-7305.1951.tb01366.x</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:5" target="_blank" rel="external"></a></li><li>Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from <a href="http://arxiv.org/abs/1602.02410" target="_blank" rel="external">http://arxiv.org/abs/1602.02410</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:6" target="_blank" rel="external"></a></li><li>Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738, 1–19. Retrieved from <a href="http://arxiv.org/abs/1411.2738" target="_blank" rel="external">http://arxiv.org/abs/1411.2738</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:7" target="_blank" rel="external"></a></li><li>Mnih, A., &amp; Hinton, G. E. (2008). A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, 1–8. Retrieved from <a href="http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf" target="_blank" rel="external">http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:8" target="_blank" rel="external"></a></li><li>Chen, W., Grangier, D., &amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. Retrieved from <a href="http://arxiv.org/abs/1512.04906" target="_blank" rel="external">http://arxiv.org/abs/1512.04906</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:9" target="_blank" rel="external"></a></li><li>Jean, S., Cho, K., Memisevic, R., &amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1–10. Retrieved from <a href="http://www.aclweb.org/anthology/P15-1001" target="_blank" rel="external">http://www.aclweb.org/anthology/P15-1001</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:10" target="_blank" rel="external"></a></li><li>Andreas, J., &amp; Klein, D. (2015). When and why are log-linear models self-normalizing? Naacl-2015, 244–249. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:11" target="_blank" rel="external"></a></li><li>Vijayanarasimhan, S., Shlens, J., Monga, R., &amp; Yagnik, J. (2015). Deep Networks With Large Output Spaces. Iclr, 1–9. Retrieved from <a href="http://arxiv.org/abs/1412.7479" target="_blank" rel="external">http://arxiv.org/abs/1412.7479</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:12" target="_blank" rel="external"></a></li><li>Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from <a href="http://arxiv.org/abs/1508.06615" target="_blank" rel="external">http://arxiv.org/abs/1508.06615</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:13" target="_blank" rel="external"></a></li><li>Ling, W., Trancoso, I., Dyer, C., &amp; Black, A. W. (2016). Character-based Neural Machine Translation. ICLR, 1–11. Retrieved from <a href="http://arxiv.org/abs/1511.04586" target="_blank" rel="external">http://arxiv.org/abs/1511.04586</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:14" target="_blank" rel="external"></a></li><li>Bengio, Y., &amp; Senécal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713–722. <a href="http://doi.org/10.1109/TNN.2007.912312" target="_blank" rel="external">http://doi.org/10.1109/TNN.2007.912312</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:15" target="_blank" rel="external"></a></li><li>Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer. <a href="http://doi.org/10.1017/CBO9781107415324.004" target="_blank" rel="external">http://doi.org/10.1017/CBO9781107415324.004</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:16" target="_blank" rel="external"></a></li><li>Gutmann, M., &amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. International Conference on Artificial Intelligence and Statistics, 1–8. Retrieved from <a href="http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf" target="_blank" rel="external">http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:17" target="_blank" rel="external"></a></li><li>Mnih, A., &amp; Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICML’12), 1751–1758. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:18" target="_blank" rel="external"></a></li><li>Zoph, B., Vaswani, A., May, J., &amp; Knight, K. (2016). Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. NAACL. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:19" target="_blank" rel="external"></a></li><li>Vaswani, A., Zhao, Y., Fossum, V., &amp; Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), (October), 1387–1392. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:20" target="_blank" rel="external"></a></li><li>Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. Arxiv preprint. Retrieved from <a href="http://arxiv.org/abs/1410.8251" target="_blank" rel="external">http://arxiv.org/abs/1410.8251</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:21" target="_blank" rel="external"></a></li><li>Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method. arXiv Preprint arXiv:1402.3722, (2), 1–5. Retrieved from <a href="http://arxiv.org/abs/1402.3722" target="_blank" rel="external">http://arxiv.org/abs/1402.3722</a> <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:22" target="_blank" rel="external"></a></li><li>Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., &amp; Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. Proc. ACL’2014, 1370–1380. <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:23" target="_blank" rel="external"></a></li><li>Gouws, S. (2016). Training neural word embeddings for transfer learning and translation (Doctoral dissertation, Stellenbosch: Stellenbosch University). <a href="http://ruder.io/word-embeddings-softmax/index.html#fnref:24" target="_blank" rel="external"></a></li></ol><p>Credit for the cover image goes to <a href="http://stephangouws.com/" target="_blank" rel="external">Stephan Gouws</a> who included the image in his <a href="http://scholar.sun.ac.za/handle/10019.1/98758" target="_blank" rel="external">PhD dissertation</a> and in the <a href="https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html" target="_blank" rel="external">Tensorflow word2vec tutorial</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">122</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">58</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>