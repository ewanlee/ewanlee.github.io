<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/55/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/55/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/" itemprop="url">GRADIENTS, BATCH NORMALIZATION AND LAYER NORMALIZATION</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-09T10:17:48+08:00">2017-05-09 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/" itemprop="commentsCount"></span> </a></span><span id="/2017/05/09/GRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION/" class="leancloud_visitors" data-flag-title="GRADIENTS, BATCH NORMALIZATION AND LAYER NORMALIZATION"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In this post, we will take a look at common pit falls with optimization and solutions to some of these issues. The main topics that will be covered are:</p><ul><li>Gradients</li><li>Exploding gradients</li><li>Vanishing gradients</li><li>LSTMs (pertaining to vanishing gradients)</li><li>Normalization</li></ul><p>And then we will see how to implement batch and layer normalization and apply them to our cells.</p><h2 id="GRADIENTS"><a href="#GRADIENTS" class="headerlink" title="GRADIENTS"></a>GRADIENTS</h2><p>First, we will take a closer look at gradients and backpropagation during optimization. Our example will be a simple MLP but we will extend to an RNN later on.</p><p>I want to go over what a gradient means. Let’s say we have a very simple MLP with 1 set of weights W_1 which is used to calcualte some y. We devise a very simple loss function J, and our gradient becomes dJ/dW_1 (d = partials). Sure we can take the derivative and apply chain rule and get a number, but what does this value even mean? The gradient can be thought of as several things. One is that the magnitude of the gradient represents the sensitivity or impact this weight has on determining y which determines our loss. This can be seen below:</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-10-26-at-6-20-24-pm.png?w=620" alt="Screen Shot 2016-10-26 at 6.20.24 PM.png"></p><p><strong>CS231n</strong></p><p>What the gradients (dfdx, dfdy, dfdz, dfdq, dfdz) tell us is the sensitivity of each variable on our result f. In an MLP, we will produce a result (logits) and compare it with our targets to determine the deviance in what we got and what we should have gotten. From this we can use backpropagation to determine how much adjusting needs to be made for each variable along the way, all the way to the beginning.</p><p>The gradient also holds another key piece of information. It repesents how much we need to change the weights in order to move towards our goal (minimizing the loss, maximizing some objective, etc.). With simple SGD, we get the gradient and we apply an update to the weights (W_i_new = W_i_old – alpha * gradient). If we follow the direction of the gradient, we will be maximizing the goal function. Our loss functions (NLL or cross entropy) are functions we wish to minimize, so we subtract the gradient. We use the learning parameter alpha to control how quickly we change. This is where all of the normalization techniques in this post will come in handy.</p><p>If we have an alpha that is 1 or larger, we will allow the gradient to directly impact our weights. In the beginning of training a neural net, our weight initializations are bound to be far off from the weights we actually need. This creates a large error and so, results in large gradients. If we choose to update our weights with these large gradients, we will be never reach the minimum point for our loss function. We will keep overshooting and bouncing back and forth. So, we use this alpha (small value) to control how much impact the gradient has. Eventually, the gradient will get smaller as well because of less error and we will reach our goal, but with such a small alpha, this can take a while. With techniques, such as batch normalization and layer normalization, we can afford to use large alpha because the gradients will be controlled due to controlled outputs from the neurons.</p><p>Now, even with a simple RNN structure, backpropagation can pose several issues. When we get our result, we need to backpropagate all the way back to the very first cell in order to complete our updates. The main principles to really understand are: if I multiply a number greater than 1 over and over, I will reach infinity (explosion) and vice versa, if I multiply a number less than 1 over and over, I will reach 0 (vanishing).</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-10-04-at-5-54-13-am.png?w=620" alt="screen-shot-2016-10-04-at-5-54-13-am"></p><h2 id="EXPLODING-GRADIENTS"><a href="#EXPLODING-GRADIENTS" class="headerlink" title="EXPLODING GRADIENTS"></a>EXPLODING GRADIENTS</h2><p>The first issue is that our gradients can be greater than 1. As we backpropagate the gradient through the network, we can end up with massive gradients. So far, the solution to exploding gradients is a very hacky but cheap solution; just clip the norm of the gradient at some threshold.</p><p><img src="https://qph.ec.quoracdn.net/main-qimg-3e453fb7dd33a6e6e4d82adf6165d39a?convert_to_webp=true" alt="img"></p><h2 id="VANISHING-GRADIENTS"><a href="#VANISHING-GRADIENTS" class="headerlink" title="VANISHING GRADIENTS"></a>VANISHING GRADIENTS</h2><p>We could also experience the other issue where the gradient is less than 1 to start with and as we backpropagate, the effect of the gradient weakens and it will eventually be negligible. A common scenario where this occurs is when we have saturation at the tails of the sigmoidal function (0 or 1). This is problematic because now the derivative will always be near 0. During backpropagation, we will be multiplying this near zero derivative with our error repeatedly.</p><p>Let’s look at the sigmoidal activation function. You can replicate this example for tanh too.</p><p><img src="https://qph.ec.quoracdn.net/main-qimg-45bad3db11225318bd4aa686a823181c?convert_to_webp=true" alt="img"><img src="https://qph.ec.quoracdn.net/main-qimg-4635c1521f87e2d6f5cf4fe8f39ca76d?convert_to_webp=true" alt="img"></p><p>To solve this issue, we can use rectified linear units (ReLU) which don’t suffer from this tail saturation as much. <img src="https://qph.ec.quoracdn.net/main-qimg-bb38bf7ef543aa6a0c24134f61d15ba7?convert_to_webp=true" alt="img"></p><p>The derivative is 1 if x &gt; 0, so now error signal won’t weaken as it backpropagates through the network. But we do have the problem in the negative region (x &lt;0) where the derivative is zero. This can nullify our error signal so it’s best to add a leaky factor (<a href="http://arxiv.org/abs/1502.01852" target="_blank" rel="external">http://arxiv.org/abs/1502.01852</a>) to the ReLU unit, where the negative region will have some small negative slope. This parameter can be fixed or be a randomized parameter and be fixed after training. There’s also maxout (<a href="http://arxiv.org/abs/1302.4389" target="_blank" rel="external">http://arxiv.org/abs/1302.4389</a>) but this will have twice the amount of weights as a regular ReLU unit.</p><h2 id="LSTMS-VANISHING-GRADIENTS"><a href="#LSTMS-VANISHING-GRADIENTS" class="headerlink" title="LSTMS (VANISHING GRADIENTS)"></a>LSTMS (VANISHING GRADIENTS)</h2><p>As for how LSTMs solve the vanishing gradient issue, they don’t have to worry about the error signal weakening as with a regular basic RNN cell. It’s a bit complicated but the basic idea is that they have a forget gate that determines how much previous memory is stored in the network. This architecture allows the error signal to be transferred effectively to the previous time step. This is usually referred to as the constant error carousel (CEC).</p><h2 id="NORMALIZATION"><a href="#NORMALIZATION" class="headerlink" title="NORMALIZATION"></a>NORMALIZATION</h2><p>There are several types of normalization techniques but the idea behind all of them is the same, which is shifting our inputs to a zero mean and unit variance.</p><p>Techniques like batch norm (<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">https://arxiv.org/abs/1502.03167</a>) may help with the gradient issues as a side effect but the main object is to improve overall optimization. When we first initialize our weights, we are bound to have very large deviances from the true weights. These outliers need to be compensated for by the gradients and this further delays convergence during training. Batchnorm helps us here by normalizing the gradients (reducing influence from weight deviances) on a batched implementation and allows us to train faster (can even safely use larger learning rates now).</p><p>With batch norm, the main idea is to normalize at each layer for every minibatch. We initially may normalize our inputs, but as they travel through the layers, the inputs are operated on by weights and neurons and effectively change. As this progresses, the deviances get larger and larger and our backpropagation will need to account for these large deviances. This restricts us to using a small learning rate to prevent gradient explosion/vanishing. With <strong>batch norm</strong>, we will normalize the inputs (<strong>activations</strong> coming from the previous layer) going into each layer using the mean and variance of the activations for the <strong>entire</strong> <strong>minibatch</strong>. The normalization is a bit different during training and inference but it is beyond the scope of this post. (details in paper).</p><p>Batch normalization is very nice but it is based on minibatch size and so it’s a bit difficult to use with recurrent architectures. With <strong>layer normalization</strong>, we instead compute the mean and variance using ALL of the summed inputs to the neurons in a layer for EVERY <strong>single</strong> <strong>training**</strong>case<strong>. This removes the dependency on a minibatch size. Unlike batch normalization, the normalization operation for layer norm is same for training and inference. More details can be found on Hinton’s paper </strong>here**.</p><p>######</p><h2 id="IMPLEMENTING-BATCH-NORMALIZATION"><a href="#IMPLEMENTING-BATCH-NORMALIZATION" class="headerlink" title="IMPLEMENTING BATCH NORMALIZATION"></a>IMPLEMENTING BATCH NORMALIZATION</h2><p>As stated above, the main goal of batch normalization is optimization. By normalizing the inputs to a layer to zero mean and unit variance, we can help our net learn faster by minimizing the effects from large errors (especially during initial training).</p><p>Batch norm is given by the operation below, where \epsilon is a small random noise (for stability). When we apply batch norm on a layer, we are restricting the inputs to follow a normal distribution, which ultimately will restrict the nets ability to learn. In order to fix this, we multiply by a scale parameter (\alpha) and add a shift parameter (\beta). Both of these parameters are trainable.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-08-at-8-09-28-pm.png?w=620" alt="Screen Shot 2016-11-08 at 8.09.28 PM.png"></p><p>Note that both alpha and beta are applied element wise, so there will be a scale and shift for each neuron in the subsequent layer. With batchnorm, we compute mean and variance across an entire batch and we have a value for each neuron we are feeding our normalized inputs into.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-14-at-9-00-40-pm.png?w=620" alt="Screen Shot 2016-11-14 at 9.00.40 PM.png"></p><p>So for a given layer, the mean during BN will be 1X. Each training data gets this mean subtracted from it and divided by sqrt(var + epsilon) and then shifted and scaled. To find the mean and var, we use all the examples in the training batch.</p><p>In order to accurately evaluate the effectiveness of batchnorm, we will use a simple MLP to classify MNIST digits. We will run a normal MLP and an MLP with batchnorm, both initialized with the same starting weights. Let’s take a look at both the naive and TF implementations.</p><p>First, the naive version:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Naive BN layer</span></div><div class="line">scale1 = tf.Variable(tf.ones([<span class="number">100</span>]))</div><div class="line">shift1 = tf.Variable(tf.zeros([<span class="number">100</span>]))</div><div class="line">W1_BN = tf.Variable(W1_init)</div><div class="line">b1_BN = tf.Variable(tf.zeros([<span class="number">100</span>]))</div><div class="line">z1_BN = tf.matmul(X,W1_BN)+b1_BN</div><div class="line">mean1, var1 = tf.nn.moments(z1_BN, [<span class="number">0</span>])</div><div class="line">BN1 = (z1_BN - mean1) / tf.sqrt(var1 + FLAGS.epsilon)</div><div class="line">BN1 = scale1*BN1 + shift1</div><div class="line">fc1_BN = tf.nn.relu(BN1)</div></pre></td></tr></table></figure><p>TF implementation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># TF BN layer</span></div><div class="line">scale2 = tf.Variable(tf.ones([<span class="number">100</span>]))</div><div class="line">shift2 = tf.Variable(tf.zeros([<span class="number">100</span>]))</div><div class="line">W2_BN = tf.Variable(W2_init)</div><div class="line">b2_BN = tf.Variable(tf.zeros([<span class="number">100</span>]))</div><div class="line">z2_BN = tf.matmul(fc1_BN,W2_BN)+b2_BN</div><div class="line">mean2, var2 = tf.nn.moments(z2_BN, [<span class="number">0</span>])</div><div class="line">BN2 = tf.nn.batch_normalization(z2_BN,mean2,var2,shift2,scale2,FLAGS.epsilon)</div><div class="line">fc2_BN = tf.nn.relu(BN2)</div></pre></td></tr></table></figure><p>We first need to compute the mean and variance of the inputs coming into the layer. Then normalize them and scale/shift and then apply the activation function and pass to the next layer.</p><p>Let’s compare the performance of the normal MLP and the MLP with batchnorm. We will focus of the massive impact on our cost with and without BN. Other interesting features to look at would be gradient norm, neuron inputs, etc.</p><h3 id="CROSS-ENTROPY-LOSS"><a href="#CROSS-ENTROPY-LOSS" class="headerlink" title="CROSS ENTROPY LOSS"></a>CROSS ENTROPY LOSS</h3><p>###</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-10-at-8-58-44-pm.png?w=620" alt="Screen Shot 2016-11-10 at 8.58.44 PM.png"></p><h2 id="NUANCE"><a href="#NUANCE" class="headerlink" title="NUANCE:"></a>NUANCE:</h2><p>Training is all fine and well, but what about testing. When doing BN on our test set, with the implementation from above, we will be using the mean and variance from our test set. Now think about what will happen if our test set is very small or even size 1. This will homogenize all the outputs we get since all inputs will be close to mean 0 and variance 1. The solution to this is to calculate the population mean and variance during testing and then use those values during testing.</p><p>Now there are couple ways we can try to calculate the population, even simple as taking the average of the training batch and using it for testing. This isn’t the true population measure so we will calculate the unbiased mean and variance as they do in the original <strong>paper</strong>. But first, let’s see the accuracy when we feed in test samples of size 1.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-10-at-9-40-52-pm.png?w=620" alt="Screen Shot 2016-11-10 at 9.40.52 PM.png"></p><p>Not exactly state of the art anymore. So let’s see how to calculate population mean and variance.<br><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-09-at-7-45-05-pm.png?w=620" alt="Screen Shot 2016-11-09 at 7.45.05 PM.png"></p><p>We will be updating the population mean and variance after each training batch and we will use them for inference. In fact we can simple replace the inference batchnorm process with a simple linear transformation:</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-09-at-7-50-58-pm.png?w=620" alt="Screen Shot 2016-11-09 at 7.50.58 PM.png"></p><p>Below is the tensorflow implementation for batchnorm with the exponential moving average to use during inference. Take a look <strong>here</strong> for more implementation specifications for batch_norm but the required parameters for us is the actual input that we wish to normalize and wether or not we are training. Note: TF batchnorm with inference is in <strong>batch_norm2.py</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> tensorflow.contrib.layers <span class="keyword">import</span> (</div><div class="line">    batch_norm</div><div class="line">)</div><div class="line">...</div><div class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'BN_1'</span>) <span class="keyword">as</span> BN_1:</div><div class="line">    self.BN1 = tf.cond(self.is_training_ph,</div><div class="line">        <span class="keyword">lambda</span>: batch_norm(</div><div class="line">            self.z1_BN, is_training=<span class="keyword">True</span>, center=<span class="keyword">True</span>,</div><div class="line">            scale=<span class="keyword">True</span>, activation_fn=tf.nn.relu,</div><div class="line">            updates_collections=<span class="keyword">None</span>, scope=BN_1),</div><div class="line">        <span class="keyword">lambda</span>: batch_norm(</div><div class="line">            self.z1_BN, is_training=<span class="keyword">False</span>, center=<span class="keyword">True</span>,</div><div class="line">            scale=<span class="keyword">True</span>, activation_fn=tf.nn.relu,</div><div class="line">            updates_collections=<span class="keyword">None</span>, scope=BN_1, reuse=<span class="keyword">True</span>))</div></pre></td></tr></table></figure><p>Here are the inference results with the population mean and variance:</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-14-at-6-45-55-pm.png?w=620" alt="Screen Shot 2016-11-14 at 6.45.55 PM.png"></p><p>######</p><h2 id="IMPLEMENTING-LAYER-NORMALIZATION"><a href="#IMPLEMENTING-LAYER-NORMALIZATION" class="headerlink" title="IMPLEMENTING LAYER NORMALIZATION"></a>IMPLEMENTING LAYER NORMALIZATION</h2><p>Layernorm is very similar to batch normalization in many ways as you can see with the equation below but it usually reserved for use with recurrent architectures.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2017-01-19-at-6-23-36-pm.png?w=352&amp;h=105" alt="Screen Shot 2017-01-19 at 6.23.36 PM.png"></p><p>Layernorm acts on a per layer per sample basis, where the mean and variance are calculated for a specific layer for a specific training point. To understand the different between layernorm and batchnorm let’s see how these mean and variances are computed for both with figures.</p><p>With layernorm it’s a bit different from BN. We compute the mean and var for every single sample for each layer independently and then do the LN operations using those computed values.</p><p><img src="https://theneuralperspective.files.wordpress.com/2016/10/screen-shot-2016-11-14-at-8-59-44-pm.png?w=620" alt="Screen Shot 2016-11-14 at 8.59.44 PM.png"></p><p>First, we will make a function that will apply batch norm given an input tensor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># LN funcition</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ln</span><span class="params">(inputs, epsilon = <span class="number">1e-5</span>, scope = None)</span>:</span></div><div class="line"> </div><div class="line">    <span class="string">""" Computer LN given an input tensor. We get in an input of shape</span></div><div class="line">    [N X D] and with LN we compute the mean and var for each individual</div><div class="line">    training point across all it's hidden dimensions rather than across</div><div class="line">    the training batch as we do in BN. This gives us a mean and var of shape</div><div class="line">    [N X 1].</div><div class="line">    """</div><div class="line">    mean, var = tf.nn.moments(inputs, [<span class="number">1</span>], keep_dims=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(scope + <span class="string">'LN'</span>):</div><div class="line">        scale = tf.get_variable(<span class="string">'alpha'</span>,</div><div class="line">                                shape=[inputs.get_shape()[<span class="number">1</span>]],</div><div class="line">                                initializer=tf.constant_initializer(<span class="number">1</span>))</div><div class="line">        shift = tf.get_variable(<span class="string">'beta'</span>,</div><div class="line">                                shape=[inputs.get_shape()[<span class="number">1</span>]],</div><div class="line">                                initializer=tf.constant_initializer(<span class="number">0</span>))</div><div class="line">    LN = scale * (inputs - mean) / tf.sqrt(var + epsilon) + shift</div><div class="line"> </div><div class="line">    <span class="keyword">return</span> LN</div></pre></td></tr></table></figure><p>Now we can apply our LN function to a GRUCell class. Note that I am using tensorflow’s <strong>GRUCell class</strong> but we can apply LN to all of their other RNN variants as well (LSTM, peephole LSTM, etc.)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GRUCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">  <span class="string">"""Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)."""</span></div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_units, input_size=None, activation=tanh)</span>:</span></div><div class="line">    <span class="keyword">if</span> input_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">      logging.warn(<span class="string">"%s: The input_size parameter is deprecated."</span>, self)</div><div class="line">    self._num_units = num_units</div><div class="line">    self._activation = activation</div><div class="line"> </div><div class="line"><span class="meta">  @property</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">state_size</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> self._num_units</div><div class="line"> </div><div class="line"><span class="meta">  @property</span></div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">output_size</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> self._num_units</div><div class="line"> </div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, state, scope=None)</span>:</span></div><div class="line">    <span class="string">"""Gated recurrent unit (GRU) with nunits cells."""</span></div><div class="line">    <span class="keyword">with</span> vs.variable_scope(scope <span class="keyword">or</span> type(self).__name__):  <span class="comment"># "GRUCell"</span></div><div class="line">      <span class="keyword">with</span> vs.variable_scope(<span class="string">"Gates"</span>):  <span class="comment"># Reset gate and update gate.</span></div><div class="line">        <span class="comment"># We start with bias of 1.0 to not reset and not update.</span></div><div class="line">        r, u = array_ops.split(<span class="number">1</span>, <span class="number">2</span>, _linear([inputs, state],</div><div class="line">                                             <span class="number">2</span> * self._num_units, <span class="keyword">True</span>, <span class="number">1.0</span>))</div><div class="line"> </div><div class="line">        <span class="comment"># Apply Layer Normalization to the two gates</span></div><div class="line">        r = ln(r, scope = <span class="string">'r/'</span>)</div><div class="line">        u = ln(r, scope = <span class="string">'u/'</span>)</div><div class="line"> </div><div class="line">        r, u = sigmoid(r), sigmoid(u)</div><div class="line">      <span class="keyword">with</span> vs.variable_scope(<span class="string">"Candidate"</span>):</div><div class="line">        c = self._activation(_linear([inputs, r * state],</div><div class="line">                                     self._num_units, <span class="keyword">True</span>))</div><div class="line">      new_h = u * state + (<span class="number">1</span> - u) * c</div><div class="line">    <span class="keyword">return</span> new_h, new_h</div></pre></td></tr></table></figure><h2 id="SHAPES"><a href="#SHAPES" class="headerlink" title="SHAPES:"></a>SHAPES:</h2><p>I received quite a few PMs about some confusing aspects of BN and LN, mostly centered around what is actually the input. Let’s look at BN first. The input to a hidden layer will be [NXH]. Applying BN involves calculating the mean value for each H across all N samples. So we will have a mean of shape [1XH]. This “batch” mean will be used for BN, basically subtracting this batch mean from each sample.</p><p>Now for LN, let’s imagine a simple RNN situation. Batch major inputs are of shape [N, M, H], where N is the batch size, M is the max number of time steps and H is the number of hidden units. Before feeing to an RNN, we can reshape to time-major which becomes [M, N, H]. Now we feed in one time step at a time into the RNN, so the shape of each time-step’s input is [N,H]. Applying LN involves calculating the mean for sample across dimension [1], which means looking at all hidden states for each sample (for this particular time step). This gives us a mean of size [NX1]. We use this “layer” mean for each sample.</p><hr><p>Source page is <a href="https://theneuralperspective.com/2016/10/27/gradient-topics/" target="_blank" rel="external">HERE</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/54/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/54/">54</a><span class="page-number current">55</span><a class="page-number" href="/page/56/">56</a><span class="space">&hellip;</span><a class="page-number" href="/page/112/">112</a><a class="extend next" rel="next" href="/page/56/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">112</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">57</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>