<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/6/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/6/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/15/DropConnect-Implementation-in-Python-and-TensorFlow-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/15/DropConnect-Implementation-in-Python-and-TensorFlow-Repost/" itemprop="url">DropConnect Implementation in Python and TensorFlow [Repost]</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-15T22:36:19+08:00">2018-01-15 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/15/DropConnect-Implementation-in-Python-and-TensorFlow-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/15/DropConnect-Implementation-in-Python-and-TensorFlow-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/15/DropConnect-Implementation-in-Python-and-TensorFlow-Repost/" class="leancloud_visitors" data-flag-title="DropConnect Implementation in Python and TensorFlow [Repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Source post is <a href="https://nickcdryan.wordpress.com/2017/06/13/dropconnect-implementation-in-python-and-tensorflow/" target="_blank" rel="external">here</a>.</p><hr><p>I wouldn’t expect DropConnect to appear in TensorFlow, Keras, or Theano since, as far as I know, it’s used pretty rarely and doesn’t seem as well-studied or demonstrably more useful than its cousin, Dropout. However, there don’t seem to be any implementations out there, so I’ll provide a few ways of doing so.</p><p><img src="https://nickcdryan.files.wordpress.com/2017/06/screen-shot-2017-06-13-at-3-01-19-am.png?w=840" alt="Screen Shot 2017-06-13 at 3.01.19 AM"></p><p>For the briefest of refreshers, DropConnect (<a href="http://proceedings.mlr.press/v28/wan13.pdf" target="_blank" rel="external">Wan et al.</a>) regularizes networks like Dropout. Instead of dropping neurons, DropConnect regularizes by randomly dropping a subset of weights. A binary mask drawn from a Bernoulli distribution is applied to the original weight matrix (we’re just setting some connections to 0 with a certain probability):</p><p><img src="https://s0.wp.com/latex.php?latex=output+%3D+a%28%28M%C2%A0%5Codot+W%29v%29&amp;bg=ffffff&amp;fg=1a1a1a&amp;s=0" alt="output = a((M \odot W)v)"></p><p>where a is an activation function, v is input matrix, W is weight matrix, <img src="https://s0.wp.com/latex.php?latex=%5Codot&amp;bg=ffffff&amp;fg=1a1a1a&amp;s=0" alt="\odot"> is Hadamard (element-wise multiplication), and M is the binary mask drawn from a Bernoulli distribution with probability p.</p><p>Pure Python:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> operator</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask_size_helper</span><span class="params">(args)</span>:</span></div><div class="line">    <span class="comment"># multiply n dimensions to get array size</span></div><div class="line">    <span class="keyword">return</span> reduce(operator.mul, args) </div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dropconnect_mask</span><span class="params">(dc_keep_prob, dimensions)</span>:</span></div><div class="line">    <span class="comment"># get binary mask of size=*dimensions from binomial dist. with dc_keep_prob = prob of drawing a 1</span></div><div class="line">    mask_vector = np.random.binomial(<span class="number">1</span>, dc_keep_prob, mask_size_helper(dimensions))</div><div class="line">    <span class="comment"># reshape mask to correct dimensions (we could just broadcast, but that's messy)</span></div><div class="line">    mask_array = mask_vector.reshape(dimensions)</div><div class="line">    <span class="keyword">return</span> mask_array</div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropconnect</span><span class="params">(W, dc_keep_prob)</span>:</span></div><div class="line">    dimensions = W.shape</div><div class="line">    <span class="keyword">return</span> W * create_dropconnect_mask(dc_keep_prob, dimensions)</div></pre></td></tr></table></figure><p>TensorFlow (unnecessarily hard way):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropconnect</span><span class="params">(W, p)</span>:</span></div><div class="line">    M_vector = tf.multinomial(tf.log([[<span class="number">1</span>-p, p]]), np.prod(W_shape))</div><div class="line">    M = tf.reshape(M_vector, W_shape)</div><div class="line">    M = tf.cast(M, tf.float32)</div><div class="line">    <span class="keyword">return</span> M * W</div></pre></td></tr></table></figure><p>TensorFlow (easy way / recommended):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropconnect</span><span class="params">(W, p)</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.nn.dropout(W, keep_prob=p) * p</div></pre></td></tr></table></figure><p>Yes, sadly after a good amount of time spent searching for existing implementations and then creating my own, I took a look at the <a href="https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/ops/nn_ops.py" target="_blank" rel="external">dropout source code</a> and found that plain old dropout does the job so long as you remember to scale the weight matrix back down by keep_prob. After realizing that a connection weight matrix used for DropConnect is compatible input for the layer of neurons used in dropout, the only actual implementation difference between Dropout and DropConnect on TensorFlow is whether or not the weights in the masked matrix get scaled up (to preserve the expected sum).</p><p>I find DropConnect interesting, not so much as a regularization method but for some novel extensions that I’d like to try. I’ve played around with using keep_prob in our new DropConnect function as a trainable variable in the graph so that, if you incorporate keep_prob into the loss function in a way that creates interesting gradients, you can punish your network for the amount of connections it makes between neurons.</p><p>More interesting would be to see if we can induce modularity in the network by persisting dropped connections. That is, instead of randomly dropping an entirely new subset of connections at each training example, connections would drop and stay dropped perhaps as a result of the input data class or the connection’s contribution to deeper layers. For another post…</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/12/06/Short-Video-Title-Classification-Problem/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/12/06/Short-Video-Title-Classification-Problem/" itemprop="url">Short Video Title Classification</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-06T15:22:42+08:00">2017-12-06 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/12/06/Short-Video-Title-Classification-Problem/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/12/06/Short-Video-Title-Classification-Problem/" itemprop="commentsCount"></span> </a></span><span id="/2017/12/06/Short-Video-Title-Classification-Problem/" class="leancloud_visitors" data-flag-title="Short Video Title Classification"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>本文档是<a href="https://github.com/ewanlee/video_title_classification" target="_blank" rel="external">Github项目</a>的流程解释文档，具体实现请移步。</p><p>本项目解决的是视频短标题的多分类问题，目前涉及到33个类，所采用的算法包括TextCNN，TextRNN，TextRCNN以及HAN。目前效果最好的是TextCNN算法。</p><p>项目流程大体框架如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/1.png" alt="1"></p><h1 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h1><p>数据预处理部分主要涉及到的文件有：</p><ul><li><code>ordered_set.py</code></li><li><code>preprocess.py</code></li></ul><p>大致流程如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/2.png" alt="2"></p><h2 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h2><p>初始的文件包括三个：</p><ul><li><code>all_video_info.txt</code> 该文件是后两个数据的合并，作为数据预处理算法输入</li><li><code>all_video_info_month_day.txt</code>（这里的month和day由具体数值替换）这类文件包含多个，<strong>只使用最新的</strong>，是正式的标题数据， 包括已标记的以及未标记的</li><li><code>add_signed_video_info.txt</code> 该文件是从其他数据库中选取的经人工标注的数据，只含有已标记的标题</li></ul><p>所有文件的格式都是一样的，每一行代表一个样本，分为四列，中间用制表符间隔。</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/3.png" alt="3"></p><p>其中第一列代表视频URL；第二列为该视频类别是否经过算法修改，最开始全都为0；第三列为视频标签；第四列为视频标题。</p><p>视频标签的映射表如下：</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/4.png" alt="4"></p><p>在数据加载部分，我们将数据分为有标记数据以及无标记数据，有标记数据将用来训练以及测试分类器，然后用训练好的分类器预测无标记数据的标签。</p><p>分类的依据首先是根据视频标签是否为0，如果为0，代表视频是未标记的。其次，已标记的数据中有些类别是会对算法造成干扰，这里我们也将其去掉。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>load_data</code>方法。</p><h2 id="去除特殊符号"><a href="#去除特殊符号" class="headerlink" title="去除特殊符号"></a>去除特殊符号</h2><p>由于视频标题中存在一些表情等特殊符号，在这个阶段将其去掉。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>remove_emoji</code>方法。</p><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>本项目采用结巴分词作为分词器。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>cut</code>方法。</p><h2 id="去停止词"><a href="#去停止词" class="headerlink" title="去停止词"></a>去停止词</h2><p>本项目采用了<code>data/stopword.dic</code>文件中的停止词表，值得注意的是，句子去停止词前后去停止词后，单词的相对顺序保持不变。这里我们采用了有序集合（具体实现在<code>ordered_set.py</code>文件中）实现。</p><p>经过这一步之后，句子中重复的非停止词将只会取一次。但是由于视频标题较短，出现重复词的概率非常小，因此不会有太大影响。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>remove_stop_words</code>方法。</p><h2 id="建立词典"><a href="#建立词典" class="headerlink" title="建立词典"></a>建立词典</h2><p>将所有视频标题经过分词后的单词汇总起来建立一个词典，供后续句子建模使用。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>vocab_build</code>方法。</p><h2 id="句子建模"><a href="#句子建模" class="headerlink" title="句子建模"></a>句子建模</h2><p>将分词后的视频标题中的每个词替换为其在词典中的序号，这样每个标题将会转换为由一串数组构成的向量。</p><p>具体代码参照<code>preprocess.py</code>文件中的<code>word2index</code>方法。</p><h1 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h1><p>之前提到过，本文一共运用了四种深度学习模型，采用tensorflow框架，训练过程中涉及到的文件分为两类：</p><ul><li>模型文件， 包括<code>textcnn.py</code>, <code>textrnn.py</code>, <code>textrcnn.py</code>以及<code>han.py</code></li><li>训练文件，包括<code>train_cnn.py</code>, <code>train_rnn.py</code>, <code>train_rcnn.py</code>以及<code>train_han.py</code></li></ul><p>模型文件定义了具体的模型，本篇文档将不会具体地讲解实现代码，只会从理论层面介绍模型。训练文件包含了算法的训练过程，由于不同算法的训练流程一致，这里单挑TextCNN讲解。</p><p>下面开始介绍模型，如果只关注实现可以跳过到训练部分。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>分布式表示（Distributed Representation）是Hinton 在1986年提出的，基本思想是将每个词表达成 n 维稠密、连续的实数向量，与之相对的one-hot encoding向量空间只有一个维度是1，其余都是0。分布式表示最大的优点是具备非常powerful的特征表达能力，比如 n 维向量每维 k 个值，可以表征 $k^n$个概念。事实上，不管是神经网络的隐层，还是多个潜在变量的概率主题模型，都是应用分布式表示。下图是03年Bengio在 <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">A Neural Probabilistic Language Model</a> 的网络结构：</p><p><img src="https://pic3.zhimg.com/50/v2-dc007baa415cf1674df6d323419cc2de_hd.jpg" alt="img"></p><p>这篇文章提出的神经网络语言模型（NNLM，Neural Probabilistic Language Model）采用的是文本分布式表示，即每个词表示为稠密的实数向量。NNLM模型的目标是构建语言模型：</p><p><img src="https://pic1.zhimg.com/50/v2-855f785d33895960712509982199c4b4_hd.jpg" alt="img"></p><p>词的分布式表示即词向量（word embedding）是训练语言模型的一个附加产物，即图中的Matrix C。</p><p>尽管Hinton 86年就提出了词的分布式表示，Bengio 03年便提出了NNLM，词向量真正火起来是google Mikolov 13年发表的两篇word2vec的文章 <a href="http://ttic.uchicago.edu/%7Ehaotang/speech/1301.3781.pdf" target="_blank" rel="external">Efficient Estimation of Word Representations in Vector Space</a>和<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="external">Distributed Representations of Words and Phrases and their Compositionality</a>，更重要的是发布了简单好用的<strong>word2vec工具包</strong>，在语义维度上得到了很好的验证，极大的推进了文本分析的进程。下图是文中提出的CBOW 和 Skip-Gram两个模型的结构，基本类似于NNLM，不同的是模型去掉了非线性隐层，预测目标不同，CBOW是上下文词预测当前词，Skip-Gram则相反。</p><p><img src="https://pic2.zhimg.com/50/v2-04bfc01157c1c3ae1480299947315251_hd.jpg" alt="img"></p><p>除此之外，提出了Hierarchical Softmax 和 Negative Sample两个方法，很好的解决了计算有效性，事实上这两个方法都没有严格的理论证明，有些trick之处，非常的实用主义。详细的过程不再阐述了，有兴趣深入理解word2vec的，推荐读读这篇很不错的paper: <a href="http://www-personal.umich.edu/%7Eronxin/pdf/w2vexp.pdf" target="_blank" rel="external">word2vec Parameter Learning Explained</a>。额外多提一点，实际上word2vec学习的向量和真正语义还有差距，更多学到的是具备相似上下文的词，比如“good” “bad”相似度也很高，反而是文本分类任务输入有监督的语义能够学到更好的语义表示。</p><p>至此，文本的表示通过词向量的表示方式，把文本数据从高纬度高稀疏的神经网络难处理的方式，变成了类似图像、语音的的连续稠密数据。深度学习算法本身有很强的数据迁移性，很多之前在图像领域很适用的深度学习算法比如CNN等也可以很好的迁移到文本领域了，</p><h3 id="深度学习文本分类模型"><a href="#深度学习文本分类模型" class="headerlink" title="深度学习文本分类模型"></a>深度学习文本分类模型</h3><h4 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h4><p>本篇文章的题图选用的就是14年这篇文章提出的TextCNN的结构（见下图）。卷积神经网络<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" target="_blank" rel="external">CNN Convolutional Neural Network</a>最初在图像领域取得了巨大成功，CNN原理就不讲了，核心点在于可以<strong>捕捉局部相关性</strong>，具体到文本分类任务中可以利用CNN来提取句子中类似 n-gram 的关键信息。</p><p><img src="https://pic1.zhimg.com/50/v2-ab904178abf9241329e3e2d0fa7c0584_hd.jpg" alt="img"></p><p>TextCNN的详细过程原理图见下：</p><p><img src="https://pic3.zhimg.com/50/v2-bb10ad5bbdc5294d3041662f887e60a6_hd.jpg" alt="img"></p><p><strong>TextCNN详细过程</strong>：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。</p><p><strong>特征</strong>：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。</p><p><strong>通道（Channels）</strong>：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。</p><p><strong>一维卷积（conv-1d）</strong>：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。</p><p><strong>Pooling层</strong>：利用CNN解决文本分类问题的文章还是很多的，比如这篇 <a href="https://arxiv.org/pdf/1404.2188.pdf" target="_blank" rel="external">A Convolutional Neural Network for Modelling Sentences</a> 最有意思的输入是在 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">“ 我觉得这个地方景色还不错，但是人也实在太多了 ”</div></pre></td></tr></table></figure><p>虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。</p><h4 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h4><p>尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。</p><p>RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，<a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf" target="_blank" rel="external">Recurrent Neural Network for Text Classification with Multi-Task Learning</a>文中介绍了RNN用于分类问题的设计，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。</p><p><img src="https://pic3.zhimg.com/50/v2-92e49aef6626add56e85c2ee1b36e9aa_hd.jpg" alt="img"></p><h4 id="TextRCNN-TextCNN-TextRNN"><a href="#TextRCNN-TextCNN-TextRNN" class="headerlink" title="TextRCNN (TextCNN + TextRNN)"></a>TextRCNN (TextCNN + TextRNN)</h4><p>我们参考的是中科院15年发表在AAAI上的这篇文章 Recurrent Convolutional Neural Networks for Text Classification 的结构：</p><p><img src="https://pic3.zhimg.com/50/v2-263209ce34c0941fece21de00065aa92_hd.jpg" alt="img"></p><p>利用前向和后向RNN得到每个词的前向和后向上下文的表示：</p><p><img src="https://pic1.zhimg.com/50/v2-d97b136cbb9cd98354521a827e0fd8b4_hd.jpg" alt="img"></p><p>这样词的表示就变成词向量和前向后向上下文向量concat起来的形式了，即：</p><p><img src="https://pic4.zhimg.com/50/v2-16378ac29633452e7093288fd98d3f73_hd.jpg" alt="img"></p><p>最后再接跟TextCNN相同卷积层，pooling层即可，唯一不同的是卷积层 filter_size = 1就可以了，不再需要更大 filter_size 获得更大视野，这里词的表示也可以只用双向RNN输出。</p><h4 id="HAN-TextRNN-Attention"><a href="#HAN-TextRNN-Attention" class="headerlink" title="HAN (TextRNN + Attention)"></a>HAN (TextRNN + Attention)</h4><p>CNN和RNN用在文本分类任务中尽管效果显著，但都有一个不足的地方就是不够直观，可解释性不好，特别是在分析badcase时候感受尤其深刻。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够很直观的给出每个词对结果的贡献，基本成了Seq2Seq模型的标配了。</p><p><strong>Attention机制介绍</strong>：</p><p>详细介绍Attention恐怕需要一小篇文章的篇幅，感兴趣的可参考14年这篇paper <a href="https://arxiv.org/pdf/1409.0473v7.pdf" target="_blank" rel="external">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>。</p><p>以机器翻译为例简单介绍下，下图中$x_t$是源语言的一个词，$y_t$是目标语言的一个词，机器翻译的任务就是给定源序列得到目标序列。翻译$y_t$的过程产生取决于上一个词 $y_{t-1}$ 和源语言的词的表示 $h_{j}$($x_{j}$) 的 bi-RNN 模型的表示），而每个词所占的权重是不一样的。比如源语言是中文 “我 / 是 / 中国人” 目标语言 “i / am / Chinese”，翻译出“Chinese”时候显然取决于“中国人”，而与“我 / 是”基本无关。下图公式, $\alpha _{ij}$则是翻译英文第$i$个词时，中文第$j$个词的贡献，也就是注意力。显然在翻译“Chinese”时，“中国人”的注意力值非常大。</p><p><img src="https://pic3.zhimg.com/50/v2-de9146388978dfe7ef467993b9cf12ae_hd.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/50/v2-0ebc7b64a7d34a908b8d82d87c92f6b8_hd.jpg" alt="img"></p><p>Attention的核心point是在翻译每个目标词（或 预测商品标题文本所属类别）所用的上下文是不同的，这样的考虑显然是更合理的。</p><p><strong>TextRNN + Attention 模型</strong>：</p><p>我们参考了这篇文章 <a href="https://www.cs.cmu.edu/%7Ediyiy/docs/naacl16.pdf" target="_blank" rel="external">Hierarchical Attention Networks for Document Classification</a>，下图是模型的网络结构图，它一方面用层次化的结构保留了文档的结构，另一方面在word-level和sentence-level。标题场景只需要 word-level 这一层的 Attention 即可。</p><p><img src="https://pic3.zhimg.com/50/v2-4ff2c8099ccf0b2d8eb963a0ac248296_hd.jpg" alt="img"></p><p>加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。</p><h2 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h2><p>现在来详细讲解训练过程，涉及到的文件<code>train_cnn.py</code>, <code>utils.py</code>, <code>textcnn.py</code></p><p>注意到<code>train_cnn.py</code>文件最后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span>  __name__ == <span class="string">'__main__'</span>:</div><div class="line">    os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">'1'</span></div><div class="line">    tf.app.run()</div></pre></td></tr></table></figure><p>其中第一行是指定只用一个GPU。第二行是tensorflow的一个运行框架，<code>run</code>会运行文件内的<code>main</code>方法，并且传入文件最开始设定的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># configuration</span></div><div class="line">FLAGS = tf.app.flags.FLAGS</div><div class="line"></div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">"num_classes"</span>, <span class="number">33</span>, <span class="string">"number of label"</span>)</div><div class="line">tf.app.flags.DEFINE_float(<span class="string">"learning_rate"</span>, <span class="number">0.01</span>, <span class="string">"learning rate"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"batch_size"</span>, <span class="number">64</span>, <span class="string">"Batch size for training/evaluating."</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"decay_steps"</span>, <span class="number">1000</span>, <span class="string">"how many steps before decay learning rate."</span>)</div><div class="line">tf.app.flags.DEFINE_float(</div><div class="line">  <span class="string">"decay_rate"</span>, <span class="number">0.95</span>, <span class="string">"Rate of decay for learning rate."</span>)</div><div class="line">tf.app.flags.DEFINE_string(</div><div class="line">  <span class="string">"ckpt_dir"</span>, <span class="string">"text_cnn_title_desc_checkpoint/"</span>, <span class="string">"checkpoint location for the model"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"sentence_len"</span>, <span class="number">15</span>, <span class="string">"max sentence length"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(<span class="string">"embed_size"</span>, <span class="number">64</span>, <span class="string">"embedding size"</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"is_training"</span>, <span class="keyword">True</span>, <span class="string">"is traning.true:tranining,false:testing/inference"</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"num_epochs"</span>, <span class="number">30</span>, <span class="string">"number of epochs to run."</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"validate_every"</span>, <span class="number">1</span>, <span class="string">"Validate every validate_every epochs."</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"use_embedding"</span>, <span class="keyword">True</span>, <span class="string">"whether to use embedding or not."</span>)</div><div class="line">tf.app.flags.DEFINE_integer(</div><div class="line">  <span class="string">"num_filters"</span>, <span class="number">256</span>, <span class="string">"number of filters"</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"multi_label_flag"</span>, <span class="keyword">False</span>, <span class="string">"use multi label or single label."</span>)</div><div class="line">tf.app.flags.DEFINE_boolean(</div><div class="line">  <span class="string">"just_train"</span>, <span class="keyword">False</span>, <span class="string">"whether use all data to train or not."</span>)</div></pre></td></tr></table></figure><p>第一个参数代表参数名（调用这个参数的方法：<code>FLAGS.name</code>），第二个参数是默认值，第三个参数是描述。值得说明的是这里有一个<code>just_train</code>参数，它代表是否将测试集放入训练集一起训练，一般在用模型最终确定之后。</p><p>所以运行<code>python train_cnn.py</code>就是启动训练过程，同时可以传入参数，方法为<code>python train_cnn.py --name value</code>, 这里的name就是文件定义的参数名，value就是你要设定的值。如果不传入参数，则参数为默认值。</p><p>下面我们来看一下<code>main</code>函数，流程如下：</p><p>​ <img src="http://o7ie0tcjk.bkt.clouddn.com/github-video-title-classify/5.png" alt="5"></p><h3 id="数据加载-1"><a href="#数据加载-1" class="headerlink" title="数据加载"></a>数据加载</h3><p>这个过程主要是调用<code>train_test_loader</code>方法切分训练集与测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">X_train, X_val, y_train, y_val, n_classes = </div><div class="line">	train_test_loader(FLAGS.just_train)</div></pre></td></tr></table></figure><h3 id="词典加载"><a href="#词典加载" class="headerlink" title="词典加载"></a>词典加载</h3><p>加载数据预处理过程中建立的词典。目的是用来从预训练的词向量词典中拿出对应的词向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> open(<span class="string">'data/vocab.dic'</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</div><div class="line">    vocab = pickle.load(f)</div><div class="line">vocab_size = len(vocab) + <span class="number">1</span></div><div class="line">print(<span class="string">'size of vocabulary: &#123;&#125;'</span>.format(vocab_size))</div></pre></td></tr></table></figure><p>这里将词典的长度加一是为了给一个特殊词“空”加入位置，“空”的作用是填充短标题，让所有标题长度一样。</p><h3 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h3><p>这个阶段就是将所有标题长度变成一致，短了就填充，长了就截断。标题长度是一个参数，可以设置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># padding sentences</span></div><div class="line">    X_train = pad_sequences(X_train, maxlen=FLAGS.sentence_len, </div><div class="line">    	value=float(vocab_size - <span class="number">1</span>))</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.just_train:</div><div class="line">        X_val = pad_sequences(</div><div class="line">         	X_val, maxlen=FLAGS.sentence_len, value=float(vocab_size - <span class="number">1</span>))</div></pre></td></tr></table></figure><h3 id="模型实例化"><a href="#模型实例化" class="headerlink" title="模型实例化"></a>模型实例化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">textcnn = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes,</div><div class="line">                FLAGS.learning_rate, FLAGS.batch_size,</div><div class="line">                FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sentence_len,</div><div class="line">                vocab_size, FLAGS.embed_size, FLAGS.is_training, </div><div class="line">                multi_label_flag=<span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>如果有之前训练到一半的模型，那我们就加载那个模型的参数，继续训练，否则进行参数初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Initialize save</span></div><div class="line">        saver = tf.train.Saver()</div><div class="line">        <span class="keyword">if</span> os.path.exists(FLAGS.ckpt_dir + <span class="string">'checkpoint'</span>):</div><div class="line">            print(<span class="string">'restoring variables from checkpoint'</span>)</div><div class="line">            saver.restore(</div><div class="line">            	sess, </div><div class="line">            	tf.train.latest_checkpoint(FLAGS.ckpt_dir))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            print(<span class="string">'Initializing Variables'</span>)</div><div class="line">            sess.run(tf.global_variables_initializer())</div><div class="line">            <span class="keyword">if</span> FLAGS.use_embedding:</div><div class="line">                assign_pretrained_word_embedding(</div><div class="line">                	sess, vocab, vocab_size, textcnn)</div></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>模型训练过程中包括两个循环，第一个是大循环，表示遍历所有训练数据多少遍。第二个是mini-batch循环，小循环走过一遍代表遍历了所有训练数据一遍。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(curr_epoch, total_epochs):</div><div class="line">            loss, acc, counter = <span class="number">.0</span>, <span class="number">.0</span>, <span class="number">0</span></div><div class="line">            <span class="keyword">for</span> start, end <span class="keyword">in</span> zip(</div><div class="line">                    range(<span class="number">0</span>, number_of_training_data, batch_size),</div><div class="line">                    range(batch_size, number_of_training_data, </div><div class="line">                    batch_size)):</div></pre></td></tr></table></figure><p>下面就是将训练数据喂到模型中:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">feed_dict = &#123;textcnn.input_x: X_train[start:end], 		</div><div class="line">			textcnn.dropout_keep_prob: <span class="number">0.5</span>&#125;</div></pre></td></tr></table></figure><p>第二个参数是模型相关的dropout参数，用于减少过拟合，范围是(0, 1]，基本不用改变。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">curr_loss, curr_acc, _ = sess.run(</div><div class="line">                        [textcnn.loss_val, textcnn.accuracy, </div><div class="line">                        textcnn.train_op], feed_dict)</div></pre></td></tr></table></figure><p>这一步就是得到这一小部分训练数据对应的准确率以及loss。</p><p>然后每经过<code>validate_every</code>个大循环的训练，在测试集上看看模型性能。如果性能比上一次更好，就保存模型，否则就退出，因为算法开始发散了。</p><p>模型训练完毕检查性能之后，如果模型可行，下一步就将所有数据用于训练，也即运行以下命令<code>python train_cnn.py --just_train True</code>。这个过程会迭代固定的20个大循环。训练完毕之后，下面的预测过程将使用这个模型。</p><h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><p>预测涉及到的文件<code>predict_cnn.py</code>以及<code>utils.py</code></p><p>预测的流程和训练差不多，只不过不再进行多次对数据集的遍历，只进行对未标记数据进行一次遍历，拿到结果之后，由于算法输出的结果是[0, 32]这样一个序号，我们需要转化为中文标签。</p><p>具体参照代码，不再赘述。</p><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>【1】<a href="https://zhuanlan.zhihu.com/p/25928551" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25928551</a></p><p>【2】<a href="https://github.com/brightmart/text_classification" target="_blank" rel="external">https://github.com/brightmart/text_classification</a></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/29/BFG-Repo-Cleaner/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/11/29/BFG-Repo-Cleaner/" itemprop="url">BFG Repo-Cleaner</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-11-29T19:31:19+08:00">2017-11-29 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/11/29/BFG-Repo-Cleaner/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/11/29/BFG-Repo-Cleaner/" itemprop="commentsCount"></span> </a></span><span id="/2017/11/29/BFG-Repo-Cleaner/" class="leancloud_visitors" data-flag-title="BFG Repo-Cleaner"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="An-alternative-to-git-filter-branch"><a href="#An-alternative-to-git-filter-branch" class="headerlink" title="An alternative to git-filter-branch"></a>An alternative to git-filter-branch</h1><p>The BFG is a simpler, faster alternative to <a href="http://git-scm.com/docs/git-filter-branch" target="_blank" rel="external"><code>git-filter-branch</code></a> for cleansing bad data out of your Git repository history:</p><ul><li>Removing <strong>Crazy Big Files</strong></li><li>Removing <strong>Passwords</strong>, <strong>Credentials</strong> &amp; other <strong>Private data</strong></li></ul><p>The <code>git-filter-branch</code> command is enormously powerful and can do things that the BFG can’t - but the BFG is <em>much</em> better for the tasks above, because:</p><ul><li><a href="https://rtyley.github.io/bfg-repo-cleaner/#speed" target="_blank" rel="external">Faster</a> : <strong>10 - 720x</strong> faster</li><li><a href="https://rtyley.github.io/bfg-repo-cleaner/#examples" target="_blank" rel="external">Simpler</a> : The BFG isn’t particularily clever, but <em>is</em> focused on making the above tasks easy</li><li>Beautiful : If you need to, you can use the beautiful Scala language to customise the BFG. Which has got to be better than Bash scripting at least some of the time.</li></ul><h1 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h1><p>First clone a fresh copy of your repo, using the <a href="http://stackoverflow.com/q/3959924/438886" target="_blank" rel="external"><code>--mirror</code></a> flag:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git clone --mirror git://example.com/some-big-repo.git</div></pre></td></tr></table></figure><p>This is a <a href="http://git-scm.com/docs/gitglossary.html#def_bare_repository" target="_blank" rel="external">bare</a> repo, which means your normal files won’t be visible, but it is a <em>full</em> copy of the Git database of your repository, and at this point you should <strong>make a backup of it</strong> to ensure you don’t lose anything.</p><p>Now you can run the BFG to clean your repository up:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -jar bfg.jar --strip-blobs-bigger-than 100M some-big-repo.git</div></pre></td></tr></table></figure><p>The BFG will update your commits and all branches and tags so they are clean, but it doesn’t physically delete the unwanted stuff. Examine the repo to make sure your history has been updated, and then use the standard <a href="http://git-scm.com/docs/git-gc" target="_blank" rel="external"><code>git gc</code></a> command to strip out the unwanted dirty data, which Git will now recognise as surplus to requirements:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ cd some-big-repo.git</div><div class="line">$ git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive</div></pre></td></tr></table></figure><p>Finally, once you’re happy with the updated state of your repo, push it back up <em>(note that because your clone command used the –mirror flag, this push will update *<em>all*</em> refs on your remote server)</em>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git push</div></pre></td></tr></table></figure><p>At this point, you’re ready for everyone to ditch their old copies of the repo and do fresh clones of the nice, new pristine data. It’s best to delete all old clones, as they’ll have dirty history that you <em>don’t</em> want to risk pushing back into your newly cleaned repo.</p><h1 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h1><p>In all these examples <code>bfg</code> is an alias for <code>java -jar bfg.jar</code>.</p><p>Delete all files named ‘id_rsa’ or ‘id_dsa’ :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --delete-files id_&#123;dsa,rsa&#125;  my-repo.git</div></pre></td></tr></table></figure><p>Remove all blobs bigger than 50 megabytes :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --strip-blobs-bigger-than 50M  my-repo.git</div></pre></td></tr></table></figure><p>Replace all passwords listed in a file <em>(prefix lines ‘regex:’ or ‘glob:’ if required)</em> with <code>***REMOVED***</code>wherever they occur in your repository :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --replace-text passwords.txt  my-repo.git</div></pre></td></tr></table></figure><p>Remove all folders or files named ‘.git’ - a <a href="https://github.com/git/git/blob/d29e9c89d/fsck.c#L228-L229" target="_blank" rel="external">reserved filename</a> in Git. These often <a href="http://stackoverflow.com/q/16821649/438886" target="_blank" rel="external">become a problem</a>when migrating to Git from other source-control systems like Mercurial :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --delete-folders .git --delete-files .git  --no-blob-protection  my-repo.git</div></pre></td></tr></table></figure><p>For further command-line options, you can run the BFG without any arguments, which will output <a href="https://repository.sonatype.org/service/local/artifact/maven/redirect?r=central-proxy&amp;g=com.madgag&amp;a=bfg&amp;v=LATEST&amp;e=txt" target="_blank" rel="external">text like this</a>.</p><h1 id="Your-current-files-are-sacred…"><a href="#Your-current-files-are-sacred…" class="headerlink" title="Your current files are sacred…"></a>Your <em>current</em> files are sacred…</h1><p>The BFG treats you like a reformed alcoholic: you’ve made some mistakes in the past, but now you’ve cleaned up your act. Thus the BFG assumes that your latest commit is a <em>good</em> one, with none of the dirty files you want removing from your history still in it. This assumption by the BFG protects your work, and gives you peace of mind knowing that the BFG is <em>only</em> changing your repo history, not meddling with the <em>current</em> files of your project.</p><p>By default the <code>HEAD</code> branch is protected, and while its history will be cleaned, the very latest commit (the ‘tip’) is a <strong>protected commit</strong> and its file-hierarchy won’t be changed at all.</p><p>If you want to protect the tips of several branches or tags (not just HEAD), just name them for the BFG:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bfg --strip-biggest-blobs 100 --protect-blobs-from master,maint,next repo.git</div></pre></td></tr></table></figure><p>Note:</p><ul><li>Cleaning Git repos is about <em>completely</em> eradicating bad stuff from history. If something ‘bad’ (like a 10MB file, when you’re specifying <code>--strip-blobs-bigger-than 5M</code>) is in a protected commit, it <em>won’t</em> be deleted - it’ll persist in your repository, <a href="https://github.com/rtyley/bfg-repo-cleaner/issues/53#issuecomment-50088997" target="_blank" rel="external">even if the BFG deletes if from earlier commits</a>. If you want the BFG to delete something <strong>you need to make sure your current commits are clean</strong>.</li><li>Note that although the files in those protected commits won’t be changed, when those commits follow on from earlier dirty commits, their commit ids <strong>will</strong> change, to reflect the changed history - only the SHA-1 id of the filesystem-tree will remain the same.</li></ul></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/28/Understanding-LSTM-Networks-repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/11/28/Understanding-LSTM-Networks-repost/" itemprop="url">Understanding LSTM Networks [repost]</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-11-28T15:51:36+08:00">2017-11-28 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/11/28/Understanding-LSTM-Networks-repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/11/28/Understanding-LSTM-Networks-repost/" itemprop="commentsCount"></span> </a></span><span id="/2017/11/28/Understanding-LSTM-Networks-repost/" class="leancloud_visitors" data-flag-title="Understanding LSTM Networks [repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>source post is <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">here</a>.</p><h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p>Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.</p><p>Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.</p><p>Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" alt="img"></p><p><strong>Recurrent Neural Networks have loops.</strong></p><p>In the above diagram, a chunk of neural network, AA, looks at some input xtxt and outputs a value htht. A loop allows information to be passed from one step of the network to the next.</p><p>These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" alt="An unrolled recurrent neural network."></p><p><strong>An unrolled recurrent neural network.</strong></p><p>This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.</p><p>And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a>. But they really are pretty amazing.</p><p>Essential to these successes is the use of “LSTMs,” a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore.</p><h2 id="The-Problem-of-Long-Term-Dependencies"><a href="#The-Problem-of-Long-Term-Dependencies" class="headerlink" title="The Problem of Long-Term Dependencies"></a>The Problem of Long-Term Dependencies</h2><p>One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends.</p><p>Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the <em>sky</em>,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png" alt="img"></p><p>But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent <em>French</em>.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.</p><p>Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png" alt="Neural networks struggle with long term dependencies."></p><p>In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by <a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="external">Hochreiter (1991) [German]</a> and <a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf" target="_blank" rel="external">Bengio, et al. (1994)</a>, who found some pretty fundamental reasons why it might be difficult.</p><p>Thankfully, LSTMs don’t have this problem!</p><h2 id="LSTM-Networks"><a href="#LSTM-Networks" class="headerlink" title="LSTM Networks"></a>LSTM Networks</h2><p>Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by <a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="external">Hochreiter &amp; Schmidhuber (1997)</a>, and were refined and popularized by many people in following work.<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/#fn1" target="_blank" rel="external">1</a> They work tremendously well on a large variety of problems, and are now widely used.</p><p>LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</p><p>All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.</p><p><strong>The repeating module in a standard RNN contains a single layer.</strong></p><p>LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.</p><p><strong>The repeating module in an LSTM contains four interacting layers.</strong></p><p>Don’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png" alt="img"></p><p>In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.</p><h2 id="The-Core-Idea-Behind-LSTMs"><a href="#The-Core-Idea-Behind-LSTMs" class="headerlink" title="The Core Idea Behind LSTMs"></a>The Core Idea Behind LSTMs</h2><p>The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.</p><p>The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" alt="img"></p><p>The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.</p><p>Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png" alt="img"></p><p>The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!”</p><p>An LSTM has three of these gates, to protect and control the cell state.</p><h2 id="Step-by-Step-LSTM-Walk-Through"><a href="#Step-by-Step-LSTM-Walk-Through" class="headerlink" title="Step-by-Step LSTM Walk Through"></a>Step-by-Step LSTM Walk Through</h2><p>The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1ht−1and xtxt, and outputs a number between 00 and 11 for each number in the cell state Ct−1Ct−1. A 11represents “completely keep this” while a 00 represents “completely get rid of this.”</p><p>Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" alt="img"></p><p>The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, C~tC~t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.</p><p>In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" alt="img"></p><p>It’s now time to update the old cell state, Ct−1Ct−1, into the new cell state CtCt. The previous steps already decided what to do, we just need to actually do it.</p><p>We multiply the old state by ftft, forgetting the things we decided to forget earlier. Then we add it∗C~tit∗C~t. This is the new candidate values, scaled by how much we decided to update each state value.</p><p>In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" alt="img"></p><p>Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanhtanh (to push the values to be between −1−1 and 11) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.</p><p>For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" alt="img"></p><h2 id="Variants-on-Long-Short-Term-Memory"><a href="#Variants-on-Long-Short-Term-Memory" class="headerlink" title="Variants on Long Short Term Memory"></a>Variants on Long Short Term Memory</h2><p>What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them.</p><p>One popular LSTM variant, introduced by <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="external">Gers &amp; Schmidhuber (2000)</a>, is adding “peephole connections.” This means that we let the gate layers look at the cell state.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png" alt="img"></p><p>The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.</p><p>Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png" alt="img"></p><p>A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by <a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">Cho, et al. (2014)</a>. It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.</p><p><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png" alt="A gated recurrent unit neural network."></p><p>These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by <a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="external">Yao, et al. (2015)</a>. There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by <a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="external">Koutnik, et al. (2014)</a>.</p><p>Which of these variants is best? Do the differences matter? <a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">Greff, et al. (2015)</a> do a nice comparison of popular variants, finding that they’re all about the same. <a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="external">Jozefowicz, et al. (2015)</a>tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!</p><p>Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.</p><p>LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, <a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="external">Xu, <em>et al.</em>(2015)</a> do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner…</p><p>Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by <a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="external">Kalchbrenner, <em>et al.</em> (2015)</a> seem extremely promising. Work using RNNs in generative models – such as <a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="external">Gregor, <em>et al.</em> (2015)</a>, <a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="external">Chung, <em>et al.</em> (2015)</a>, or <a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="external">Bayer &amp; Osendorfer (2015)</a> – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/21/A-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/11/21/A-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn/" itemprop="url">A Simple Multi-Class Classification Task: Keras and Scikit-Learn</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-11-21T16:59:17+08:00">2017-11-21 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/11/21/A-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/11/21/A-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn/" itemprop="commentsCount"></span> </a></span><span id="/2017/11/21/A-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn/" class="leancloud_visitors" data-flag-title="A Simple Multi-Class Classification Task: Keras and Scikit-Learn"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-Problem-Description"><a href="#1-Problem-Description" class="headerlink" title="1. Problem Description"></a>1. Problem Description</h2><p>In this tutorial, we will use the standard machine learning problem called the <a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="external">iris flowers dataset</a>.</p><p>This dataset is well studied and is a good problem for practicing on neural networks because all of the 4 input variables are numeric and have the same scale in centimeters. Each instance describes the properties of an observed flower measurements and the output variable is specific iris species.</p><p>This is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species. This is an important type of problem on which to practice with neural networks because the three class values require specialized handling.</p><p>The iris flower dataset is a well-studied problem and a such we can <a href="http://www.is.umk.pl/projects/rules.html#Iris" target="_blank" rel="external">expect to achieve a model accuracy</a> in the range of 95% to 97%. This provides a good target to aim for when developing our models.</p><p>You can <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" target="_blank" rel="external">download the iris flowers dataset</a> from the UCI Machine Learning repository and place it in your current working directory with the filename “<em>iris.csv</em>“.</p><p>Need help with Deep Learning in Python?Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with sample code).Click to sign-up now and also get a free PDF Ebook version of the course.<a href="https://machinelearningmastery.leadpages.co/leadbox/142d6e873f72a2%3A164f8be4f346dc/5657382461898752/" target="_blank" rel="external">Start Your FREE Mini-Course Now!</a></p><h2 id="2-Import-Classes-and-Functions"><a href="#2-Import-Classes-and-Functions" class="headerlink" title="2. Import Classes and Functions"></a>2. Import Classes and Functions</h2><p>We can begin by importing all of the classes and functions we will need in this tutorial.</p><p>This includes both the functionality we require from Keras, but also data loading from <a href="http://pandas.pydata.org/" target="_blank" rel="external">pandas</a>as well as data preparation and model evaluation from <a href="http://scikit-learn.org/" target="_blank" rel="external">scikit-learn</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="keyword">import</span> pandas</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">from</span> keras.wrappers.scikit_learn <span class="keyword">import</span> KerasClassifier</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</div></pre></td></tr></table></figure><h2 id="3-Initialize-Random-Number-Generator"><a href="#3-Initialize-Random-Number-Generator" class="headerlink" title="3. Initialize Random Number Generator"></a>3. Initialize Random Number Generator</h2><p>Next, we need to initialize the random number generator to a constant value (7).</p><p>This is important to ensure that the results we achieve from this model can be achieved again precisely. It ensures that the stochastic process of training a neural network model can be reproduced.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># fix random seed for reproducibility</span></div><div class="line">seed = <span class="number">7</span></div><div class="line">numpy.random.seed(seed)</div></pre></td></tr></table></figure><h2 id="4-Load-The-Dataset"><a href="#4-Load-The-Dataset" class="headerlink" title="4. Load The Dataset"></a>4. Load The Dataset</h2><p>The dataset can be loaded directly. Because the output variable contains strings, it is easiest to load the data using pandas. We can then split the attributes (columns) into input variables (X) and output variables (Y).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># load dataset</span></div><div class="line">dataframe = pandas.read_csv(<span class="string">"iris.csv"</span>, header=<span class="keyword">None</span>)</div><div class="line">dataset = dataframe.values</div><div class="line">X = dataset[:,<span class="number">0</span>:<span class="number">4</span>].astype(float)</div><div class="line">Y = dataset[:,<span class="number">4</span>]</div></pre></td></tr></table></figure><h2 id="5-Encode-The-Output-Variable"><a href="#5-Encode-The-Output-Variable" class="headerlink" title="5. Encode The Output Variable"></a>5. Encode The Output Variable</h2><p>The output variable contains three different string values.</p><p>When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not.</p><p>This is called <a href="https://en.wikipedia.org/wiki/One-hot" target="_blank" rel="external">one hot encoding</a> or creating dummy variables from a categorical variable.</p><p>For example, in this problem three class values are Iris-setosa, Iris-versicolor and Iris-virginica. If we had the observations:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Iris-setosa</div><div class="line">Iris-versicolor</div><div class="line">Iris-virginica</div></pre></td></tr></table></figure><p>We can turn this into a one-hot encoded binary matrix for each data instance that would look as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Iris-setosa,	Iris-versicolor,	Iris-virginica</div><div class="line"><span class="number">1</span>,		<span class="number">0</span>,			<span class="number">0</span></div><div class="line"><span class="number">0</span>,		<span class="number">1</span>, 			<span class="number">0</span></div><div class="line"><span class="number">0</span>, 		<span class="number">0</span>, 			<span class="number">1</span></div></pre></td></tr></table></figure><p>We can do this by first encoding the strings consistently to integers using the scikit-learn class LabelEncoder. Then convert the vector of integers to a one hot encoding using the Keras function to_categorical().</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># encode class values as integers</span></div><div class="line">encoder = LabelEncoder()</div><div class="line">encoder.fit(Y)</div><div class="line">encoded_Y = encoder.transform(Y)</div><div class="line"><span class="comment"># convert integers to dummy variables (i.e. one hot encoded)</span></div><div class="line">dummy_y = np_utils.to_categorical(encoded_Y)</div></pre></td></tr></table></figure><h2 id="6-Define-The-Neural-Network-Model"><a href="#6-Define-The-Neural-Network-Model" class="headerlink" title="6. Define The Neural Network Model"></a>6. Define The Neural Network Model</h2><p>The Keras library provides wrapper classes to allow you to use neural network models developed with Keras in scikit-learn.</p><p>There is a KerasClassifier class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.</p><p>Below is a function that will create a baseline neural network for the iris classification problem. It creates a simple fully connected network with one hidden layer that contains 8 neurons.</p><p>The hidden layer uses a rectifier activation function which is a good practice. Because we used a one-hot encoding for our iris dataset, the output layer must create 3 output values, one for each class. The output value with the largest value will be taken as the class predicted by the model.</p><p>The network topology of this simple one-layer neural network can be summarized as:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">4 inputs -&gt; [8 hidden nodes] -&gt; 3 outputs</div></pre></td></tr></table></figure><p>Note that we use a “<em>softmax</em>” activation function in the output layer. This is to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities.</p><p>Finally, the network uses the efficient Adam gradient descent optimization algorithm with a logarithmic loss function, which is called “<em>categorical_crossentropy</em>” in Keras.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># define baseline model</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">baseline_model</span><span class="params">()</span>:</span></div><div class="line">	<span class="comment"># create model</span></div><div class="line">	model = Sequential()</div><div class="line">	model.add(Dense(<span class="number">8</span>, input_dim=<span class="number">4</span>, activation=<span class="string">'relu'</span>))</div><div class="line">	model.add(Dense(<span class="number">3</span>, activation=<span class="string">'softmax'</span>))</div><div class="line">	<span class="comment"># Compile model</span></div><div class="line">	model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</div><div class="line">	<span class="keyword">return</span> model</div></pre></td></tr></table></figure><p>We can now create our KerasClassifier for use in scikit-learn.</p><p>We can also pass arguments in the construction of the KerasClassifier class that will be passed on to the fit() function internally used to train the neural network. Here, we pass the number of epochs as 200 and batch size as 5 to use when training the model. Debugging is also turned off when training by setting verbose to 0.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">estimator = KerasClassifier(</div><div class="line">  build_fn=baseline_model, epochs=<span class="number">200</span>, batch_size=<span class="number">5</span>, verbose=<span class="number">0</span>)</div></pre></td></tr></table></figure><h2 id="7-Evaluate-The-Model-with-k-Fold-Cross-Validation"><a href="#7-Evaluate-The-Model-with-k-Fold-Cross-Validation" class="headerlink" title="7. Evaluate The Model with k-Fold Cross Validation"></a>7. Evaluate The Model with k-Fold Cross Validation</h2><p>We can now evaluate the neural network model on our training data.</p><p>The scikit-learn has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating machine learning models is k-fold cross validation.</p><p>First we can define the model evaluation procedure. Here, we set the number of folds to be 10 (an excellent default) and to shuffle the data before partitioning it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kfold = KFold(n_splits=<span class="number">10</span>, shuffle=<span class="keyword">True</span>, random_state=seed)</div></pre></td></tr></table></figure><p>Now we can evaluate our model (estimator) on our dataset (X and dummy_y) using a 10-fold cross-validation procedure (kfold).</p><p>Evaluating the model only takes approximately 10 seconds and returns an object that describes the evaluation of the 10 constructed models for each of the splits of the dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">results = cross_val_score(estimator, X, dummy_y, cv=kfold)</div><div class="line">print(<span class="string">"Baseline: %.2f%% (%.2f%%)"</span> % (results.mean()*<span class="number">100</span>, results.std()*<span class="number">100</span>))</div></pre></td></tr></table></figure><p>The results are summarized as both the mean and standard deviation of the model accuracy on the dataset. This is a reasonable estimation of the performance of the model on unseen data. It is also within the realm of known top results for this problem.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Accuracy: <span class="number">97.33</span>% (<span class="number">4.42</span>%)</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">122</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">58</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>