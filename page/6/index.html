<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/6/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/6/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/23/Introduction-to-Genetic-Algorithm/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/23/Introduction-to-Genetic-Algorithm/" itemprop="url">Introduction to Genetic Algorithm</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-23T15:13:09+08:00">2018-01-23 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/23/Introduction-to-Genetic-Algorithm/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/23/Introduction-to-Genetic-Algorithm/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/23/Introduction-to-Genetic-Algorithm/" class="leancloud_visitors" data-flag-title="Introduction to Genetic Algorithm"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-Intuition-behind-Genetic-Algorithms"><a href="#1-Intuition-behind-Genetic-Algorithms" class="headerlink" title="1. Intuition behind Genetic Algorithms"></a>1. Intuition behind Genetic Algorithms</h2><p>Let’s start with the famous quote by Charles Darwin:</p><blockquote><p><em>It is not the strongest of the species that survives, nor the most intelligent , but the one most responsive to change.</em></p></blockquote><p>You must be thinking what has this quote got to do with genetic algorithm? Actually, the entire concept of a genetic algorithm is based on the above line.</p><p>Let us understand with a basic example:</p><p>Let’s take a hypothetical situation where, you are head of a country, and in order to keep your city safe from bad things, you implement a policy like this.</p><ul><li>You select all the good people, and ask them to extend their generation by having their children.</li><li>This repeats for a few generations.</li><li>You will notice that now you have an entire population of good people.</li></ul><p>Now, that may not be entirely possible, but this example was just to help you understand the concept. So the basic idea was that we changed the input (i.e. population) such that we get better output (i.e. better country).</p><p>Now, I suppose you have got some intuition that the concept of a genetic algorithm is somewhat related to biology. So let’s us quickly grasp some little concepts, so that we can draw a parallel line between them.</p><h2 id="2-Biological-Inspiration"><a href="#2-Biological-Inspiration" class="headerlink" title="2. Biological Inspiration"></a>2. Biological Inspiration</h2><p>I am sure you would remember:</p><p><em>Cells are the basic building block of all living things.</em></p><p>Therefore in each cell, there is the same set of chromosomes. Chromosome are basically the strings of DNA.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153917/dna-300x194.png" alt="img"></p><p>Traditionally, these chromosomes are represented in binary as strings of 0’s and 1’s.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153928/gene.png" alt="img"></p><p>Source : <a href="https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_fundamentals.htm" target="_blank" rel="external">link</a></p><p>A chromosome consists of genes, commonly referred as blocks of DNA, where each gene encodes a specific trait, for example hair color or eye color.</p><p>I wanted you to recall these basics concept of biology before going further. Let’s get back and understand what actually is a genetic algorithm?</p><h2 id="3-What-is-a-Genetic-Algorithm"><a href="#3-What-is-a-Genetic-Algorithm" class="headerlink" title="3. What is a Genetic Algorithm?"></a>3. What is a Genetic Algorithm?</h2><p>Let’s get back to the example we discussed above and summarize what we did.</p><ol><li>Firstly, we defined our initial population as our countrymen.</li><li>We defined a function to classify whether is a person is good or bad.</li><li>Then we selected good people for mating to produce their off-springs.</li><li>And finally, these off-springs replace the bad people from the population and this process repeats.</li></ol><p>This is how genetic algorithm actually works, which basically tries to mimic the human evolution to some extent.</p><p>So to formalize a definition of a genetic algorithm, we can say that it is an optimization technique, which tries to find out such values of input so that we get the best output values or results.</p><p>The working of a genetic algorithm is also derived from biology, which is as shown in the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154007/steps-210x300.png" alt="img"></p><p>Source: <a href="https://www.neuraldesigner.com/blog/genetic_algorithms_for_feature_selection" target="_blank" rel="external">link</a></p><p>So, let us try to understand the steps one by one.</p><h2 id="4-Steps-Involved-in-Genetic-Algorithm"><a href="#4-Steps-Involved-in-Genetic-Algorithm" class="headerlink" title="4. Steps Involved in Genetic Algorithm"></a><strong>4. Steps Involved in Genetic Algorithm</strong></h2><p>Here, to make things easier, let us understand it by the famous <a href="https://en.wikipedia.org/wiki/Knapsack_problem" target="_blank" rel="external">Knapsack problem</a>.</p><p>If you haven’t come across this problem, let me introduce my version of this problem.</p><p>Let’s say, you are going to spend a month in the wilderness. Only thing you are carrying is the backpack which can hold a maximum weight of <strong>30 kg</strong>. Now you have different survival items, each having its own “Survival Points” (which are given for each item in the table). So, your objective is maximise the survival points.</p><p>Here is the table giving details about each item.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154400/table1-300x99.png" alt="img"></p><h3 id="4-1-Initialisation"><a href="#4-1-Initialisation" class="headerlink" title="4.1 Initialisation"></a>4.1 Initialisation</h3><p>To solve this problem using genetic algorithm, our first step would be defining our population. So our population will contain individuals, each having their own set of chromosomes.</p><p>We know that, chromosomes are binary strings, where for this problem 1 would mean that the following item is taken and 0 meaning that it is dropped.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22170200/Capture1-300x186.png" alt="img"></p><p>This set of chromosome is considered as our initial population.</p><h3 id="4-2-Fitness-Function"><a href="#4-2-Fitness-Function" class="headerlink" title="4.2 Fitness Function"></a>4.2 Fitness Function</h3><p>Let us calculate fitness points for our first two chromosomes.</p><p>For A1 chromosome [100110],</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/24105541/table-300x74.png" alt="img"></p><p>Similarly for A2 chromosome [001110],</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22154417/table3-300x74.png" alt="img"></p><p>So, for this problem, our chromosome will be considered as more fit when it contains more survival points.</p><p>Therefore chromosome 1 is more fit than chromosome 2.</p><h3 id="4-3-Selection"><a href="#4-3-Selection" class="headerlink" title="4.3 Selection"></a>4.3 Selection</h3><p>Now, we can select fit chromosomes from our population which can mate and create their off-springs.</p><p>General thought is that we should select the fit chromosomes and allow them to produce off-springs. But that would lead to chromosomes that are more close to one another in a few next generation, and therefore less diversity.</p><p>Therefore, we generally use Roulette Wheel Selection method.</p><p>Don’t be afraid of name, just take a look at the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22153953/roulette-wheel-300x188.jpg" alt="img"></p><p>I suppose we all have seen this, either in real or in movies. So, let’s build our roulette wheel.</p><p>Consider a wheel, and let’s divide that into m divisions, where m is the number of chromosomes in our populations. The area occupied by each chromosome will be proportional to its fitness value.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22155317/table4-300x72.png" alt="img"></p><p>Based on these values, let us create our roulette wheel.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22171149/roulette-300x204.png" alt="img"></p><p>So, now this wheel is rotated and the region of wheel which comes in front of the fixed point is chosen as the parent. For the second parent, the same process is repeated.</p><p>Sometimes we mark two fixed point as shown in the figure below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22171418/stio-300x194.png" alt="img"></p><p>So, in this method we can get both our parents in one go. This method is known as Stochastic Universal Selection method.</p><h3 id="4-4-Crossover"><a href="#4-4-Crossover" class="headerlink" title="4.4 Crossover"></a>4.4 Crossover</h3><p>So in this previous step, we have selected parent chromosomes that will produce off-springs. So in biological terms, crossover is nothing but reproduction.</p><p>So let us find the crossover of chromosome 1 and 4, which were selected in the previous step. Take a look at the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22173337/one-point-300x93.png" alt="img"></p><p>This is the most basic form of crossover, known as one point crossover. Here we select a random crossover point and the tails of both the chromosomes are swapped to produce a new off-springs.</p><p>If you take two crossover point, then it will called as multi point crossover which is as shown below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22174145/multi-point-300x101.png" alt="img"></p><h3 id="4-5-Mutation"><a href="#4-5-Mutation" class="headerlink" title="4.5 Mutation"></a>4.5 Mutation</h3><p>Now if you think in the biological sense, are the children produced have the same traits as their parents? The answer is NO. During their growth, there is some change in the genes of children which makes them different from its parents.</p><p>This process is known as mutation, which may be defined as a random tweak in the chromosome, which also promotes the idea of diversity in the population.</p><p>A simple method of mutation is shown in the image below.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22174928/mutation-300x56.png" alt="img"></p><p>So the entire process is summarise as shown in the figure.</p><p><img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/22175311/gadiagram-300x196.png" alt="img"></p><p>Source : <a href="http://www.jade-cheng.com/au/coalhmm/optimization/" target="_blank" rel="external">link</a></p><p>The off-springs thus produced are again validated using our fitness function, and if considered fit then will replace the less fit chromosomes from the population.</p><p>But the question is how we will get to know that we have reached our best possible solution?</p><p>So basically there are different termination conditions, which are listed below:</p><ol><li>There is no improvement in the population for over x iterations.</li><li>We have already predefined an absolute number of generation for our algorithm.</li><li>When our fitness function has reached a predefined value.</li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/" itemprop="url">Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-22T16:49:43+08:00">2018-01-22 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/22/Evolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost/" class="leancloud_visitors" data-flag-title="Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Source blog is <a href="https://blog.openai.com/evolution-strategies/" target="_blank" rel="external">here</a>.</p><hr><p>We’ve <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">discovered</a> that <strong>evolution strategies (ES)</strong>, an optimization technique that’s been known for decades, rivals the performance of standard <strong>reinforcement learning (RL)</strong>techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RL’s inconveniences.</p><p>In particular, ES is simpler to implement (there is no need for <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="external">backpropagation</a>), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer <a href="https://www.quora.com/What-are-hyperparameters-in-machine-learning" target="_blank" rel="external">hyperparameters</a>. This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on <a href="https://en.wikipedia.org/wiki/Finite_difference" target="_blank" rel="external">finite differences</a> along a few random directions at each step.</p><p>Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank" rel="external">“AlexNet” paper</a> showed how to design, scale and train convolutional neural networks (CNNs) to achieve extremely strong results on image recognition tasks, at a time when most researchers thought that CNNs were not a promising approach to computer vision. Similarly, in 2013, the <a href="https://arxiv.org/abs/1312.5602" target="_blank" rel="external">Deep Q-Learning paper</a> showed how to combine Q-Learning with CNNs to successfully solve Atari games, reinvigorating RL as a research field with exciting experimental (rather than theoretical) results. Likewise, our work demonstrates that ES achieves strong performance on RL benchmarks, dispelling the common belief that ES methods are impossible to apply to high dimensional problems.</p><p>ES is easy to implement and scale. Running on a computing cluster of 80 machines and 1,440 CPU cores, our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes about 10 hours). Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour.</p><p>In what follows, we’ll first briefly describe the conventional RL approach, contrast that with our ES approach, discuss the tradeoffs between ES and RL, and finally highlight some of our experiments.</p><h4 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h4><p>Let’s briefly look at how RL works. Suppose we are given some environment (e.g. a game) that we’d like to train an agent on. To describe the behavior of the agent, we define a policy function (the brain of the agent), which computes how the agent should act in any given situation. In practice, the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions. A typical policy function might have about 1,000,000 parameters, so our task comes down to finding the precise setting of these parameters such that the policy plays well (i.e. wins a lot of games).</p><p><img src="https://blog.openai.com/content/images/2017/03/first-graphic-1.png" alt="img"></p><p><em>Above: In the game of Pong, the policy could take the pixels of the screen and compute the probability of moving the player’s paddle (in green, on right) Up, Down, or neither.</em></p><p>The training process for the policy works as follows. Starting from a random initialization, we let the agent interact with the environment for a while and collect episodes of interaction (e.g. each episode is one game of Pong). We thus obtain a complete recording of what happened: what sequence of states we encountered, what actions we took in each state, and what the reward was at each step. As an example, below is a diagram of three episodes that each took 10 time steps in a hypothetical environment. Each rectangle is a state, and rectangles are colored green if the reward was positive (e.g. we just got the ball past our opponent) and red if the reward was negative (e.g. we missed the ball):</p><p><img src="https://blog.openai.com/content/images/2017/03/second-graphic-1.png" alt="img"></p><p>This diagram suggests a recipe for how we can improve the policy; whatever we happened to do leading up to the green states was good, and whatever we happened to do in the states leading up to the red areas was bad. We can then use backpropagation to compute a small update on the network’s parameters that would make the green actions more likely in those states in the future, and the red actions less likely in those states in the future. We expect that the updated policy works a bit better as a result. We then iterate the process: collect another batch of episodes, do another update, etc.</p><p><strong>Exploration by injecting noise in the actions.</strong> The policies we usually use in RL are stochastic, in that they only compute probabilities of taking any action. This way, during the course of training, the agent may find itself in a particular state many times, and at different times it will take different actions due to the sampling. This provides the signal needed for learning; some of those actions will lead to good outcomes, and get encouraged, and some of them will not work out, and get discouraged. We therefore say that we introduce exploration into the learning process by injecting noise into the agent’s actions, which we do by sampling from the action distribution at each time step. This will be in contrast to ES, which we describe next.</p><h4 id="Evolution-Strategies"><a href="#Evolution-Strategies" class="headerlink" title="Evolution Strategies"></a>Evolution Strategies</h4><p><strong>On “Evolution”.</strong> Before we dive into the ES approach, it is important to note that despite the word “evolution”, ES has very little to do with biological evolution. Early versions of these techniques may have been inspired by biological evolution and the approach can, on an abstract level, be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations. However, the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black-box stochastic optimization techniques.</p><p><strong>Black-box optimization.</strong> In ES, we forget entirely that there is an agent, an environment, that there are neural networks involved, or that interactions take place over time, etc. The whole setup is that 1,000,000 numbers (which happen to describe the parameters of the policy network) go in, 1 number comes out (the total reward), and we want to find the best setting of the 1,000,000 numbers. Mathematically, we would say that we are optimizing a function <code>f(w)</code> with respect to the input vector <code>w</code>(the parameters / weights of the network), but we make no assumptions about the structure of <code>f</code>, except that we can evaluate it (hence “black box”).</p><p><strong>The ES algorithm.</strong> Intuitively, the optimization is a “guess and check” process, where we start with some random parameters and then repeatedly 1) tweak the guess a bit randomly, and 2) move our guess slightly towards whatever tweaks worked better. Concretely, at each step we take a parameter vector <code>w</code> and generate a population of, say, 100 slightly different parameter vectors <code>w1 ... w100</code> by jittering <code>w</code> with gaussian noise. We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while, and add up all the rewards in each case. The updated parameter vector then becomes the weighted sum of the 100 vectors, where each weight is proportional to the total reward (i.e. we want the more successful candidates to have a higher weight). Mathematically, you’ll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences, except we only do it along 100 random directions. Yet another way to see it is that we’re still doing RL (Policy Gradients, or <a href="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" target="_blank" rel="external">REINFORCE</a> specifically), where the agent’s actions are to emit entire parameter vectors using a gaussian policy.</p><p><img src="https://blog.openai.com/content/images/2017/03/evo.png" alt="img"></p><p><em>Above: ES optimization process, in a setting with only two parameters and a reward function (red = high, blue = low). At each iteration we show the current parameter value (in white), a population of jittered samples (in black), and the estimated gradient (white arrow). We keep moving the parameters to the top of the arrow until we converge to a local optimum. You can reproduce this figure with this notebook.</em></p><p><strong>Code sample.</strong> To make the core algorithm concrete and to highlight its simplicity, here is a short example of optimizing a quadratic function using ES (or see this <a href="https://gist.github.com/karpathy/77fbb6a8dac5395f1b73e7a89300318d" target="_blank" rel="external">longer version</a> with more comments):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># simple example: minimize a quadratic around some solution point</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">solution = np.array([<span class="number">0.5</span>, <span class="number">0.1</span>, <span class="number">-0.3</span>])</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(w)</span>:</span> <span class="keyword">return</span> -np.sum((w - solution)**<span class="number">2</span>)</div><div class="line"></div><div class="line">npop = <span class="number">50</span>      <span class="comment"># population size</span></div><div class="line">sigma = <span class="number">0.1</span>    <span class="comment"># noise standard deviation</span></div><div class="line">alpha = <span class="number">0.001</span>  <span class="comment"># learning rate</span></div><div class="line">w = np.random.randn(<span class="number">3</span>) <span class="comment"># initial guess</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</div><div class="line">  N = np.random.randn(npop, <span class="number">3</span>)</div><div class="line">  R = np.zeros(npop)</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(npop):</div><div class="line">    w_try = w + sigma*N[j]</div><div class="line">    R[j] = f(w_try)</div><div class="line">  A = (R - np.mean(R)) / np.std(R)</div><div class="line">  w = w + alpha/(npop*sigma) * np.dot(N.T, A)</div></pre></td></tr></table></figure><p><strong>Injecting noise in the parameters.</strong> Notice that the objective is identical to the one that RL optimizes: the expected reward. However, RL injects noise in the action space and uses backpropagation to compute the parameter updates, while ES injects noise directly in the parameter space. Another way to describe this is that RL is a “guess and check” on actions, while ES is a “guess and check” on parameters. Since we’re injecting noise in the parameters, it is possible to use deterministic policies (and we do, in our experiments). It is also possible to add noise in both actions and parameters to potentially combine the two approaches.</p><h4 id="Tradeoffs-between-ES-and-RL"><a href="#Tradeoffs-between-ES-and-RL" class="headerlink" title="Tradeoffs between ES and RL"></a>Tradeoffs between ES and RL</h4><p>ES enjoys multiple advantages over RL algorithms (some of them are a little technical):</p><ul><li><strong>No need for backpropagation</strong>. ES only requires the forward pass of the policy and does not require backpropagation (or value function estimation), which makes the code shorter and between 2-3 times faster in practice. On memory-constrained systems, it is also not necessary to keep a record of the episodes for a later update. There is also no need to worry about exploding gradients in RNNs. Lastly, we can explore a much larger function class of policies, including networks that are not differentiable (such as in binary networks), or ones that include complex modules (e.g. pathfinding, or various optimization layers).</li><li><strong>Highly parallelizable.</strong> ES only requires workers to communicate a few scalars between each other, while in RL it is necessary to synchronize entire parameter vectors (which can be millions of numbers). Intuitively, this is because we control the random seeds on each worker, so each worker can locally reconstruct the perturbations of the other workers. Thus, all that we need to communicate between workers is the reward of each perturbation. As a result, we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization.</li><li><strong>Higher robustness.</strong> Several hyperparameters that are difficult to set in RL implementations are side-stepped in ES. For example, RL is not “scale-free”, so one can achieve very different learning outcomes (including a complete failure) with different settings of the frame-skip hyperparameter in Atari. As we show in our work, ES works about equally well with any frame-skip.</li><li><strong>Structured exploration.</strong> Some RL algorithms (especially policy gradients) initialize with random policies, which often manifests as random jitter on spot for a long time. This effect is mitigated in Q-Learning due to epsilon-greedy policies, where the max operation can cause the agents to perform some consistent action for a while (e.g. holding down a left arrow). This is more likely to do something in a game than if the agent jitters on spot, as is the case with policy gradients. Similar to Q-learning, ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration.</li><li><strong>Credit assignment over long time scales.</strong> By studying both ES and RL gradient estimators mathematically we can see that ES is an attractive choice especially when the number of time steps in an episode is long, where actions have longlasting effects, or if no good value function estimates are available.</li></ul><p>Conversely, we also found some challenges to applying ES in practice. One core problem is that in order for ES to work, adding noise in parameters must lead to different outcomes to obtain some gradient signal. As we elaborate on in our paper, we found that the use of virtual batchnorm can help alleviate this problem, but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary. As an example of a related difficulty, we found that in Montezuma’s Revenge, one is very unlikely to get the key in the first level with a random network, while this is occasionally possible with random actions.</p><h4 id="ES-is-competitive-with-RL"><a href="#ES-is-competitive-with-RL" class="headerlink" title="ES is competitive with RL"></a>ES is competitive with RL</h4><p>We compared the performance of ES and RL on two standard RL benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo task (see examples below) contains a physically-simulated articulated figure, where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward. Below are some example agents trained on three MuJoCo control tasks, where the objective is to move forward:</p><p><img src="https://blog.openai.com/content/images/2017/03/out.gif" alt="img"></p><p>We usually compare the performance of algorithms by looking at their efficiency of learning from data; as a function of how many states we’ve seen, what is our average reward? Here are the example learning curves that we obtain, in comparison to RL (the <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="external">TRPO</a> algorithm in this case):</p><p><img src="https://blog.openai.com/content/images/2017/03/es_vs_trpo_full.png" alt="img"></p><p><strong>Data efficiency comparison</strong>. The comparisons above show that ES (orange) can reach a comparable performance to TRPO (blue), although it doesn’t quite match or surpass it in all cases. Moreover, by scanning horizontally we can see that ES is less efficient, but no worse than about a factor of 10 (note the x-axis is in log scale).</p><p><strong>Wall clock comparison</strong>. Instead of looking at the raw number of states seen, one can argue that the most important metric to look at is the wall clock time: how long (in number of seconds) does it take to solve a given problem? This quantity ultimately dictates the achievable speed of iteration for a researcher. Since ES requires negligible communication between workers, we were able to solve one of the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 machines in only 10 minutes. As a comparison, in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours. It is also possible that the performance of RL could also improve with more algorithmic and engineering effort, but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements.</p><p>Below are a few videos of 3D humanoid walkers trained with ES. As we can see, the results have quite a bit of variety, based on which local minimum the optimization ends up converging into.</p><p><img src="https://blog.openai.com/content/images/2017/03/out-1.gif" alt="img"></p><p>On Atari, ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day. Below are some result snippets on Pong, Seaquest and Beamrider. These videos show the preprocessed frames, which is exactly what the agent sees when it is playing:</p><p><img src="https://blog.openai.com/content/images/2017/03/atari.gif" alt="img"></p><p>In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels.</p><h4 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h4><p>ES is an algorithm from the neuroevolution literature, which has a long history in AI and a complete literature review is beyond the scope of this post. However, we encourage an interested reader to look at <a href="https://en.wikipedia.org/wiki/Neuroevolution" target="_blank" rel="external">Wikipedia</a>, <a href="http://www.scholarpedia.org/article/Neuroevolution" target="_blank" rel="external">Scholarpedia</a>, and Jürgen Schmidhuber’s <a href="https://arxiv.org/abs/1404.7828" target="_blank" rel="external">review article (Section 6.6)</a>. The work that most closely informed our approach is <a href="http://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf" target="_blank" rel="external">Natural Evolution Strategies</a> by Wierstra et al. 2014. Compared to this work and much of the work it has inspired, our focus is specifically on scaling these algorithms to large-scale, distributed settings, finding components that make the algorithms work better with deep neural networks (e.g. <a href="https://arxiv.org/abs/1606.03498" target="_blank" rel="external">virtual batch norm</a>), and evaluating them on modern RL benchmarks.</p><p>It is also worth noting that neuroevolution-related approaches have seen some recent resurgence in the machine learning literature, for example with <a href="https://arxiv.org/abs/1609.09106" target="_blank" rel="external">HyperNetworks</a>, <a href="https://arxiv.org/abs/1703.01041" target="_blank" rel="external">“Large-Scale Evolution of Image Classifiers”</a> and <a href="https://arxiv.org/abs/1606.02580" target="_blank" rel="external">“Convolution by Evolution”</a>.</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>Our work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent-environment benchmarks, while offering significant benefits related to code complexity and ease of scaling to large-scale distributed settings. We also expect that more exciting work can be done by revisiting other ideas from this line of work, such as indirect encoding methods, or evolving the network structure in addition to the parameters.</p><p><strong>Note on supervised learning</strong>. It is also important to note that supervised learning problems (e.g. image classification, speech recognition, or most other tasks in the industry), where one can compute the exact gradient of the loss function with backpropagation, are not directly impacted by these findings. For example, in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1,000 times slower than using backpropagation. It is only in RL settings, where one has to estimate the gradient of the expected reward by sampling, where ES becomes competitive.</p><p><strong>Code release</strong>. Finally, if you’d like to try running ES yourself, we encourage you to dive into the full details by reading <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="external">our paper</a> or looking at our code on this <a href="https://github.com/openai/evolution-strategies-starter" target="_blank" rel="external">Github repo</a>.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" itemprop="url">Seq2Seq with Attention and Beam Search [Repost]</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-21T17:35:50+08:00">2018-01-21 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/21/Seq2Seq-with-Attention-and-Beam-Search-Repost/" class="leancloud_visitors" data-flag-title="Seq2Seq with Attention and Beam Search [Repost]"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Source Post is <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">here</a></p><h2 id="Sequence-to-Sequence-basics"><a href="#Sequence-to-Sequence-basics" class="headerlink" title="Sequence to Sequence basics"></a>Sequence to Sequence basics</h2><p>Let’s explain the sequence to sequence framework as we’ll rely on it for our model. Let’s start with the simplest version on the translation task.</p><blockquote><p>As an example, let’s translate <code>how are you</code> in French <code>comment vas tu</code>.</p></blockquote><h3 id="Vanilla-Seq2Seq"><a href="#Vanilla-Seq2Seq" class="headerlink" title="Vanilla Seq2Seq"></a>Vanilla Seq2Seq</h3><p>The Seq2Seq framework relies on the <strong>encoder-decoder</strong> paradigm. The <strong>encoder</strong> <em>encodes</em> the input sequence, while the <strong>decoder</strong> <em>produces</em> the target sequence</p><p><strong>Encoder</strong></p><p>Our input sequence is <code>how are you</code>. Each word from the input sequence is associated to a vector $w \in \mathbb{R}^d$ (via a lookup table). In our case, we have 3 words, thus our input will be transformed into $[w_0, w_1, w_2] \in \mathbb{R}^{d \times 3}$. Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation $e$. Let’s write the hidden states $[e_0,e_1,e_2]$ (and thus $e=e_2$)</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_encoder.svg" alt="Vanilla Encoder"></p><p><em>Vanilla Encoder</em></p><p><strong>Decoder</strong></p><p>Now that we have a vector ee that captures the meaning of the input sequence, we’ll use it to generate the target sequence word by word. Feed to another LSTM cell: ee as hidden state and a special <em>start of sentence</em> vector $w_{\text{sos}}$ as input. The LSTM computes the next hidden state $h_0 \in \mathbb{R}^{h}$. Then, we apply some function $g : \mathbb{R}^h \mapsto \mathbb{R}^V$ so that $s_0 := g(h_0) \in \mathbb{R}^V$ is a vector of the same size as the vocabulary.<br>$$<br>\begin{align}<br>h_0 &amp;= \operatorname{LSTM}\left(e, w_{sos} \right)\\<br>s_0 &amp;= g(h_0)\\<br>p_0 &amp;= \operatorname{softmax}(s_0)\\<br>i_0 &amp;= \operatorname{argmax}(p_0)\\<br>\end{align}<br>$$<br>Then, apply a softmax to $s_0$ to normalize it into a vector of probabilities $p_0 \in \mathbb{R}^V$ . Now, each entry of $p_0$ will measure how likely is each word in the vocabulary. Let’s say that the word <em>“comment”</em> has the highest probability (and thus $i_0 = \operatorname{argmax}(p_0)$) corresponds to the index of <em>“comment”</em>). Get a corresponding vector and $w_{i_0} = w_{comment}$ repeat the procedure: the LSTM will take $h_0$ as hidden state and $w_{comment}$ as input and will output a probability vector $p_1$ over the second word, etc.<br>$$<br>\begin{align}<br>h_1 &amp;= \operatorname{LSTM}\left(h_0, w_{i_0} \right)\\<br>s_1 &amp;= g(h_1)\\<br>p_1 &amp;= \operatorname{softmax}(s_1)\\<br>i_1 &amp;= \operatorname{argmax}(p_1)<br>\end{align}<br>$$<br>The decoding stops when the predicted word is a special <em>end of sentence</em> token.</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg" alt="Vanilla Decoder"></p><p><em>Vanilla Decoder</em></p><blockquote><p>Intuitively, the hidden vector represents the “amount of meaning” that has not been decoded yet.</p></blockquote><p>The above method aims at modelling the distribution of the next word conditionned on the beginning of the sentence<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_1, \dots, y_{t}, x_0, \dots, x_n \right]<br>$$<br>by writing<br>$$<br>\mathbb{P}\left[ y_{t+1} | y_t, h_{t}, e \right]<br>$$</p><h3 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h3><p>The previous model has been refined over the past few years and greatly benefited from what is known as <strong>attention</strong>. Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoder’s LSTM. One way of performing attention is explained by <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Bahdanau et al.</a>. We slightly modify the reccurrence formula that we defined above by adding a new vector $c_t$ to the input of the LSTM<br>$$<br>\begin{align}<br>h_{t} &amp;= \operatorname{LSTM}\left(h_{t-1}, [w_{i_{t-1}}, c_t] \right)\\<br>s_t &amp;= g(h_t)\\<br>p_t &amp;= \operatorname{softmax}(s_t)\\<br>i_t &amp;= \operatorname{argmax}(p_t)<br>\end{align}<br>$$<br>The vector ctct is the attention (or <strong>context</strong>) vector. We compute a new context vector at each decoding step. First, with a function $f (h_{t-1}, e_{t’}) \mapsto \alpha_{t’} \in \mathbb{R}$, compute a score for each hidden state $e_{t^{\prime}}$ of the encoder. Then, normalize the sequence of using a softmax and compute $c_t$ as the weighted average of the $e_{t^{\prime}}$.<br>$$<br>\begin{align}<br>\alpha_{t’} &amp;= f(h_{t-1}, e_{t’}) \in \mathbb{R} &amp; \text{for all } t’\\<br>\bar{\alpha} &amp;= \operatorname{softmax} (\alpha)\\<br>c_t &amp;= \sum_{t’=0}^n \bar{\alpha}_{t’} e_{t’}<br>\end{align}<br>$$<br><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg" alt="Attention Mechanism"></p><p><em>Attention Mechanism</em></p><p>The choice of the function ff varies, but is usually one of the following<br>$$<br>f(h_{t-1}, e_{t’}) =<br>\begin{cases}<br>h_{t-1}^T e_{t’} &amp; \text{dot}\\<br>h_{t-1}^T W e_{t’} &amp; \text{general}\\<br>v^T \tanh \left(W [h_{t-1}, e_{t’}]\right) &amp; \text{concat}\\<br>\end{cases}<br>$$<br>It turns out that the attention weighs $\bar{\alpha}$ can be easily interpreted. When generating the word <code>vas</code>(corresponding to <code>are</code> in English), we expect $\bar{\alpha} _ {\text{are}}$ are to be close to $1$ while $\bar{\alpha} _ {\text{how}}$ and $\bar{\alpha} _ {\text{you}}$ to be close to $0$. Intuitively, the context vector $c$ will be roughly equal to the hidden vector of <code>are</code> and it will help to generate the French word <code>vas</code>.</p><p>By putting the attention weights into a matrix (rows = input sequence, columns = output sequence), we would have access to the <strong>alignment</strong> between the words from the English and French sentences… (see <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="external">page 6</a>) There is still a lot of things to say about sequence to sequence models (for instance, it works better if the encoder processes the input sequence <em>backwards</em>…).</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><blockquote><p>What happens if the first time step is not sure about wether it should generate <code>comment</code> or <code>vas</code> (most likely case at the beginning of the training)? Then it would mess up the entire sequence, and the model will hardly learn anything…</p></blockquote><p>If we use the predicted token as input to the next step during training (as explained above), errors would accumulate and the model would rarely be exposed to the correct distribution of inputs, making training slow or impossible. To speedup things, one trick is to feed the actual output sequence (<code>&lt;sos&gt;</code> <code>comment</code> <code>vas</code> <code>tu</code>) into the decoder’s LSTM and predict the next token at every position (<code>comment</code> <code>vas</code> <code>tu</code> <code>&lt;eos&gt;</code>).</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_training.svg" alt="Training"></p><p><em>Training</em></p><p>The decoder outputs vectors of probability over the vocabulary $p_i \in \mathbb{R}^V$ for each time step. Then, for a given target sequence $y_1, \dots, y_n$, we can compute its probability as the product of the probabilities of each token being produced at each relevant time step:<br>$$<br>\mathbb{P}\left(y_1, \dots, y_m \right) = \prod_{i=1}^m p_i [y_i]<br>$$<br>where $p_i [y_i]$ means that we extract the $y_i$-th entry of the probability vector $p_i$ from the $i$-th decoding step. In particular, we can compute the probability of the actual target sequence. A perfect system would give a probabilty of 1 to this target sequence, so we are going to train our network to maximize the probability of the target sequence, which is the same as minimizing<br>$$<br>\begin{align}<br>-\log \mathbb{P} \left(y_1, \dots, y_m \right) &amp;= - \log \prod_{i=1}^m p_i [y_i]\\<br>&amp;= - \sum_{i=1}^n \log p_i [y_i]\\<br>\end{align}<br>$$<br>in our example, this is equal to<br>$$</p><ul><li>\log p_1[\text{comment}] - \log p_2[\text{vas}] - \log p_3[\text{tu}] - \log p_4[\text{<eos>}]<br>$$<br>and you recognize the standard cross entropy: we actually are minimizing the cross entropy between the target distribution (all one-hot vectors) and the predicted distribution outputed by our model (our vectors $p_i$).</eos></li></ul><h2 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h2><p>The main takeaway from the discussion above is that for the same model, we can define different behaviors. In particular, we defined a specific behavior that speeds up training.</p><blockquote><p>What about inference/testing time then? Is there an other way to decode a sentence?</p></blockquote><p>There indeed are 2 main ways of performing decoding at testing time (translating a sentence for which we don’t have a translation). The first of these methods is the one covered at the beginning of the article: <strong>greedy decoding</strong>. It is the most natural way and it consists in feeding to the next step the most likely word predicted at the previous step.</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/seq2seq_vanilla_decoder.svg" alt="Greedy Decoder - feeds the best token to the next step"></p><p><em>Greedy Decoder - feeds the best token to the next step</em></p><blockquote><p>But didn’t we say that this behavior is likely to accumulate errors?</p></blockquote><p>Even after having trained the model, it can happen that the model makes a small error (and gives a small advantage to <code>vas</code> over <code>comment</code> for the first step of the decoding). This would mess up the entire decoding…</p><p>There is a better way of performing decoding, called <strong>Beam Search</strong>. Instead of only predicting the token with the best score, we keep track of $k$ hypotheses (for example $k=5$, we refer to kk as the <strong>beam size</strong>). At each new time step, for these 5 hypotheses we have $V$ new possible tokens. It makes a total of $5V$ new hypotheses. Then, only keep the $5$ best ones, and so on… Formally, define $\mathcal{H}_t$ the set of hypotheses decoded at time step $t$.<br>$$<br>\mathcal{H}_ t := \{ (w^1_1, \dots, w^1_t), \dots, (w^k_1, \dots, w^k_t) \}<br>$$<br>For instance if $k=2$, one possible $\mathcal{H}_2$ would be<br>$$<br>\mathcal{H}_ 2 := \{ (\text{comment vas}), (\text{comment tu}) \}<br>$$<br>Now we consider all the possible candidates $\mathcal{C}_{t+1}$, produced from $\mathcal{H}_t$ by adding all possible new tokens<br>$$<br>\mathcal{C}_ {t+1} := \bigcup_{i=1}^k \{ (w^i_1, \dots, w^i_t, 1), \dots, (w^i_1, \dots, w^i_t, V) \}<br>$$<br>and keep the $k$ highest scores (probability of the sequence). If we keep our example<br>$$<br>\begin{align}<br>\mathcal{C}_ 3 =&amp; \{ (\text{comment vas comment}), (\text{comment vas vas}), (\text{comment vas tu})\} \\<br>\cup &amp; \{ (\text{comment tu comment}), \ \ (\text{comment tu vas}), \ \ (\text{comment tu tu}) \}<br>\end{align}<br>$$<br>and for instance we can imagine that the 2 best ones would be<br>$$<br>\mathcal{H}_ 3 := \{ (\text{comment vas tu}), (\text{comment tu vas}) \}<br>$$<br>Once every hypothesis reached the <code>&lt;eos&gt;</code> token, we return the hypothesis with the highest score.</p><blockquote><p>If we use <strong>beam search</strong>, a small error at the first step might be rectified at the next step, as we keep the gold hypthesis in the beam!</p></blockquote><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this article we covered the seq2seq concepts. We showed that training is different than decoding. We covered two methods for decoding: <strong>greedy</strong> and <strong>beam search</strong>. While beam search generally achieves better results, it is not perfect and still suffers from <strong>exposure bias</strong>. During training, the model is never exposed to its errors! It also suffers from <strong>Loss-Evaluation Mismatch</strong>. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</p><p>Now, let’s apply Seq2Seq for LaTeX generation from images!</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_task.svg" alt="Producing LaTeX code from an image"></p><p><em>Producing LaTeX code from an image</em></p><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>Previous part covered the concepts of <strong>sequence-to-sequence</strong> applied to Machine Translation. The same framework can be applied to our LaTeX generation problem. The input sequence would just be replaced by an image, preprocessed with some convolutional model adapted to OCR (in a sense, if we <em>unfold</em> the pixels of an image into a sequence, this is exactly the same problem). This idea proved to be efficient for image captioning (see the reference paper <a href="https://arxiv.org/abs/1502.03044" target="_blank" rel="external">Show, Attend and Tell</a>). Building on some <a href="https://arxiv.org/pdf/1609.04938v1.pdf" target="_blank" rel="external">great work</a> from the Harvard NLP group, my teammate <a href="https://www.linkedin.com/in/romain-sauvestre-241171a2" target="_blank" rel="external">Romain</a> and I chose to follow a similar approach.</p><blockquote><p>Keep the seq2seq framework but replace the encoder by a convolutional network over the image!</p></blockquote><p>Good Tensorflow implementations of such models were hard to find. Together with this post, I am releasing the <a href="https://github.com/guillaumegenthial/im2latex" target="_blank" rel="external">code</a> and hope some will find it useful. You can use it to train your own image captioning model or adapt it for a more advanced use. <a href="https://github.com/guillaumegenthial/im2latex" target="_blank" rel="external">The code</a> does <strong>not</strong> rely on the <a href="https://www.tensorflow.org/versions/master/api_guides/python/contrib.seq2seq" target="_blank" rel="external">Tensorflow Seq2Seq library</a> as it was not entirely ready at the time of the project and I also wanted more flexibility (but adopts a similar interface).</p><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>To train our model, we’ll need labeled examples: images of formulas along with the LaTeX code used to generate the images. A good source of LaTeX code is <a href="https://arxiv.org/" target="_blank" rel="external">arXiv</a>, that has thousands of articles under the <code>.tex</code> format. After applying some heuristics to find equations in the <code>.tex</code> files, keeping only the ones that actually compile, the <a href="https://zenodo.org/record/56198#.WflVu0yZPLZ" target="_blank" rel="external">Harvard NLP group</a> extracted $\sim 100,000$ formulas.</p><blockquote><p>Wait… Don’t you have a problem as different LaTeX codes can give the same image?</p></blockquote><p>Good point: <code>(x^2 + 1)</code> and <code>\left( x^{2} + 1 \right)</code> indeed give the same output. That’s why Harvard’s paper found out that normalizing the data using a parser (<a href="https://khan.github.io/KaTeX/" target="_blank" rel="external">KaTeX</a>) improved performance. It forces adoption of some conventions, like writing <code>x ^ { 2 }</code> instead of <code>x^2</code>, etc. After normalization, they end up with a <code>.txt</code> file containing one formula per line that looks like</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">\alpha + \beta</div><div class="line">\frac &#123; 1 &#125; &#123; 2 &#125;</div><div class="line">\frac &#123; \alpha &#125; &#123; \beta &#125;</div><div class="line">1 + 2</div></pre></td></tr></table></figure><p>From this file, we’ll produce images <code>0.png</code>, <code>1.png</code>, etc. and a matching file mapping the image files to the indices (=line numbers) of the formulas</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">0.png 0</div><div class="line">1.png 1</div><div class="line">2.png 2</div><div class="line">3.png 3</div></pre></td></tr></table></figure><p>The reason why we use this format is that it is flexible and allows you to use the pre-built <a href="https://zenodo.org/record/56198#.WflVu0yZPLZ" target="_blank" rel="external">dataset from Harvard</a> (You may need to use the preprocessing scripts as explained <a href="https://github.com/harvardnlp/im2markup" target="_blank" rel="external">here</a>). You’ll also need to have <code>pdflatex</code> and <code>ImageMagick</code> installed.</p><p>We also build a vocabulary, to map LaTeX tokens to indices that will be given as input to our model. If we keep the same data as above, our vocabulary will look like</p><p><code>+</code> <code>1</code> <code>2</code> <code>\alpha</code> <code>\beta</code> <code>\frac</code> <code>{</code> <code>}</code></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>Our model is going to rely on a variation of the Seq2Seq model, adapted to images. First, let’s define the input of our graph. Not surprisingly we get as input a batch of black-and-white images of shape $[H,W]$ and a batch of formulas (ids of the LaTeX tokens):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># batch of images, shape = (batch size, height, width, 1)</span></div><div class="line">img = tf.placeholder(tf.uint8, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="number">1</span>), name=<span class="string">'img'</span>)</div><div class="line"><span class="comment"># batch of formulas, shape = (batch size, length of the formula)</span></div><div class="line">formula = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, <span class="keyword">None</span>), name=<span class="string">'formula'</span>)</div><div class="line"><span class="comment"># for padding</span></div><div class="line">formula_length = tf.placeholder(tf.int32, shape=(<span class="keyword">None</span>, ), name=<span class="string">'formula_length'</span>)</div></pre></td></tr></table></figure><blockquote><p>A special note on the type of the image input. You may have noticed that we use <code>tf.uint8</code>. This is because our image is encoded in grey-levels (integers from <code>0</code> to <code>255</code> - and $2^8=256$). Even if we could give a <code>tf.float32</code> Tensor as input to Tensorflow, this would be 4 times more expensive in terms of memory bandwith. As data starvation is one of the main bottlenecks of GPUs, this simple trick can save us some training time. For further improvement of the data pipeline, have a look at <a href="https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/data" target="_blank" rel="external">the new Tensorflow data pipeline</a>.</p></blockquote><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p><strong>High-level idea</strong> Apply some convolutional network on top of the image an flatten the output into a sequence of vectors $[e_1, \cdots, e_n]$, each of those corresponding to a region of the input image. These vectors will correspond to the hidden vectors of the LSTM that we used for translation.</p><blockquote><p>Once our image is transformed into a sequence, we can use the seq2seq model!</p></blockquote><p><img src="https://guillaumegenthial.github.io/assets/img2latex/img2latex_encoder.svg" alt="Convolutional Encoder - produces a sequence of vectors"></p><p><em>Convolutional Encoder - produces a sequence of vectors</em></p><p>We need to extract features from our image, and for this, nothing has (<a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="external">yet</a>) been proven more effective than convolutions. Here, there is nothing much to say except that we pick some architecture that has been proven to be effective for Optical Character Recognition (OCR), which stacks convolutional layers and max-pooling to produce a Tensor of shape $[H^{\prime},W^{\prime},512]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># casting the image back to float32 on the GPU</span></div><div class="line">img = tf.cast(img, tf.float32) / <span class="number">255.</span></div><div class="line"></div><div class="line">out = tf.layers.conv2d(img, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>, <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"SAME"</span>, activation=tf.nn.relu)</div><div class="line">out = tf.layers.max_pooling2d(out, (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>), <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line"><span class="comment"># encoder representation, shape = (batch size, height', width', 512)</span></div><div class="line">out = tf.layers.conv2d(out, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="string">"VALID"</span>, activation=tf.nn.relu)</div></pre></td></tr></table></figure><p>Now that we have extracted some features from the image, let’s <strong>unfold</strong> the image to get a sequence so that we can use our sequence to sequence framework. We end up with a sequence of length $[H^{\prime},W^{\prime}]$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">H, W = tf.shape(out)[<span class="number">1</span>:<span class="number">2</span>]</div><div class="line">seq = tf.reshape(out, shape=[<span class="number">-1</span>, H*W, <span class="number">512</span>])</div></pre></td></tr></table></figure><blockquote><p>Don’t you loose a lot of structural information by reshaping? I’m afraid that when performing attention over the image, my decoder won’t be able to understand the location of each feature vector in the original image!</p></blockquote><p>It turns out that the model manages to work despite this issue, but that’s not completely satisfying. In the case of translation, the hidden states of the LSTM contained some positional information that was computed by the LSTM (after all, LSTM are by essence sequential). Can we fix this issue?</p><p><strong>Positional Embeddings</strong> I decided to follow the idea from <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="external">Attention is All you Need</a> that adds <em>positional embeddings</em> to the image representation (<code>out</code>), and has the huge advantage of not adding any new trainable parameter to our model. The idea is that for each position of the image, we compute a vector of size $512$ such that its components are coscos or sinsin. More formally, the (2i)-th and (2i+1)-th entries of my positional embedding $v$ at position $p$ will be<br>$$<br>\begin{align}<br>v_{2i} &amp;= \sin\left(p / f^{2i}\right)\\<br>v_{2i+1} &amp;= \cos\left(p / f^{2i}\right)\\<br>\end{align}<br>$$<br>where $f$ is some frequency parameter. Intuitively, because $\sin(a+b)$ and $\cos⁡(a+b)$ can be expressed in terms of $\sin⁡(b)$ , $\sin(a)$ , $\cos⁡(b)$ and $\cos⁡(a)$, there will be linear dependencies between the components of distant embeddings, authorizing the model to extract relative positioning information. Good news: the tensorflow code for this technique is available in the library <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="external">tensor2tensor</a>, so we just need to reuse the same function and transform our <code>out</code> with the following call</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">out = add_timing_signal_nd(out)</div></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>Now that we have a sequence of vectors $[e_1, \dots, e_n]$ that represents our input image, let’s decode it! First, let’s explain what variant of the Seq2Seq framework we are going to use.</p><p><strong>First hidden vector of the decoder’s LSTM</strong> In the seq2seq framework, this is usually just the last hidden vector of the encoder’s LSTM. Here, we don’t have such a vector, so a good choice would be to learn to compute it with a matrix $W$ and a vector $b$<br>$$<br>h_0 = \tanh\left( W \cdot \left( \frac{1}{n} \sum_{i=1}^n e_i\right) + b \right)<br>$$<br>This can be done in Tensorflow with the following logic</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">img_mean = tf.reduce_mean(seq, axis=<span class="number">1</span>)</div><div class="line">W = tf.get_variable(<span class="string">"W"</span>, shape=[<span class="number">512</span>, <span class="number">512</span>])</div><div class="line">b = tf.get_variable(<span class="string">"b"</span>, shape=[<span class="number">512</span>])</div><div class="line">h = tf.tanh(tf.matmul(img_mean, W) + b)</div></pre></td></tr></table></figure><p><strong>Attention Mechanism</strong> We first need to compute a score $\alpha_{t^{\prime}}$ for each vector $e_{t^{\prime}}$ of the sequence. We use the following method<br>$$<br>\begin{align}<br>\alpha_{t’} &amp;= \beta^T \tanh\left( W_1 \cdot e_{t’} + W_2 \cdot h_{t} \right)\\<br>\bar{\alpha} &amp;= \operatorname{softmax}\left(\alpha\right)\\<br>c_t &amp;= \sum_{i=1}^n \bar{\alpha}_{t’} e_{t’}\\<br>\end{align}<br>$$<br>This can be done in Tensorflow with the follwing code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># over the image, shape = (batch size, n, 512)</span></div><div class="line">W1_e = tf.layers.dense(inputs=seq, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># over the hidden vector, shape = (batch size, 512)</span></div><div class="line">W2_h = tf.layers.dense(inputs=h, units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># sums the two contributions</span></div><div class="line">a = tf.tanh(W1_e + tf.expand_dims(W2_h, axis=<span class="number">1</span>))</div><div class="line">beta = tf.get_variable(<span class="string">"beta"</span>, shape=[<span class="number">512</span>, <span class="number">1</span>], dtype=tf.float32)</div><div class="line">a_flat = tf.reshape(a, shape=[<span class="number">-1</span>, <span class="number">512</span>])</div><div class="line">a_flat = tf.matmul(a_flat, beta)</div><div class="line">a = tf.reshape(a, shape=[<span class="number">-1</span>, n])</div><div class="line"></div><div class="line"><span class="comment"># compute weights</span></div><div class="line">a = tf.nn.softmax(a)</div><div class="line">a = tf.expand_dims(a, axis=<span class="number">-1</span>)</div><div class="line">c = tf.reduce_sum(a * seq, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Note that the line <code>W1_e = tf.layers.dense(inputs=seq, units=512, use_bias=False)</code> is common to every decoder time step, so we can just compute it once and for all. The dense layer with no bias is just a matrix multiplication.</p></blockquote><p>Now that we have our attention vector, let’s just add a small modification and compute an other vector $o_{t-1}$ (as in <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank" rel="external">Luong, Pham and Manning</a>) that we will use to make our final prediction and that we will feed as input to the LSTM at the next step. Here $w_{t−1}$ denotes the embedding of the token generated at the previous step.</p><blockquote><p>$o_t$ passes some information about the distribution from the previous time step as well as the confidence it had for the predicted token</p></blockquote><p>$$<br>\begin{align}<br>h_t &amp;= \operatorname{LSTM}\left( h_{t-1}, [w_{t-1}, o_{t-1}] \right)\\<br>c_t &amp;= \operatorname{Attention}([e_1, \dots, e_n], h_t)\\<br>o_{t} &amp;= \tanh\left(W_3 \cdot [h_{t}, c_t] \right)\\<br>p_t &amp;= \operatorname{softmax}\left(W_4 \cdot o_{t} \right)\\<br>\end{align}<br>$$</p><p>and now the code</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute o</span></div><div class="line">W3_o = tf.layers.dense(inputs=tf.concat([h, c], axis=<span class="number">-1</span>), units=<span class="number">512</span>, use_bias=<span class="keyword">False</span>)</div><div class="line">o = tf.tanh(W3_o)</div><div class="line"></div><div class="line"><span class="comment"># compute the logits scores (before softmax)</span></div><div class="line">logits = tf.layers.dense(inputs=o, units=vocab_size, use_bias=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># the softmax will be computed in the loss or somewhere else</span></div></pre></td></tr></table></figure><blockquote><p>If I read carefully, I notice that for the first step of the decoding process, we need to compute an $o_0$ too, right?</p></blockquote><p>This is a good point, and we just use the same technique that we used to generate $h_0$ but with different weights.</p><h2 id="Training-1"><a href="#Training-1" class="headerlink" title="Training"></a>Training</h2><blockquote><p>We’ll need to create 2 different outputs in the Tensorflow graph: one for training (that uses the <code>formula</code>and feeds the ground truth at each time step, see <a href="https://guillaumegenthial.github.io/sequence-to-sequence.html" target="_blank" rel="external">part I</a>) and one for test time (that ignores everything about the actual <code>formula</code> and uses the prediction from the previous step).</p></blockquote><h3 id="AttentionCell"><a href="#AttentionCell" class="headerlink" title="AttentionCell"></a>AttentionCell</h3><p>We’ll need to encapsulate the reccurent logic into a custom cell that inherits <code>RNNCell</code>. Our custom cell will be able to call the LSTM cell (initialized in the <code>__init__</code>). It also has a special recurrent state that combines the LSTM state and the vector oo (as we need to pass it through). An elegant way is to define a namedtuple for this recurrent state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">AttentionState = collections.namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.lstm_cell = LSTMCell(<span class="number">512</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Args:</div><div class="line">            inputs: shape = (batch_size, dim_embeddings) embeddings from previous time step</div><div class="line">            cell_state: (AttentionState) state from previous time step</div><div class="line">        """</div><div class="line">        lstm_state, o = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([inputs, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># apply previous logic</span></div><div class="line">        c = ...</div><div class="line">        new_o  = ...</div><div class="line">        logits = ...</div><div class="line"></div><div class="line">        new_state = AttentionState(new_lstm_state, new_o)</div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><p>Then, to compute our output sequence, we just need to call the previous cell on the sequence of LaTeX tokens. We first produce the sequence of token embeddings to which we concatenate the special <code>&lt;sos&gt;</code> token. Then, we call <code>dynamic_rnn</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. get token embeddings</span></div><div class="line">E = tf.get_variable(<span class="string">"E"</span>, shape=[vocab_size, <span class="number">80</span>], dtype=tf.float32)</div><div class="line"><span class="comment"># special &lt;sos&gt; token</span></div><div class="line">start_token = tf.get_variable(<span class="string">"start_token"</span>, dtype=tf.float32, shape=[<span class="number">80</span>])</div><div class="line">tok_embeddings = tf.nn.embedding_lookup(E, formula)</div><div class="line"></div><div class="line"><span class="comment"># 2. add the special &lt;sos&gt; token embedding at the beggining of every formula</span></div><div class="line">start_token_ = tf.reshape(start_token, [<span class="number">1</span>, <span class="number">1</span>, dim])</div><div class="line">start_tokens = tf.tile(start_token_, multiples=[batch_size, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line"><span class="comment"># remove the &lt;eos&gt; that won't be used because we reached the end</span></div><div class="line">tok_embeddings = tf.concat([start_tokens, tok_embeddings[:, :<span class="number">-1</span>, :]], axis=<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># 3. decode</span></div><div class="line">attn_cell = AttentionCell()</div><div class="line">seq_logits, _ = tf.nn.dynamic_rnn(attn_cell, tok_embeddings, initial_state=AttentionState(h_0, o_0))</div></pre></td></tr></table></figure><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p>Code speaks for itself</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># compute - log(p_i[y_i]) for each time step, shape = (batch_size, formula length)</span></div><div class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=seq_logits, labels=formula)</div><div class="line"><span class="comment"># masking the losses</span></div><div class="line">mask = tf.sequence_mask(formula_length)</div><div class="line">losses = tf.boolean_mask(losses, mask)</div><div class="line"><span class="comment"># averaging the loss over the batch</span></div><div class="line">loss = tf.reduce_mean(losses)</div><div class="line"><span class="comment"># building the train op</span></div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate)</div><div class="line">train_op = optimizer.minimize(loss)</div></pre></td></tr></table></figure><p>and when iterating over the batches during training, <code>train_op</code> will be given to the <code>tf.Session</code> along with a <code>feed_dict</code> containing the data for the placeholders.</p><h2 id="Decoding-in-Tensorflow"><a href="#Decoding-in-Tensorflow" class="headerlink" title="Decoding in Tensorflow"></a>Decoding in Tensorflow</h2><blockquote><p>Let’s have a look at the Tensorflow implementation of the Greedy Method before dealing with Beam Search</p></blockquote><h3 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h3><p>While greedy decoding is easy to conceptualize, implementing it in Tensorflow is not straightforward, as you need to use the previous prediction and can’t use <code>dynamic_rnn</code> on the <code>formula</code>. There are basically <strong>2 ways of approaching the problem</strong></p><ol><li><p>Modify our <code>AttentionCell</code> and <code>AttentionState</code> so that <code>AttentionState</code> also contains the embedding of the predicted word at the previous time step,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">AttentionState = namedtuple(<span class="string">"AttentionState"</span>, (<span class="string">"lstm_state"</span>, <span class="string">"o"</span>, <span class="string">"embedding"</span>))</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttentionCell</span><span class="params">(RNNCell)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, inputs, cell_state)</span>:</span></div><div class="line">        lstm_state, o, embbeding = cell_state</div><div class="line">        <span class="comment"># compute h</span></div><div class="line">        h, new_lstm_state = self.lstm_cell(tf.concat([embedding, o], axis=<span class="number">-1</span>), lstm_state)</div><div class="line">        <span class="comment"># usual logic</span></div><div class="line">        logits = ...</div><div class="line">        <span class="comment"># compute new embeddding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        new_state = AttentionState(new_lstm_state, new_o, new_embedding)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> logits, new_state</div></pre></td></tr></table></figure><blockquote><p>This technique has a few downsides. It <strong>doesn’t use inputs</strong> (which used to be the embedding of the gold token from the <code>formula</code> and thus we would have to call <code>dynamic_rnn</code> on a “fake” sequence). Also, how do you know when to stop decoding, once you’ve reached the <code>&lt;eos&gt;</code> token?</p></blockquote></li><li><p>Implement a variant of <code>dynamic_rnn</code> that would not run on a sequence but feed the prediction from the previous time step to the cell, while having a maximum number of decoding steps. This would involve delving deeper into Tensorflow, using <code>tf.while_loop</code>. That’s the method we’re going to use as it fixes all the problems of the first technique. We eventually want something that looks like</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">attn_cell = AttentionCell(...)</div><div class="line"><span class="comment"># wrap the attention cell for decoding</span></div><div class="line">decoder_cell = GreedyDecoderCell(attn_cell)</div><div class="line"><span class="comment"># call a special dynamic_decode primitive</span></div><div class="line">test_outputs, _ = dynamic_decode(decoder_cell, max_length_formula+<span class="number">1</span>)</div></pre></td></tr></table></figure><blockquote><p>Much better isn’t it? Now let’s see what <code>GreedyDecoderCell</code> and <code>dynamic_decode</code> look like.</p></blockquote></li></ol><h3 id="Greedy-Decoder-Cell"><a href="#Greedy-Decoder-Cell" class="headerlink" title="Greedy Decoder Cell"></a>Greedy Decoder Cell</h3><p>We first wrap the attention cell in a <code>GreedyDecoderCell</code> that takes care of the greedy logic for us, without having to modify the <code>AttentionCell</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderOutput</span><span class="params">(collections.namedtuple<span class="params">(<span class="string">"DecoderOutput"</span>, <span class="params">(<span class="string">"logits"</span>, <span class="string">"ids"</span>)</span>)</span>)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GreedyDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># next step of attention cell</span></div><div class="line">        logits, new_state = self._attention_cell.step(embedding, state)</div><div class="line">        <span class="comment"># get ids of words predicted and get embedding</span></div><div class="line">        new_ids = tf.cast(tf.argmax(logits, axis=<span class="number">-1</span>), tf.int32)</div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div><div class="line">        <span class="comment"># create new state of decoder</span></div><div class="line">        new_output = DecoderOutput(logits, new_ids)</div><div class="line">        new_finished = tf.logical_or(finished, tf.equal(new_ids,</div><div class="line">                self._end_token))</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (new_output, new_state, new_embedding, new_finished)</div></pre></td></tr></table></figure><h3 id="Dynamic-Decode-primitive"><a href="#Dynamic-Decode-primitive" class="headerlink" title="Dynamic Decode primitive"></a>Dynamic Decode primitive</h3><p>We need to implement a function <code>dynamic_decode</code> that will recursively call the above <code>step</code> function. We do this with a <code>tf.while_loop</code> that stops when all the hypotheses reached <code>&lt;eos&gt;</code> or <code>time</code> is greater than the max number of iterations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dynamic_decode</span><span class="params">(decoder_cell, maximum_iterations)</span>:</span></div><div class="line">    <span class="comment"># initialize variables (details on github)</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">condition</span><span class="params">(time, unused_outputs_ta, unused_state, unused_inputs, finished)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.logical_not(tf.reduce_all(finished))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">body</span><span class="params">(time, outputs_ta, state, inputs, finished)</span>:</span></div><div class="line">        new_output, new_state, new_inputs, new_finished = decoder_cell.step(</div><div class="line">            time, state, inputs, finished)</div><div class="line">        <span class="comment"># store the outputs in TensorArrays (details on github)</span></div><div class="line">        new_finished = tf.logical_or(tf.greater_equal(time, maximum_iterations), new_finished)</div><div class="line"></div><div class="line">        <span class="keyword">return</span> (time + <span class="number">1</span>, outputs_ta, new_state, new_inputs, new_finished)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"rnn"</span>):</div><div class="line">        res = tf.while_loop(</div><div class="line">            condition,</div><div class="line">            body,</div><div class="line">            loop_vars=[initial_time, initial_outputs_ta, initial_state, initial_inputs, initial_finished])</div><div class="line"></div><div class="line">    <span class="comment"># return the final outputs (details on github)</span></div></pre></td></tr></table></figure><blockquote><p>Some details using <code>TensorArrays</code> or <code>nest.map_structure</code> have been omitted for clarity but may be found on <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/dynamic_decode.py" target="_blank" rel="external">github</a></p><p>Notice that we place the <code>tf.while_loop</code> inside a scope named <code>rnn</code>. This is because <code>dynamic_rnn</code>does the same thing and thus the weights of our LSTM are defined in that scope.</p></blockquote><h3 id="Beam-Search-Decoder-Cell"><a href="#Beam-Search-Decoder-Cell" class="headerlink" title="Beam Search Decoder Cell"></a>Beam Search Decoder Cell</h3><blockquote><p>We can follow the same approach as in the greedy method and use <code>dynamic_decode</code></p></blockquote><p>Let’s create a new wrapper for <code>AttentionCell</code> in the same way we did for <code>GreedyDecoderCell</code>. This time, the code is going to be more complicated and the following is just for intuition. Note that when selecting the top kk hypotheses from the set of candidates, we must know which “beginning” they used (=parent hypothesis).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BeamSearchDecoderCell</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    <span class="comment"># notice the same arguments as for GreedyDecoderCell</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, time, state, embedding, finished)</span>:</span></div><div class="line">        <span class="comment"># compute new logits</span></div><div class="line">        logits, new_cell_state = self._attention_cell.step(embedding, state.cell_state)</div><div class="line"></div><div class="line">        <span class="comment"># compute log probs of the step (- log p(w) for all words w)</span></div><div class="line">        <span class="comment"># shape = [batch_size, beam_size, vocab_size]</span></div><div class="line">        step_log_probs = tf.nn.log_softmax(new_logits)</div><div class="line"></div><div class="line">        <span class="comment"># compute scores for the (beam_size * vocabulary_size) new hypotheses</span></div><div class="line">        log_probs = state.log_probs + step_log_probs</div><div class="line"></div><div class="line">        <span class="comment"># get top k hypotheses</span></div><div class="line">        new_probs, indices = tf.nn.top_k(log_probs, self._beam_size)</div><div class="line"></div><div class="line">        <span class="comment"># get ids of next token along with the parent hypothesis</span></div><div class="line">        new_ids = ...</div><div class="line">        new_parents = ...</div><div class="line"></div><div class="line">        <span class="comment"># compute new embeddings, new_finished, new_cell state...</span></div><div class="line">        new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)</div></pre></td></tr></table></figure><blockquote><p>Look at <a href="https://github.com/guillaumegenthial/im2latex/blob/master/model/components/beam_search_decoder_cell.py" target="_blank" rel="external">github</a> for the details. The main idea is that we add a beam dimension to every tensor, but when feeding it into <code>AttentionCell</code> we merge the beam dimension with the batch dimension. There is also some trickery involved to compute the parents and the new ids using modulos.</p></blockquote><h2 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>I hope that you learned something with this post, either about the technique or Tensorflow. While the model achieves impressive performance (at least on short formulas with roughly 85% of the LaTeX being reconstructed), it still raises some questions that I list here:</p><p><em>How do we evaluate the performance of our model?</em>. We can use standard metrics from Machine Translation like <a href="https://en.wikipedia.org/wiki/BLEU" target="_blank" rel="external">BLEU</a> to evaluate how good the decoded LaTeX is compared to the reference. We can also choose to compile the predicted LaTeX sequence to get the image of the formula, and then compare this image to the orignal. As a formula is a sequence, computing the pixel-wise distance wouldn’t really make sense. A good idea is proposed by <a href="http://lstm.seas.harvard.edu/latex" target="_blank" rel="external">Harvard’s paper</a>. First, slice the image vertically. Then, compare the edit distance between these slices…</p><p><em>How to fix exposure bias?</em> While beam search generally achieves better results, it is not perfect and still suffers from <strong>exposure bias</strong>. During training, the model is never exposed to its errors! It also suffers from <strong>Loss-Evaluation Mismatch</strong>. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentence…</p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/ref.png" alt=""></p><p><img src="https://guillaumegenthial.github.io/assets/img2latex/pred.png" alt=""></p><p><em>An Example of LaTeX generation - which one is the reference?</em></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/18/Understanding-Convolutions/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/18/Understanding-Convolutions/" itemprop="url">Understanding Convolutions</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-18T17:01:29+08:00">2018-01-18 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/18/Understanding-Convolutions/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/18/Understanding-Convolutions/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/18/Understanding-Convolutions/" class="leancloud_visitors" data-flag-title="Understanding Convolutions"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Lessons-from-a-Dropped-Ball"><a href="#Lessons-from-a-Dropped-Ball" class="headerlink" title="Lessons from a Dropped Ball"></a>Lessons from a Dropped Ball</h2><p>Imagine we drop a ball from some height onto the ground, where it only has one dimension of motion. <em>How likely is it that a ball will go a distance $c$ if you drop it and then drop it again from above the point at which it landed?</em></p><p>Let’s break this down. After the first drop, it will land aa units away from the starting point with probability $f(a)$, where $f$ is the probability distribution.</p><p>Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed. The probability of the ball rolling $b$ units away from the new starting point is $g(b)$, where $g$ may be a different probability distribution if it’s dropped from a different height.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-fagb.png" alt="img"></p><p>If we fix the result of the first drop so we know the ball went distance aa, for the ball to go a total distance cc, the distance traveled in the second drop is also fixed at bb, where $a+b=c$. So the probability of this happening is simply $f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn1" target="_blank" rel="external">1</a></p><p>Let’s think about this with a specific discrete example. We want the total distance $c$ to be 3. If the first time it rolls, $a=2$, the second time it must roll $b=1$ in order to reach our total distance $a+b=3$. The probability of this is $f(2)⋅g(1)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-split-21.png" alt="img"></p><p>However, this isn’t the only way we could get to a total distance of 3. The ball could roll 1 units the first time, and 2 the second. Or 0 units the first time and all 3 the second. It could go any $a$ and $b$, as long as they add to 3.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-splits-12-03.png" alt="img"></p><p>The probabilities are $f(1)⋅g(2)$ and $f(0)⋅g(3)$, respectively.</p><p>In order to find the <em>total likelihood</em> of the ball reaching a total distance of $c$, we can’t consider only one possible way of reaching $c$. Instead, we consider <em>all the possible ways</em> of partitioning $c$ into two drops $a$ and $b$ and sum over the <em>probability of each way</em>.<br>$$<br>…\;\; f(0)\cdot g(3) ~+~ f(1)\cdot g(2) ~+~ f(2)\cdot g(1)\;\;…<br>$$<br>We already know that the probability for each case of $a+b=c$ is simply $f(a)⋅g(b)$. So, summing over every solution to $a+b=c$, we can denote the total likelihood as:<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Turns out, we’re doing a convolution! In particular, the convolution of $f$ and $g$, evluated at $c$ is defined:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)~~~~<br>$$<br>If we substitute $b=c−a$, we get:<br>$$<br>(f\ast g)(c) = \sum_a f(a) \cdot g(c-a)<br>$$<br>This is the standard definition<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn2" target="_blank" rel="external">2</a> of convolution.</p><p>To make this a bit more concrete, we can think about this in terms of positions the ball might land. After the first drop, it will land at an intermediate position aa with probability $f(a)$. If it lands at $a$, it has probability $g(c−a)$ of landing at a position $c$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-OnePath.png" alt="img"></p><p>To get the convolution, we consider all intermediate positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-SumPaths.png" alt="img"></p><h2 id="Visualizing-Convolutions"><a href="#Visualizing-Convolutions" class="headerlink" title="Visualizing Convolutions"></a>Visualizing Convolutions</h2><p>There’s a very nice trick that helps one think about convolutions more easily.</p><p>First, an observation. Suppose the probability that a ball lands a certain distance $x$ from where it started is $f(x)$. Then, afterwards, the probability that it started a distance $x$ from where it landed is $f(−x)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Reverse.png" alt="img"></p><p>If we know the ball lands at a position $c$ after the second drop, what is the probability that the previous position was $a$?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-BackProb.png" alt="img"></p><p>So the probability that the previous position was $a$ is $g(−(a−c))=g(c−a)$.</p><p>Now, consider the probability each intermediate position contributes to the ball finally landing at $c$. We know the probability of the first drop putting the ball into the intermediate position a is $f(a)$. We also know that the probability of it having been in $a$, if it lands at $c$ is $g(c−a)$.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate.png" alt="img"></p><p>Summing over the $a$s, we get the convolution.</p><p>The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value $c$ in a single picture. By shifting the bottom half around, we can evaluate the convolution at other values of $c$. This allows us to understand the convolution as a whole.</p><p>For example, we can see that it peaks when the distributions align.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Align.png" alt="img"></p><p>And shrinks as the intersection between the distributions gets smaller.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-Intermediate-Sep.png" alt="img"></p><p>By using this trick in an animation, it really becomes possible to visually understand convolutions.</p><p>Below, we’re able to visualize the convolution of two box functions:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Wiki-BoxConvAnim.gif" alt="Wiki-BoxConvAnim"></p><p><em>From Wikipedia</em></p><p>Armed with this perspective, a lot of things become more intuitive.</p><p>Let’s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy.</p><h2 id="Higher-Dimensional-Convolutions"><a href="#Higher-Dimensional-Convolutions" class="headerlink" title="Higher Dimensional Convolutions"></a>Higher Dimensional Convolutions</h2><p>Convolutions are an extremely general idea. We can also use them in a higher number of dimensions.</p><p>Let’s consider our example of a falling ball again. Now, as it falls, it’s position shifts not only in one dimension, but in two.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/ProbConv-TwoDim.png" alt="img"></p><p>Convolution is the same as before:<br>$$<br>(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)<br>$$<br>Except, now $a$, $b$ and $c$ are vectors. To be more explicit,<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{\begin{array}{c}a_1+b_1=c_1\\a_2+b_2=c_2\end{array}} f(a_1,a_2) \cdot g(b_1,b_2)<br>$$<br>Or in the standard definition:<br>$$<br>(f\ast g)(c_1, c_2) = \sum_{a_1, a_2} f(a_1, a_2) \cdot g(c_1-a_1,~ c_2-a_2)<br>$$<br>Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding.</p><p>One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a “kernel.”</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/RiverTrain-ImageConvDiagram.png" alt="RiverTrain-ImageConvDiagram"></p><p><em>From the <a href="http://intellabs.github.io/RiverTrail/tutorial/" target="_blank" rel="external">River Trail documentation</a></em></p><p>The kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over.</p><p>For example, by averaging a 3x3 box of pixels, we can blur an image. To do this, our kernel takes the value $\dfrac{1}{9}$ on each pixel in the box,</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Blur.png" alt="Gimp-Blur"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>We can also detect edges by taking the values −1−1 and 11 on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Gimp-Edge.png" alt="Gimp-Edge"></p><p><em>Derived from the <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">Gimp documentation</a></em></p><p>The gimp documentation has <a href="http://docs.gimp.org/en/plug-in-convmatrix.html" target="_blank" rel="external">many other examples</a>.</p><h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><p>So, how does convolution relate to convolutional neural networks?</p><p>Consider a 1-dimensional convolutional layer with inputs $\{x_n\}$ and outputs $\{y_n\}$, like we discussed in the <a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" target="_blank" rel="external">previous post</a>:</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY.png" alt="img"></p><p>As we observed, we can describe the outputs in terms of the inputs:<br>$$<br>y_n = A(x_{n}, x_{n+1}, …)<br>$$<br>Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.</p><p>Recall that a typical neuron in a neural network is described by:<br>$$<br>\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~…~ + b)<br>$$<br>Where $x_0$, $x_1$… are the inputs. The weights, $w_0$, $w_1$, … describe how the neuron connects to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fn3" target="_blank" rel="external">3</a> Saying that multiple neurons are identical is the same thing as saying that the weights are the same.</p><p>It’s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.</p><p>Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:<br>$$<br>y = \sigma(Wx + b)<br>$$<br>For example, we get:<br>$$<br>y_0 = \sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 …)<br>$$</p><p>$$<br>y_1 = \sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 …)<br>$$</p><p>Each row of the matrix describes the weights connecting a neuron to its inputs.</p><p>Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv-9-Conv2-XY-W.png" alt="img"></p><p>Which corresponds to the equations:<br>$$<br>y_0 = \sigma(W_0x_0 + W_1x_1 -b)<br>$$</p><p>$$<br>y_1 = \sigma(W_0x_1 + W_1x_2 -b)<br>$$</p><p>So while, normally, a weight matrix connects every input to every neuron with different weights:<br>$$<br>W = \left[\begin{array}{ccccc}<br>W_{0,0} &amp; W_{0,1} &amp; W_{0,2} &amp; W_{0,3} &amp; …\\<br>W_{1,0} &amp; W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; …\\<br>W_{2,0} &amp; W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; …\\<br>W_{3,0} &amp; W_{3,1} &amp; W_{3,2} &amp; W_{3,3} &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons don’t connect to many possible inputs, there’s lots of zeros.<br>$$<br>W = \left[\begin{array}{ccccc}<br>w_0 &amp; w_1 &amp; 0 &amp; 0 &amp; …\\<br>0 &amp; w_0 &amp; w_1 &amp; 0 &amp; …\\<br>0 &amp; 0 &amp; w_0 &amp; w_1 &amp; …\\<br>0 &amp; 0 &amp; 0 &amp; w_0 &amp; …\\<br>… &amp; … &amp; … &amp; … &amp; …\\<br>\end{array}\right]<br>$$<br>Multiplying by the above matrix is the same thing as convolving with $[…0, w_1, w_0, 0…]$. The function sliding to different positions corresponds to having neurons at those positions.</p><p>What about two-dimensional convolutional layers?</p><p><img src="http://colah.github.io/posts/2014-07-Understanding-Convolutions/img/Conv2-5x5-Conv2-XY.png" alt="img"></p><p>The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.</p><p>Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image.</p><ol><li><p>We want the probability of the ball rolling aa units the first time and also rolling bb units the second time. The distributions $P(A)=f(a)$ and $P(b)=g(b)$ are independent, with both distributions centered at 0. So $P(a,b)=P(a)∗P(b)=f(a)⋅g(b)$.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref1" target="_blank" rel="external">↩</a></p></li><li><p>The non-standard definition, which I haven’t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious.</p><p>For example, convolution is a commutative operation. That is, $f∗g=g∗f$. Why?</p><p>​<br>$$<br>\sum_{a+b=c} f(a) \cdot g(b) ~~=~ \sum_{b+a=c} g(b) \cdot f(a)<br>$$<br>Convolution is also associative. That is, $(f∗g)∗h=f∗(g∗h)$. Why?</p><p>​<br>$$<br>\sum_{(a+b)+c=d} (f(a) \cdot g(b)) \cdot h(c) ~~=~ \sum_{a+(b+c)=d} f(a) \cdot (g(b) \cdot h(c))<br>$$<br>↩</p><p>​</p></li><li><p>There’s also the bias, which is the “threshold” for whether the neuron fires, but it’s much simpler and I don’t want to clutter this section talking about it.<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/#fnref3" target="_blank" rel="external">↩</a></p></li></ol></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/17/PCA-With-Tensorflow/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2018/01/17/PCA-With-Tensorflow/" itemprop="url">PCA With Tensorflow</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-17T17:04:36+08:00">2018-01-17 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2018/01/17/PCA-With-Tensorflow/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2018/01/17/PCA-With-Tensorflow/" itemprop="commentsCount"></span> </a></span><span id="/2018/01/17/PCA-With-Tensorflow/" class="leancloud_visitors" data-flag-title="PCA With Tensorflow"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>PCA (<strong>P</strong>rincipal <strong>C</strong>omponent <strong>A</strong>nalysis) is probably the oldest trick in the book.</p><p>PCA is well studied and there are numerous ways to get to the same solution, we will talk about two of them here, Eigen decomposition and Singular Value Decomposition (SVD) and then we will implement the SVD way in TensorFlow.</p><p>From now on, X will be our data matrix, of shape (n, p) where n is the number of examples, and p are the dimensions.</p><p>So given X, both methods will try to find, in their own way, a way to manipulate and decompose X in a manner that later on we could multiply the decomposed results to represent maximum information in less dimensions. I know I know, sounds horrible but I will spare you most of the math but keep the parts that contribute to the understanding of the method pros and cons.</p><p>So Eigen decomposition and SVD are both ways to decompose matrices, lets see how they help us in PCA and how they are connected.</p><p>Take a glance at the flow chart below and I will explain right after.</p><p><img src="https://cdn-images-1.medium.com/max/1600/1*xnomew0zpnxftxutG8xoFw.png" alt="img"></p><p>Figure 1 PCA workflow</p><p>So why should you care about this? Well there is something very fundamental about the two procedures that tells us a lot about PCA.</p><p>As you can see both methods are pure linear algebra, that basically tells us that using PCA is looking at the real data, from a different angle — this is unique to PCA since the other methods start with random representation of lower dimensional data and try to get it to behave like the high dimensional data.</p><p>Some other notable things are that all operations are linear and with SVD are super-super fast.</p><p>Also given the same data PCA will always give the same answer (which is not true about the other two methods).</p><p>Notice how in SVD we choose the r (r is the number of dimensions we want to reduce to) left most values of Σ to lower dimensionality?</p><p>Well there is something special about Σ .</p><p>Σ is a diagonal matrix, there are p (number of dimensions) diagonal values (called singular values) and their magnitude indicates how significant they are to preserving the information.</p><p>So we can choose to reduce dimensionality, to the number of dimensions that will preserve approx. given amount of percentage of the data and I will demonstrate that in the code (e.g. gives us the ability to reduce dimensionality with a constraint of losing a max of 15% of the data).</p><p>As you will see, coding this in TensorFlow is pretty simple — what we are are going to code is a class that has <code>fit</code> method and a <code>reduce</code> method which we will supply the dimensions to.</p><h3 id="CODE-PCA"><a href="#CODE-PCA" class="headerlink" title="CODE (PCA)"></a><strong>CODE (PCA)</strong></h3><p>Lets see how the <code>fit</code> method looks like, given <code>self.X</code> contains the data and <code>self.dtype=tf.float32</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self)</span>:</span></div><div class="line">    self.graph = tf.Graph()</div><div class="line">    <span class="keyword">with</span> self.graph.as_default():</div><div class="line">        self.X = tf.placeholder(self.dtype, shape=self.data.shape)</div><div class="line"></div><div class="line">        <span class="comment"># Perform SVD</span></div><div class="line">        singular_values, u, _ = tf.svd(self.X)</div><div class="line"></div><div class="line">        <span class="comment"># Create sigma matrix</span></div><div class="line">        sigma = tf.diag(singular_values)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session(graph=self.graph) <span class="keyword">as</span> session:</div><div class="line">        self.u, self.singular_values, self.sigma = session.run([u, singular_values, sigma],</div><div class="line">                                                               feed_dict=&#123;self.X: self.data&#125;)</div></pre></td></tr></table></figure><p>So the goal of <code>fit</code> is to create our Σ and U for later use.</p><p>We’ll start with the line <code>tf.svd</code> which gives us the singular values, which are the diagonal values of what was denoted as Σ in Figure 1, and the matrices U and V.</p><p>Then <code>tf.diag</code> is TensorFlow’s way of converting a 1D vector, to a diagonal matrix, which in our case will result in Σ.</p><p>At the end of the <code>fit</code> call we will have the singular values, Σ and U.</p><p>Now lets lets implement <code>reduce</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span><span class="params">(self, n_dimensions=None, keep_info=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> keep_info:</div><div class="line">        <span class="comment"># Normalize singular values</span></div><div class="line">        normalized_singular_values = self.singular_values / sum(self.singular_values)</div><div class="line"></div><div class="line">        <span class="comment"># Create the aggregated ladder of kept information per dimension</span></div><div class="line">        ladder = np.cumsum(normalized_singular_values)</div><div class="line"></div><div class="line">        <span class="comment"># Get the first index which is above the given information threshold</span></div><div class="line">        index = next(idx <span class="keyword">for</span> idx, value <span class="keyword">in</span> enumerate(ladder) <span class="keyword">if</span> value &gt;= keep_info) + <span class="number">1</span></div><div class="line">        n_dimensions = index</div><div class="line"></div><div class="line">    <span class="keyword">with</span> self.graph.as_default():</div><div class="line">        <span class="comment"># Cut out the relevant part from sigma</span></div><div class="line">        sigma = tf.slice(self.sigma, [<span class="number">0</span>, <span class="number">0</span>], [self.data.shape[<span class="number">1</span>], n_dimensions])</div><div class="line"></div><div class="line">        <span class="comment"># PCA</span></div><div class="line">        pca = tf.matmul(self.u, sigma)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session(graph=self.graph) <span class="keyword">as</span> session:</div><div class="line">        <span class="keyword">return</span> session.run(pca, feed_dict=&#123;self.X: self.data&#125;)</div></pre></td></tr></table></figure><p>So as you can see <code>reduce</code> gets either <code>keep_info</code> or <code>n_dimensions</code> (I didn’t implement the input check where <strong>only one must be supplied</strong>).</p><p>If we supply <code>n_dimensions</code> it will simply reduce to that number, but if we supply <code>keep_info</code> which should be a float between 0 and 1, we will preserve that much information from the original data (0.9 — preserve 90% of the data).</p><p>In the first ‘if’, we normalize and check how many singular values are needed, basically figuring out <code>n_dimensions</code> out of <code>keep_info</code>.</p><p>In the graph, we just slice the Σ (sigma) matrix for as much data as we need and perform the matrix multiplication.</p><p>So lets try it out on the iris dataset, which is (150, 4) dataset of 3 species of iris flowers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line"></div><div class="line">tf_pca = TF_PCA(iris_dataset.data, iris_dataset.target)</div><div class="line">tf_pca.fit()</div><div class="line">pca = tf_pca.reduce(keep_info=<span class="number">0.9</span>)  <span class="comment"># Results in 2 dimensions</span></div><div class="line"></div><div class="line">color_mapping = &#123;<span class="number">0</span>: sns.xkcd_rgb[<span class="string">'bright purple'</span>], <span class="number">1</span>: sns.xkcd_rgb[<span class="string">'lime'</span>], <span class="number">2</span>: sns.xkcd_rgb[<span class="string">'ochre'</span>]&#125;</div><div class="line">colors = list(map(<span class="keyword">lambda</span> x: color_mapping[x], tf_pca.target))</div><div class="line"></div><div class="line">plt.scatter(pca[:, <span class="number">0</span>], pca[:, <span class="number">1</span>], c=colors)</div></pre></td></tr></table></figure><p><img src="https://cdn-images-1.medium.com/max/2000/1*-am5UfbZoJkUA4C8z5d0vQ.png" alt="img"></p><p>Figure 2 Iris dataset PCA 2 dimensional plot</p><p>Not so bad huh?</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/26/">26</a><a class="extend next" rel="next" href="/page/7/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">128</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">63</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>