<!doctype html><html class="theme-next mist use-motion" lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Abracadabra" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0"><meta name="description" content="Ewan&apos;s IT Blog"><meta property="og:type" content="website"><meta property="og:title" content="Abracadabra"><meta property="og:url" content="http://yoursite.com/page/9/index.html"><meta property="og:site_name" content="Abracadabra"><meta property="og:description" content="Ewan&apos;s IT Blog"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Abracadabra"><meta name="twitter:description" content="Ewan&apos;s IT Blog"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post"},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"Author"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/page/9/"><title>Abracadabra</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="en"><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?dc405a79ad500922134d14cdf288f646";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container one-collumn sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Abracadabra</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">Do it yourself</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>Home</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>Categories</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>About</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>Archives</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>Tags</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="menu-item-icon fa fa-fw fa-sitemap"></i><br>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>Search</a></li></ul><div class="site-search"><div class="popup"><span class="search-icon fa fa-search"></span> <input type="text" id="local-search-input"><div id="local-search-result"></div><span class="popup-btn-close">close</span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/10/Policy-Gradient-Methods/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/10/Policy-Gradient-Methods/" itemprop="url">Policy Gradient Methods</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-10T17:23:16+08:00">2017-07-10 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/10/Policy-Gradient-Methods/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/10/Policy-Gradient-Methods/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/10/Policy-Gradient-Methods/" class="leancloud_visitors" data-flag-title="Policy Gradient Methods"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Until now almost all the methods have learned the values of actions and then selected actions based on their estimated action values; their policies would not even exist without the action-value estimates. In this post we consider methods that instead learn a <em>parameterized policy</em> that can select actions without consulting a value function. A value function may still be used to <em>learn</em> the policy parameter, but is not required for action selection. We use the notation $\boldsymbol{\theta} \in \mathbb{R}^d$ for the policy’s parameter vector. Thus we write $\pi(a|s, \boldsymbol{\theta}) = \text{Pr}(A_t=a | S_t=s, \boldsymbol{\theta}_t=\boldsymbol{\theta})$ for the probability that action $a$ is taken at time $t$ given that the agent is in state $s$ at time $t$ with parameter $\boldsymbol{\theta}$. If a method uses a learned value function as well, then the value function’s weight vector is denoted $\mathbf{w} \in \mathbb{R}^m$, as in $\hat{v}(s, \mathbf{w})$.</p><p>In this chapter we consider methods for learning the policy parameter based on the gradient of some performance measure $J(\boldsymbol{\theta})$ with respect to the policy parameter. These methods seek to maximize performance, so their updates approximate gradient ascent in $J$ :<br>$$<br>\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \alpha \widehat{\nabla J(\boldsymbol{\theta}_t)}.<br>$$<br>All methods that follow this general schema we call <strong>policy gradient methods</strong>, whether or not they also learn an approximate value function. Methods that learn approximations to both policy and value functions are often called actor–critic methods, where ‘actor’ is a reference to the learned policy, and ‘critic’ refers to the learned value function, usually a state-value function.</p><h3 id="Policy-Approximation"><a href="#Policy-Approximation" class="headerlink" title="Policy Approximation"></a>Policy Approximation</h3><p>The most preferred actions in each state are given the highest probability of being selected, for example, according to an exponential softmax distribution:<br>$$<br>\pi(a|s, \boldsymbol{\theta}) = \frac{\exp(h(s, a, \boldsymbol{\theta}))}{\sum_b \exp(h(s, b, \boldsymbol{\theta}))}.<br>$$<br>For example, they might be computed by a deep neural network, where $\boldsymbol{\theta}$ is the vector of all the connection weights of the network. Or the preferences could simply be linear in features,<br>$$<br>h(s, a, \boldsymbol{\theta}) = \boldsymbol{\theta}^{\top} \mathbf{x}(s, a).<br>$$</p><h3 id="The-Policy-Gradient-Theorem"><a href="#The-Policy-Gradient-Theorem" class="headerlink" title="The Policy Gradient Theorem"></a>The Policy Gradient Theorem</h3><p>We deﬁne the performance measure as the value of the start state of the episode. We can simplify the notation without losing any meaningful generality by assuming that every episode starts in some particular (non-random) state s0. Then, in the episodic case we deﬁne performance as<br>$$<br>J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}}(s_0),<br>$$<br>where $ v_{\pi_{\boldsymbol{\theta}}}$ is the true value function for $\pi_{\boldsymbol{\theta}}$, the policy determined by $\boldsymbol{\theta}$.</p><p>The policy gradient theorem is that<br>$$<br>\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}),<br>$$<br>where $\mu_{\pi}(s)$ we mentioned in <a href="https://ewanlee.github.io/2017/07/05/On-policy-Prediction-with-Approximation/" target="_blank" rel="external">earlier</a>.</p><h3 id="REINFORCE-Monte-Carlo-Policy-Gradient"><a href="#REINFORCE-Monte-Carlo-Policy-Gradient" class="headerlink" title="REINFORCE: Monte Carlo Policy Gradient"></a>REINFORCE: Monte Carlo Policy Gradient</h3><p>$$<br>\begin{align}<br>\nabla J(\boldsymbol{\theta}) &amp;= \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}) \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a q_{\pi}(S_t, a) \nabla_{\boldsymbol{\theta}} \pi(a | S_t, \boldsymbol{\theta}) \Bigg] \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a \pi(a|S_t, \boldsymbol{\theta}) q_{\pi}(S_t, a) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})} \Bigg] \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t q_{\pi}(S_t, A_t) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \text{(replacing a by the sample } A_t \sim \pi \;) \\<br>&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \;\;\; \;\;\; \;\;\; \;\; (\text{because } \mathbb{E}_{\pi}[G_t|S_t,A_t]=q_{\pi}(S_t,A_t)).<br>\end{align}<br>$$</p><p>So we get<br>$$<br>\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.<br>$$<br>This is shown explicitly in the boxed pseudocode below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/pgm/reinforce.png" alt="reinforce"></p><p>Notice that $\nabla \log x = \frac{\nabla x}{x}$.</p><h3 id="REINFORCE-with-Baseline"><a href="#REINFORCE-with-Baseline" class="headerlink" title="REINFORCE with Baseline"></a>REINFORCE with Baseline</h3><p>The policy gradient theorem can be generalized to include a comparison of the action value to an arbitrary <strong>baseline</strong> $b(s)$:</p><p>$$<br>\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a \big(q_{\pi}(s, a) - b(s)\big) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}).<br>$$<br>The baseline can be any function, even a random variable, as long as it does not vary with $a$; the equation remains true, because the subtracted quantity is zero:<br>$$<br>\sum_a b(s) \nabla_{\boldsymbol{\theta}} \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} \sum_a \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} 1 = 0 \;\;\;\; \forall s \in \mathcal{S}.<br>$$<br>The update rule that we end up with is a new version of REINFORCE that includes a general baseline:<br>$$<br>\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \big(G_t-b(S_t) \big) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.<br>$$<br>One natural choice for the baseline is an estimate of the state value, $\hat{v}(S_t, \mathbf{w})$, where $\mathbf{w} \in \mathbb{R}^m$ is a weight vector learned by one of the methods presented in previous posts. A complete pseudocode algorithm for REINFROCE with baseline is given in the box (use Monte Carlo method for learning the policy parameter and state-value weights).</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/pgm/reinforce_baseline.png" alt="reinforce_baseline"></p><h3 id="Actor-Critic-Methods"><a href="#Actor-Critic-Methods" class="headerlink" title="Actor-Critic Methods"></a>Actor-Critic Methods</h3><p>Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor-critic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating a state from the estimated values of subsequent states), but only as a baseline for the state being updated. In<br>order to gain these advantages in the case of policy gradient methods we use actor-critic methods with a true bootstrapping critic.</p><p>One-step actor-critic methods replace the full return of REINFORCE with the one-step return (and use a learned state-value function as the baseline) as follow:<br>$$<br>\begin{align}<br>\boldsymbol{\theta}_{t+1} &amp;\doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( G_{t:t+1} - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\<br>&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\<br>&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \delta_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})}.<br>\end{align}<br>$$<br>The natural state-value-function learning method to pair with this is semi-gradient TD(0). Pseudocode for the complete algorithm is given in the box below. Note that it is now a fully online, incremental algorithm, with states, actions, and rewards processed as they occur and then never revisited.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/pgm/one-step-ac.png" alt="one-step-ac"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/" itemprop="url">Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-09T11:34:18+08:00">2017-07-09 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/09/Using-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout/" class="leancloud_visitors" data-flag-title="Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In previous <a href="https://ewanlee.github.io/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" target="_blank" rel="external">blog</a>, we use the Keras to play the FlappyBird. Similarity, we will use another deep learning toolkit Tensorflow to develop the DQN and Double DQN and to play the another game Breakout (Atari 3600).</p><p>Here, we will use the OpenAI gym toolkit to construct out environment. Detail implementation is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env = gym.envs.make(<span class="string">"Breakout-v0"</span>)</div></pre></td></tr></table></figure><p>And then we look some demos:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">print(<span class="string">"Action space size: &#123;&#125;"</span>.format(env.action_space.n))</div><div class="line"><span class="comment"># print(env.get_action_meanings())</span></div><div class="line"></div><div class="line">observation = env.reset()</div><div class="line">print(<span class="string">"Observation space shape: &#123;&#125;"</span>.format(observation.shape))</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">[env.step(<span class="number">2</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>)]</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">env.render(close=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/games/breakout-env.png" alt="breakout-env"></p><p>For deep learning purpose, we need to crop the image to a square image:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Check out what a cropped image looks like</span></div><div class="line">plt.imshow(observation[<span class="number">34</span>:<span class="number">-16</span>,:,:])</div></pre></td></tr></table></figure><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/games/croped-breakout-image.png" alt="croped-breakout-image"></p><p>Not bad !</p><p>Ok, now let us to use the Tensorflow to develop the DQN algorithm first.</p><p>First of all, we need to reference some packages and initialize the environment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"></div><div class="line"><span class="keyword">import</span> gym</div><div class="line"><span class="keyword">from</span> gym.wrappers <span class="keyword">import</span> Monitor</div><div class="line"><span class="keyword">import</span> itertools</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> sys</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="keyword">if</span> <span class="string">"../"</span> <span class="keyword">not</span> <span class="keyword">in</span> sys.path:</div><div class="line">  sys.path.append(<span class="string">"../"</span>)</div><div class="line"></div><div class="line"><span class="keyword">from</span> lib <span class="keyword">import</span> plotting</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque, namedtuple</div><div class="line"></div><div class="line">env = gym.envs.make(<span class="string">"Breakout-v0"</span>)</div><div class="line"><span class="comment"># Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions</span></div><div class="line">VALID_ACTIONS = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</div></pre></td></tr></table></figure><p>As mentioned above, we need to crop the image and preprocess the input before feed the raw image into the algorithm. So we define a <strong>StateProcessor</strong> class to do this.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StateProcessor</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Processes a raw Atari images. Resizes it and converts it to grayscale.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># Build the Tensorflow graph</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"state_processor"</span>):</div><div class="line">            self.input_state = tf.placeholder(shape=[<span class="number">210</span>, <span class="number">160</span>, <span class="number">3</span>], dtype=tf.uint8)</div><div class="line">            self.output = tf.image.rgb_to_grayscale(self.input_state)</div><div class="line">            self.output = tf.image.crop_to_bounding_box(self.output, <span class="number">34</span>, <span class="number">0</span>, <span class="number">160</span>, <span class="number">160</span>)</div><div class="line">            self.output = tf.image.resize_images(</div><div class="line">                self.output, [<span class="number">84</span>, <span class="number">84</span>], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</div><div class="line">            self.output = tf.squeeze(self.output)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(self, sess, state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Args:</div><div class="line">            sess: A Tensorflow session object</div><div class="line">            state: A [210, 160, 3] Atari RGB State</div><div class="line"></div><div class="line">        Returns:</div><div class="line">            A processed [84, 84, 1] state representing grayscale values.</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> sess.run(self.output, &#123; self.input_state: state &#125;)</div></pre></td></tr></table></figure><p>We first convert the image to gray image and then resize it to 84 by 84 (the size DQN paper used). Then, we construct the neural network to estimate the value function. The structure of the network as the same as the DQN paper.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Estimator</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""Q-Value Estimator neural network.</span></div><div class="line"></div><div class="line">    This network is used for both the Q-Network and the Target Network.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, scope=<span class="string">"estimator"</span>, summaries_dir=None)</span>:</span></div><div class="line">        self.scope = scope</div><div class="line">        <span class="comment"># Writes Tensorboard summaries to disk</span></div><div class="line">        self.summary_writer = <span class="keyword">None</span></div><div class="line">        <span class="keyword">with</span> tf.variable_scope(scope):</div><div class="line">            <span class="comment"># Build the graph</span></div><div class="line">            self._build_model()</div><div class="line">            <span class="keyword">if</span> summaries_dir:</div><div class="line">                summary_dir = os.path.join(summaries_dir, <span class="string">"summaries_&#123;&#125;"</span>.format(scope))</div><div class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(summary_dir):</div><div class="line">                    os.makedirs(summary_dir)</div><div class="line">                self.summary_writer = tf.summary.FileWriter(summary_dir)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_model</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Builds the Tensorflow graph.</div><div class="line">        """</div><div class="line"></div><div class="line">        <span class="comment"># Placeholders for our input</span></div><div class="line">        <span class="comment"># Our input are 4 RGB frames of shape 160, 160 each</span></div><div class="line">        self.X_pl = tf.placeholder(shape=[<span class="keyword">None</span>, <span class="number">84</span>, <span class="number">84</span>, <span class="number">4</span>], dtype=tf.uint8, name=<span class="string">"X"</span>)</div><div class="line">        <span class="comment"># The TD target value</span></div><div class="line">        self.y_pl = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.float32, name=<span class="string">"y"</span>)</div><div class="line">        <span class="comment"># Integer id of which action was selected</span></div><div class="line">        self.actions_pl = tf.placeholder(shape=[<span class="keyword">None</span>], dtype=tf.int32, name=<span class="string">"actions"</span>)</div><div class="line"></div><div class="line">        X = tf.to_float(self.X_pl) / <span class="number">255.0</span></div><div class="line">        batch_size = tf.shape(self.X_pl)[<span class="number">0</span>]</div><div class="line"></div><div class="line">        <span class="comment"># Three convolutional layers</span></div><div class="line">        conv1 = tf.contrib.layers.conv2d(</div><div class="line">            X, <span class="number">32</span>, <span class="number">8</span>, <span class="number">4</span>, activation_fn=tf.nn.relu)</div><div class="line">        conv2 = tf.contrib.layers.conv2d(</div><div class="line">            conv1, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>, activation_fn=tf.nn.relu)</div><div class="line">        conv3 = tf.contrib.layers.conv2d(</div><div class="line">            conv2, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, activation_fn=tf.nn.relu)</div><div class="line"></div><div class="line">        <span class="comment"># Fully connected layers</span></div><div class="line">        flattened = tf.contrib.layers.flatten(conv3)</div><div class="line">        fc1 = tf.contrib.layers.fully_connected(flattened, <span class="number">512</span>)</div><div class="line">        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))</div><div class="line"></div><div class="line">        <span class="comment"># Get the predictions for the chosen actions only</span></div><div class="line">        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[<span class="number">1</span>] + self.actions_pl</div><div class="line">        self.action_predictions = tf.gather(tf.reshape(self.predictions, [<span class="number">-1</span>]), gather_indices)</div><div class="line"></div><div class="line">        <span class="comment"># Calcualte the loss</span></div><div class="line">        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)</div><div class="line">        self.loss = tf.reduce_mean(self.losses)</div><div class="line"></div><div class="line">        <span class="comment"># Optimizer Parameters from original paper</span></div><div class="line">        self.optimizer = tf.train.RMSPropOptimizer(<span class="number">0.00025</span>, <span class="number">0.99</span>, <span class="number">0.0</span>, <span class="number">1e-6</span>)</div><div class="line">        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())</div><div class="line"></div><div class="line">        <span class="comment"># Summaries for Tensorboard</span></div><div class="line">        self.summaries = tf.summary.merge([</div><div class="line">            tf.summary.scalar(<span class="string">"loss"</span>, self.loss),</div><div class="line">            tf.summary.histogram(<span class="string">"loss_hist"</span>, self.losses),</div><div class="line">            tf.summary.histogram(<span class="string">"q_values_hist"</span>, self.predictions),</div><div class="line">            tf.summary.scalar(<span class="string">"max_q_value"</span>, tf.reduce_max(self.predictions))</div><div class="line">        ])</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sess, s)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Predicts action values.</div><div class="line"></div><div class="line">        Args:</div><div class="line">          sess: Tensorflow session</div><div class="line">          s: State input of shape [batch_size, 4, 160, 160, 3]</div><div class="line"></div><div class="line">        Returns:</div><div class="line">          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated </div><div class="line">          action values.</div><div class="line">        """</div><div class="line">        <span class="keyword">return</span> sess.run(self.predictions, &#123; self.X_pl: s &#125;)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, sess, s, a, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Updates the estimator towards the given targets.</div><div class="line"></div><div class="line">        Args:</div><div class="line">          sess: Tensorflow session object</div><div class="line">          s: State input of shape [batch_size, 4, 160, 160, 3]</div><div class="line">          a: Chosen actions of shape [batch_size]</div><div class="line">          y: Targets of shape [batch_size]</div><div class="line"></div><div class="line">        Returns:</div><div class="line">          The calculated loss on the batch.</div><div class="line">        """</div><div class="line">        feed_dict = &#123; self.X_pl: s, self.y_pl: y, self.actions_pl: a &#125;</div><div class="line">        summaries, global_step, _, loss = sess.run(</div><div class="line">            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],</div><div class="line">            feed_dict)</div><div class="line">        <span class="keyword">if</span> self.summary_writer:</div><div class="line">            self.summary_writer.add_summary(summaries, global_step)</div><div class="line">        <span class="keyword">return</span> loss</div></pre></td></tr></table></figure><p>As mentioned in DQN paper, there are two network that share the same parameters in DQN algorithm. We need to copy the parameters to the target network on each $t$ steps.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">copy_model_parameters</span><span class="params">(sess, estimator1, estimator2)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Copies the model parameters of one estimator to another.</div><div class="line"></div><div class="line">    Args:</div><div class="line">      sess: Tensorflow session instance</div><div class="line">      estimator1: Estimator to copy the paramters from</div><div class="line">      estimator2: Estimator to copy the parameters to</div><div class="line">    """</div><div class="line">    e1_params = [t <span class="keyword">for</span> t <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> t.name.startswith(estimator1.scope)]</div><div class="line">    e1_params = sorted(e1_params, key=<span class="keyword">lambda</span> v: v.name)</div><div class="line">    e2_params = [t <span class="keyword">for</span> t <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> t.name.startswith(estimator2.scope)]</div><div class="line">    e2_params = sorted(e2_params, key=<span class="keyword">lambda</span> v: v.name)</div><div class="line"></div><div class="line">    update_ops = []</div><div class="line">    <span class="keyword">for</span> e1_v, e2_v <span class="keyword">in</span> zip(e1_params, e2_params):</div><div class="line">        op = e2_v.assign(e1_v)</div><div class="line">        update_ops.append(op)</div><div class="line"></div><div class="line">    sess.run(update_ops)</div></pre></td></tr></table></figure><p>We also need a policy to take an action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(estimator, nA)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.</div><div class="line"></div><div class="line">    Args:</div><div class="line">        estimator: An estimator that returns q values for a given state</div><div class="line">        nA: Number of actions in the environment.</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        A function that takes the (sess, observation, epsilon) as an argument and returns</div><div class="line">        the probabilities for each action in the form of a numpy array of length nA.</div><div class="line"></div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(sess, observation, epsilon)</span>:</span></div><div class="line">        A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">        q_values = estimator.predict(sess, np.expand_dims(observation, <span class="number">0</span>))[<span class="number">0</span>]</div><div class="line">        best_action = np.argmax(q_values)</div><div class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">        <span class="keyword">return</span> A</div><div class="line">    <span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Now let us to develop the DQN algorithm (we skip the details here because we explained it earlier).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">deep_q_learning</span><span class="params">(sess,</span></span></div><div class="line">                    env,</div><div class="line">                    q_estimator,</div><div class="line">                    target_estimator,</div><div class="line">                    state_processor,</div><div class="line">                    num_episodes,</div><div class="line">                    experiment_dir,</div><div class="line">                    replay_memory_size=<span class="number">500000</span>,</div><div class="line">                    replay_memory_init_size=<span class="number">50000</span>,</div><div class="line">                    update_target_estimator_every=<span class="number">10000</span>,</div><div class="line">                    discount_factor=<span class="number">0.99</span>,</div><div class="line">                    epsilon_start=<span class="number">1.0</span>,</div><div class="line">                    epsilon_end=<span class="number">0.1</span>,</div><div class="line">                    epsilon_decay_steps=<span class="number">500000</span>,</div><div class="line">                    batch_size=<span class="number">32</span>,</div><div class="line">                    record_video_every=<span class="number">50</span>):</div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm for fff-policy TD control using Function Approximation.</div><div class="line">    Finds the optimal greedy policy while following an epsilon-greedy policy.</div><div class="line"></div><div class="line">    Args:</div><div class="line">        sess: Tensorflow Session object</div><div class="line">        env: OpenAI environment</div><div class="line">        q_estimator: Estimator object used for the q values</div><div class="line">        target_estimator: Estimator object used for the targets</div><div class="line">        state_processor: A StateProcessor object</div><div class="line">        num_episodes: Number of episodes to run for</div><div class="line">        experiment_dir: Directory to save Tensorflow summaries in</div><div class="line">        replay_memory_size: Size of the replay memory</div><div class="line">        replay_memory_init_size: Number of random experiences to sampel when initializing </div><div class="line">          the reply memory.</div><div class="line">        update_target_estimator_every: Copy parameters from the Q estimator to the </div><div class="line">          target estimator every N steps</div><div class="line">        discount_factor: Lambda time discount factor</div><div class="line">        epsilon_start: Chance to sample a random action when taking an action.</div><div class="line">          Epsilon is decayed over time and this is the start value</div><div class="line">        epsilon_end: The final minimum value of epsilon after decaying is done</div><div class="line">        epsilon_decay_steps: Number of steps to decay epsilon over</div><div class="line">        batch_size: Size of batches to sample from the replay memory</div><div class="line">        record_video_every: Record a video every N episodes</div><div class="line"></div><div class="line">    Returns:</div><div class="line">        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line"></div><div class="line">    Transition = namedtuple(<span class="string">"Transition"</span>, [<span class="string">"state"</span>, <span class="string">"action"</span>, <span class="string">"reward"</span>, <span class="string">"next_state"</span>, <span class="string">"done"</span>])</div><div class="line"></div><div class="line">    <span class="comment"># The replay memory</span></div><div class="line">    replay_memory = []</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))</div><div class="line"></div><div class="line">    <span class="comment"># Create directories for checkpoints and summaries</span></div><div class="line">    checkpoint_dir = os.path.join(experiment_dir, <span class="string">"checkpoints"</span>)</div><div class="line">    checkpoint_path = os.path.join(checkpoint_dir, <span class="string">"model"</span>)</div><div class="line">    monitor_path = os.path.join(experiment_dir, <span class="string">"monitor"</span>)</div><div class="line">    </div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):</div><div class="line">        os.makedirs(checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(monitor_path):</div><div class="line">        os.makedirs(monitor_path)</div><div class="line"></div><div class="line">    saver = tf.train.Saver()</div><div class="line">    <span class="comment"># Load a previous checkpoint if we find one</span></div><div class="line">    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)</div><div class="line">    <span class="keyword">if</span> latest_checkpoint:</div><div class="line">        print(<span class="string">"Loading model checkpoint &#123;&#125;...\n"</span>.format(latest_checkpoint))</div><div class="line">        saver.restore(sess, latest_checkpoint)</div><div class="line">    </div><div class="line">    <span class="comment"># Get the current time step</span></div><div class="line">    total_t = sess.run(tf.contrib.framework.get_global_step())</div><div class="line"></div><div class="line">    <span class="comment"># The epsilon decay schedule</span></div><div class="line">    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)</div><div class="line"></div><div class="line">    <span class="comment"># The policy we're following</span></div><div class="line">    policy = make_epsilon_greedy_policy(</div><div class="line">        q_estimator,</div><div class="line">        len(VALID_ACTIONS))</div><div class="line"></div><div class="line">    <span class="comment"># Populate the replay memory with initial experience</span></div><div class="line">    print(<span class="string">"Populating replay memory..."</span>)</div><div class="line">    state = env.reset()</div><div class="line">    state = state_processor.process(sess, state)</div><div class="line">    state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(replay_memory_init_size):</div><div class="line">        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps<span class="number">-1</span>)])</div><div class="line">        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])</div><div class="line">        next_state = state_processor.process(sess, next_state)</div><div class="line">        next_state = np.append(state[:,:,<span class="number">1</span>:], np.expand_dims(next_state, <span class="number">2</span>), axis=<span class="number">2</span>)</div><div class="line">        replay_memory.append(Transition(state, action, reward, next_state, done))</div><div class="line">        <span class="keyword">if</span> done:</div><div class="line">            state = env.reset()</div><div class="line">            state = state_processor.process(sess, state)</div><div class="line">            state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            state = next_state</div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment"># Record videos</span></div><div class="line">    <span class="comment"># Add env Monitor wrapper</span></div><div class="line">    env = Monitor(env, directory=monitor_path, video_callable=<span class="keyword">lambda</span> count: count % record_video_every == <span class="number">0</span>, resume=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line"></div><div class="line">        <span class="comment"># Save the current checkpoint</span></div><div class="line">        saver.save(tf.get_default_session(), checkpoint_path)</div><div class="line"></div><div class="line">        <span class="comment"># Reset the environment</span></div><div class="line">        state = env.reset()</div><div class="line">        state = state_processor.process(sess, state)</div><div class="line">        state = np.stack([state] * <span class="number">4</span>, axis=<span class="number">2</span>)</div><div class="line">        loss = <span class="keyword">None</span></div><div class="line"></div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line"></div><div class="line">            <span class="comment"># Epsilon for this time step</span></div><div class="line">            epsilon = epsilons[min(total_t, epsilon_decay_steps<span class="number">-1</span>)]</div><div class="line"></div><div class="line">            <span class="comment"># Add epsilon to Tensorboard</span></div><div class="line">            episode_summary = tf.Summary()</div><div class="line">            episode_summary.value.add(simple_value=epsilon, tag=<span class="string">"epsilon"</span>)</div><div class="line">            q_estimator.summary_writer.add_summary(episode_summary, total_t)</div><div class="line"></div><div class="line">            <span class="comment"># Maybe update the target estimator</span></div><div class="line">            <span class="keyword">if</span> total_t % update_target_estimator_every == <span class="number">0</span>:</div><div class="line">                copy_model_parameters(sess, q_estimator, target_estimator)</div><div class="line">                print(<span class="string">"\nCopied model parameters to target network."</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Print out which step we're on, useful for debugging.</span></div><div class="line">            print(<span class="string">"\rStep &#123;&#125; (&#123;&#125;) @ Episode &#123;&#125;/&#123;&#125;, loss: &#123;&#125;"</span>.format(</div><div class="line">                    t, total_t, i_episode + <span class="number">1</span>, num_episodes, loss), end=<span class="string">""</span>)</div><div class="line">            sys.stdout.flush()</div><div class="line"></div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            action_probs = policy(sess, state, epsilon)</div><div class="line">            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])</div><div class="line">            next_state = state_processor.process(sess, next_state)</div><div class="line">            next_state = np.append(state[:,:,<span class="number">1</span>:], np.expand_dims(next_state, <span class="number">2</span>), axis=<span class="number">2</span>)</div><div class="line"></div><div class="line">            <span class="comment"># If our replay memory is full, pop the first element</span></div><div class="line">            <span class="keyword">if</span> len(replay_memory) == replay_memory_size:</div><div class="line">                replay_memory.pop(<span class="number">0</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Save transition to replay memory</span></div><div class="line">            replay_memory.append(Transition(state, action, reward, next_state, done))   </div><div class="line"></div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line"></div><div class="line">            <span class="comment"># Sample a minibatch from the replay memory</span></div><div class="line">            samples = random.sample(replay_memory, batch_size)</div><div class="line">            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))</div><div class="line"></div><div class="line">            <span class="comment"># Calculate q values and targets</span></div><div class="line">            q_values_next = target_estimator.predict(sess, next_states_batch)</div><div class="line">            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">            <span class="comment"># Perform gradient descent update</span></div><div class="line">            states_batch = np.array(states_batch)</div><div class="line">            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line"></div><div class="line">            state = next_state</div><div class="line">            total_t += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="comment"># Add summaries to tensorboard</span></div><div class="line">        episode_summary = tf.Summary()</div><div class="line">        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=<span class="string">"episode_reward"</span>, tag=<span class="string">"episode_reward"</span>)</div><div class="line">        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=<span class="string">"episode_length"</span>, tag=<span class="string">"episode_length"</span>)</div><div class="line">        q_estimator.summary_writer.add_summary(episode_summary, total_t)</div><div class="line">        q_estimator.summary_writer.flush()</div><div class="line"></div><div class="line">        <span class="keyword">yield</span> total_t, plotting.EpisodeStats(</div><div class="line">            episode_lengths=stats.episode_lengths[:i_episode+<span class="number">1</span>],</div><div class="line">            episode_rewards=stats.episode_rewards[:i_episode+<span class="number">1</span>])</div><div class="line"></div><div class="line">    <span class="keyword">return</span> stats</div></pre></td></tr></table></figure><p>Finally, run it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">tf.reset_default_graph()</div><div class="line"></div><div class="line"><span class="comment"># Where we save our checkpoints and graphs</span></div><div class="line">experiment_dir = os.path.abspath(<span class="string">"./experiments/&#123;&#125;"</span>.format(env.spec.id))</div><div class="line"></div><div class="line"><span class="comment"># Create a glboal step variable</span></div><div class="line">global_step = tf.Variable(<span class="number">0</span>, name=<span class="string">'global_step'</span>, trainable=<span class="keyword">False</span>)</div><div class="line">    </div><div class="line"><span class="comment"># Create estimators</span></div><div class="line">q_estimator = Estimator(scope=<span class="string">"q"</span>, summaries_dir=experiment_dir)</div><div class="line">target_estimator = Estimator(scope=<span class="string">"target_q"</span>)</div><div class="line"></div><div class="line"><span class="comment"># State processor</span></div><div class="line">state_processor = StateProcessor()</div><div class="line"></div><div class="line"><span class="comment"># Run it!</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    <span class="keyword">for</span> t, stats <span class="keyword">in</span> deep_q_learning(sess,</div><div class="line">                                    env,</div><div class="line">                                    q_estimator=q_estimator,</div><div class="line">                                    target_estimator=target_estimator,</div><div class="line">                                    state_processor=state_processor,</div><div class="line">                                    experiment_dir=experiment_dir,</div><div class="line">                                    num_episodes=<span class="number">10000</span>,</div><div class="line">                                    replay_memory_size=<span class="number">500000</span>,</div><div class="line">                                    replay_memory_init_size=<span class="number">50000</span>,</div><div class="line">                                    update_target_estimator_every=<span class="number">10000</span>,</div><div class="line">                                    epsilon_start=<span class="number">1.0</span>,</div><div class="line">                                    epsilon_end=<span class="number">0.1</span>,</div><div class="line">                                    epsilon_decay_steps=<span class="number">500000</span>,</div><div class="line">                                    discount_factor=<span class="number">0.99</span>,</div><div class="line">                                    batch_size=<span class="number">32</span>):</div><div class="line"></div><div class="line">        print(<span class="string">"\nEpisode Reward: &#123;&#125;"</span>.format(stats.episode_rewards[<span class="number">-1</span>]))</div></pre></td></tr></table></figure><hr><p>Next, we will develop the Double-DQN algorithm. This algorithm only has a little changes.</p><p>In DQN <strong>q_learning</strong> method,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate q values and targets</span></div><div class="line">q_values_next = target_estimator.predict(sess, next_states_batch)</div><div class="line">targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p>we just change these codes to,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Calculate q values and targets</span></div><div class="line"><span class="comment"># This is where Double Q-Learning comes in!</span></div><div class="line">q_values_next = q_estimator.predict(sess, next_states_batch)</div><div class="line">best_actions = np.argmax(q_values_next, axis=<span class="number">1</span>)</div><div class="line">q_values_next_target = target_estimator.predict(sess, next_states_batch)</div><div class="line">targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * q_values_next_target[np.arange(batch_size), best_actions]</div></pre></td></tr></table></figure></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" itemprop="url">Using Keras and Deep Q-Network to Play FlappyBird (Repost)</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-07T04:19:18+08:00">2017-07-07 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/07/Using-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost/" class="leancloud_visitors" data-flag-title="Using Keras and Deep Q-Network to Play FlappyBird (Repost)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>200 lines of python code to demonstrate DQN with Keras</p><p><img src="https://yanpanlau.github.io/img/animation1.gif" alt="img"></p><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>This project demonstrates how to use the Deep-Q Learning algorithm with Keras together to play FlappyBird.</p><p>This article is intended to target newcomers who are interested in Reinforcement Learning.</p><h1 id="Installation-Dependencies"><a href="#Installation-Dependencies" class="headerlink" title="Installation Dependencies:"></a>Installation Dependencies:</h1><p>(Update : 13 March 2017, code and weight file has been updated to support latest version of tensorflow and keras)</p><ul><li>Python 2.7</li><li>Keras 1.0</li><li>pygame</li><li>scikit-image</li></ul><h1 id="How-to-Run"><a href="#How-to-Run" class="headerlink" title="How to Run?"></a>How to Run?</h1><p><strong>CPU only/TensorFlow</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><p><strong>GPU version (Theano)</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">THEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=0.2 python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><p><strong>lib.cnmem=0.2</strong> means you assign only 20% of the GPU’s memory to the program.</p><p><strong>If you want to train the network from beginning, delete “model.h5” and run qlearn.py -m “Train”</strong></p><h1 id="What-is-Deep-Q-Network"><a href="#What-is-Deep-Q-Network" class="headerlink" title="What is Deep Q-Network?"></a>What is Deep Q-Network?</h1><p>Deep Q-Network is a learning algorithm developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the algorithm is generic enough to play various games.</p><p>The following post is a must-read for those who are interested in deep reinforcement learning.</p><p><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">Demystifying Deep Reinforcement Learning</a></p><h1 id="Code-Explanation-in-details"><a href="#Code-Explanation-in-details" class="headerlink" title="Code Explanation (in details)"></a>Code Explanation (in details)</h1><p>Let’s go though the example in <strong>qlearn.py</strong>, line by line. If you familiar with Keras and DQN, you can skip this session</p><p>The code simply does the following:</p><ol><li>The code receives the Game Screen Input in the form of a pixel array</li><li>The code does some image pre-processing</li><li>The processed image will be fed into a neural network (Convolution Neural Network), and the network will then decide the best action to execute (Flap or Not Flap)</li><li>The network will be trained millions of times, via an algorithm called Q-learning, to maximize the future expected reward.</li></ol><h3 id="Game-Screen-Input"><a href="#Game-Screen-Input" class="headerlink" title="Game Screen Input"></a>Game Screen Input</h3><p>First of all, the FlappyBird is already written in Python via pygame, so here is the code snippet to access the FlappyBird API</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import wrapped_flappy_bird as game</div><div class="line">x_t1_colored, r_t, terminal = game_state.frame_step(a_t)</div></pre></td></tr></table></figure><p>The idea is quite simple, the input is <strong>a_t</strong> (0 represent don’t flap, 1 represent flap), the API will give you the next frame <strong>x_t1_colored</strong>, the <strong>reward</strong> (0.1 if alive, +1 if pass the pipe, -1 if die) and <strong>terminal</strong> is a boolean flag indicates whether the game is FINISHED or NOT. We also followed DeepMind suggestion to clip the reward between [-1,+1] to improve the stability. I have not yet get a chance to test out different reward functions but it would be an interesting exercise to see how the performance is changed with different reward functions.</p><p>Interesting readers can modify the reward function in <strong>game/wrapped_flappy_bird.py”, under the function **def frame_step(self, input_actions)</strong></p><h3 id="Image-pre-processing"><a href="#Image-pre-processing" class="headerlink" title="Image pre-processing"></a>Image pre-processing</h3><p><img src="https://yanpanlau.github.io/img/bird.jpg" alt="img"></p><p>In order to make the code train faster, it is vital to do some image processing. Here are the key elements:</p><ol><li>I first convert the color image into grayscale</li><li>I crop down the image size into 80x80 pixel</li><li>I stack 4 frames together before I feed into neural network.</li></ol><p>Why do I need to stack 4 frames together? This is one way for the model to be able to infer the velocity information of the bird.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x_t1 = skimage.color.rgb2gray(x_t1_colored)</div><div class="line">x_t1 = skimage.transform.resize(x_t1,(<span class="number">80</span>,<span class="number">80</span>))</div><div class="line">x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(<span class="number">0</span>, <span class="number">255</span>))</div><div class="line"></div><div class="line">x_t1 = x_t1.reshape(<span class="number">1</span>, <span class="number">1</span>, x_t1.shape[<span class="number">0</span>], x_t1.shape[<span class="number">1</span>])</div><div class="line">s_t1 = np.append(x_t1, s_t[:, :<span class="number">3</span>, :, :], axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><p><strong>x_t1</strong> is a single frame with shape (1x1x80x80) and <strong>s_t1</strong> is the stacked frame with shape (1x4x80x80). You might ask, why the input dimension is (1x4x80x80) but not (4x80x80)? Well, it is a requirement in Keras so let’s stick with it.</p><p>Note: Some readers may ask what is <strong>axis=1</strong>? It means that when I stack the frames, I want to stack on the “2nd” dimension. i.e. I am stacking under (1x<strong>4</strong>x80x80), the 2nd index.</p><h3 id="Convolution-Neural-Network"><a href="#Convolution-Neural-Network" class="headerlink" title="Convolution Neural Network"></a>Convolution Neural Network</h3><p>Now, we can input the pre-processed screen into the neural network, which is a convolution neural network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildmodel</span><span class="params">()</span>:</span></div><div class="line">    print(<span class="string">"Now we build the model"</span>)</div><div class="line">    model = Sequential()</div><div class="line">    model.add(Convolution2D(<span class="number">32</span>, <span class="number">8</span>, <span class="number">8</span>, subsample=(<span class="number">4</span>,<span class="number">4</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>,input_shape=(img_channels,img_rows,img_cols)))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">4</span>, <span class="number">4</span>, subsample=(<span class="number">2</span>,<span class="number">2</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Convolution2D(<span class="number">64</span>, <span class="number">3</span>, <span class="number">3</span>, subsample=(<span class="number">1</span>,<span class="number">1</span>),init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name), border_mode=<span class="string">'same'</span>))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Flatten())</div><div class="line">    model.add(Dense(<span class="number">512</span>, init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)))</div><div class="line">    model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">    model.add(Dense(<span class="number">2</span>,init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)))</div><div class="line">   </div><div class="line">    adam = Adam(lr=<span class="number">1e-6</span>)</div><div class="line">    model.compile(loss=<span class="string">'mse'</span>,optimizer=adam)</div><div class="line">    print(<span class="string">"We finish building the model"</span>)</div><div class="line">    <span class="keyword">return</span> model</div></pre></td></tr></table></figure><p>The exact architecture is following : The input to the neural network consists of an 4x80x80 images. The first hidden layer convolves 32 filters of 8 x 8 with stride 4 and applies ReLU activation function. The 2nd layer convolves a 64 filters of 4 x 4 with stride 2 and applies ReLU activation function. The 3rd layer convolves a 64 filters of 3 x 3 with stride 1 and applies ReLU activation function. The final hidden layer is fully-connected consisted of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action.</p><p>So wait, what is convolution? The easiest way to understand a convolution is by thinking of it as a sliding window function apply to a matrix. The following gif file should help to understand.</p><p><img src="https://yanpanlau.github.io/img/Convolution_schematic.gif" alt="Convolution with 3×3 Filter. Source: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution"></p><p>You might ask what’s the purpose the convolution? It actually help computer to learn higher features like edges and shapes. The example below shows how the edges are stand out after a convolution filter is applied</p><p><img src="https://yanpanlau.github.io/img/generic-taj-convmatrix-edge-detect.jpg" alt="Using Convolution to detect Edges"></p><p>For more details about Convolution in Neural Network, please read <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_blank" rel="external">Understanding Convolution Neural Networks for NLP</a></p><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>Keras makes it very easy to build convolution neural network. However, there are few things I would like to highlight</p><p>A) It is important to choose a right initialization method. I choose normal distribution with $\sigma=0.01$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">init=<span class="keyword">lambda</span> shape, name: normal(shape, scale=<span class="number">0.01</span>, name=name)</div></pre></td></tr></table></figure><p>B) The ordering of the dimension is important, the default setting is 4x80x80 (Theano setting), so if your input is 80x80x4 (Tensorflow setting) then you are in trouble because the dimension is wrong. <strong>Alert</strong>: If your input dimension is 80x80x4 (Tensorflow setting) you need to set <strong>dim_ordering = tf</strong> (tf means tensorflow, th means theano)</p><p>C) In Keras, <strong>subsample=(2,2)</strong> means you down sample the image size from (80x80) to (40x40). In ML literature it is often called “stride”</p><p>D) We have used an adaptive learning algorithm called ADAM to do the optimization. The learning rate is <strong>1-e6</strong>.</p><p>Interested readers who want to learn more various learning algoithms please read below</p><p><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">An overview of gradient descent optimization algorithms</a></p><h3 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h3><p>Finally, we can using the Q-learning algorithm to train the neural network.</p><p>So, what is Q-learning? In Q-learning the most important thing is the Q-function : Q(s, a) representing the maximum discounted future reward when we perform action <strong>a</strong> in state <strong>s</strong>. <strong>Q(s, a)</strong> gives you an estimation of how good to choose an action <strong>a</strong> in state <strong>s</strong>.</p><p>REPEAT : <strong>Q(s, a)</strong> representing the maximum discounted future reward when we perform action <strong>a</strong> in state <strong>s</strong></p><p>You might ask 1) Why Q-function is useful? 2) How can I get the Q-function?</p><p>Let me give you an analogy of the Q-function: Suppose you are playing a difficult RPG game and you don’t know how to play it well. If you have bought a strategy guide, which is an instruction book that contain hints or complete solutions to a specific video game, then playing that video game is easy. You just follow the guidiance from the strategy book. Here, Q-function is similar to a strategy guide. Suppose you are in state <strong>s</strong> and you need to decide whether you take action <strong>a</strong> or <strong>b</strong>. If you have this magical Q-function, the answers become really simple – pick the action with highest Q-value!</p><p>$$<br>{\pi(s) = {argmax}_{a} Q(s,a)}<br>$$<br>Here, $\pi$ represents the policy, which you will often see in the ML literature.</p><p>How do we get the Q-function? That’s where Q-learning is coming from. Let me quickly derive here:</p><p>Define total future reward from time <strong>t</strong> onward<br>$$<br>R_t = r_t + r_{t+1} + r_{t+2} … + r_n<br>$$<br>But because our environment is stochastic, we can never be sure, the more future we go, the more uncertain. Therefore, it is common to use <strong>discount future reward</strong> instead<br>$$<br>R_t = r_t + \gamma r_{t+1} + \gamma^{2} r_{t+2} … + \gamma^{n-t} r_n<br>$$<br>which, can be written as<br>$$<br>R_t = r_t + \gamma \ast R_{t+1}<br>$$<br>Recall the definition of Q-function (maximum discounted future reward if we choose action <strong>a</strong> in state <strong>s</strong>)<br>$$<br>Q(s_t, a_t) = max R_{t+1}<br>$$<br>therefore, we can rewrite the Q-function as below<br>$$<br>Q(s, a) = r + \gamma \ast max_{a^{‘}} Q(s^{\prime}, a^{\prime})<br>$$<br>In plain English, it means maximum future reward for this state and action (s,a) is the immediate reward r <strong>plus</strong> maximum future reward for the next state $s^{\prime}$ , action $a^{\prime}$</p><p>We could now use an iterative method to solve for the Q-function. Given a transition $(s, a, r, s^{\prime})$ , we are going to convert this episode into training set for the network. i.e. We want $r + \gamma max_a Q (s,a)$ to be equal to $Q(s,a)$. You can think of finding a Q-value is a regession task now, I have a estimator $r + \gamma max_a Q (s,a)$ and a predictor $Q(s,a)$, I can define the mean squared error (MSE), or the loss function, as below:</p><p>Define a loss function<br>$$<br>L = [r + \gamma max_{a^{\prime}} Q (s^{\prime},a^{\prime}) - Q(s,a)]^{2}<br>$$<br>Given a transition $(s, a, r, s^{\prime})$, how can I optimize my Q-function such that it returns the smallest mean squared error loss? If L getting smaller, we know the Q-function is getting converged into the optimal value, which is our “strategy book”.</p><p>Now, you might ask, where is the role of the neural network? This is where the <strong>DEEP Q-Learning</strong> comes in. You recall that $Q(s,a)$, is a stategy book, which contains millions or trillions of states and actions if you display as a table. The idea of the DQN is that I use the neural network to <strong>COMPRESS</strong> this Q-table, using some parameters \thetaθ <strong>(We called it weight in Neural Network)</strong>. So instead of handling a large table, I just need to worry the weights of the neural network. By smartly tuning the weight parameters, I can find the optimal Q-function via the various Neural Network training algorithm.<br>$$<br>Q(s,a) = f_{\theta}(s)<br>$$<br>where $f$ is our neural network with input $s$ and weight parameters $\theta$</p><p>Here is the code below to demonstrate how it works</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> t &gt; OBSERVE:</div><div class="line">    <span class="comment">#sample a minibatch to train on</span></div><div class="line">    minibatch = random.sample(D, BATCH)</div><div class="line"></div><div class="line">    inputs = np.zeros((BATCH, s_t.shape[<span class="number">1</span>], s_t.shape[<span class="number">2</span>], s_t.shape[<span class="number">3</span>]))   <span class="comment">#32, 80, 80, 4</span></div><div class="line">    targets = np.zeros((inputs.shape[<span class="number">0</span>], ACTIONS))                         <span class="comment">#32, 2</span></div><div class="line"></div><div class="line">    <span class="comment">#Now we do the experience replay</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(minibatch)):</div><div class="line">        state_t = minibatch[i][<span class="number">0</span>]</div><div class="line">        action_t = minibatch[i][<span class="number">1</span>]   <span class="comment">#This is action index</span></div><div class="line">        reward_t = minibatch[i][<span class="number">2</span>]</div><div class="line">        state_t1 = minibatch[i][<span class="number">3</span>]</div><div class="line">        terminal = minibatch[i][<span class="number">4</span>]</div><div class="line">        <span class="comment"># if terminated, only equals reward</span></div><div class="line"></div><div class="line">        inputs[i:i + <span class="number">1</span>] = state_t    <span class="comment">#I saved down s_t</span></div><div class="line"></div><div class="line">        targets[i] = model.predict(state_t)  <span class="comment"># Hitting each buttom probability</span></div><div class="line">        Q_sa = model.predict(state_t1)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> terminal:</div><div class="line">            targets[i, action_t] = reward_t</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)</div><div class="line"></div><div class="line">        loss += model.train_on_batch(inputs, targets)</div><div class="line"></div><div class="line">    s_t = s_t1</div><div class="line">    t = t + <span class="number">1</span></div></pre></td></tr></table></figure><h3 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h3><p>If you examine the code above, there is a comment called “Experience Replay”. Let me explain what it does: It was found that approximation of Q-value using non-linear functions like neural network is not very stable. The most important trick to solve this problem is called <strong>experience replay</strong>. During the gameplay all the episode $(s, a, r, s^{‘})$ are stored in replay memory <strong>D</strong>. (I use Python function deque() to store it). When training the network, random mini-batches from the replay memory are used instead of most the recent transition, which will greatly improve the stability.</p><h3 id="Exploration-vs-Exploitation"><a href="#Exploration-vs-Exploitation" class="headerlink" title="Exploration vs. Exploitation"></a>Exploration vs. Exploitation</h3><p>There is another issue in the reinforcement learning algorithm which called Exploration vs. Exploitation. How much of an agent’s time should be spent exploiting its existing known-good policy, and how much time should be focused on exploring new, possibility better, actions? We often encounter similar situations in real life too. For example, we face on which restaurant to go to on Saturday night. We all have a set of restaurants that we prefer, based on our policy/strategy book $Q(s,a)$. If we stick to our normal preference, there is a strong probability that we’ll pick a good restaurant. However, sometimes, we occasionally like to try new restaurants to see if they are better. The RL agents face the same problem. In order to maximize future reward, they need to balance the amount of time that they follow their current policy (this is called being “greedy”), and the time they spend exploring new possibilities that might be better. A popular approach is called $\epsilon$ greedy approach. Under this approach, the policy tells the agent to try a random action some percentage of the time, as defined by the variable $\epsilon$ (epsilon), which is a number between 0 and 1. The strategy will help the RL agent to occasionally try something new and see if we can achieve ultimate strategy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> random.random() &lt;= epsilon:</div><div class="line">    print(<span class="string">"----------Random Action----------"</span>)</div><div class="line">    action_index = random.randrange(ACTIONS)</div><div class="line">    a_t[action_index] = <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">    q = model.predict(s_t)       <span class="comment">#input a stack of 4 images, get the prediction</span></div><div class="line">    max_Q = np.argmax(q)</div><div class="line">    action_index = max_Q</div><div class="line">    a_t[max_Q] = <span class="number">1</span></div></pre></td></tr></table></figure><p>I think that’s it. I hope this blog will help you to understand how DQN works.</p><h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h3 id="My-training-is-very-slow"><a href="#My-training-is-very-slow" class="headerlink" title="My training is very slow"></a>My training is very slow</h3><p>You might need a GPU to accelerate the calculation. I used a TITAN X and train for at least 1 million frames to make it work</p><h1 id="Future-works-and-thoughts"><a href="#Future-works-and-thoughts" class="headerlink" title="Future works and thoughts"></a>Future works and thoughts</h1><ol><li>Current DQN depends on large experience replay. Is it possible to replace it or even remove it?</li><li>How can one decide on the optimal Convolution Neural Network?</li><li>Training is very slow, how to speed it up/to make the model converge faster?</li><li>What does the Neural Network actually learn? Is the knowledge transferable?</li></ol><p>I believe the questions are still not resolved and it’s an active research area in Machine Learning.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. <strong>Human-level Control through Deep Reinforcement Learning</strong>. Nature, 529-33, 2015.</p><h1 id="Disclaimer"><a href="#Disclaimer" class="headerlink" title="Disclaimer"></a>Disclaimer</h1><p>This work is highly based on the following repos:</p><p><a href="https://github.com/yenchenlin/DeepLearningFlappyBird" target="_blank" rel="external">https://github.com/yenchenlin/DeepLearningFlappyBird</a></p><p><a href="http://edersantana.github.io/articles/keras_rl/" target="_blank" rel="external">http://edersantana.github.io/articles/keras_rl/</a></p><h1 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h1><p>I must thank to <a href="https://twitter.com/hardmaru" target="_blank" rel="external">@hardmaru</a> to encourage me to write this blog. I also thank to <a href="https://twitter.com/fchollet" target="_blank" rel="external">@fchollet</a> to help me on the weight initialization in Keras and <a href="https://twitter.com/edersantana" target="_blank" rel="external">@edersantana</a> his post on Keras and reinforcement learning which really help me to understand it.</p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/07/Demystifying-Deep-Reinforcement-Learning/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/07/Demystifying-Deep-Reinforcement-Learning/" itemprop="url">Demystifying Deep Reinforcement Learning (Repost)</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-07T01:36:44+08:00">2017-07-07 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/07/Demystifying-Deep-Reinforcement-Learning/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/07/Demystifying-Deep-Reinforcement-Learning/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/07/Demystifying-Deep-Reinforcement-Learning/" class="leancloud_visitors" data-flag-title="Demystifying Deep Reinforcement Learning (Repost)"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>Two years ago, a small company in London called DeepMind uploaded their pioneering paper “<a href="http://arxiv.org/abs/1312.5602" target="_blank" rel="external">Playing Atari with Deep Reinforcement Learning</a>” to Arxiv. In this paper they demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable, because the games and the goals in every game were very different and designed to be challenging for humans. The same model architecture, without any change, was used to learn seven different games, and in three of them the algorithm performed even better than a human!</p><p>It has been hailed since then as the first step towards <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence" target="_blank" rel="external">general artificial intelligence</a> – an AI that can survive in a variety of environments, instead of being confined to strict realms such as playing chess. No wonder <a href="http://techcrunch.com/2014/01/26/google-deepmind/" target="_blank" rel="external">DeepMind was immediately bought by Google</a> and has been on the forefront of deep learning research ever since. In February 2015 their paper “<a href="http://www.nature.com/articles/nature14236" target="_blank" rel="external">Human-level control through deep reinforcement learning</a>” was featured on the cover of Nature, one of the most prestigious journals in science. In this paper they applied the same model to 49 different games and achieved superhuman performance in half of them.</p><p>Still, while deep models for supervised and unsupervised learning have seen widespread adoption in the community, deep reinforcement learning has remained a bit of a mystery. In this blog post I will be trying to demystify this technique and understand the rationale behind it. The intended audience is someone who already has background in machine learning and possibly in neural networks, but hasn’t had time to delve into reinforcement learning yet.</p><p>The roadmap ahead:</p><ol><li><strong>What are the main challenges in reinforcement learning?</strong> We will cover the credit assignment problem and the exploration-exploitation dilemma here.</li><li><strong>How to formalize reinforcement learning in mathematical terms?</strong> We will define Markov Decision Process and use it for reasoning about reinforcement learning.</li><li><strong>How do we form long-term strategies?</strong> We define “discounted future reward”, that forms the main basis for the algorithms in the next sections.</li><li><strong>How can we estimate or approximate the future reward?</strong> Simple table-based Q-learning algorithm is defined and explained here.</li><li><strong>What if our state space is too big?</strong> Here we see how Q-table can be replaced with a (deep) neural network.</li><li><strong>What do we need to make it actually work?</strong> Experience replay technique will be discussed here, that stabilizes the learning with neural networks.</li><li><strong>Are we done yet?</strong> Finally we will consider some simple solutions to the exploration-exploitation problem.</li></ol><h1 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h1><p>Consider the game Breakout. In this game you control a paddle at the bottom of the screen and have to bounce the ball back to clear all the bricks in the upper half of the screen. Each time you hit a brick, it disappears and your score increases – you get a reward.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.08.53-AM.png" alt="img">Figure 1: Atari Breakout game. Image credit: DeepMind.</p><p>Suppose you want to teach a neural network to play this game. Input to your network would be screen images, and output would be three actions: left, right or fire (to launch the ball). It would make sense to treat it as a classification problem – for each game screen you have to decide, whether you should move left, right or press fire. Sounds straightforward? Sure, but then you need training examples, and a lots of them. Of course you could go and record game sessions using expert players, but that’s not really how we learn. We don’t need somebody to tell us a million times which move to choose at each screen. We just need occasional feedback that we did the right thing and can then figure out everything else ourselves.</p><p>This is the task <strong>reinforcement learning </strong>tries to solve. Reinforcement learning lies somewhere in between supervised and unsupervised learning. Whereas in supervised learning one has a target label for each training example and in unsupervised learning one has no labels at all, in reinforcement learning one has sparse and time-delayed labels – the rewards. Based only on those rewards the agent has to learn to behave in the environment.</p><p>While the idea is quite intuitive, in practice there are numerous challenges. For example when you hit a brick and score a reward in the Breakout game, it often has nothing to do with the actions (paddle movements) you did just before getting the reward. All the hard work was already done, when you positioned the paddle correctly and bounced the ball back. This is called the <strong>credit assignment problem</strong> – i.e., which of the preceding actions was responsible for getting the reward and to what extent.</p><p>Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards? In the above Breakout game a simple strategy is to move to the left edge and wait there. When launched, the ball tends to fly left more often than right and you will easily score about 10 points before you die. Will you be satisfied with this or do you want more? This is called the <strong>explore-exploit dilemma</strong> – should you exploit the known working strategy or explore other, possibly better strategies.</p><p>Reinforcement learning is an important model of how we (and all animals in general) learn. Praise from our parents, grades in school, salary at work – these are all examples of rewards. Credit assignment problems and exploration-exploitation dilemmas come up every day both in business and in relationships. That’s why it is important to study this problem, and games form a wonderful sandbox for trying out new approaches.</p><h1 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h1><p>Now the question is, how do you formalize a reinforcement learning problem, so that you can reason about it? The most common method is to represent it as a Markov decision process.</p><p>Suppose you are an <strong>agent</strong>, situated in an <strong>environment</strong> (e.g. Breakout game). The environment is in a certain <strong>state</strong>(e.g. location of the paddle, location and direction of the ball, existence of every brick and so on). The agent can perform certain <strong>actions</strong> in the environment (e.g. move the paddle to the left or to the right). These actions sometimes result in a <strong>reward</strong> (e.g. increase in score). Actions transform the environment and lead to a new state, where the agent can perform another action, and so on. The rules for how you choose those actions are called <strong>policy</strong>. The environment in general is stochastic, which means the next state may be somewhat random (e.g. when you lose a ball and launch a new one, it goes towards a random direction).</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-12.01.04-PM.png" alt="img">Figure 2: <em>Left: </em>reinforcement learning problem. <em>Right: </em>Markov decision process.</p><p>The set of states and actions, together with rules for transitioning from one state to another, make up a <strong>Markov decision process</strong>. One <strong>episode</strong> of this process (e.g. one game) forms a finite sequence of states, actions and rewards:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.19-AM.png" alt="Screen Shot 2015-12-21 at 11.09.19 AM"></p><p>Here si represents the state, ai is the action and ri+1 is the reward after performing the action. The episode ends with <strong>terminal</strong> state sn (e.g. “game over” screen). A Markov decision process relies on the Markov assumption, that the probability of the next state si+1 depends only on current state si and action ai, but not on preceding states or actions.</p><h1 id="Discounted-Future-Reward"><a href="#Discounted-Future-Reward" class="headerlink" title="Discounted Future Reward"></a>Discounted Future Reward</h1><p>To perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. How should we go about that?</p><p>Given one run of the Markov decision process, we can easily calculate the <strong>total reward</strong> for one episode:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.26-AM.png" alt="Screen Shot 2015-12-21 at 11.09.26 AM"></p><p>Given that, the <strong>total future reward</strong> from time point <em>t</em> onward can be expressed as:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.32-AM.png" alt="Screen Shot 2015-12-21 at 11.09.32 AM"></p><p>But because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use <strong>discounted future reward </strong>instead:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.36-AM.png" alt="Screen Shot 2015-12-21 at 11.09.36 AM"></p><p>Here <em>γ</em> is the discount factor between 0 and 1 – the more into the future the reward is, the less we take it into consideration. It is easy to see, that discounted future reward at time step <em>t</em> can be expressed in terms of the same thing at time step <em>t+1</em>:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.40-AM.png" alt="Screen Shot 2015-12-21 at 11.09.40 AM"></p><p>If we set the discount factor <em>γ</em>=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like <em>γ=</em>0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor <em>γ</em>=1.</p><p>A good strategy for an agent would be to <strong>always choose an action that maximizes the (discounted) future reward</strong>.</p><h1 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h1><p>In Q-learning we define a function <em>Q(s, a)</em> representing <strong>the maximum discounted future reward when we perform action </strong>a<strong> in state </strong>s<strong>, and continue optimally from that point on.</strong></p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.47-AM.png" alt="Screen Shot 2015-12-21 at 11.09.47 AM"></p><p>The way to think about <em>Q(s, a)</em> is that it is “the best possible score at the end of the game after performing action a<strong>in state </strong>s<strong>“. It is called Q-function, because it represents the “quality” of a certain action in a given state.</strong></p><p>This may sound like quite a puzzling definition. How can we estimate the score at the end of game, if we know just the current state and action, and not the actions and rewards coming after that? We really can’t. But as a theoretical construct we can assume existence of such a function. Just close your eyes and repeat to yourself five times: “<em>Q(s, a) </em>exists, <em>Q(s, a) </em>exists, …”. Feel it?</p><p>If you’re still not convinced, then consider what the implications of having such a function would be. Suppose you are in state and pondering whether you should take action <em>a</em> or <em>b</em>. You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer becomes really simple – pick the action with the highest Q-value!</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.09.56-AM.png" alt="Screen Shot 2015-12-21 at 11.09.56 AM"></p><p>Here π represents the policy, the rule how we choose an action in each state.</p><p>OK, how do we get that Q-function then? Let’s focus on just one transition &lt;<em>s, a, r, s’</em>&gt;. Just like with discounted future rewards in the previous section, we can express the Q-value of state <em>s</em> and action <em>a</em> in terms of the Q-value of the next state <em>s’</em>.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.10.00-AM.png" alt="Screen Shot 2015-12-21 at 11.10.00 AM">This is called the <strong>Bellman equation</strong>. If you think about it, it is quite logical – maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state.</p><p>The main idea in Q-learning is that <strong>we can iteratively approximate the Q-function using the Bellman equation</strong>. In the simplest case the Q-function is implemented as a table, with states as rows and actions as columns. The gist of the Q-learning algorithm is as simple as the following<a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/#_ftn1" target="_blank" rel="external">[1]</a>:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.55-AM.png" alt="Screen Shot 2015-12-21 at 11.23.55 AM"></p><p><em>α</em> in the algorithm is a learning rate that controls how much of the difference between previous Q-value and newly proposed Q-value is taken into account. In particular, when <em>α</em>=1, then two <em>Q[s,a]</em> cancel and the update is exactly the same as the Bellman equation.</p><p>The max<em>a’</em> <em>Q</em>[<em>s’</em>,<em>a’</em>] that we use to update <em>Q</em>[<em>s</em>,<em>a</em>] is only an approximation and in early stages of learning it may be completely wrong. However the approximation get more and more accurate with every iteration and <a href="http://simplecore-dev.intel.com/nervana/wp-content/uploads/sites/55/2015/12/ProofQlearning.pdf" target="_blank" rel="external">it has been shown</a>, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.</p><h1 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a>Deep Q Network</h1><p>The state of the environment in the Breakout game can be defined by the location of the paddle, location and direction of the ball and the presence or absence of each individual brick. This intuitive representation however is game specific. Could we come up with something more universal, that would be suitable for all the games? The obvious choice is screen pixels – they implicitly contain all of the relevant information about the game situation, except for the speed and direction of the ball. Two consecutive screens would have these covered as well.</p><p>If we apply the same preprocessing to game screens as in the DeepMind paper – take the four last screen images, resize them to 84×84 and convert to grayscale with 256 gray levels – we would have 25684x84x4 ≈ 1067970 possible game states. This means 1067970 rows in our imaginary Q-table – more than the number of atoms in the known universe! One could argue that many pixel combinations (and therefore states) never occur – we could possibly represent it as a sparse table containing only visited states. Even so, most of the states are very rarely visited and it would take a lifetime of the universe for the Q-table to converge. Ideally, we would also like to have a good guess for Q-values for states we have never seen before.</p><p>This is the point where deep learning steps in. Neural networks are exceptionally good at coming up with good features for highly structured data. We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. This approach has the advantage, that if we want to perform a Q-value update or pick the action with the highest Q-value, we only have to do one forward pass through the network and have all Q-values for all actions available immediately.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.27.12-AM.png" alt="img"></p><p>Figure 3: <em>Left: </em>Naive formulation of deep Q-network. <em>Right: </em>More optimized architecture of deep Q-network, used in DeepMind paper.</p><p>The network architecture that DeepMind used is as follows:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.28-AM.png" alt="Screen Shot 2015-12-21 at 11.23.28 AM"></p><p>This is a classical convolutional neural network with three convolutional layers, followed by two fully connected layers. People familiar with object recognition networks may notice that there are no pooling layers. But if you really think about it, pooling layers buy you translation invariance – the network becomes insensitive to the location of an object in the image. That makes perfectly sense for a classification task like ImageNet, but for games the location of the ball is crucial in determining the potential reward and we wouldn’t want to discard this information!</p><p>Input to the network are four 84×84 grayscale game screens. Outputs of the network are Q-values for each possible action (18 in Atari). Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/formula.png" alt="img"></p><p>Given a transition &lt;<em> s, a, r, s’</em> &gt;, the Q-table update rule in the previous algorithm must be replaced with the following:</p><ol><li>Do a feedforward pass for the current state <em>s</em> to get predicted Q-values for all actions.</li><li>Do a feedforward pass for the next state <em>s’ </em>and calculate maximum overall network outputs <em>max a’ Q(s’, a’).</em></li><li>Set Q-value target for action to <em>r + γmax a’ Q(s’, a’)</em> (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.</li><li>Update the weights using backpropagation.</li></ol><h1 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h1><p>By now we have an idea how to estimate the future reward in each state using Q-learning and approximate the Q-function using a convolutional neural network. But it turns out that approximation of Q-values using non-linear functions is not very stable. There is a whole bag of tricks that you have to use to actually make it converge. And it takes a long time, almost a week on a single GPU.</p><p>The most important trick is <strong>experience replay</strong>. During gameplay all the experiences &lt;<em> s, a, r, s’</em> &gt; are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.</p><h1 id="Exploration-Exploitation"><a href="#Exploration-Exploitation" class="headerlink" title="Exploration-Exploitation"></a>Exploration-Exploitation</h1><p>Q-learning attempts to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward. But we haven’t touched the exploration-exploitation dilemma yet…</p><p>Firstly observe, that when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude “exploration”. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is “greedy”, it settles with the first effective strategy it finds.</p><p>A simple and effective fix for the above problem is <strong>ε-greedy exploration</strong> – with probability <em>ε</em> choose a random action, otherwise go with the “greedy” action with the highest Q-value. In their system DeepMind actually decreases <em>ε</em> over time from 1 to 0.1 – in the beginning the system makes completely random moves to explore the state space maximally, and then it settles down to a fixed exploration rate.</p><h1 id="Deep-Q-learning-Algorithm"><a href="#Deep-Q-learning-Algorithm" class="headerlink" title="Deep Q-learning Algorithm"></a>Deep Q-learning Algorithm</h1><p>This gives us the final deep Q-learning algorithm with experience replay:</p><p><img src="https://www.intelnervana.com/wp-content/uploads/sites/53/2017/06/Screen-Shot-2015-12-21-at-11.23.43-AM-1.png" alt="Screen Shot 2015-12-21 at 11.23.43 AM"></p><p>There are many more tricks that DeepMind used to actually make it work – like target network, error clipping, reward clipping etc, but these are out of scope for this introduction.</p><p>The most amazing part of this algorithm is that it learns anything at all. Just think about it – because our Q-function is initialized randomly, it initially outputs complete garbage. And we are using this garbage (the maximum Q-value of the next state) as targets for the network, only occasionally folding in a tiny reward. That sounds insane, how could it learn anything meaningful at all? The fact is, that it does.</p><h1 id="Final-notes"><a href="#Final-notes" class="headerlink" title="Final notes"></a>Final notes</h1><p>Many improvements to deep Q-learning have been proposed since its first introduction – <a href="http://arxiv.org/abs/1509.06461" target="_blank" rel="external">Double Q-learning</a>, <a href="http://arxiv.org/abs/1511.05952" target="_blank" rel="external">Prioritized Experience Replay</a>, <a href="http://arxiv.org/abs/1511.06581" target="_blank" rel="external">Dueling Network Architecture</a> and <a href="http://arxiv.org/abs/1509.02971" target="_blank" rel="external">extension to continuous action space</a> to name a few. For latest advancements check out the <a href="http://rll.berkeley.edu/deeprlworkshop/" target="_blank" rel="external">NIPS 2015 deep reinforcement learning workshop</a> and <a href="https://cmt.research.microsoft.com/ICLR2016Conference/Protected/PublicComment.aspx" target="_blank" rel="external">ICLR 2016</a>(search for “reinforcement” in title). But beware, that <a href="http://www.google.com/patents/US20150100530" target="_blank" rel="external">deep Q-learning has been patented by Google</a>.</p><p>It is often said, that artificial intelligence is something we haven’t figured out yet. Once we know how it works, it doesn’t seem intelligent any more. But deep Q-networks still continue to amaze me. Watching them figure out a new game is like observing an animal in the wild – a rewarding experience by itself.</p><h1 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a>Credits</h1><p>Thanks to Ardi Tampuu, Tanel Pärnamaa, Jaan Aru, Ilya Kuzovkin, Arjun Bansal and Urs Köster for comments and suggestions on the drafts of this post.</p><h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><ul><li><a href="http://videolectures.net/rldm2015_silver_reinforcement_learning/" target="_blank" rel="external">David Silver’s lecture about deep reinforcement learning</a></li><li><a href="https://www.youtube.com/watch?v=b1a53hE0yQs" target="_blank" rel="external">Slightly awkward but accessible illustration of Q-learning</a></li><li><a href="http://rll.berkeley.edu/deeprlcourse/" target="_blank" rel="external">UC Berkley’s course on deep reinforcement learning</a></li><li><a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html" target="_blank" rel="external">David Silver’s reinforcement learning course</a></li><li><a href="https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/" target="_blank" rel="external">Nando de Freitas’ course on machine learning</a> (two lectures about reinforcement learning in the end)</li><li><a href="http://cs231n.github.io/" target="_blank" rel="external">Andrej Karpathy’s course on convolutional neural networks</a></li></ul><p><a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/#_ftnref1" target="_blank" rel="external">[1]</a> Algorithm adapted from <a href="http://artint.info/html/ArtInt_265.html" target="_blank" rel="external">http://artint.info/html/ArtInt_265.html</a><br>This blog was first published at: <a href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/</a></p><h4 id="This-is-the-part-1-of-my-series-on-deep-reinforcement-learning-Tune-in-next-week-for-“Deep-Reinforcement-Learning-with-Neon”-for-an-actual-implementation-with-Neon-deep-learning-toolkit"><a href="#This-is-the-part-1-of-my-series-on-deep-reinforcement-learning-Tune-in-next-week-for-“Deep-Reinforcement-Learning-with-Neon”-for-an-actual-implementation-with-Neon-deep-learning-toolkit" class="headerlink" title="This is the part 1 of my series on deep reinforcement learning. Tune in next week for “Deep Reinforcement Learning with Neon” for an actual implementation with Neon deep learning toolkit."></a>This is the part 1 of my series on deep reinforcement learning. Tune in next week for <a href="https://www.intelnervana.com/deep-reinforcement-learning-with-neon/" target="_blank" rel="external">“Deep Reinforcement Learning with Neon”</a> for an actual implementation with <a href="https://github.com/NervanaSystems/neon" target="_blank" rel="external">Neon</a> deep learning toolkit.</h4></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/07/06/On-policy-Control-with-Approximation/"><span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ewan Li"><meta itemprop="description" content=""><meta itemprop="image" content="/images/avatar.jpg"></span><span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Abracadabra"><span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject"><img style="display:none" itemprop="url image" alt="Abracadabra" src=""></span></span><header class="post-header"><h1 class="post-title" itemprop="name headline"><a class="post-title-link" href="/2017/07/06/On-policy-Control-with-Approximation/" itemprop="url">On-policy Control with Approximation</a></h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-07-06T17:41:34+08:00">2017-07-06 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2017/07/06/On-policy-Control-with-Approximation/#comments" itemprop="discussionUrl"><span class="post-comments-count hc-comment-count" data-xid="2017/07/06/On-policy-Control-with-Approximation/" itemprop="commentsCount"></span> </a></span><span id="/2017/07/06/On-policy-Control-with-Approximation/" class="leancloud_visitors" data-flag-title="On-policy Control with Approximation"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Visitors </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a finite-dimensional weight vector.</p><h3 id="Episodic-Semi-gradient-Control"><a href="#Episodic-Semi-gradient-Control" class="headerlink" title="Episodic Semi-gradient Control"></a>Episodic Semi-gradient Control</h3><p>The general gradient-descent update for action-value prediction is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>For example, the update for the one-step Sarsa method is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t)- \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).<br>$$<br>We call this method <strong>episode semi-gradient one-step sarsa</strong>.</p><p>To form control methods, we need to couple such action-value prediction methods with techniques for policy improvement and action selection. Suitable techniques applicable to continuous actions, or to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution. On the other hand, if the action set is discrete and not too large, then we can use the techniques already developed in previous chapters.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/episode-semi-grad-sarsa.png" alt="episode-semi-grad-sarsa"></p><h4 id="Example-Mountain-Car-Task"><a href="#Example-Mountain-Car-Task" class="headerlink" title="Example: Mountain-Car Task"></a>Example: Mountain-Car Task</h4><p>Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram in the below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mountain-car.png" alt="mountain-car"></p><p>The diﬃculty is that gravity is stronger than the car’s engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to ﬁrst move away from the goal and up the opposite slope on the left. Then, by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way. This is a simple example of a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. Many control methodologies have great diﬃculties with tasks of this kind unless explicitly aided by a human designer.</p><p>The reward in this problem is −1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (−1), and zero throttle (0). The car moves according to a simpliﬁed physics. Its position, $x_t$, and velocity, $\dot{x}_t$, are updated by<br>$$<br>\begin{align}<br>x_{t+1} &amp;\doteq bound \big[x_t + \dot{x}_{t+1} \big] \\<br>\dot{x}_{t+1} &amp;\doteq bound \big[\dot{x}_t + 0.001 A_t - 0.0025 \cos(3x_t) \big],<br>\end{align}<br>$$<br>where the bound operation enforces $-1.2 \leq x_{t+1} \leq 0.5 \;\; \text{and} \; -0.07 \leq \dot{x}_{t+1} \leq 0.07$. In addition, when $x_{t+1}$ reached the left bound, $\dot{x}_{t+1}$ was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position $x_t \in [−0.6, −0.4)$ and zero velocity. To convert the two continuous state variables to binary features, we used grid-tilings.</p><p>First of all, we define the environment of this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># all possible actions</span></div><div class="line">ACTION_REVERSE = <span class="number">-1</span></div><div class="line">ACTION_ZERO = <span class="number">0</span></div><div class="line">ACTION_FORWARD = <span class="number">1</span></div><div class="line"><span class="comment"># order is important</span></div><div class="line">ACTIONS = [ACTION_REVERSE, ACTION_ZERO, ACTION_FORWARD]</div><div class="line"></div><div class="line"><span class="comment"># bound for position and velocity</span></div><div class="line">POSITION_MIN = <span class="number">-1.2</span></div><div class="line">POSITION_MAX = <span class="number">0.5</span></div><div class="line">VELOCITY_MIN = <span class="number">-0.07</span></div><div class="line">VELOCITY_MAX = <span class="number">0.07</span></div><div class="line"></div><div class="line"><span class="comment"># use optimistic initial value, so it's ok to set epsilon to 0</span></div><div class="line">EPSILON = <span class="number">0</span></div></pre></td></tr></table></figure><p>After take an action, we transition to a new state and get a reward:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># take an @action at @position and @velocity</span></div><div class="line"><span class="comment"># @return: new position, new velocity, reward (always -1)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeAction</span><span class="params">(position, velocity, action)</span>:</span></div><div class="line">    newVelocity = velocity + <span class="number">0.001</span> * action - <span class="number">0.0025</span> * np.cos(<span class="number">3</span> * position)</div><div class="line">    newVelocity = min(max(VELOCITY_MIN, newVelocity), VELOCITY_MAX)</div><div class="line">    newPosition = position + newVelocity</div><div class="line">    newPosition = min(max(POSITION_MIN, newPosition), POSITION_MAX)</div><div class="line">    reward = <span class="number">-1.0</span></div><div class="line">    <span class="keyword">if</span> newPosition == POSITION_MIN:</div><div class="line">        newVelocity = <span class="number">0.0</span></div><div class="line">    <span class="keyword">return</span> newPosition, newVelocity, reward</div></pre></td></tr></table></figure><p>The $\varepsilon$-greedy policy:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># get action at @position and @velocity based on epsilon greedy policy and @valueFunction</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(position, velocity, valueFunction)</span>:</span></div><div class="line">    <span class="keyword">if</span> np.random.binomial(<span class="number">1</span>, EPSILON) == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> np.random.choice(ACTIONS)</div><div class="line">    values = []</div><div class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">        values.append(valueFunction.value(position, velocity, action))</div><div class="line">    <span class="keyword">return</span> np.argmax(values) - <span class="number">1</span></div></pre></td></tr></table></figure><p>We need map out continuous state to discrete state:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># wrapper class for state action value function</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ValueFunction</span>:</span></div><div class="line">    <span class="comment"># In this example I use the tiling software instead of implementing standard tiling by myself</span></div><div class="line">    <span class="comment"># One important thing is that tiling is only a map from (state, action) to a series of indices</span></div><div class="line">    <span class="comment"># It doesn't matter whether the indices have meaning, only if this map satisfy some property</span></div><div class="line">    <span class="comment"># View the following webpage for more information</span></div><div class="line">    <span class="comment"># http://incompleteideas.net/sutton/tiles/tiles3.html</span></div><div class="line">    <span class="comment"># @maxSize: the maximum # of indices</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, stepSize, numOfTilings=<span class="number">8</span>, maxSize=<span class="number">2048</span>)</span>:</span></div><div class="line">        self.maxSize = maxSize</div><div class="line">        self.numOfTilings = numOfTilings</div><div class="line"></div><div class="line">        <span class="comment"># divide step size equally to each tiling</span></div><div class="line">        self.stepSize = stepSize / numOfTilings</div><div class="line"></div><div class="line">        self.hashTable = IHT(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># weight for each tile</span></div><div class="line">        self.weights = np.zeros(maxSize)</div><div class="line"></div><div class="line">        <span class="comment"># position and velocity needs scaling to satisfy the tile software</span></div><div class="line">        self.positionScale = self.numOfTilings / (POSITION_MAX - POSITION_MIN)</div><div class="line">        self.velocityScale = self.numOfTilings / (VELOCITY_MAX - VELOCITY_MIN)</div><div class="line"></div><div class="line">    <span class="comment"># get indices of active tiles for given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getActiveTiles</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="comment"># I think positionScale * (position - position_min) would be a good normalization.</span></div><div class="line">        <span class="comment"># However positionScale * position_min is a constant, so it's ok to ignore it.</span></div><div class="line">        activeTiles = tiles(self.hashTable, self.numOfTilings,</div><div class="line">                            [self.positionScale * position, self.velocityScale * velocity],</div><div class="line">                            [action])</div><div class="line">        <span class="keyword">return</span> activeTiles</div><div class="line"></div><div class="line">    <span class="comment"># estimate the value of given state and action</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span><span class="params">(self, position, velocity, action)</span>:</span></div><div class="line">        <span class="keyword">if</span> position == POSITION_MAX:</div><div class="line">            <span class="keyword">return</span> <span class="number">0.0</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        <span class="keyword">return</span> np.sum(self.weights[activeTiles])</div><div class="line"></div><div class="line">    <span class="comment"># learn with given state, action and target</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(self, position, velocity, action, target)</span>:</span></div><div class="line">        activeTiles = self.getActiveTiles(position, velocity, action)</div><div class="line">        estimation = np.sum(self.weights[activeTiles])</div><div class="line">        delta = self.stepSize * (target - estimation)</div><div class="line">        <span class="keyword">for</span> activeTile <span class="keyword">in</span> activeTiles:</div><div class="line">            self.weights[activeTile] += delta</div><div class="line"></div><div class="line">    <span class="comment"># get # of steps to reach the goal under current state value function</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">costToGo</span><span class="params">(self, position, velocity)</span>:</span></div><div class="line">        costs = []</div><div class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> ACTIONS:</div><div class="line">            costs.append(self.value(position, velocity, action))</div><div class="line">        <span class="keyword">return</span> -np.max(costs)</div></pre></td></tr></table></figure><p>Because the one-step semi-gradient sarsa is a special case of n-step, so we develop the n-step algorithm<br>$$<br>G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \; n \geq 1,0 \leq t \leq T-n.<br>$$<br>The n-step equation is<br>$$<br>\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ G_{t:t+n}- \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}), \;\;\; 0 \leq t \leq T.<br>$$<br>Complete pseudocode is given in the box below.</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/n-step-sg-sarsa.png" alt="n-step-sg-sarsa"></p><p>So the code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># semi-gradient n-step Sarsa</span></div><div class="line"><span class="comment"># @valueFunction: state value function to learn</span></div><div class="line"><span class="comment"># @n: # of steps</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">semiGradientNStepSarsa</span><span class="params">(valueFunction, n=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="comment"># start at a random position around the bottom of the valley</span></div><div class="line">    currentPosition = np.random.uniform(<span class="number">-0.6</span>, <span class="number">-0.4</span>)</div><div class="line">    <span class="comment"># initial velocity is 0</span></div><div class="line">    currentVelocity = <span class="number">0.0</span></div><div class="line">    <span class="comment"># get initial action</span></div><div class="line">    currentAction = getAction(currentPosition, currentVelocity, valueFunction)</div><div class="line"></div><div class="line">    <span class="comment"># track previous position, velocity, action and reward</span></div><div class="line">    positions = [currentPosition]</div><div class="line">    velocities = [currentVelocity]</div><div class="line">    actions = [currentAction]</div><div class="line">    rewards = [<span class="number">0.0</span>]</div><div class="line"></div><div class="line">    <span class="comment"># track the time</span></div><div class="line">    time = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="comment"># the length of this episode</span></div><div class="line">    T = float(<span class="string">'inf'</span>)</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># go to next time step</span></div><div class="line">        time += <span class="number">1</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> time &lt; T:</div><div class="line">            <span class="comment"># take current action and go to the new state</span></div><div class="line">            newPostion, newVelocity, reward = takeAction(currentPosition, currentVelocity, currentAction)</div><div class="line">            <span class="comment"># choose new action</span></div><div class="line">            newAction = getAction(newPostion, newVelocity, valueFunction)</div><div class="line"></div><div class="line">            <span class="comment"># track new state and action</span></div><div class="line">            positions.append(newPostion)</div><div class="line">            velocities.append(newVelocity)</div><div class="line">            actions.append(newAction)</div><div class="line">            rewards.append(reward)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> newPostion == POSITION_MAX:</div><div class="line">                T = time</div><div class="line"></div><div class="line">        <span class="comment"># get the time of the state to update</span></div><div class="line">        updateTime = time - n</div><div class="line">        <span class="keyword">if</span> updateTime &gt;= <span class="number">0</span>:</div><div class="line">            returns = <span class="number">0.0</span></div><div class="line">            <span class="comment"># calculate corresponding rewards</span></div><div class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> range(updateTime + <span class="number">1</span>, min(T, updateTime + n) + <span class="number">1</span>):</div><div class="line">                returns += rewards[t]</div><div class="line">            <span class="comment"># add estimated state action value to the return</span></div><div class="line">            <span class="keyword">if</span> updateTime + n &lt;= T:</div><div class="line">                returns += valueFunction.value(positions[updateTime + n],</div><div class="line">                                               velocities[updateTime + n],</div><div class="line">                                               actions[updateTime + n])</div><div class="line">            <span class="comment"># update the state value function</span></div><div class="line">            <span class="keyword">if</span> positions[updateTime] != POSITION_MAX:</div><div class="line">                valueFunction.learn(positions[updateTime], velocities[updateTime], actions[updateTime], returns)</div><div class="line">        <span class="keyword">if</span> updateTime == T - <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        currentPosition = newPostion</div><div class="line">        currentVelocity = newVelocity</div><div class="line">        currentAction = newAction</div><div class="line"></div><div class="line">    <span class="keyword">return</span> time</div></pre></td></tr></table></figure><p>Next, we use the method mentioned earlier to solve this problem:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">episodes = <span class="number">9000</span></div><div class="line">targetEpisodes = [<span class="number">1</span><span class="number">-1</span>, <span class="number">12</span><span class="number">-1</span>, <span class="number">104</span><span class="number">-1</span>, <span class="number">1000</span><span class="number">-1</span>, episodes - <span class="number">1</span>]</div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alpha = <span class="number">0.3</span></div><div class="line">valueFunction = ValueFunction(alpha, numOfTilings)</div><div class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">    print(<span class="string">'episode:'</span>, episode)</div><div class="line">    semiGradientNStepSarsa(valueFunction)</div><div class="line">    <span class="keyword">if</span> episode <span class="keyword">in</span> targetEpisodes:</div><div class="line">        prettyPrint(valueFunction, <span class="string">'Episode: '</span> + str(episode + <span class="number">1</span>))</div></pre></td></tr></table></figure><p>Result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa.png" alt="mcar-sg-sarsa"></p><p>The result shows what typically happens while learning to solve this task with this form of function approximation. Shown is the negative of the value function (the cost-to-go function) learned on a single run. The initial action values were all zero, which was optimistic (all true values are negative in this task), causing extensive exploration to occur even though the exploration parameter, $\varepsilon$, was 0. All the states visited frequently are valued worse than unexplored states, because the actual rewards have been worse than what was (unrealistically) expected. This continually drives the agent away from wherever it has been, to explore new states, until a solution is found.</p><p>Next, let us test the performance of various step size (learning rate).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'alpha:'</span>, alphas[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'alpha = '</span>+str(alphas[i])+<span class="string">'/'</span>+str(numOfTilings))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha.png" alt="mcar-sg-sarsa-var-alpha"></p><p>And then, let us test the performance of one-step sarsa and multi-step sarsa on the Mountain Car task:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">runs = <span class="number">10</span></div><div class="line">episodes = <span class="number">500</span></div><div class="line">numOfTilings = <span class="number">8</span></div><div class="line">alphas = [<span class="number">0.5</span>, <span class="number">0.3</span>]</div><div class="line">nSteps = [<span class="number">1</span>, <span class="number">8</span>]</div><div class="line"></div><div class="line">steps = np.zeros((len(alphas), episodes))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    valueFunctions = [ValueFunction(alpha, numOfTilings) <span class="keyword">for</span> alpha <span class="keyword">in</span> alphas]</div><div class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">0</span>, len(valueFunctions)):</div><div class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">            print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nSteps[index], <span class="string">'episode:'</span>, episode)</div><div class="line">            step = semiGradientNStepSarsa(valueFunctions[index], nSteps[index])</div><div class="line">            steps[index, episode] += step</div><div class="line"></div><div class="line">steps /= runs</div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(alphas)):</div><div class="line">    plt.plot(steps[i], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'Episode'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.yscale(<span class="string">'log'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-n.png" alt="mcar-sg-sarsa-var-n"></p><p>Finally, let me shows the results of a more detailed study of the eﬀect of the parameters $\alpha$ and $n$ on the rate of learning on this task.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">alphas = np.arange(<span class="number">0.25</span>, <span class="number">1.75</span>, <span class="number">0.25</span>)</div><div class="line">nSteps = np.power(<span class="number">2</span>, np.arange(<span class="number">0</span>, <span class="number">5</span>))</div><div class="line">episodes = <span class="number">50</span></div><div class="line">runs = <span class="number">5</span></div><div class="line"></div><div class="line">truncateStep = <span class="number">300</span></div><div class="line">steps = np.zeros((len(nSteps), len(alphas)))</div><div class="line"><span class="keyword">for</span> run <span class="keyword">in</span> range(<span class="number">0</span>, runs):</div><div class="line">    <span class="keyword">for</span> nStepIndex, nStep <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(nSteps)), nSteps):</div><div class="line">        <span class="keyword">for</span> alphaIndex, alpha <span class="keyword">in</span> zip(range(<span class="number">0</span>, len(alphas)), alphas):</div><div class="line">            <span class="keyword">if</span> (nStep == <span class="number">8</span> <span class="keyword">and</span> alpha &gt; <span class="number">1</span>) <span class="keyword">or</span> \</div><div class="line">                    (nStep == <span class="number">16</span> <span class="keyword">and</span> alpha &gt; <span class="number">0.75</span>):</div><div class="line">                <span class="comment"># In these cases it won't converge, so ignore them</span></div><div class="line">                steps[nStepIndex, alphaIndex] += truncateStep * episodes</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            valueFunction = ValueFunction(alpha)</div><div class="line">            <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">0</span>, episodes):</div><div class="line">                print(<span class="string">'run:'</span>, run, <span class="string">'steps:'</span>, nStep, <span class="string">'alpha:'</span>, alpha, <span class="string">'episode:'</span>, episode)</div><div class="line">                step = semiGradientNStepSarsa(valueFunction, nStep)</div><div class="line">                steps[nStepIndex, alphaIndex] += step</div><div class="line"><span class="comment"># average over independent runs and episodes</span></div><div class="line">steps /= runs * episodes</div><div class="line"><span class="comment"># truncate high values for better display</span></div><div class="line">steps[steps &gt; truncateStep] = truncateStep</div><div class="line"></div><div class="line"><span class="keyword">global</span> figureIndex</div><div class="line">plt.figure(figureIndex)</div><div class="line">figureIndex += <span class="number">1</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(nSteps)):</div><div class="line">    plt.plot(alphas, steps[i, :], label=<span class="string">'n = '</span>+str(nSteps[i]))</div><div class="line">plt.xlabel(<span class="string">'alpha * number of tilings(8)'</span>)</div><div class="line">plt.ylabel(<span class="string">'Steps per episode'</span>)</div><div class="line">plt.legend()</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-sg-sarsa-var-alpha-n.png" alt="mcar-sg-sarsa-var-alpha-n"></p><h3 id="Update"><a href="#Update" class="headerlink" title="Update"></a>Update</h3><h4 id="Use-OpenAI-gym"><a href="#Use-OpenAI-gym" class="headerlink" title="Use OpenAI gym"></a>Use OpenAI gym</h4><p>Now, let us use the OpenAI gym toolkit to simply our Mountain Car task. As we know, we need to define the environment of task before develop the detail algorithm. It’s easy to make mistakes with some detail. So it is fantastic if we do not need to care about that. The gym toolkit help us to define the environment. Such as the Mountain Car task, there is a build-in model in the gym toolkit. We just need to write one row code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env = gym.envs.make(<span class="string">"MountainCar-v0"</span>)</div></pre></td></tr></table></figure><p>That is amazing!</p><p>We also can test the environment very convenience and get a pretty good user graphic:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">env.reset()</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line"><span class="comment"># for x in range(10000):</span></div><div class="line"><span class="comment">#     env.step(0)</span></div><div class="line"><span class="comment">#     plt.figure()</span></div><div class="line"><span class="comment">#     plt.imshow(env.render(mode='rgb_array'))  </span></div><div class="line">[env.step(<span class="number">0</span>) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)]</div><div class="line">plt.figure()</div><div class="line">plt.imshow(env.render(mode=<span class="string">'rgb_array'</span>))</div><div class="line"></div><div class="line">env.render(close=<span class="keyword">True</span>)</div></pre></td></tr></table></figure><p>These codes will return the result as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-gym-test.png" alt="mcar-gym-test"></p><p>Bravo~</p><p>Now, let us solve the task by to use the Q-Learning algorithm. Meanwhile, we will use the RBF kernel to construct the features and use the SGDRegressor gradient-descent method of scikit-learn package to update $\mathbf{w}$.</p><p>First of all, we need to normalized our features to accelerate the convergence speed and use the RBF kernel to reconstruct our feature space. The both methods (Normalization and Reconstruction) need to use some training set to train a model. Then we use this model to transform our raw data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Feature Preprocessing: Normalize to zero mean and unit variance</span></div><div class="line"><span class="comment"># We use a few samples from the observation space to do this</span></div><div class="line">observation_examples = np.array([env.observation_space.sample() <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10000</span>)])</div><div class="line">scaler = sklearn.preprocessing.StandardScaler()</div><div class="line">scaler.fit(observation_examples)</div><div class="line"></div><div class="line"><span class="comment"># Used to converte a state to a featurizes represenation.</span></div><div class="line"><span class="comment"># We use RBF kernels with different variances to cover different parts of the space</span></div><div class="line">featurizer = sklearn.pipeline.FeatureUnion([</div><div class="line">        (<span class="string">"rbf1"</span>, RBFSampler(gamma=<span class="number">5.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf2"</span>, RBFSampler(gamma=<span class="number">2.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf3"</span>, RBFSampler(gamma=<span class="number">1.0</span>, n_components=<span class="number">100</span>)),</div><div class="line">        (<span class="string">"rbf4"</span>, RBFSampler(gamma=<span class="number">0.5</span>, n_components=<span class="number">100</span>))</div><div class="line">        ])</div><div class="line">featurizer.fit(scaler.transform(observation_examples))</div></pre></td></tr></table></figure><p>Next, we define a class named Estimator to simply the gradient descent process:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Estimator</span><span class="params">()</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Value Function approximator. </div><div class="line">    """</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="comment"># We create a separate model for each action in the environment's</span></div><div class="line">        <span class="comment"># action space. Alternatively we could somehow encode the action</span></div><div class="line">        <span class="comment"># into the features, but this way it's easier to code up.</span></div><div class="line">        self.models = []</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(env.action_space.n):</div><div class="line">            model = SGDRegressor(learning_rate=<span class="string">"constant"</span>)</div><div class="line">            <span class="comment"># We need to call partial_fit once to initialize the model</span></div><div class="line">            <span class="comment"># or we get a NotFittedError when trying to make a prediction</span></div><div class="line">            <span class="comment"># This is quite hacky.</span></div><div class="line">            model.partial_fit([self.featurize_state(env.reset())], [<span class="number">0</span>])</div><div class="line">            self.models.append(model)</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">featurize_state</span><span class="params">(self, state)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Returns the featurized representation for a state.</div><div class="line">        """</div><div class="line">        scaled = scaler.transform([state])</div><div class="line">        featurized = featurizer.transform(scaled)</div><div class="line">        <span class="keyword">return</span> featurized[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, s, a=None)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Makes value function predictions.</div><div class="line">        </div><div class="line">        Args:</div><div class="line">            s: state to make a prediction for</div><div class="line">            a: (Optional) action to make a prediction for</div><div class="line">            </div><div class="line">        Returns</div><div class="line">            If an action a is given this returns a single number as the prediction.</div><div class="line">            If no action is given this returns a vector or predictions for all actions</div><div class="line">            in the environment where pred[i] is the prediction for action i.</div><div class="line">            </div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> a:</div><div class="line">            <span class="keyword">return</span> np.array([m.predict([features])[<span class="number">0</span>] <span class="keyword">for</span> m <span class="keyword">in</span> self.models])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> self.models[a].predict([features])[<span class="number">0</span>]</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, s, a, y)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Updates the estimator parameters for a given state and action towards</div><div class="line">        the target y.</div><div class="line">        """</div><div class="line">        features = self.featurize_state(s)</div><div class="line">        self.models[a].partial_fit([features], [y])</div></pre></td></tr></table></figure><p>We also need a $\varepsilon$-greedy policy to select action:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(estimator, epsilon, nA)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        estimator: An estimator that returns q values for a given state</div><div class="line">        epsilon: The probability to select a random action . float between 0 and 1.</div><div class="line">        nA: Number of actions in the environment.</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        A function that takes the observation as an argument and returns</div><div class="line">        the probabilities for each action in the form of a numpy array of length nA.</div><div class="line">    </div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></div><div class="line">        A = np.ones(nA, dtype=float) * epsilon / nA</div><div class="line">        q_values = estimator.predict(observation)</div><div class="line">        best_action = np.argmax(q_values)</div><div class="line">        A[best_action] += (<span class="number">1.0</span> - epsilon)</div><div class="line">        <span class="keyword">return</span> A</div><div class="line">    <span class="keyword">return</span> policy_fn</div></pre></td></tr></table></figure><p>Then we develop the Q-Learning method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_learning</span><span class="params">(env, estimator, num_episodes, discount_factor=<span class="number">1.0</span>, epsilon=<span class="number">0.1</span>, epsilon_decay=<span class="number">1.0</span>)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Q-Learning algorithm for fff-policy TD control using Function Approximation.</div><div class="line">    Finds the optimal greedy policy while following an epsilon-greedy policy.</div><div class="line">    </div><div class="line">    Args:</div><div class="line">        env: OpenAI environment.</div><div class="line">        estimator: Action-Value function estimator</div><div class="line">        num_episodes: Number of episodes to run for.</div><div class="line">        discount_factor: Lambda time discount factor.</div><div class="line">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</div><div class="line">        epsilon_decay: Each episode, epsilon is decayed by this factor</div><div class="line">    </div><div class="line">    Returns:</div><div class="line">        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="comment"># Keeps track of useful statistics</span></div><div class="line">    stats = plotting.EpisodeStats(</div><div class="line">        episode_lengths=np.zeros(num_episodes),</div><div class="line">        episode_rewards=np.zeros(num_episodes))    </div><div class="line">    </div><div class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</div><div class="line">        </div><div class="line">        <span class="comment"># The policy we're following</span></div><div class="line">        policy = make_epsilon_greedy_policy(</div><div class="line">            estimator, epsilon * epsilon_decay**i_episode, env.action_space.n)</div><div class="line">        </div><div class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></div><div class="line">        <span class="comment"># Also print reward for last episode</span></div><div class="line">        last_reward = stats.episode_rewards[i_episode - <span class="number">1</span>]</div><div class="line">        sys.stdout.flush()</div><div class="line">        </div><div class="line">        <span class="comment"># Reset the environment and pick the first action</span></div><div class="line">        state = env.reset()</div><div class="line">        </div><div class="line">        <span class="comment"># Only used for SARSA, not Q-Learning</span></div><div class="line">        next_action = <span class="keyword">None</span></div><div class="line">        </div><div class="line">        <span class="comment"># One step in the environment</span></div><div class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> itertools.count():</div><div class="line">                        </div><div class="line">            <span class="comment"># Choose an action to take</span></div><div class="line">            <span class="comment"># If we're using SARSA we already decided in the previous step</span></div><div class="line">            <span class="keyword">if</span> next_action <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                action_probs = policy(state)</div><div class="line">                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                action = next_action</div><div class="line">            </div><div class="line">            <span class="comment"># Take a step</span></div><div class="line">            next_state, reward, done, _ = env.step(action)</div><div class="line">    </div><div class="line">            <span class="comment"># Update statistics</span></div><div class="line">            stats.episode_rewards[i_episode] += reward</div><div class="line">            stats.episode_lengths[i_episode] = t</div><div class="line">            </div><div class="line">            <span class="comment"># TD Update</span></div><div class="line">            q_values_next = estimator.predict(next_state)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for Q-Learning</span></div><div class="line">            <span class="comment"># Q-Value TD Target</span></div><div class="line">            td_target = reward + discount_factor * np.max(q_values_next)</div><div class="line">            </div><div class="line">            <span class="comment"># Use this code for SARSA TD Target for on policy-training:</span></div><div class="line">            <span class="comment"># next_action_probs = policy(next_state)</span></div><div class="line">            <span class="comment"># next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)             </span></div><div class="line">            <span class="comment"># td_target = reward + discount_factor * q_values_next[next_action]</span></div><div class="line">            </div><div class="line">            <span class="comment"># Update the function approximator using our target</span></div><div class="line">            estimator.update(state, action, td_target)</div><div class="line">            </div><div class="line">            print(<span class="string">"\rStep &#123;&#125; @ Episode &#123;&#125;/&#123;&#125; (&#123;&#125;)"</span>.format(t, i_episode + <span class="number">1</span>, num_episodes, last_reward), end=<span class="string">""</span>)</div><div class="line">                </div><div class="line">            <span class="keyword">if</span> done:</div><div class="line">                <span class="keyword">break</span></div><div class="line">                </div><div class="line">            state = next_state</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> stats</div></pre></td></tr></table></figure><p>Run this method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">estimator = Estimator()</div><div class="line"><span class="comment"># Note: For the Mountain Car we don't actually need an epsilon &gt; 0.0</span></div><div class="line"><span class="comment"># because our initial estimate for all states is too "optimistic" which leads</span></div><div class="line"><span class="comment"># to the exploration of all states.</span></div><div class="line">stats = q_learning(env, estimator, <span class="number">100</span>, epsilon=<span class="number">0.0</span>)</div></pre></td></tr></table></figure><p>The result is as follows:</p><p><img src="http://o7ie0tcjk.bkt.clouddn.com/rl/on_policy_appr_control/mcar-ql-gym.png" alt="mcar-ql-gym"></p></div><div></div><div></div><footer class="post-footer"><div class="post-eof"></div></footer></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Ewan Li"><p class="site-author-name" itemprop="name">Ewan Li</p><p class="site-description motion-element" itemprop="description">Ewan's IT Blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">119</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-tags"><a href="/tags"><span class="site-state-item-count">58</span> <span class="site-state-item-name">tags</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/ewanlee" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="http://weibo.com/3946248928/profile?topnav=1&wvr=6" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ewan Li</span></div><div class="powered-by">Powered by <a class="theme-link" href="https://hexo.io">Hexo</a></div><div class="theme-info">Theme - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user">本站访客数</i><span class="busuanzi-value" id="busuanzi_value_site_uv"></span>人次</span> <span class="site-pv"><i class="fa fa-eye">本站总访问量</i><span class="busuanzi-value" id="busuanzi_value_site_pv"></span>次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><script type="text/javascript">_hcwp=window._hcwp||[],_hcwp.push({widget:"Bloggerstream",widget_id:89825,selector:".hc-comment-count",label:"{%COUNT%}"}),function(){if(!("HC_LOAD_INIT"in window)){HC_LOAD_INIT=!0;var t=(navigator.language||navigator.systemLanguage||navigator.userLanguage||"en").substr(0,2).toLowerCase(),e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src=("https:"==document.location.protocol?"https":"http")+"://w.hypercomments.com/widget/hc/89825/"+t+"/widget.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n.nextSibling)}}()</script><script type="text/javascript">function proceedsearch(){$("body").append('<div class="popoverlay">').css("overflow","hidden"),$(".popup").toggle()}var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path,searchFunc=function(e,t,a){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var r=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),c=document.getElementById(t),n=document.getElementById(a);c.addEventListener("input",function(){var e=0,t='<ul class="search-result-list">',a=this.value.trim().toLowerCase().split(/[\s\-]+/);n.innerHTML="",this.value.trim().length>1&&r.forEach(function(r){var c=!1,n=r.title.trim().toLowerCase(),s=r.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),o=decodeURIComponent(r.url),i=-1,l=-1,p=-1;if(""!=n&&a.forEach(function(e,t){i=n.indexOf(e),l=s.indexOf(e),(i>=0||l>=0)&&(c=!0,0==t&&(p=l))}),c){e+=1,t+="<li><a href='"+o+"' class='search-result-title'>"+n+"</a>";var h=r.content.trim().replace(/<[^>]+>/g,"");if(p>=0){var u=p-20,d=p+80;u<0&&(u=0),0==u&&(d=50),d>h.length&&(d=h.length);var f=h.substring(u,d);a.forEach(function(e){var t=new RegExp(e,"gi");f=f.replace(t,'<b class="search-keyword">'+e+"</b>")}),t+='<p class="search-result">'+f+"...</p>"}t+="</li>"}}),t+="</ul>",0==e&&(t='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==a&&(t='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),n.innerHTML=t}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".popoverlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("e27VKX5tTklQLCtF7iNMmhcA-gzGzoHsz","nnQn2znNgXXEdK7W2bVJ3bfK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script></body></html>