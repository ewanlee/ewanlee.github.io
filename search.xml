<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[cs231n Assignment#1 softmax]]></title>
      <url>%2F2017%2F03%2F05%2Fcs231n-Assignment-1-softmax%2F</url>
      <content type="text"><![CDATA[Softmax exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website. This exercise is analogous to the SVM exercise. You will: implement a fully-vectorized loss function for the Softmax classifier implement the fully-vectorized expression for its analytic gradient check your implementation with numerical gradient use a validation set to tune the learning rate and regularization strength optimize the loss function with SGD visualize the final learned weights 12345678910111213import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# for auto-reloading extenrnal modules# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500): """ Load the CIFAR-10 dataset from disk and perform preprocessing to prepare it for the linear classifier. These are the same steps as we used for the SVM, but condensed to a single function. """ # Load the raw CIFAR-10 data cifar10_dir = 'cs231n/datasets/cifar-10-batches-py' X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir) # subsample the data mask = range(num_training, num_training + num_validation) X_val = X_train[mask] y_val = y_train[mask] mask = range(num_training) X_train = X_train[mask] y_train = y_train[mask] mask = range(num_test) X_test = X_test[mask] y_test = y_test[mask] mask = np.random.choice(num_training, num_dev, replace=False) X_dev = X_train[mask] y_dev = y_train[mask] # Preprocessing: reshape the image data into rows X_train = np.reshape(X_train, (X_train.shape[0], -1)) X_val = np.reshape(X_val, (X_val.shape[0], -1)) X_test = np.reshape(X_test, (X_test.shape[0], -1)) X_dev = np.reshape(X_dev, (X_dev.shape[0], -1)) # Normalize the data: subtract the mean image mean_image = np.mean(X_train, axis = 0) X_train -= mean_image X_val -= mean_image X_test -= mean_image X_dev -= mean_image # add bias dimension and transform into columns X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]) X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]) X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]) X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))]) return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev# Invoke the above function to get our data.X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeprint 'dev data shape: ', X_dev.shapeprint 'dev labels shape: ', y_dev.shape Train data shape: (49000L, 3073L) Train labels shape: (49000L,) Validation data shape: (1000L, 3073L) Validation labels shape: (1000L,) Test data shape: (1000L, 3073L) Test labels shape: (1000L,) dev data shape: (500L, 3073L) dev labels shape: (500L,) Softmax ClassifierYour code for this section will all be written inside cs231n/classifiers/softmax.py. 1234567891011121314# First implement the naive softmax loss function with nested loops.# Open the file cs231n/classifiers/softmax.py and implement the# softmax_loss_naive function.from cs231n.classifiers.softmax import softmax_loss_naiveimport time# Generate a random softmax weight matrix and use it to compute the loss.W = np.random.randn(3073, 10) * 0.0001loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)# As a rough sanity check, our loss should be something close to -log(0.1).print 'loss: %f' % lossprint 'sanity check: %f' % (-np.log(0.1)) loss: 2.395985 sanity check: 2.302585 Inline Question 1:Why do we expect our loss to be close to -log(0.1)? Explain briefly. Your answer: Because the W is selected by random, so the probability of select the true class is 1/10. That is, 0.1. 1234567891011121314# Complete the implementation of softmax_loss_naive and implement a (naive)# version of the gradient that uses nested loops.loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)# As we did for the SVM, use numeric gradient checking as a debugging tool.# The numeric gradient should be close to the analytic gradient.from cs231n.gradient_check import grad_check_sparsef = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]grad_numerical = grad_check_sparse(f, W, grad, 10)# similar to SVM case, do another gradient check with regularizationloss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]grad_numerical = grad_check_sparse(f, W, grad, 10) numerical: 2.368141 analytic: 2.368141, relative error: 2.349797e-08 numerical: 1.324690 analytic: 1.324690, relative error: 7.140560e-08 numerical: 3.170412 analytic: 3.170411, relative error: 1.324741e-08 numerical: 0.249509 analytic: 0.249509, relative error: 2.647240e-08 numerical: 1.536095 analytic: 1.536095, relative error: 4.345856e-08 numerical: 1.075819 analytic: 1.075819, relative error: 3.902323e-08 numerical: -0.198098 analytic: -0.198098, relative error: 5.737134e-08 numerical: -0.089902 analytic: -0.089902, relative error: 8.604010e-07 numerical: -0.339487 analytic: -0.339487, relative error: 3.992996e-08 numerical: -4.819781 analytic: -4.819781, relative error: 3.465667e-09 numerical: 1.869922 analytic: 1.869921, relative error: 7.536693e-08 numerical: 0.783465 analytic: 0.783465, relative error: 6.960291e-08 numerical: -3.206007 analytic: -3.206007, relative error: 2.337350e-09 numerical: 0.532183 analytic: 0.532183, relative error: 1.498128e-07 numerical: 0.900500 analytic: 0.900500, relative error: 6.954913e-09 numerical: -0.353224 analytic: -0.353224, relative error: 1.836960e-07 numerical: -1.331470 analytic: -1.331470, relative error: 2.726426e-08 numerical: -0.082452 analytic: -0.082452, relative error: 7.712355e-07 numerical: -1.322133 analytic: -1.322133, relative error: 5.516628e-09 numerical: 0.345814 analytic: 0.345814, relative error: 1.251858e-07 1234567891011121314151617181920# Now that we have a naive implementation of the softmax loss function and its gradient,# implement a vectorized version in softmax_loss_vectorized.# The two versions should compute the same results, but the vectorized version should be# much faster.tic = time.time()loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)from cs231n.classifiers.softmax import softmax_loss_vectorizedtic = time.time()loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)# As we did for the SVM, we use the Frobenius norm to compare the two versions# of the gradient.grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)print 'Gradient difference: %f' % grad_difference naive loss: 2.395985e+00 computed in 0.080000s vectorized loss: 2.395985e+00 computed in 0.003000s Loss difference: 0.000000 Gradient difference: 0.000000 1234567891011121314151617181920212223242526272829303132333435363738394041424344# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of over 0.35 on the validation set.from cs231n.classifiers import Softmaxresults = &#123;&#125;best_val = -1best_softmax = Nonelearning_rates = [1e-8, 1e-7, 2e-7]regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained softmax classifer in best_softmax. #################################################################################iters = 2000for lr in learning_rates: for rs in regularization_strengths: softmax = Softmax() softmax.train(X_train, y_train, learning_rate=lr, reg=rs, num_iters=iters) y_train_pred = softmax.predict(X_train) acc_train = np.mean(y_train == y_train_pred) y_val_pred = softmax.predict(X_val) acc_val = np.mean(y_val == y_val_pred) results[(lr, rs)] = (acc_train, acc_val) if best_val &lt; acc_val: best_val = acc_val best_softmax = softmax################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print 'lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy) print 'best validation accuracy achieved during cross-validation: %f' % best_val lr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.175633 val accuracy: 0.179000 lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.174102 val accuracy: 0.161000 lr 1.000000e-08 reg 3.000000e+04 train accuracy: 0.203490 val accuracy: 0.210000 lr 1.000000e-08 reg 4.000000e+04 train accuracy: 0.191367 val accuracy: 0.202000 lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.208000 val accuracy: 0.197000 lr 1.000000e-08 reg 6.000000e+04 train accuracy: 0.203571 val accuracy: 0.215000 lr 1.000000e-08 reg 7.000000e+04 train accuracy: 0.213551 val accuracy: 0.215000 lr 1.000000e-08 reg 8.000000e+04 train accuracy: 0.238347 val accuracy: 0.229000 lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.245102 val accuracy: 0.242000 lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.358265 val accuracy: 0.362000 lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.356306 val accuracy: 0.374000 lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.347327 val accuracy: 0.362000 lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.336347 val accuracy: 0.354000 lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.331490 val accuracy: 0.348000 lr 1.000000e-07 reg 6.000000e+04 train accuracy: 0.320163 val accuracy: 0.336000 lr 1.000000e-07 reg 7.000000e+04 train accuracy: 0.314551 val accuracy: 0.325000 lr 1.000000e-07 reg 8.000000e+04 train accuracy: 0.313082 val accuracy: 0.324000 lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.303000 val accuracy: 0.315000 lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.374163 val accuracy: 0.389000 lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.353184 val accuracy: 0.365000 lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.340265 val accuracy: 0.359000 lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.334673 val accuracy: 0.351000 lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.326531 val accuracy: 0.337000 lr 2.000000e-07 reg 6.000000e+04 train accuracy: 0.319857 val accuracy: 0.336000 lr 2.000000e-07 reg 7.000000e+04 train accuracy: 0.317878 val accuracy: 0.329000 lr 2.000000e-07 reg 8.000000e+04 train accuracy: 0.310449 val accuracy: 0.329000 lr 2.000000e-07 reg 1.000000e+05 train accuracy: 0.316286 val accuracy: 0.315000 best validation accuracy achieved during cross-validation: 0.389000 12345# evaluate on test set# Evaluate the best softmax on test sety_test_pred = best_softmax.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ) softmax on raw pixels final test set accuracy: 0.375000 ​ 123456789101112131415# Visualize the learned weights for each classw = best_softmax.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in xrange(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i]) Codes123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import numpy as npfrom random import shuffledef softmax_loss_naive(W, X, y, reg): """ Softmax loss function, naive implementation (with loops) Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# num_train = X.shape[0] num_classes = W.shape[1] for i in xrange(num_train): f = X[i, :].dot(W) f -= np.max(f) correct_f = f[y[i]] denom = np.sum(np.exp(f)) p = np.exp(correct_f) / denom loss += -np.log(p) for j in xrange(num_classes): if j == y[i]: dW[:, y[i]] += (np.exp(f[j]) / denom - 1) * X[i, :] else: dW[:, j] += (np.exp(f[j]) / denom) * X[i, :] loss /= num_train loss += 0.5 * reg * np.sum(W * W) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dWdef softmax_loss_vectorized(W, X, y, reg): """ Softmax loss function, vectorized version. Inputs and outputs are the same as softmax_loss_naive. """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using no explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# num_train = X.shape[0] f = X.dot(W) f = f - np.max(f, axis=1)[:, np.newaxis] loss = -np.sum( np.log(np.exp(f[np.arange(num_train), y]) / np.sum(np.exp(f), axis=1))) loss /= num_train loss += 0.5 * reg * np.sum(W * W) ind = np.zeros_like(f) ind[np.arange(num_train), y] = 1 dW = X.T.dot(np.exp(f) / np.sum(np.exp(f), axis=1, keepdims=True) - ind) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 svm]]></title>
      <url>%2F2017%2F03%2F03%2Fcs231n-Assignment-1-svm%2F</url>
      <content type="text"><![CDATA[Multiclass Support Vector Machine exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website. In this exercise you will:​ implement a fully-vectorized loss function for the SVM implement the fully-vectorized expression for its analytic gradient check your implementation using numerical gradient use a validation set to tune the learning rate and regularization strength optimize the loss function with SGD visualize the final learned weights 123456789101112131415161718# Run some setup code for this notebook.import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the# notebook rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2 CIFAR-10 Data Loading and Preprocessing123456789# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print 'Training data shape: ', X_train.shapeprint 'Training labels shape: ', y_train.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shape Training data shape: (50000L, 32L, 32L, 3L) Training labels shape: (50000L,) Test data shape: (10000L, 32L, 32L, 3L) Test labels shape: (10000L,) 12345678910111213141516# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show() 1?np.random.choice 1234567891011121314151617181920212223242526272829303132333435363738# Split the data into train, val, and test sets. In addition we will# create a small development set as a subset of the training data;# we can use this for development so our code runs faster.num_training = 49000num_validation = 1000num_test = 1000num_dev = 500# Our validation set will be num_validation points from the original# training set.mask = range(num_training, num_training + num_validation)X_val = X_train[mask]y_val = y_train[mask]# Our training set will be the first num_train points from the original# training set.mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]# We will also make a development set, which is a small subset of# the training set.mask = np.random.choice(num_training, num_dev, replace=False)X_dev = X_train[mask]y_dev = y_train[mask]# We use the first num_test points of the original test set as our# test set.mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask]print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shape Train data shape: (49000L, 32L, 32L, 3L) Train labels shape: (49000L,) Validation data shape: (1000L, 32L, 32L, 3L) Validation labels shape: (1000L,) Test data shape: (1000L, 32L, 32L, 3L) Test labels shape: (1000L,) 1234567891011# Preprocessing: reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1))X_val = np.reshape(X_val, (X_val.shape[0], -1))X_test = np.reshape(X_test, (X_test.shape[0], -1))X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))# As a sanity check, print out the shapes of the dataprint 'Training data shape: ', X_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Test data shape: ', X_test.shapeprint 'dev data shape: ', X_dev.shape Training data shape: (49000L, 3072L) Validation data shape: (1000L, 3072L) Test data shape: (1000L, 3072L) dev data shape: (500L, 3072L) 1234567# Preprocessing: subtract the mean image# first: compute the image mean based on the training datamean_image = np.mean(X_train, axis=0)print mean_image[:10] # print a few of the elementsplt.figure(figsize=(4,4))plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean imageplt.show() [ 130.64189796 135.98173469 132.47391837 130.05569388 135.34804082 131.75402041 130.96055102 136.14328571 132.47636735 131.48467347] 12345# second: subtract the mean image from train and test dataX_train -= mean_imageX_val -= mean_imageX_test -= mean_imageX_dev -= mean_image 12345678# third: append (at the last) the bias dimension of ones (i.e. bias trick) so that our SVM# only has to worry about optimizing a single weight matrix W.X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])print X_train.shape, X_val.shape, X_test.shape, X_dev.shape (49000L, 3073L) (1000L, 3073L) (1000L, 3073L) (500L, 3073L) ​ SVM ClassifierYour code for this section will all be written inside cs231n/classifiers/linear_svm.py. As you can see, we have prefilled the function compute_loss_naive which uses for loops to evaluate the multiclass SVM loss function. 123456789# Evaluate the naive implementation of the loss we provided for you:from cs231n.classifiers.linear_svm import svm_loss_naiveimport time# generate a random SVM weight matrix of small numbersW = np.random.randn(3073, 10) * 0.0001 loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.00001)print 'loss: %f' % (loss, ) loss: 8.831645 ​ The grad returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function svm_loss_naive. You will find it helpful to interleave your new code inside the existing function. To check that you have correctly implemented the gradient correctly, you can numerically estimate the gradient of the loss function and compare the numeric estimate to the gradient that you computed. We have provided code that does this for you: 123456789101112131415161718# Once you've implemented the gradient, recompute it with the code below# and gradient check it with the function we provided for you# Compute the loss and its gradient at W.loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.0)# Numerically compute the gradient along several randomly chosen dimensions, and# compare them with your analytically computed gradient. The numbers should match# almost exactly along all dimensions.from cs231n.gradient_check import grad_check_sparsef = lambda w: svm_loss_naive(w, X_dev, y_dev, 0.0)[0]grad_numerical = grad_check_sparse(f, W, grad)# do the gradient check once again with regularization turned on# you didn't forget the regularization gradient did you?loss, grad = svm_loss_naive(W, X_dev, y_dev, 1e2)f = lambda w: svm_loss_naive(w, X_dev, y_dev, 1e2)[0]grad_numerical = grad_check_sparse(f, W, grad) numerical: -13.865929 analytic: -13.865929, relative error: 1.283977e-12 numerical: 7.842142 analytic: 7.735021, relative error: 6.876784e-03 numerical: 3.464393 analytic: 3.464393, relative error: 9.040092e-11 numerical: -23.034911 analytic: -23.034911, relative error: 6.876266e-12 numerical: -0.185311 analytic: -0.185311, relative error: 2.538774e-10 numerical: 25.825504 analytic: 25.825504, relative error: 1.336035e-11 numerical: 4.457836 analytic: 4.457836, relative error: 1.015819e-10 numerical: 3.184691 analytic: 3.184691, relative error: 8.849109e-11 numerical: 10.428446 analytic: 10.374317, relative error: 2.601982e-03 numerical: 12.479957 analytic: 12.479957, relative error: 6.825191e-12 numerical: 12.237949 analytic: 12.326308, relative error: 3.597051e-03 numerical: 4.377103 analytic: 4.377103, relative error: 3.904758e-11 numerical: -1.951930 analytic: -1.951930, relative error: 1.432276e-10 numerical: 33.752503 analytic: 33.752503, relative error: 4.254520e-12 numerical: 11.367149 analytic: 11.367149, relative error: 1.682727e-11 numerical: 16.461879 analytic: 16.461879, relative error: 4.766805e-12 numerical: 3.814562 analytic: 3.814562, relative error: 1.087469e-10 numerical: 13.931226 analytic: 13.931226, relative error: 9.578349e-12 numerical: -27.291095 analytic: -27.395406, relative error: 1.907445e-03 numerical: -7.610407 analytic: -7.610407, relative error: 1.015282e-12 Inline Question 1:It is possible that once in a while a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a gradient check could fail? Hint: the SVM loss function is not strictly speaking differentiable Your Answer: Maybe the SVM loss function is not differentiable on that dimension 1?np.max 1np.sum(np.maximum(0, X_dev.dot(W) - X_dev.dot(W)[np.arange(len(y_dev)), [y_dev]].T + 1)) 4915.822409730994 123456789101112131415# Next implement the function svm_loss_vectorized; for now only compute the loss;# we will implement the gradient in a moment.tic = time.time()loss_naive, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Naive loss: %e computed in %fs' % (loss_naive, toc - tic)from cs231n.classifiers.linear_svm import svm_loss_vectorizedtic = time.time()loss_vectorized, _ = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)# The losses should match but your vectorized implementation should be much faster.print 'difference: %f' % (loss_naive - loss_vectorized) Naive loss: 8.831645e+00 computed in 0.071000s Vectorized loss: 8.831645e+00 computed in 0.000000s difference: 0.000000 1234567891011121314151617181920# Complete the implementation of svm_loss_vectorized, and compute the gradient# of the loss function in a vectorized way.# The naive implementation and the vectorized implementation should match, but# the vectorized version should still be much faster.tic = time.time()_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Naive loss and gradient: computed in %fs' % (toc - tic)tic = time.time()_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Vectorized loss and gradient: computed in %fs' % (toc - tic)# The loss is a single number, so it is easy to compare the values computed# by the two implementations. The gradient on the other hand is a matrix, so# we use the Frobenius norm to compare them.difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')print 'difference: %f' % difference Naive loss and gradient: computed in 0.084000s Vectorized loss and gradient: computed in 0.005000s difference: 0.000000 Stochastic Gradient DescentWe now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss. 123456789# In the file linear_classifier.py, implement SGD in the function# LinearClassifier.train() and then run it with the code below.from cs231n.classifiers import LinearSVMsvm = LinearSVM()tic = time.time()loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4, num_iters=1500, verbose=True)toc = time.time()print 'That took %fs' % (toc - tic) iteration 0 / 1500: loss 791.772037 iteration 100 / 1500: loss 286.021346 iteration 200 / 1500: loss 107.673095 iteration 300 / 1500: loss 41.812791 iteration 400 / 1500: loss 18.665578 iteration 500 / 1500: loss 10.614984 iteration 600 / 1500: loss 6.664814 iteration 700 / 1500: loss 6.509693 iteration 800 / 1500: loss 5.792204 iteration 900 / 1500: loss 4.986855 iteration 1000 / 1500: loss 5.914691 iteration 1100 / 1500: loss 5.058078 iteration 1200 / 1500: loss 5.491475 iteration 1300 / 1500: loss 5.609450 iteration 1400 / 1500: loss 5.376595 That took 5.454000s 123456# A useful debugging strategy is to plot the loss as a function of# iteration number:plt.plot(loss_hist)plt.xlabel('Iteration number')plt.ylabel('Loss value')plt.show() 123456# Write the LinearSVM.predict function and evaluate the performance on both the# training and validation sety_train_pred = svm.predict(X_train)print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )y_val_pred = svm.predict(X_val)print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), ) training accuracy: 0.364980 validation accuracy: 0.378000 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of about 0.4 on the validation set.learning_rates = [1e-8, 1e-7, 2e-7]regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]# results is dictionary mapping tuples of the form# (learning_rate, regularization_strength) to tuples of the form# (training_accuracy, validation_accuracy). The accuracy is simply the fraction# of data points that are correctly classified.results = &#123;&#125;best_val = -1 # The highest validation accuracy that we have seen so far.best_svm = None # The LinearSVM object that achieved the highest validation rate.################################################################################# TODO: ## Write code that chooses the best hyperparameters by tuning on the validation ## set. For each combination of hyperparameters, train a linear SVM on the ## training set, compute its accuracy on the training and validation sets, and ## store these numbers in the results dictionary. In addition, store the best ## validation accuracy in best_val and the LinearSVM object that achieves this ## accuracy in best_svm. ## ## Hint: You should use a small value for num_iters as you develop your ## validation code so that the SVMs don't take much time to train; once you are ## confident that your validation code works, you should rerun the validation ## code with a larger value for num_iters. #################################################################################for learning_rate in learning_rates: for regularization_strength in regularization_strengths: svm = LinearSVM() loss_hist = svm.train( X_train, y_train, learning_rate, \ regularization_strength, num_iters=1500, batch_size=200) y_train_pred = svm.predict(X_train) y_val_pred = svm.predict(X_val) training_accuracy = np.mean(y_train == y_train_pred) validation_accuracy = np.mean(y_val == y_val_pred) results[(learning_rate, regularization_strength)] = \ (training_accuracy, validation_accuracy) if validation_accuracy &gt; best_val: best_val = validation_accuracy best_svm = svm################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print 'lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy) print 'best validation accuracy achieved during cross-validation: %f' % best_val lr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.221898 val accuracy: 0.247000 lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.233653 val accuracy: 0.258000 lr 1.000000e-08 reg 3.000000e+04 train accuracy: 0.234694 val accuracy: 0.225000 lr 1.000000e-08 reg 4.000000e+04 train accuracy: 0.255959 val accuracy: 0.249000 lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.259755 val accuracy: 0.273000 lr 1.000000e-08 reg 6.000000e+04 train accuracy: 0.267408 val accuracy: 0.269000 lr 1.000000e-08 reg 7.000000e+04 train accuracy: 0.269102 val accuracy: 0.287000 lr 1.000000e-08 reg 8.000000e+04 train accuracy: 0.277102 val accuracy: 0.285000 lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.295306 val accuracy: 0.301000 lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.369388 val accuracy: 0.374000 lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.380265 val accuracy: 0.390000 lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.375490 val accuracy: 0.378000 lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.375633 val accuracy: 0.385000 lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.369694 val accuracy: 0.375000 lr 1.000000e-07 reg 6.000000e+04 train accuracy: 0.372469 val accuracy: 0.383000 lr 1.000000e-07 reg 7.000000e+04 train accuracy: 0.356000 val accuracy: 0.370000 lr 1.000000e-07 reg 8.000000e+04 train accuracy: 0.352816 val accuracy: 0.355000 lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.356796 val accuracy: 0.377000 lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.393510 val accuracy: 0.395000 lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.377020 val accuracy: 0.382000 lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.363857 val accuracy: 0.373000 lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.368714 val accuracy: 0.372000 lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.361531 val accuracy: 0.364000 lr 2.000000e-07 reg 6.000000e+04 train accuracy: 0.354714 val accuracy: 0.368000 lr 2.000000e-07 reg 7.000000e+04 train accuracy: 0.348306 val accuracy: 0.365000 lr 2.000000e-07 reg 8.000000e+04 train accuracy: 0.358082 val accuracy: 0.378000 lr 2.000000e-07 reg 1.000000e+05 train accuracy: 0.347898 val accuracy: 0.358000 best validation accuracy achieved during cross-validation: 0.395000 123456789101112131415161718192021222324# Visualize the cross-validation resultsimport mathx_scatter = [math.log10(x[0]) for x in results]y_scatter = [math.log10(x[1]) for x in results]# plot training accuracymarker_size = 100colors = [results[x][0] for x in results]plt.subplot(3, 1, 1)plt.scatter(x_scatter, y_scatter, marker_size, c=colors)plt.colorbar()plt.xlabel('log learning rate')plt.ylabel('log regularization strength')plt.title('CIFAR-10 training accuracy')# plot validation accuracycolors = [results[x][1] for x in results] # default size of markers is 20plt.subplot(3, 1, 3)plt.scatter(x_scatter, y_scatter, marker_size, c=colors)plt.colorbar()plt.xlabel('log learning rate')plt.ylabel('log regularization strength')plt.title('CIFAR-10 validation accuracy')plt.show() 1234# Evaluate the best svm on test sety_test_pred = best_svm.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print 'linear SVM on raw pixels final test set accuracy: %f' % test_accuracy linear SVM on raw pixels final test set accuracy: 0.383000 ​ 12x = np.array([[[0], [1], [2]]])np.squeeze(x) array([0, 1, 2]) 123456789101112131415# Visualize the learned weights for each class.# Depending on your choice of learning rate and regularization strength, these may# or may not be nice to look at.w = best_svm.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in xrange(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i]) Inline question 2:Describe what your visualized SVM weights look like, and offer a brief explanation for why they look they way that they do. Your answer: fill this in Codeslinear_svm.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import numpy as npfrom random import shuffledef svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in xrange(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in xrange(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: dW[:, y[i]] -= X[i, :] dW[:, j] += X[i, :] loss += margin # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += 0.5 * reg * np.sum(W * W) dW += reg * W ############################################################################# # TODO: # # Compute the gradient of the loss function and store it dW. # # Rather that first computing the loss and then computing the derivative, # # it may be simpler to compute the derivative at the same time that the # # loss is being computed. As a result you may need to modify some of the # # code above to compute the gradient. # ############################################################################# return loss, dWdef svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero ############################################################################# # TODO: # # Implement a vectorized version of the structured SVM loss, storing the # # result in loss. # ############################################################################# num_train = X.shape[0] delta = 1.0 scores = X.dot(W) correct_class_score = scores[np.arange(num_train), y] margins = np.maximum( 0, scores - correct_class_score[:, np.newaxis] + delta) margins[np.arange(num_train), y] = 0 loss = np.sum(margins) loss /= num_train loss += 0.5 * reg * np.sum(W.T.dot(W)) ############################################################################# # END OF YOUR CODE # ############################################################################# ############################################################################# # TODO: # # Implement a vectorized version of the gradient for the structured SVM # # loss, storing the result in dW. # # # # Hint: Instead of computing the gradient from scratch, it may be easier # # to reuse some of the intermediate values that you used to compute the # # loss. # ############################################################################# X_mask = np.zeros(margins.shape) X_mask[margins &gt; 0] = 1 count = np.sum(X_mask, axis=1) X_mask[np.arange(num_train), y] = -count dW = X.T.dot(X_mask) dW /= num_train dW += np.multiply(W, reg) ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW linear_classifier123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import numpy as npfrom cs231n.classifiers.linear_svm import *from cs231n.classifiers.softmax import *class LinearClassifier(object): def __init__(self): self.W = None def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): """ Train this linear classifier using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label 0 &lt;= c &lt; C for C classes. - learning_rate: (float) learning rate for optimization. - reg: (float) regularization strength. - num_iters: (integer) number of steps to take when optimizing - batch_size: (integer) number of training examples to use at each step. - verbose: (boolean) If true, print progress during optimization. Outputs: A list containing the value of the loss function at each training iteration. """ num_train, dim = X.shape num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in xrange(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: # # Sample batch_size elements from the training data and their # # corresponding labels to use in this round of gradient descent. # # Store the data in X_batch and their corresponding labels in # # y_batch; after sampling X_batch should have shape (dim, batch_size) # # and y_batch should have shape (batch_size,) # # # # Hint: Use np.random.choice to generate indices. Sampling with # # replacement is faster than sampling without replacement. # ######################################################################### mask = np.random.choice(num_train, batch_size, replace=True) X_batch = X[mask] y_batch = y[mask] ######################################################################### # END OF YOUR CODE # ######################################################################### # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) # perform parameter update ######################################################################### # TODO: # # Update the weights using the gradient and the learning rate. # ######################################################################### self.W = self.W - learning_rate * grad ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print 'iteration %d / %d: loss %f' % (it, num_iters, loss) return loss_history def predict(self, X): """ Use the trained weights of this linear classifier to predict labels for data points. Inputs: - X: D x N array of training data. Each column is a D-dimensional point. Returns: - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional array of length N, and each element is an integer giving the predicted class. """ X = X.T y_pred = np.zeros(X.shape[1]) ########################################################################### # TODO: # # Implement this method. Store the predicted labels in y_pred. # ########################################################################### scores = X.T.dot(self.W) y_pred = np.argsort(scores, axis=1)[:, -1] ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred def loss(self, X_batch, y_batch, reg): """ Compute the loss function and its derivative. Subclasses will override this. Inputs: - X_batch: A numpy array of shape (N, D) containing a minibatch of N data points; each point has dimension D. - y_batch: A numpy array of shape (N,) containing labels for the minibatch. - reg: (float) regularization strength. Returns: A tuple containing: - loss as a single float - gradient with respect to self.W; an array of the same shape as W """ passclass LinearSVM(LinearClassifier): """ A subclass that uses the Multiclass SVM loss function """ def loss(self, X_batch, y_batch, reg): return svm_loss_vectorized(self.W, X_batch, y_batch, reg)class Softmax(LinearClassifier): """ A subclass that uses the Softmax + Cross-entropy loss function """ def loss(self, X_batch, y_batch, reg): return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 kNN]]></title>
      <url>%2F2017%2F03%2F02%2Fcs231n-Assignment-1-kNN%2F</url>
      <content type="text"><![CDATA[k-Nearest Neighbor (kNN) exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website. The kNN classifier consists of two stages: During training, the classifier takes the training data and simply remembers it During testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examples The value of k is cross-validated In this exercise you will implement these steps and understand the basic Image Classification pipeline, cross-validation, and gain proficiency in writing efficient, vectorized code. 123456789101112131415161718# Run some setup code for this notebook.import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the notebook# rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2 123456789# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print 'Training data shape: ', X_train.shapeprint 'Training labels shape: ', y_train.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shape Training data shape: (50000L, 32L, 32L, 3L) Training labels shape: (50000L,) Test data shape: (10000L, 32L, 32L, 3L) Test labels shape: (10000L,) 12345678910111213141516# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show() 12345678910# Subsample the data for more efficient code execution in this exercisenum_training = 5000mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]num_test = 500mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask] 1234# Reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1)) # Wow~X_test = np.reshape(X_test, (X_test.shape[0], -1))print X_train.shape, X_test.shape (5000L, 3072L) (500L, 3072L) ​ 1234567from cs231n.classifiers import KNearestNeighbor# Create a kNN classifier instance. # Remember that training a kNN classifier is a noop: # the Classifier simply remembers the data and does no further processing classifier = KNearestNeighbor()classifier.train(X_train, y_train) We would now like to classify the test data with the kNN classifier. Recall that we can break down this process into two steps: First we must compute the distances between all test examples and all train examples. Given these distances, for each test example we find the k nearest examples and have them vote for the label Lets begin with computing the distance matrix between all training and test examples. For example, if there are Ntr training examples and Nte test examples, this stage should result in a Nte x Ntr matrix where each element (i,j) is the distance between the i-th test and j-th train example. First, open cs231n/classifiers/k_nearest_neighbor.py and implement the function compute_distances_two_loops that uses a (very inefficient) double loop over all pairs of (test, train) examples and computes the distance matrix one element at a time. 123456# Open cs231n/classifiers/k_nearest_neighbor.py and implement# compute_distances_two_loops.# Test your implementation:dists = classifier.compute_distances_two_loops(X_test)print dists.shape (500L, 5000L) ​ 1234# We can visualize the distance matrix: each row is a single test example and# its distances to training examplesplt.imshow(dists, interpolation='none')plt.show() Inline Question #1: Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.) What in the data is the cause behind the distinctly bright rows? What causes the columns? Your Answer: Maybe exists noises in test data set and train data set. 12345678# Now implement the function predict_labels and run the code below:# We use k = 1 (which is Nearest Neighbor).y_test_pred = classifier.predict_labels(dists, k=1)# Compute and print the fraction of correctly predicted examplesnum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy) Got 137 / 500 correct =&gt; accuracy: 0.274000 ​ You should expect to see approximately 27% accuracy. Now lets try out a larger k, say k = 5: 1234y_test_pred = classifier.predict_labels(dists, k=5)num_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy) Got 142 / 500 correct =&gt; accuracy: 0.284000 ​ You should expect to see a slightly better performance than with k = 1. 1234567891011121314151617# Now lets speed up distance matrix computation by using partial vectorization# with one loop. Implement the function compute_distances_one_loop and run the# code below:dists_one = classifier.compute_distances_one_loop(X_test)# To ensure that our vectorized implementation is correct, we make sure that it# agrees with the naive implementation. There are many ways to decide whether# two matrices are similar; one of the simplest is the Frobenius norm. In case# you haven't seen it before, the Frobenius norm of two matrices is the square# root of the squared sum of differences of all elements; in other words, reshape# the matrices into vectors and compute the Euclidean distance between them.difference = np.linalg.norm(dists - dists_one, ord='fro')print 'Difference was: %f' % (difference, )if difference &lt; 0.001: print 'Good! The distance matrices are the same'else: print 'Uh-oh! The distance matrices are different' Difference was: 0.000000 Good! The distance matrices are the same 1234567891011# Now implement the fully vectorized version inside compute_distances_no_loops# and run the codedists_two = classifier.compute_distances_no_loops(X_test)# check that the distance matrix agrees with the one we computed before:difference = np.linalg.norm(dists - dists_two, ord='fro')print 'Difference was: %f' % (difference, )if difference &lt; 0.001: print 'Good! The distance matrices are the same'else: print 'Uh-oh! The distance matrices are different' Difference was: 0.000000 Good! The distance matrices are the same 123456789101112131415161718192021# Let's compare how fast the implementations aredef time_function(f, *args): """ Call a function f with args and return the time (in seconds) that it took to execute. """ import time tic = time.time() f(*args) toc = time.time() return toc - tictwo_loop_time = time_function(classifier.compute_distances_two_loops, X_test)print 'Two loop version took %f seconds' % two_loop_timeone_loop_time = time_function(classifier.compute_distances_one_loop, X_test)print 'One loop version took %f seconds' % one_loop_timeno_loop_time = time_function(classifier.compute_distances_no_loops, X_test)print 'No loop version took %f seconds' % no_loop_time# you should see significantly faster performance with the fully vectorized implementation Two loop version took 27.001000 seconds One loop version took 59.630000 seconds No loop version took 0.205000 seconds Cross-validationWe have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]X_train_folds = []y_train_folds = []################################################################################# TODO: ## Split up the training data into folds. After splitting, X_train_folds and ## y_train_folds should each be lists of length num_folds, where ## y_train_folds[i] is the label vector for the points in X_train_folds[i]. ## Hint: Look up the numpy array_split function. #################################################################################X_train_folds = np.array_split(X_train, num_folds)y_train_folds = np.array_split(y_train, num_folds)################################################################################# END OF YOUR CODE ################################################################################## A dictionary holding the accuracies for different values of k that we find# when running cross-validation. After running cross-validation,# k_to_accuracies[k] should be a list of length num_folds giving the different# accuracy values that we found when using that value of k.k_to_accuracies = &#123;&#125;################################################################################# TODO: ## Perform k-fold cross validation to find the best value of k. For each ## possible value of k, run the k-nearest-neighbor algorithm num_folds times, ## where in each case you use all but one of the folds as training data and the ## last fold as a validation set. Store the accuracies for all fold and all ## values of k in the k_to_accuracies dictionary. #################################################################################for k in k_choices: k_to_accuracies[k] = [] for fold in xrange(num_folds): train_X = np.append( X_train_folds[:fold], X_train_folds[fold+1:]).reshape( (X_train.shape[0] - X_train.shape[0]/num_folds, -1)) train_y = np.append( y_train_folds[:fold], y_train_folds[fold+1:]).reshape( (y_train.shape[0] - y_train.shape[0]/num_folds, -1)).flatten() classifier.train(train_X, train_y) dists = classifier.compute_distances_no_loops(X_train_folds[fold]) y_test_pred = classifier.predict_labels(dists, k) num_correct = np.sum(y_test_pred == y_train_folds[fold]) accuracy = float(num_correct) / len(y_train_folds[fold]) k_to_accuracies[k].append(accuracy) ################################################################################# END OF YOUR CODE ################################################################################## Print out the computed accuraciesfor k in sorted(k_to_accuracies): for accuracy in k_to_accuracies[k]: print 'k = %d, accuracy = %f' % (k, accuracy) k = 1, accuracy = 0.263000 k = 1, accuracy = 0.257000 k = 1, accuracy = 0.264000 k = 1, accuracy = 0.278000 k = 1, accuracy = 0.266000 k = 3, accuracy = 0.241000 k = 3, accuracy = 0.249000 k = 3, accuracy = 0.243000 k = 3, accuracy = 0.273000 k = 3, accuracy = 0.264000 k = 5, accuracy = 0.258000 k = 5, accuracy = 0.273000 k = 5, accuracy = 0.281000 k = 5, accuracy = 0.290000 k = 5, accuracy = 0.272000 k = 8, accuracy = 0.263000 k = 8, accuracy = 0.288000 k = 8, accuracy = 0.278000 k = 8, accuracy = 0.285000 k = 8, accuracy = 0.277000 k = 10, accuracy = 0.265000 k = 10, accuracy = 0.296000 k = 10, accuracy = 0.278000 k = 10, accuracy = 0.284000 k = 10, accuracy = 0.286000 k = 12, accuracy = 0.260000 k = 12, accuracy = 0.294000 k = 12, accuracy = 0.281000 k = 12, accuracy = 0.282000 k = 12, accuracy = 0.281000 k = 15, accuracy = 0.255000 k = 15, accuracy = 0.290000 k = 15, accuracy = 0.281000 k = 15, accuracy = 0.281000 k = 15, accuracy = 0.276000 k = 20, accuracy = 0.270000 k = 20, accuracy = 0.281000 k = 20, accuracy = 0.280000 k = 20, accuracy = 0.282000 k = 20, accuracy = 0.284000 k = 50, accuracy = 0.271000 k = 50, accuracy = 0.288000 k = 50, accuracy = 0.278000 k = 50, accuracy = 0.269000 k = 50, accuracy = 0.266000 k = 100, accuracy = 0.256000 k = 100, accuracy = 0.270000 k = 100, accuracy = 0.263000 k = 100, accuracy = 0.256000 k = 100, accuracy = 0.263000 12345678910111213# plot the raw observationsfor k in k_choices: accuracies = k_to_accuracies[k] plt.scatter([k] * len(accuracies), accuracies)# plot the trend line with error bars that correspond to standard deviationaccuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)plt.title('Cross-validation on k')plt.xlabel('k')plt.ylabel('Cross-validation accuracy')plt.show() 12345678910111213# Based on the cross-validation results above, choose the best value for k, # retrain the classifier using all the training data, and test it on the test# data. You should be able to get above 28% accuracy on the test data.best_k = 10classifier = KNearestNeighbor()classifier.train(X_train, y_train)y_test_pred = classifier.predict(X_test, k=best_k)# Compute and display the accuracynum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy) Got 139 / 500 correct =&gt; accuracy: 0.278000 ​ k_nearest_neighbor.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186import numpy as npimport operatorclass KNearestNeighbor(object): """ a kNN classifier with L2 distance """ def __init__(self): pass def train(self, X, y): """ Train the classifier. For k-nearest neighbors this is just memorizing the training data. Inputs: - X: A numpy array of shape (num_train, D) containing the training data consisting of num_train samples each of dimension D. - y: A numpy array of shape (N,) containing the training labels, where y[i] is the label for X[i]. """ self.X_train = X self.y_train = y def predict(self, X, k=1, num_loops=0): """ Predict labels for test data using this classifier. Inputs: - X: A numpy array of shape (num_test, D) containing test data consisting of num_test samples each of dimension D. - k: The number of nearest neighbors that vote for the predicted labels. - num_loops: Determines which implementation to use to compute distances between training points and testing points. Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. """ if num_loops == 0: dists = self.compute_distances_no_loops(X) elif num_loops == 1: dists = self.compute_distances_one_loop(X) elif num_loops == 2: dists = self.compute_distances_two_loops(X) else: raise ValueError('Invalid value %d for num_loops' % num_loops) return self.predict_labels(dists, k=k) def compute_distances_two_loops(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using a nested loop over both the training data and the test data. Inputs: - X: A numpy array of shape (num_test, D) containing test data. Returns: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] is the Euclidean distance between the ith test point and the jth training point. """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): for j in xrange(num_train): ############################################################### # TODO: # # Compute the l2 distance between the ith test point and the jth # # training point, and store the result in dists[i, j]. You should # # not use a loop over dimension. # ############################################################### dists[i, j] = np.sqrt(np.sum((X[i, :] - self.X_train[j, :]) ** 2)) ############################################################### # END OF YOUR CODE # ############################################################### return dists def compute_distances_one_loop(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using a single loop over the test data. Input / Output: Same as compute_distances_two_loops """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): ################################################################### # TODO: # # Compute the l2 distance between the ith test point and all training # # points, and store the result in dists[i, :]. # ################################################################### dists[i, :] = np.sqrt(np.sum(np.square(X[i, :] - self.X_train), axis=1)) ################################################################### # END OF YOUR CODE # ################################################################### return dists def compute_distances_no_loops(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using no explicit loops. Input / Output: Same as compute_distances_two_loops """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) ####################################################################### # TODO: # # Compute the l2 distance between all test points and all training # # points without using any explicit loops, and store the result in # # dists. # # # # You should implement this function using only basic array operations; # # in particular you should not use functions from scipy. # # # # HINT: Try to formulate the l2 distance using matrix multiplication # # and two broadcast sums. # ####################################################################### dists = np.sqrt(np.multiply(np.dot(X, self.X_train.T), -2) + np.sum(self.X_train ** 2, axis=1) + np.sum(X ** 2, axis=1)[:, np.newaxis]) ####################################################################### # END OF YOUR CODE # ####################################################################### return dists def predict_labels(self, dists, k=1): """ Given a matrix of distances between test points and training points, predict a label for each test point. Inputs: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] gives the distance betwen the ith test point and the jth training point. Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. """ num_test = dists.shape[0] y_pred = np.zeros(num_test) for i in xrange(num_test): # A list of length k storing the labels of the k nearest neighbors to # the ith test point. closest_y = [] ################################################################### # TODO: # # Use the distance matrix to find the k nearest neighbors of the ith # # testing point, and use self.y_train to find the labels of these # # neighbors. Store these labels in closest_y. # # Hint: Look up the function numpy.argsort. # ################################################################### k_nearest_index = np.argsort(dists[i, :])[:k] ################################################################### # TODO: # # Now that you have found the labels of the k nearest neighbors, you # # need to find the most common label in the list closest_y of labels. # # Store this label in y_pred[i]. Break ties by choosing the smaller # # label. # ################################################################### closest_y = self.y_train[k_nearest_index] labels_counts = &#123;&#125; for label in closest_y: if label in labels_counts.keys(): labels_counts[label] += 1 else: labels_counts[label] = 0 sorted_labels_counts = sorted( labels_counts.items(), key=operator.itemgetter(1), reverse=True) y_pred[i] = sorted_labels_counts[0][0] ################################################################### # END OF YOUR CODE # ################################################################### return y_pred]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS231n Lecture3 note]]></title>
      <url>%2F2017%2F03%2F02%2FCS231n-Lecture3-note%2F</url>
      <content type="text"><![CDATA[Loss functionSource is here. Multiclass Support Vector Machine lossThe SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$ The Multiclass SVM loss for the i-th example is formalized as follows: Example $s = [13, -7, 11]$ and $\Delta = 10$, then, L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)In summary, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by at least by $\Delta$ (delta). If this is not the case, we will accumulate loss. Note that $f(x_i; W) = W x_i$, so we can also rewrite the loss function in this equivalent form: L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)A last piece of terminology we’ll mention before we finish with this section is that the threshold at zero $\max(0,−)$ function is often called the hinge loss. You’ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form $\max(0,−)^2$ that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation. The follow image shows the motivation of the SVM loss function” RegularizationThere is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters W that correctly classify every example (i.e. all scores are so that all the margins are met, and $L_i=0$ for all i). The issue is that this set of W is not necessarily unique: there might be many similar W that correctly classify the examples. One easy way to see this is that if some parameters W correctly classify all examples (so loss is zero for each example), then any multiple of these parameters $\lambda W$ where $\lambda &gt; 1$ will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30. We can avoid this by extending the loss function with a regularization penalty $R(W)$. The most common regularization penalty is the L2 norm: R(W) = \sum_k\sum_l W_{k,l}^2 That is, the full Multiclass SVM loss becomes: L = \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\Or expanding this out in its full form: L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2Including the L2 penalty leads to the appealing max margin property in SVMs (See CS229 lecture notes for full details if you are interested). The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. For example, suppose that we have some input vector $x=[1,1,1,1]$ and two weight vectors $w_1=[1,0,0,0]$, $w_2=[0.25,0.25,0.25,0.25]$. Then $w^T_1x=w^T_2x=1$ so both weight vectors lead to the same dot product, but the L2 penalty of $w_1$ is 1.0 while the L2 penalty of $w_2$ is only 0.25. Therefore, according to the L2 penalty the weight vector $w_2$ would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in $w_2$ are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting. Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546def L_i(x, y, W): """ unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) """ delta = 1.0 # see notes about delta later in this section scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class correct_class_score = scores[y] D = W.shape[0] # number of classes, e.g. 10 loss_i = 0.0 for j in xrange(D): # iterate over all wrong classes if j == y: # skip for the true class to only loop over incorrect classes continue # accumulate loss for the i-th example loss_i += max(0, scores[j] - correct_class_score + delta) return loss_idef L_i_vectorized(x, y, W): """ A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) """ delta = 1.0 scores = W.dot(x) # compute the margins for all classes in one vector operation margins = np.maximum(0, scores - scores[y] + delta) # on y-th position scores[y] - scores[y] canceled and gave delta. We want # to ignore the y-th position and only consider margin on max wrong class margins[y] = 0 loss_i = np.sum(margins) return loss_idef L(X, y, W): """ fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) """ # evaluate loss over all examples in X without using any for loops # left as exercise to reader in the assignment Practice ConsiderationsSetting DeltaNote that we brushed over the hyperparameter $\Delta$ and its setting. What value should it be set to, and do we have to cross-validate it? It turns out that this hyperparameter can safely be set to $\Delta = 1.0$ in all cases. The hyperparameters $\Delta$ and $\lambda$ seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights W has direct effect on the scores (and hence also their differences): As we shrink all values inside W the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. $\Delta = 10$, or$\Delta = 100$) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength $\lambda$). Softmax classifierIn the Softmax classifier, the function mapping $f(x_i; W) = W x_i$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form: L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}The function $f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}$ is called the softmax function. Information theory viewThe cross-entropy between a “true” distribution $p$ and an estimated distribution $q$ is defined as: H(p,q) = - \sum_x p(x) \log q(x)The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ( $q = e^{f_{y_i}} / \sum_j e^{f_j}$ as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. $p = [0, \ldots 1, \ldots, 0]$ contains a single 1 at the $y_i$ -th position.). Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q) = H(p) + D_{KL}(p||q)$, and the entropy of the delta function $p$ is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer. Probabilistic interpretationP(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }can be interpreted as the (normalized) probability assigned to the correct label $y_i$given the image $x_i$ and parameterized by W. To see this, remember that the Softmax classifier interprets the scores inside the output vector $f$ as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE). A nice feature of this view is that we can now also interpret the regularization term $R(W)$ in the full loss function as coming from a Gaussian prior over the weight matrix W, where instead of MLE we are performing the Maximum a posteriori (MAP) estimation. Numeric stabilityWhen you’re writing code for computing the Softmax function in practice, the intermediate term $e^{f_{y_i}}$ and $\sum_j e^{f_j}$ may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant C and push it into the sum, we get the following (mathematically equivalent) expression: \frac{e^{f_{y_i}}}{\sum_j e^{f_j}}= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}We are free to choose the value of C. This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for C is to set $\log C = -\max_j f_j$. This simply states that we should shift the values inside the vector ff so that the highest value is zero. In code: 123456f = np.array([123, 456, 789]) # example with 3 classes and each having large scoresp = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup# instead: first shift the values of f so that the highest number is 0:f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer Some tricksHow peaky or diffuse these probabilities are depends directly on the regularization strength $\lambda$ - which you are in charge of as input to the system. For example, suppose that the unnormalized log-probabilities for some three classes come out to be [1, -2, 0]. The softmax function would then compute: [1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]Where the steps taken are to exponentiate and normalize to sum to one. Now, if the regularization strength $\lambda$ was higher, the weights W would be penalized more and this would lead to smaller weights. For example, suppose that the weights became one half smaller ([0.5, -1, 0]). The softmax would now compute: [0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]where the probabilites are now more diffuse. Futher Reading Deep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax. OptimizationStrategy #1: A first very bad idea solution: Random search12345678910111213141516171819202122# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)# assume Y_train are the labels (e.g. 1D array of 50,000)# assume the function L evaluates the loss functionbestloss = float("inf") # Python assigns the highest possible float valuefor num in xrange(1000): W = np.random.randn(10, 3073) * 0.0001 # generate random parameters loss = L(X_train, Y_train, W) # get the loss over the entire training set if loss &lt; bestloss: # keep track of the best solution bestloss = loss bestW = W print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)# prints:# in attempt 0 the loss was 9.401632, best 9.401632# in attempt 1 the loss was 8.959668, best 8.959668# in attempt 2 the loss was 9.044034, best 8.959668# in attempt 3 the loss was 9.278948, best 8.959668# in attempt 4 the loss was 8.857370, best 8.857370# in attempt 5 the loss was 8.943151, best 8.857370# in attempt 6 the loss was 8.605604, best 8.605604# ... (trunctated: continues for 1000 lines) We can take the best weights W found by this search and try it out on the test set: 1234567# Assume X_test is [3073 x 10000], Y_test [10000 x 1]scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples# find the index with max score in each column (the predicted class)Yte_predict = np.argmax(scores, axis = 0)# and calculate accuracy (fraction of predictions that are correct)np.mean(Yte_predict == Yte)# returns 0.1555 With the best W this gives an accuracy of about 15.5%. Core idea: iterative refinement. Of course, it turns out that we can do much better. The core idea is that finding the best set of weights W is a very difficult or even impossible problem (especially once W contains weights for entire complex neural networks), but the problem of refining a specific set of weights W to be slightly better is significantly less difficult. In other words, our approach will be to start with a random W and then iteratively refine it, making it slightly better each time. Our strategy will be to start with random weights and iteratively refine them over time to get lower loss Strategy #2: Random Local SearchConcretely, we will start out with a random WW, generate random perturbations $\delta W$ to it and if the loss at the perturbed $W + \delta W$ is lower, we will perform an update. The code for this procedure is as follows: 12345678910W = np.random.randn(10, 3073) * 0.001 # generate random starting Wbestloss = float("inf")for i in xrange(1000): step_size = 0.0001 Wtry = W + np.random.randn(10, 3073) * step_size loss = L(Xtr_cols, Ytr, Wtry) if loss &lt; bestloss: W = Wtry bestloss = loss print 'iter %d loss is %f' % (i, bestloss) This approach achieves test set classification accuracy of 21.4%. Strategy #3: Following the GradientThe mathematical expression for the derivative of a 1-D function with respect its input is: \frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension. Computing the gradientThere are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone way that requires calculus (analytic gradient). We will now present both. Computing the gradient numerically with finite differences123456789101112131415161718192021222324252627def eval_numerical_gradient(f, x): """ a naive implementation of numerical gradient of f at x - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at """ fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.00001 # iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # evaluate function at x+h ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh = f(x) # evalute f(x + h) x[ix] = old_value # restore to previous value (very important!) # compute the partial derivative grad[ix] = (fxh - fx) / h # the slope it.iternext() # step to next dimension return grad Practical considerations. Note that in the mathematical formulation the gradient is defined in the limit as h goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the centered difference formula: [f(x+h) - f(x-h)] / 2 hSee wiki for details. 1234567# to use the generic code above we want a function that takes a single argument# (the weights in our case) so we close over X_train and Y_traindef CIFAR10_loss_fun(W): return L(X_train, Y_train, W)W = np.random.rand(10, 3073) * 0.001 # random weight vectordf = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient Then we can use to make an update: 12345678910111213141516171819202122loss_original = CIFAR10_loss_fun(W) # the original lossprint 'original loss: %f' % (loss_original, )# lets see the effect of multiple step sizesfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]: step_size = 10 ** step_size_log W_new = W - step_size * df # new position in the weight space loss_new = CIFAR10_loss_fun(W_new) print 'for step size %f new loss: %f' % (step_size, loss_new)# prints:# original loss: 2.200718# for step size 1.000000e-10 new loss: 2.200652# for step size 1.000000e-09 new loss: 2.200057# for step size 1.000000e-08 new loss: 2.194116# for step size 1.000000e-07 new loss: 2.135493# for step size 1.000000e-06 new loss: 1.647802# for step size 1.000000e-05 new loss: 2.844355# for step size 1.000000e-04 new loss: 25.558142# for step size 1.000000e-03 new loss: 254.086573# for step size 1.000000e-02 new loss: 2539.370888# for step size 1.000000e-01 new loss: 25392.214036 Effect of step size Computing the gradient analytically with CalculusHowever, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a gradient check. Lets use the example of the SVM loss function for a single datapoint: L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right] \nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) \right) x_iwhen you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector $x_i$ scaled by this number is the gradient. \nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) x_i]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note Ch07]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-Ch07%2F</url>
      <content type="text"><![CDATA[数据规整化：清理、转换、合并、重塑1234567891011121314from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandasimport pandas as pdnp.set_printoptions(precision=4, threshold=500)pd.options.display.max_rows = 100from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all" 1%matplotlib inline 合并数据集数据库风格的DataFrame合并12345df1 = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)&#125;)df2 = DataFrame(&#123;'key': ['a', 'b', 'd'], 'data2': range(3)&#125;)df1 data1 key 0 0 b 1 1 b 2 2 a 3 3 c 4 4 a 5 5 a 6 6 b 1df2 data2 key 0 0 a 1 1 b 2 2 d 默认情况下根据重叠的列名进行合并 1pd.merge(df1, df2) data1 key data2 0 0 b 1 1 1 b 1 2 6 b 1 3 2 a 0 4 4 a 0 5 5 a 0 最好进行显式地指定 1pd.merge(df1, df2, on='key') data1 key data2 0 0 b 1 1 1 b 1 2 6 b 1 3 2 a 0 4 4 a 0 5 5 a 0 1234df3 = DataFrame(&#123;'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)&#125;)df4 = DataFrame(&#123;'rkey': ['a', 'b', 'd'], 'data2': range(3)&#125;) 1df3 data1 lkey 0 0 b 1 1 b 2 2 a 3 3 c 4 4 a 5 5 a 6 6 b 1df4 data2 rkey 0 0 a 1 1 b 2 2 d 如果两个对象的列名不同，那么就需要分别指定 1pd.merge(df3, df4, left_on='lkey', right_on='rkey') data1 lkey data2 rkey 0 0 b 1 b 1 1 b 1 b 2 6 b 1 b 3 2 a 0 a 4 4 a 0 a 5 5 a 0 a 默认是进行inner连接（交集）， outer是求取并集 1pd.merge(df1, df2, how='outer') data1 key data2 0 0.0 b 1.0 1 1.0 b 1.0 2 6.0 b 1.0 3 2.0 a 0.0 4 4.0 a 0.0 5 5.0 a 0.0 6 3.0 c NaN 7 NaN d 2.0 1234df1 = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)&#125;)df2 = DataFrame(&#123;'key': ['a', 'b', 'a', 'b', 'd'], 'data2': range(5)&#125;) 1df1 data1 key 0 0 b 1 1 b 2 2 a 3 3 c 4 4 a 5 5 b 1df2 data2 key 0 0 a 1 1 b 2 2 a 3 3 b 4 4 d 1pd.merge(df1, df2, on='key', how='left') data1 key data2 0 0 b 1.0 1 0 b 3.0 2 1 b 1.0 3 1 b 3.0 4 2 a 0.0 5 2 a 2.0 6 3 c NaN 7 4 a 0.0 8 4 a 2.0 9 5 b 1.0 10 5 b 3.0 1pd.merge(df1, df2, how='inner') data1 key data2 0 0 b 1 1 0 b 3 2 1 b 1 3 1 b 3 4 5 b 1 5 5 b 3 6 2 a 0 7 2 a 2 8 4 a 0 9 4 a 2 123456left = DataFrame(&#123;'key1': ['foo', 'foo', 'bar'], 'key2': ['one', 'two', 'one'], 'lval': [1, 2, 3]&#125;)right = DataFrame(&#123;'key1': ['foo', 'foo', 'bar', 'bar'], 'key2': ['one', 'one', 'one', 'two'], 'rval': [4, 5, 6, 7]&#125;) 12 1left key1 key2 lval 0 foo one 1 1 foo two 2 2 bar one 3 1right key1 key2 rval 0 foo one 4 1 foo one 5 2 bar one 6 3 bar two 7 1pd.merge(left, right, on=['key1', 'key2'], how='outer') key1 key2 lval rval 0 foo one 1.0 4.0 1 foo one 1.0 5.0 2 foo two 2.0 NaN 3 bar one 3.0 6.0 4 bar two NaN 7.0 列名重复问题 1pd.merge(left, right, on='key1') key1 key2_x lval key2_y rval 0 foo one 1 one 4 1 foo one 1 one 5 2 foo two 2 one 4 3 foo two 2 one 5 4 bar one 3 one 6 5 bar one 3 two 7 1pd.merge(left, right, on='key1', suffixes=('_left', '_right')) key1 key2_left lval key2_right rval 0 foo one 1 one 4 1 foo one 1 one 5 2 foo two 2 one 4 3 foo two 2 one 5 4 bar one 3 one 6 5 bar one 3 two 7 索引上的合并123left1 = DataFrame(&#123;'key': ['a', 'b', 'a', 'a', 'b', 'c'], 'value': range(6)&#125;)right1 = DataFrame(&#123;'group_val': [3.5, 7]&#125;, index=['a', 'b']) 1left1 key value 0 a 0 1 b 1 2 a 2 3 a 3 4 b 4 5 c 5 1right1 group_val a 3.5 b 7.0 1pd.merge(left1, right1, left_on='key', right_index=True) key value group_val 0 a 0 3.5 2 a 2 3.5 3 a 3 3.5 1 b 1 7.0 4 b 4 7.0 1pd.merge(left1, right1, left_on='key', right_index=True, how='outer') key value group_val 0 a 0 3.5 2 a 2 3.5 3 a 3 3.5 1 b 1 7.0 4 b 4 7.0 5 c 5 NaN 12345678lefth = DataFrame(&#123;'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'key2': [2000, 2001, 2002, 2001, 2002], 'data': np.arange(5.)&#125;)righth = DataFrame(np.arange(12).reshape((6, 2)), index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'], [2001, 2000, 2000, 2000, 2001, 2002]], columns=['event1', 'event2'])lefth data key1 key2 0 0.0 Ohio 2000 1 1.0 Ohio 2001 2 2.0 Ohio 2002 3 3.0 Nevada 2001 4 4.0 Nevada 2002 1righth event1 event2 Nevada 2001 0 1 2000 2 3 Ohio 2000 4 5 2000 6 7 2001 8 9 2002 10 11 1pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True) data key1 key2 event1 event2 0 0.0 Ohio 2000 4 5 0 0.0 Ohio 2000 6 7 1 1.0 Ohio 2001 8 9 2 2.0 Ohio 2002 10 11 3 3.0 Nevada 2001 0 1 12pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True, how='outer') data key1 key2 event1 event2 0 0.0 Ohio 2000.0 4.0 5.0 0 0.0 Ohio 2000.0 6.0 7.0 1 1.0 Ohio 2001.0 8.0 9.0 2 2.0 Ohio 2002.0 10.0 11.0 3 3.0 Nevada 2001.0 0.0 1.0 4 4.0 Nevada 2002.0 NaN NaN 4 NaN Nevada 2000.0 2.0 3.0 1234left2 = DataFrame([[1., 2.], [3., 4.], [5., 6.]], index=['a', 'c', 'e'], columns=['Ohio', 'Nevada'])right2 = DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]], index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama']) 1left2 Ohio Nevada a 1.0 2.0 c 3.0 4.0 e 5.0 6.0 1right2 Missouri Alabama b 7.0 8.0 c 9.0 10.0 d 11.0 12.0 e 13.0 14.0 1pd.merge(left2, right2, how='outer', left_index=True, right_index=True) Ohio Nevada Missouri Alabama a 1.0 2.0 NaN NaN b NaN NaN 7.0 8.0 c 3.0 4.0 9.0 10.0 d NaN NaN 11.0 12.0 e 5.0 6.0 13.0 14.0 1left2.join(right2, how='outer') Ohio Nevada Missouri Alabama a 1.0 2.0 NaN NaN b NaN NaN 7.0 8.0 c 3.0 4.0 9.0 10.0 d NaN NaN 11.0 12.0 e 5.0 6.0 13.0 14.0 1left1.join(right1, on='key') key value group_val 0 a 0 3.5 1 b 1 7.0 2 a 2 3.5 3 a 3 3.5 4 b 4 7.0 5 c 5 NaN 12another = DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]], index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon']) New York Oregon a 7.0 8.0 c 9.0 10.0 e 11.0 12.0 f 16.0 17.0 相当于三个表进行合并 1234left2right2anotherleft2.join([right2, another]) Ohio Nevada a 1.0 2.0 c 3.0 4.0 e 5.0 6.0 Missouri Alabama b 7.0 8.0 c 9.0 10.0 d 11.0 12.0 e 13.0 14.0 New York Oregon a 7.0 8.0 c 9.0 10.0 e 11.0 12.0 f 16.0 17.0 Ohio Nevada Missouri Alabama New York Oregon a 1.0 2.0 NaN NaN 7.0 8.0 c 3.0 4.0 9.0 10.0 9.0 10.0 e 5.0 6.0 13.0 14.0 11.0 12.0 1left2.join([right2, another], how='outer') 轴向连接之前指的都是行级别的连接操作 1arr = np.arange(12).reshape((3, 4)) 1arr array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) 1np.concatenate([arr, arr], axis=1) array([[ 0, 1, 2, 3, 0, 1, 2, 3], [ 4, 5, 6, 7, 4, 5, 6, 7], [ 8, 9, 10, 11, 8, 9, 10, 11]]) 123s1 = Series([0, 1], index=['a', 'b'])s2 = Series([2, 3, 4], index=['c', 'd', 'e'])s3 = Series([5, 6], index=['f', 'g']) 1pd.concat([s1, s2, s3]) a 0 b 1 c 2 d 3 e 4 f 5 g 6 dtype: int64 1pd.concat([s1, s2, s3], axis=1) 0 1 2 a 0.0 NaN NaN b 1.0 NaN NaN c NaN 2.0 NaN d NaN 3.0 NaN e NaN 4.0 NaN f NaN NaN 5.0 g NaN NaN 6.0 12s4 = pd.concat([s1 * 5, s3])s4 a 0 b 5 f 5 g 6 dtype: int64 1pd.concat([s1, s4], axis=1) 0 1 a 0.0 0 b 1.0 5 f NaN 5 g NaN 6 1pd.concat([s1, s4], axis=1, join='inner') 0 1 a 0 0 b 1 5 1pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']]) 0 1 a 0.0 0.0 c NaN NaN b 1.0 5.0 e NaN NaN 在连接轴上建立一个层次化索引 123s1s3result = pd.concat([s1, s1, s3], keys=['one', 'two', 'three']) a 0 b 1 dtype: int64 f 5 g 6 dtype: int64 1result one a 0 b 1 two a 0 b 1 three f 5 g 6 dtype: int64 12# Much more on the unstack function laterresult.unstack() a b f g one 0.0 1.0 NaN NaN two 0.0 1.0 NaN NaN three NaN NaN 5.0 6.0 1pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three']) one two three a 0.0 NaN NaN b 1.0 NaN NaN c NaN 2.0 NaN d NaN 3.0 NaN e NaN 4.0 NaN f NaN NaN 5.0 g NaN NaN 6.0 12345678df1 = DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'], columns=['one', 'two'])df2 = DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'], columns=['three', 'four'])df1df2pd.concat([df1, df2], keys=['level1', 'level2'])pd.concat([df1, df2], axis=1, keys=['level1', 'level2']) one two a 0 1 b 2 3 c 4 5 three four a 5 6 c 7 8 four one three two level1 a NaN 0.0 NaN 1.0 b NaN 2.0 NaN 3.0 c NaN 4.0 NaN 5.0 level2 a 6.0 NaN 5.0 NaN c 8.0 NaN 7.0 NaN level1 level2 one two three four a 0 1 5.0 6.0 b 2 3 NaN NaN c 4 5 7.0 8.0 12pd.concat(&#123;'level1': df1, 'level2': df2&#125;, axis=0)pd.concat(&#123;'level1': df1, 'level2': df2&#125;, axis=1) four one three two level1 a NaN 0.0 NaN 1.0 b NaN 2.0 NaN 3.0 c NaN 4.0 NaN 5.0 level2 a 6.0 NaN 5.0 NaN c 8.0 NaN 7.0 NaN level1 level2 one two three four a 0 1 5.0 6.0 b 2 3 NaN NaN c 4 5 7.0 8.0 12pd.concat([df1, df2], axis=1, keys=['level1', 'level2'], names=['upper', 'lower']) upper level1 level2 lower one two three four a 0 1 5.0 6.0 b 2 3 NaN NaN c 4 5 7.0 8.0 12df1 = DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])df2 = DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a']) 1df1 a b c d 0 -0.204708 0.478943 -0.519439 -0.555730 1 1.965781 1.393406 0.092908 0.281746 2 0.769023 1.246435 1.007189 -1.296221 1df2 b d a 0 0.274992 0.228913 1.352917 1 0.886429 -2.001637 -0.371843 去除无关的行索引 12pd.concat([df1, df2], ignore_index=False)pd.concat([df1, df2], ignore_index=True) a b c d 0 -0.204708 0.478943 -0.519439 -0.555730 1 1.965781 1.393406 0.092908 0.281746 2 0.769023 1.246435 1.007189 -1.296221 0 1.352917 0.274992 NaN 0.228913 1 -0.371843 0.886429 NaN -2.001637 a b c d 0 -0.204708 0.478943 -0.519439 -0.555730 1 1.965781 1.393406 0.092908 0.281746 2 0.769023 1.246435 1.007189 -1.296221 3 1.352917 0.274992 NaN 0.228913 4 -0.371843 0.886429 NaN -2.001637 合并重叠数据12345a = Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan], index=['f', 'e', 'd', 'c', 'b', 'a'])b = Series(np.arange(len(a), dtype=np.float64), index=['f', 'e', 'd', 'c', 'b', 'a'])b[-1] = np.nan 1a f NaN e 2.5 d NaN c 3.5 b 4.5 a NaN dtype: float64 1b f 0.0 e 1.0 d 2.0 c 3.0 b 4.0 a NaN dtype: float64 1np.where(pd.isnull(a), b, a) array([ 0. , 2.5, 2. , 3.5, 4.5, nan]) combine_first, 重叠值合并，且进行数据对其 1b[:-2].combine_first(a[2:]) a NaN b 4.5 c 3.0 d 2.0 e 1.0 f 0.0 dtype: float64 12345678df1 = DataFrame(&#123;'a': [1., np.nan, 5., np.nan], 'b': [np.nan, 2., np.nan, 6.], 'c': range(2, 18, 4)&#125;)df2 = DataFrame(&#123;'a': [5., 4., np.nan, 3., 7.], 'b': [np.nan, 3., 4., 6., 8.]&#125;)df1df2df1.combine_first(df2) a b c 0 1.0 NaN 2 1 NaN 2.0 6 2 5.0 NaN 10 3 NaN 6.0 14 a b 0 5.0 NaN 1 4.0 3.0 2 NaN 4.0 3 3.0 6.0 4 7.0 8.0 a b c 0 1.0 NaN 2.0 1 4.0 2.0 6.0 2 5.0 4.0 10.0 3 3.0 6.0 14.0 4 7.0 8.0 NaN 重塑和轴向旋转重塑层次化索引1234data = DataFrame(np.arange(6).reshape((2, 3)), index=pd.Index(['Ohio', 'Colorado'], name='state'), columns=pd.Index(['one', 'two', 'three'], name='number'))data number one two three state Ohio 0 1 2 Colorado 3 4 5 stack将列旋转为行 12result = data.stack()result state number Ohio one 0 two 1 three 2 Colorado one 3 two 4 three 5 dtype: int32 unstack将行旋转为列，默认操作最内层 1result.unstack() number one two three state Ohio 0 1 2 Colorado 3 4 5 1result.unstack(0) state Ohio Colorado number one 0 3 two 1 4 three 2 5 1result.unstack('state') state Ohio Colorado number one 0 3 two 1 4 three 2 5 1234567s1 = Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])s2 = Series([4, 5, 6], index=['c', 'd', 'e'])s1s2data2 = pd.concat([s1, s2], keys=['one', 'two'])data2data2.unstack() a 0 b 1 c 2 d 3 dtype: int64 c 4 d 5 e 6 dtype: int64 one a 0 b 1 c 2 d 3 two c 4 d 5 e 6 dtype: int64 a b c d e one 0.0 1.0 2.0 3.0 NaN two NaN NaN 4.0 5.0 6.0 stack默认会滤除缺失值，因此两者可逆 1data2.unstack().stack() one a 0.0 b 1.0 c 2.0 d 3.0 two c 4.0 d 5.0 e 6.0 dtype: float64 1data2.unstack().stack(dropna=False) one a 0.0 b 1.0 c 2.0 d 3.0 e NaN two a NaN b NaN c 4.0 d 5.0 e 6.0 dtype: float64 1234resultdf = DataFrame(&#123;'left': result, 'right': result + 5&#125;, columns=pd.Index(['left', 'right'], name='side'))df state number Ohio one 0 two 1 three 2 Colorado one 3 two 4 three 5 dtype: int32 side left right state number Ohio one 0 5 two 1 6 three 2 7 Colorado one 3 8 two 4 9 three 5 10 DataFrame作为旋转轴的级别将成为结果中的最低级别（axis=MAX） 1df.unstack('state') side left right state Ohio Colorado Ohio Colorado number one 0 3 5 8 two 1 4 6 9 three 2 5 7 10 stack操作将axis-1? 1df.unstack('state').stack('side') state Ohio Colorado number side one left 0 3 right 5 8 two left 1 4 right 6 9 three left 2 5 right 7 10 12df.unstack('state')df.unstack('state').stack('state') side left right state Ohio Colorado Ohio Colorado number one 0 3 5 8 two 1 4 6 9 three 2 5 7 10 side left right number state one Ohio 0 5 Colorado 3 8 two Ohio 1 6 Colorado 4 9 three Ohio 2 7 Colorado 5 10 将长格式旋转为宽格式12345678910111213data = pd.read_csv('ch07/macrodata.csv')data[:10]periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name='date')periods[:10]data = DataFrame(data.to_records(), columns=pd.Index(['realgdp', 'infl', 'unemp'], name='item'), index=periods.to_timestamp('D', 'end'))data[:10]data.to_records()[:10]data.stack()[:10]ldata = data.stack().reset_index().rename(columns=&#123;0: 'value'&#125;)wdata = ldata.pivot('date', 'item', 'value') year quarter realgdp realcons realinv realgovt realdpi cpi m1 tbilrate unemp pop infl realint 0 1959.0 1.0 2710.349 1707.4 286.898 470.045 1886.9 28.98 139.7 2.82 5.8 177.146 0.00 0.00 1 1959.0 2.0 2778.801 1733.7 310.859 481.301 1919.7 29.15 141.7 3.08 5.1 177.830 2.34 0.74 2 1959.0 3.0 2775.488 1751.8 289.226 491.260 1916.4 29.35 140.5 3.82 5.3 178.657 2.74 1.09 3 1959.0 4.0 2785.204 1753.7 299.356 484.052 1931.3 29.37 140.0 4.33 5.6 179.386 0.27 4.06 4 1960.0 1.0 2847.699 1770.5 331.722 462.199 1955.5 29.54 139.6 3.50 5.2 180.007 2.31 1.19 5 1960.0 2.0 2834.390 1792.9 298.152 460.400 1966.1 29.55 140.2 2.68 5.2 180.671 0.14 2.55 6 1960.0 3.0 2839.022 1785.8 296.375 474.676 1967.8 29.75 140.9 2.36 5.6 181.528 2.70 -0.34 7 1960.0 4.0 2802.616 1788.2 259.764 476.434 1966.6 29.84 141.1 2.29 6.3 182.287 1.21 1.08 8 1961.0 1.0 2819.264 1787.7 266.405 475.854 1984.5 29.81 142.1 2.37 6.8 182.992 -0.40 2.77 9 1961.0 2.0 2872.005 1814.3 286.246 480.328 2014.4 29.92 142.9 2.29 7.0 183.691 1.47 0.81 PeriodIndex([&#39;1959Q1&#39;, &#39;1959Q2&#39;, &#39;1959Q3&#39;, &#39;1959Q4&#39;, &#39;1960Q1&#39;, &#39;1960Q2&#39;, &#39;1960Q3&#39;, &#39;1960Q4&#39;, &#39;1961Q1&#39;, &#39;1961Q2&#39;], dtype=&#39;int64&#39;, name=&#39;date&#39;, freq=&#39;Q-DEC&#39;) item realgdp infl unemp date 1959-03-31 2710.349 0.00 5.8 1959-06-30 2778.801 2.34 5.1 1959-09-30 2775.488 2.74 5.3 1959-12-31 2785.204 0.27 5.6 1960-03-31 2847.699 2.31 5.2 1960-06-30 2834.390 0.14 5.2 1960-09-30 2839.022 2.70 5.6 1960-12-31 2802.616 1.21 6.3 1961-03-31 2819.264 -0.40 6.8 1961-06-30 2872.005 1.47 7.0 rec.array([(datetime.datetime(1959, 3, 31, 0, 0), 2710.349, 0.0, 5.8), (datetime.datetime(1959, 6, 30, 0, 0), 2778.801, 2.34, 5.1), (datetime.datetime(1959, 9, 30, 0, 0), 2775.488, 2.74, 5.3), (datetime.datetime(1959, 12, 31, 0, 0), 2785.204, 0.27, 5.6), (datetime.datetime(1960, 3, 31, 0, 0), 2847.699, 2.31, 5.2), (datetime.datetime(1960, 6, 30, 0, 0), 2834.39, 0.14, 5.2), (datetime.datetime(1960, 9, 30, 0, 0), 2839.022, 2.7, 5.6), (datetime.datetime(1960, 12, 31, 0, 0), 2802.616, 1.21, 6.3), (datetime.datetime(1961, 3, 31, 0, 0), 2819.264, -0.4, 6.8), (datetime.datetime(1961, 6, 30, 0, 0), 2872.005, 1.47, 7.0)], dtype=[(&#39;date&#39;, &#39;O&#39;), (&#39;realgdp&#39;, &#39;&lt;f8&#39;), (&#39;infl&#39;, &#39;&lt;f8&#39;), (&#39;unemp&#39;, &#39;&lt;f8&#39;)]) date item 1959-03-31 realgdp 2710.349 infl 0.000 unemp 5.800 1959-06-30 realgdp 2778.801 infl 2.340 unemp 5.100 1959-09-30 realgdp 2775.488 infl 2.740 unemp 5.300 1959-12-31 realgdp 2785.204 dtype: float64 1ldata[:10] date item value 0 1959-03-31 realgdp 2710.349 1 1959-03-31 infl 0.000 2 1959-03-31 unemp 5.800 3 1959-06-30 realgdp 2778.801 4 1959-06-30 infl 2.340 5 1959-06-30 unemp 5.100 6 1959-09-30 realgdp 2775.488 7 1959-09-30 infl 2.740 8 1959-09-30 unemp 5.300 9 1959-12-31 realgdp 2785.204 12pivoted = ldata.pivot('date', 'item', 'value')pivoted.head() item infl realgdp unemp date 1959-03-31 0.00 2710.349 5.8 1959-06-30 2.34 2778.801 5.1 1959-09-30 2.74 2775.488 5.3 1959-12-31 0.27 2785.204 5.6 1960-03-31 2.31 2847.699 5.2 12ldata['value2'] = np.random.randn(len(ldata))ldata[:10] date item value value2 0 1959-03-31 realgdp 2710.349 -0.204708 1 1959-03-31 infl 0.000 0.478943 2 1959-03-31 unemp 5.800 -0.519439 3 1959-06-30 realgdp 2778.801 -0.555730 4 1959-06-30 infl 2.340 1.965781 5 1959-06-30 unemp 5.100 1.393406 6 1959-09-30 realgdp 2775.488 0.092908 7 1959-09-30 infl 2.740 0.281746 8 1959-09-30 unemp 5.300 0.769023 9 1959-12-31 realgdp 2785.204 1.246435 12pivoted = ldata.pivot('date', 'item')pivoted[:5] value value2 item infl realgdp unemp infl realgdp unemp date 1959-03-31 0.00 2710.349 5.8 0.478943 -0.204708 -0.519439 1959-06-30 2.34 2778.801 5.1 1.965781 -0.555730 1.393406 1959-09-30 2.74 2775.488 5.3 0.281746 0.092908 0.769023 1959-12-31 0.27 2785.204 5.6 1.007189 1.246435 -1.296221 1960-03-31 2.31 2847.699 5.2 0.228913 0.274992 1.352917 1pivoted['value'][:5] item infl realgdp unemp date 1959-03-31 0.00 2710.349 5.8 1959-06-30 2.34 2778.801 5.1 1959-09-30 2.74 2775.488 5.3 1959-12-31 0.27 2785.204 5.6 1960-03-31 2.31 2847.699 5.2 123ldata.set_index(['date', 'item'])[:9]unstacked = ldata.set_index(['date', 'item']).unstack('item')unstacked[:7] value value2 date item 1959-03-31 realgdp 2710.349 -0.204708 infl 0.000 0.478943 unemp 5.800 -0.519439 1959-06-30 realgdp 2778.801 -0.555730 infl 2.340 1.965781 unemp 5.100 1.393406 1959-09-30 realgdp 2775.488 0.092908 infl 2.740 0.281746 unemp 5.300 0.769023 value value2 item infl realgdp unemp infl realgdp unemp date 1959-03-31 0.00 2710.349 5.8 0.478943 -0.204708 -0.519439 1959-06-30 2.34 2778.801 5.1 1.965781 -0.555730 1.393406 1959-09-30 2.74 2775.488 5.3 0.281746 0.092908 0.769023 1959-12-31 0.27 2785.204 5.6 1.007189 1.246435 -1.296221 1960-03-31 2.31 2847.699 5.2 0.228913 0.274992 1.352917 1960-06-30 0.14 2834.390 5.2 -2.001637 0.886429 -0.371843 1960-09-30 2.70 2839.022 5.6 -0.438570 1.669025 -0.539741 数据转换移除重复值123data = DataFrame(&#123;'k1': ['one'] * 3 + ['two'] * 4, 'k2': [1, 1, 2, 3, 3, 4, 4]&#125;)data k1 k2 0 one 1 1 one 1 2 one 2 3 two 3 4 two 3 5 two 4 6 two 4 1data.duplicated() 0 False 1 True 2 False 3 False 4 True 5 False 6 True dtype: bool 1data.drop_duplicates() k1 k2 0 one 1 2 one 2 3 two 3 5 two 4 123data['v1'] = range(7)datadata.drop_duplicates(['k1']) k1 k2 v1 0 one 1 0 1 one 1 1 2 one 2 2 3 two 3 3 4 two 3 4 5 two 4 5 6 two 4 6 k1 k2 v1 0 one 1 0 3 two 3 3 12data.drop_duplicates(['k1', 'k2'], keep='last')data.drop_duplicates(['k1', 'k2'], keep='first') k1 k2 v1 1 one 1 1 2 one 2 2 4 two 3 4 6 two 4 6 k1 k2 v1 0 one 1 0 2 one 2 2 3 two 3 3 5 two 4 5 利用函数或映射进行数据转换12345data = DataFrame(&#123;'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', 'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'], 'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]&#125;)data food ounces 0 bacon 4.0 1 pulled pork 3.0 2 bacon 12.0 3 Pastrami 6.0 4 corned beef 7.5 5 Bacon 8.0 6 pastrami 3.0 7 honey ham 5.0 8 nova lox 6.0 12345678meat_to_animal = &#123; 'bacon': 'pig', 'pulled pork': 'pig', 'pastrami': 'cow', 'corned beef': 'cow', 'honey ham': 'pig', 'nova lox': 'salmon'&#125; 12data['animal'] = data['food'].map(str.lower).map(meat_to_animal)data food ounces animal 0 bacon 4.0 pig 1 pulled pork 3.0 pig 2 bacon 12.0 pig 3 Pastrami 6.0 cow 4 corned beef 7.5 cow 5 Bacon 8.0 pig 6 pastrami 3.0 cow 7 honey ham 5.0 pig 8 nova lox 6.0 salmon 1data['food'].map(lambda x: meat_to_animal[x.lower()]) 0 pig 1 pig 2 pig 3 cow 4 cow 5 pig 6 cow 7 pig 8 salmon Name: food, dtype: object 替换值12data = Series([1., -999., 2., -999., -1000., 3.])data 0 1.0 1 -999.0 2 2.0 3 -999.0 4 -1000.0 5 3.0 dtype: float64 1data.replace(-999, np.nan) 0 1.0 1 NaN 2 2.0 3 NaN 4 -1000.0 5 3.0 dtype: float64 1data.replace([-999, -1000], np.nan) 0 1.0 1 NaN 2 2.0 3 NaN 4 NaN 5 3.0 dtype: float64 1data.replace([-999, -1000], [np.nan, 0]) 0 1.0 1 NaN 2 2.0 3 NaN 4 0.0 5 3.0 dtype: float64 1data.replace(&#123;-999: np.nan, -1000: 0&#125;) 0 1.0 1 NaN 2 2.0 3 NaN 4 0.0 5 3.0 dtype: float64 重命名轴索引1234data = DataFrame(np.arange(12).reshape((3, 4)), index=['Ohio', 'Colorado', 'New York'], columns=['one', 'two', 'three', 'four'])data one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 New York 8 9 10 11 1data.index.map(str.upper) array([&#39;OHIO&#39;, &#39;COLORADO&#39;, &#39;NEW YORK&#39;], dtype=object) 12data.index = data.index.map(str.upper)data one two three four OHIO 0 1 2 3 COLORADO 4 5 6 7 NEW YORK 8 9 10 11 1data.rename(index=str.title, columns=str.upper) ONE TWO THREE FOUR Ohio 0 1 2 3 Colorado 4 5 6 7 New York 8 9 10 11 12data.rename(index=&#123;'OHIO': 'INDIANA'&#125;, columns=&#123;'three': 'peekaboo'&#125;) one two peekaboo four INDIANA 0 1 2 3 COLORADO 4 5 6 7 NEW YORK 8 9 10 11 123# Always returns a reference to a DataFrame_ = data.rename(index=&#123;'OHIO': 'INDIANA'&#125;, inplace=True)data one two three four INDIANA 0 1 2 3 COLORADO 4 5 6 7 NEW YORK 8 9 10 11 离散化和面元划分1ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32] 123bins = [18, 25, 35, 60, 100]cats = pd.cut(ages, bins)cats [(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]] Length: 12 Categories (4, object): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]] 1cats.codes array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8) 1cats.categories Index([&#39;(18, 25]&#39;, &#39;(25, 35]&#39;, &#39;(35, 60]&#39;, &#39;(60, 100]&#39;], dtype=&#39;object&#39;) 1pd.value_counts(cats) (18, 25] 5 (35, 60] 3 (25, 35] 3 (60, 100] 1 dtype: int64 1pd.cut(ages, [18, 26, 36, 61, 100], right=False) [[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)] Length: 12 Categories (4, object): [[18, 26) &lt; [26, 36) &lt; [36, 61) &lt; [61, 100)] 12group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']pd.cut(ages, bins, labels=group_names) [Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult] Length: 12 Categories (4, object): [Youth &lt; YoungAdult &lt; MiddleAged &lt; Senior] 12data = np.random.rand(20)pd.cut(data, 4, precision=2) [(0.25, 0.49], (0.25, 0.49], (0.73, 0.98], (0.25, 0.49], (0.25, 0.49], ..., (0.25, 0.49], (0.73, 0.98], (0.49, 0.73], (0.49, 0.73], (0.49, 0.73]] Length: 20 Categories (4, object): [(0.0032, 0.25] &lt; (0.25, 0.49] &lt; (0.49, 0.73] &lt; (0.73, 0.98]] 123data = np.random.randn(1000) # Normally distributedcats = pd.qcut(data, 4) # Cut into quartilescats [(0.636, 3.26], [-3.745, -0.648], (0.636, 3.26], (-0.022, 0.636], (-0.648, -0.022], ..., (0.636, 3.26], (-0.022, 0.636], [-3.745, -0.648], (-0.022, 0.636], (-0.022, 0.636]] Length: 1000 Categories (4, object): [[-3.745, -0.648] &lt; (-0.648, -0.022] &lt; (-0.022, 0.636] &lt; (0.636, 3.26]] 1pd.value_counts(cats) (0.636, 3.26] 250 (-0.022, 0.636] 250 (-0.648, -0.022] 250 [-3.745, -0.648] 250 dtype: int64 1pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.]) [(-0.022, 1.298], [-3.745, -1.274], (-0.022, 1.298], (-0.022, 1.298], (-1.274, -0.022], ..., (-0.022, 1.298], (-0.022, 1.298], [-3.745, -1.274], (-0.022, 1.298], (-0.022, 1.298]] Length: 1000 Categories (4, object): [[-3.745, -1.274] &lt; (-1.274, -0.022] &lt; (-0.022, 1.298] &lt; (1.298, 3.26]] 检测和过滤异常值123np.random.seed(12345)data = DataFrame(np.random.randn(1000, 4))data.describe() 0 1 2 3 count 1000.000000 1000.000000 1000.000000 1000.000000 mean -0.067684 0.067924 0.025598 -0.002298 std 0.998035 0.992106 1.006835 0.996794 min -3.428254 -3.548824 -3.184377 -3.745356 25% -0.774890 -0.591841 -0.641675 -0.644144 50% -0.116401 0.101143 0.002073 -0.013611 75% 0.616366 0.780282 0.680391 0.654328 max 3.366626 2.653656 3.260383 3.927528 12col = data[3]col[np.abs(col) &gt; 3] 97 3.927528 305 -3.399312 400 -3.745356 Name: 3, dtype: float64 1data[(np.abs(data) &gt; 3).any(1)] 0 1 2 3 5 -0.539741 0.476985 3.248944 -1.021228 97 -0.774363 0.552936 0.106061 3.927528 102 -0.655054 -0.565230 3.176873 0.959533 305 -2.315555 0.457246 -0.025907 -3.399312 324 0.050188 1.951312 3.260383 0.963301 400 0.146326 0.508391 -0.196713 -3.745356 499 -0.293333 -0.242459 -3.056990 1.918403 523 -3.428254 -0.296336 -0.439938 -0.867165 586 0.275144 1.179227 -3.184377 1.369891 808 -0.362528 -3.548824 1.553205 -2.186301 900 3.366626 -2.372214 0.851010 1.332846 12data[np.abs(data) &gt; 3] = np.sign(data) * 3data.describe() 0 1 2 3 count 1000.000000 1000.000000 1000.000000 1000.000000 mean -0.067623 0.068473 0.025153 -0.002081 std 0.995485 0.990253 1.003977 0.989736 min -3.000000 -3.000000 -3.000000 -3.000000 25% -0.774890 -0.591841 -0.641675 -0.644144 50% -0.116401 0.101143 0.002073 -0.013611 75% 0.616366 0.780282 0.680391 0.654328 max 3.000000 2.653656 3.000000 3.000000 排列和随机采样123df = DataFrame(np.arange(5 * 4).reshape((5, 4)))sampler = np.random.permutation(5)sampler array([1, 0, 2, 3, 4]) 1df 0 1 2 3 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 4 16 17 18 19 1df.take(sampler) 0 1 2 3 1 4 5 6 7 0 0 1 2 3 2 8 9 10 11 3 12 13 14 15 4 16 17 18 19 1df.take(np.random.permutation(len(df))[:3]) 0 1 2 3 1 4 5 6 7 0 0 1 2 3 4 16 17 18 19 12bag = np.array([5, 7, -1, 6, 4])sampler = np.random.randint(0, len(bag), size=10) 1sampler array([3, 0, 4, 1, 1, 2, 3, 0, 1, 2]) 12draws = bag.take(sampler)draws array([ 6, 5, 4, 7, 7, -1, 6, 5, 7, -1]) 计算指标 / 哑变量1234df = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)&#125;)dfpd.get_dummies(df['key']) data1 key 0 0 b 1 1 b 2 2 a 3 3 c 4 4 a 5 5 b a b c 0 0.0 1.0 0.0 1 0.0 1.0 0.0 2 1.0 0.0 0.0 3 0.0 0.0 1.0 4 1.0 0.0 0.0 5 0.0 1.0 0.0 1234dummies = pd.get_dummies(df['key'], prefix='key')dummiesdf_with_dummy = df[['data1']].join(dummies)df_with_dummy key_a key_b key_c 0 0.0 1.0 0.0 1 0.0 1.0 0.0 2 1.0 0.0 0.0 3 0.0 0.0 1.0 4 1.0 0.0 0.0 5 0.0 1.0 0.0 data1 key_a key_b key_c 0 0 0.0 1.0 0.0 1 1 0.0 1.0 0.0 2 2 1.0 0.0 0.0 3 3 0.0 0.0 1.0 4 4 1.0 0.0 0.0 5 5 0.0 1.0 0.0 1234mnames = ['movie_id', 'title', 'genres']movies = pd.read_table('ch02/movielens/movies.dat', sep='::', header=None, names=mnames)movies[:10] C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: ParserWarning: Falling back to the &#39;python&#39; engine because the &#39;c&#39; engine does not support regex separators (separators &gt; 1 char and different from &#39;\s+&#39; are interpreted as regex); you can avoid this warning by specifying engine=&#39;python&#39;. app.launch_new_instance() movie_id title genres 0 1 Toy Story (1995) Animation|Children's|Comedy 1 2 Jumanji (1995) Adventure|Children's|Fantasy 2 3 Grumpier Old Men (1995) Comedy|Romance 3 4 Waiting to Exhale (1995) Comedy|Drama 4 5 Father of the Bride Part II (1995) Comedy 5 6 Heat (1995) Action|Crime|Thriller 6 7 Sabrina (1995) Comedy|Romance 7 8 Tom and Huck (1995) Adventure|Children's 8 9 Sudden Death (1995) Action 9 10 GoldenEye (1995) Action|Adventure|Thriller 123genre_iter = (set(x.split('|')) for x in movies.genres)genres = sorted(set.union(*genre_iter))genres [&#39;Action&#39;, &#39;Adventure&#39;, &#39;Animation&#39;, &quot;Children&#39;s&quot;, &#39;Comedy&#39;, &#39;Crime&#39;, &#39;Documentary&#39;, &#39;Drama&#39;, &#39;Fantasy&#39;, &#39;Film-Noir&#39;, &#39;Horror&#39;, &#39;Musical&#39;, &#39;Mystery&#39;, &#39;Romance&#39;, &#39;Sci-Fi&#39;, &#39;Thriller&#39;, &#39;War&#39;, &#39;Western&#39;] 12dummies = DataFrame(np.zeros((len(movies), len(genres))), columns=genres)dummies[:10].ix[:, :5] Action Adventure Animation Children's Comedy 0 0.0 0.0 0.0 0.0 0.0 1 0.0 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0.0 3 0.0 0.0 0.0 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 5 0.0 0.0 0.0 0.0 0.0 6 0.0 0.0 0.0 0.0 0.0 7 0.0 0.0 0.0 0.0 0.0 8 0.0 0.0 0.0 0.0 0.0 9 0.0 0.0 0.0 0.0 0.0 123for i, gen in enumerate(movies.genres): dummies.ix[i, gen.split('|')] = 1dummies[:10].ix[:, :5] Action Adventure Animation Children's Comedy 0 0.0 0.0 1.0 1.0 1.0 1 0.0 1.0 0.0 1.0 0.0 2 0.0 0.0 0.0 0.0 1.0 3 0.0 0.0 0.0 0.0 1.0 4 0.0 0.0 0.0 0.0 1.0 5 1.0 0.0 0.0 0.0 0.0 6 0.0 0.0 0.0 0.0 1.0 7 0.0 1.0 0.0 1.0 0.0 8 1.0 0.0 0.0 0.0 0.0 9 1.0 1.0 0.0 0.0 0.0 12movies_windic = movies.join(dummies.add_prefix('Genre_'))movies_windic.ix[0] movie_id 1 title Toy Story (1995) genres Animation|Children&#39;s|Comedy Genre_Action 0 Genre_Adventure 0 Genre_Animation 1 Genre_Children&#39;s 1 Genre_Comedy 1 Genre_Crime 0 Genre_Documentary 0 Genre_Drama 0 Genre_Fantasy 0 Genre_Film-Noir 0 Genre_Horror 0 Genre_Musical 0 Genre_Mystery 0 Genre_Romance 0 Genre_Sci-Fi 0 Genre_Thriller 0 Genre_War 0 Genre_Western 0 Name: 0, dtype: object 1np.random.seed(12345) 12values = np.random.rand(10)values array([ 0.9296, 0.3164, 0.1839, 0.2046, 0.5677, 0.5955, 0.9645, 0.6532, 0.7489, 0.6536]) 12bins = [0, 0.2, 0.4, 0.6, 0.8, 1]pd.get_dummies(pd.cut(values, bins)) (0, 0.2] (0.2, 0.4] (0.4, 0.6] (0.6, 0.8] (0.8, 1] 0 0.0 0.0 0.0 0.0 1.0 1 0.0 1.0 0.0 0.0 0.0 2 1.0 0.0 0.0 0.0 0.0 3 0.0 1.0 0.0 0.0 0.0 4 0.0 0.0 1.0 0.0 0.0 5 0.0 0.0 1.0 0.0 0.0 6 0.0 0.0 0.0 0.0 1.0 7 0.0 0.0 0.0 1.0 0.0 8 0.0 0.0 0.0 1.0 0.0 9 0.0 0.0 0.0 1.0 0.0 字符串操作字符串对象方法12val = 'a,b, guido'val.split(',') [&#39;a&#39;, &#39;b&#39;, &#39; guido&#39;] 12pieces = [x.strip() for x in val.split(',')]pieces [&#39;a&#39;, &#39;b&#39;, &#39;guido&#39;] 12first, second, third = piecesfirst + '::' + second + '::' + third &#39;a::b::guido&#39; Surprise :P 1'::'.join(pieces) &#39;a::b::guido&#39; 1'guido' in val True 1val.index(',') 1 1val.find(':') -1 1val.index(':') --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-110-280f8b2856ce&gt; in &lt;module&gt;() ----&gt; 1 val.index(&#39;:&#39;) ValueError: substring not found 1val.count(',') 2 1val.replace(',', '::') &#39;a::b:: guido&#39; 1val.replace(',', '') &#39;ab guido&#39; 正则表达式123import retext = "foo bar\t baz \tqux"re.split('\s+', text) [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;, &#39;qux&#39;] 12regex = re.compile('\s+')regex.split(text) [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;, &#39;qux&#39;] 1regex.findall(text) [&#39; &#39;, &#39;\t &#39;, &#39; \t&#39;] 123456789text = """Dave dave@google.comSteve steve@gmail.comRob rob@gmail.comRyan ryan@yahoo.com"""pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]&#123;2,4&#125;'# re.IGNORECASE makes the regex case-insensitiveregex = re.compile(pattern, flags=re.IGNORECASE) 1regex.findall(text) [&#39;dave@google.com&#39;, &#39;steve@gmail.com&#39;, &#39;rob@gmail.com&#39;, &#39;ryan@yahoo.com&#39;] Search只返回第一项 12m = regex.search(text)m &lt;_sre.SRE_Match object; span=(5, 20), match=&#39;dave@google.com&#39;&gt; 1text[m.start():m.end()] &#39;dave@google.com&#39; 只匹配出现在字符串开头的模式 1print(regex.match(text)) None ​ 替换 1print(regex.sub('REDACTED', text)) Dave REDACTED Steve REDACTED Rob REDACTED Ryan REDACTED ​ 12pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]&#123;2,4&#125;)'regex = re.compile(pattern, flags=re.IGNORECASE) 12m = regex.match('wesm@bright.net')m.groups() (&#39;wesm&#39;, &#39;bright&#39;, &#39;net&#39;) 1regex.findall(text) [(&#39;dave&#39;, &#39;google&#39;, &#39;com&#39;), (&#39;steve&#39;, &#39;gmail&#39;, &#39;com&#39;), (&#39;rob&#39;, &#39;gmail&#39;, &#39;com&#39;), (&#39;ryan&#39;, &#39;yahoo&#39;, &#39;com&#39;)] 1print(regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text)) Dave Username: dave, Domain: google, Suffix: com Steve Username: steve, Domain: gmail, Suffix: com Rob Username: rob, Domain: gmail, Suffix: com Ryan Username: ryan, Domain: yahoo, Suffix: com ​ 123456regex = re.compile(r""" (?P&lt;username&gt;[A-Z0-9._%+-]+) @ (?P&lt;domain&gt;[A-Z0-9.-]+) \. (?P&lt;suffix&gt;[A-Z]&#123;2,4&#125;)""", flags=re.IGNORECASE|re.VERBOSE) 12m = regex.match('wesm@bright.net')m.groupdict() {&#39;domain&#39;: &#39;bright&#39;, &#39;suffix&#39;: &#39;net&#39;, &#39;username&#39;: &#39;wesm&#39;} pandas中矢量化的字符串函数123data = &#123;'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan&#125;data = Series(data) 1data Dave dave@google.com Rob rob@gmail.com Steve steve@gmail.com Wes NaN dtype: object 1data.isnull() Dave False Rob False Steve False Wes True dtype: bool 1data.str.contains('gmail') Dave False Rob True Steve True Wes NaN dtype: object 1pattern &#39;([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})&#39; 1data.str.findall(pattern, flags=re.IGNORECASE) Dave [(dave, google, com)] Rob [(rob, gmail, com)] Steve [(steve, gmail, com)] Wes NaN dtype: object 12matches = data.str.match(pattern, flags=re.IGNORECASE)matches C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: In future versions of pandas, match will change to always return a bool indexer. if __name__ == &#39;__main__&#39;: Dave (dave, google, com) Rob (rob, gmail, com) Steve (steve, gmail, com) Wes NaN dtype: object 1matches.str.get(1) Dave google Rob gmail Steve gmail Wes NaN dtype: object 1matches.str[0] Dave dave Rob rob Steve steve Wes NaN dtype: object 1data.str[:5] Dave dave@ Rob rob@g Steve steve Wes NaN dtype: object Example: USDA Food Database123456789101112131415161718192021222324252627&#123; "id": 21441, "description": "KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,Wing, meat and skin with breading", "tags": ["KFC"], "manufacturer": "Kentucky Fried Chicken", "group": "Fast Foods", "portions": [ &#123; "amount": 1, "unit": "wing, with skin", "grams": 68.0 &#125;, ... ], "nutrients": [ &#123; "value": 20.8, "units": "g", "description": "Protein", "group": "Composition" &#125;, ... ]&#125; 123import jsondb = json.load(open('ch07/foods-2011-10-03.json'))len(db) 6636 1db[0].keys() dict_keys([&#39;id&#39;, &#39;tags&#39;, &#39;portions&#39;, &#39;nutrients&#39;, &#39;description&#39;, &#39;group&#39;, &#39;manufacturer&#39;]) 1db[0]['nutrients'][0] {&#39;description&#39;: &#39;Protein&#39;, &#39;group&#39;: &#39;Composition&#39;, &#39;units&#39;: &#39;g&#39;, &#39;value&#39;: 25.18} 12nutrients = DataFrame(db[0]['nutrients'])nutrients[:7] description group units value 0 Protein Composition g 25.18 1 Total lipid (fat) Composition g 29.20 2 Carbohydrate, by difference Composition g 3.06 3 Ash Other g 3.28 4 Energy Energy kcal 376.00 5 Water Composition g 39.28 6 Energy Energy kJ 1573.00 12info_keys = ['description', 'group', 'id', 'manufacturer']info = DataFrame(db, columns=info_keys) 1info[:5] description group id manufacturer 0 Cheese, caraway Dairy and Egg Products 1008 1 Cheese, cheddar Dairy and Egg Products 1009 2 Cheese, edam Dairy and Egg Products 1018 3 Cheese, feta Dairy and Egg Products 1019 4 Cheese, mozzarella, part skim milk Dairy and Egg Products 1028 1pd.value_counts(info.group)[:10] Vegetables and Vegetable Products 812 Beef Products 618 Baked Products 496 Breakfast Cereals 403 Legumes and Legume Products 365 Fast Foods 365 Lamb, Veal, and Game Products 345 Sweets 341 Pork Products 328 Fruits and Fruit Juices 328 Name: group, dtype: int64 12345678nutrients = []for rec in db: fnuts = DataFrame(rec['nutrients']) fnuts['id'] = rec['id'] nutrients.append(fnuts)nutrients = pd.concat(nutrients, ignore_index=True) 1nutrients[:10] description group units value id 0 Protein Composition g 25.18 1008 1 Total lipid (fat) Composition g 29.20 1008 2 Carbohydrate, by difference Composition g 3.06 1008 3 Ash Other g 3.28 1008 4 Energy Energy kcal 376.00 1008 5 Water Composition g 39.28 1008 6 Energy Energy kJ 1573.00 1008 7 Fiber, total dietary Composition g 0.00 1008 8 Calcium, Ca Elements mg 673.00 1008 9 Iron, Fe Elements mg 0.64 1008 1nutrients.duplicated().sum() 14179 1nutrients = nutrients.drop_duplicates() 1234col_mapping = &#123;'description' : 'food', 'group' : 'fgroup'&#125;info = info.rename(columns=col_mapping, copy=False)info[:10] food fgroup id manufacturer 0 Cheese, caraway Dairy and Egg Products 1008 1 Cheese, cheddar Dairy and Egg Products 1009 2 Cheese, edam Dairy and Egg Products 1018 3 Cheese, feta Dairy and Egg Products 1019 4 Cheese, mozzarella, part skim milk Dairy and Egg Products 1028 5 Cheese, mozzarella, part skim milk, low moisture Dairy and Egg Products 1029 6 Cheese, romano Dairy and Egg Products 1038 7 Cheese, roquefort Dairy and Egg Products 1039 8 Cheese spread, pasteurized process, american, ... Dairy and Egg Products 1048 9 Cream, fluid, half and half Dairy and Egg Products 1049 1234col_mapping = &#123;'description' : 'nutrient', 'group' : 'nutgroup'&#125;nutrients = nutrients.rename(columns=col_mapping, copy=False)nutrients[:10] nutrient nutgroup units value id 0 Protein Composition g 25.18 1008 1 Total lipid (fat) Composition g 29.20 1008 2 Carbohydrate, by difference Composition g 3.06 1008 3 Ash Other g 3.28 1008 4 Energy Energy kcal 376.00 1008 5 Water Composition g 39.28 1008 6 Energy Energy kJ 1573.00 1008 7 Fiber, total dietary Composition g 0.00 1008 8 Calcium, Ca Elements mg 673.00 1008 9 Iron, Fe Elements mg 0.64 1008 1ndata = pd.merge(nutrients, info, on='id', how='outer') 1ndata[:10] nutrient nutgroup units value id food fgroup manufacturer 0 Protein Composition g 25.18 1008 Cheese, caraway Dairy and Egg Products 1 Total lipid (fat) Composition g 29.20 1008 Cheese, caraway Dairy and Egg Products 2 Carbohydrate, by difference Composition g 3.06 1008 Cheese, caraway Dairy and Egg Products 3 Ash Other g 3.28 1008 Cheese, caraway Dairy and Egg Products 4 Energy Energy kcal 376.00 1008 Cheese, caraway Dairy and Egg Products 5 Water Composition g 39.28 1008 Cheese, caraway Dairy and Egg Products 6 Energy Energy kJ 1573.00 1008 Cheese, caraway Dairy and Egg Products 7 Fiber, total dietary Composition g 0.00 1008 Cheese, caraway Dairy and Egg Products 8 Calcium, Ca Elements mg 673.00 1008 Cheese, caraway Dairy and Egg Products 9 Iron, Fe Elements mg 0.64 1008 Cheese, caraway Dairy and Egg Products 1ndata.ix[30000] nutrient Glycine nutgroup Amino Acids units g value 0.04 id 6158 food Soup, tomato bisque, canned, condensed fgroup Soups, Sauces, and Gravies manufacturer Name: 30000, dtype: object 123result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)result[:10]result['Zinc, Zn'].sort_values().plot(kind='barh') nutrient fgroup Adjusted Protein Sweets 12.900 Vegetables and Vegetable Products 2.180 Alanine Baby Foods 0.085 Baked Products 0.248 Beef Products 1.550 Beverages 0.003 Breakfast Cereals 0.311 Cereal Grains and Pasta 0.373 Dairy and Egg Products 0.271 Ethnic Foods 1.290 Name: value, dtype: float64 &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba08c9e780&gt; 123456789by_nutrient = ndata.groupby(['nutgroup', 'nutrient'])get_maximum = lambda x: x.xs(x.value.idxmax())get_minimum = lambda x: x.xs(x.value.idxmin())max_foods = by_nutrient.apply(get_maximum)[['value', 'food']]# make the food a little smallermax_foods.food = max_foods.food.str[:50] 1max_foods.ix['Amino Acids']['food'] nutrient Alanine Gelatins, dry powder, unsweetened Arginine Seeds, sesame flour, low-fat Aspartic acid Soy protein isolate Cystine Seeds, cottonseed flour, low fat (glandless) Glutamic acid Soy protein isolate Glycine Gelatins, dry powder, unsweetened Histidine Whale, beluga, meat, dried (Alaska Native) Hydroxyproline KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINA... Isoleucine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Leucine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Lysine Seal, bearded (Oogruk), meat, dried (Alaska Na... Methionine Fish, cod, Atlantic, dried and salted Phenylalanine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Proline Gelatins, dry powder, unsweetened Serine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Threonine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Tryptophan Sea lion, Steller, meat with fat (Alaska Native) Tyrosine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Valine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Name: food, dtype: object]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note Ch06]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-Ch06%2F</url>
      <content type="text"><![CDATA[Data loading, storage, and file formats12345678910111213from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport sysimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all" 1%pwd &#39;C:\\Users\\Ewan\\Downloads\\pydata-book-master&#39; Reading and Writing Data in Text Format1!more ch06\ex1.csv a,b,c,d,message 1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo 12df = pd.read_csv('ch06/ex1.csv')df a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo 1pd.read_table('ch06/ex1.csv', sep=',') a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo 1!more ch06\ex2.csv 1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo 12pd.read_csv('ch06/ex2.csv', header=None)pd.read_csv('ch06/ex2.csv', names=['a', 'b', 'c', 'd', 'message']) 0 1 2 3 4 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo 12names = ['a', 'b', 'c', 'd', 'message']pd.read_csv('ch06/ex2.csv', names=names, index_col='message') a b c d message hello 1 2 3 4 world 5 6 7 8 foo 9 10 11 12 123!more ch06\csv_mindex.csvparsed = pd.read_csv('ch06/csv_mindex.csv', index_col=['key1', 'key2'])parsed key1,key2,value1,value2 one,a,1,2 one,b,3,4 one,c,5,6 one,d,7,8 two,a,9,10 two,b,11,12 two,c,13,14 two,d,15,16 value1 value2 key1 key2 one a 1 2 b 3 4 c 5 6 d 7 8 two a 9 10 b 11 12 c 13 14 d 15 16 1list(open('ch06/ex3.txt')) [&#39; A B C\n&#39;, &#39;aaa -0.264438 -1.026059 -0.619500\n&#39;, &#39;bbb 0.927272 0.302904 -0.032399\n&#39;, &#39;ccc -0.264273 -0.386314 -0.217601\n&#39;, &#39;ddd -0.871858 -0.348382 1.100491\n&#39;] 采用正则表达式作为分隔符 12result = pd.read_table('ch06/ex3.txt', sep='\s+')result A B C aaa -0.264438 -1.026059 -0.619500 bbb 0.927272 0.302904 -0.032399 ccc -0.264273 -0.386314 -0.217601 ddd -0.871858 -0.348382 1.100491 12!more ch06\ex4.csvpd.read_csv('ch06/ex4.csv', skiprows=[0, 2, 3]) # hey! a,b,c,d,message # just wanted to make things more difficult for you # who reads CSV files with computers, anyway? 1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo 1234!more ch06\ex5.csvresult = pd.read_csv('ch06/ex5.csv')resultpd.isnull(result) something,a,b,c,d,message one,1,2,3,4,NA two,5,6,,8,world three,9,10,11,12,foo something a b c d message 0 one 1 2 3.0 4 NaN 1 two 5 6 NaN 8 world 2 three 9 10 11.0 12 foo something a b c d message 0 False False False False False True 1 False False False True False False 2 False False False False False False 12result = pd.read_csv('ch06/ex5.csv', na_values=['NULL'])result something a b c d message 0 one 1 2 3.0 4 NaN 1 two 5 6 NaN 8 world 2 three 9 10 11.0 12 foo 12sentinels = &#123;'message': ['foo', 'NA'], 'something': ['two']&#125;pd.read_csv('ch06/ex5.csv', na_values=sentinels) something a b c d message 0 one 1 2 3.0 4 NaN 1 NaN 5 6 NaN 8 world 2 three 9 10 11.0 12 NaN 逐块读取文本文件12result = pd.read_csv('ch06/ex6.csv')result one two three four key 0 0.467976 -0.038649 -0.295344 -1.824726 L 1 -0.358893 1.404453 0.704965 -0.200638 B 2 -0.501840 0.659254 -0.421691 -0.057688 G 3 0.204886 1.074134 1.388361 -0.982404 R 4 0.354628 -0.133116 0.283763 -0.837063 Q 5 1.817480 0.742273 0.419395 -2.251035 Q 6 -0.776764 0.935518 -0.332872 -1.875641 U 7 -0.913135 1.530624 -0.572657 0.477252 K 8 0.358480 -0.497572 -0.367016 0.507702 S 9 -1.740877 -1.160417 -1.637830 2.172201 G 10 0.240564 -0.328249 1.252155 1.072796 8 11 0.764018 1.165476 -0.639544 1.495258 R 12 0.571035 -0.310537 0.582437 -0.298765 1 13 2.317658 0.430710 -1.334216 0.199679 P 14 1.547771 -1.119753 -2.277634 0.329586 J 15 -1.310608 0.401719 -1.000987 1.156708 E 16 -0.088496 0.634712 0.153324 0.415335 B 17 -0.018663 -0.247487 -1.446522 0.750938 A 18 -0.070127 -1.579097 0.120892 0.671432 F 19 -0.194678 -0.492039 2.359605 0.319810 H 20 -0.248618 0.868707 -0.492226 -0.717959 W 21 -1.091549 -0.867110 -0.647760 -0.832562 C 22 0.641404 -0.138822 -0.621963 -0.284839 C 23 1.216408 0.992687 0.165162 -0.069619 V 24 -0.564474 0.792832 0.747053 0.571675 I 25 1.759879 -0.515666 -0.230481 1.362317 S 26 0.126266 0.309281 0.382820 -0.239199 L 27 1.334360 -0.100152 -0.840731 -0.643967 6 28 -0.737620 0.278087 -0.053235 -0.950972 J 29 -1.148486 -0.986292 -0.144963 0.124362 Y ... ... ... ... ... ... 9970 0.633495 -0.186524 0.927627 0.143164 4 9971 0.308636 -0.112857 0.762842 -1.072977 1 9972 -1.627051 -0.978151 0.154745 -1.229037 Z 9973 0.314847 0.097989 0.199608 0.955193 P 9974 1.666907 0.992005 0.496128 -0.686391 S 9975 0.010603 0.708540 -1.258711 0.226541 K 9976 0.118693 -0.714455 -0.501342 -0.254764 K 9977 0.302616 -2.011527 -0.628085 0.768827 H 9978 -0.098572 1.769086 -0.215027 -0.053076 A 9979 -0.019058 1.964994 0.738538 -0.883776 F 9980 -0.595349 0.001781 -1.423355 -1.458477 M 9981 1.392170 -1.396560 -1.425306 -0.847535 H 9982 -0.896029 -0.152287 1.924483 0.365184 6 9983 -2.274642 -0.901874 1.500352 0.996541 N 9984 -0.301898 1.019906 1.102160 2.624526 I 9985 -2.548389 -0.585374 1.496201 -0.718815 D 9986 -0.064588 0.759292 -1.568415 -0.420933 E 9987 -0.143365 -1.111760 -1.815581 0.435274 2 9988 -0.070412 -1.055921 0.338017 -0.440763 X 9989 0.649148 0.994273 -1.384227 0.485120 Q 9990 -0.370769 0.404356 -1.051628 -1.050899 8 9991 -0.409980 0.155627 -0.818990 1.277350 W 9992 0.301214 -1.111203 0.668258 0.671922 A 9993 1.821117 0.416445 0.173874 0.505118 X 9994 0.068804 1.322759 0.802346 0.223618 H 9995 2.311896 -0.417070 -1.409599 -0.515821 L 9996 -0.479893 -0.650419 0.745152 -0.646038 E 9997 0.523331 0.787112 0.486066 1.093156 K 9998 -0.362559 0.598894 -1.843201 0.887292 G 9999 -0.096376 -1.012999 -0.657431 -0.573315 0 10000 rows × 5 columns 1pd.read_csv('ch06/ex6.csv', nrows=5) one two three four key 0 0.467976 -0.038649 -0.295344 -1.824726 L 1 -0.358893 1.404453 0.704965 -0.200638 B 2 -0.501840 0.659254 -0.421691 -0.057688 G 3 0.204886 1.074134 1.388361 -0.982404 R 4 0.354628 -0.133116 0.283763 -0.837063 Q 12chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)chunker &lt;pandas.io.parsers.TextFileReader at 0x2035229de80&gt; 1234567chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)tot = Series([])for piece in chunker: tot = tot.add(piece['key'].value_counts(), fill_value=0)tot = tot.sort_values(ascending=False) 1tot[:10] E 368.0 X 364.0 L 346.0 O 343.0 Q 340.0 M 338.0 J 337.0 F 335.0 K 334.0 H 330.0 dtype: float64 将数据写出到文本格式12data = pd.read_csv('ch06/ex5.csv')data something a b c d message 0 one 1 2 3.0 4 NaN 1 two 5 6 NaN 8 world 2 three 9 10 11.0 12 foo 12data.to_csv('ch06/out.csv')!more ch06\out.csv ,something,a,b,c,d,message 0,one,1,2,3.0,4, 1,two,5,6,,8,world 2,three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, sep='|') |something|a|b|c|d|message 0|one|1|2|3.0|4| 1|two|5|6||8|world 2|three|9|10|11.0|12|foo 1data.to_csv(sys.stdout, na_rep='NULL') ,something,a,b,c,d,message 0,one,1,2,3.0,4,NULL 1,two,5,6,NULL,8,world 2,three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, index=False, header=False) one,1,2,3.0,4, two,5,6,,8,world three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c']) a,b,c 1,2,3.0 5,6, 9,10,11.0 123456dates = pd.date_range('1/1/2000', periods=7)datests = Series(np.arange(7), index=dates)tsts.to_csv('ch06/tseries.csv')!more ch06\tseries.csv DatetimeIndex([&#39;2000-01-01&#39;, &#39;2000-01-02&#39;, &#39;2000-01-03&#39;, &#39;2000-01-04&#39;, &#39;2000-01-05&#39;, &#39;2000-01-06&#39;, &#39;2000-01-07&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) 2000-01-01 0 2000-01-02 1 2000-01-03 2 2000-01-04 3 2000-01-05 4 2000-01-06 5 2000-01-07 6 Freq: D, dtype: int32 2000-01-01,0 2000-01-02,1 2000-01-03,2 2000-01-04,3 2000-01-05,4 2000-01-06,5 2000-01-07,6 1Series.from_csv('ch06/tseries.csv', parse_dates=True) 2000-01-01 0 2000-01-02 1 2000-01-03 2 2000-01-04 3 2000-01-05 4 2000-01-06 5 2000-01-07 6 dtype: int64 手动处理分隔符格式1!more ch06\ex7.csv &quot;a&quot;,&quot;b&quot;,&quot;c&quot; &quot;1&quot;,&quot;2&quot;,&quot;3&quot; &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot; 1234import csvf = open('ch06/ex7.csv')reader = csv.reader(f) 12for line in reader: print(line) [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;] 12345678910lines = list(csv.reader(open('ch06/ex7.csv')))header, values = lines[0], lines[1:]for item in zip(*values): print(item) for h, v in zip(header, zip(*values)): print(h, v)data_dict = &#123;h: v for h, v in zip(header, zip(*values))&#125;data_dict (&#39;1&#39;, &#39;1&#39;) (&#39;2&#39;, &#39;2&#39;) (&#39;3&#39;, &#39;3&#39;) a (&#39;1&#39;, &#39;1&#39;) b (&#39;2&#39;, &#39;2&#39;) c (&#39;3&#39;, &#39;3&#39;) {&#39;a&#39;: (&#39;1&#39;, &#39;1&#39;), &#39;b&#39;: (&#39;2&#39;, &#39;2&#39;), &#39;c&#39;: (&#39;3&#39;, &#39;3&#39;)} 12345class my_dialect(csv.Dialect): lineterminator = '\n' delimiter = ';' quotechar = '"' quoting = csv.QUOTE_MINIMAL 123456with open('mydata.csv', 'w') as f: writer = csv.writer(f, dialect=my_dialect) writer.writerow(('one', 'two', 'three')) writer.writerow(('1', '2', '3')) writer.writerow(('4', '5', '6')) writer.writerow(('7', '8', '9')) 14 6 6 6 1!more mydata.csv one;two;three 1;2;3 4;5;6 7;8;9 JSON数据12345678obj = """&#123;"name": "Wes", "places_lived": ["United States", "Spain", "Germany"], "pet": null, "siblings": [&#123;"name": "Scott", "age": 25, "pet": "Zuko"&#125;, &#123;"name": "Katie", "age": 33, "pet": "Cisco"&#125;]&#125;""" 123import jsonresult = json.loads(obj)result {&#39;name&#39;: &#39;Wes&#39;, &#39;pet&#39;: None, &#39;places_lived&#39;: [&#39;United States&#39;, &#39;Spain&#39;, &#39;Germany&#39;], &#39;siblings&#39;: [{&#39;age&#39;: 25, &#39;name&#39;: &#39;Scott&#39;, &#39;pet&#39;: &#39;Zuko&#39;}, {&#39;age&#39;: 33, &#39;name&#39;: &#39;Katie&#39;, &#39;pet&#39;: &#39;Cisco&#39;}]} 1asjson = json.dumps(result) # convert to json 12siblings = DataFrame(result['siblings'], columns=['name', 'age'])siblings name age 0 Scott 25 1 Katie 33 XML和HTML： Web信息收集NB. The Yahoo! Finance API has changed and this example no longer works 123456from lxml.html import parsefrom urllib.request import urlopenparsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options'))doc = parsed.getroot() 12links = doc.findall('.//a')links[15:20] [&lt;Element a at 0x20352cad598&gt;, &lt;Element a at 0x20352cad5e8&gt;, &lt;Element a at 0x20352cad638&gt;, &lt;Element a at 0x20352cad688&gt;, &lt;Element a at 0x20352cad6d8&gt;] 1234lnk = links[28]lnklnk.get('href')lnk.text_content() &lt;Element a at 0x20352cad9a8&gt; &#39;/quote/NFLX?p=NFLX&#39; &#39;NFLX&#39; 12urls = [lnk.get('href') for lnk in doc.findall('.//a')]urls[-10:] [&#39;//finance.yahoo.com/broker-comparison?bypass=true&#39;, &#39;https://help.yahoo.com/kb/index?page=content&amp;y=PROD_MAIL_ML&amp;locale=en_US&amp;id=SLN2310&amp;actp=productlink&#39;, &#39;http://help.yahoo.com/l/us/yahoo/finance/&#39;, &#39;https://yahoo.uservoice.com/forums/382977&#39;, &#39;http://info.yahoo.com/privacy/us/yahoo/&#39;, &#39;http://info.yahoo.com/relevantads/&#39;, &#39;http://info.yahoo.com/legal/us/yahoo/utos/utos-173.html&#39;, &#39;http://twitter.com/YahooFinance&#39;, &#39;http://facebook.com/yahoofinance&#39;, &#39;http://yahoofinance.tumblr.com&#39;] 123tables = doc.findall('.//table')len(tables)calls = tables[0] 1 1rows = calls.findall('.//tr') 123def _unpack(row, kind='td'): elts = row.findall('.//%s' % kind) return [val.text_content() for val in elts] 12_unpack(rows[0], kind='th')_unpack(rows[1], kind='td') [] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-87-7d371ed47023&gt; in &lt;module&gt;() 1 _unpack(rows[0], kind=&#39;th&#39;) ----&gt; 2 _unpack(rows[1], kind=&#39;td&#39;) IndexError: list index out of range 1234567from pandas.io.parsers import TextParserdef parse_options_data(table): rows = table.findall('.//tr') header = _unpack(rows[0], kind='th') data = [_unpack(r) for r in rows[1:]] return TextParser(data, names=header).get_chunk() 123call_data = parse_options_data(calls)put_data = parse_options_data(puts)call_data[:10] Parsing XML with lxml.objectify12345from lxml import objectifypath = '.\ch06\mta_perf\Performance_MNR.xml'parsed = objectify.parse(open(path))root = parsed.getroot() 12345678910111213data = []skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ', 'DESIRED_CHANGE', 'DECIMAL_PLACES']for elt in root.INDICATOR: el_data = &#123;&#125; for child in elt.getchildren(): if child.tag in skip_fields: continue el_data[child.tag] = child.pyval data.append(el_data) 12perf = DataFrame(data)perf.ix[:10, 5:] INDICATOR_UNIT MONTHLY_ACTUAL MONTHLY_TARGET PERIOD_MONTH PERIOD_YEAR YTD_ACTUAL YTD_TARGET 0 % 96.9 95 1 2008 96.9 95 1 % 95 95 2 2008 96 95 2 % 96.9 95 3 2008 96.3 95 3 % 98.3 95 4 2008 96.8 95 4 % 95.8 95 5 2008 96.6 95 5 % 94.4 95 6 2008 96.2 95 6 % 96 95 7 2008 96.2 95 7 % 96.4 95 8 2008 96.2 95 8 % 93.7 95 9 2008 95.9 95 9 % 96.4 95 10 2008 96 95 10 % 96.9 95 11 2008 96.1 95 二进制数据格式123frame = pd.read_csv('ch06/ex1.csv')frameframe.to_pickle('ch06/frame_pickle') a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo 1pd.read_pickle('ch06/frame_pickle') a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo 使用HDF5格式1234store = pd.HDFStore('mydata.h5')store['obj1'] = framestore['obj1_col'] = frame['a']store &lt;class &#39;pandas.io.pytables.HDFStore&#39;&gt; File path: mydata.h5 /obj1 frame (shape-&gt;[3,5]) /obj1_col series (shape-&gt;[3]) 1store['obj1'] a b c d message 0 1 2 3 4 hello 1 5 6 7 8 world 2 9 10 11 12 foo 12store.close()os.remove('mydata.h5') 使用数据库1234567891011import sqlite3query = """CREATE TABLE test(a VARCHAR(20), b VARCHAR(20), c REAL, d INTEGER);"""con = sqlite3.connect(':memory:')con.execute(query)con.commit() &lt;sqlite3.Cursor at 0x2035487c880&gt; 1234567data = [('Atlanta', 'Georgia', 1.25, 6), ('Tallahassee', 'Florida', 2.6, 3), ('Sacramento', 'California', 1.7, 5)]stmt = "INSERT INTO test VALUES(?, ?, ?, ?)"con.executemany(stmt, data)con.commit() &lt;sqlite3.Cursor at 0x2035487c810&gt; 123cursor = con.execute('select * from test')rows = cursor.fetchall()rows [(&#39;Atlanta&#39;, &#39;Georgia&#39;, 1.25, 6), (&#39;Tallahassee&#39;, &#39;Florida&#39;, 2.6, 3), (&#39;Sacramento&#39;, &#39;California&#39;, 1.7, 5)] 12import pandas.io.sql as sqlsql.read_sql('select * from test', con) a b c d 0 Atlanta Georgia 1.25 6 1 Tallahassee Florida 2.60 3 2 Sacramento California 1.70 5]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note ch05]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-ch05%2F</url>
      <content type="text"><![CDATA[pandas入门按照以下约定引用相关package 12from pandas import Series, DataFrameimport pandas as pd 123456789101112from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all" pandas数据结构介绍SeriesSeries是一种类似于一维数组的对象，由一组数据以及一组与之相关的数据标签（类似于字典的键）组成，所以可以看成是一个有序的字典 12obj = Series([4, 7, -5, 3])obj 0 4 1 7 2 -5 3 3 dtype: int64 12obj.valuesobj.index array([ 4, 7, -5, 3], dtype=int64) RangeIndex(start=0, stop=4, step=1) 12obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])obj2 d 4 b 7 a -5 c 3 dtype: int64 1obj2.index Index([&#39;d&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;], dtype=&#39;object&#39;) 1obj2['a'] -5 12obj2['d'] = 6obj2[['c', 'a', 'd']] c 3 a -5 d 6 dtype: int64 各种Numpy运算都是作用在数据上，同时索引与数据的链接会一直保持 1obj2[obj2 &gt; 0] d 6 b 7 c 3 dtype: int64 1obj2 * 2 d 12 b 14 a -10 c 6 dtype: int64 1np.exp(obj2) d 403.428793 b 1096.633158 a 0.006738 c 20.085537 dtype: float64 1'b' in obj2 True 1'e' in obj2 False 因此可以直接根据Numpy的Dict来创建Series 123sdata = &#123;'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000&#125;obj3 = Series(sdata)obj3 Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 如果传入了index参数的话，那么就会与传入的Dict做键匹配，没有匹配上的就设为NaN 123states = ['California', 'Ohio', 'Oregon', 'Texas']obj4 = Series(sdata, index=states)obj4 California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64 1pd.isnull(obj4) California True Ohio False Oregon False Texas False dtype: bool 1pd.notnull(obj4) California False Ohio True Oregon True Texas True dtype: bool 1obj4.isnull() California True Ohio False Oregon False Texas False dtype: bool Series有一个非常重要的数据对齐的功能 1obj3 Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 1obj4 California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64 1obj3 + obj4 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 Series本身以及其索引都有一个叫做name的属性，这个属性十分重要，以后很多高级功能都会用到 123obj4.name = 'population'obj4.index.name = 'state'obj4 state California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 Name: population, dtype: float64 可以通过直接赋值的方式修改index属性 12obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']obj Bob 4 Steve 7 Jeff -5 Ryan 3 dtype: int64 DataFrameDataFrame是一个表格型的数据结构，可以看成由Series组成的字典，只不过这些Series共用一套索引 1234data = &#123;'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2002, 2001, 2002], 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]&#125;frame = DataFrame(data) 数据会被排序 1frame pop state year 0 1.5 Ohio 2000 1 1.7 Ohio 2001 2 3.6 Ohio 2002 3 2.4 Nevada 2001 4 2.9 Nevada 2002 1DataFrame(data, columns=['year', 'state', 'pop']) year state pop 0 2000 Ohio 1.5 1 2001 Ohio 1.7 2 2002 Ohio 3.6 3 2001 Nevada 2.4 4 2002 Nevada 2.9 123frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'], index=['one', 'two', 'three', 'four', 'five'])frame2 year state pop debt one 2000 Ohio 1.5 NaN two 2001 Ohio 1.7 NaN three 2002 Ohio 3.6 NaN four 2001 Nevada 2.4 NaN five 2002 Nevada 2.9 NaN 1frame2.columns Index([&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;, &#39;debt&#39;], dtype=&#39;object&#39;) DataFrame每一个Key对应的Value都是一个Series 1frame2['state'] one Ohio two Ohio three Ohio four Nevada five Nevada Name: state, dtype: object 1frame2.year one 2000 two 2001 three 2002 four 2001 five 2002 Name: year, dtype: int64 注意到name属性也已经被设置好了 ix相当于一个行索引？ 1frame2.ix['three'] year 2002 state Ohio pop 3.6 debt NaN Name: three, dtype: object 可以利用Numpy的广播功能 12frame2['debt'] = 16.5frame2 year state pop debt one 2000 Ohio 1.5 16.5 two 2001 Ohio 1.7 16.5 three 2002 Ohio 3.6 16.5 four 2001 Nevada 2.4 16.5 five 2002 Nevada 2.9 16.5 也可以赋值一个列表，但是长度必须匹配 12frame2['debt'] = np.arange(5.)frame2 year state pop debt one 2000 Ohio 1.5 0.0 two 2001 Ohio 1.7 1.0 three 2002 Ohio 3.6 2.0 four 2001 Nevada 2.4 3.0 five 2002 Nevada 2.9 4.0 如果是赋值一个Series，则会匹配上索引，没有匹配上的就是置为NaN 123val = Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])frame2['debt'] = valframe2 year state pop debt one 2000 Ohio 1.5 NaN two 2001 Ohio 1.7 -1.2 three 2002 Ohio 3.6 NaN four 2001 Nevada 2.4 -1.5 five 2002 Nevada 2.9 -1.7 也可以进行逻辑操作 12frame2['eastern'] = frame2.state == 'Ohio'frame2 year state pop debt eastern one 2000 Ohio 1.5 NaN True two 2001 Ohio 1.7 -1.2 True three 2002 Ohio 3.6 NaN True four 2001 Nevada 2.4 -1.5 False five 2002 Nevada 2.9 -1.7 False del用于删除一列 12del frame2['eastern']frame2.columns Index([&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;, &#39;debt&#39;], dtype=&#39;object&#39;) 只要是通过索引方式进行的操作，都是直接在原数据上进行的操作，不是一个副本 嵌套的字典也可直接生成DataFrame，只不过内层的键被当作index，外层的键被当作colums 12pop = &#123;'Nevada': &#123;2001: 2.4, 2002: 2.9&#125;, 'Ohio': &#123;2000: 1.5, 2001: 1.7, 2002: 3.6&#125;&#125; 12frame3 = DataFrame(pop)frame3 Nevada Ohio 2000 NaN 1.5 2001 2.4 1.7 2002 2.9 3.6 同样可以进行转置，这样的话index和column就会互换 1frame3.T 2000 2001 2002 Nevada NaN 2.4 2.9 Ohio 1.5 1.7 3.6 显式地指定索引，不匹配的会置为NaN 1DataFrame(pop, index=[2001, 2002, 2003]) Nevada Ohio 2001 2.4 1.7 2002 2.9 3.6 2003 NaN NaN 1frame3 Nevada Ohio 2000 NaN 1.5 2001 2.4 1.7 2002 2.9 3.6 还可以这样构建 123pdata = &#123;'Ohio': frame3['Ohio'][:-1], 'Nevada': frame3['Nevada'][:2]&#125;DataFrame(pdata) Nevada Ohio 2000 NaN 1.5 2001 2.4 1.7 1frame3 Nevada Ohio 2000 NaN 1.5 2001 2.4 1.7 2002 2.9 3.6 name属性也会在表格中显示出来 12frame3.index.name = 'year'; frame3.columns.name = 'state'frame3 state Nevada Ohio year 2000 NaN 1.5 2001 2.4 1.7 2002 2.9 3.6 values方法只返回数据，不返回index以及key 1frame3.values array([[ nan, 1.5], [ 2.4, 1.7], [ 2.9, 3.6]]) 1frame2 year state pop debt one 2000 Ohio 1.5 NaN two 2001 Ohio 1.7 -1.2 three 2002 Ohio 3.6 NaN four 2001 Nevada 2.4 -1.5 five 2002 Nevada 2.9 -1.7 1frame2.values array([[2000, &#39;Ohio&#39;, 1.5, nan], [2001, &#39;Ohio&#39;, 1.7, -1.2], [2002, &#39;Ohio&#39;, 3.6, nan], [2001, &#39;Nevada&#39;, 2.4, -1.5], [2002, &#39;Nevada&#39;, 2.9, -1.7]], dtype=object) 索引对象Index是一个可以单独提取出来的对象 123obj = Series(range(3), index=['a', 'b', 'c'])index = obj.indexindex Index([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], dtype=&#39;object&#39;) 1index[1:] Index([&#39;b&#39;, &#39;c&#39;], dtype=&#39;object&#39;) 不可修改~！ 1index[1] = 'd' --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-52-676fdeb26a68&gt; in &lt;module&gt;() ----&gt; 1 index[1] = &#39;d&#39; C:\Users\Ewan\Anaconda3\lib\site-packages\pandas\indexes\base.py in __setitem__(self, key, value) 1243 1244 def __setitem__(self, key, value): -&gt; 1245 raise TypeError(&quot;Index does not support mutable operations&quot;) 1246 1247 def __getitem__(self, key): TypeError: Index does not support mutable operations 直接创建Index对象 123index = pd.Index(np.arange(3))obj2 = Series([1.5, -2.5, 0], index=index)obj2.index is index True 1frame3 state Nevada Ohio year 2000 NaN 1.5 2001 2.4 1.7 2002 2.9 3.6 1'Ohio' in frame3.columns True 12003 in frame3.index False 基本功能重新索引12obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])obj d 4.5 b 7.2 a -5.3 c 3.6 dtype: float64 重排索引形成新对象 12obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])obj2 a -5.3 b 7.2 c 3.6 d 4.5 e NaN dtype: float64 1obj.reindex(['a', 'b', 'c', 'd', 'e'], fill_value=0) a -5.3 b 7.2 c 3.6 d 4.5 e 0.0 dtype: float64 12obj3 = Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])obj3.reindex(range(6), method='ffill') # 前向填充 0 blue 1 blue 2 purple 3 purple 4 yellow 5 yellow dtype: object 123frame = DataFrame(np.arange(9).reshape((3, 3)), index=['a', 'c', 'd'], columns=['Ohio', 'Texas', 'California'])frame Ohio Texas California a 0 1 2 c 3 4 5 d 6 7 8 12frame2 = frame.reindex(['a', 'b', 'c', 'd'])frame2 Ohio Texas California a 0.0 1.0 2.0 b NaN NaN NaN c 3.0 4.0 5.0 d 6.0 7.0 8.0 12states = ['Texas', 'Utah', 'California']frame.reindex(columns=states) Texas Utah California a 1 NaN 2 c 4 NaN 5 d 7 NaN 8 插值只能按行 12frame.reindex(index=['a', 'b', 'c', 'd'], method='ffill', columns=states) Texas Utah California a 1 NaN 2 b 1 NaN 2 c 4 NaN 5 d 7 NaN 8 用ix方法进行重新索引操作会使得代码很简洁 1frame.ix[['a', 'b', 'c', 'd'], states] Texas Utah California a 1.0 NaN 2.0 b NaN NaN NaN c 4.0 NaN 5.0 d 7.0 NaN 8.0 丢弃指定轴上的项1frame.ix[['a', 'b', 'c', 'd'], states] Texas Utah California a 1.0 NaN 2.0 b NaN NaN NaN c 4.0 NaN 5.0 d 7.0 NaN 8.0 123obj = Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])new_obj = obj.drop('c')new_obj a 0.0 b 1.0 d 3.0 e 4.0 dtype: float64 1obj.drop(['d', 'c']) a 0.0 b 1.0 e 4.0 dtype: float64 1234data = DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])data one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 1data.drop(['Colorado', 'Ohio']) one two three four Utah 8 9 10 11 New York 12 13 14 15 1data.drop('two', axis=1) one three four Ohio 0 2 3 Colorado 4 6 7 Utah 8 10 11 New York 12 14 15 1data.drop(['two', 'four'], axis=1) one three Ohio 0 2 Colorado 4 6 Utah 8 10 New York 12 14 索引，选取和过滤多种索引方式 123obj = Series(np.arange(4.), index=['a', 'b', 'c', 'd'])objobj['b'] a 0.0 b 1.0 c 2.0 d 3.0 dtype: float64 1.0 1obj[1] 1.0 1obj[2:4] c 2.0 d 3.0 dtype: float64 1obj[['b', 'a', 'd']] b 1.0 a 0.0 d 3.0 dtype: float64 1obj[[1, 3]] b 1.0 d 3.0 dtype: float64 1obj[obj &lt; 2] # 直接对data进行操作 a 0.0 b 1.0 dtype: float64 这种切片方式…末端包含 1obj['b':'c'] b 1.0 c 2.0 dtype: float64 12obj['b':'c'] = 5obj a 0.0 b 5.0 c 5.0 d 3.0 dtype: float64 1234data = DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])data one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 1data['two'] Ohio 1 Colorado 5 Utah 9 New York 13 Name: two, dtype: int32 1data[['three', 'one']] three one Ohio 2 0 Colorado 6 4 Utah 10 8 New York 14 12 1data[:2] # axis=0 one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 1data[data['three'] &gt; 5] one two three four Colorado 4 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 1data &lt; 5 one two three four Ohio True True True True Colorado True False False False Utah False False False False New York False False False False 1data[data &lt; 5] = 0 1data one two three four Ohio 0 0 0 0 Colorado 0 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 索引的另外一种选择 1data.ix['Colorado', ['two', 'three']] two 5 three 6 Name: Colorado, dtype: int32 1data.ix[['Colorado', 'Utah'], [3, 0, 1]] four one two Colorado 7 0 5 Utah 11 8 9 1data.ix[2] one 8 two 9 three 10 four 11 Name: Utah, dtype: int32 1data.ix[:'Utah', 'two'] Ohio 0 Colorado 5 Utah 9 Name: two, dtype: int32 1data.ix[data.three &gt; 5, :3] one two three Colorado 0 5 6 Utah 8 9 10 New York 12 13 14 总结一下就是说，DataFrame是一个二维的数组，只不过每一维的索引方式除了序号之外，还可以用name属性来进行索引，且一切行为与序号无异 算术运算和数据对齐12s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g']) 1s1 a 7.3 c -2.5 d 3.4 e 1.5 dtype: float64 1s2 a -2.1 c 3.6 e -1.5 f 4.0 g 3.1 dtype: float64 数据对齐操作就是一种特殊的并集操作 1s1 + s2 a 5.2 c 1.1 d NaN e 0.0 f NaN g NaN dtype: float64 12345df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'), index=['Ohio', 'Texas', 'Colorado'])df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])df1 b c d Ohio 0.0 1.0 2.0 Texas 3.0 4.0 5.0 Colorado 6.0 7.0 8.0 1df2 b d e Utah 0.0 1.0 2.0 Ohio 3.0 4.0 5.0 Texas 6.0 7.0 8.0 Oregon 9.0 10.0 11.0 并且数据对齐操作是在所有维度上同时进行的 1df1 + df2 b c d e Colorado NaN NaN NaN NaN Ohio 3.0 NaN 6.0 NaN Oregon NaN NaN NaN NaN Texas 9.0 NaN 12.0 NaN Utah NaN NaN NaN NaN 在算术方法中填充词下面这种定义column的方式值得注意 123df1 = DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))df2 = DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))df1 a b c d 0 0.0 1.0 2.0 3.0 1 4.0 5.0 6.0 7.0 2 8.0 9.0 10.0 11.0 1df2 a b c d e 0 0.0 1.0 2.0 3.0 4.0 1 5.0 6.0 7.0 8.0 9.0 2 10.0 11.0 12.0 13.0 14.0 3 15.0 16.0 17.0 18.0 19.0 1df1 + df2 a b c d e 0 0.0 2.0 4.0 6.0 NaN 1 9.0 11.0 13.0 15.0 NaN 2 18.0 20.0 22.0 24.0 NaN 3 NaN NaN NaN NaN NaN 要想填充值必须使用add方法 1df1.add(df2, fill_value=0) a b c d e 0 0.0 2.0 4.0 6.0 4.0 1 9.0 11.0 13.0 15.0 9.0 2 18.0 20.0 22.0 24.0 14.0 3 15.0 16.0 17.0 18.0 19.0 reindex方法与add方法还是存在差异的 1df1.reindex(columns=df2.columns, fill_value=0) a b c d e 0 0.0 1.0 2.0 3.0 0 1 4.0 5.0 6.0 7.0 0 2 8.0 9.0 10.0 11.0 0 DataFrame和Series之间的运算12arr = np.arange(12.).reshape((3, 4))arr array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) 1arr[0] array([ 0., 1., 2., 3.]) 广播操作 1arr - arr[0] array([[ 0., 0., 0., 0.], [ 4., 4., 4., 4.], [ 8., 8., 8., 8.]]) 1234frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])series = frame.ix[0]frame b d e Utah 0.0 1.0 2.0 Ohio 3.0 4.0 5.0 Texas 6.0 7.0 8.0 Oregon 9.0 10.0 11.0 Series的name等于DataFrame的切片属性 1series b 0.0 d 1.0 e 2.0 Name: Utah, dtype: float64 默认情况下，DataFrame和Series之间的算术运算会将Series的index匹配到DataFrame的column, 然后沿着行向下广播 1frame - series b d e Utah 0.0 0.0 0.0 Ohio 3.0 3.0 3.0 Texas 6.0 6.0 6.0 Oregon 9.0 9.0 9.0 如果Series的index与DataFrame的column不匹配，则进行数据对齐 12series2 = Series(range(3), index=['b', 'e', 'f'])frame + series2 b d e f Utah 0.0 NaN 3.0 NaN Ohio 3.0 NaN 6.0 NaN Texas 6.0 NaN 9.0 NaN Oregon 9.0 NaN 12.0 NaN 12series3 = frame['d']frame b d e Utah 0.0 1.0 2.0 Ohio 3.0 4.0 5.0 Texas 6.0 7.0 8.0 Oregon 9.0 10.0 11.0 1series3 Utah 1.0 Ohio 4.0 Texas 7.0 Oregon 10.0 Name: d, dtype: float64 匹配行并且在列上进行广播， 就必须要指定axis 12frame.sub(series3, axis=0)frame.sub(series3, axis=1) b d e Utah -1.0 0.0 1.0 Ohio -1.0 0.0 1.0 Texas -1.0 0.0 1.0 Oregon -1.0 0.0 1.0 Ohio Oregon Texas Utah b d e Utah NaN NaN NaN NaN NaN NaN NaN Ohio NaN NaN NaN NaN NaN NaN NaN Texas NaN NaN NaN NaN NaN NaN NaN Oregon NaN NaN NaN NaN NaN NaN NaN 函数应用和映射12frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon']) 1frame b d e Utah -0.204708 0.478943 -0.519439 Ohio -0.555730 1.965781 1.393406 Texas 0.092908 0.281746 0.769023 Oregon 1.246435 1.007189 -1.296221 Numpy的元素级方法也可以应用到DataFrame上，直接把DataFrame当作二维的Numpy.array即可 1np.abs(frame) b d e Utah 0.204708 0.478943 0.519439 Ohio 0.555730 1.965781 1.393406 Texas 0.092908 0.281746 0.769023 Oregon 1.246435 1.007189 1.296221 1f = lambda x: x.max() - x.min() apply方法将函数映射到由各行或者各列形成的一维数组上 1frame.apply(f) # axis=0 b 1.802165 d 1.684034 e 2.689627 dtype: float64 1frame.apply(f, axis=1) Utah 0.998382 Ohio 2.521511 Texas 0.676115 Oregon 2.542656 dtype: float64 1234def f(x): return Series([x.min(), x.max()], index=['min', 'max'])frame.apply(f)frame.apply(f, axis=1) b d e min -0.555730 0.281746 -1.296221 max 1.246435 1.965781 1.393406 min max Utah -0.519439 0.478943 Ohio -0.555730 1.965781 Texas 0.092908 0.769023 Oregon -1.296221 1.246435 元素级的函数映射applymap， 之所以叫这个名字是因为Series有一个元素级的映射函数map 12format = lambda x: '%.2f' % xframe.applymap(format) b d e Utah -0.20 0.48 -0.52 Ohio -0.56 1.97 1.39 Texas 0.09 0.28 0.77 Oregon 1.25 1.01 -1.30 1frame['e'].map(format) Utah -0.52 Ohio 1.39 Texas 0.77 Oregon -1.30 Name: e, dtype: object 排序和排名对索引或者column进行（字典）排序 12obj = Series(range(4), index=['d', 'a', 'b', 'c'])obj.sort_index() a 1 b 2 c 3 d 0 dtype: int32 123frame = DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'], columns=['d', 'a', 'b', 'c'])frame.sort_index() d a b c one 4 5 6 7 three 0 1 2 3 1frame.sort_index(axis=1) a b c d three 1 2 3 0 one 5 6 7 4 降序 1frame.sort_index(axis=1, ascending=False) d c b a three 0 3 2 1 one 4 7 6 5 按照data进行排序 12obj = Series([4, 7, -3, 2])obj.sort_values() 2 -3 3 2 0 4 1 7 dtype: int64 在排序时，任何缺失值默认都会被放到Series的末尾 12obj = Series([4, np.nan, 7, np.nan, -3, 2])obj.sort_values() 4 -3.0 5 2.0 0 4.0 2 7.0 1 NaN 3 NaN dtype: float64 12frame = DataFrame(&#123;'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]&#125;)frame a b 0 0 4 1 1 7 2 0 -3 3 1 2 对指定index或者column进行排序 1frame.sort_values(by='b') a b 2 0 -3 3 1 2 0 0 4 1 1 7 或者根据multi-index亦或multi-column进行排序 1frame.sort_values(by=['a', 'b']) a b 2 0 -3 0 0 4 3 1 2 1 1 7 默认情况下，rank方法通过“为各组分配一个平均排名”的方式破坏平级关系。也就是说，如果有多个相同的值，则这些值的rank就是这些相同值rand的算术平均。 12obj = Series([7, -5, 7, 4, 2, 0, 4, 4])obj.rank() 0 7.5 1 1.0 2 7.5 3 5.0 4 3.0 5 2.0 6 5.0 7 5.0 dtype: float64 如果想按照一般方式排名 1obj.rank(method='first') 0 7.0 1 1.0 2 8.0 3 4.0 4 3.0 5 2.0 6 5.0 7 6.0 dtype: float64 使用每个分组的最大排名 1obj.rank(ascending=False, method='max') 0 2.0 1 8.0 2 2.0 3 5.0 4 6.0 5 7.0 6 5.0 7 5.0 dtype: float64 123frame = DataFrame(&#123;'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1], 'c': [-2, 5, 8, -2.5]&#125;)frame a b c 0 0 4.3 -2.0 1 1 7.0 5.0 2 0 -3.0 8.0 3 1 2.0 -2.5 指定维度 1frame.rank(axis=1) a b c 0 2.0 3.0 1.0 1 1.0 3.0 2.0 2 2.0 1.0 3.0 3 2.0 3.0 1.0 带有重复值的轴索引12obj = Series(range(5), index=['a', 'a', 'b', 'b', 'c'])obj a 0 a 1 b 2 b 3 c 4 dtype: int32 1obj.index.is_unique False 返回一个Series 1obj['a'] a 0 a 1 dtype: int32 1obj['c'] 4 12df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])df 0 1 2 a 0.274992 0.228913 1.352917 a 0.886429 -2.001637 -0.371843 b 1.669025 -0.438570 -0.539741 b 0.476985 3.248944 -1.021228 1df.ix['b'] 0 1 2 b 1.669025 -0.438570 -0.539741 b 0.476985 3.248944 -1.021228 汇总和计算描述统计12345df = DataFrame([[1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]], index=['a', 'b', 'c', 'd'], columns=['one', 'two'])df one two a 1.40 NaN b 7.10 -4.5 c NaN NaN d 0.75 -1.3 1df.sum() # axis=0 skipna=True one 9.25 two -5.80 dtype: float64 1df.sum(axis=1) # skipna=True a 1.40 b 2.60 c 0.00 d -0.55 dtype: float64 1df.mean(axis=1, skipna=False) a NaN b 1.300 c NaN d -0.275 dtype: float64 返回的是索引 1df.idxmax() one b two d dtype: object 1df.cumsum() one two a 1.40 NaN b 8.50 -4.5 c NaN NaN d 9.25 -5.8 describe对于数值型和非数值型数据的行为不一样 1df.describe() C:\Users\Ewan\Anaconda3\lib\site-packages\numpy\lib\function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile RuntimeWarning) one two count 3.000000 2.000000 mean 3.083333 -2.900000 std 3.493685 2.262742 min 0.750000 -4.500000 25% NaN NaN 50% NaN NaN 75% NaN NaN max 7.100000 -1.300000 123obj = Series(['a', 'a', 'b', 'c'] * 4)objobj.describe() 0 a 1 a 2 b 3 c 4 a 5 a 6 b 7 c 8 a 9 a 10 b 11 c 12 a 13 a 14 b 15 c dtype: object count 16 unique 3 top a freq 8 dtype: object 相关系数和xi12345678910import pandas_datareader.data as weball_data = &#123;&#125;for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']: all_data[ticker] = web.get_data_yahoo(ticker)price = DataFrame(&#123;tic: data['Adj Close'] for tic, data in all_data.items()&#125;)volume = DataFrame(&#123;tic: data['Volume'] for tic, data in all_data.items()&#125;) 1price[:10] AAPL GOOG IBM MSFT Date 2010-01-04 27.727039 313.062468 111.405000 25.555485 2010-01-05 27.774976 311.683844 110.059232 25.563741 2010-01-06 27.333178 303.826685 109.344283 25.406859 2010-01-07 27.282650 296.753749 108.965786 25.142634 2010-01-08 27.464034 300.709808 110.059232 25.316031 2010-01-11 27.221758 300.255255 108.906903 24.994007 2010-01-12 26.912110 294.945572 109.773245 24.828866 2010-01-13 27.291720 293.252243 109.537735 25.060064 2010-01-14 27.133657 294.630868 111.287245 25.563741 2010-01-15 26.680198 289.710772 110.841458 25.481172 1volume[:10] AAPL GOOG IBM MSFT Date 2010-01-04 123432400 3927000 6155300 38409100 2010-01-05 150476200 6031900 6841400 49749600 2010-01-06 138040000 7987100 5605300 58182400 2010-01-07 119282800 12876600 5840600 50559700 2010-01-08 111902700 9483900 4197200 51197400 2010-01-11 115557400 14479800 5730400 68754700 2010-01-12 148614900 9742900 8081500 65912100 2010-01-13 151473000 13041800 6455400 51863500 2010-01-14 108223500 8511900 7111800 63228100 2010-01-15 148516900 10909600 8494400 79913200 price.pct_change Signature: price.pct_change(periods=1, fill_method=’pad’, limit=None, freq=None, **kwargs)Docstring:Percent change over given number of periods. Parameters periods : int, default 1 Periods to shift for forming percent change fill_method : str, default ‘pad’ How to handle NAs before computing percent changes limit : int, default None The number of consecutive NAs to fill before stopping freq : DateOffset, timedelta, or offset alias string, optional Increment to use from time series API (e.g. ‘M’ or BDay()) Returns chg : NDFrame Notes By default, the percentage change is calculated along the stataxis: 0, or Index, for DataFrame and 1, or minor forPanel. You can change this with the axis keyword argument. 12returns = price.pct_change()returns.tail() AAPL GOOG IBM MSFT Date 2017-02-21 0.007221 0.004335 -0.002269 -0.002012 2017-02-22 0.002999 -0.001082 0.004937 -0.002016 2017-02-23 -0.004230 0.000686 0.002760 0.004040 2017-02-24 0.000952 -0.003236 -0.001651 0.000000 2017-02-27 0.001976 0.000772 -0.010753 -0.006035 1returns.MSFT.corr(returns.IBM) 0.49525655865062668 1returns.MSFT.cov(returns.IBM) 8.5880535146740545e-05 1returns.corr() AAPL GOOG IBM MSFT AAPL 1.000000 0.409523 0.381374 0.388875 GOOG 0.409523 1.000000 0.402781 0.470781 IBM 0.381374 0.402781 1.000000 0.495257 MSFT 0.388875 0.470781 0.495257 1.000000 1returns.cov() AAPL GOOG IBM MSFT AAPL 0.000269 0.000105 0.000075 0.000092 GOOG 0.000105 0.000244 0.000075 0.000106 IBM 0.000075 0.000075 0.000144 0.000086 MSFT 0.000092 0.000106 0.000086 0.000209 1returns.corrwith(returns.IBM) AAPL 0.381374 GOOG 0.402781 IBM 1.000000 MSFT 0.495257 dtype: float64 1returns.corrwith(volume) AAPL -0.074055 GOOG -0.009543 IBM -0.194107 MSFT -0.090724 dtype: float64 唯一值， 值计数以及成员资格1obj = Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c']) 12uniques = obj.unique()uniques array([&#39;c&#39;, &#39;a&#39;, &#39;d&#39;, &#39;b&#39;], dtype=object) 1obj.value_counts() c 3 a 3 b 2 d 1 dtype: int64 1pd.value_counts(obj.values, sort=False) a 3 d 1 b 2 c 3 dtype: int64 12mask = obj.isin(['b', 'c'])mask 0 True 1 False 2 False 3 False 4 False 5 True 6 True 7 True 8 True dtype: bool 1obj[mask] 0 c 5 b 6 b 7 c 8 c dtype: object 1234data = DataFrame(&#123;'Qu1': [1, 3, 4, 3, 4], 'Qu2': [2, 3, 1, 2, 3], 'Qu3': [1, 5, 2, 4, 4]&#125;)data Qu1 Qu2 Qu3 0 1 2 1 1 3 3 5 2 4 1 2 3 3 2 4 4 4 3 4 12result = data.apply(pd.value_counts).fillna(0)result Qu1 Qu2 Qu3 1 1.0 1.0 1.0 2 0.0 2.0 1.0 3 2.0 2.0 0.0 4 2.0 0.0 2.0 5 0.0 0.0 1.0 处理缺失数据12string_data = Series(['aardvark', 'artichoke', np.nan, 'avocado'])string_data 0 aardvark 1 artichoke 2 NaN 3 avocado dtype: object 1string_data.isnull() 0 False 1 False 2 True 3 False dtype: bool 12string_data[0] = Nonestring_data.isnull() 0 True 1 False 2 True 3 False dtype: bool 滤除缺失数据123from numpy import nan as NAdata = Series([1, NA, 3.5, NA, 7])data.dropna() 0 1.0 2 3.5 4 7.0 dtype: float64 1data[data.notnull()] 0 1.0 2 3.5 4 7.0 dtype: float64 1234data = DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])cleaned = data.dropna()data 0 1 2 0 1.0 6.5 3.0 1 1.0 NaN NaN 2 NaN NaN NaN 3 NaN 6.5 3.0 1cleaned 0 1 2 0 1.0 6.5 3.0 1data.dropna(how='all') 0 1 2 0 1.0 6.5 3.0 1 1.0 NaN NaN 3 NaN 6.5 3.0 12data[4] = NAdata 0 1 2 4 0 1.0 6.5 3.0 NaN 1 1.0 NaN NaN NaN 2 NaN NaN NaN NaN 3 NaN 6.5 3.0 NaN 1data.dropna(axis=1, how='all') 0 1 2 0 1.0 6.5 3.0 1 1.0 NaN NaN 2 NaN NaN NaN 3 NaN 6.5 3.0 123df = DataFrame(np.random.randn(7, 3))df.ix[:4, 1] = NA; df.ix[:2, 2] = NAdf 0 1 2 0 -0.204708 NaN NaN 1 -0.555730 NaN NaN 2 0.092908 NaN NaN 3 1.246435 NaN -1.296221 4 0.274992 NaN 1.352917 5 0.886429 -2.001637 -0.371843 6 1.669025 -0.438570 -0.539741 1df.dropna(thresh=2) 0 1 2 3 1.246435 NaN -1.296221 4 0.274992 NaN 1.352917 5 0.886429 -2.001637 -0.371843 6 1.669025 -0.438570 -0.539741 填充缺失数据1df.fillna(0) 0 1 2 0 -0.204708 0.000000 0.000000 1 -0.555730 0.000000 0.000000 2 0.092908 0.000000 0.000000 3 1.246435 0.000000 -1.296221 4 0.274992 0.000000 1.352917 5 0.886429 -2.001637 -0.371843 6 1.669025 -0.438570 -0.539741 1df.fillna(&#123;1: 0.5, 3: -1&#125;) 0 1 2 0 -0.204708 0.500000 NaN 1 -0.555730 0.500000 NaN 2 0.092908 0.500000 NaN 3 1.246435 0.500000 -1.296221 4 0.274992 0.500000 1.352917 5 0.886429 -2.001637 -0.371843 6 1.669025 -0.438570 -0.539741 fillna默认返回新对象，但也可以对现有对象就地修改 123# always returns a reference to the filled object_ = df.fillna(0, inplace=True)df 0 1 2 0 -0.204708 0.000000 0.000000 1 -0.555730 0.000000 0.000000 2 0.092908 0.000000 0.000000 3 1.246435 0.000000 -1.296221 4 0.274992 0.000000 1.352917 5 0.886429 -2.001637 -0.371843 6 1.669025 -0.438570 -0.539741 123df = DataFrame(np.random.randn(6, 3))df.ix[2:, 1] = NA; df.ix[4:, 2] = NAdf 0 1 2 0 0.476985 3.248944 -1.021228 1 -0.577087 0.124121 0.302614 2 0.523772 NaN 1.343810 3 -0.713544 NaN -2.370232 4 -1.860761 NaN NaN 5 -1.265934 NaN NaN 1df.fillna(method='ffill') 0 1 2 0 0.476985 3.248944 -1.021228 1 -0.577087 0.124121 0.302614 2 0.523772 0.124121 1.343810 3 -0.713544 0.124121 -2.370232 4 -1.860761 0.124121 -2.370232 5 -1.265934 0.124121 -2.370232 1df.fillna(method='ffill', limit=2) 0 1 2 0 0.476985 3.248944 -1.021228 1 -0.577087 0.124121 0.302614 2 0.523772 0.124121 1.343810 3 -0.713544 0.124121 -2.370232 4 -1.860761 NaN -2.370232 5 -1.265934 NaN -2.370232 12data = Series([1., NA, 3.5, NA, 7])data.fillna(data.mean()) 0 1.000000 1 3.833333 2 3.500000 3 3.833333 4 7.000000 dtype: float64 层次索引1234data = Series(np.random.randn(10), index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd', 'd'], [1, 2, 3, 1, 2, 3, 1, 2, 2, 3]])data a 1 0.332883 2 -2.359419 3 -0.199543 b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 d 2 -0.753887 3 0.331286 dtype: float64 1data.index MultiIndex(levels=[[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;], [1, 2, 3]], labels=[[0, 0, 0, 1, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 1, 2]]) 1data['b'] 1 -1.541996 2 -0.970736 3 -1.307030 dtype: float64 1data['b':'c'] b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 dtype: float64 1data.ix[['b', 'd']] b 1 -1.541996 2 -0.970736 3 -1.307030 d 2 -0.753887 3 0.331286 dtype: float64 1data[:, 2] a -2.359419 b -0.970736 c 0.377984 d -0.753887 dtype: float64 1data.unstack() 1 2 3 a 0.332883 -2.359419 -0.199543 b -1.541996 -0.970736 -1.307030 c 0.286350 0.377984 NaN d NaN -0.753887 0.331286 1data.unstack().stack() a 1 0.332883 2 -2.359419 3 -0.199543 b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 d 2 -0.753887 3 0.331286 dtype: float64 12345frame = DataFrame(np.arange(12).reshape((4, 3)), index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]], columns=[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])frame Ohio Colorado Green Red Green a 1 0 1 2 2 3 4 5 b 1 6 7 8 2 9 10 11 123frame.index.names = ['key1', 'key2']frame.columns.names = ['state', 'color']frame state Ohio Colorado color Green Red Green key1 key2 a 1 0 1 2 2 3 4 5 b 1 6 7 8 2 9 10 11 1frame['Ohio'] color Green Red key1 key2 a 1 0 1 2 3 4 b 1 6 7 2 9 10 创建MultiIndex对象复用12MultiIndex.from_arrays([[&apos;Ohio&apos;, &apos;Ohio&apos;, &apos;Colorado&apos;], [&apos;Green&apos;, &apos;Red&apos;, &apos;Green&apos;]], names=[&apos;state&apos;, &apos;color&apos;]) 重排分级顺序1frame.swaplevel('key1', 'key2') state Ohio Colorado color Green Red Green key2 key1 1 a 0 1 2 2 a 3 4 5 1 b 6 7 8 2 b 9 10 11 1frame.sortlevel(1) state Ohio Colorado color Green Red Green key1 key2 a 1 0 1 2 b 1 6 7 8 a 2 3 4 5 b 2 9 10 11 1frame.swaplevel(0, 1).sortlevel(0) state Ohio Colorado color Green Red Green key2 key1 1 a 0 1 2 b 6 7 8 2 a 3 4 5 b 9 10 11 根据级别汇总统计1frame.sum(level='key2') state Ohio Colorado color Green Red Green key2 1 6 8 10 2 12 14 16 1frame.sum(level='color', axis=1) color Green Red key1 key2 a 1 2 1 2 8 4 b 1 14 7 2 20 10 使用DataFrame的列1234frame = DataFrame(&#123;'a': range(7), 'b': range(7, 0, -1), 'c': ['one', 'one', 'one', 'two', 'two', 'two', 'two'], 'd': [0, 1, 2, 0, 1, 2, 3]&#125;)frame a b c d 0 0 7 one 0 1 1 6 one 1 2 2 5 one 2 3 3 4 two 0 4 4 3 two 1 5 5 2 two 2 6 6 1 two 3 12frame2 = frame.set_index(['c', 'd'])frame2 a b c d one 0 0 7 1 1 6 2 2 5 two 0 3 4 1 4 3 2 5 2 3 6 1 1frame.set_index(['c', 'd'], drop=False) a b c d c d one 0 0 7 one 0 1 1 6 one 1 2 2 5 one 2 two 0 3 4 two 0 1 4 3 two 1 2 5 2 two 2 3 6 1 two 3 1frame2.reset_index() c d a b 0 one 0 0 7 1 one 1 1 6 2 one 2 2 5 3 two 0 3 4 4 two 1 4 3 5 two 2 5 2 6 two 3 6 1 拓展话题整数索引12ser = Series(np.arange(3.))ser.iloc[-1] 2.0 1ser 0 0.0 1 1.0 2 2.0 dtype: float64 12ser2 = Series(np.arange(3.), index=['a', 'b', 'c'])ser2[-1] 2.0 1ser.ix[:1] 0 0.0 1 1.0 dtype: float64 12ser3 = Series(range(3), index=[-5, 1, 3])ser3.iloc[2] 2 123frame = DataFrame(np.arange(6).reshape((3, 2)), index=[2, 0, 1])frameframe.iloc[0] 0 1 2 0 1 0 2 3 1 4 5 0 0 1 1 Name: 2, dtype: int32 面板数据1234import pandas_datareader.data as webpdata = pd.Panel(dict((stk, web.get_data_yahoo(stk)) for stk in ['AAPL', 'GOOG', 'MSFT', 'DELL'])) 1pdata &lt;class &#39;pandas.core.panel.Panel&#39;&gt; Dimensions: 4 (items) x 1820 (major_axis) x 6 (minor_axis) Items axis: AAPL to MSFT Major_axis axis: 2010-01-04 00:00:00 to 2017-02-27 00:00:00 Minor_axis axis: Open to Adj Close 12pdata = pdata.swapaxes('items', 'minor')pdata['Adj Close'].iloc[:10] AAPL DELL GOOG MSFT Date 2010-01-04 27.727039 14.06528 313.062468 25.555485 2010-01-05 27.774976 14.38450 311.683844 25.563741 2010-01-06 27.333178 14.10397 303.826685 25.406859 2010-01-07 27.282650 14.23940 296.753749 25.142634 2010-01-08 27.464034 14.36516 300.709808 25.316031 2010-01-11 27.221758 14.37483 300.255255 24.994007 2010-01-12 26.912110 14.56830 294.945572 24.828866 2010-01-13 27.291720 14.57797 293.252243 25.060064 2010-01-14 27.133657 14.22005 294.630868 25.563741 2010-01-15 26.680198 13.92985 289.710772 25.481172 1pdata.ix[:, '6/1/2012', :] Open High Low Close Volume Adj Close AAPL 569.159996 572.650009 560.520012 560.989983 130246900.0 72.681610 DELL 12.150000 12.300000 12.045000 12.070000 19397600.0 11.675920 GOOG 571.790972 572.650996 568.350996 570.981000 6138700.0 285.205295 MSFT 28.760000 28.959999 28.440001 28.450001 56634300.0 24.942239 1pdata.ix['Adj Close', '5/22/2012':, :].iloc[:10] AAPL DELL GOOG MSFT Date 2012-05-22 72.160786 14.58765 300.100412 26.090721 2012-05-23 73.921494 12.08221 304.426106 25.520864 2012-05-24 73.242607 12.04351 301.528978 25.485795 2012-05-25 72.850038 12.05319 295.470050 25.477028 2012-05-28 NaN 12.05319 NaN NaN 2012-05-29 74.143041 12.24666 296.873645 25.915380 2012-05-30 75.037005 12.14992 293.821674 25.722505 2012-05-31 74.850442 11.92743 290.140354 25.591000 2012-06-01 72.681610 11.67592 285.205295 24.942239 2012-06-04 73.109156 11.60821 289.006480 25.029908 12stacked = pdata.ix[:, '5/30/2012':, :].to_frame()stacked Open High Low Close Volume Adj Close Date minor 2012-05-30 AAPL 569.199997 579.989990 566.559990 579.169998 132357400.0 75.037005 DELL 12.590000 12.700000 12.460000 12.560000 19787800.0 12.149920 GOOG 588.161028 591.901014 583.530999 588.230992 3827600.0 293.821674 MSFT 29.350000 29.480000 29.120001 29.340000 41585500.0 25.722505 2012-05-31 AAPL 580.740021 581.499985 571.460022 577.730019 122918600.0 74.850442 DELL 12.530000 12.540000 12.330000 12.330000 19955600.0 11.927430 GOOG 588.720982 590.001032 579.001013 580.860990 5958800.0 290.140354 MSFT 29.299999 29.420000 28.940001 29.190001 39134000.0 25.591000 2012-06-01 AAPL 569.159996 572.650009 560.520012 560.989983 130246900.0 72.681610 DELL 12.150000 12.300000 12.045000 12.070000 19397600.0 11.675920 GOOG 571.790972 572.650996 568.350996 570.981000 6138700.0 285.205295 MSFT 28.760000 28.959999 28.440001 28.450001 56634300.0 24.942239 2012-06-04 AAPL 561.500008 567.499985 548.499977 564.289978 139248900.0 73.109156 DELL 12.110000 12.112500 11.800000 12.000000 17015700.0 11.608210 GOOG 570.220958 580.491016 570.011006 578.590973 4883500.0 289.006480 MSFT 28.620001 28.780001 28.320000 28.549999 47926300.0 25.029908 2012-06-05 AAPL 561.269989 566.470001 558.330002 562.830025 97053600.0 72.920005 DELL 11.950000 12.240000 11.950000 12.160000 15620900.0 11.762980 GOOG 575.451008 578.131003 566.470986 570.410999 4697200.0 284.920579 MSFT 28.510000 28.750000 28.389999 28.510000 45715400.0 24.994841 2012-06-06 AAPL 567.770004 573.849983 565.499992 571.460022 100363900.0 74.038104 DELL 12.210000 12.280000 12.090000 12.215000 20779900.0 11.816190 GOOG 576.480979 581.970971 573.611004 580.570966 4207200.0 289.995487 MSFT 28.879999 29.370001 28.809999 29.350000 46860500.0 25.731273 2012-06-07 AAPL 577.290009 577.320023 570.500000 571.720001 94941700.0 74.071787 DELL 12.320000 12.410000 12.120000 12.130000 20074000.0 11.733960 GOOG 587.601014 587.891038 577.251006 578.230986 3530100.0 288.826666 MSFT 29.639999 29.700001 29.170000 29.230000 37792800.0 25.626067 2012-06-08 AAPL 571.599998 580.580017 568.999992 580.319984 86879100.0 75.185997 DELL 12.130000 12.225000 12.020000 12.120000 18155600.0 11.724290 ... ... ... ... ... ... ... ... 2017-02-13 AAPL 133.080002 133.820007 132.750000 133.289993 23035400.0 133.289993 GOOG 816.000000 820.958984 815.489990 819.239990 1198100.0 819.239990 MSFT 64.239998 64.860001 64.129997 64.720001 22920100.0 64.330000 2017-02-14 AAPL 133.470001 135.089996 133.250000 135.020004 32815500.0 135.020004 GOOG 819.000000 823.000000 816.000000 820.450012 1053600.0 820.450012 MSFT 64.410004 64.720001 64.019997 64.570000 23065900.0 64.570000 2017-02-15 AAPL 135.520004 136.270004 134.619995 135.509995 35501600.0 135.509995 GOOG 819.359985 823.000000 818.469971 818.979980 1304000.0 818.979980 MSFT 64.500000 64.570000 64.160004 64.529999 16917000.0 64.529999 2017-02-16 AAPL 135.669998 135.899994 134.839996 135.350006 22118000.0 135.350006 GOOG 819.929993 824.400024 818.979980 824.159973 1281700.0 824.159973 MSFT 64.739998 65.239998 64.440002 64.519997 20524700.0 64.519997 2017-02-17 AAPL 135.100006 135.830002 135.100006 135.720001 22084500.0 135.720001 GOOG 823.020020 828.070007 821.655029 828.070007 1597800.0 828.070007 MSFT 64.470001 64.690002 64.300003 64.620003 21234600.0 64.620003 2017-02-21 AAPL 136.229996 136.750000 135.979996 136.699997 24265100.0 136.699997 GOOG 828.659973 833.450012 828.349976 831.659973 1247700.0 831.659973 MSFT 64.610001 64.949997 64.449997 64.489998 19384900.0 64.489998 2017-02-22 AAPL 136.429993 137.119995 136.110001 137.110001 20745300.0 137.110001 GOOG 828.659973 833.250000 828.640015 830.760010 982900.0 830.760010 MSFT 64.330002 64.389999 64.050003 64.360001 19259700.0 64.360001 2017-02-23 AAPL 137.380005 137.479996 136.300003 136.529999 20704100.0 136.529999 GOOG 830.119995 832.460022 822.880005 831.330017 1470100.0 831.330017 MSFT 64.419998 64.730003 64.190002 64.620003 20235200.0 64.620003 2017-02-24 AAPL 135.910004 136.660004 135.279999 136.660004 21690900.0 136.660004 GOOG 827.729980 829.000000 824.200012 828.640015 1386600.0 828.640015 MSFT 64.529999 64.800003 64.139999 64.620003 21705200.0 64.620003 2017-02-27 AAPL 137.139999 137.440002 136.279999 136.929993 20196400.0 136.929993 GOOG 824.549988 830.500000 824.000000 829.280029 1099500.0 829.280029 MSFT 64.540001 64.540001 64.050003 64.230003 15850400.0 64.230003 3952 rows × 6 columns 1stacked.to_panel() &lt;class &#39;pandas.core.panel.Panel&#39;&gt; Dimensions: 6 (items) x 1207 (major_axis) x 4 (minor_axis) Items axis: Open to Adj Close Major_axis axis: 2012-05-30 00:00:00 to 2017-02-27 00:00:00 Minor_axis axis: AAPL to MSFT]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis-Learning note-Ch04]]></title>
      <url>%2F2017%2F02%2F26%2FPython-data-analysis-Learning-note-Ch04%2F</url>
      <content type="text"><![CDATA[Numpy基础：数组和矢量计算1%matplotlib inline 1234from __future__ import divisionfrom numpy.random import randnimport numpy as npnp.set_printoptions(precision=4, suppress=True) 12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all" NumPy ndarray: 一种多维数组对象1data = randn(2, 3) 123datadata * 10data + data array([[ 0.1584, 0.299 , -0.2555], [ 0.3277, -0.6934, 1.3191]]) array([[ 1.5842, 2.9896, -2.5545], [ 3.2767, -6.9342, 13.1913]]) array([[ 0.3168, 0.5979, -0.5109], [ 0.6553, -1.3868, 2.6383]]) 12data.shapedata.dtype (2, 3) dtype(&#39;float64&#39;) 创建ndarray123data1 = [6, 7.5, 8, 0, 1]arr1 = np.array(data1)arr1 array([ 6. , 7.5, 8. , 0. , 1. ]) 12345data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]arr2 = np.array(data2)arr2arr2.ndimarr2.shape array([[1, 2, 3, 4], [5, 6, 7, 8]]) 2 (2, 4) 除非显示说明，np.array会尝试为新建的数组选择一个合适的类型 12arr1.dtypearr2.dtype dtype(&#39;float64&#39;) dtype(&#39;int32&#39;) 123np.zeros(10)np.zeros((3, 6))np.empty((2, 3, 2)) array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) array([[ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.]]) array([[[ 0., 0.], [ 0., 0.], [ 0., 0.]], [[ 0., 0.], [ 0., 0.], [ 0., 0.]]]) 1np.arange(15) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) ones_like, zeros_like, empty_like这三个方法接受一个数组为对象，创建和这个数组形状和dtype一样的全1， 全0和分配的初始空间 ndarray的数据类型1234arr1 = np.array([1, 2, 3], dtype=np.float64)arr2 = np.array([1, 2, 3], dtype=np.int32)arr1.dtypearr2.dtype dtype(&#39;float64&#39;) dtype(&#39;int32&#39;) 当需要控制数据在内存和磁盘中的存储方式时（尤其是对大数据集），那就得了解如何控制存储类型 1234arr = np.array([1, 2, 3, 4, 5])arr.dtypefloat_arr = arr.astype(np.float64) float_arr.dtype dtype(&#39;int32&#39;) dtype(&#39;float64&#39;) 123arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])arrarr.astype(np.int32) array([ 3.7, -1.2, -2.6, 0.5, 12.9, 10.1]) array([ 3, -1, -2, 0, 12, 10]) 12numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_)numeric_strings.astype(float) array([ 1.25, -9.6 , 42. ]) 123int_array = np.arange(10)calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)int_array.astype(calibers.dtype) array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) 12empty_uint32 = np.empty(8, dtype='u4')empty_uint32 array([1, 2, 3, 4, 5, 6, 7, 8], dtype=uint32) 调用astype会创建原数组的一份拷贝 数组和标量之间的运算1234arr = np.array([[1., 2., 3.], [4., 5., 6.]])arrarr * arrarr - arr array([[ 1., 2., 3.], [ 4., 5., 6.]]) array([[ 1., 4., 9.], [ 16., 25., 36.]]) array([[ 0., 0., 0.], [ 0., 0., 0.]]) 121 / arrarr ** 0.5 array([[ 1. , 0.5 , 0.3333], [ 0.25 , 0.2 , 0.1667]]) array([[ 1. , 1.4142, 1.7321], [ 2. , 2.2361, 2.4495]]) 基本的索引和切片123456arr = np.arange(10)arrarr[5]arr[5:8]arr[5:8] = 12arr array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 5 array([5, 6, 7]) array([ 0, 1, 2, 3, 4, 12, 12, 12, 8, 9]) 切片直接在原数组上操作如果想要得到一个复制的版本，需要显示地调用copy()方法 12345arr_slice = arr[5:8]arr_slice[1] = 12345arrarr_slice[:] = 64arr array([ 0, 1, 2, 3, 4, 12, 12345, 12, 8, 9]) array([ 0, 1, 2, 3, 4, 64, 64, 64, 8, 9]) 12arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])arr2d[2] array([7, 8, 9]) 注意下面这种索引方式 12arr2d[0][2]arr2d[0, 2] 3 3 12arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])arr3d array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[0] array([[1, 2, 3], [4, 5, 6]]) 12345old_values = arr3d[0].copy()arr3d[0] = 42arr3darr3d[0] = old_valuesarr3d array([[[42, 42, 42], [42, 42, 42]], [[ 7, 8, 9], [10, 11, 12]]]) array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[1, 0] array([7, 8, 9]) 切片索引1arr[1:6] array([ 1, 2, 3, 4, 64]) 12arr2darr2d[:2] array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) array([[1, 2, 3], [4, 5, 6]]) 1arr2d[:2, 1:] array([[2, 3], [5, 6]]) 12arr2d[1, :2]arr2d[2, :1] array([4, 5]) array([7]) 1arr2d[:, :1] array([[1], [4], [7]]) 1arr2d[:2, 1:] = 0 布尔型索引1234names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])data = randn(7, 4)namesdata array([&#39;Bob&#39;, &#39;Joe&#39;, &#39;Will&#39;, &#39;Bob&#39;, &#39;Will&#39;, &#39;Joe&#39;, &#39;Joe&#39;], dtype=&#39;&lt;U4&#39;) array([[-2.9033, 1.4721, 0.9512, 1.7727], [ 2.2303, -1.0259, 1.0664, 0.534 ], [-0.9725, 0.2226, -0.1538, -0.4994], [-1.4289, 0.1665, -1.2874, -1.0817], [ 1.3581, -1.0734, -0.1387, 0.1673], [ 1.2816, 1.8883, 0.5699, -0.5843], [-0.0464, -0.9633, 0.2855, -0.6473]]) 1names == 'Bob' array([ True, False, False, True, False, False, False], dtype=bool) 1data[names == 'Bob'] array([[-2.9033, 1.4721, 0.9512, 1.7727], [-1.4289, 0.1665, -1.2874, -1.0817]]) 12data[names == 'Bob', 2:]data[names == 'Bob', 3] array([[ 0.9512, 1.7727], [-1.2874, -1.0817]]) array([ 1.7727, -1.0817]) 12names != 'Bob'data[~(names == 'Bob')] array([False, True, True, False, True, True, True], dtype=bool) array([[ 2.2303, -1.0259, 1.0664, 0.534 ], [-0.9725, 0.2226, -0.1538, -0.4994], [ 1.3581, -1.0734, -0.1387, 0.1673], [ 1.2816, 1.8883, 0.5699, -0.5843], [-0.0464, -0.9633, 0.2855, -0.6473]]) 123mask = (names == 'Bob') | (names == 'Will')maskdata[mask] array([ True, False, True, True, True, False, False], dtype=bool) array([[-2.9033, 1.4721, 0.9512, 1.7727], [-0.9725, 0.2226, -0.1538, -0.4994], [-1.4289, 0.1665, -1.2874, -1.0817], [ 1.3581, -1.0734, -0.1387, 0.1673]]) Python关键字and和or在布尔型数组中无效 12data[data &lt; 0] = 0data array([[ 0. , 1.4721, 0.9512, 1.7727], [ 2.2303, 0. , 1.0664, 0.534 ], [ 0. , 0.2226, 0. , 0. ], [ 0. , 0.1665, 0. , 0. ], [ 1.3581, 0. , 0. , 0.1673], [ 1.2816, 1.8883, 0.5699, 0. ], [ 0. , 0. , 0.2855, 0. ]]) 12data[names != 'Joe'] = 7data array([[ 7. , 7. , 7. , 7. ], [ 2.2303, 0. , 1.0664, 0.534 ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 1.2816, 1.8883, 0.5699, 0. ], [ 0. , 0. , 0.2855, 0. ]]) 花式索引花式索引创建新的数组 1234arr = np.empty((8, 4))for i in range(8): arr[i] = iarr array([[ 0., 0., 0., 0.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], [ 3., 3., 3., 3.], [ 4., 4., 4., 4.], [ 5., 5., 5., 5.], [ 6., 6., 6., 6.], [ 7., 7., 7., 7.]]) 1arr[[4, 3, 0, 6]] array([[ 4., 4., 4., 4.], [ 3., 3., 3., 3.], [ 0., 0., 0., 0.], [ 6., 6., 6., 6.]]) 1arr[[-3, -5, -7]] array([[ 5., 5., 5., 5.], [ 3., 3., 3., 3.], [ 1., 1., 1., 1.]]) 1234# more on reshape in Chapter 12arr = np.arange(32).reshape((8, 4))arrarr[[1, 5, 7, 2], [0, 3, 1, 2]] array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]) array([ 4, 23, 29, 10]) 根据以上可知，传入两个索引数组相当于进行了同位置组合 注意以下这种方式 1arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]] array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) np.ix_方法将两个一维数组转换为一个矩形区域的索引选择器 1arr[np.ix_([1, 5, 7, 2], [0, 3, 1, 2])] array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) 数组转置和轴对换123arr = np.arange(15).reshape((3, 5))arrarr.T array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]]) 12arr = np.random.randn(6, 3)np.dot(arr.T, arr) array([[ 3.6804, 0.0133, 1.0388], [ 0.0133, 1.6074, 0.1836], [ 1.0388, 0.1836, 3.5281]]) 123arr = np.arange(16).reshape((2, 2, 4))arrarr.transpose((1, 0, 2)) array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) Refered from here. In short: transposing an array means that NumPy just needs to permute the stride and shape information for each axis: &gt;&gt;&gt; arr.strides (64, 32, 8) &gt;&gt;&gt; arr.transpose(1, 0, 2).strides (32, 64, 8) Notice that the strides for the first and second axes were swapped here. This means that no data needs to be copied; NumPy can simply change how it looks at the memory to construct the array. What are strides? The values in a 3D array arr are stored in a contiguous block of memory like this: [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] In the case of arr, each integer takes up 8 bytes of memory (i.e. we’re using the int64 dtype). A stride tells NumPy how many bytes to skip in order to move to the next value along an axis. For example, to get the next value in a row in arr (axis 2), we just need to move 8 bytes (1 number). The strides for arr.transpose(1, 0, 2) are (32, 64, 8). To move along the first axis, instead of 64 bytes (8 numbers) NumPy will now only skip 32 bytes (4 numbers) each time: [[[0 ...] [... ...]] [[4 ...] [... ...]]] Similarly, NumPy will now skip 64 bytes (8 numbers) in order to move along axis 1: [[[0 ...] [8 ...]] [[4 ...] [12 ...]]] The actual code that does the transposing is written in C and can be found here. 也可以使用swapaxes方法 123arrarr.swapaxes(1, 2)arr array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) array([[[ 0, 4], [ 1, 5], [ 2, 6], [ 3, 7]], [[ 8, 12], [ 9, 13], [10, 14], [11, 15]]]) array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) 通用函数：快速的元素级数组函数123arr = np.arange(10)np.sqrt(arr)np.exp(arr) array([ 0. , 1. , 1.4142, 1.7321, 2. , 2.2361, 2.4495, 2.6458, 2.8284, 3. ]) array([ 1. , 2.7183, 7.3891, 20.0855, 54.5982, 148.4132, 403.4288, 1096.6332, 2980.958 , 8103.0839]) 12345x = randn(8)y = randn(8)xynp.maximum(x, y) # 对应元素进行比较 array([ 0.811 , -0.0214, -0.3702, -0.4856, 1.1449, -0.4246, 0.9396, 0.0382]) array([-1.223 , 0.3271, -1.7197, -2.2636, -0.1154, -1.4122, -0.0989, 0.4477]) array([ 0.811 , 0.3271, -0.3702, -0.4856, 1.1449, -0.4246, 0.9396, 0.4477]) modf函数挺有意思 123arr = randn(7) * 5arrnp.modf(arr) array([ 10.3171, -4.733 , -6.3358, 3.2457, -7.3823, 2.7036, -2.6173]) (array([ 0.3171, -0.733 , -0.3358, 0.2457, -0.3823, 0.7036, -0.6173]), array([ 10., -4., -6., 3., -7., 2., -2.])) 利用数组进行数据处理meshgrid产生两个二维数组，对应points中所有的二元组 1234points = np.arange(-5, 5, 0.01) # 1000 equally spaced pointsxs, ys = np.meshgrid(points, points)xsys array([[-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], ..., [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99]]) array([[-5. , -5. , -5. , ..., -5. , -5. , -5. ], [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99], [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98], ..., [ 4.97, 4.97, 4.97, ..., 4.97, 4.97, 4.97], [ 4.98, 4.98, 4.98, ..., 4.98, 4.98, 4.98], [ 4.99, 4.99, 4.99, ..., 4.99, 4.99, 4.99]]) 1from matplotlib.pyplot import imshow, title 12345import matplotlib.pyplot as pltz = np.sqrt(xs ** 2 + ys ** 2)zplt.imshow(z, cmap=plt.cm.gray); plt.colorbar()plt.title("Image plot of $\sqrt&#123;x^2 + y^2&#125;$ for a grid of values") array([[ 7.0711, 7.064 , 7.0569, ..., 7.0499, 7.0569, 7.064 ], [ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569], [ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499], ..., [ 7.0499, 7.0428, 7.0357, ..., 7.0286, 7.0357, 7.0428], [ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499], [ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569]]) &lt;matplotlib.image.AxesImage at 0x23400a22b38&gt; &lt;matplotlib.colorbar.Colorbar at 0x23400a7c7b8&gt; &lt;matplotlib.text.Text at 0x23400a03da0&gt; 1plt.draw() &lt;matplotlib.figure.Figure at 0x23401396eb8&gt; 将条件逻辑表述为数组运算123xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])cond = np.array([True, False, True, True, False]) 注意下面列表生成式的写法 123result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]result [1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5] 上述方法具有一些缺点： 大数组处理速度慢（纯Python实现） 无法处理多维数组 所以可以使用下面这种方法： 12result = np.where(cond, xarr, yarr)result array([ 1.1, 2.2, 1.3, 1.4, 2.5]) 1234arr = randn(4, 4)arrnp.where(arr &gt; 0, 2, -2)np.where(arr &gt; 0, 2, arr) # set only positive values to 2 array([[-0.7355, -0.3188, -0.2358, 0.3137], [-0.6196, -0.5803, -0.5504, -1.1508], [ 0.1719, -1.1599, -0.7115, 1.7869], [-0.2306, 0.2068, 1.5366, 1.6154]]) array([[-2, -2, -2, 2], [-2, -2, -2, -2], [ 2, -2, -2, 2], [-2, 2, 2, 2]]) array([[-0.7355, -0.3188, -0.2358, 2. ], [-0.6196, -0.5803, -0.5504, -1.1508], [ 2. , -1.1599, -0.7115, 2. ], [-0.2306, 2. , 2. , 2. ]]) 显然where还可以应用于更复杂的操作。考虑下面这种逻辑： 12345678910result = []for i in range(n): if cond1[i] and cond2[i]: result.append(0) elif cond1[i]: result.append(1) elif cond2[i]: result.append(2) else: result.append(3) 用where可以这样实现： 123np.where(cond1 &amp; cond2, 0, np.where(cond1, 1, np.where(cond2, 2, 3))) 更加magic一点： 1result = 1 * cond1 + 2 * cond2 + 3 * -(cond1 | cond2) 数学和统计方法123456arr = np.random.randn(5, 4) # 正态分布arr# 下面两种方式都可以使用arr.mean()np.mean(arr)arr.sum() array([[ 1.4513, -0.8225, 0.7011, -0.617 ], [ 1.5872, 1.2937, 1.0151, 0.7123], [-0.2012, -0.0168, -0.3847, 0.5274], [-0.6312, -0.2762, 0.4869, 0.0462], [-0.5268, -1.1071, 1.8642, 0.2282]]) 0.26650934393195791 0.26650934393195791 5.3301868786391582 12arr.mean(axis=1)arr.sum(0) # axis=0 array([ 0.1782, 1.1521, -0.0188, -0.0936, 0.1146]) array([ 1.6793, -0.9289, 3.6826, 0.8971]) 1234arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])arrarr.cumsum(0) # axis=0arr.cumprod(1) # axis=1 array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) array([[ 0, 1, 2], [ 3, 5, 7], [ 9, 12, 15]], dtype=int32) array([[ 0, 0, 0], [ 3, 12, 60], [ 6, 42, 336]], dtype=int32) 用于布尔数组的方法123arr = randn(100)arr(arr &gt; 0).sum() # Number of positive values array([ 0.7828, 0.1372, -0.6264, 1.8927, -0.2104, 0.2822, -0.3672, -0.3601, 0.5918, 0.9285, 0.1808, -0.4021, 0.4086, -0.2949, 0.5633, -0.7462, -0.1635, 0.1482, -0.3226, -1.2127, -0.9821, 0.0536, -0.1772, -0.4714, -0.9002, -0.0037, 0.7352, 0.5675, -1.1612, 0.5288, 0.3319, 0.7315, 0.6841, -0.6881, 1.5654, -0.4605, -0.5423, 0.0184, -0.8153, -0.1313, 0.4594, 0.0228, 0.255 , -2.2361, 0.8703, -1.5153, -0.9458, 0.2769, 0.9986, 0.7699, -0.7948, -1.2508, 1.7059, 0.1805, -1.0265, -0.0181, -0.9415, 0.1265, -0.2576, 0.6791, 0.3969, 0.8027, -0.6792, -0.7487, -1.9949, -0.9595, 0.5706, -0.5727, -1.0204, 0.1521, -0.9755, -0.4094, 0.67 , 0.212 , 0.4081, -0.1435, 0.3964, -0.1865, -0.6018, -2.6185, -0.5073, -0.6328, -0.2631, 0.6637, -0.5586, 1.3346, -0.5317, 0.8572, 1.1159, 0.9563, -0.0434, -1.0534, 0.5869, 0.0502, -0.0479, -0.8673, 0.1531, 1.0646, -0.2624, -0.3726]) 47 1234bools = np.array([False, False, True, False])boolsbools.any()bools.all() array([False, False, True, False], dtype=bool) True False 排序1234arr = randn(8)arrarr.sort()arr array([ 1.0584, 1.9062, -0.2923, 0.7169, 0.5186, -0.6089, -2.0444, -0.5661]) array([-2.0444, -0.6089, -0.5661, -0.2923, 0.5186, 0.7169, 1.0584, 1.9062]) 1234arr = randn(5, 3)arrarr.sort(1) # axis=1arr array([[ 0.0118, -2.8916, -0.4477], [-1.9768, 1.859 , -1.128 ], [-2.6262, 0.5791, 0.7594], [-0.5254, -0.9059, 0.0203], [-1.4029, -1.8566, 0.1892]]) array([[-2.8916, -0.4477, 0.0118], [-1.9768, -1.128 , 1.859 ], [-2.6262, 0.5791, 0.7594], [-0.9059, -0.5254, 0.0203], [-1.8566, -1.4029, 0.1892]]) 12345large_arr = randn(1000)large_arrlarge_arr.sort()large_arrlarge_arr[int(0.05 * len(large_arr))] # 5% quantile array([ 1.2296, 0.3794, -0.1526, 2.1223, -0.0675, 0.6867, -0.5742, -1.4291, 0.6856, 0.1364, -0.3966, -0.7793, 0.4965, 0.2447, -0.7487, 0.7695, 0.5358, -0.4813, 0.9949, -0.6489, -0.3656, 1.9551, 0.8327, 1.497 , -0.4431, -0.8357, -0.821 , -0.7348, 1.9294, -0.3144, 0.1396, -0.9111, 0.0943, 0.8043, 1.067 , 0.9362, -2.2574, 0.7475, -1.0152, -1.1234, -0.3774, 1.076 , 0.8743, 1.1864, 0.0801, 0.3995, 0.2536, -0.9371, -1.669 , -2.2444, 1.2544, 1.0539, -0.7579, 0.2963, 0.7496, -1.3655, 0.1552, -0.6259, 0.2621, -1.5415, -0.1036, -0.5794, 1.2098, 1.3388, 0.3159, 1.0998, 0.5109, -0.3927, 1.4797, -1.4891, 0.3624, 0.966 , 0.0756, -0.4703, 0.1859, 1.6091, 0.662 , -0.4808, 0.8744, 0.4738, 1.1351, 0.0251, -1.017 , -0.849 , -0.1602, -1.5392, 0.0601, 1.7323, 1.1837, 0.4657, 0.8858, -0.211 , 0.1865, 0.673 , 0.3086, -1.2527, -0.7802, 0.407 , -1.118 , -0.2058, 0.7921, 0.5284, -2.3038, -0.4038, -1.1087, -0.827 , -2.6518, 0.3711, -0.0244, 1.1103, 0.2748, -0.7962, 1.9456, 0.5347, 0.1862, -0.3734, -0.3036, 0.6831, -0.9419, 1.4848, -0.1247, -0.4138, -0.601 , 0.6138, 1.1334, 0.4386, 0.0466, -0.0588, 0.6883, -1.2912, -0.2381, 0.3934, 0.2132, -0.4143, 1.0844, -0.5258, -0.9944, 1.0977, 0.3528, 1.9928, 1.421 , 0.8634, 0.1973, -1.1799, -2.9433, 2.697 , 0.4778, 0.6464, 0.049 , -0.2339, 1.6945, -0.6568, -0.5972, -0.8324, -0.6443, 0.0882, -0.3686, 0.0419, 0.5119, -0.641 , 1.1545, 1.0735, -0.5329, -0.1126, 0.0375, -1.0699, -1.3153, -1.6097, 2.5671, -0.9516, -0.388 , -0.0129, -0.0171, -1.0763, -0.7125, 0.767 , 0.2254, -0.7638, -0.2065, 1.2797, 0.0784, -0.7762, 1.7106, -0.0136, -0.4435, 1.2946, -2.5489, 0.4241, 0.5675, -0.7596, 0.6128, 1.1161, -1.2456, -0.131 , -0.2684, 1.6461, -0.2497, -0.4294, 1.122 , 0.5969, 0.3335, -0.0453, 1.1567, 0.0216, -0.7277, -2.5465, -2.4542, -1.5895, 0.4607, -0.8303, 0.0263, 0.0301, -1.2365, -0.146 , -0.8632, 0.6449, 0.1958, -0.6914, -0.3223, 0.4037, 0.9918, -0.3542, 0.8442, 0.7751, -1.6248, 2.6081, 0.3524, 1.5298, 0.4421, 1.5228, -1.5263, -1.3994, 0.0285, -0.5389, 1.4047, -2.1117, -1.0397, 0.6495, 0.9073, 1.8738, 0.2913, -1.069 , -0.7835, -0.6437, 0.6739, 0.3272, -0.8483, -0.2971, 0.2882, 0.1778, -0.6705, -1.4129, -0.1935, 0.6615, -0.4423, -1.2472, -0.9816, 0.927 , -2.2774, 0.5736, 1.3996, 1.1653, -0.3253, -0.2074, -0.2447, 0.4925, 1.8415, -1.1551, -0.5131, -0.6407, 0.5033, -0.817 , 0.0479, -0.9106, 1.4391, -1.5824, -0.4652, 1.253 , -0.6051, 0.6699, 0.3803, 1.0767, 1.5449, 0.106 , -0.7215, -0.354 , 0.1016, -1.3191, -0.6596, -0.9632, -0.3655, 0.8411, -0.2314, 1.9493, -0.6966, -1.2598, 0.4023, 0.1704, -0.452 , 1.5924, 0.381 , -0.4731, -1.2467, -0.4264, -0.2298, -0.1792, -0.5009, -1.0032, 1.0126, 0.5436, 1.1366, -1.0318, 1.3289, 0.3218, -0.2828, 0.5597, -0.0213, -0.078 , 0.7667, -0.3984, -1.0263, -0.5557, -2.0724, -0.9343, -0.6877, 1.0567, -0.605 , 1.7923, 0.6351, -1.769 , 0.4175, 0.8266, 0.3767, -0.1508, -0.4301, -0.3397, 0.7248, 0.188 , 1.1632, -1.0831, -0.5726, -0.475 , 0.092 , -0.1566, 1.9074, -1.4261, 1.8589, -0.7534, -1.0767, -0.2704, 0.7567, 0.5903, -1.5612, -1.1097, 0.3504, -0.9086, -0.1691, 0.6714, -0.6033, 1.8315, -0.8141, 0.5968, -0.408 , -1.1843, 0.5146, 0.6201, 0.4293, 0.9797, 1.066 , -1.3325, -1.733 , 0.8545, 0.3993, -0.2041, -0.4624, 0.0272, -0.005 , 0.9237, -0.5523, 0.9975, -0.4374, 0.1351, -0.6148, 0.3185, 0.0572, -0.3002, 0.0889, -0.0894, -0.5617, -2.0553, -0.2923, 0.7227, 0.604 , -0.6623, -0.6126, -0.4991, 0.0923, -0.6982, 0.2099, -0.6853, -0.4752, -1.625 , 0.0443, 2.5507, -1.1597, 0.3504, -0.7654, -1.4366, -1.3755, 0.3702, -1.7853, -0.7326, -1.2803, -0.6089, -0.4472, 0.462 , 0.7799, 0.3141, 0.8064, -1.0487, 0.7317, -0.2446, 0.3061, 0.1384, -0.572 , -0.0311, 0.3572, -0.6371, -0.2236, 0.0806, 0.6648, -0.148 , -0.2547, 1.3649, -0.1595, 1.3632, -0.8858, 1.1801, 0.5533, 2.3306, 0.2724, 0.7073, -0.5605, -0.8849, 0.9533, 0.3683, -0.2901, -0.0453, 0.1064, 1.3342, -0.7036, 0.7127, 1.2156, 0.9017, 1.2378, -1.1017, 1.0558, 1.4273, 0.7003, 1.1649, 0.0334, 0.3433, -0.3997, -0.1195, 1.3725, -0.3746, 0.8444, 0.961 , -0.2644, 0.3245, -1.3583, 0.387 , 1.2944, 0.0274, -0.5057, 0.15 , 0.6 , -0.5752, 0.3746, 1.7114, -0.0026, -0.1221, -0.8084, -0.9521, -0.6332, 0.7254, 1.7032, -0.0879, 0.3329, -1.9525, -0.7083, -0.4113, 1.163 , 0.9018, -0.3667, 0.8419, 0.4417, 0.2904, 0.1666, 1.3722, -0.4455, -1.4876, 0.4103, 2.3672, 0.3569, -0.8546, 0.5152, 0.9623, 1.1777, 1.6789, -1.7793, -0.7797, -1.0923, 0.07 , -0.8974, -0.3151, -0.3675, -1.9851, -2.3352, 0.3566, 1.1929, 1.5275, 1.4349, -1.4742, -0.1913, 1.5874, -0.7264, -0.5594, 0.3166, -0.9377, -0.6452, 0.394 , -0.2238, -1.1239, -0.0324, 1.3866, -0.6174, -0.1301, -0.0328, -0.92 , 1.8067, 0.2576, -0.5248, 0.4114, 0.1655, -0.1674, 0.2743, 0.0835, -0.145 , 1.1658, 1.2624, 0.0404, 2.0929, 0.6047, 1.0317, -0.4956, -1.5666, 1.1729, 0.484 , 0.955 , 1.0546, 0.0106, 0.5062, 0.3211, 0.8503, 0.4706, 1.9953, -0.9362, 0.6326, -0.3154, 1.4987, -0.1695, 1.0906, -0.686 , 0.2501, -0.316 , 0.3032, 0.4873, 0.6402, -0.1209, -0.1857, -0.3707, -0.3082, -0.4769, -0.858 , -0.1521, -0.3403, -0.9853, -0.5049, 0.3338, -0.3197, -0.5789, -0.7124, -0.8867, -0.0228, -1.5519, 1.8517, 0.5229, 0.7613, -0.5586, 0.4827, -1.3011, -0.5284, -0.3806, -0.7719, 1.6304, 0.0375, -0.9122, -0.1006, 0.382 , 0.0969, 1.7784, 0.1831, -1.8866, 0.2996, 0.4778, -0.2491, -1.6537, 0.022 , -0.101 , 0.5912, -0.2249, -1.1422, -0.6436, -1.4096, -0.7446, 0.8055, 1.0727, 0.2426, -0.8079, -1.4692, 0.062 , -0.4466, 0.3786, -2.0461, 0.7238, -1.6195, 1.4005, 0.4881, -0.8161, -0.582 , 0.3456, 1.2922, 0.2469, 1.9035, 0.9072, -0.0729, -0.9424, -1.1129, 0.8922, -0.5628, 1.6215, -0.7022, 0.8395, -0.3423, 0.6048, -0.248 , 0.7411, 0.3546, 0.6176, -0.8221, -0.338 , -2.1051, -1.0049, -0.0659, 0.0917, -0.6661, -0.5234, 0.9574, -0.6316, -0.0047, -0.4773, 0.1562, -0.116 , -1.6255, -0.9108, -1.4767, -0.7765, -1.7101, 0.0557, 0.8112, 0.7382, 1.8806, 0.9239, 1.8638, 0.8426, 0.0359, 0.2743, 1.9204, 1.2223, 0.4575, -0.3408, 0.3727, 0.5036, 0.5392, -1.3331, -0.4008, -0.1341, -1.5197, 0.1923, 0.2128, 1.1533, -1.4284, -0.7483, -0.4092, 1.2843, -0.4489, -0.6624, 0.9255, -0.0895, 0.3199, -0.2564, -0.1166, -1.4701, 1.1799, -1.6238, 0.0508, 0.2312, 0.7322, -1.3623, -0.232 , -0.2206, 0.566 , 1.2411, -1.1563, 1.1777, -1.1481, -0.6716, 0.4596, -0.2422, -0.8654, 0.4441, 0.1869, 1.4626, 0.7621, 0.4249, 0.252 , 0.632 , 0.5626, -0.7925, 1.1995, 1.5665, 0.6096, 0.4821, -0.7324, -0.7624, 1.858 , -0.8434, -0.4408, 0.2011, 0.7552, -0.8955, -1.3255, 0.7022, 0.1507, 0.662 , -1.2229, 0.5199, 0.9837, -0.3947, -0.5262, -1.0424, -1.4582, 0.5126, -0.3606, 0.4427, -2.3922, 1.2784, -0.8382, -0.0198, 1.2136, -0.4212, -0.7798, -1.3387, -0.7141, 0.9581, -0.8575, -0.2255, 0.8436, -2.2162, 0.0742, 0.9683, -0.3633, -0.0227, -1.2176, 1.1482, -0.6697, 0.9643, -1.2802, -0.3651, -1.29 , 0.851 , 1.0167, 1.0011, -1.3014, -0.7205, 1.3621, -0.692 , 1.0637, 0.5637, 0.0851, 2.1514, -0.272 , 0.3136, 0.2179, 0.7035, -1.3028, -0.1032, 0.0611, 1.2002, -0.7346, 0.9991, -0.3747, 0.7908, -0.9573, -0.5114, -0.8607, -0.6711, 1.3335, -0.6671, -0.1687, 0.4601, 0.5747, -0.0767, -0.8428, 0.3372, -1.7756, -2.5264, -1.503 , -0.5669, 0.0167, -1.961 , 0.8861, 1.1902, 2.239 , 0.2481, 0.7361, -1.1103, 0.8368, -1.0434, 0.6809, -0.0839, -0.6972, -1.5492, -1.4129, 0.5889, 0.2138, 1.7689, -0.4861, -0.1124, 0.2032, 1.0664, -0.369 , 2.3793, 0.4406, -1.1741, 1.0812, 1.3965, -0.149 , 0.8793, 1.3494, 1.2159, -0.0001, 1.1929, -0.1966, -0.1666, 1.7097, -0.4273, 0.4831, -0.2411, -1.4517, -0.7317, 0.099 , 1.7922, 0.2313, -0.5031, -0.0849, 0.7331, -0.1483, -0.8003, 1.1897, 0.031 , -0.3624, -1.1133, 1.4647, 2.5653, -1.9536, -0.4528, -1.693 , 0.4847, 0.1368, 0.6859, -0.9872, 0.8425, -0.1492, -0.1335, -0.0229, -0.0903, -0.4381, 1.2552, 1.5763, 0.2375, -0.7597, 0.0845, 0.0894, -1.6022, -0.1988, 0.3095, -1.0785, -1.6044, -0.4922, 0.4583, 0.3168, -2.0485, -1.2147, -0.2803, -0.2071, 0.0767, 1.9544, -1.7648, 0.2873, -0.4029, -0.8128, -0.1081, 0.0332, 2.5288, 0.9933, 0.4378, -0.8208, -0.2451, 0.3472, -0.2917, 2.0775, 1.7381, -0.467 , -0.8943, -1.4171, -0.3905, 0.2591, 0.8118, -0.643 , 1.0387, 0.0049, 1.7299, -0.6882, -1.4132, -1.0893, 0.4606, -1.546 , -2.87 , 0.3492, -1.5968, 0.9858, 0.1384, -0.6016, -0.9632, -0.9088, 0.3711, 1.3509, 0.4601, -1.4963, -0.043 , 0.5588, 0.2638, -1.1118, -0.9376, -0.9139, 0.6551, 0.4876, -1.7039, -0.2915, 0.3867, -0.1795, 1.2298, 0.0893, -0.6019, 1.4109, -1.1918, 0.5009, 0.0157, -1.1307, 1.0407, 1.9742, -1.0377, -0.6151, -0.8398, 1.4096, -0.012 , -1.5323, 0.3323, 0.0539, 0.2383, -0.4059, 2.285 , 0.1536, 0.3838, 0.3623, -0.4326, -0.0975, -1.8119]) array([-2.9433, -2.87 , -2.6518, -2.5489, -2.5465, -2.5264, -2.4542, -2.3922, -2.3352, -2.3038, -2.2774, -2.2574, -2.2444, -2.2162, -2.1117, -2.1051, -2.0724, -2.0553, -2.0485, -2.0461, -1.9851, -1.961 , -1.9536, -1.9525, -1.8866, -1.8119, -1.7853, -1.7793, -1.7756, -1.769 , -1.7648, -1.733 , -1.7101, -1.7039, -1.693 , -1.669 , -1.6537, -1.6255, -1.625 , -1.6248, -1.6238, -1.6195, -1.6097, -1.6044, -1.6022, -1.5968, -1.5895, -1.5824, -1.5666, -1.5612, -1.5519, -1.5492, -1.546 , -1.5415, -1.5392, -1.5323, -1.5263, -1.5197, -1.503 , -1.4963, -1.4891, -1.4876, -1.4767, -1.4742, -1.4701, -1.4692, -1.4582, -1.4517, -1.4366, -1.4291, -1.4284, -1.4261, -1.4171, -1.4132, -1.4129, -1.4129, -1.4096, -1.3994, -1.3755, -1.3655, -1.3623, -1.3583, -1.3387, -1.3331, -1.3325, -1.3255, -1.3191, -1.3153, -1.3028, -1.3014, -1.3011, -1.2912, -1.29 , -1.2803, -1.2802, -1.2598, -1.2527, -1.2472, -1.2467, -1.2456, -1.2365, -1.2229, -1.2176, -1.2147, -1.1918, -1.1843, -1.1799, -1.1741, -1.1597, -1.1563, -1.1551, -1.1481, -1.1422, -1.1307, -1.1239, -1.1234, -1.118 , -1.1133, -1.1129, -1.1118, -1.1103, -1.1097, -1.1087, -1.1017, -1.0923, -1.0893, -1.0831, -1.0785, -1.0767, -1.0763, -1.0699, -1.069 , -1.0487, -1.0434, -1.0424, -1.0397, -1.0377, -1.0318, -1.0263, -1.017 , -1.0152, -1.0049, -1.0032, -0.9944, -0.9872, -0.9853, -0.9816, -0.9632, -0.9632, -0.9573, -0.9521, -0.9516, -0.9424, -0.9419, -0.9377, -0.9376, -0.9371, -0.9362, -0.9343, -0.92 , -0.9139, -0.9122, -0.9111, -0.9108, -0.9106, -0.9088, -0.9086, -0.8974, -0.8955, -0.8943, -0.8867, -0.8858, -0.8849, -0.8654, -0.8632, -0.8607, -0.858 , -0.8575, -0.8546, -0.849 , -0.8483, -0.8434, -0.8428, -0.8398, -0.8382, -0.8357, -0.8324, -0.8303, -0.827 , -0.8221, -0.821 , -0.8208, -0.817 , -0.8161, -0.8141, -0.8128, -0.8084, -0.8079, -0.8003, -0.7962, -0.7925, -0.7835, -0.7802, -0.7798, -0.7797, -0.7793, -0.7765, -0.7762, -0.7719, -0.7654, -0.7638, -0.7624, -0.7597, -0.7596, -0.7579, -0.7534, -0.7487, -0.7483, -0.7446, -0.7348, -0.7346, -0.7326, -0.7324, -0.7317, -0.7277, -0.7264, -0.7215, -0.7205, -0.7141, -0.7125, -0.7124, -0.7083, -0.7036, -0.7022, -0.6982, -0.6972, -0.6966, -0.692 , -0.6914, -0.6882, -0.6877, -0.686 , -0.6853, -0.6716, -0.6711, -0.6705, -0.6697, -0.6671, -0.6661, -0.6624, -0.6623, -0.6596, -0.6568, -0.6489, -0.6452, -0.6443, -0.6437, -0.6436, -0.643 , -0.641 , -0.6407, -0.6371, -0.6332, -0.6316, -0.6259, -0.6174, -0.6151, -0.6148, -0.6126, -0.6089, -0.6051, -0.605 , -0.6033, -0.6019, -0.6016, -0.601 , -0.5972, -0.582 , -0.5794, -0.5789, -0.5752, -0.5742, -0.5726, -0.572 , -0.5669, -0.5628, -0.5617, -0.5605, -0.5594, -0.5586, -0.5557, -0.5523, -0.5389, -0.5329, -0.5284, -0.5262, -0.5258, -0.5248, -0.5234, -0.5131, -0.5114, -0.5057, -0.5049, -0.5031, -0.5009, -0.4991, -0.4956, -0.4922, -0.4861, -0.4813, -0.4808, -0.4773, -0.4769, -0.4752, -0.475 , -0.4731, -0.4703, -0.467 , -0.4652, -0.4624, -0.4528, -0.452 , -0.4489, -0.4472, -0.4466, -0.4455, -0.4435, -0.4431, -0.4423, -0.4408, -0.4381, -0.4374, -0.4326, -0.4301, -0.4294, -0.4273, -0.4264, -0.4212, -0.4143, -0.4138, -0.4113, -0.4092, -0.408 , -0.4059, -0.4038, -0.4029, -0.4008, -0.3997, -0.3984, -0.3966, -0.3947, -0.3927, -0.3905, -0.388 , -0.3806, -0.3774, -0.3747, -0.3746, -0.3734, -0.3707, -0.369 , -0.3686, -0.3675, -0.3667, -0.3656, -0.3655, -0.3651, -0.3633, -0.3624, -0.3606, -0.3542, -0.354 , -0.3423, -0.3408, -0.3403, -0.3397, -0.338 , -0.3253, -0.3223, -0.3197, -0.316 , -0.3154, -0.3151, -0.3144, -0.3082, -0.3036, -0.3002, -0.2971, -0.2923, -0.2917, -0.2915, -0.2901, -0.2828, -0.2803, -0.272 , -0.2704, -0.2684, -0.2644, -0.2564, -0.2547, -0.2497, -0.2491, -0.248 , -0.2451, -0.2447, -0.2446, -0.2422, -0.2411, -0.2381, -0.2339, -0.232 , -0.2314, -0.2298, -0.2255, -0.2249, -0.2238, -0.2236, -0.2206, -0.211 , -0.2074, -0.2071, -0.2065, -0.2058, -0.2041, -0.1988, -0.1966, -0.1935, -0.1913, -0.1857, -0.1795, -0.1792, -0.1695, -0.1691, -0.1687, -0.1674, -0.1666, -0.1602, -0.1595, -0.1566, -0.1526, -0.1521, -0.1508, -0.1492, -0.149 , -0.1483, -0.148 , -0.146 , -0.145 , -0.1341, -0.1335, -0.131 , -0.1301, -0.1247, -0.1221, -0.1209, -0.1195, -0.1166, -0.116 , -0.1126, -0.1124, -0.1081, -0.1036, -0.1032, -0.101 , -0.1006, -0.0975, -0.0903, -0.0895, -0.0894, -0.0879, -0.0849, -0.0839, -0.078 , -0.0767, -0.0729, -0.0675, -0.0659, -0.0588, -0.0453, -0.0453, -0.043 , -0.0328, -0.0324, -0.0311, -0.0244, -0.0229, -0.0228, -0.0227, -0.0213, -0.0198, -0.0171, -0.0136, -0.0129, -0.012 , -0.005 , -0.0047, -0.0026, -0.0001, 0.0049, 0.0106, 0.0157, 0.0167, 0.0216, 0.022 , 0.0251, 0.0263, 0.0272, 0.0274, 0.0285, 0.0301, 0.031 , 0.0332, 0.0334, 0.0359, 0.0375, 0.0375, 0.0404, 0.0419, 0.0443, 0.0466, 0.0479, 0.049 , 0.0508, 0.0539, 0.0557, 0.0572, 0.0601, 0.0611, 0.062 , 0.07 , 0.0742, 0.0756, 0.0767, 0.0784, 0.0801, 0.0806, 0.0835, 0.0845, 0.0851, 0.0882, 0.0889, 0.0893, 0.0894, 0.0917, 0.092 , 0.0923, 0.0943, 0.0969, 0.099 , 0.1016, 0.106 , 0.1064, 0.1351, 0.1364, 0.1368, 0.1384, 0.1384, 0.1396, 0.15 , 0.1507, 0.1536, 0.1552, 0.1562, 0.1655, 0.1666, 0.1704, 0.1778, 0.1831, 0.1859, 0.1862, 0.1865, 0.1869, 0.188 , 0.1923, 0.1958, 0.1973, 0.2011, 0.2032, 0.2099, 0.2128, 0.2132, 0.2138, 0.2179, 0.2254, 0.2312, 0.2313, 0.2375, 0.2383, 0.2426, 0.2447, 0.2469, 0.2481, 0.2501, 0.252 , 0.2536, 0.2576, 0.2591, 0.2621, 0.2638, 0.2724, 0.2743, 0.2743, 0.2748, 0.2873, 0.2882, 0.2904, 0.2913, 0.2963, 0.2996, 0.3032, 0.3061, 0.3086, 0.3095, 0.3136, 0.3141, 0.3159, 0.3166, 0.3168, 0.3185, 0.3199, 0.3211, 0.3218, 0.3245, 0.3272, 0.3323, 0.3329, 0.3335, 0.3338, 0.3372, 0.3433, 0.3456, 0.3472, 0.3492, 0.3504, 0.3504, 0.3524, 0.3528, 0.3546, 0.3566, 0.3569, 0.3572, 0.3623, 0.3624, 0.3683, 0.3702, 0.3711, 0.3711, 0.3727, 0.3746, 0.3767, 0.3786, 0.3794, 0.3803, 0.381 , 0.382 , 0.3838, 0.3867, 0.387 , 0.3934, 0.394 , 0.3993, 0.3995, 0.4023, 0.4037, 0.407 , 0.4103, 0.4114, 0.4175, 0.4241, 0.4249, 0.4293, 0.4378, 0.4386, 0.4406, 0.4417, 0.4421, 0.4427, 0.4441, 0.4575, 0.4583, 0.4596, 0.4601, 0.4601, 0.4606, 0.4607, 0.462 , 0.4657, 0.4706, 0.4738, 0.4778, 0.4778, 0.4821, 0.4827, 0.4831, 0.484 , 0.4847, 0.4873, 0.4876, 0.4881, 0.4925, 0.4965, 0.5009, 0.5033, 0.5036, 0.5062, 0.5109, 0.5119, 0.5126, 0.5146, 0.5152, 0.5199, 0.5229, 0.5284, 0.5347, 0.5358, 0.5392, 0.5436, 0.5533, 0.5588, 0.5597, 0.5626, 0.5637, 0.566 , 0.5675, 0.5736, 0.5747, 0.5889, 0.5903, 0.5912, 0.5968, 0.5969, 0.6 , 0.604 , 0.6047, 0.6048, 0.6096, 0.6128, 0.6138, 0.6176, 0.6201, 0.632 , 0.6326, 0.6351, 0.6402, 0.6449, 0.6464, 0.6495, 0.6551, 0.6615, 0.662 , 0.662 , 0.6648, 0.6699, 0.6714, 0.673 , 0.6739, 0.6809, 0.6831, 0.6856, 0.6859, 0.6867, 0.6883, 0.7003, 0.7022, 0.7035, 0.7073, 0.7127, 0.7227, 0.7238, 0.7248, 0.7254, 0.7317, 0.7322, 0.7331, 0.7361, 0.7382, 0.7411, 0.7475, 0.7496, 0.7552, 0.7567, 0.7613, 0.7621, 0.7667, 0.767 , 0.7695, 0.7751, 0.7799, 0.7908, 0.7921, 0.8043, 0.8055, 0.8064, 0.8112, 0.8118, 0.8266, 0.8327, 0.8368, 0.8395, 0.8411, 0.8419, 0.8425, 0.8426, 0.8436, 0.8442, 0.8444, 0.8503, 0.851 , 0.8545, 0.8634, 0.8743, 0.8744, 0.8793, 0.8858, 0.8861, 0.8922, 0.9017, 0.9018, 0.9072, 0.9073, 0.9237, 0.9239, 0.9255, 0.927 , 0.9362, 0.9533, 0.955 , 0.9574, 0.9581, 0.961 , 0.9623, 0.9643, 0.966 , 0.9683, 0.9797, 0.9837, 0.9858, 0.9918, 0.9933, 0.9949, 0.9975, 0.9991, 1.0011, 1.0126, 1.0167, 1.0317, 1.0387, 1.0407, 1.0539, 1.0546, 1.0558, 1.0567, 1.0637, 1.066 , 1.0664, 1.067 , 1.0727, 1.0735, 1.076 , 1.0767, 1.0812, 1.0844, 1.0906, 1.0977, 1.0998, 1.1103, 1.1161, 1.122 , 1.1334, 1.1351, 1.1366, 1.1482, 1.1533, 1.1545, 1.1567, 1.163 , 1.1632, 1.1649, 1.1653, 1.1658, 1.1729, 1.1777, 1.1777, 1.1799, 1.1801, 1.1837, 1.1864, 1.1897, 1.1902, 1.1929, 1.1929, 1.1995, 1.2002, 1.2098, 1.2136, 1.2156, 1.2159, 1.2223, 1.2296, 1.2298, 1.2378, 1.2411, 1.253 , 1.2544, 1.2552, 1.2624, 1.2784, 1.2797, 1.2843, 1.2922, 1.2944, 1.2946, 1.3289, 1.3335, 1.3342, 1.3388, 1.3494, 1.3509, 1.3621, 1.3632, 1.3649, 1.3722, 1.3725, 1.3866, 1.3965, 1.3996, 1.4005, 1.4047, 1.4096, 1.4109, 1.421 , 1.4273, 1.4349, 1.4391, 1.4626, 1.4647, 1.4797, 1.4848, 1.497 , 1.4987, 1.5228, 1.5275, 1.5298, 1.5449, 1.5665, 1.5763, 1.5874, 1.5924, 1.6091, 1.6215, 1.6304, 1.6461, 1.6789, 1.6945, 1.7032, 1.7097, 1.7106, 1.7114, 1.7299, 1.7323, 1.7381, 1.7689, 1.7784, 1.7922, 1.7923, 1.8067, 1.8315, 1.8415, 1.8517, 1.858 , 1.8589, 1.8638, 1.8738, 1.8806, 1.9035, 1.9074, 1.9204, 1.9294, 1.9456, 1.9493, 1.9544, 1.9551, 1.9742, 1.9928, 1.9953, 2.0775, 2.0929, 2.1223, 2.1514, 2.239 , 2.285 , 2.3306, 2.3672, 2.3793, 2.5288, 2.5507, 2.5653, 2.5671, 2.6081, 2.697 ]) -1.5519406239259821 唯一化以及其他的集合逻辑1234names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])np.unique(names)ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])np.unique(ints) array([&#39;Bob&#39;, &#39;Joe&#39;, &#39;Will&#39;], dtype=&#39;&lt;U4&#39;) array([1, 2, 3, 4]) 1sorted(set(names)) [&#39;Bob&#39;, &#39;Joe&#39;, &#39;Will&#39;] in1d感觉挺有用 12values = np.array([6, 0, 0, 3, 2, 5, 6])np.in1d(values, [2, 3, 6]) array([ True, False, False, True, True, False, True], dtype=bool) 用于数组的文件输入输出将数组以二进制的形式保存到磁盘12arr = np.arange(10)np.save('some_array', arr) 1np.load('some_array.npy') array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 压缩存储，并且可以存储多个 1np.savez('array_archive.npz', a=arr[:4], b=arr) 123arch = np.load('array_archive.npz')arch['a']arch['b'] array([0, 1, 2, 3]) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 存取文本文件1!more ch04\array_ex.txt 0.580052,0.186730,1.040717,1.134411 0.194163,-0.636917,-0.938659,0.124094 -0.126410,0.268607,-0.695724,0.047428 -1.484413,0.004176,-0.744203,0.005487 2.302869,0.200131,1.670238,-1.881090 -0.193230,1.047233,0.482803,0.960334 12arr = np.loadtxt('.\\ch04\\array_ex.txt', delimiter=',')arr array([[ 0.5801, 0.1867, 1.0407, 1.1344], [ 0.1942, -0.6369, -0.9387, 0.1241], [-0.1264, 0.2686, -0.6957, 0.0474], [-1.4844, 0.0042, -0.7442, 0.0055], [ 2.3029, 0.2001, 1.6702, -1.8811], [-0.1932, 1.0472, 0.4828, 0.9603]]) 线性代数12345x = np.array([[1., 2., 3.], [4., 5., 6.]])y = np.array([[6., 23.], [-1, 7], [8, 9]])xyx.dot(y) # equivalently np.dot(x, y) array([[ 1., 2., 3.], [ 4., 5., 6.]]) array([[ 6., 23.], [ -1., 7.], [ 8., 9.]]) array([[ 28., 64.], [ 67., 181.]]) 1np.dot(x, np.ones(3)) array([ 6., 15.]) 1np.random.seed(12345) 123456789from numpy.linalg import inv, qrX = randn(5, 5)Xmat = X.T.dot(X)matinv(mat)mat.dot(inv(mat))q, r = qr(mat) # QR分解r array([[-0.5031, -0.6223, -0.9212, -0.7262, 0.2229], [ 0.0513, -1.1577, 0.8167, 0.4336, 1.0107], [ 1.8249, -0.9975, 0.8506, -0.1316, 0.9124], [ 0.1882, 2.1695, -0.1149, 2.0037, 0.0296], [ 0.7953, 0.1181, -0.7485, 0.585 , 0.1527]]) array([[ 4.2538, -1.0645, 1.4407, 0.9898, 1.7318], [-1.0645, 7.4431, -1.5585, 4.4972, -2.1367], [ 1.4407, -1.5585, 2.8126, 0.243 , 1.2786], [ 0.9898, 4.4972, 0.243 , 5.0897, 0.305 ], [ 1.7318, -2.1367, 1.2786, 0.305 , 1.928 ]]) array([[ 0.4057, -0.1875, -0.0764, 0.1229, -0.541 ], [-0.1875, 2.462 , 0.2537, -2.3367, 3.0984], [-0.0764, 0.2537, 0.5435, -0.2369, 0.0268], [ 0.1229, -2.3367, -0.2369, 2.4239, -2.9264], [-0.541 , 3.0984, 0.0268, -2.9264, 4.8837]]) array([[ 1., 0., -0., -0., -0.], [ 0., 1., -0., -0., -0.], [ 0., 0., 1., 0., -0.], [ 0., -0., 0., 1., 0.], [ 0., 0., -0., 0., 1.]]) array([[-5.0281, 2.7734, -2.8428, -1.0619, -3.0078], [ 0. , -8.7212, 1.2925, -6.5614, 1.622 ], [ 0. , 0. , -2.0873, -1.0487, -0.6291], [ 0. , 0. , 0. , -1.408 , -0.955 ], [ 0. , 0. , 0. , 0. , 0.1537]]) 随机数生成12samples = np.random.normal(size=(4, 4))samples array([[-0.5196, 1.297 , 0.9062, 0.5809], [ 1.2233, -1.3301, 1.0483, 0.357 ], [-0.7935, -0.406 , -0.0096, -0.596 ], [ 1.3833, -0.2029, -1.0547, -0.9795]]) 1234from random import normalvariateN = 1000000%timeit samples = [normalvariate(0, 1) for _ in range(N)]%timeit np.random.normal(size=N) 1 loop, best of 3: 814 ms per loop 10 loops, best of 3: 28.4 ms per loop 可以看出numpy确实要快很多 Example: 随机游走123456789import randomposition = 0walk = [position]steps = 1000for i in range(steps): step = 1 if random.randint(0, 1) else -1 position += step walk.append(position) 通过numpy来实现上述过程 1np.random.seed(12345) 1234nsteps = 1000draws = np.random.randint(0, 2, size=nsteps)steps = np.where(draws &gt; 0, 1, -1)walk = steps.cumsum() 1234import matplotlib.pyplot as pltindex = [x + 1 for x in range(len(walk))]plt.plot(index, walk) [&lt;matplotlib.lines.Line2D at 0x234014e9f98&gt;] 12walk.min()walk.max() -3 31 1(np.abs(walk) &gt;= 10).argmax() # the first index 37 一次模拟多次随机漫步123456nwalks = 5000 # 5000 random walknsteps = 1000draws = np.random.randint(0, 2, size=(nwalks, nsteps)) # 0 or 1steps = np.where(draws &gt; 0, 1, -1)walks = steps.cumsum(1)walks array([[ -1, 0, -1, ..., 24, 23, 22], [ -1, 0, -1, ..., -36, -37, -36], [ 1, 2, 3, ..., -42, -41, -40], ..., [ 1, 0, -1, ..., 48, 49, 50], [ -1, -2, -3, ..., -38, -39, -40], [ -1, 0, 1, ..., -48, -47, -48]], dtype=int32) 12walks.max()walks.min() 130 -117 123hits30 = (np.abs(walks) &gt;= 30).any(1)hits30hits30.sum() # Number that hit 30 or -30 array([ True, True, True, ..., True, True, True], dtype=bool) 3412 12crossing_times = (np.abs(walks[hits30]) &gt;= 30).argmax(1)crossing_times.mean() 497.04103165298943]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Naive Bayes]]></title>
      <url>%2F2017%2F02%2F25%2FNaive-Bayes%2F</url>
      <content type="text"><![CDATA[利用朴素贝叶斯进行文本分类准备数据从文本中构建词向量 下面写一个词表到向量的转换函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def LoadDataSet(): """ Load a vector-like data set tranfered by a data set list that generated by artifical. Returns: return_vec: the vector-like data set. class_vec: the class label corresponds to the data items. """ posting_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] class_vec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not return posting_list, class_vecdef CreateVocabList(data_set): """ Create vocabulary list from vector-like data set. Arguments: data_set: the data source. Returns: vocab_list: the vocabulary list. """ vocab_set = set([]) for document in data_set: vocab_set = vocab_set | set(document) return list(vocab_set)def SetOfWords2Vec(vocab_list, input_set): """ Transfer the words list to vector for each posting. Arguments: vocab_list: The vocabulary list. input_set: The posting that ready to transfer to vector. Returns: return_vec: the result vector. """ # Initialize return_vec = [0] * len(vocab_list) for word in input_set: if word in vocab_list: return_vec[vocab_list.index(word)] = 1 else: print("the word: %s is not in my Vocabulary" % word) return return_vec 下面对函数的功能进行测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110In [13]: import bayesIn [14]: list_of_posts, list_classes = bayes.LoadDataSet()In [15]: my_vocab_list = bayes.CreateVocabList(list_of_posts)In [16]: my_vocab_listOut[16]:['help', 'worthless', 'I', 'take', 'love', 'maybe', 'stupid', 'to', 'not', 'please', 'quit', 'park', 'posting', 'dog', 'dalmation', 'steak', 'my', 'how', 'food', 'so', 'stop', 'is', 'garbage', 'flea', 'problems', 'has', 'buying', 'ate', 'him', 'licks', 'mr', 'cute']In [17]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[0])Out[17]:[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]In [18]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[3])Out[18]:[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] 看上去一切都work，可以进入下一步了。 训练从词向量计算概率 我们来看看贝叶斯公式： p(c_i | w) = \frac{p(w | c_i) p(c_i)}{p(w)}这里$w$代表词向量， 可以看出$c_i$的计算十分简单，值得注意的是，根据朴素贝叶斯的假设，有： p(w | c_i) = p(w_0, w_1, w_2, \cdots, w_N | c_i) = p(w_0 | c_i)p(w_1 | c_i)p(w_2 | c_i) \cdots p(w_N | c_i)当需要预测新样本的类别时： 这样一切就很清楚了，下面给出伪代码： 12345678910计算每个类别的文档数目for 每一篇文档： for 每一个类别： if 词条出现在文档中： 增加该词条的计数值 增加总词条数的计数值 for 每一个类别： for 每一个词条： 将该词条的数目除以总词条数目得到条件概率 返回每个类别的条件概率 注意，这里的$p(w_j | c_i)$是要根据整个训练集来算，代码实现如下： 123456789101112131415161718192021222324252627282930313233def TrainNaiveBayes0(train_matrix, train_category): """ the training method. Arguments: train_matrix: The train data. train_category: The train label. Returns: p0_vect: The conditional probability of w by c0 p1_vect: The conditional probability of w by c1 p_abusive: The conditional probability of c1 """ num_train_docs = len(train_matrix) num_words = len(train_matrix[0]) p_abusive = sum(train_category) / num_train_docs p0_num = zeros(num_words) p1_num = zeros(num_words) p0_denom = 0.0 p1_denom = 0.0 for i in range(num_train_docs): if train_category[i] == 1: p1_num += train_matrix[i] p1_denom += sum(train_matrix[i]) else: p0_num += train_matrix[i] p0_denom += sum(train_matrix[i]) p1_vect = p1_num / p1_denom p0_vect = p0_num / p0_denom return p0_vect, p1_vect, p_abusive 同样进行一下测试： 123456789101112131415161718192021222324252627282930In [25]: train_mat = []In [26]: for post_in_doc in list_of_posts: ...: train_mat.append(bayes.SetOfWords2Vec(my_vocab_list, post_in_doc)) ...:In [27]: p0_v, p1_v, p_ab = bayes.TrainNaiveBayes0(train_mat, list_classes)In [28]: p_abOut[28]: 0.5In [29]: p0_vOut[29]:array([ 0.04166667, 0. , 0.04166667, 0. , 0.04166667, 0. , 0. , 0.04166667, 0. , 0.04166667, 0. , 0. , 0. , 0.04166667, 0.04166667, 0.04166667, 0.125 , 0.04166667, 0. , 0.04166667, 0.04166667, 0.04166667, 0. , 0.04166667, 0.04166667, 0.04166667, 0. , 0.04166667, 0.08333333, 0.04166667, 0.04166667, 0.04166667])In [30]: p1_vOut[30]:array([ 0. , 0.10526316, 0. , 0.05263158, 0. , 0.05263158, 0.15789474, 0.05263158, 0.05263158, 0. , 0.05263158, 0.05263158, 0.05263158, 0.10526316, 0. , 0. , 0. , 0. , 0.05263158, 0. , 0.05263158, 0. , 0.05263158, 0. , 0. , 0. , 0.05263158, 0. , 0.05263158, 0. , 0. , 0. ]) 但是上述代码存在一些缺陷，首先，计算$p(w_j | c_i)$可能会出现结果为0的情况，那么最后的结果就会为0，那么需要进行一些修改 (为什么是2？)： 1234p0_num = ones(num_words)p1_num = ones(num_words)p0_denom = 2.0p1_denom = 2.0 另外一个就是下溢的问题，所以要改用log函数 12p1_vect = log(p1_num / p1_denom)p0_vect = log(p0_num / p0_denom) 测试最后写一个分类和测试函数： 123456789101112131415161718192021222324252627282930313233343536373839404142def ClassifyNaiveBayes(vec_to_classify, p0_vect, p1_vect, p_abusive): """ Classify. Arguments: vec_to_classify: The vector to classify. p0_vect: The conditional probability of w by c0. p1_vect: The conditional probability of w by c1. p_abusive: The conditional probability of c1. Returns: 0: The predict class is 0 1: The predict class is 1 """ p1 = sum(vec_to_classify * p1_vect) + log(p_abusive) p0 = sum(vec_to_classify * p0_vect) + log(1 - p_abusive) if p1 &gt; p0: return 1 else: return 0def TestNaiveBayes(): """ A test method. """ list_of_posts, list_of_classes = LoadDataSet() my_vocab_list = CreateVocabList(list_of_posts) train_mat = [] for post_in_doc in list_of_posts: train_mat.append(SetOfWords2Vec(my_vocab_list, post_in_doc)) p0_v, p1_v, p_ab = TrainNaiveBayes0( array(train_mat), array(list_of_classes)) test_entry = ['love', 'my', 'dalmation'] this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry)) print(test_entry, 'classified as: ', ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab)) test_entry = ['stupid', 'garbage'] this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry)) print(test_entry, 'classified as: ', ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab)) Test 123In [32]: bayes.TestNaiveBayes()['love', 'my', 'dalmation'] classified as: 0['stupid', 'garbage'] classified as: 1 Ok, bravo! 词袋模型现在有一个问题， 到目前为止，我们将每个词是否出现作为特征，这被称为词集模型。但是如果有一个词在文档中不止出现一次，那么就需要词袋模型进行建模。 1234567891011121314151617181920def BagOfWords2Vec(vocab_list, input_set): """ Transfer the words list to vector for each posting. Arguments: vocab_list: The vocabulary list. input_set: The posting that ready to transfer to vector. Returns: return_vec: the result vector. """ # Initialize return_vec = [0] * len(vocab_list) for word in input_set: if word in vocab_list: return_vec[vocab_list.index(word)] += 1 else: print("the word: %s is not in my Vocabulary" % word) return return_vec 高斯朴素贝叶斯一般的朴素贝叶斯算法的输入特征为离散值，那么当输入变量为连续值时就不能处理了，一般这时候假设输入变量服从一个正态分布，这样$p(w_j | c_i)$就可以计算了，所以整个的流程如下： 采用sk-learn进行下实验 123456789In [33]: from sklearn import datasets ...: iris = datasets.load_iris() ...: from sklearn.naive_bayes import GaussianNB ...: gnb = GaussianNB() ...: y_pred = gnb.fit(iris.data, iris.target).predict(iris.data) ...: print("Number of mislabeled points out of a total %d points : %d" ...: % (iris.data.shape[0],(iris.target != y_pred).sum())) ...:Number of mislabeled points out of a total 150 points : 6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS231n Lecture2 note]]></title>
      <url>%2F2017%2F02%2F24%2FCS231n-Lecture2-note%2F</url>
      <content type="text"><![CDATA[图像分类目标给一张输入图片赋予一个标签， 这个标签属于事先定义好的类别集合中 地位计算机视觉的核心问题 例子 难点 拍摄点的视角多样 拍摄点的远近距离多样 物体的变形 物体部分遮挡 光线 背景相似 品种多样 方法数据驱动（即包含训练数据） pipeline输入 -&gt; 学习 -&gt; 评估 最近邻分类器 距离度量L1 distanced_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right| L2 distanced_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}示例代码数据读取1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide# flatten out all images to be one-dimensionalXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072 预测及评估123456nn = NearestNeighbor() # create a Nearest Neighbor classifier classnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labelsYte_predict = nn.predict(Xte_rows) # predict labels on the test images# and now print the classification accuracy, which is the average number# of examples that are correctly predicted (i.e. label matches)print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) ) 基本方法123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred 实验结果L1-distance 38.6% on CIFAR-10 L2-distance 35.4% on CIFAR-10 L1 vs. L2 L2比L1的差异容忍度更小 K近邻分类器 采用验证集进行超参调参实例代码123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc)) 交叉验证 最近邻分类器的优缺点优点训练速度快 缺点 测试速度很慢 ​ 解决方法：1. ANN 2. FANN 距离度量不合适 ​ (http://cs231n.github.io/assets/pixels_embed_cifar10_big.jpg) 接下来…t-SNE http://lvdmaaten.github.io/tsne/ random projection http://scikit-learn.org/stable/modules/random_projection.html INTUITION FAILS IN HIGH DIMENSIONS http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf Recognizing and Learning Object Categories http://people.csail.mit.edu/torralba/shortCourseRLOC/index.html 线性分类一个从图像到标签的映射函数f(x_i, W, b) = W x_i + bx shape is [D x 1], W shape is [K x D], b shape is [K x 1] 注意点： W代表K个分类器的参数放在一起，因此整个模型是K个分类器的一个整合 向量化能够大大提升计算速度 线性分类器的解释 权重W表示不同的标签对于图像不同位置不同颜色的重视程度。比如太阳可能对于圆形的区域以及黄颜色比较看重 将图像看成高维空间中的点 将线性分类器看成模板匹配 将W的每一行看成一个模板，通过内积计算，每一张图片张成的列向量都与每一个模板作比较，最后选出最匹配的，这也是一种最近邻算法。 从上图可以看出，这里的模板是各个图像的一种折中。 将bias项放入W中 数据预处理​ 数据中心化]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Qiniu cloud images batch upload and directory synchronization]]></title>
      <url>%2F2017%2F02%2F24%2FQiniu-cloud-images-batch-upload-and-directory-synchronization%2F</url>
      <content type="text"><![CDATA[最近写博客的时候会用到图片，因此用了七牛云的图片外链功能，但是其内容管理不能创建目录，所以图片的命名以及上传都很麻烦，然后去网上查了一下，也看了一下官方文档，发现官方有一个批量上传的工具挺好用，所以记录一下，下面把官方文档中的一些东西贴出来，方便日后查阅。 ​]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Correlation Analysis]]></title>
      <url>%2F2017%2F02%2F24%2FCorrelation-Analysis%2F</url>
      <content type="text"><![CDATA[问题描述最近有一个关于关联分析的小作业，问题描述如下： 孩子的情感状况、品行症状、亲社会行为等等与很多因素相关，这个作业主要着眼点在于孩子每天看视频的时间是否对于以上这些指标有着重要的影响，因此需要对输入特征与输出标签之间的相关关系做一个分析。 属性描述如下： 由以上属性描述表可以看出，属性类型同时包括类别以及数值，因此进行相关分析时针对这两种情况需要采用不同的方法，这里采用的策略如下： 对于离散-离散的情况，采用卡方检验 对于连续-离散的情况，采用ANOVA方差检验 下面对于这两种检验方法进行介绍。 卡方检验一维情况假设一条河里有三种鱼，gumpies, sticklebarbs, 以及spotheads。如果这条河的生态环境没有遭到干扰，那么这三种鱼的数量是相等的（下表第三行）。现在从河里进行300次抽样，最后抽样得到的结果如下表第二行所示： gumpies sticklebarbs spothheads Totals 观察到的频数 89 120 91 300 期望的频数 100 100 100 300 现在需要解决的问题是，是否这条河的生态环境收到了干扰？我们假设生态环境正常，这是我们的原假设。 （注意，期望的频数是根据实际情况自行定义的） 很容易想到的是，我们可以建立一种度量来衡量现实情况与原假设的偏离程度，例如： \frac{观察值 - 期望值}{期望值}把实际的数据带入，可以得到以下的结果： gumpies: $\frac{89 - 100}{100} = -0.11$ sticklebarbs: $\frac{120 - 100}{100} = +0.20$ spothheads: $\frac{91 - 100}{100} = -0.09$ 发现结果还不错，但是这只能用来衡量单个类别的偏差程度，而不能用来衡量整体的偏差程度，因为这三者的加和为零。既然这样，很容易想到可以对之前的度量进行简单的修改，变成这样： \frac{(观察值 - 期望值)^2}{期望值}再把数据带入看看： gumpies: $\frac{(89 - 100)^2}{100} = 1.21$ sticklebarbs: $\frac{(120 - 100)^2}{100} = 4.0$ spothheads: $\frac{(91 - 100)}{100} = 0.81$ sum: $1.21 + 4.0 + 0.81 = 6.02$ 这样一来问题便得到了解决，而这也正是卡方检验所采取的方式。而这个sum值就是卡方（chi-square），记作$\chi^2$， 为了更加形式化地表示，我们把观察值记为$O$， 期望值记为$E$， 那么有如下等式成立： \chi^2 = \sum \frac{(O - E) ^ 2}{E}下面的问题是，我们达到了卡方值，但是这个值到底好还是不好呢？是更加支持原假设还是拒绝原假设呢？ 可以设想这样一种情况，我们假设河里的鱼服从原假设的分布，也就是三种鱼出现的概率相等。然后我们把三百次采样看作一次实验，每一次实验完毕之后记录下采样出的300条鱼中每一种鱼的频数，然后计算卡方值。在进行很多次这样的实验之后，我们可以画一个柱状图，这个图记录下了卡方值的分布情况。然后再把实际的观察值（也就是上面表格的第二行）计算的卡方值（6.02）带入进去，看看大于或者等于这个值在柱状图中所有卡方值中占有多少比例。如果占有的比例很大，说明这个值是由跟原假设很近似的假设得出的，这就证明了原假设是对的；反之。如果这个比例很小，说明如果分布服从原假设，那么所计算出的卡方值基本不可能包含这个观测出的卡方值，表明原假设是不对的，我们就可以拒绝原假设。 其实统计检验的基本思想就是这样。但是存在一个问题，我们不可能进行真实的采样（从河里抓鱼），所以一般采用计算机模拟的方式，具体步骤如下所示： 等概率的产生a（代表gumpies）, b（代表sticklebarbs）, c（代表spothheads）的序列 计算一个大小为300的序列中a, b, c三者的频数，作为观察值，然后将a = b = c = 100作为期望值，计算并记录下算出的卡方值 重复1~2步10000次， 画出柱状图如下： 可以看出只有5%左右的值大于6.02，说明我们可以以95%的置信度拒绝原假设。 二维情况对于二维的情况，卡方检验又被称为卡方关联度检验，也就是检验两个变量之间的相关程度（独立程度），考虑下面这个数据表$O$： Alzhemer’s onset - during 5-year period no yes recieved- yes 147 9 156 estrogen no 810 158 968 957 167 1124 这是观察值，为了计算卡方值，很明显我们需要计算期望值。 为了方便表示，把上表变成如下形式： （A）Alzhemer’s onset - during 5-year period no yes (R) recieved- yes [cell a] [cell b] 156 estrogen no [cell c] [cell d] 968 957 167 1124 拿a做例子： E_a = \frac{156}{1124} \times \frac{957}{1124} \times 1124这个式子如何解释呢？如果这两个变量是独立的，那么$A$变量取$no$值与$R$变量取$yes$值这两个事件之间就是独立的，那么$[cell_a]$事件发生的概率就是两者相乘，也就是上述等式右边前两个数相乘，最后的期望值自然就是概率乘以实验总数。 上面的解释比较不正式，换一种较为正式的表达方式。假设我们要求$cell$的期望值，设$R$为$cell$所在列的边缘事件总数，$C$为$cell$所在行的边缘事件总数，$N$为实验总数目，这样就有： E_{cell} =\frac{R}{N} \times \frac{C}{N} \times N所以就有期望值$E$数据表如下： （A）Alzhemer’s onset - during 5-year period no yes (R) recieved- yes $E_a = \frac{156 \times 957}{1124} = 132.82$ $E_b = \frac{156 \times 167}{1124} = 23.18$ 156 estrogen no $E_c = \frac{968 \times 957}{1124} = 824.18$ $E_d = \frac{968 \times 167}{1124} = 143.82$ 968 957 167 1124 这样就可以调用公式： \chi^2 = \sum \frac{(O - E) ^ 2}{E}特别地，当行数以及列数都为2时，上述公式需要进行一下修改： \chi^2 = \sum \frac{(|O - E| - 0.5) ^ 2}{E}这样算出来的卡方值为$11.01$ 最后涉及到自由度的问题，因为比较简单，所以只写出结论： df = (r - 1)(c - 1)r = number of rows c = number of columns ANOVA方差检验为了方便比较，同样采用上述阿尔兹海默病的例子。研究表明，老年痴呆症患者患病之后会经常经历情绪非常不稳定地阶段，原因是因为患者患病之前的生活中经常有恐惧或者焦虑的体验，正是这些一直存在于脑海中的记忆出发了患病后的不稳定情绪的产生。 现在我们假设有一个实验团队发明了一种药物，可以缓解这种情绪问题，他们对小白鼠进行了实验。实验设计如下： 将小白鼠随机分为四组A， B， C， D A组作为参照组，不给药；B， C， D三组分别注射一个单位，两个单位，三个单位的药剂 记录实验结果，数值越低表明实验效果越好 实验结果如下： A B C D Total 27.0 22.8 21.9 23.5 26.2 23.1 23.4 19.6 28.8 27.7 20.1 23.7 33.5 27.6 27.8 20.8 28.8 24.0 19.3 23.9 $M_a = 28.86$ $M_b = 25.04$ $M_c = 22.50$ $M_d = 22.30$ $M_T = 24.68$ 下面需要进行一下相关性分析，判断药物是否对症状的缓解产生作用。 和卡方检验一样，ANOVA检验最后也有一个衡量指标，记为$F$，定义如下： F = \frac{MS_{bg}}{MS_{wg}} = \frac{组间相似度}{组内相似度}具体的计算步骤如下所示（推导过程省略） （1） 首先计算出如下值： A B C D Total $N_A = 5$ $N_B = 5$ $N_C = 5$ $N_D = 5$ $N_T = 20$ $\sum X_{Ai} = 144.30$ $\sum X_{Bi} = 125.20$ $\sum X_{Ci} = 112.50$ $\sum X_{Di} = 111.50$ $\sum X_{Ti} = 493.50$ $\sum X^2_{Ai} = 4196.57$ $\sum X^2_{Bi} = 3158.50$ $\sum X^2_{Ci} = 2576.51$ $\sum X^2_{Di} = 2501.95$ $\sum X^2_{Ti} = 12433.53$ $SS_A = 32.07$ $SS_B = 23.49$ $SS_C = 45.26$ $SS_D = 15.50$ $SS_T = 256.42$ 其中： SS = \sum X^2_i - \frac{(\sum X_i)^2}{N}（2）计算$SS{wg}$以及$SS{bg}$ SS_{wg} = SS_A + SS_B + SS_C + SS_DSS_{bg} = SS_T - SS_{wg}（4）计算相关自由度 df_{bg} = k - 1 = 4 - 1 = 3df_{wg} = (N_A - 1) + (N_B - 1) + (N_C - 1) + (N_D - 1)（5）计算 $MS{bg}$以及$MS{wg}$ MS_{bg} = \frac{SS_{bg}}{df_{bg}}MS_{wg} = \frac{SS_{wg}}{df_{wg}}（6）计算$F$ 最后得出F = 6.42 （df = 3, 16） 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269import osimport xlrdimport numpy as npimport scipy.stats as statsdef GetChildDetailLab(combine=True): """ Get the detail labs of each child data. Returns: child_detail_lab: the detail labs. """ CHILDS_FEATURE_LAB = ['tid', 'age_m', 'female', 'onlychild', 'divorce', 'medu_newcat', 'income_newcat', 'scr_ave', 'edu_ave', 'edu_of_scr', 'scr_h_cat', 'mediacoview', 'mediacontact'] CHILDS_CAT_LAB = ['emo_cat', 'con_cat', 'hyp_cat', 'pee_cat', 'difficulties_cat', 'pro_cat'] if combine: CHILDS_DETAIL_LAB = CHILDS_FEATURE_LAB CHILDS_DETAIL_LAB.extend(CHILDS_CAT_LAB) return CHILDS_DETAIL_LAB else: return CHILDS_FEATURE_LAB, CHILDS_CAT_LABdef ReadChildInfoFromExcel( file_name='屏幕暴露与SDQ.xlsx', sheet_name='data'): """ Read the screen-exposed vs.SDQ detail information of each child from the excel-type file. Arguments: file_name: the name of the excel-type file. sheet_name: the name of the sheet of the excel file. Returns: child_scr_exp_sdq: A list that contains the detail information of each child. labs: The lab corresponds to each colume of the data. """ CHILDS_FILE_NAME = 'child_scr_exp_sdq.npy' CHILDS_DETAIL_LAB = GetChildDetailLab() # print(CHILDS_DETAIL_LAB) NOT_INT_LAB_INDEIES = [CHILDS_DETAIL_LAB.index('age_m'), CHILDS_DETAIL_LAB.index('scr_ave'), CHILDS_DETAIL_LAB.index('edu_ave'), CHILDS_DETAIL_LAB.index('edu_of_scr')] child_scr_exp_sdq = [] if (os.path.isfile(CHILDS_FILE_NAME)): with open(CHILDS_FILE_NAME, 'rb') as f: child_scr_exp_sdq = np.load(f) else: workBook = xlrd.open_workbook(file_name) bookSheet = workBook.sheet_by_name(sheet_name) # read from second row because of the first row has tabs for row in range(1, bookSheet.nrows): child_row = [] for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = str(cel.value) # tolerant the value error if val == '': val = '-1.0' except Exception as e: print(e) # because of the type is different try: if col in NOT_INT_LAB_INDEIES: val = float(val) else: # in Excel, if cel.value is 1, then str(cel.value) is # '1.0' val = val.split('.')[0] val = int(val) except Exception as e: print(e) val = -1 child_row.append(val) child_scr_exp_sdq.append(child_row) child_scr_exp_sdq = np.array(child_scr_exp_sdq) with open(CHILDS_FILE_NAME, 'wb') as f: np.save(f, child_scr_exp_sdq) return child_scr_exp_sdq, CHILDS_DETAIL_LABdef SplitDataSet(data_set, feature, cat): """ Split the data set to two column, that is feature and cat Arguments: data_set: the source data set. feature: the input vector cat: the correspond category. Returns: splited_data_set: self-explation. """ CHILDS_DETAIL_LAB = GetChildDetailLab() feature_index = CHILDS_DETAIL_LAB.index(feature) cat_index = CHILDS_DETAIL_LAB.index(cat) # print(feature_index, cat_index) return data_set[:, (feature_index, cat_index)]def CalChi2(data_set): """ Calculate the chi-square value and p-value corresponds to the data set. Arguments: data_set: the object data set. Returns: chi2: the chi-square value. p: the p-value. """ rows_number = len(set(data_set[:, -1])) columns_number = len(set(data_set[:, 0])) # print(rows_number, columns_number) counts = np.zeros((rows_number, columns_number)) for row in data_set: if row[-1] != -1: if row[0] != -1: try: counts[int(row[-1])][int(row[0])] += 1 except: pass # drop the row that all item is 0 del_row_index = [] for index, count in enumerate(counts): if not count.any(): del_row_index.append(index) counts = np.delete(counts, tuple(del_row_index), axis=0) # drop the column that all item is 0 del_col_index = [] for index, count in enumerate(counts.T): if not count.any(): del_col_index.append(index) counts = np.delete(counts, tuple(del_col_index), axis=1) # print(counts) # calculate the chi-square value and correspond p-value chi2, p, dof, excepted = stats.chi2_contingency(counts) return chi2, pdef ANOVATest(data_set): """ Implement a ANOVA test on the data set. Arguments: data_set: the object data set. Return: f: The computed F-value of the test. p: The associated p-value from the F-distribution. """ # Initial the three categories normal = [] critical = [] abnormal = [] for data in data_set: if data[0] != -1: if data[-1] == 0: normal.append(data[0]) elif data[-1] == 1: critical.append(data[0]) elif data[-1] == 2: abnormal.append(data[0]) f, p = stats.f_oneway(normal, critical, abnormal) return f, pdef GenerateCoffMatrix(data_set): """ Calculate the chi-square and p-value of each feature-category pair. Arguments: data_set: the source data set. Returns: coff_matrix: the final cofficient matrix. """ coff_matrix = &#123;&#125; CHILDS_FEATURE_LAB, CHILDS_CAT_LAB = GetChildDetailLab(combine=False) NOT_KEEP_FEATURE_LAB = ['tid', 'age_m', 'scr_ave', 'edu_ave', 'edu_of_scr'] for feature in CHILDS_FEATURE_LAB: if feature in NOT_KEEP_FEATURE_LAB: if feature != NOT_KEEP_FEATURE_LAB[0]: for cat in CHILDS_CAT_LAB: splited_data_set = SplitDataSet(data_set, feature, cat) # print(feature, cat) f, p = ANOVATest(splited_data_set) key = feature + '-' + cat coff_matrix[key] = (f, p) else: for cat in CHILDS_CAT_LAB: splited_data_set = SplitDataSet(data_set, feature, cat) # print(feature, cat) chi2, p = CalChi2(splited_data_set) key = feature + '-' + cat coff_matrix[key] = (chi2, p) return coff_matrixdef SiftRelativeFeature(coff_matrix, conf=1e-5): """ Sift the feature that satisfy the caonfident condition. Arguemnts: coff_matrix: the calculated cofficient matrix for all features and categories. conf: the confident. Returns: relative_feature_matrix: the satisfied feature and correspond category and chi2 and p-value. """ relative_feature_matrix = &#123;&#125; for key in coff_matrix.keys(): if coff_matrix[key][-1] &lt;= conf: relative_feature_matrix[key] = coff_matrix[key] return relative_feature_matrixdef WriteResult(coff_matrix, file_name='result.txt'): """ Write the result to file. Arguments: relative_feature_matrix: the satisfied feature and correspond category and chi2 and p-value. file_name: the result file name. """ # Sort sorted_coff_matrix = sorted(coff_matrix.items(), key=lambda item: item[1][-1], reverse=False) # print(sorted_coff_matrix) with open(file_name, 'w') as f: for item in sorted_coff_matrix: f.write(str(item)) f.write('\n')if __name__ == '__main__': data_set, labels = ReadChildInfoFromExcel() # splited_data_set = SplitDataSet(data_set, 'female', 'difficulties_cat') # chi2, p = CalChi2(splited_data_set) coff_matrix = GenerateCoffMatrix(data_set) # relative_feature = SiftRelativeFeature(coff_matrix, 1) WriteResult(coff_matrix) 因子分析先说说因子分析与主成分分析的区别，下面的话引自知乎(https://www.zhihu.com/question/24524693) 具体实现使用SPSS软件进行实现： 操作步骤如下(引自 SPSS数据分析从入门到精通-陈胜可)： 实验结果如下： 2017.2.27 Updates 决策树1234567import pandas as pdfrom sklearn import treeimport numpy as npfrom sklearn.model_selection import cross_val_scorefrom IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all" Read data1data = pd.read_excel('./scr_SDQ.xlsx', sheetname='data', index_col=0).dropna().sort_index() Convert the data format1234float_columns = ['scr_ave', 'edu_ave','scr_of_edu']for column in data.columns: if column not in float_columns: data[column] = data[column].astype(int) Split the data123data_feature_with_edu_ave = data.ix[:, :'mediacontact'].drop('scr_of_edu', axis=1)data_feature_with_scr_of_edu = data.ix[:, :'mediacontact'].drop('edu_ave', axis=1)data_classes = data.ix[:, 'emo_cat':] 12# dtree = tree.DecisionTreeClassifier(min_samples_leaf=500)# cross_val_score(dtree,data_feature_with_edu_ave, data_classes['difficulties_cat'], cv=10) array([ 0.64356436, 0.64356436, 0.64391855, 0.64391855, 0.64407713, 0.64407713, 0.64407713, 0.64407713, 0.64407713, 0.64407713]) difficulties_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['difficulties_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['difficulties_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_diff.pngwith open('dtree_with_edu_ave_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_diff.pngwith open('dtree_with_scr_of_edu_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns) Imp scr_ave 0.553782 medu_newcat 0.217218 age_m 0.085820 income_newcat 0.048912 edu_ave 0.029740 female 0.026161 onlychild 0.021336 mediacontact 0.017030 divorce 0.000000 scr_h_cat 0.000000 mediacoview 0.000000 Imp scr_ave 0.525587 medu_newcat 0.213067 age_m 0.084708 scr_of_edu 0.080575 mediacontact 0.029232 female 0.025661 onlychild 0.020929 income_newcat 0.020240 divorce 0.000000 scr_h_cat 0.000000 mediacoview 0.000000 emo_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['emo_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['emo_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_emo.pngwith open('dtree_with_edu_ave_emo.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_emo.pngwith open('dtree_with_scr_of_edu_emo.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns) Imp scr_ave 0.521694 medu_newcat 0.159798 income_newcat 0.147978 edu_ave 0.083233 age_m 0.040750 female 0.023781 mediacontact 0.019127 mediacoview 0.003639 onlychild 0.000000 divorce 0.000000 scr_h_cat 0.000000 Imp scr_ave 0.520711 income_newcat 0.155111 medu_newcat 0.146762 scr_of_edu 0.115745 age_m 0.023767 female 0.021924 mediacontact 0.011410 mediacoview 0.004571 onlychild 0.000000 divorce 0.000000 scr_h_cat 0.000000 con_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['con_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['con_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_con.pngwith open('dtree_with_edu_ave_con.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_con.pngwith open('dtree_with_scr_of_edu_con.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns) Imp scr_ave 0.562325 medu_newcat 0.098819 mediacontact 0.095089 edu_ave 0.077572 female 0.064213 income_newcat 0.056334 age_m 0.023696 onlychild 0.013513 mediacoview 0.008440 divorce 0.000000 scr_h_cat 0.000000 Imp scr_ave 0.511282 scr_of_edu 0.110155 medu_newcat 0.097473 mediacontact 0.093793 female 0.082843 income_newcat 0.055566 age_m 0.035552 onlychild 0.013335 divorce 0.000000 scr_h_cat 0.000000 mediacoview 0.000000 hyp_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['hyp_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['hyp_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_hyp.pngwith open('dtree_with_edu_ave_hyp.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_hyp.pngwith open('dtree_with_scr_of_edu_hyp.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns) Imp medu_newcat 0.343448 scr_ave 0.265652 onlychild 0.165487 edu_ave 0.076194 income_newcat 0.054466 age_m 0.046774 mediacontact 0.020992 mediacoview 0.015532 female 0.011454 divorce 0.000000 scr_h_cat 0.000000 Imp medu_newcat 0.335192 scr_ave 0.257218 onlychild 0.161509 scr_of_edu 0.105471 age_m 0.065618 income_newcat 0.033129 mediacontact 0.022542 mediacoview 0.012901 female 0.006421 divorce 0.000000 scr_h_cat 0.000000 pee_cat1234567891011121314151617dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['pee_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['pee_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_pee.pngwith open('dtree_with_edu_ave_pee.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_pee.pngwith open('dtree_with_scr_of_edu_pee.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns) Imp scr_ave 0.441246 income_newcat 0.210646 female 0.152568 age_m 0.151888 medu_newcat 0.033530 mediacoview 0.009625 edu_ave 0.000498 onlychild 0.000000 divorce 0.000000 scr_h_cat 0.000000 mediacontact 0.000000 Imp scr_ave 0.430946 income_newcat 0.200188 age_m 0.151286 female 0.139104 medu_newcat 0.033289 scr_of_edu 0.033259 mediacoview 0.011927 onlychild 0.000000 divorce 0.000000 scr_h_cat 0.000000 mediacontact 0.000000 pro_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['pro_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['pro_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_pro.pngwith open('dtree_with_edu_ave_pro.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_pro.pngwith open('dtree_with_scr_of_edu_pro.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns) Imp female 0.295389 scr_ave 0.252091 mediacontact 0.145010 age_m 0.137453 income_newcat 0.085421 edu_ave 0.062697 medu_newcat 0.021938 onlychild 0.000000 divorce 0.000000 scr_h_cat 0.000000 mediacoview 0.000000 Imp female 0.299316 scr_ave 0.204496 mediacontact 0.131972 age_m 0.125169 scr_of_edu 0.111909 income_newcat 0.080072 medu_newcat 0.047066 onlychild 0.000000 divorce 0.000000 scr_h_cat 0.000000 mediacoview 0.000000 Convert continuous variables to categorical variables1234data_category = data.copy()for column in data_category.columns: if column in float_columns: data_category[column] = pd.cut(data_category[column], 10, labels=np.arange(10)) 123data_cate_feature_with_edu_ave = data_category.ix[:, :'mediacontact'].drop('scr_of_edu', axis=1)data_cate_feature_with_scr_of_edu = data_category.ix[:, :'mediacontact'].drop('edu_ave', axis=1)data_cate_classes = data.ix[:, 'emo_cat':] 12dtree_cate = tree.DecisionTreeClassifier(min_samples_leaf=50)cross_val_score(dtree_cate,data_cate_feature_with_edu_ave, data_cate_classes['difficulties_cat'], cv=10).sum() / 10 0.63948514409153423 12345678910111213141516dtree_cate_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_cate_with_edu_ave = dtree_with_edu_ave.fit(data_cate_feature_with_edu_ave, data_cate_classes['difficulties_cat'])pd.DataFrame(dtree_cate_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_cate_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_cate_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_cate_with_scr_of_edu = dtree_cate_with_scr_of_edu.fit(data_cate_feature_with_scr_of_edu, data_cate_classes['difficulties_cat'])pd.DataFrame(dtree_cate_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_cate_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_cate_with_edu_ave_diff.pngwith open('dtree_cate_with_edu_ave_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_cate_with_edu_ave, out_file=dot_file, feature_names=data_cate_feature_with_edu_ave.columns)# dtree_cate_with_scr_of_edu_diff.pngwith open('dtree_cate_with_scr_of_edu_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_cate_with_scr_of_edu, out_file=dot_file, feature_names=data_cate_feature_with_scr_of_edu.columns) Imp scr_h_cat 0.528976 medu_newcat 0.221779 age_m 0.092168 income_newcat 0.057032 onlychild 0.034724 mediacontact 0.027193 female 0.021036 scr_ave 0.011653 mediacoview 0.005439 divorce 0.000000 edu_ave 0.000000 Imp scr_h_cat 0.507678 medu_newcat 0.215210 age_m 0.089438 scr_of_edu 0.042832 mediacontact 0.041463 income_newcat 0.035742 onlychild 0.033695 female 0.020413 scr_ave 0.013527 divorce 0.000000 mediacoview 0.000000]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Get Pois uses God-map apis]]></title>
      <url>%2F2017%2F02%2F21%2FGet-Pois-uses-God-map-apis%2F</url>
      <content type="text"><![CDATA[伪代码如下： 12345从Excel文件中读出数据对于每一个house: 提取出其location字段（经纬度） 将location字段作为输入参数传给map api 将返回值进行适当筛选最后存入原数据集中 代码实现如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import osimport xlrdimport pickleimport requestsdef ReadHousesInfoFromExcel( file_name='houses_nadrop.xls', sheet_name='小区信息'): """ Read the houses detail information from the excel-type file. Arguments: file_name: the name of the excel-type file. sheet_name: the name of the sheet of the excel file. Returns: houses: A dict that contains the detail information of each house. """ HOUSES_FILE_NAME = 'houses.pkl' HOUSES_DETAIL_TAB = ['name', 'address', 'property_category', 'area', 'avg_price', 'location', 'property_costs', 'volume_rate', 'green_rate'] houses = [] if (os.path.isfile(HOUSES_FILE_NAME)): with open(HOUSES_FILE_NAME, 'rb') as f: houses = pickle.load(f) else: workBook = xlrd.open_workbook(file_name) bookSheet = workBook.sheet_by_name(sheet_name) # read from second row because of the first row has tabs for row in range(1, bookSheet.nrows): house = &#123;&#125; for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = cel.value except: pass val = str(val) house[HOUSES_DETAIL_TAB[col]] = val houses.append(house) with open(HOUSES_FILE_NAME, 'wb') as f: pickle.dump(houses, f) return housesdef Geocode(location, poi_type): """ A tool that call the God-Map api. Arguments: location: The location of house. poi_type: The poi type. Returns: answer: The JSON-type data that contains pois infomation. """ location = str(location).strip() parameters = &#123;'location': location, 'key': 'e798a5bfb344a09977b79552ae415974', 'types': poi_type, 'offset': 10, 'page': 1, 'extensions': 'base'&#125; base = 'http://restapi.amap.com/v3/place/around' try: response = requests.get(base, parameters) answer = response.json() except Exception as e: print('error!', e) answer = 'null' finally: pass return answerdef GetPOI(houses): """ Get the pois information of the houses according to the location. Arguments: houses: The house detail information. Returns: houses_with_pois: The house detail information that contains the pois information. """ POI_TYPE_LAB = ['subway_station', 'bus_station', 'parking_lot', 'primary_school', 'secondary_school', 'university', 'mall', 'park'] POI_TYPE_CODE = ['150500', '150700', '150904', '141203', '141202', '141201', '060100', '110101'] KEEP_INFO_LAB = ['name', 'location', 'distance'] NO_INFO_NOW = '-' SIZE = len(houses) houses_with_pois = houses.copy() count = 0 for house in houses_with_pois: count = count + 1 if count % 100 == 0: print(count, '', SIZE) house['pois'] = &#123;&#125; for poi_type_index in range(len(POI_TYPE_LAB)): poi_info_json = Geocode(house['location'], POI_TYPE_CODE[poi_type_index]) if poi_info_json == 'null' or poi_info_json['pois'] is None: house['pois'][POI_TYPE_LAB[poi_type_index]] = NO_INFO_NOW else: house['pois'][POI_TYPE_LAB[poi_type_index]] = [] for poi in poi_info_json['pois']: pois_without_useless = &#123;&#125; for key in poi.keys(): if key in KEEP_INFO_LAB: pois_without_useless[key] = poi[key] house['pois'][POI_TYPE_LAB[poi_type_index]].append( pois_without_useless) # return houses_with_pois return houses_with_poisif __name__ == '__main__': houses = ReadHousesInfoFromExcel() # answer = Geocode(houses[0]['location'], '150905') houses_with_pois = GetPOI(houses) 总结一下有几个注意点： 传给parameters的location参数的格式一定要规范，前后都不能有空格 for循环中不能改变字典的大小，这里的大小不仅指其元素的数目，也包括其总占用空间的大小 注意pickle的用法 从Excel中读出的内容要转成str格式 整个过程十分清晰明了，值得注意的是细节问题]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis-Learning note-Ch02]]></title>
      <url>%2F2017%2F02%2F19%2FPython%20data%20analysis-Learning%20notes-ch02%2F</url>
      <content type="text"><![CDATA[利用Python内置的JSON模块对数据进行解析并转化为字典数据如下： 123456'&#123; "a": "Mozilla\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\/535.11 (KHTML, like Gecko)Chrome\\/17.0.963.78 Safari\\/535.11", "c": "US", "nk": 1, "tz": "America\\/New_York", "gr":"MA", "g": "A6qOVH", "h": "wfLQtf", "l": "orofrog", "al": "en-US,en;q=0.8", "hh": "1.usa.gov","r": "http:\\/\\/www.facebook.com\\/l\\/7AQEFzjSi\\/1.usa.gov\\/wfLQtf", "u":"http:\\/\\/www.ncbi.nlm.nih.gov\\/pubmed\\/22415991", "t": 1331923247, "hc": 1331822918,"cy": "Danvers", "ll": [ 42.576698, -70.954903 ] &#125;\n' 核心代码： 123import jsonpath = 'ch02/usagov_bitly_data2012-03-16-1331923249.txt'records = [json.loads(line) for line in open(path)] 123456789101112131415161718records[0]-----------------------------------&#123;'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11', 'al': 'en-US,en;q=0.8', 'c': 'US', 'cy': 'Danvers', 'g': 'A6qOVH', 'gr': 'MA', 'h': 'wfLQtf', 'hc': 1331822918, 'hh': '1.usa.gov', 'l': 'orofrog', 'll': [42.576698, -70.954903], 'nk': 1, 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf', 't': 1331923247, 'tz': 'America/New_York', 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991'&#125; 对时区字段进行计数（pure python vs. pandas）首先从记录中提取时区字段并且放入一个列表中 1time_zones = [rec['tz'] for rec in records if 'tz' in rec] 123456789101112time_zones[:10]-----------------------------------['America/New_York', 'America/Denver', 'America/New_York', 'America/Sao_Paulo', 'America/New_York', 'America/New_York', 'Europe/Warsaw', '', '', ''] 使用纯粹的python进行计数 12345678def get_counts(sequence): counts = &#123;&#125; for x in sequence: if x in counts: counts[x] += 1 else: counts[x] = 1 return counts 使用下列方法更加简洁 1234567from collections import defaultdictdef get_counts2(sequence): counts = defaultdict(int) # values will initialize to 0 for x in sequence: counts[x] += 1 return counts 如果需要返回前十位的时区及其计数值 1234def top_counts(count_dict, n=10): value_key_pairs = [(count, tz) for tz, count in count_dict.items()] value_key_pairs.sort() return value_key_pairs[-n:] 123456789101112top_counts(counts)--------------------------------------[(33, 'America/Sao_Paulo'), (35, 'Europe/Madrid'), (36, 'Pacific/Honolulu'), (37, 'Asia/Tokyo'), (74, 'Europe/London'), (191, 'America/Denver'), (382, 'America/Los_Angeles'), (400, 'America/Chicago'), (521, ''), (1251, 'America/New_York')] 可以使用python自带的库 1from collections import Counter 1counts = Counter(time_zones) 123456789101112counts.most_common(10)--------------------------------[('America/New_York', 1251), ('', 521), ('America/Chicago', 400), ('America/Los_Angeles', 382), ('America/Denver', 191), ('Europe/London', 74), ('Asia/Tokyo', 37), ('Pacific/Honolulu', 36), ('Europe/Madrid', 35), ('America/Sao_Paulo', 33)] 使用pandas进行相同的任务 pandas中主要的数据结构是DataFrame， 作用是将数据表示成表格 12345from pandas import DataFrame, Seriesimport pandas as pdframe = DataFrame(records)frame 12345678910111213frame['tz'][:10]-------------------------------0 America/New_York1 America/Denver2 America/New_York3 America/Sao_Paulo4 America/New_York5 America/New_York6 Europe/Warsaw7 8 9 Name: tz, dtype: object 计数· 1234567891011121314tz_counts = frame['tz'].value_counts()tz_counts[:10]--------------------------------------------------America/New_York 1251 521America/Chicago 400America/Los_Angeles 382America/Denver 191Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35America/Sao_Paulo 33Name: tz, dtype: int64 填补缺失值以及未知值 12345678910111213141516clean_tz = frame['tz'].fillna('Missing')clean_tz[clean_tz == ''] = 'Unknown'tz_counts = clean_tz.value_counts()tz_counts[:10]----------------------------------------------America/New_York 1251Unknown 521America/Chicago 400America/Los_Angeles 382America/Denver 191Missing 120Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35Name: tz, dtype: int64 画个图展示一下 12plt.figure(figsize=(10, 4))tz_counts[:10].plot(kind='barh', rot=0) 下面我们对用户使用的浏览器的信息做一些操作 Series应该代表的是DataFrame中的一列 123456789results = Series([x.split()[0] for x in frame.a.dropna()])results[:5]---------------------------------------------------0 Mozilla/5.01 GoogleMaps/RochesterNY2 Mozilla/4.03 Mozilla/5.04 Mozilla/5.0dtype: object 同样可以进行计数 1234567891011results.value_counts()[:8]-----------------------------------------Mozilla/5.0 2594Mozilla/4.0 601GoogleMaps/RochesterNY 121Opera/9.80 34TEST_INTERNET_AGENT 24GoogleProducer 21Mozilla/6.0 5BlackBerry8520/5.0.0.681 4dtype: int64 根据Windows和Non-Windows用户进行时区的分组操作1cframe = frame[frame.a.notnull()] 123456operating_system = np.where(cframe['a'].str.contains('Windows'), 'Windows', 'Not Windows')operating_system[:5]-----------------------------------------------------------------array(['Windows', 'Not Windows', 'Windows', 'Not Windows', 'Windows'], dtype='&lt;U11') 1by_tz_os = cframe.groupby(['tz', operating_system]) 来看看这个by_tz_os长什么样 1by_tz_os.size() 再来看看unstack()的炫酷效果 排下序， 看看排名多少 12345678910111213141516# Use to sort in ascending orderindexer = agg_counts.sum(1).argsort()indexer[:10]------------------------------------------------tz 24Africa/Cairo 20Africa/Casablanca 21Africa/Ceuta 92Africa/Johannesburg 87Africa/Lusaka 53America/Anchorage 54America/Argentina/Buenos_Aires 57America/Argentina/Cordoba 26America/Argentina/Mendoza 55dtype: int64 取出前十的来看看 12count_subset = agg_counts.take(indexer)[-10:]count_subset 同样画个图 1count_subset.plot(kind='barh', stacked=True) 看看两个类别所占的比例是多少 12normed_subset = count_subset.div(count_subset.sum(1), axis=0)normed_subset.plot(kind='barh', stacked=True) 电影评分数据表连接操作123456789101112131415import pandas as pdimport osencoding = 'latin1'upath = os.path.expanduser('ch02/movielens/users.dat')rpath = os.path.expanduser('ch02/movielens/ratings.dat')mpath = os.path.expanduser('ch02/movielens/movies.dat')unames = ['user_id', 'gender', 'age', 'occupation', 'zip']rnames = ['user_id', 'movie_id', 'rating', 'timestamp']mnames = ['movie_id', 'title', 'genres']users = pd.read_csv(upath, sep='::', header=None, names=unames, encoding=encoding)ratings = pd.read_csv(rpath, sep='::', header=None, names=rnames, encoding=encoding)movies = pd.read_csv(mpath, sep='::', header=None, names=mnames, encoding=encoding) 看看数据长什么样 1users[:5] 1ratings[:5] 1movies[:5] 多表连接 12data = pd.merge(pd.merge(ratings, users), movies)data 12345678910111213data.ix[0]--------------------------------------------user_id 1movie_id 1193rating 5timestamp 978300760gender Fage 1occupation 10zip 48067title One Flew Over the Cuckoo's Nest (1975)genres DramaName: 0, dtype: object 根据性别计算每部电影的平均评分123mean_ratings = data.pivot_table('rating', index='title', columns='gender', aggfunc='mean')mean_ratings[:5] 过滤掉评分数小于250的电影 1ratings_by_title = data.groupby('title').size() 123456789ratings_by_title[:5]-----------------------------------------title$1,000,000 Duck (1971) 37'Night Mother (1986) 70'Til There Was You (1997) 52'burbs, The (1989) 303...And Justice for All (1979) 199dtype: int64 1active_titles = ratings_by_title.index[ratings_by_title &gt;= 250] 12345678active_titles[:10]-----------------------------------------Index([''burbs, The (1989)', '10 Things I Hate About You (1999)', '101 Dalmatians (1961)', '101 Dalmatians (1996)', '12 Angry Men (1957)', '13th Warrior, The (1999)', '2 Days in the Valley (1996)', '20,000 Leagues Under the Sea (1954)', '2001: A Space Odyssey (1968)', '2010 (1984)'], dtype='object', name='title') ix应该是一个交集操作 12mean_ratings = mean_ratings.ix[active_titles]mean_ratings 按照女性最喜欢的电影进行降序排序 12top_female_ratings = mean_ratings.sort_values(by='F', ascending=False)top_female_ratings[:10] ​ US Baby Names 1880-2010123import pandas as pdnames1880 = pd.read_csv('ch02/names/yob1880.txt', names=['name', 'sex', 'births'])names1880 把所有年份的数据合并一下 123456789101112131415# 2010 is the last available year right nowyears = range(1880, 2011)pieces = []columns = ['name', 'sex', 'births']for year in years: path = 'ch02/names/yob%d.txt' % year frame = pd.read_csv(path, names=columns) frame['year'] = year pieces.append(frame)# Concatenate everything into a single DataFramenames = pd.concat(pieces, ignore_index=True) 进行聚合操作 12total_births = names.pivot_table('births', index='year', columns='sex', aggfunc=sum) 1total_births.tail() 计算一下每个名字的出生比例 1234567def add_prop(group): # Integer division floors births = group.births.astype(float) group['prop'] = births / births.sum() return groupnames = names.groupby(['year', 'sex']).apply(add_prop) 1names 进行一下有效性检查 123np.allclose(names.groupby(['year', 'sex']).prop.sum(), 1)--------------------------------------------True 筛选出每一对year/sex下总数前1000的名字 1234def get_top1000(group): return group.sort_values(by='births', ascending=False)[:1000]grouped = names.groupby(['year', 'sex'])top1000 = grouped.apply(get_top1000) 加个索引，结合了numpy 1top1000.index = np.arange(len(top1000)) Analyzing naming trends将数据分为男女 12boys = top1000[top1000.sex == 'M']girls = top1000[top1000.sex == 'F'] 计算每一年每个名字的出生总数 123total_births = top1000.pivot_table('births', index='year', columns='name', aggfunc=sum)total_births 选出几个名字看看总数随年份的变化情况 123subset = total_births[['John', 'Harry', 'Mary', 'Marilyn']]subset.plot(subplots=True, figsize=(12, 10), grid=False, title="Number of births per year") Measuring the increase in naming diversity通过统计前1000项名字所占的比例来判断多样性的变化 1234table = top1000.pivot_table('prop', index='year', columns='sex', aggfunc=sum)table.plot(title='Sum of table1000.prop by year and sex', yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10)) 另一种方法，计算占出生人数50%的名字的数量 也即从开始累加，看加到第几个名字时所占比例为50% 先来看看2010年的男孩 1df = boys[boys.year == 2010] 1234567891011121314prop_cumsum = df.sort_index(by='prop', ascending=False).prop.cumsum()prop_cumsum[:10]--------------------------------------------------260877 0.011523260878 0.020934260879 0.029959260880 0.038930260881 0.047817260882 0.056579260883 0.065155260884 0.073414260885 0.081528260886 0.089621Name: prop, dtype: float64 看来是第116个，不过序号从0开始，应该是117 123prop_cumsum.values.searchsorted(0.5)---------------------------------------------------116 再来看看1900年的男孩儿 12345df = boys[boys.year == 1900]in1900 = df.sort_index(by='prop', ascending=False).prop.cumsum()in1900.values.searchsorted(0.5) + 1---------------------------------------------------25 所以这样做是可行的 把相同的操作赋予整个数据集 123456def get_quantile_count(group, q=0.5): group = group.sort_values(by='prop', ascending=False) return group.prop.cumsum().values.searchsorted(q) + 1diversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)diversity = diversity.unstack('sex')diversity.head() 1diversity.plot(title="Number of popular names in top 50%") The “Last letter” Revolution取出每个名字对应的最后一个字母，同时序号对应 1234567# extract last letter from name columnget_last_letter = lambda x: x[-1]last_letters = names.name.map(get_last_letter)last_letters.name = 'last_letter'table = names.pivot_table('births', index=last_letters, columns=['sex', 'year'], aggfunc=sum) 单独取出三年的来看看 12subtable = table.reindex(columns=[1910, 1960, 2010], level='year')subtable.head() 计算一下字母比例 12345678910subtable.sum()-------------------------------------sex yearF 1910 396416.0 1960 2022062.0 2010 1759010.0M 1910 194198.0 1960 2132588.0 2010 1898382.0dtype: float64 1letter_prop = subtable / subtable.sum().astype(float) 123456import matplotlib.pyplot as pltfig, axes = plt.subplots(2, 1, figsize=(10, 8))letter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')letter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female', legend=False) 最后看一下所有的年份并生成一个趋势图 12letter_prop = table / table.sum().astype(float)dny_ts = letter_prop.ix[['d', 'n', 'y'], 'M'].T 1dny_ts.plot() Boy names that became girl names (and vice versa)以lesl开头的名字为例 123456all_names = top1000.name.unique()mask = np.array(['lesl' in x.lower() for x in all_names])lesley_like = all_names[mask]lesley_like----------------------------------------------array(['Leslie', 'Lesley', 'Leslee', 'Lesli', 'Lesly'], dtype=object) 从原数据集中筛选出来 12345678910filtered = top1000[top1000.name.isin(lesley_like)]filtered.groupby('name').births.sum()----------------------------------------------nameLeslee 1082Lesley 35022Lesli 929Leslie 370429Lesly 10067Name: births, dtype: int64 做一下聚合操作并计算比例 1234table = filtered.pivot_table('births', index='year', columns='sex', aggfunc='sum')table = table.div(table.sum(1), axis=0)table.tail() 看一下趋势 1table.plot(style=&#123;'M': 'k-', 'F': 'k--'&#125;)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spider the house infomation and save to excel file]]></title>
      <url>%2F2017%2F02%2F18%2FSpider-the-house-infomation-and-save-to-excel-file%2F</url>
      <content type="text"><![CDATA[数据来源http://sh.fang.com/ 项目目标爬取二手房信息中的小区信息 实现步骤【1】爬取小区信息（核心代码，下同） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150""" &lt;A spider to crawl the house information.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;. """class HouseSpider(scrapy.Spider): name = "house" head = "http://esf.sh.fang.com" allowed_domains = ["sh.fang.com"] start_urls = [ "http://esf.sh.fang.com/housing/" ] # 各区对应的编号（由于丧心病狂的url） area_map = &#123;25: 'pudong', 18: 'minhang', 19: 'xuhui', 30: 'baoshan', 28: 'putuo', 20: 'changning', 26: 'yangpu', 586: 'songjiang', 29: 'jiading', 23: 'hongkou', 27: 'zhabei', 21: 'jingan', 24: 'huangpu', 22: 'luwan', 31: 'qingpu', 32: 'fengxian', 35: 'jinshan', 996: 'chongming'&#125; estate_to_area_map = &#123;&#125; seperator = '=\n' def __init__(self): for key, value in self.area_map.items(): self.estate_to_area_map[key] = [] # print(self.estate_to_area_map) def parse(self, response): # 解析出上海市各区的地址 area_lis = response.xpath('//*[@id="houselist_B03_02"]/div[1]') for a in area_lis.xpath('./a'): # areas = items.AreaItem() # areas['name'] = a.xpath('text()').extract()[0] yield Request(self.head + a.xpath('@href').extract()[0], callback=self.parse_area) # print(a.xpath('text()').extract()[0]) def parse_area(self, response): # 确定response来源于哪一个区 area_index = str(response).split('/')[-2].split('_')[0] if area_index == '': return else: # 解析出各区中小区的详情页面地址 detail_str = 'xiangqing' estate_list = response.xpath('/html/body/div[4]/div[5]/div[4]') for a in estate_list.xpath('.//a[@class="plotTit"]'): estate_url = a.xpath('@href').extract()[0] if estate_url.find('esf') != -1: estate_url = estate_url.replace('esf', detail_str) else: estate_url = estate_url + detail_str if estate_url.find('http') != -1: # print(estate_url) self.estate_to_area_map[int(area_index)].append(estate_url) # print(len(self.estate_to_area_map[int(area_index)])) next_page = response.xpath('//*[@id="PageControl1_hlk_next"]') if len(next_page) != 0: yield Request(self.head + next_page.xpath('@href').extract()[0], callback=self.parse_area) else: # print(len(self.estate_to_area_map[int(area_index)])) for url in self.estate_to_area_map[int(area_index)]: request = Request(url, callback=self.parse_house, dont_filter=True) request.meta['index'] = int(area_index) yield request def parse_house(self, response): flag = 0 area_index = response.meta['index'] area_name = self.area_map[area_index] filename = area_name + '.txt' # print(response.xpath('/html')) # 详情页面存在两种，因此分情况讨论 house_name = response.xpath( '/html/body/div[4]/div[2]/div[2]/h1/a/text()') if len(house_name) == 0: # house_name = response.xpath( # '/html/body/div[1]/div[3]/div[2]/h1/a/text()') # flag = 1 return house_name = house_name.extract()[0] # 清洁小区名 house_name = re.sub(r'小区网', '', house_name) result_str = '【小区名称】' + house_name + '\n' if flag == 0: avg_price_xpath = response.xpath( '/html/body/div[4]/div[4]/div[1]/div[1]/dl[1]/dd/span/text()') avg_price = avg_price_xpath.extract()[0] result_str = result_str + '【平均价格】' + avg_price + '\n' detail_block_list = response.xpath( '/html/body/div[4]/div[4]/div[1]') for headline in detail_block_list.xpath('.//h3'): head_str = headline.xpath('./text()').extract()[0] if head_str == '基本信息': result_str = result_str + \ '【' + \ head_str + '】\n' for item in headline.xpath( '../../div[@class="inforwrap clearfix"]/dl/dd'): if len(item.xpath('./strong/text()')) != 0: if len(item.xpath('./text()')) != 0: result_str = result_str + \ item.xpath( './strong/text()').extract()[0] result_str = result_str + \ item.xpath('./text()').extract()[0] + '\n' # print(result_str) # elif head_str == '交通状况': # result_str = result_str + \ # '【' + \ # head_str + '】\n' # tempstr = headline.xpath( # '../../div[@class="inforwrap clearfix"]/dl/dt/text()').extract()[0] # result_str = result_str + tempstr + '\n' # # print(result_str) # elif head_str == '周边信息': # result_str = result_str + \ # '【' + \ # head_str + '】\n' # for item in headline.xpath( # '../../div[@class="inforwrap clearfix"]/dl/dt'): # result_str = result_str + \ # item.xpath('./text()').extract()[0] + '\n' # # print(result_str) elif head_str == '就近楼群': result_str = result_str + \ '【' + \ head_str + '】\n' for item in headline.xpath( '../../div[@class="inforwrap clearfix"]/dl/dd'): result_str = result_str + \ item.xpath('./a/text()').extract()[0] + '\n' result_str = result_str + self.seperator # print(result_str) with open(filename, 'a', errors='ignore') as f: f.write(result_str) 【2】格式化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136""" &lt;A formatter&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;."""def GetDataFileList(path='.'): """ Get the houses file list. Arguments: path: Dir path. Returns: file_list: the list of data file that find houses data. """ file_list = [x for x in os.listdir(path) if os.path.isfile( x) and os.path.splitext(x)[1] == '.txt'] return file_listdef Parse(file_list): """ Parse the txt file that find houses data. Extract some import infomation such as house name, avarage price, address and so on. Arguments: file_list: the list of data file that find houses data. Returns: houses_dict_list: the list that each item find the detail dict of each house. """ HOUSE_NAME = '小区名称' HOUSE_NAME_SPLITOR = '】' HOUSE_ADDRESS = '小区地址' HOUSE_ADDRESS_SPLITOR = '：' HOUSE_AVG_PRICE = '平均价格' HOUSE_AVG_PRICE_SPLITOR = '】' AREA_OF_HOUSE_BELONGS_TO = '所属区域' AREA_OF_HOUSE_BELONGS_TO_SPLITOR_1 = '：' AREA_OF_HOUSE_BELONGS_TO_SPLITOR_2 = ' ' PROPERTY_CATEGORY = '物业类别' PROPERTY_CATEGORY_SPLITOR = '：' GREEN_RATE = '绿 化 率' GREEN_RATE_SPLITOR = '：' VOLUME_RATE = '容 积 率' VOLUME_RATE_SPLITOR = '：' PROPERTY_COSTS = '物 业 费' PROPERTY_COSTS_SPLITOR = '：' NO_INFO_NOW = '暂无信息' DETAIL_LIST = [HOUSE_NAME, HOUSE_AVG_PRICE, HOUSE_ADDRESS, AREA_OF_HOUSE_BELONGS_TO, PROPERTY_CATEGORY, GREEN_RATE, VOLUME_RATE, PROPERTY_COSTS] houses_dict_list = [] for file_name in file_list: raw_houses_string = '' # read all lines as a string with open(file_name, 'r', errors='ignore') as f: for line in f.readlines(): raw_houses_string += line # split the string to the houses raw info list raw_houses_list = raw_houses_string.split('=\n') raw_houses_details_list = [] for raw_house in raw_houses_list: # format house raw info to lines raw_houses_details = raw_house.split('\n')[:-1] if len(raw_houses_details) == 0: continue # combine the all formated house raw info to a list raw_houses_details_list.append(raw_houses_details) for raw_house_details in raw_houses_details_list: house_details_dict = &#123;&#125; for raw_detail in raw_house_details: # search house name if re.search(HOUSE_NAME, raw_detail): house_details_dict[HOUSE_NAME] = raw_detail.split( HOUSE_NAME_SPLITOR)[-1] # search house avarage price elif re.search(HOUSE_AVG_PRICE, raw_detail): # print(raw_detail) house_details_dict[HOUSE_AVG_PRICE] = raw_detail.split( HOUSE_AVG_PRICE_SPLITOR)[-1] # search house address elif re.search(HOUSE_ADDRESS, raw_detail): house_details_dict[HOUSE_ADDRESS] = raw_detail.split( HOUSE_ADDRESS_SPLITOR)[-1] # search the area of house belongs to elif re.search(AREA_OF_HOUSE_BELONGS_TO, raw_detail): temp_detail_value = raw_detail.split( AREA_OF_HOUSE_BELONGS_TO_SPLITOR_1)[-1] detail_value = temp_detail_value.split( AREA_OF_HOUSE_BELONGS_TO_SPLITOR_2)[0] house_details_dict[AREA_OF_HOUSE_BELONGS_TO] = detail_value # search the property category of house elif re.search(PROPERTY_CATEGORY, raw_detail): house_details_dict[PROPERTY_CATEGORY] = raw_detail.split( PROPERTY_CATEGORY_SPLITOR)[-1] # search the green rate elif re.search(GREEN_RATE, raw_detail): house_details_dict[GREEN_RATE] = raw_detail.split( GREEN_RATE_SPLITOR)[-1] # search the volume rate elif re.search(VOLUME_RATE, raw_detail): house_details_dict[VOLUME_RATE] = raw_detail.split( VOLUME_RATE_SPLITOR)[-1] # search the property costs elif re.search(PROPERTY_COSTS, raw_detail): house_details_dict[PROPERTY_COSTS] = raw_detail.split( PROPERTY_COSTS_SPLITOR)[-1] # Judge if all details are contained. # If not, set to null. house_details_dict_keys = house_details_dict.keys() for detail_name in DETAIL_LIST: if detail_name not in house_details_dict_keys: house_details_dict[detail_name] = NO_INFO_NOW houses_dict_list.append(house_details_dict) return houses_dict_list 【3】通过高德地图api获取经纬度信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100""" &lt;A toolto transfer position.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;."""def Geocode(address): """ A tool that call the God-Map api. Arguments: address: the address to transfer. Returns: location: the transfered location. """ CITY_NAME = '上海' parameters = &#123;'address': address, 'key': 'your key', 'city': CITY_NAME&#125; base = 'http://restapi.amap.com/v3/geocode/geo' try: response = requests.get(base, parameters) except Exception as e: print('error!', e) finally: pass answer = response.json() return answerdef GETGodMapLocation(houses): """ Get the location that corresponds to the house name. Use the God-Map api to get the corresponding location. Arguments: houses_dict_list: the houses info. Returns: houses_dict_list_contains_loc: the houses info that contains the location info. """ HOUSE_NAME = '小区名称' HOUSE_LOCATION = '经纬度' NO_INFO_NOW = '暂无信息' houses_dict_list = houses.copy() error_count = 0 count = 0 size = len(houses) for house_dict in houses_dict_list: # Count count = count + 1 # Loading needs if count % 1000 == 0: print(count, '/', size) address = house_dict[HOUSE_NAME] answer = Geocode(address) # print(answer) # If find if len(answer['geocodes']) != 0: # print(address + "的经纬度：", answer['geocodes'][0]['location']) house_dict[HOUSE_LOCATION] = answer['geocodes'][0]['location'] else: # remaking the invalid address # print('address remaking...') if re.search(r'别墅', address): re.sub(r'别墅', '', address) else: address = address + '小区' # print('retransfering...') # transfer again answer = Geocode(address) if len(answer['geocodes']) != 0: # print(address + "的经纬度：", answer['geocodes'][0]['location']) house_dict[HOUSE_LOCATION] = answer['geocodes'][0]['location'] else: # print(address) error_count += 1 house_dict[HOUSE_LOCATION] = NO_INFO_NOW print('error counts: ', error_count) return houses_dict_list 【4】存储成excel文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344""" &lt;A tool to save the excel file.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;.""" def Save2ExcelFile(houses): """ Save the python based list file to excel file. Arguments: houses: the houses list. """ houses_dict_list = houses.copy() house_list = [] # format the source data to fit the xlwt package keys = houses[0].keys() for key in keys: house = [] house.append(key) for house_dict in houses_dict_list: house.append(house_dict[key]) house_list.append(house) # return house_list xls = ExcelWrite.Workbook() sheet = xls.add_sheet('小区信息') for i in range(len(house_list)): for j in range(len(house_list[0])): sheet.write(j, i, house_list[i][j]) xls.save('houses.xls') 结果展示]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python_god_web_api]]></title>
      <url>%2F2017%2F02%2F17%2Fpython-god-web-api%2F</url>
      <content type="text"><![CDATA[http://lbs.amap.com/api/webservice/guide/api/search 123456789101112131415161718#!/usr/bin/env python3#-*- coding:utf-8 -*-'''利用高德地图api实现地址和经纬度的转换'''import requestsdef geocode(address): parameters = &#123;'address': address, 'key': 'e798a5bfb344a09977b79552ae415974'&#125; base = 'http://restapi.amap.com/v3/geocode/geo' response = requests.get(base, parameters) answer = response.json() print(address + "的经纬度：", answer['geocodes'][0]['location'])if __name__=='__main__': #address = input("请输入地址:") address = '北京市海淀区' geocode(address) 12345678910111213141516171819202122232425262728293031import xlrddef readXlsx(self, filename='CenterBottom2013.xlsx', sheetname='Sheet1'): rawData = [] if (os.path.isfile(self.fn_rawDat)): with open(self.fn_rawDat, 'rb') as f: self.rawDat = np.load(f) else: workBook = xlrd.open_workbook(filename) bookSheet = workBook.sheet_by_name(sheetname) # 从第二行开始读取，因为第一行有标签 for row in range(1, bookSheet.nrows): rowData = [] for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = cel.value except: pass if type(val) == float: val = float(val) else: val = str(val) rowData.append(val) rawData.append(rowData) self.rawDat = np.array(rawData) with open(self.fn_rawDat, 'wb') as f: np.save(f, self.rawDat) return self.rawDat Read Excel files Transfer the address to locaion info Put back]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Decision tree]]></title>
      <url>%2F2017%2F02%2F15%2FDecision-tree%2F</url>
      <content type="text"><![CDATA[决策树（ID3）决策树的构建构造决策树时，所需要解决的第一个问题就是，每划分一个分支时，应该根据哪一维特征进行划分。这时候我们需要定义某种指标，然后对每一维特征进行该指标的评估，最后选择指标值最高的特征进行划分。 划分完毕之后，原始数据集就被划分为几个数据子集。如果某一个下的数据属于同一类型，则算法停止；否则，重复划分过程。 伪代码（创建分支）1234567891011createbranch()检测数据集中的每个子项是否属于同一分类: If so return 类标签; Else 寻找划分数据集的最好特征 划分数据集 创建分支节点 for 每个划分的子集 调用函数createBranch并增加返回结果到分支节点中 return 分支节点 那么接下来的重点便是如何寻找划分数据集的最好特征，在这里我们使用ID3算法中使用的划分数据集的方法，也即根据熵来划分。 信息增益熵划分数据的核心思想是将无序的数据变得更加有序。而一个数据有序程度可以进行量化表示，也就是信息，其度量方式就是熵。其显然，数据集划分前后其所含的信息会发生变化，这个变化便称为信息增益。 熵定义为信息的期望，其中信息的定义如下，信息一般针对的对象为多个类别中的某一个类别： l(x_i) = -log_{2}p(x_i)其中$x_i$表示某一类别，$p(x_i)$表示从多个类别中选择该类别的概率。 接下来，熵的定义如下： H = \sum_{i=1}^{n}p(x_i)l(x_i)=-\sum_{i=1}^{n}p(x_i)log_{2}p(x_i)信息增益定义如下： IG(S|T) = H(S) - \sum_{value(T)} \frac{|S_v|}{|S|} H(S_v)其中$S$ 为全部样本集合，$value(T) $是属性 $T$所有取值的集合，$v$ 是 $T$ 的其中一个属性值，$S_v$是 $S$ 中属性 $T$ 的值为 $v$ 的样例集合，$|S_v|$ 为 $S_v$ 中所含样例数，$|S|$ 为 $S$ 中所含样例数。 代码实现： 123456789101112131415161718192021222324252627from math import logdef CalcShannonEnt(data_set): """ Calculate the Shannon Entropy. Arguments: data_set: The object dataset. Returns: shannon_ent: The Shannon entropy of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Shannon entropy shannon_ent = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries shannon_ent -= prob * log(prob, 2) return shannon_ent 为了进行测试，以及之后的算法运行，我们写一个十分naive的数据生成方法： 123456789101112131415def CreateDataSet(): """ A naive data generation method. Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] return data_set, labels 注意，这里的labels并不代表分类标签，yes以及no才是，labels代表特征名。 下面进行一个简单的demo: 123456789In [22]: import treesIn [23]: my_dat, labels = trees.CreateDataSet()In [24]: my_datOut[24]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [25]: trees.CalcShannonEnt(my_dat)Out[25]: 0.9709505944546686 熵越高，表明数据集中类别数越多。 另一个度量无序程度的方法是基尼不纯度（Gini impurity）。 基尼不纯度基尼不纯度的定义为，对于每一个节点，从所有类别标签中随机选择一个，选择出来的类别标签与其本身的类别标签不一致的概率之和。形式化地定义如下： G = \sum_{i \ne j}p(x_i)p(x_j) = \sum_{i}p(x_i)\sum_{j \ne i}p(x_j) = \sum_{i}p(x_i)(1-p(x_i)) = \sum_{i}p(x_i) - \sum_{i}(p(x_i))^2 = 1 - \sum_{i}(p(x_i))^2代码实现如下： 1234567891011121314151617181920212223242526def CalcGiniImpurity(data_set): """ Calculate the Gini impurity. Arguments: data_set: The object dataset. Returns: gini_impurity: The Gini impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Gini impurity gini_impurity = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries gini_impurity += pow(prob, 2) gini_impurity = 1 - gini_impurity return gini_impurity 同样进行一个简单的demo： 123456789In [4]: import treesIn [5]: my_dat, labels = trees.CreateDataSet()In [6]: my_datOut[6]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [7]: trees.CalcGiniImpurity(my_dat)Out[7]: 0.48 最后再介绍一种度量无序程度的方式，误分类不纯度。 误分类不纯度定义如下： M = 1 - \max_{i}(p(x_i))代码实现如下： 123456789101112131415161718192021222324def CalcMisClassifyImpurity(data_set): """ Calculate the misclassification impurity. Arguments: data_set: The object dataset. Returns: mis_classify_impurity: The misclassification impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the misclassification impurity mis_classify_impurity = 0.0 max_prob = max(label_counts.values()) / num_entries mis_classify_impurity = 1 - max_prob return mis_classify_impurity 进行一个简单的demo: 12345678910In [25]: reload(trees)Out[25]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [26]: my_dat, labels = trees.CreateDataSet()In [27]: my_datOut[27]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [28]: trees.CalcMisClassifyImpurity(my_dat)Out[28]: 0.4 最后用一个图来总结一下这三种不纯度度量的函数图像（以二类情况为例） [Ref: http://www.cse.msu.edu/~cse802/DecisionTrees.pdf]： 数据划分根据以上，数据划分的思路是，基于每一维特征的每一个值进行划分，并计算划分前后的信息增益，最后选取增益最大的特征及其所对应的值进行划分，由于这里运用的是ID3算法，因此选择的信息度量方式是熵。 代码实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def SplitDataSet(data_set, axis, value): """ Split the data set according to the given axis and correspond value. Arguments: data_set: Object data set. axis: The split-feature index. value: The value of the split-feature. Returns: ret_data_set: The splited data set. """ ret_data_set = [] for feat_vec in data_set: if feat_vec[axis] == value: reduced_feat_vec = feat_vec[:axis] reduced_feat_vec.extend(feat_vec[axis + 1:]) ret_data_set.append(reduced_feat_vec) return ret_data_setdef ChooseBestFeatureToSplit(data_set): """ Choose the best feature to split. Arguments: data_set: Object data set. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannonEnt(data_set) best_info_gain = 0.0 best_feature = -1 for i in range(num_features): # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) new_entropy = 0.0 # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannonEnt(sub_data_set) # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_feature 由以上代码（ID3算法）可以看出，其计算熵的依据是根据最后一个特征，是否这种naive的选取方式能够达到平均的最好结果？另外，其划分依据仅仅根据划分一次后的子数据集的熵之和，属于一种贪心策略，这样是否可以达到最优解？ 递归构建决策树既然是递归算法，那么必须设定递归结束条件： 遍历完所有属性 每个分支下的数据都属于相同的分类 这里存在一个问题，如果遍历完所有属性后，某些分支下还是存在多个分类，这种情况下一般采用多数表决的方式，代码实现方式如下： 12345678910111213141516171819202122def majority_cnt(class_list): """ Decided the final class. When the splited data is not belongs to the same class while all feature is handled, the final class is decided by the majority class. Arguments: class_list: The class list of the splited data set. Returns: sorted_class_count[0][0]: The majority class. """ class_count = &#123;&#125; for vote in class_list: if vote not in class_count.keys(): class_count[vote] = 0 class_count[vote] += 1 sorted_class_count = sorted( class_count.iteritems(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0] 下面进行树的创建： 1234567891011121314151617181920212223242526272829303132333435def CreateTree(data_set, labels): """ Create decision tree. Arguments: data_set: The object data set. labels: The feature labels in the data_set. Returns: my_tree: A dict that represents the decision tree. """ class_list = [example[-1] for example in data_set] # If the classes are fully same if class_list.count(class_list[0]) == len(class_list): return class_list[0] # If all feature is handled if len(data_set[0]) == 1: return majority_cnt(class_list) # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set) best_feat_label = labels[best_feat] # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Get the next step labels parameter del(labels[best_feat]) # Next step start feat_values = [example[best_feat] for example in data_set] unique_vals = set(feat_values) for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels) return my_tree 下面进行一下简单的测试： 123456789In [27]: reload(trees)Out[27]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [28]: my_dat, labels = trees.CreateDataSet()In [29]: my_tree = trees.CreateTree(my_dat, labels)In [30]: my_treeOut[30]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125; 可见决策树已经构造成功（图形化如下所示），但是这显然不够，我们需要的是用决策树进行分类。 决策树分类demo如下： 123456789101112131415161718192021In [63]: reload(trees)Out[63]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [64]: my_dat, labels = trees.CreateDataSet()In [65]: labelsOut[65]: ['no surfacing', 'flippers']In [66]: my_tree = trees.CreateTree(my_dat, labels)In [67]: labelsOut[67]: ['no surfacing', 'flippers']In [68]: my_treeOut[68]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;In [69]: trees.Classify(my_tree, labels, [1, 0])Out[69]: 'no'In [70]: trees.Classify(my_tree, labels, [1, 1])Out[70]: 'yes' C4.5C4.5算法是由ID3算法引申而来，主要改进有以下两点： 选取最优分裂属性时根据信息增益率 (IGR) 使算法对连续变量兼容 下面分别对分裂信息以及信息增益率进行定义： IGR = \frac{IG}{IV}因此只需对ID3算法的代码做一些改动即可，为了兼容ID3， 具体实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def ChooseBestFeatureToSplit(data_set, flag='ID3'): """ Choose the best feature to split. Arguments: data_set: Object data set. flag: Decide if use the infomation gain rate or not. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannon(data_set) method = 'ID3' best_feature = -1 best_info_gain = 0.0 best_info_gain_rate = 0.0 for i in range(num_features): new_entropy = 0.0 # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) if len(unique_vals) &gt; 3: method = 'C4.5' if method == 'ID3': # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannon(sub_data_set) else: data_set = np.array(data_set) sorted_feat = np.argsort(feat_list) for index in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [index + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; new_entropy: new_entropy = iv # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if flag == 'C4.5': info_gain_rate = info_gain / new_entropy # print('index', i, 'info_gain_rate', info_gain_rate) if info_gain_rate &gt; best_info_gain_rate: best_info_gain_rate = info_gain_rate best_feature = i if flag == 'ID3': if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_feature 下面需要解决的问题是连续变量的问题，为了实验的方便，我们更改一下naive的数据生成方法（Ref: http://blog.csdn.net/lemon_tree12138/article/details/51840361 ）： 1234567891011121314151617181920212223242526272829303132333435363738def CreateDataSet(method='ID3'): """ A naive data generation method. Arguments: method: The algorithm class Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) if method == 'ID3': data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] else: data_set = [[85, 85, 'no'], [80, 90, 'yes'], [83, 78, 'no'], [70, 96, 'no'], [68, 80, 'no'], [65, 70, 'yes'], [64, 65, 'yes'], [72, 95, 'no'], [69, 70, 'no'], [75, 80, 'no'], [75, 70, 'yes'], [72, 90, 'yes'], [81, 75, 'no'], [71, 80, 'yes']] labels = ['temperature', 'humidity'] return data_set, labels 假设我们选择了温度属性，则被提取的关键数据为： [[85, No], [80, No], [83, Yes], [70, Yes], [68, Yes], [65, No], [64, Yes], [72, No], [69, Yes], [75, Yes], [75, Yes], [72, Yes], [81, Yes], [71, No]] 现在我们对这批数据进行从小到大进行排序，排序后数据集就变成： [[64, Yes], [65, No], [68, Yes], [69, Yes], [70, Yes], [71, No], [72, No], [72, Yes], [75, Yes], [75, Yes], [80, No], [81, Yes], [83, Yes], [85, No]] 绘制成如下图例： 当我们拿到一个已经排好序的（温度，结果）的列表之后，分别计算被某个单元分隔的左边和右边的分裂信息，比如现在计算 index = 4 时的分裂信息。则： IV(v_4) = IV([4, 1], [5, 4]) = \frac{5}{14}IV([4, 1]) + \frac{9}{14}IV([5, 4])IV(v_4) = \frac{5}{14}(-\frac{4}{5} \log_{2} \frac{4}{5} - \frac{1}{5} \log_{2} \frac{1}{5}) + \frac{9}{14}(-\frac{5}{9} \log_{2} \frac{5}{9} - \frac{4}{9} \log_{2} \frac{4}{9})下图表示了不同分裂位置所得到的分裂信息： 最后给出完整的代码实现 (最后的Classify方法还需修改)： trees.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383from math import logimport operatorimport numpy as npdef CalcShannon(data_set): """ Calculate the Shan0n Entropy. Arguments: data_set: The object dataset. Returns: shan0n_ent: The Shan0n entropy of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # print(label_counts) # Calculates the Shan0n entropy shan0n_ent = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries shan0n_ent -= prob * log(prob, 2) return shan0n_entdef CalcGiniImpurity(data_set): """ Calculate the Gini impurity. Arguments: data_set: The object dataset. Returns: gini_impurity: The Gini impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Gini impurity gini_impurity = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries gini_impurity += pow(prob, 2) gini_impurity = 1 - gini_impurity return gini_impuritydef CalcMisClassifyImpurity(data_set): """ Calculate the misclassification impurity. Arguments: data_set: The object dataset. Returns: mis_classify_impurity: The misclassification impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the misclassification impurity mis_classify_impurity = 0.0 max_prob = max(label_counts.values()) / num_entries mis_classify_impurity = 1 - max_prob return mis_classify_impuritydef CreateDataSet(method='ID3'): """ A naive data generation method. Arguments: method: The algorithm class Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) if method == 'ID3': data_set = [[1, 1, 1], [1, 1, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0]] labels = ['0 surfacing', 'flippers'] else: data_set = [[1, 85, 85, 0, 0], [1, 80, 90, 1, 0], [2, 83, 78, 0, 1], [3, 70, 96, 0, 1], [3, 68, 80, 0, 1], [3, 65, 70, 1, 0], [2, 64, 65, 1, 1], [1, 72, 95, 0, 0], [1, 69, 70, 0, 1], [3, 75, 80, 0, 1], [1, 75, 70, 1, 1], [2, 72, 90, 1, 1], [2, 81, 75, 0, 1], [3, 71, 80, 1, 0]] labels = ['outlook', 'temperature', 'humidity', 'windy'] return data_set, labelsdef SplitDataSet(data_set, axis, value): """ Split the data set according to the given axis and correspond value. Arguments: data_set: Object data set. axis: The split-feature index. value: The value of the split-feature. Returns: ret_data_set: The splited data set. """ ret_data_set = [] for feat_vec in data_set: if feat_vec[axis] == value: reduced_feat_vec = feat_vec[:axis] reduced_feat_vec.extend(feat_vec[axis + 1:]) ret_data_set.append(reduced_feat_vec) return ret_data_setdef ChooseBestFeatureToSplit(data_set, flag='ID3'): """ Choose the best feature to split. Arguments: data_set: Object data set. flag: Decide if use the infomation gain rate or not. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannon(data_set) method = 'ID3' best_feature = -1 best_info_gain = 0.0 best_info_gain_rate = 0.0 for i in range(num_features): new_entropy = 0.0 # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) if len(unique_vals) &gt; 3: method = 'C4.5' if method == 'ID3': # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannon(sub_data_set) else: data_set = np.array(data_set) sorted_feat = np.argsort(feat_list) for index in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [index + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; new_entropy: new_entropy = iv # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if flag == 'C4.5': info_gain_rate = info_gain / new_entropy # print('index', i, 'info_gain_rate', info_gain_rate) if info_gain_rate &gt; best_info_gain_rate: best_info_gain_rate = info_gain_rate best_feature = i if flag == 'ID3': if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_featuredef majority_cnt(class_list): """ Decided the final class. When the splited data is 0t belongs to the same class while all feature is handled, the final class is decided by the majority class. Arguments: class_list: The class list of the splited data set. Returns: sorted_class_count[0][0]: The majority class. """ class_count = &#123;&#125; for vote in class_list: if vote not in class_count.keys(): class_count[vote] = 0 class_count[vote] += 1 sorted_class_count = sorted( class_count. items(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0]def CreateTree(data_set, feat_labels, method='ID3'): """ Create decision tree. Arguments: data_set: The object data set. labels: The feature labels in the data_set. method: The algorithm class. Returns: my_tree: A dict that represents the decision tree. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) labels = feat_labels.copy() class_list = [example[-1] for example in data_set] # print(class_list) # If the classes are fully same print('class_list', class_list) if class_list.count(class_list[0]) == len(class_list): return class_list[0] # If all feature is handled if len(data_set[0]) == 1: return majority_cnt(class_list) if method == 'ID3': # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set) best_feat_label = labels[best_feat] # print(best_feat_label) # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Next step start feat_values = [example[best_feat] for example in data_set] # Get the next step labels parameter del(labels[best_feat]) unique_vals = set(feat_values) for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels) return my_tree else: flag = 'ID3' # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set, 'C4.5') best_feat_label = labels[best_feat] print(best_feat_label) # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Next step start feat_values = [example[best_feat] for example in data_set] del(labels[best_feat]) unique_vals = set(feat_values) if len(unique_vals) &gt; 3: flag = 'C4.5' if flag == 'ID3': for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels, 'C4.5') return my_tree else: data_set = np.array(data_set) best_iv = 0.0 best_split_value = -1 sorted_feat = np.argsort(feat_values) for i in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [i + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; best_iv: best_iv = iv best_split_value = feat_values[sorted_feat[i]] print(best_feat, best_split_value) # print(best_split_value) left_data_set = data_set[ data_set[:, best_feat] &lt;= best_split_value] left_data_set = np.delete(left_data_set, best_feat, axis=1) # if len(left_data_set) == 1: # return left_data_set[0][-1] right_data_set = data_set[ data_set[:, best_feat] &gt; best_split_value] right_data_set = np.delete(right_data_set, best_feat, axis=1) # if len(right_data_set) == 1: # return right_data_set[0][-1] sub_labels = labels[:] my_tree[best_feat_label][ '&lt;=' + str(best_split_value)] = CreateTree( left_data_set.tolist(), sub_labels, 'C4.5') my_tree[best_feat_label][ '&gt;' + str(best_split_value)] = CreateTree( right_data_set.tolist(), sub_labels, 'C4.5') # print('continious tree', my_tree) return my_treedef Classify(input_tree, feat_labels, test_vec): """ Classify that uses the given decision tree. Arguments: input_tree: The Given decision tree. feat_labels: The labels of correspond feature. test_vec: The test data. Returns: class_label: The class label that corresponds to the test data. """ # Get the start feature label to split first_str = list(input_tree.keys())[0] # Get the sub-tree that corresponds to the start feature to split second_dict = input_tree[first_str] # Get the feature index that the label is the start feature label feat_index = feat_labels.index(first_str) # Start recurrence search for key in second_dict.keys(): if test_vec[feat_index] == key: if type(second_dict[key]).__name__ == 'dict': # Recurrence calls class_label = Classify(second_dict[key], feat_labels, test_vec) else: class_label = second_dict[key] return class_label 一个小demo： 123456789101112131415161718192021222324252627282930313233In [108]: reload(trees)Out[108]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [109]: my_dat, labels = trees.CreateDataSet('C4.5')In [110]: my_tree_c = trees.CreateTree(my_dat, labels, 'C4.5')class_list [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]outlookclass_list [0, 0, 0, 1, 1]humidity1 90class_list [0, 0, 1, 1]temperature0 69class_list [1]class_list [0, 0, 1]windyclass_list [0]class_list [0, 1]class_list [0]class_list [1, 1, 1, 1]class_list [1, 1, 0, 1, 0]windyclass_list [1, 1, 1]class_list [0, 0]In [111]: my_tree_cOut[111]:&#123;'outlook': &#123;1: &#123;'humidity': &#123;'&lt;=90': &#123;'temperature': &#123;'&lt;=69': 1, '&gt;69': &#123;'windy': &#123;0: 0, 1: 0&#125;&#125;&#125;&#125;, '&gt;90': 0&#125;&#125;, 2: 1, 3: &#123;'windy': &#123;0: 1, 1: 0&#125;&#125;&#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kNN and kd-tree]]></title>
      <url>%2F2017%2F02%2F15%2FkNN-and-kd-tree%2F</url>
      <content type="text"><![CDATA[k-近邻算法工作原理存在一组带标签的训练集[1]，每当有新的不带标签的样本[2]出现时，将训练集中数据的特征与测试集的特征逐个比较，通过某种测度来提取出与测试集最相似的k个训练集样本，然后将这k个样本中占大多数[4]的标签赋予测试集样本。 伪代码对测试集中的每个点依次执行如下操作： 计算训练集中的每个点与当前点的距离 按照距离递增次序排序 在排序好的点中选取前k个点 统计出k个点中不同类别的出现频率 选择频率最高的类别为当前点的预测分类 ​ 代码实现首先创建测试数据集 1from numpy import * 1234def createDataset(): group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labels 返回预测分类 123456789101112131415def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] diffMat = tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 sortedDistIndices = distances.argsort() classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndices[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 # the return of sorted() is a list and its item is a tuple sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0] # returns the predict class label 进一步探索k-近邻算法的缺点在于当数据量很大时，拥有不可接受的空间复杂度以及时间复杂度 其次该算法最关键的地方在与超参k的选取。当k选取的过小时容易造成过拟合，反之容易造成欠拟合。考虑两个极端情况，当k=1时，该算法又叫最近邻算法；当k=N[3]时，表示直接从原始数据中选取占比最大的类别，显然这个算法太naive了。 为了解决kNN算法时间复杂度的问题，最关键的便是在于如何对数据进行快速的k近邻搜索，一种解决方法是引入kd树来进行加速。 kd树简介以二维空间为例，假设有6个二维数据点{(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2)}，可以用下图来表明kd树所能达到的效果: kd树算法主要分为两个部分： kd树数据结构的建立 在kd树上进行查找 kd树是一种对k维空间上的数据点进行存储以便进行高效查找的树形数据结构，属于二叉树。构造kd树相当于不断用垂直于坐标轴的超平面对k维空间进行划分，构成一系列k维超矩形区域。kd树的每一个结点对应一个超矩形区域，表示一个空间范围。 数据结构下表给出每个结点主要包含的数据结构: 域名 数据类型 描述 Node-data 数据矢量 数据集中的某个数据点，k维矢量 Split 整数 垂直于分割超平面的方向轴序号 Left kd树 由位于该节点分割超平面左子空间内所有数据点构成的kd树 Right kd树 由位于该节点分割超平面右子空间内所有数据点构成的kd树 建立树伪代码下面给出构建kd树的伪代码： 算法：构建k-d树（createKDTree） 输入：数据点集Data-set 输出：Kd，类型为k-d tree 1. If Data-set为空，则返回空的k-d tree 2. 调用节点生成程序： （1）确定split域：对于所有描述子数据（特征矢量），统计它们在每个维上的数据方差。以SURF特征为例，描述子为64维，可计算64个方差。挑选出最大值，对应的维就是split域的值。数据方差大表明沿该坐标轴方向上的数据分散得比较开，在这个方向上进行数据分割有较好的分辨率； （2）确定Node-data域：数据点集Data-set按其第split域的值排序。位于正中间的那个数据点被选为Node-data。此时新的Data-set’ = Data-set\Node-data（除去其中Node-data这一点）。 3. dataleft = {d属于Data-set’ &amp;&amp; d[split] ≤ Node-data[split]} dataright = {d属于Data-set’ &amp;&amp; d[split] &gt; Node-data[split]} 4. left = 由（dataleft）建立的k-d tree，即递归调用createKDTree（dataleft）并设置left的parent域为Kd； right = 由（dataright）建立的k-d tree，即调用createKDTree（dataleft）并设置right的parent域为Kd。 实例用最开始的6个二维数据点的例子，来具体化这个过程： 确定split域的首先该取的值。分别计算x，y方向上数据的方差得知x方向上的方差最大，所以split域值首先取0，也就是x轴方向； 确定Node-data的域值。根据x轴方向的值2,5,9,4,8,7排序选出中值为7，所以Node-data = (7, 2)。这样，该节点的分割超平面就是通过(7, 2)并垂直于split = 0（x轴）的直线x = 7； 确定左子空间和右子空间。分割超平面x = 7将整个空间分为两部分，如下图所示。x &lt; = 7的部分为左子空间，包含3个节点{(2, 3), (5, 4), (4, 7)}；另一部分为右子空间，包含2个节点{(9, 6), (8, 1)}。 如算法所述，k-d树的构建是一个递归的过程。然后对左子空间和右子空间内的数据重复根节点的过程就可以得到下一级子节点（5,4）和（9,6）（也就是左右子空间的’根’节点），同时将空间和数据集进一步细分。如此反复直到空间中只包含一个数据点，如图1所示。最后生成的k-d树如下图所示。 注意：每一级节点旁边的’x’和’y’表示以该节点分割左右子空间时split所取的值。 这里进行一点补充说明，kd树其实就是二叉树，其与普通的二叉查找树不同之处在于，其每一层根据split的维度进行二叉拆分。具体来说，根据上图，第一层的拆分是根据x，那么其左孩子的x值就小于根结点的x值，右孩子则反之。y值则没有规定（这里出现的左大右小只是纯粹的巧合）。第二层是根据y值来进行split，因此第三层的规律显而易见。 代码实现运行环境：Windows 10 Pro 64-bit x64-based(Ver. 10.0.14393), Python 3.5.2, Anaconda 4.1.1(64-bit), IPython 5.0.0, Windows CMD, kdTreeCreate.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import numpy as npfrom kdTreeNode import *def createDataSet(): """ Create the test dataset. Returns: A numpy array that contains the test data. """ dataSet = np.array([[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]) return dataSetdef split(dataSet): """ Split the given dataset. Returns: LeftDataSet: A kdTreeNode object. RightDataSet: A kdTreeNode object. NodeData: A tuple. """ # Ensure the dimension to split dimenIndex = np.var(dataSet, axis=0).argmax() partitionDataSet = dataSet[:, dimenIndex] # print(partitionDataSet) # Ensure the position to split partitionDataSetArgSort = partitionDataSet.argsort() # print(partitionDataSetArgSort) lenOfPartitionDataSetArgSort = len(partitionDataSetArgSort) if lenOfPartitionDataSetArgSort % 2 == 0: posIndex = lenOfPartitionDataSetArgSort // 2 splitIndex = partitionDataSetArgSort[posIndex] else: posIndex = (lenOfPartitionDataSetArgSort - 1) // 2 splitIndex = partitionDataSetArgSort[posIndex] # print(splitIndex) # Split nodeData = dataSet[splitIndex] leftIndeies = partitionDataSetArgSort[:posIndex] rightIndeies = partitionDataSetArgSort[posIndex + 1:] leftDataSet = dataSet[leftIndeies] rightDataSet = dataSet[rightIndeies] return nodeData, dimenIndex, leftDataSet, rightDataSetdef createKDTree(dataSet): """ Create the KD tree. Returns: A kdTreeNode object. """ if len(dataSet) == 0: return nodeData, dimenIndex, leftDataSet, rightDataSet = split(dataSet) # print(nodeData, dimenIndex, leftDataSet, rightDataSet) node = kdTreeNode(nodeData, dimenIndex) node.setLeft(createKDTree(leftDataSet)) node.setRight(createKDTree(rightDataSet)) return nodedef midTravel(node): if node is None: return midTravel(node.getLeft()) print(node.getData()) midTravel(node.getRight())if __name__ == "__main__": dataSet = createDataSet() node = createKDTree(dataSet) midTravel(node) kdTreeNode.py 123456789101112131415161718192021222324252627282930313233class kdTreeNode(object): """ Class of k-d tree nodes """ def __init__(self, data=None, split=None, left=None, right=None): self.__data = data self.__split = split self.__left = left self.__right = right def getData(self): return self.__data def setData(self, data): self.__data = data def getSplit(self): return self.__split def setSplit(self, split): self.__split = split def getLeft(self): return self.__left def setLeft(self, left): self.__left = left def getRight(self): return self.__right def setRight(self, right): self.__right = right 运行结果： 12345678In [1]: run kdTreeCreate.py-------------------------------------[2 3][5 4][4 7][7 2][8 1][9 6] 时间复杂度： N个K维数据进行查找操作时时间复杂度为 $t=O(KN^{2})$ 下面就要在已经建立好的kd树上进行查找操作。 查找kd树中进行的查找与普通的查找操作存在较大的差异，其目的是为了找出与查询点距离最近的点。 星号表示要查询的点(2.1, 3.1)。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点(2, 3)。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行’回溯’操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从(7, 2)点开始进行二叉查找，然后到达(5, 4)，最后到达(2, 3)，此时搜索路径中的节点为&lt;(7, 2), (5, 4), (2, 3)&gt;，首先以(2, 3)作为当前最近邻点，计算其到查询点(2.1, 3.1)的距离为0.1414，然后回溯到其父节点(5, 4)，并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以(2.1, 3.1)为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入(5, 4)节点右子空间中去搜索。 再回溯到(7, 2)，以(2.1, 3.1)为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入(7, 2)右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点(2, 3)，最近距离为0.1414。 一个复杂点了例子如查找点为(2, 4.5)。同样先进行二叉查找，先从(7, 2)查找到(5, 4)节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到(4, 7)，形成搜索路径&lt;(7, 2), (5, 4), (4, 7)&gt;，取(4, 7)为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到(5, 4)，计算其与查找点之间的距离为3.041。以(2, 4.5)为圆心，以3.041为半径作圆。 可见该圆和y = 4超平面交割，所以需要进入(5, 4)左子空间进行查找。此时需将(2, 3)节点加入搜索路径中得&lt;(7, 2), (2, 3)&gt;。回溯至(2, 3)叶子节点，(2, 3)距离(2, 4.5)比(5, 4)要近，所以最近邻点更新为(2, 3)，最近距离更新为1.5。回溯至(7, 2)，以(2, 4.5)为圆心1.5为半径作圆，并不和x = 7分割超平面交割。至此，搜索路径回溯完。返回最近邻点(2, 3)，最近距离1.5。 k-d树查询算法的伪代码如下所示。 查找伪代码 算法： k-d树最邻近查找 输入：Kd， //k-d tree类型 target //查询数据点 输出：nearest， //最邻近数据点 dist //最邻近数据点和查询点间的距离 123456789101112131415161718192021222324252627282930311. If Kd为NULL，则设dist为infinite并返回2. //进行二叉查找，生成搜索路径 Kd_point = &amp;Kd； //Kd-point中保存k-d tree根节点地址 nearest = Kd_point -&gt; Node-data； //初始化最近邻点 while（Kd_point） push（Kd_point）到search_path中； //search_path是一个堆栈结构，存储着搜索路径节点指针 /*** If Dist（nearest，target） &gt; Dist（Kd_point -&gt; Node-data，target） nearest = Kd_point -&gt; Node-data； //更新最近邻点 Max_dist = Dist(Kd_point，target）； //更新最近邻点与查询点间的距离 ***/ s = Kd_point -&gt; split； //确定待分割的方向 If target[s] &lt;= Kd_point -&gt; Node-data[s] //进行二叉查找 Kd_point = Kd_point -&gt; left； else Kd_point = Kd_point -&gt;right； nearest = search_path中最后一个叶子节点； //注意：二叉搜索时不比计算选择搜索路径中的最邻近点，这部分已被注释 Max_dist = Dist（nearest，target）； //直接取最后叶子节点作为回溯前的初始最近邻点 3. //回溯查找 while（search_path != NULL） back_point = 从search_path取出一个节点指针； //从search_path堆栈弹栈 s = back_point -&gt; split； //确定分割方向 If Dist（target[s]，back_point -&gt; Node-data[s]） &lt; Max_dist //判断还需进入的子空间 If target[s] &lt;= back_point -&gt; Node-data[s] Kd_point = back_point -&gt; right； //如果target位于左子空间，就应进入右子空间 else Kd_point = back_point -&gt; left; //如果target位于右子空间，就应进入左子空间 将Kd_point压入search_path堆栈； If Dist（nearest，target） &gt; Dist（Kd_Point -&gt; Node-data，target） nearest = Kd_point -&gt; Node-data； //更新最近邻点 Min_dist = Dist（Kd_point -&gt; Node-data,target）； //更新最近邻点与查询点间的距离 代码实现kdTreeSearch.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import numpy as npdef cal_dist(node, target): """ Calculate the distance between the node and the target. Arguments: node: The kd-tree's one node. target: Search target. Returns: dist: The distance between the two nodes. """ node_data = np.array(node) target_data = np.array(target) square_dist_vector = (node_data - target_data) ** 2 square_dist = np.sum(square_dist_vector) dist = square_dist ** 0.5 return distdef search(root_node, target): """ Search the nearest node of the target node in the kd-tree that root node is the root_node Arguments: root_node: The kd-tree's root node. target: Search target. Returns: nearest: The nearest node of the target node in the kd-tree. min_dist: The nearest distance. """ if root_node is None: min_dist = float('inf') return min_dist # Two-fork search kd_point = root_node # Save the root node nearest = kd_point.getData() # Initial the nearest node search_path = [] # Initial the search stack while kd_point: search_path.append(kd_point) split_index = kd_point.getSplit() # Ensure the split path if target[split_index] &lt;= kd_point.getData()[split_index]: kd_point = kd_point.getLeft() else: kd_point = kd_point.getRight() nearest = search_path.pop().getData() min_dist = cal_dist(nearest, target) # Retrospect search while search_path: back_point = search_path.pop() # Ensure the back-split path back_split_index = back_point.getSplit() # Judge if needs to enter the subspace if cal_dist(target[back_split_index], back_point.getData()[back_split_index]) &lt; min_dist: # If the target is in the left subspace, then enter the right if target[back_split_index] &lt;= back_point.getData()[back_split_index]: kd_point = back_point.getRight() # Otherwise enter the left else: kd_point = back_point.getLeft() # Add the node to the search path if kd_point is not None: search_path.append(kd_point) if cal_dist(nearest, target) &gt; cal_dist(kd_point.getData(), target): # Update the nearest node nearest = kd_point.getData() # Update the maximum distance min_dist = cal_dist(kd_point.getData(), target) return nearest, min_dist 运行结果： 12345678910111213141516171819202122232425262728293031323334In [1]: run kdTreeCreate.py-------------------------------------[2 3][5 4][4 7][7 2][8 1][9 6]In [2]: node-------------------------------------Out [2]: &lt;kdTreeNode.kdTreeNode at 0x26bff22f160&gt;In [3]: import kdTreeSearchIn [4]: nearest, min_dist = kdTreeSearch.search(node, [2.1, 3.1])In [5]: nearest-------------------------------------Out [5]: array([2, 3])In [6]: min_dist-------------------------------------Out [6]: 0.14142135623730964In [7]: nearest, min_dist = kdTreeSearch.search(node, [2, 4.5])In [8]: nearest-------------------------------------Out [8]: array([2, 3])In [9]: min_dist-------------------------------------Out [9]: 1.5 时间复杂度： N个结点的K维kd树进行查找操作时最坏时间复杂度为 $t_{worst}=O(KN^{1-1/k})$ 根据相关研究，当数据维度为K时，只有当数据量N满足 $N&gt;&gt;2^K$ 时，才能达到高效的搜索（K&lt;20，超过20维时可采用ball-tree算法），所以引出了一系列的改进算法（BBF算法，和一系列M树、VP树、MVP树等高维空间索引树），留待后续补充。 采用kd树的k-近邻算法接下来便是将两者相结合。 [1] 说是训练集其实是不准确的，因为k-近邻算法是一个无参数方法，只存在一个超参k，因此其不存在一个训练的过程 [2] 测试集 [3] N代表训练集的数目 [4] 多数表决]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F02%2F15%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
