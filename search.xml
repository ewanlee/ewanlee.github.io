<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Hadoop namenode not getting started]]></title>
      <url>%2F2017%2F04%2F18%2FHadoop-namenode-not-getting-started%2F</url>
      <content type="text"><![CDATA[First delete all contents from temporary folder: rm -rf &lt;tmp dir&gt; (my was /usr/local/hadoop/tmp)Format the namenode: bin/hadoop namenode -formatStart all processes againbin/start-dfs.shbin/start-yarn.shbin/mr-jobhistory-daemon.sh start historyserverYou may consider rolling back as well using checkpoint (if you had it enabled).]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop ch02 MapReduce notes]]></title>
      <url>%2F2017%2F04%2F17%2FHadoop-ch02-MapReduce-notes%2F</url>
      <content type="text"><![CDATA[MapReduce首先我们有一个数据集，关于天气的，然后它的每一条记录是这样的：123456789101112131415161718192021222324252627282930310057332130 # USAF weather station identifier99999 # WBAN weather station identifier19500101 # observation date0300 # observation time4+51317 # latitude (degrees x 1000)+028783 # longitude (degrees x 1000)FM-12+0171 # elevation (meters)99999V020320 # wind direction (degrees)1 # quality codeN0072100450 # sky ceiling height (meters)1 # quality codeCN010000 # visibility distance (meters)1 # quality codeN9-0128 # air temperature (degrees Celsius x 10)1 # quality code-0139 # dew point temperature (degrees Celsius x 10)1 # quality code10268 # atmospheric pressure (hectopascals x 10)1 # quality code当然以上数据是经过处理之后的，一开始它长这样：123450067011990999991950051507004...9999999N9+00001+99999999999...0043011990999991950051512004...9999999N9+00221+99999999999...0043011990999991950051518004...9999999N9-00111+99999999999...0043012650999991949032412004...0500001N9+01111+99999999999...0043012650999991949032418004...0500001N9+00781+99999999999...Hmmm….这个天气数据集按照气象站编号-年份的形式来组织的：12345678910010010-99999-1990.gz010014-99999-1990.gz010015-99999-1990.gz010016-99999-1990.gz010017-99999-1990.gz010030-99999-1990.gz010040-99999-1990.gz010080-99999-1990.gz010100-99999-1990.gz010150-99999-1990.gz这个原始数据显然用起来不方便，所以按照年份给它聚个类，用了如下方法：123456789hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \ -D mapred.reduce.tasks=0 \ -D mapred.map.tasks.speculative.execution=false \ -D mapred.task.timeout=12000000 \ -input ncdc_files.txt \ -inputformat org.apache.hadoop.mapred.lib.NLineInputFormat \ -output output \ -mapper load_ncdc_map.sh \ -file load_ncdc_map.sh然后里面用到的ncdc_files以及load_ncdc_map.sh这两个文件是这样的：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100s3n://hadoopbook/ncdc/raw/isd-1901.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1902.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1903.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1904.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1905.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1906.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1907.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1908.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1909.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1910.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1911.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1912.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1913.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1914.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1915.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1916.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1917.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1918.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1919.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1920.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1921.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1922.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1923.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1924.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1925.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1926.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1927.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1928.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1929.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1930.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1931.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1932.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1933.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1934.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1935.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1936.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1937.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1938.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1939.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1940.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1941.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1942.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1943.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1944.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1945.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1946.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1947.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1948.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1949.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1950.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1951.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1952.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1953.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1954.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1955.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1956.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1957.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1958.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1959.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1960.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1961.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1962.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1963.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1964.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1965.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1966.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1967.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1968.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1969.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1970.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1971.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1972.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1973.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1974.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1975.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1976.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1977.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1978.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1979.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1980.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1981.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1982.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1983.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1984.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1985.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1986.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1987.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1988.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1989.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1990.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1991.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1992.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1993.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1994.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1995.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1996.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1997.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1998.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1999.tar.bz2s3n://hadoopbook/ncdc/raw/isd-2000.tar.bz21234567891011121314151617181920212223242526#!/usr/bin/env bash# NLineInputFormat gives a single line: key is offset, value is S3 URIread offset s3file# Retrieve file from S3 to local diskecho "reporter:status:Retrieving $s3file" &gt;&amp;2$HADOOP_INSTALL/bin/hadoop fs -get $s3file .# Un-bzip and un-tar the local filetarget=`basename $s3file .tar.bz2`mkdir -p $targetecho "reporter:status:Un-tarring $s3file to $target" &gt;&amp;2tar jxf `basename $s3file` -C $target# Un-gzip each station file and concat into one fileecho "reporter:status:Un-gzipping $target" &gt;&amp;2for file in $target/*/*do gunzip -c $file &gt;&gt; $target.all echo "reporter:status:Processed $file" &gt;&amp;2done# Put gzipped version into HDFSecho "reporter:status:Gzipping $target and putting in HDFS" &gt;&amp;2gzip -c $target.all | $HADOOP_INSTALL/bin/hadoop fs -put - gz/$target.gz嗯…顺便说一句，这个文件是存在AWS上的，所以想用的话要有一个AWS账号，想要有个账号呢，你得先有个可以支付美刀的信用卡。Hmmmmm…其实作者给的sample data也挺好的我觉得，在这里.那么我们的问题就是说，找出每一年的最高的温度。先看看不用Hadoop的实现方法，事实证明我shell脚本还是宝刀未老的。12345678910#!/usr/bin/env bashfor year in all/*do echo -ne `basename $year .gz`"\t" gunzip -c $year | \ awk '&#123; temp = substr($0, 88, 5) + 0; q = substr($0, 93, 1); if (temp !=9999 &amp;&amp; q ~ /[01459]/ &amp;&amp; temp &gt; max) max = temp &#125; END &#123; print max &#125;'done结果如下：1234567% ./max_temperature.sh1901 3171902 2441903 2891904 2561905 283...啊嘞，还不错的样子，但是对于大数据速度还是慢了点儿，所以直接上Hadoop看看。对于以上的问题呢，MapReduce是这样解决的注意了，上面一行是hadoop的术语，下面呢，其实就是Unix的pipe了，这给我们不用Java来实现提供了可能。好了下面开始coding了，拿起键盘就是GAN为了实现我们的任务，我们需要三个java文件，一个mapper，一个reducer。这俩是苦工，还要一个监工。Mapper123456789101112131415161718192021222324252627282930313233// cc MaxTemperatureMapper Mapper for maximum temperature example// vv MaxTemperatureMapperimport java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class MaxTemperatureMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private static final int MISSING = 9999; @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String year = line.substring(15, 19); int airTemperature; if (line.charAt(87) == '+') &#123; // parseInt doesn't like leading plus signs airTemperature = Integer.parseInt(line.substring(88, 92)); &#125; else &#123; airTemperature = Integer.parseInt(line.substring(87, 92)); &#125; String quality = line.substring(92, 93); if (airTemperature != MISSING &amp;&amp; quality.matches("[01459]")) &#123; context.write(new Text(year), new IntWritable(airTemperature)); &#125; &#125;&#125;// ^^ MaxTemperatureMapperReducer123456789101112131415161718192021222324// cc MaxTemperatureReducer Reducer for maximum temperature example// vv MaxTemperatureReducerimport java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class MaxTemperatureReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int maxValue = Integer.MIN_VALUE; for (IntWritable value : values) &#123; maxValue = Math.max(maxValue, value.get()); &#125; context.write(key, new IntWritable(maxValue)); &#125;&#125;// ^^ MaxTemperatureReducerJob12345678910111213141516171819202122232425262728293031323334// cc MaxTemperature Application to find the maximum temperature in the weather dataset// vv MaxTemperatureimport org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MaxTemperature &#123; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;"); System.exit(-1); &#125; Job job = new Job(); job.setJarByClass(MaxTemperature.class); job.setJobName("Max temperature"); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTemperatureMapper.class); job.setReducerClass(MaxTemperatureReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;// ^^ MaxTemperature然后这么运行：12% export HADOOP_CLASSPATH=hadoop-examples.jar% hadoop MaxTemperature input/ncdc/sample.txt output但是如果数据量非常大的话，需要在Mapper和Reducer之间传递大量的数据，这个时候可以引入Combiner，它的机理是这样的。假如我有两个mapper，它们的输出结果是这样子的：123(1950, 0)(1950, 20)(1950, 10)以及这样子的：12(1950, 25)(1950, 15)如果没有combiner的话，它们会先变成这样子：1(1950, [0, 20, 10, 25, 15])然后作为reducer的输入，但是如果加入了combiner的话，相当于上面的问题变成了这样max(0, 20, 10, 25, 15) = max(max(0, 20, 10), max(25, 15)) = max(20, 25) = 25是不是简单多了。但是注意了，并不是所有的问题都是这样，比如下面这个问题：mean(0, 20, 10, 25, 15) = 14mean(mean(0, 20, 10), mean(25, 15)) = mean(10, 20) = 15所以说要根据具体情况来定，不能直接套用。好了我们继续combiner的话题，我们怎么把这货加到hadoop的流程中去呢，其实很简单，这样就可以：123456789101112131415161718192021222324252627282930313233343536// cc MaxTemperatureWithCombiner Application to find the maximum temperature, using a combiner function for efficiencyimport org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;// vv MaxTemperatureWithCombinerpublic class MaxTemperatureWithCombiner &#123; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("Usage: MaxTemperatureWithCombiner &lt;input path&gt; " + "&lt;output path&gt;"); System.exit(-1); &#125; Job job = new Job(); job.setJarByClass(MaxTemperatureWithCombiner.class); job.setJobName("Max temperature"); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTemperatureMapper.class); /*[*/job.setCombinerClass(MaxTemperatureReducer.class)/*]*/; job.setReducerClass(MaxTemperatureReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;// ^^ MaxTemperatureWithCombiner没错，combiner和reducer是一样的。其实仔细想想这也很自然，因为它们俩实际实现的功能是一样的。Hadoop Streaming作为一个machine learning专业的，有时候用Java还是感觉挺不爽的，哪有Python啊，Ruby啊这种脚本语言方便嘛。所以hadoop还是很人性地提供了解决方法，就是标题所表示的技术。直接看代码怎么用吧。RubyMap1234567#!/usr/bin/env rubySTDIN.each_line do |line| val = line year, temp, q = val[15,4], val[87,5], val[92,1] puts "#&#123;year&#125;\t#&#123;temp&#125;" if (temp != "+9999" &amp;&amp; q =~ /[01459]/)endReduce12345678910111213#!/usr/bin/env rubylast_key, max_val = nil, -1000000STDIN.each_line do |line| key, val = line.split("\t") if last_key &amp;&amp; last_key != key puts "#&#123;last_key&#125;\t#&#123;max_val&#125;" last_key, max_val = key, val.to_i else last_key, max_val = key, [max_val, val.to_i].max endendputs "#&#123;last_key&#125;\t#&#123;max_val&#125;" if last_key然后这样调用：12345% hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \ -input input/ncdc/sample.txt \ -output output \ -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \ -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb是不是很方便？如果要加上combiner的话，更方便了，都不用再写额外的文件：12345678% hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \ -files ch02-mr-intro/src/main/ruby/max_temperature_map.rb,\ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \ -input input/ncdc/all \ -output output \ -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \ -combiner ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \ -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb注意，以上的-files命令是为了在集群环境下运行时，将脚本复制到各子节点上。Python啊，Python大大出场，其实和Ruby没啥区别。Map12345678910#!/usr/bin/env pythonimport reimport sysfor line in sys.stdin: val = line.strip() (year, temp, q) = (val[15:19], val[87:92], val[92:93]) if (temp != "+9999" and re.match("[01459]", q)): print "%s\t%s" % (year, temp)Reduce123456789101112131415#!/usr/bin/env pythonimport sys(last_key, max_val) = (None, -sys.maxint)for line in sys.stdin: (key, val) = line.strip().split("\t") if last_key and last_key != key: print "%s\t%s" % (last_key, max_val) (last_key, max_val) = (key, int(val)) else: (last_key, max_val) = (key, max(max_val, int(val)))if last_key: print "%s\t%s" % (last_key, max_val)运行都是一样的，就不多做赘述了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning [ECNU] Assignment 1]]></title>
      <url>%2F2017%2F04%2F16%2FMachine-Learning-ECNU-Assignment-1%2F</url>
      <content type="text"><![CDATA[Use a crawler to get at least 20 webpages from a website.Count theoccurrences of words in the webpages on Hadoop.Hand in:Each one should crawl different websites, list the website URL, as well as the URLsof the crawled webpages.Count the word occurrence on Hadoop, code in both JAVA and another language such asPig Latin. print out your code.Print out your result.Home work due: 4/12You are allowed toform a group of no more than 4 fellow students.https://github.com/ewanlee/machine-learning-ECNU-/blob/master/Hadoop%20wordcount%20demo_cutted.pdf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignments [2 & 3]]]></title>
      <url>%2F2017%2F04%2F16%2Fcs231n-Assignments-2-3%2F</url>
      <content type="text"><![CDATA[Assignment 2In this assignment you will practice writing backpropagation code, and training Neural Networks and Convolutional Neural Networks. The goals of this assignment are as follows:understand Neural Networks and how they are arranged in layered architecturesunderstand and be able to implement (vectorized) backpropagationimplement various update rules used to optimize Neural Networksimplement batch normalization for training deep networksimplement dropout to regularize networkseffectively cross-validate and find the best hyperparameters for Neural Network architectureunderstand the architecture of Convolutional Neural Networks and train gain experience with training these models on dataSetupYou can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine through Terminal.com.Working in the cloud on TerminalTerminal has created a separate subdomain to serve our class, www.stanfordterminalcloud.com. Register your account there. The Assignment 2 snapshot can then be found HERE. If you are registered in the class you can contact the TA (see Piazza for more information) to request Terminal credits for use on the assignment. Once you boot up the snapshot everything will be installed for you, and you will be ready to start on your assignment right away. We have written a small tutorial on Terminal here.Working locallyGet the code as a zip file here. As for the dependencies:[Option 1] Use Anaconda: The preferred approach for installing all the assignment dependencies is to useAnaconda, which is a Python distribution that includes many of the most popular Python packages for science, math, engineering and data analysis. Once you install it you can skip all mentions of requirements and you are ready to go directly to working on the assignment.[Option 2] Manual install, virtual environment: If you do not want to use Anaconda and want to go with a more manual and risky installation route you will likely want to create a virtual environment for the project. If you choose not to use a virtual environment, it is up to you to make sure that all dependencies for the code are installed globally on your machine. To set up a virtual environment, run the following:1234567cd assignment2sudo pip install virtualenv # This may already be installedvirtualenv .env # Create a virtual environmentsource .env/bin/activate # Activate the virtual environmentpip install -r requirements.txt # Install dependencies# Work on the assignment for a while ...deactivate # Exit the virtual environmentDownload data: Once you have the starter code, you will need to download the CIFAR-10 dataset. Run the following from the assignment2 directory:12cd cs231n/datasets./get_datasets.shCompile the Cython extension: Convolutional Neural Networks require a very efficient implementation. We have implemented of the functionality using Cython; you will need to compile the Cython extension before you can run the code. From the cs231n directory, run the following command:1python setup.py build_ext --inplaceStart IPython: After you have the CIFAR-10 data, you should start the IPython notebook server from the assignment2 directory. If you are unfamiliar with IPython, you should read our IPython tutorial.NOTE: If you are working in a virtual environment on OSX, you may encounter errors with matplotlib due to theissues described here. You can work around this issue by starting the IPython server using thestart_ipython_osx.sh script from the assignment2 directory; the script assumes that your virtual environment is named .env.Submitting your work:Whether you work on the assignment locally or using Terminal, once you are done working run the collectSubmission.sh script; this will produce a file called assignment2.zip. Upload this file under the Assignments tab on the coursework page for the course.Q1: Fully-connected Neural Network (30 points)The IPython notebook FullyConnectedNets.ipynb will introduce you to our modular layer design, and then use those layers to implement fully-connected networks of arbitrary depth. To optimize these models you will implement several popular update rules.Q2: Batch Normalization (30 points)In the IPython notebook BatchNormalization.ipynb you will implement batch normalization, and use it to train deep fully-connected networks.Q3: Dropout (10 points)The IPython notebook Dropout.ipynb will help you implement Dropout and explore its effects on model generalization.Q4: ConvNet on CIFAR-10 (30 points)In the IPython Notebook ConvolutionalNetworks.ipynb you will implement several new layers that are commonly used in convolutional networks. You will train a (shallow) convolutional network on CIFAR-10, and it will then be up to you to train the best network that you can.Q5: Do something extra! (up to +10 points)In the process of training your network, you should feel free to implement anything that you want to get better performance. You can modify the solver, implement additional layers, use different types of regularization, use an ensemble of models, or anything else that comes to mind. If you implement these or other ideas not covered in the assignment then you will be awarded some bonus points.](https://github.com/ewanlee/cs231n/tree/master/cs231n-assignments/assignmentAssignment 3In this assignment you will implement recurrent networks, and apply them to image captioning on Microsoft COCO. We will also introduce the TinyImageNet dataset, and use a pretrained model on this dataset to explore different applications of image gradients.The goals of this assignment are as follows:Understand the architecture of recurrent neural networks (RNNs) and how they operate on sequences by sharing weights over timeUnderstand the difference between vanilla RNNs and Long-Short Term Memory (LSTM) RNNsUnderstand how to sample from an RNN at test-timeUnderstand how to combine convolutional neural nets and recurrent nets to implement an image captioning systemUnderstand how a trained convolutional network can be used to compute gradients with respect to the input imageImplement and different applications of image gradients, including saliency maps, fooling images, class visualizations, feature inversion, and DeepDream.SetupYou can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine through Terminal.com.Working in the cloud on TerminalTerminal has created a separate subdomain to serve our class, www.stanfordterminalcloud.com. Register your account there. The Assignment 3 snapshot can then be found HERE. If you are registered in the class you can contact the TA (see Piazza for more information) to request Terminal credits for use on the assignment. Once you boot up the snapshot everything will be installed for you, and you will be ready to start on your assignment right away. We have written a small tutorial on Terminal here.Working locallyGet the code as a zip file here. As for the dependencies:[Option 1] Use Anaconda: The preferred approach for installing all the assignment dependencies is to useAnaconda, which is a Python distribution that includes many of the most popular Python packages for science, math, engineering and data analysis. Once you install it you can skip all mentions of requirements and you are ready to go directly to working on the assignment.[Option 2] Manual install, virtual environment: If you do not want to use Anaconda and want to go with a more manual and risky installation route you will likely want to create a virtual environment for the project. If you choose not to use a virtual environment, it is up to you to make sure that all dependencies for the code are installed globally on your machine. To set up a virtual environment, run the following:1234567cd assignment3sudo pip install virtualenv # This may already be installedvirtualenv .env # Create a virtual environmentsource .env/bin/activate # Activate the virtual environmentpip install -r requirements.txt # Install dependencies# Work on the assignment for a while ...deactivate # Exit the virtual environmentDownload data: Once you have the starter code, you will need to download the processed MS-COCO dataset, the TinyImageNet dataset, and the pretrained TinyImageNet model. Run the following from the assignment3directory:1234cd cs231n/datasets./get_coco_captioning.sh./get_tiny_imagenet_a.sh./get_pretrained_model.shCompile the Cython extension: Convolutional Neural Networks require a very efficient implementation. We have implemented of the functionality using Cython; you will need to compile the Cython extension before you can run the code. From the cs231n directory, run the following command:1python setup.py build_ext --inplaceStart IPython: After you have the data, you should start the IPython notebook server from the assignment3directory. If you are unfamiliar with IPython, you should read our IPython tutorial.NOTE: If you are working in a virtual environment on OSX, you may encounter errors with matplotlib due to theissues described here. You can work around this issue by starting the IPython server using thestart_ipython_osx.sh script from the assignment3 directory; the script assumes that your virtual environment is named .env.Submitting your work:Whether you work on the assignment locally or using Terminal, once you are done working run the collectSubmission.sh script; this will produce a file called assignment3.zip. Upload this file under the Assignments tab on the coursework page for the course.Q1: Image Captioning with Vanilla RNNs (40 points)The IPython notebook RNN_Captioning.ipynb will walk you through the implementation of an image captioning system on MS-COCO using vanilla recurrent networks.Q2: Image Captioning with LSTMs (35 points)The IPython notebook LSTM_Captioning.ipynbwill walk you through the implementation of Long-Short Term Memory (LSTM) RNNs, and apply them to image captioning on MS-COCO.Q3: Image Gradients: Saliency maps and Fooling Images (10 points)The IPython notebook ImageGradients.ipynb will introduce the TinyImageNet dataset. You will use a pretrained model on this dataset to compute gradients with respect to the image, and use them to produce saliency maps and fooling images.Q4: Image Generation: Classes, Inversion, DeepDream (15 points)In the IPython notebook ImageGeneration.ipynb you will use the pretrained TinyImageNet model to generate images. In particular you will generate class visualizations and implement feature inversion and DeepDream.Q5: Do something extra! (up to +10 points)Given the components of the assignment, try to do something cool. Maybe there is some way to generate images that we did not implement in the assignment?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Software Packages notes]]></title>
      <url>%2F2017%2F04%2F13%2Fcs231n-Software-Packages-notes%2F</url>
      <content type="text"><![CDATA[Software PackagesCaffehttp://caffe.berkeleyvision.orgOverviewFrom U.C. BerkeleyWritten in C++Has Python and Matlab bindingsGood for training or finetuning feedforward modelsTipDon’t be afraid to read the code!Main classesBlob: Stores data and derivativesLayer: Transforms bottom blobs to top blobsNet:Many layersComputes gradients via forward / backwardSolver: Uses gradients to update weightsProtocol Buffers“Typed JSON” from GoogleDefine “message types” in .proto files12345message Person &#123; required string name = 1; required int32 id = 2; optional string email = 3;&#125;Serialize instances to text files (.prototxt)123name: "John Doe"id: 1234email: "jdoe@example.com"Compile classes for different languagesTraining / FinetuningConvert data (run a script)Define net (edit prototxt)Define solver (edit prototxt)Train (with pretrained weights) (run a script)Step1: Convert DataDataLayer reading from LMDB is the easiestCreate LMDB using convert_imagesetNeed text file where each line is“[path/to/image.jpeg][label]”Create HDF5 file yourself using h5py[extras] some methods:ImageDataLayer: Read from image filesWindowDataLayer: For detectionHDF5Layer: Read from HDF5 fileFrom memory, using Python interfaceAll of these are harder to use (except Python)Step2: Define Net123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127name: "ResNet-152"input: "data"input_dim: 1input_dim: 3input_dim: 224input_dim: 224layer &#123; bottom: "data" top: "conv1" name: "conv1" type: "Convolution" convolution_param &#123; num_output: 64 kernel_size: 7 pad: 3 stride: 2 bias_term: false &#125;&#125;layer &#123; bottom: "conv1" top: "conv1" name: "bn_conv1" type: "BatchNorm" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: "conv1" top: "conv1" name: "scale_conv1" type: "Scale" scale_param &#123; bias_term: true &#125;&#125;layer &#123; top: "conv1" bottom: "conv1" name: "conv1_relu" type: "ReLU"&#125;layer &#123; bottom: "conv1" top: "pool1" name: "pool1" type: "Pooling" pooling_param &#123; kernel_size: 3 stride: 2 pool: MAX &#125;&#125;layer &#123; bottom: "pool1" top: "res2a_branch1" name: "res2a_branch1" type: "Convolution" convolution_param &#123; num_output: 256 kernel_size: 1 pad: 0 stride: 1 bias_term: false &#125;&#125;layer &#123; bottom: "res2a_branch1" top: "res2a_branch1" name: "bn2a_branch1" type: "BatchNorm" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: "res2a_branch1" top: "res2a_branch1" name: "scale2a_branch1" type: "Scale" scale_param &#123; bias_term: true &#125;&#125;layer &#123; bottom: "pool1" top: "res2a_branch2a" name: "res2a_branch2a" type: "Convolution" convolution_param &#123; num_output: 64 kernel_size: 1 pad: 0 stride: 1 bias_term: false &#125;&#125;layer &#123; bottom: "res2a_branch2a" top: "res2a_branch2a" name: "bn2a_branch2a" type: "BatchNorm" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: "res2a_branch2a" top: "res2a_branch2a" name: "scale2a_branch2a" type: "Scale" scale_param &#123; bias_term: true &#125;&#125;.prototxt can get ugly for big modelsResNet-152 prototxt is 6775 lines long!Not “compositional”; can’t easily define a residual block and reuseStep2: Define Net (finetuning)Same name: weights copiedDifferent name: weights reinitializedStep3: Define SolverWrite a prototxt file defining a SolverParameter123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166message SolverParameter &#123; ////////////////////////////////////////////////////////////////////////////// // Specifying the train and test networks // // Exactly one train net must be specified using one of the following fields: // train_net_param, train_net, net_param, net // One or more test nets may be specified using any of the following fields: // test_net_param, test_net, net_param, net // If more than one test net field is specified (e.g., both net and // test_net are specified), they will be evaluated in the field order given // above: (1) test_net_param, (2) test_net, (3) net_param/net. // A test_iter must be specified for each test_net. // A test_level and/or a test_stage may also be specified for each test_net. ////////////////////////////////////////////////////////////////////////////// // Proto filename for the train net, possibly combined with one or more // test nets. optional string net = 24; // Inline train net param, possibly combined with one or more test nets. optional NetParameter net_param = 25; optional string train_net = 1; // Proto filename for the train net. repeated string test_net = 2; // Proto filenames for the test nets. optional NetParameter train_net_param = 21; // Inline train net params. repeated NetParameter test_net_param = 22; // Inline test net params. // The states for the train/test nets. Must be unspecified or // specified once per net. // // By default, all states will have solver = true; // train_state will have phase = TRAIN, // and all test_state's will have phase = TEST. // Other defaults are set according to the NetState defaults. optional NetState train_state = 26; repeated NetState test_state = 27; // The number of iterations for each test net. repeated int32 test_iter = 3; // The number of iterations between two testing phases. optional int32 test_interval = 4 [default = 0]; optional bool test_compute_loss = 19 [default = false]; // If true, run an initial test pass before the first iteration, // ensuring memory availability and printing the starting value of the loss. optional bool test_initialization = 32 [default = true]; optional float base_lr = 5; // The base learning rate // the number of iterations between displaying info. If display = 0, no info // will be displayed. optional int32 display = 6; // Display the loss averaged over the last average_loss iterations optional int32 average_loss = 33 [default = 1]; optional int32 max_iter = 7; // the maximum number of iterations optional string lr_policy = 8; // The learning rate decay policy. optional float gamma = 9; // The parameter to compute the learning rate. optional float power = 10; // The parameter to compute the learning rate. optional float momentum = 11; // The momentum value. optional float weight_decay = 12; // The weight decay. // regularization types supported: L1 and L2 // controlled by weight_decay optional string regularization_type = 29 [default = "L2"]; // the stepsize for learning rate policy "step" optional int32 stepsize = 13; // the stepsize for learning rate policy "multistep" repeated int32 stepvalue = 34; // Set clip_gradients to &gt;= 0 to clip parameter gradients to that L2 norm, // whenever their actual L2 norm is larger. optional float clip_gradients = 35 [default = -1]; optional int32 snapshot = 14 [default = 0]; // The snapshot interval optional string snapshot_prefix = 15; // The prefix for the snapshot. // whether to snapshot diff in the results or not. Snapshotting diff will help // debugging but the final protocol buffer size will be much larger. optional bool snapshot_diff = 16 [default = false]; // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default. enum SolverMode &#123; CPU = 0; GPU = 1; &#125; optional SolverMode solver_mode = 17 [default = GPU]; // the device_id will that be used in GPU mode. Use device_id = 0 in default. optional int32 device_id = 18 [default = 0]; // If non-negative, the seed with which the Solver will initialize the Caffe // random number generator -- useful for reproducible results. Otherwise, // (and by default) initialize using a seed derived from the system clock. optional int64 random_seed = 20 [default = -1]; // Solver type enum SolverType &#123; SGD = 0; NESTEROV = 1; ADAGRAD = 2; &#125; optional SolverType solver_type = 30 [default = SGD]; // numerical stability for AdaGrad optional float delta = 31 [default = 1e-8]; // If true, print information about the state of the net that may help with // debugging learning problems. optional bool debug_info = 23 [default = false]; // If false, don't save a snapshot after training finishes. optional bool snapshot_after_train = 28 [default = true];&#125;// A message that stores the solver snapshotsmessage SolverState &#123; optional int32 iter = 1; // The current iteration optional string learned_net = 2; // The file that stores the learned net. repeated BlobProto history = 3; // The history for sgd solvers optional int32 current_step = 4 [default = 0]; // The current step for learning rate&#125;enum Phase &#123; TRAIN = 0; TEST = 1;&#125;message NetState &#123; optional Phase phase = 1 [default = TEST]; optional int32 level = 2 [default = 0]; repeated string stage = 3;&#125;message NetStateRule &#123; // Set phase to require the NetState have a particular phase (TRAIN or TEST) // to meet this rule. optional Phase phase = 1; // Set the minimum and/or maximum levels in which the layer should be used. // Leave undefined to meet the rule regardless of level. optional int32 min_level = 2; optional int32 max_level = 3; // Customizable sets of stages to include or exclude. // The net must have ALL of the specified stages and NONE of the specified // "not_stage"s to meet the rule. // (Use multiple NetStateRules to specify conjunctions of stages.) repeated string stage = 4; repeated string not_stage = 5;&#125;// Specifies training parameters (multipliers on global learning constants,// and the name and other settings used for weight sharing).message ParamSpec &#123; // The names of the parameter blobs -- useful for sharing parameters among // layers, but never required otherwise. To share a parameter between two // layers, give it a (non-empty) name. optional string name = 1; // Whether to require shared weights to have the same shape, or just the same // count -- defaults to STRICT if unspecified. optional DimCheckMode share_mode = 2; enum DimCheckMode &#123; // STRICT (default) requires that num, channels, height, width each match. STRICT = 0; // PERMISSIVE requires only the count (num*channels*height*width) to match. PERMISSIVE = 1; &#125; // The multiplier on the global learning rate for this parameter. optional float lr_mult = 3 [default = 1.0]; // The multiplier on the global weight decay for this parameter. optional float decay_mult = 4 [default = 1.0];&#125;If finetuning, copy existing solver.prototxt fileChange net to be your netChange snapshot_prefix to your outputReduce base learning rate (divide by 100)Maybe change max_iter and snapshotStep 4: Train12345678./build/tools/caffe train \ -gpu 0 \ -model path/to/trainval.prototxt \ -solver path/to/solver.prototxt \ -weights path/to/pretrained_weights.caffemodel # -gpu -1 for CPU mode # -gpu all for multi-GPU data parallelismModel Zoohttps://github.com/BVLC/caffe/wiki/Model-ZooPython InterfaceRead the code! Two most important files:caffe/python/caffe/_caffe.cppExports Blob, Layer, Net, and Solver classescaffe/python/caffe/pycaffe.pyAdds extra methods to Net classGood for:Interfacing with numpyExtract features: Run net forwardCompute gradients: Run net backward (DeepDream, etc)Define layers in Python with numpy (CPU only)Pros / Cons(+) Good for feedforward networks(+) Good for finetuning existing networks(+) Train models without writing any code!(+) Python interface is pretty useful!(-) Need to write C++ / CUDA for new GPU layers(-) Not good for recurrent networks(-) Cumbersome for big networks (GoogLeNet, ResNet)Torchhttp://torch.chOverviewFrom NYU + IDIAPWritten in C and LuaUsed a lot a Facebook, DeepMindLuaLearn Lua in 15 MinutesHigh level scripting language, easy to interface with CSimilar to Javascript:One data structure: table == JS objectPrototypical inheritance: metatable == JS prototypeFirst-class functionsSome gotchas:1-indexed =(Variables global by default =(Small standard libraryTensorTorch tensors are just like numpy arraysDocumentation on GitHub:https://github.com/torch/torch7/blob/master/doc/tensor.mdhttps://github.com/torch/torch7/blob/master/doc/maths.mdnnnn module lets you easily build and train neural nets123456789101112131415-- our optimization procedure will iterate over the modules, so only share-- the parametersmlp = nn.Sequential()linear = nn.Linear(2,2)linear_clone = linear:clone('weight','bias') -- clone sharing the parametersmlp:add(linear)mlp:add(linear_clone)function gradUpdate(mlp, x, y, criterion, learningRate) local pred = mlp:forward(x) local err = criterion:forward(pred, y) local gradCriterion = criterion:backward(pred, y) mlp:zeroGradParameters() mlp:backward(x, gradCriterion) mlp:updateParameters(learningRate)end12345678910111213141516171819-- our optimization procedure will use all the parameters at once, because-- it requires the flattened parameters and gradParameters Tensors. Thus,-- we need to share both the parameters and the gradParametersmlp = nn.Sequential()linear = nn.Linear(2,2)-- need to share the parameters and the gradParameters as welllinear_clone = linear:clone('weight','bias','gradWeight','gradBias')mlp:add(linear)mlp:add(linear_clone)params, gradParams = mlp:getParameters()function gradUpdate(mlp, x, y, criterion, learningRate, params, gradParams) local pred = mlp:forward(x) local err = criterion:forward(pred, y) local gradCriterion = criterion:backward(pred, y) mlp:zeroGradParameters() mlp:backward(x, gradCriterion) -- adds the gradients to all the parameters at once params:add(-learningRate, gradParams)endcunnRunning on GPU is easy123456789101112local model = nn.Sequential()model:add(nn.Linear(2,2))model:add(nn.LogSoftMax())model:cuda() -- convert model to CUDAlocal input = torch.Tensor(32,2):uniform()input = input:cuda()local output = model:forward(input)local input = torch.CudaTensor(32,2):uniform()local output = model:forward(input)optimoptim package implements different update rules: momentum, Adam, etc123456789101112131415161718192021require 'optim'for epoch = 1, 50 do -- local function we give to optim -- it takes current weights as input, and outputs the loss -- and the gradient of the loss with respect to the weights -- gradParams is calculated implicitly by calling 'backward', -- because the model's weight and bias gradient tensors -- are simply views onto gradParams function feval(params) gradParams:zero() local outputs = model:forward(batchInputs) local loss = criterion:forward(outputs, batchLabels) local dloss_doutputs = criterion:backward(outputs, batchLabels) model:backward(batchInputs, dloss_doutputs) return loss, gradParams end optim.sgd(feval, params, optimState)endModulesCaffe has Nets and Layers; Torch just has ModulesModules are classes written in Lua; easy to read and writeForward / backward written in Lua using Tensor methodsSame code runs on CPU / GPU123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122local Linear, parent = torch.class('nn.Linear', 'nn.Module')function Linear:__init(inputSize, outputSize, bias) parent.__init(self) local bias = ((bias == nil) and true) or bias self.weight = torch.Tensor(outputSize, inputSize) self.gradWeight = torch.Tensor(outputSize, inputSize) if bias then self.bias = torch.Tensor(outputSize) self.gradBias = torch.Tensor(outputSize) end self:reset()endfunction Linear:noBias() self.bias = nil self.gradBias = nil return selfendfunction Linear:reset(stdv) if stdv then stdv = stdv * math.sqrt(3) else stdv = 1./math.sqrt(self.weight:size(2)) end if nn.oldSeed then for i=1,self.weight:size(1) do self.weight:select(1, i):apply(function() return torch.uniform(-stdv, stdv) end) end if self.bias then for i=1,self.bias:nElement() do self.bias[i] = torch.uniform(-stdv, stdv) end end else self.weight:uniform(-stdv, stdv) if self.bias then self.bias:uniform(-stdv, stdv) end end return selfendlocal function updateAddBuffer(self, input) local nframe = input:size(1) self.addBuffer = self.addBuffer or input.new() if self.addBuffer:nElement() ~= nframe then self.addBuffer:resize(nframe):fill(1) endendfunction Linear:updateOutput(input) if input:dim() == 1 then self.output:resize(self.weight:size(1)) if self.bias then self.output:copy(self.bias) else self.output:zero() end self.output:addmv(1, self.weight, input) elseif input:dim() == 2 then local nframe = input:size(1) local nElement = self.output:nElement() self.output:resize(nframe, self.weight:size(1)) if self.output:nElement() ~= nElement then self.output:zero() end updateAddBuffer(self, input) self.output:addmm(0, self.output, 1, input, self.weight:t()) if self.bias then self.output:addr(1, self.addBuffer, self.bias) end else error('input must be vector or matrix') end return self.outputendfunction Linear:updateGradInput(input, gradOutput) if self.gradInput then local nElement = self.gradInput:nElement() self.gradInput:resizeAs(input) if self.gradInput:nElement() ~= nElement then self.gradInput:zero() end if input:dim() == 1 then self.gradInput:addmv(0, 1, self.weight:t(), gradOutput) elseif input:dim() == 2 then self.gradInput:addmm(0, 1, gradOutput, self.weight) end return self.gradInput endendfunction Linear:accGradParameters(input, gradOutput, scale) scale = scale or 1 if input:dim() == 1 then self.gradWeight:addr(scale, gradOutput, input) if self.bias then self.gradBias:add(scale, gradOutput) end elseif input:dim() == 2 then self.gradWeight:addmm(scale, gradOutput:t(), input) if self.bias then -- update the size of addBuffer if the input is not the same size as the one we had in last updateGradInput updateAddBuffer(self, input) self.gradBias:addmv(scale, gradOutput:t(), self.addBuffer) end endendfunction Linear:sharedAccUpdateGradParameters(input, gradOutput, lr) -- we do not need to accumulate parameters when sharing: self:defaultAccUpdateGradParameters(input, gradOutput, lr)endfunction Linear:clearState() if self.addBuffer then self.addBuffer:set() end return parent.clearState(self)endfunction Linear:__tostring__() return torch.type(self) .. string.format('(%d -&gt; %d)', self.weight:size(2), self.weight:size(1)) .. (self.bias == nil and ' without bias' or '')endTons of built-in modules and loss functionshttps://github.com/torch/nnContainerContainer modules allow you to combine multiple modulesnngraphA multi-layer network where each layer takes output of previous two layers as input.1234567891011121314input = nn.Identity()()L1 = nn.Tanh()(nn.Linear(10, 20)(input))L2 = nn.Tanh()(nn.Linear(30, 60)(nn.JoinTable(1)(&#123;input, L1&#125;)))L3 = nn.Tanh()(nn.Linear(80, 160)(nn.JoinTable(1)(&#123;L1, L2&#125;)))g = nn.gModule(&#123;input&#125;, &#123;L3&#125;)indata = torch.rand(10)gdata = torch.rand(160)g:forward(indata)g:backward(indata, gdata)graph.dot(g.fg, 'Forward Graph')graph.dot(g.bg, 'Backward Graph')More InfoPretrained Modelsloadcaffe: Load pretrained Caffe models: AlexNet, VGG, some othershttps://github.com/szagoruyko/loadcaffeGoogLeNet v1: https://github.com/soumith/inception.torchGoogLeNet v3: https://github.com/Moodstocks/inception-v3.torchResNet: https://github.com/facebook/fb.resnet.torchPackage ManagementAfter installing torch, use luarocks to install or update Lua packages(Similar to pip install from Python)Other useful packagestorch.cudnn: Bindings for NVIDIA cuDNN kernelshttps://github.com/soumith/cudnn.torchtorch-hdf5: Read and write HDF5 files from Torchhttps://github.com/deepmind/torch-hdf5lua-cjson: Read and write JSON files from Luahttps://luarocks.org/modules/luarocks/lua-cjsoncltorch, clnn: OpenCL backend for Torch, and port of nnhttps://github.com/hughperkins/cltorch, https://github.com/hughperkins/clnntorch-autograd: Automatic differentiation; sort of like more powerful nngraph, similar to Theano or TensorFlowhttps://github.com/twitter/torch-autogradfbcunn: Facebook: FFT conv, multi-GPU (DataParallel, ModelParallel)https://github.com/facebook/fbcunnTypical WorkflowPreprocess data; usually use a Python script to dump data to HDF5Train a model in Lua / Torch; read from HDF5 datafile, save trained model to diskUse trained model for something, often with an evaluation scriptExample: https://github.com/jcjohnson/torch-rnnStep 1: Preprocess data; usually use a Python script to dump data to HDF5 (https://github.com/jcjohnson/torch-rnn/blob/master/scripts/preprocess.py)Step 2: Train a model in Lua / Torch; read from HDF5 datafile, save trained model to disk (https://github.com/jcjohnson/torch-rnn/blob/master/train.lua )Step 3: Use trained model for something, often with an evaluation script (https://github.com/jcjohnson/torch-rnn/blob/master/sample.lua)Pros / Cons(-) Lua(-) Less plug-and-play than CaffeYou usually write your own training code(+) Lots of modular pieces that are easy to combine(+) Easy to write your own layer types and run on GPU(+) Most of the library code is in Lua, easy to read(+) Lots of pretrained models!(-) Not great for RNNsTheanohttp://deeplearning.net/software/theano/OverviewFrom Yoshua Bengio’s group at University of MontrealEmbracing computation graphs, symbolic computationHigh-level wrappers: Keras, LasagneOther TopicsConditionals: The ifelse and switch functions allow conditional control flow in the graphLoops: The scan function allows for (some types) of loops in the computational graph; good for RNNsDerivatives: Efficient Jacobian / vector products with R and L operators, symbolic hessians (gradient of gradient)Sparse matrices, optimizations, etcMulti-GPUExperimental model parallelism:http://deeplearning.net/software/theano/tutorial/using_multi_gpu.htmlData parallelism using platoon:https://github.com/mila-udem/platoonHigh level wrapperLasagneKerasPretrained ModelsLasagne Model Zoo has pretrained common architectures:https://github.com/Lasagne/Recipes/tree/master/modelzooAlexNet with weights: https://github.com/uoguelph-mlrg/theano_alexnetsklearn-theano: Run OverFeat and GoogLeNet forward, but no fine-tuning? http://sklearn-theano.github.iocaffe-theano-conversion: CS 231n project from last year: load models and weights from caffe! Not sure if full-featured https://github.com/kitofans/caffe-theano-conversionPros / Cons(+) Python + numpy(+) Computational graph is nice abstraction(+) RNNs fit nicely in computational graph(-) Raw Theano is somewhat low-level(+) High level wrappers (Keras, Lasagne) ease the pain(-) Error messages can be unhelpful(-) Large models can have long compile times(-) Much “fatter” than Torch; more magic(-) Patchy support for pretrained modelsTensorFlowhttps://www.tensorflow.orgOverviewFrom GoogleVery similar to Theano - all about computation graphsEasy visualizations (TensorBoard)Multi-GPU and multi-node trainingTensorboardTensorboard makes it easy to visualize what’s happening inside your modelsMulti-GPUDistributedPretrained ModelsYou can get a pretrained version of Inception here:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md(In an Android example?? Very well-hidden)The only one I could find =(Pros / Cons(+) Python + numpy(+) Computational graph abstraction, like Theano; great for RNNs(+) Much faster compile times than Theano(+) Slightly more convenient than raw Theano?(+) TensorBoard for visualization(+) Data AND model parallelism; best of all frameworks(+/-) Distributed models, but not open-source yet(-) Slower than other frameworks right now(-) Much “fatter” than Torch; more magic(-) Not many pretrained modelsUse CasesExtract AlexNet or VGG features? Use CaffeFine-tune AlexNet for new classes? Use CaffeImage Captioning with finetuning?-&gt; Need pretrained models (Caffe, Torch, Lasagne)-&gt; Need RNNs (Torch or Lasagne)-&gt; Use Torch or LasagnaSegmentation? (Classify every pixel)-&gt; Need pretrained model (Caffe, Torch, Lasagna)-&gt; Need funny loss function-&gt; If loss function exists in Caffe: Use Caffe-&gt; If you want to write your own loss: Use TorchObject Detection?-&gt; Need pretrained model (Torch, Caffe, Lasagne)-&gt; Need lots of custom imperative code (NOT Lasagne)-&gt; Use Caffe + Python or TorchLanguage modeling with new RNN structure?-&gt; Need easy recurrent nets (NOT Caffe, Torch)-&gt; No need for pretrained models-&gt; Use Theano or TensorFlowImplement BatchNorm?-&gt; Don’t want to derive gradient? Theano or TensorFlow-&gt; Implement efficient backward pass? Use TorchRecommendation:Feature extraction / finetuning existing models: Use CaffeComplex uses of pretrained models: Use Lasagne or TorchWrite your own layers: Use TorchCrazy RNNs: Use Theano or TensorflowHuge model, need model parallelism: Use TensorFlow]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[usr/bin/ld: cannot find -lxxx Solutions]]></title>
      <url>%2F2017%2F04%2F11%2Fusr-bin-ld-cannot-find-lxxx-Solutions%2F</url>
      <content type="text"><![CDATA[在Ubuntu上运行Qt5的过程中报错：1usr/bin/ld: cannot find -lGL最后发现问题是系统中没有对应的库文件 libgl.so那么解决方式也很简单，安装即可：1sudo apt-get install libgl-devTada =)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Reinforcement Learning (GT) Notes]]></title>
      <url>%2F2017%2F04%2F10%2FReinforcement-Learning-GT-Notes%2F</url>
      <content type="text"><![CDATA[Decision Making &amp; Reinforcement LearningSupervised Learning: $y = f(x)$Unsupervised Learning: $f(x)$Reinforcement Learning: $y = f(x), z$Markov Decision ProcessStates: $S$Model: $T(s, a, s^{\prime}) \sim Pr(s^{\prime} | s, a)$Actions: $A(s), A$Reword: $R(s), R(s, a), R(s, a, s^{\prime})$Policy: $\pi(s) \rightarrow a$​ $\pi^{*}$Sequences of Rewards: AssumptionInfinite HorizonsUtility of sequencesif $U(s_0, s_1, s_2, \cdots) &gt; U(s_0, s^{\prime}_1, s^{\prime}_2, \cdots)$then $U(s_1, s_2, \cdots) &gt; U(s^{\prime}_1, s^{\prime}_2, \cdots)$U(s_0, s_1, s_2, \cdots)=\sum_{t=0}^{\infty}\gamma^{t}R(s_t), 0 \leq \gamma \leq 1U\leq\frac{R_{max}}{1 - \gamma}Policies\pi^{\star}=argmax_{\pi} E[\sum_{t=0}^{\infty}\gamma^{t}R(S_t)|\pi]U^{\pi}(s)=E[\sum_{t=0}^{\infty}\gamma^{t}R(s_t)|\pi,s_0=s]\pi^{\star}(s)=argmax_{a}\sum_{s^{\prime}}T(s, a, s^{\prime})U(s^{\prime})U(s)=R(s)+\gamma \max_{a}\sum_{s^{\prime}}T(s, a, s^{\prime})U(s^{\prime})Above is the Bellman Equation.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Lecture 11 Recap]]></title>
      <url>%2F2017%2F04%2F10%2Fcs231n-Lecture-11-Recap%2F</url>
      <content type="text"><![CDATA[Working with CNNs in practiceMaking the most of your dataData augmentationTransfer learningAll about convolutionsHow to arrange themHow to compute them fastImplementation detailsGPU / CPU, bottlenecks, ditributed trainingData AugmentationHorizontal flipsRandom crops/scalesTraining: sample random crops /scalesResNet:Pick random L in range [256, 480]Resize training image, short side = LSample random 224 x 224 patchTesting: average a fixed set of cropsResNet:Resize image at 5 scales: {224, 256, 384, 480, 640}For each size, use 10 224 x 224 crops: 4 corners + center, + flipsColor jitterSimple:Randomly jitter contrastComplex:Apply PCA to all [R, G, B] pixels in training setSample a “color offset” along principal component directionsAdd offset to all pixels of a training image(As seen in [Krizhevsky et al. 2012], ResNet, etc)Transfer Learning“You need a lot of a data if you want to train/use CNNs”some tricks:very similar datasetvery different datasetvery little dataUse Linear Classifer on top layerTry linear classifer from different stagesquite a lot of dataFinetune a few layersFinetune a larger number of layersAll about ConvolutionsHow to stack themReplace large convolutions (5 x 5, 7 x 7) with stacks of 3 x 3 convolutions1 x 1 “bottleneck” convolutions are very efficientCan factor N x N convolutions into 1 x N and N x 1All of the above give fewer parameters, less compute, more nonlinearityHow to compute themim2colBLASFFTCompute FFT of weights: F(W)Compute FFT of image: F(X)Compute elementwise product: F(W) ○ F(X)Compute inverse FFT: Y = F-1(F(W) ○ F(X))FFT convolutions get a big speedup for larger filtersNot much speedup for 3x3 filters =(Fast algorithmsStrassen’s AlgorithmAnd so on…Implementation DetailsGPUs much faster than CPUsDistributed training is sometimes usedNot needed for small problemsBe aware of bottlenecks: CPU / GPU, CPU / diskLow precison makes things faster and still works32 bit is standard now, 16 bit soonIn the future: binary nets?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dynet xor demo [python version]]]></title>
      <url>%2F2017%2F04%2F09%2FDynet-xor-demo-python-version%2F</url>
      <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546import dynet as dyimport random# Parameters of the model and trainingHIDDEN_SIZE = 20NUM_EPOCHS = 20# Define the model and SGD optimizermodel = dy.Model()W_xh_p = model.add_parameters((HIDDEN_SIZE, 2))b_h_p = model.add_parameters(HIDDEN_SIZE)W_hy_p = model.add_parameters((1, HIDDEN_SIZE))b_y_p = model.add_parameters(1)trainer = dy.SimpleSGDTrainer(model)# Define the training data, consisting of (x,y) tuplesdata = [([1,1],1), ([-1,1],-1), ([1,-1],-1), ([-1,-1],1)]# Define the function we would like to calculatedef calc_function(x): dy.renew_cg() w_xh = dy.parameter(W_xh_p) b_h = dy.parameter(b_h_p) W_hy = dy.parameter(W_hy_p) b_y = dy.parameter(b_y_p) x_val = dy.inputVector(x) h_val = dy.tanh(w_xh * x_val + b_h) y_val = W_hy * h_val + b_y return y_val# Perform trainingfor epoch in range(NUM_EPOCHS): epoch_loss = 0 random.shuffle(data) for x, ystar in data: y = calc_function(x) loss = dy.squared_distance(y, dy.scalarInput(ystar)) epoch_loss += loss.value() loss.backward() trainer.update() print("Epoch %d: loss=%f" % (epoch, epoch_loss))# Print results of predictionfor x, ystar in data: y = calc_function(x) print("%r -&gt; %f" % (x, y.value()))Output:123456789101112131415161718192021222324252627[dynet] random seed: 1174664263[dynet] allocating memory: 512MB[dynet] memory allocation done.Epoch 0: loss=12.391680Epoch 1: loss=8.196088Epoch 2: loss=8.103037Epoch 3: loss=8.636450Epoch 4: loss=7.573008Epoch 5: loss=4.910318Epoch 6: loss=3.079966Epoch 7: loss=1.328273Epoch 8: loss=1.171368Epoch 9: loss=0.515850Epoch 10: loss=1.885216Epoch 11: loss=0.568994Epoch 12: loss=0.278629Epoch 13: loss=0.025215Epoch 14: loss=0.018466Epoch 15: loss=0.055305Epoch 16: loss=0.014131Epoch 17: loss=0.010476Epoch 18: loss=0.003893Epoch 19: loss=0.003332[1, 1] -&gt; 1.049703[-1, 1] -&gt; -0.996379[1, -1] -&gt; -0.974599[-1, -1] -&gt; 0.995763]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Installing the Python Dynet module]]></title>
      <url>%2F2017%2F04%2F09%2FInstalling-the-Python-Dynet-module%2F</url>
      <content type="text"><![CDATA[Installing the Python Dynet module(for instructions on installing on a computer with GPU, see below)Python bindings to DyNet are supported for both Python 2.x and 3.x.TL;DR(see below for the details)123456789101112131415161718192021222324# Installing Python DyNet:pip install cython # if you don&apos;t have it already.mkdir dynet-basecd dynet-base# getting dynet and eigengit clone https://github.com/clab/dynet.githg clone https://bitbucket.org/eigen/eigen -r 346ecdb # -r NUM specified a known working revisioncd dynetmkdir buildcd build# without GPU support:cmake .. -DEIGEN3_INCLUDE_DIR=../../eigen -DPYTHON=`which python`# or with GPU support:cmake .. -DEIGEN3_INCLUDE_DIR=../../eigen -DPYTHON=`which python` -DBACKEND=cudamake -j 2 # replace 2 with the number of available corescd pythonpython setup.py install # or `python setup.py install --user` for a user-local install.# this should suffice, but on some systems you may need to add the following line to your# init files in order for the compiled .so files be accessible to Python.# /path/to/dynet/build/dynet is the location in which libdynet.dylib resides.export DYLD_LIBRARY_PATH=/path/to/dynet/build/dynet/:$DYLD_LIBRARY_PATHDetailed InstructionsFirst, get DyNet:1234567cd $HOMEmkdir dynet-basecd dynet-basegit clone https://github.com/clab/dynet.gitcd dynetgit submodule init # To be consistent with DyNet&apos;s installation instructions.git submodule update # To be consistent with DyNet&apos;s installation instructions.Then get Eigen:123cd $HOMEcd dynet-basehg clone https://bitbucket.org/eigen/eigen/ -r 346ecdb(-r NUM specifies a known working revision of Eigen. You can remove this in order to get the bleeding edge Eigen, with the risk of some compile breaks, and the possible benefit of added optimizations.)We also need to make sure the cython module is installed. (you can replace pip with your favorite package manager, such as conda, or install within a virtual environment)1pip install cythonTo simplify the following steps, we can set a bash variable to hold where we have saved the main directories of DyNet and Eigen. In case you have gotten DyNet and Eigen differently from the instructions above and saved them in different location(s), these variables will be helpful:12PATH_TO_DYNET=$HOME/dynet-base/dynet/PATH_TO_EIGEN=$HOME/dynet-base/eigen/Compile DyNet.This is pretty much the same process as compiling DyNet, with the addition of the -DPYTHON= flag, pointing to the location of your Python interpreter.If Boost is installed in a non-standard location, you should add the corresponding flags to the cmake commandline, see the DyNet installation instructions page.123456cd $PATH_TO_DYNETPATH_TO_PYTHON=`which python`mkdir buildcd buildcmake .. -DEIGEN3_INCLUDE_DIR=$PATH_TO_EIGEN -DPYTHON=$PATH_TO_PYTHONmake -j 2Assuming that the cmake command found all the needed libraries and didn’t fail, the make command will take a while, and compile DyNet as well as the Python bindings. You can change make -j 2 to a higher number, depending on the available cores you want to use while compiling.You now have a working Python binding inside of build/dynet. To verify this is working:12cd $PATH_TO_DYNET/build/pythonpythonthen, within Python:123import dynet as dyprint dy.__version__model = dy.Model()In order to install the module so that it is accessible from everywhere in the system, run the following:12cd $PATH_TO_DYNET/build/pythonpython setup.py install --userThe --user switch will install the module in your local site-packages, and works without root privileges. To install the module to the system site-packages (for all users), or to the current virtualenv (if you are on one), run python setup.py installwithout this switch.You should now have a working python binding (the dynet module).Note however that the installation relies on the compiled DyNet library being in $PATH_TO_DYNET/build/dynet, so make sure not to move it from there.Now, check that everything works:1234cd $PATH_TO_DYNETcd examples/pythonpython xor.pypython rnnlm.py rnnlm.pyAlternatively, if the following script works for you, then your installation is likely to be working:12from dynet import *model = Model()If it doesn’t work and you get an error similar to the following:123ImportError: dlopen(/Users/sneharajana/.python-eggs/dyNET-0.0.0-py2.7-macosx-10.11-intel.egg-tmp/_dynet.so, 2): Library not loaded: @rpath/libdynet.dylibReferenced from: /Users/sneharajana/.python-eggs/dyNET-0.0.0-py2.7-macosx-10.11-intel.egg-tmp/_dynet.soReason: image not found``then you may need to run the following (and add it to your shell init files):export DYLD_LIBRARY_PATH=/path/to/dynet/build/dynet/:$DYLD_LIBRARY_PATHUsageThere are two ways to import the dynet module :1import dynetimports dynet and automatically initializes the global dynet parameters with the command line arguments (see the documentation). The amount of memory allocated, GPU/CPU usage is fixed from there on.123import _dynet# orimport _gdynet # For GPUImports dynet for CPU (resp. GPU) and doesn’t initialize the global parameters. These must be initialized manually before using dynet, using one of the following :123# Same as import dynet as dyimport _dynet as dydy.init()123456789101112131415# Same as import dynet as dyimport _dynet as dy# Declare a DynetParams objectdyparams = dy.DynetParams()# Fetch the command line arguments (optional)dyparams.from_args()# Set some parameters manualy (see the command line arguments documentation)dyparams.set_mem(2048)dyparams.set_random_seed(666)dyparams.set_weight_decay(1e-7)dyparams.set_shared_parameters(False)dyparams.set_requested_gpus(1)dyparams.set_gpu_mask([0,1,1,0])# Initialize with the given parametersdyparams.init() # or init_from_params(dyparams)Anaconda SupportAnaconda is a popular package management system for Python. DyNet can be used from within an Anaconda environment, but be sure to activate the environmentsource activate my_environment_namethen install some necessary packages as follows:conda install gcc cmake boost cythonAfter this, the build process should be the same as normal.Note that on some conda environments, people have reported build errors related to the interaction between the icu and boost packages. If you encounter this, try the solution in this comment.Windows SupportYou can also use Python on Windows by following similar steps to the above. For simplicity, we recommend using a Python distribution that already has Cython installed. The following has been tested to work:Install WinPython 2.7.10 (comes with Cython already installed).Run CMake as above with -DPYTHON=/path/to/your/python.exe.Open a command prompt and set VS90COMNTOOLS to the path to your Visual Studio “Common7/Tools” directory. One easy way to do this is a command such as:1set VS90COMNTOOLS=%VS140COMNTOOLS%Open dynet.sln from this command prompt and build the “Release” version of the solution.Follow the rest of the instructions above for testing the build and installing it for other usersNote, currently only the Release version works.GPU/MKL SupportInstalling/running on GPUFor installing on a computer with GPU, first install CUDA. The following instructions assume CUDA is installed.The installation process is pretty much the same, while adding the -DBACKEND=cuda flag to the cmake stage:1cmake .. -DEIGEN3_INCLUDE_DIR=$PATH_TO_EIGEN -DPYTHON=$PATH_TO_PYTHON -DBACKEND=cuda(if CUDA is installed in a non-standard location and cmake cannot find it, you can specify also -DCUDA_TOOLKIT_ROOT_DIR=/path/to/cuda.)Now, build the Python modules (as above, we assume Cython is installed):After running make -j 2, you should have the files _dynet.so and _gdynet.so in the build/python folder.As before, cd build/python followed by python setup.py install --user will install the module.In order to use the GPU support, you can either:Use import _gdynet as dy instead of import dynet as dyOr, (preferred), import dynet as usual, but use the commandline switch --dynet-gpu or the GPU switches detailed herewhen invoking the program. This option lets the same code work with either the GPU or the CPU version depending on how it is invoked.Running with MKLIf you’ve built DyNet to use MKL (using -DMKL or -DMKL_ROOT), Python sometimes has difficulty finding the MKL shared libraries. You can try setting LD_LIBRARY_PATH to point to your MKL library directory. If that doesn’t work, try setting the following environment variable (supposing, for example, your MKL libraries are located at /opt/intel/mkl/lib/intel64):1export LD_PRELOAD=/opt/intel/mkl/lib/intel64/libmkl_def.so:/opt/intel/mkl/lib/intel64/libmkl_avx2.so:/opt/intel/mkl/libSome Errors and correspond Solutions12345678910111213import dynet as dy-------------------------------------------Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;dynet.py&quot;, line 17, in &lt;module&gt; from _dynet import *ImportError: /home/ewan/anaconda2/lib/libstdc++.so.6: version `GLIBCXX_3.4.20&apos; not found (required by /home/ewan/dynet-base/dynet/build/dynet/libdynet.so)-------------------------------------------Solution:conda install libgcc]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[lda for news classification]]></title>
      <url>%2F2017%2F03%2F26%2Flda-for-news-classification%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[netease news spider]]></title>
      <url>%2F2017%2F03%2F26%2Fnetease-news-spider%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[LDA]]></title>
      <url>%2F2017%2F03%2F24%2FLDA%2F</url>
      <content type="text"><![CDATA[Scikit-learn example1%matplotlib inlineTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet AllocationThis is an example of applying Non-negative Matrix Factorization and Latent Dirichlet Allocation on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).The default parameters (n_samples / n_features / n_topics) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# Author: Olivier Grisel &lt;olivier.grisel@ensta.org&gt;# Lars Buitinck# Chyi-Kwei Yau &lt;chyikwei.yau@gmail.com&gt;# License: BSD 3 clausefrom __future__ import print_functionfrom time import timefrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizerfrom sklearn.decomposition import NMF, LatentDirichletAllocationfrom sklearn.datasets import fetch_20newsgroupsn_samples = 2000n_features = 1000n_topics = 10n_top_words = 20def print_top_words(model, feature_names, n_top_words): for topic_idx, topic in enumerate(model.components_): print("Topic #%d:" % topic_idx) print(" ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])) print()# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics# to filter out useless terms early on: the posts are stripped of headers,# footers and quoted replies, and common English words, words occurring in# only one document or in at least 95% of the documents are removed.print("Loading dataset...")t0 = time()dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))data_samples = dataset.data[:n_samples]print("done in %0.3fs." % (time() - t0))# Use tf-idf features for NMF.print("Extracting tf-idf features for NMF...")tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')t0 = time()tfidf = tfidf_vectorizer.fit_transform(data_samples)print("done in %0.3fs." % (time() - t0))# Use tf (raw term count) features for LDA.print("Extracting tf features for LDA...")tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')t0 = time()tf = tf_vectorizer.fit_transform(data_samples)print("done in %0.3fs." % (time() - t0))# Fit the NMF modelprint("Fitting the NMF model with tf-idf features, " "n_samples=%d and n_features=%d..." % (n_samples, n_features))t0 = time()nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)print("done in %0.3fs." % (time() - t0))print("\nTopics in NMF model:")tfidf_feature_names = tfidf_vectorizer.get_feature_names()print_top_words(nmf, tfidf_feature_names, n_top_words)print("Fitting LDA models with tf features, " "n_samples=%d and n_features=%d..." % (n_samples, n_features))lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0)t0 = time()lda.fit(tf)print("done in %0.3fs." % (time() - t0))print("\nTopics in LDA model:")tf_feature_names = tf_vectorizer.get_feature_names()print_top_words(lda, tf_feature_names, n_top_words)1Loading dataset...​1No handlers could be found for logger "sklearn.datasets.twenty_newsgroups"​123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354done in 691.647s.Extracting tf-idf features for NMF...done in 0.454s.Extracting tf features for LDA...done in 0.416s.Fitting the NMF model with tf-idf features, n_samples=2000 and n_features=1000...done in 0.367s.Topics in NMF model:Topic #0:just people don think like know time good make way really say right ve want did ll new use yearsTopic #1:windows use dos using window program os drivers application help software pc running ms screen files version card code workTopic #2:god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religionTopic #3:thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video needTopic #4:car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used boughtTopic #5:edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact bloodTopic #6:file problem files format win sound ftp pub read save site help image available create copy running memory self versionTopic #7:game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teamsTopic #8:drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internalTopic #9:key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standardFitting LDA models with tf features, n_samples=2000 and n_features=1000...done in 2.169s.Topics in LDA model:Topic #0:edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mitTopic #1:don like just know think ve way use right good going make sure ll point got need really time doesnTopic #2:christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believeTopic #3:drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16Topic #4:hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drugTopic #5:god people does just good don jesus say israel way life know true fact time law want believe make thinkTopic #6:55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16Topic #7:car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performanceTopic #8:people said did just didn know time like went think children came come don took years say dead told startedTopic #9:key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology​1data_samples[0]1u"Well i'm not sure about the story nad it did seem biased. What\nI disagree with is your statement that the U.S. Media is out to\nruin Israels reputation. That is rediculous. The U.S. media is\nthe most pro-israeli media in the world. Having lived in Europe\nI realize that incidences such as the one described in the\nletter have occured. The U.S. media as a whole seem to try to\nignore them. The U.S. is subsidizing Israels existance and the\nEuropeans are not (at least not to the same degree). So I think\nthat might be a reason they report more clearly on the\natrocities.\n\tWhat is a shame is that in Austria, daily reports of\nthe inhuman acts commited by Israeli soldiers and the blessing\nreceived from the Government makes some of the Holocaust guilt\ngo away. After all, look how the Jews are treating other races\nwhen they got power. It is unfortunate.\n"1tfidf_vectorizer.get_feature_names()[-10:]12345678910[u'worth', u'wouldn', u'write', u'written', u'wrong', u'xfree86', u'year', u'years', u'yes', u'young']1tfidf.toarray().shape1(2000L, 1000L)1dataset.target_names1234567891011121314151617181920['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']1lda.transform(tf)[1934]12array([ 0.3587206 , 0.00227337, 0.00227317, 0.50146046, 0.00227288, 0.12390701, 0.00227282, 0.00227329, 0.00227343, 0.00227299])ExtrasSome materials can find from Github.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Tic-Tac-Toe]]></title>
      <url>%2F2017%2F03%2F18%2FTic-Tac-Toe%2F</url>
      <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350######################################################################## Copyright (C) ## 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com) ## 2016 Jan Hakenberg(jan.hakenberg@gmail.com) ## 2016 Tian Jun(tianjun.cpp@gmail.com) ## Permission given to modify the code as long as you keep this ## declaration at the top ########################################################################from __future__ import print_functionimport numpy as npimport pickleBOARD_ROWS = 3BOARD_COLS = 3BOARD_SIZE = BOARD_ROWS * BOARD_COLSclass State: def __init__(self): # the board is represented by a n * n array, # 1 represents chessman of the player who moves first, # -1 represents chessman of another player # 0 represents empty position self.data = np.zeros((BOARD_ROWS, BOARD_COLS)) self.winner = None self.hashVal = None self.end = None # calculate the hash value for one state, it's unique def getHash(self): if self.hashVal is None: self.hashVal = 0 for i in self.data.reshape(BOARD_ROWS * BOARD_COLS): if i == -1: i = 2 self.hashVal = self.hashVal * 3 + i return int(self.hashVal) # determine whether a player has won the game, or it's a tie def isEnd(self): if self.end is not None: return self.end results = [] # check row for i in range(0, BOARD_ROWS): results.append(np.sum(self.data[i, :])) # check columns for i in range(0, BOARD_COLS): results.append(np.sum(self.data[:, i])) # check diagonals results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, i] results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, BOARD_ROWS - 1 - i] for result in results: if result == 3: self.winner = 1 self.end = True return self.end if result == -3: self.winner = -1 self.end = True return self.end # whether it's a tie sum = np.sum(np.abs(self.data)) if sum == BOARD_ROWS * BOARD_COLS: self.winner = 0 self.end = True return self.end # game is still going on self.end = False return self.end # @symbol 1 or -1 # put chessman symbol in position (i, j) def nextState(self, i, j, symbol): newState = State() newState.data = np.copy(self.data) newState.data[i, j] = symbol return newState # print the board def show(self): for i in range(0, BOARD_ROWS): print('-------------') out = '| ' for j in range(0, BOARD_COLS): if self.data[i, j] == 1: token = '*' if self.data[i, j] == 0: token = '0' if self.data[i, j] == -1: token = 'x' out += token + ' | ' print(out) print('-------------')def getAllStatesImpl(currentState, currentSymbol, allStates): for i in range(0, BOARD_ROWS): for j in range(0, BOARD_COLS): if currentState.data[i][j] == 0: newState = currentState.nextState(i, j, currentSymbol) newHash = newState.getHash() if newHash not in allStates.keys(): isEnd = newState.isEnd() allStates[newHash] = (newState, isEnd) if not isEnd: getAllStatesImpl(newState, -currentSymbol, allStates)def getAllStates(): currentSymbol = 1 currentState = State() allStates = dict() allStates[currentState.getHash()] = (currentState, currentState.isEnd()) getAllStatesImpl(currentState, currentSymbol, allStates) return allStates# all possible board configurationsallStates = getAllStates()class Judger: # @player1: player who will move first, its chessman will be 1 # @player2: another player with chessman -1 # @feedback: if True, both players will receive rewards when game is end def __init__(self, player1, player2, feedback=True): self.p1 = player1 self.p2 = player2 self.feedback = feedback self.currentPlayer = None self.p1Symbol = 1 self.p2Symbol = -1 self.p1.setSymbol(self.p1Symbol) self.p2.setSymbol(self.p2Symbol) self.currentState = State() self.allStates = allStates # give reward to two players def giveReward(self): if self.currentState.winner == self.p1Symbol: self.p1.feedReward(1) self.p2.feedReward(0) elif self.currentState.winner == self.p2Symbol: self.p1.feedReward(0) self.p2.feedReward(1) else: self.p1.feedReward(0) self.p2.feedReward(0) def feedCurrentState(self): self.p1.feedState(self.currentState) self.p2.feedState(self.currentState) def reset(self): self.p1.reset() self.p2.reset() self.currentState = State() self.currentPlayer = None # @show: if True, print each board during the game def play(self, show=False): self.reset() self.feedCurrentState() while True: # set current player if self.currentPlayer == self.p1: self.currentPlayer = self.p2 else: self.currentPlayer = self.p1 if show: self.currentState.show() [i, j, symbol] = self.currentPlayer.takeAction() self.currentState = self.currentState.nextState(i, j, symbol) hashValue = self.currentState.getHash() self.currentState, isEnd = self.allStates[hashValue] self.feedCurrentState() if isEnd: if self.feedback: self.giveReward() return self.currentState.winner# AI playerclass Player: # @stepSize: step size to update estimations # @exploreRate: possibility to explore def __init__(self, stepSize = 0.1, exploreRate=0.1): self.allStates = allStates self.estimations = dict() self.stepSize = stepSize self.exploreRate = exploreRate self.states = [] def reset(self): self.states = [] def setSymbol(self, symbol): self.symbol = symbol for hash in self.allStates.keys(): (state, isEnd) = self.allStates[hash] if isEnd: if state.winner == self.symbol: self.estimations[hash] = 1.0 else: self.estimations[hash] = 0 else: self.estimations[hash] = 0.5 # accept a state def feedState(self, state): self.states.append(state) # update estimation according to reward def feedReward(self, reward): if len(self.states) == 0: return self.states = [state.getHash() for state in self.states] target = reward for latestState in reversed(self.states): value = self.estimations[latestState] + self.stepSize * (target - self.estimations[latestState]) self.estimations[latestState] = value target = value self.states = [] # determine next action def takeAction(self): state = self.states[-1] nextStates = [] nextPositions = [] for i in range(BOARD_ROWS): for j in range(BOARD_COLS): if state.data[i, j] == 0: nextPositions.append([i, j]) nextStates.append(state.nextState(i, j, self.symbol).getHash()) if np.random.binomial(1, self.exploreRate): np.random.shuffle(nextPositions) # Not sure if truncating is the best way to deal with exploratory step # Maybe it's better to only skip this step rather than forget all the history self.states = [] action = nextPositions[0] action.append(self.symbol) return action values = [] for hash, pos in zip(nextStates, nextPositions): values.append((self.estimations[hash], pos)) np.random.shuffle(values) values.sort(key=lambda x: x[0], reverse=True) action = values[0][1] action.append(self.symbol) return action def savePolicy(self): fw = open('optimal_policy_' + str(self.symbol), 'wb') pickle.dump(self.estimations, fw) fw.close() def loadPolicy(self): fr = open('optimal_policy_' + str(self.symbol),'rb') self.estimations = pickle.load(fr) fr.close()# human interface# input a number to put a chessman# | 1 | 2 | 3 |# | 4 | 5 | 6 |# | 7 | 8 | 9 |class HumanPlayer: def __init__(self, stepSize = 0.1, exploreRate=0.1): self.symbol = None self.currentState = None return def reset(self): return def setSymbol(self, symbol): self.symbol = symbol return def feedState(self, state): self.currentState = state return def feedReward(self, reward): return def takeAction(self): data = int(input("Input your position:")) data -= 1 i = data // int(BOARD_COLS) j = data % BOARD_COLS if self.currentState.data[i, j] != 0: return self.takeAction() return (i, j, self.symbol)def train(epochs=20000): player1 = Player() player2 = Player() judger = Judger(player1, player2) player1Win = 0.0 player2Win = 0.0 for i in range(0, epochs): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / epochs) print(player2Win / epochs) player1.savePolicy() player2.savePolicy()def compete(turns=500): player1 = Player(exploreRate=0) player2 = Player(exploreRate=0) judger = Judger(player1, player2, False) player1.loadPolicy() player2.loadPolicy() player1Win = 0.0 player2Win = 0.0 for i in range(0, turns): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / turns) print(player2Win / turns)def play(): while True: player1 = Player(exploreRate=0) player2 = HumanPlayer() judger = Judger(player1, player2, False) player1.loadPolicy() winner = judger.play(True) if winner == player2.symbol: print("Win!") elif winner == player1.symbol: print("Lose!") else: print("Tie!")train()compete()play()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Excel merge the same value that in a column to a cell]]></title>
      <url>%2F2017%2F03%2F16%2FExcel-merge-the-same-value-that-in-a-column-to-a-cell%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Installing ptproc on Ubuntu 16.04 LTS]]></title>
      <url>%2F2017%2F03%2F16%2FInstalling-ptproc-on-Ubuntu-16-04-LTS%2F</url>
      <content type="text"><![CDATA[想对事件数据用点过程进行建模，为了不用重复造轮子，所以找到了一个R包ptproc，但是安装时报错1234install.packages("ptproc")-------------------------------package ‘ptproc’ is not available (as a binary package for R version 3.2.3)既然仓库里没有，那就只好用源码安装了，源码下载地址, 但是依旧报错：123456789101112131415&gt; install.packages("ptproc", repos="http://www.biostat.jhsph.edu/~rpeng/software", type="source")trying URL 'http://www.biostat.jhsph.edu/~rpeng/software/src/contrib/ptproc_1.5-1.tar.gz'Content type 'application/x-gzip' length 282002 bytes (275 KB)opened URL==================================================downloaded 275 KB * installing *source* package ‘ptproc’ ...ERROR: a 'NAMESPACE' file is required* removing ‘/Library/Frameworks/R.framework/Versions/3.1/Resources/library/ptproc’ The downloaded source packages are in ‘/private/var/folders/0b/qdw3f3zn0gq5yy2cjjpm8cgw0000gn/T/RtmpuW1EPA/downloaded_packages’Warning message:In install.packages("ptproc", repos = "http://www.biostat.jhsph.edu/~rpeng/software", : installation of package ‘ptproc’ had non-zero exit status看来只有手动添加一个NAMESPACE,12345cd ptprocecho &apos;exportPattern( &quot;.&quot; )&apos; &gt; NAMESPACEcd ../rm ptproc_1.5-1.tar.gztar cvzf ptproc/ ptproc_1.5-1.tar.gz继续源码安装：1R CMD INSTALL -l &lt;ourRlibrarylocation&gt; &lt;path where I saved the packagename.tar.gz file&gt;Got it.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note Ch12]]></title>
      <url>%2F2017%2F03%2F09%2Fpython-data-analysis-learning-note-Ch12%2F</url>
      <content type="text"><![CDATA[Numpy高级应用123456from __future__ import divisionfrom numpy.random import randnfrom pandas import Seriesimport numpy as npnp.set_printoptions(precision=4)import sys12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"ndarray对象的内部机制NumPy 数据类型体系检测类型是否是某种类型的子类1234ints = np.ones(10, dtype=np.uint16)floats = np.ones(10, dtype=np.float32)np.issubdtype(ints.dtype, np.integer)np.issubdtype(floats.dtype, np.floating)True True 输出某种类型的所有父类1np.float64.mro()[numpy.float64, numpy.floating, numpy.inexact, numpy.number, numpy.generic, float, object] 高级数组操作数组重塑123arr = np.arange(8)arrarr.reshape((4, 2))array([0, 1, 2, 3, 4, 5, 6, 7]) array([[0, 1], [2, 3], [4, 5], [6, 7]]) 1arr.reshape((4, 2)).reshape((2, 4))array([[0, 1, 2, 3], [4, 5, 6, 7]]) -1代表自动选择合适的维度12arr = np.arange(15)arr.reshape((5, -1))array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) 用其他数组的shape进行重塑123other_arr = np.ones((3, 5))other_arr.shapearr.reshape(other_arr.shape)(3, 5) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) 拉直123arr = np.arange(15).reshape((5, 3))arrarr.ravel()array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) 会产生一个副本1arr.flatten()array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) C vs. Fortran 顺序1234arr = np.arange(12).reshape((3, 4))arrarr.ravel()arr.ravel('F')array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) array([ 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11]) 数组的合并以及拆分1234arr1 = np.array([[1, 2, 3], [4, 5, 6]])arr2 = np.array([[7, 8, 9], [10, 11, 12]])np.concatenate([arr1, arr2], axis=0)np.concatenate([arr1, arr2], axis=1)array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]]) array([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]]) 更方便的方法12np.vstack((arr1, arr2))np.hstack((arr1, arr2))array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]]) array([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]]) 1234567from numpy.random import randnarr = randn(5, 2)arrfirst, second, third = np.split(arr, [1, 3])firstsecondthirdarray([[ 0.9659, 1.3079], [-1.7632, 0.0904], [-0.6033, 0.2266], [-0.4417, -1.8609], [-1.2463, -0.6249]]) array([[ 0.9659, 1.3079]]) array([[-1.7632, 0.0904], [-0.6033, 0.2266]]) array([[-0.4417, -1.8609], [-1.2463, -0.6249]]) 堆叠辅助类更…简洁…12345arr = np.arange(6)arr1 = arr.reshape((3, 2))arr2 = randn(3, 2)np.r_[arr1, arr2]np.c_[np.r_[arr1, arr2], arr]array([[ 0. , 1. ], [ 2. , 3. ], [ 4. , 5. ], [ 0.0376, 1.8236], [ 0.9025, -0.053 ], [-0.6849, 1.6728]]) array([[ 0. , 1. , 0. ], [ 2. , 3. , 1. ], [ 4. , 5. , 2. ], [ 0.0376, 1.8236, 3. ], [ 0.9025, -0.053 , 4. ], [-0.6849, 1.6728, 5. ]]) 1np.c_[1:6, -10:-5]array([[ 1, -10], [ 2, -9], [ 3, -8], [ 4, -7], [ 5, -6]]) 元素的重复操作: tile and repeat元素级重复12arr = np.arange(3)arr.repeat(3)array([0, 0, 0, 1, 1, 1, 2, 2, 2]) 指定重复次数1arr.repeat([2, 3, 4])array([0, 0, 1, 1, 1, 2, 2, 2, 2]) 多维数组需要指定axis123arr = randn(2, 2)arrarr.repeat(2, axis=0)array([[-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142], [-0.4628, 1.1142], [ 0.3637, 0.4341], [ 0.3637, 0.4341]]) 12arr.repeat([2, 3], axis=0)arr.repeat([2, 3], axis=1)array([[-0.4628, 1.1142], [-0.4628, 1.1142], [ 0.3637, 0.4341], [ 0.3637, 0.4341], [ 0.3637, 0.4341]]) array([[-0.4628, -0.4628, 1.1142, 1.1142, 1.1142], [ 0.3637, 0.3637, 0.4341, 0.4341, 0.4341]]) 块级重复12arrnp.tile(arr, 2)array([[-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341]]) 123arrnp.tile(arr, (2, 1))np.tile(arr, (3, 2))array([[-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142], [ 0.3637, 0.4341], [-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341], [-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341], [-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341]]) 花式索引的等价函数: take and put123arr = np.arange(10) * 100inds = [7, 1, 2, 6]arr[inds]array([700, 100, 200, 600]) 12345arr.take(inds)arr.put(inds, 42)arrarr.put(inds, [40, 41, 42, 43])arrarray([700, 100, 200, 600]) array([ 0, 42, 42, 300, 400, 500, 42, 42, 800, 900]) array([ 0, 41, 42, 300, 400, 500, 43, 40, 800, 900]) 1234inds = [2, 0, 2, 1]arr = randn(2, 4)arrarr.take(inds, axis=1)array([[ 0.2772, -1.3059, -1.4607, -0.4856], [ 1.5585, -0.4521, -1.6259, -1.6644]]) array([[-1.4607, 0.2772, -1.4607, -1.3059], [-1.6259, 1.5585, -1.6259, -0.4521]]) 广播每一个元素都乘以4123arr = np.arange(5)arrarr * 4array([0, 1, 2, 3, 4]) array([ 0, 4, 8, 12, 16]) 每一维对应减去均值12345arr = randn(4, 3)arr.mean(0)demeaned = arr - arr.mean(0)demeaneddemeaned.mean(0)array([-0.1556, 0.3494, -0.2545]) array([[-0.3753, 0.5353, 1.3534], [-0.4282, 0.5606, 0.8935], [-0.0956, -0.9767, -1.2444], [ 0.899 , -0.1192, -1.0024]]) array([ -5.5511e-17, -1.3878e-17, 0.0000e+00]) 12345arrrow_means = arr.mean(1)row_means.reshape((4, 1))demeaned = arr - row_means.reshape((4, 1))demeaned.mean(1)array([[-0.5308, 0.8848, 1.0989], [-0.5837, 0.91 , 0.639 ], [-0.2511, -0.6273, -1.4989], [ 0.7434, 0.2302, -1.2569]]) array([[ 0.4843], [ 0.3218], [-0.7924], [-0.0944]]) array([ 7.4015e-17, 0.0000e+00, 0.0000e+00, 0.0000e+00]) 沿其他轴向广播维度不对应1arr - arr.mean(1)--------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-31-7b87b85a20b2&gt; in &lt;module&gt;() ----&gt; 1 arr - arr.mean(1) ValueError: operands could not be broadcast together with shapes (4,3) (4,) 1arr - arr.mean(1).reshape((4, 1))array([[-1.0151, 0.4005, 0.6146], [-0.9055, 0.5882, 0.3173], [ 0.5413, 0.1652, -0.7065], [ 0.8378, 0.3246, -1.1625]]) 1234arr = np.zeros((4, 4))arr_3d = arr[:, np.newaxis]arr_3darr_3d.shapearray([[[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]]]) (4, 1, 4) 1234arr_1d = np.random.normal(size=3)arr_1darr_1d[:, np.newaxis]arr_1d[np.newaxis, :]array([-1.1083, 0.5576, 1.2277]) array([[-1.1083], [ 0.5576], [ 1.2277]]) array([[-1.1083, 0.5576, 1.2277]]) 123456arr = randn(3, 4, 5)arrdepth_means = arr.mean(2)depth_meansdemeaned = arr - depth_means[:, :, np.newaxis]demeaned.mean(2)array([[[-1.9966, -0.2431, -0.992 , 0.8283, -0.5073], [-0.3938, -0.1332, -0.7427, 0.3094, -0.9241], [ 1.1069, -0.5383, -0.9288, 0.0233, -0.4678], [-1.2015, 0.6905, 1.6706, -0.1703, -1.3975]], [[-0.3048, -1.7181, -0.189 , 0.6263, 1.1194], [ 0.0823, -0.7132, -0.5162, 1.5305, -1.199 ], [ 0.5777, 1.2935, 0.1547, -1.3637, 0.4251], [ 0.4923, 1.4004, 0.3646, 0.1594, -0.7334]], [[ 1.3836, -0.5313, 0.2826, 0.4739, -1.3435], [-1.141 , -0.3084, 1.1364, 1.1326, 0.3064], [-0.9692, 1.0229, -0.0246, 1.4484, -1.137 ], [ 1.7033, -1.8358, 1.2087, -0.5463, 0.5904]]]) array([[-0.5822, -0.3769, -0.1609, -0.0816], [-0.0932, -0.1631, 0.2174, 0.3367], [ 0.0531, 0.2252, 0.0681, 0.2241]]) array([[ 8.8818e-17, 0.0000e+00, -4.4409e-17, -8.8818e-17], [ 0.0000e+00, 0.0000e+00, 2.7756e-17, 8.8818e-17], [ 4.4409e-17, 5.5511e-17, 4.4409e-17, 0.0000e+00]]) 1234567def demean_axis(arr, axis=0): means = arr.mean(axis) # This generalized things like [:, :, np.newaxis] to N dimensions indexer = [slice(None)] * arr.ndim # like : indexer[axis] = np.newaxis return arr - means[indexer]通过广播设置数组的值123arr = np.zeros((4, 3))arr[:] = 5arrarray([[ 5., 5., 5.], [ 5., 5., 5.], [ 5., 5., 5.], [ 5., 5., 5.]]) 12345col = np.array([1.28, -0.42, 0.44, 1.6])arr[:] = col[:, np.newaxis]arrarr[:2] = [[-1.37], [0.509]]arrarray([[ 1.28, 1.28, 1.28], [-0.42, -0.42, -0.42], [ 0.44, 0.44, 0.44], [ 1.6 , 1.6 , 1.6 ]]) array([[-1.37 , -1.37 , -1.37 ], [ 0.509, 0.509, 0.509], [ 0.44 , 0.44 , 0.44 ], [ 1.6 , 1.6 , 1.6 ]]) ufunc高级应用ufunc实例方法reduce通过一系列的二元运算对其值进行聚合（可指明轴向）123arr = np.arange(10)np.add.reduce(arr)arr.sum()45 45 1np.random.seed(12346)这里聚合的是逻辑与操作123456arr = randn(5, 5)arrarr[::2].sort(1) # sort a few rowsarrarr[:, :-1] &lt; arr[:, 1:]np.logical_and.reduce(arr[:, :-1] &lt; arr[:, 1:], axis=1)array([[-0.7066, 0.4268, -0.2776, -0.8283, -2.7628], [ 0.9835, 0.4378, -0.8496, 0.7188, 0.7329], [ 0.5047, -0.7893, 0.5392, 1.2907, 0.8676], [ 0.4113, 0.4459, -0.3172, -1.0493, 1.3459], [ 0.356 , -0.0915, -0.535 , -0.036 , -0.2591]]) array([[-2.7628, -0.8283, -0.7066, -0.2776, 0.4268], [ 0.9835, 0.4378, -0.8496, 0.7188, 0.7329], [-0.7893, 0.5047, 0.5392, 0.8676, 1.2907], [ 0.4113, 0.4459, -0.3172, -1.0493, 1.3459], [-0.535 , -0.2591, -0.0915, -0.036 , 0.356 ]]) array([[ True, True, True, True], [False, False, True, True], [ True, True, True, True], [ True, False, False, True], [ True, True, True, True]], dtype=bool) array([ True, False, True, False, True], dtype=bool) 相对于reduce只输出最后结果，accumulate保留中间结果12arr = np.arange(15).reshape((3, 5))np.add.accumulate(arr, axis=1)array([[ 0, 1, 3, 6, 10], [ 5, 11, 18, 26, 35], [10, 21, 33, 46, 60]], dtype=int32) outer计算两个数组的叉积123arr = np.arange(3).repeat([1, 2, 2])arrnp.multiply.outer(arr, np.arange(5))array([0, 1, 1, 2, 2]) array([[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8], [0, 2, 4, 6, 8]]) outer输出结果的维度是输入两个数组的维度之和12result = np.subtract.outer(randn(3, 4), randn(5))result.shape(3, 4, 5) 12arr = np.arange(10)np.add.reduceat(arr, [0, 5, 8])array([10, 18, 17], dtype=int32) 123arr = np.multiply.outer(np.arange(4), np.arange(5))arrnp.add.reduceat(arr, [0, 2, 4], axis=1)array([[ 0, 0, 0, 0, 0], [ 0, 1, 2, 3, 4], [ 0, 2, 4, 6, 8], [ 0, 3, 6, 9, 12]]) array([[ 0, 0, 0], [ 1, 5, 4], [ 2, 10, 8], [ 3, 15, 12]], dtype=int32) 自定义 ufuncs两种不同的调用方式1234def add_elements(x, y): return x + yadd_them = np.frompyfunc(add_elements, 2, 1) # 2 input and 1 outputadd_them(np.arange(8), np.arange(8))array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object) 12add_them = np.vectorize(add_elements, otypes=[np.float64])add_them(np.arange(8), np.arange(8))array([ 0., 2., 4., 6., 8., 10., 12., 14.]) 自己实现的还是比不上内置优化过的函数123arr = randn(10000)%timeit add_them(arr, arr)%timeit np.add(arr, arr)100 loops, best of 3: 1.81 ms per loop The slowest run took 16.51 times longer than the fastest. This could mean that an intermediate result is being cached. 100000 loops, best of 3: 3.65 µs per loop 结构化和记录式数组123dtype = [('x', np.float64), ('y', np.int32)]sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)sarrarray([(1.5, 6), (3.141592653589793, -2)], dtype=[(&#39;x&#39;, &#39;&lt;f8&#39;), (&#39;y&#39;, &#39;&lt;i4&#39;)]) 12sarr[0]sarr[0]['y'](1.5, 6) 6 1sarr['x']array([ 1.5 , 3.1416]) 嵌套dtype和多维字段123dtype = [('x', np.int64, 3), ('y', np.int32)]arr = np.zeros(4, dtype=dtype)arrarray([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0)], dtype=[(&#39;x&#39;, &#39;&lt;i8&#39;, (3,)), (&#39;y&#39;, &#39;&lt;i4&#39;)]) 1arr[0]['x']array([0, 0, 0], dtype=int64) 1arr['x']array([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=int64) 12345dtype = [('x', [('a', 'f8'), ('b', 'f4')]), ('y', np.int32)]data = np.array([((1, 2), 5), ((3, 4), 6)], dtype=dtype)data['x']data['y']data['x']['a']array([(1.0, 2.0), (3.0, 4.0)], dtype=[(&#39;a&#39;, &#39;&lt;f8&#39;), (&#39;b&#39;, &#39;&lt;f4&#39;)]) array([5, 6]) array([ 1., 3.]) 更多有关排序的话题123arr = randn(6)arr.sort()arrarray([-1.3918, -0.2089, 0.2316, 0.728 , 0.8356, 1.9956]) 1234arr = randn(3, 5)arrarr[:, 0].sort() # Sort first column values in-placearrarray([[ -2.9812e-01, 1.2037e+00, -1.5768e-02, 7.4395e-01, 8.6880e-01], [ -4.2865e-01, 7.1886e-01, -1.4510e+00, 1.0510e-01, -1.7942e+00], [ -2.8792e-04, 6.1168e-01, -9.1210e-02, -1.2799e+00, -4.0230e-02]]) array([[ -4.2865e-01, 1.2037e+00, -1.5768e-02, 7.4395e-01, 8.6880e-01], [ -2.9812e-01, 7.1886e-01, -1.4510e+00, 1.0510e-01, -1.7942e+00], [ -2.8792e-04, 6.1168e-01, -9.1210e-02, -1.2799e+00, -4.0230e-02]]) 1234arr = randn(5)arrnp.sort(arr)arrarray([-0.9699, -0.5626, 1.1172, 0.2791, -1.1148]) array([-1.1148, -0.9699, -0.5626, 0.2791, 1.1172]) array([-0.9699, -0.5626, 1.1172, 0.2791, -1.1148]) 1234arr = randn(3, 5)arrarr.sort(axis=1)arrarray([[ 0.2266, 0.3405, 2.6439, -1.6262, -0.3976], [-1.4821, 1.068 , -0.252 , -0.9331, 2.2639], [-0.2311, 1.1472, 0.9287, -0.9023, 1.1761]]) array([[-1.6262, -0.3976, 0.2266, 0.3405, 2.6439], [-1.4821, -0.9331, -0.252 , 1.068 , 2.2639], [-0.9023, -0.2311, 0.9287, 1.1472, 1.1761]]) 1arr[:, ::-1]array([[ 2.6439, 0.3405, 0.2266, -0.3976, -1.6262], [ 2.2639, 1.068 , -0.252 , -0.9331, -1.4821], [ 1.1761, 1.1472, 0.9287, -0.2311, -0.9023]]) 间接排序: argsort and lexsort1234values = np.array([5, 0, 1, 3, 2])indexer = values.argsort()indexervalues[indexer]array([1, 2, 4, 3, 0], dtype=int64) array([0, 1, 2, 3, 5]) 1234arr = randn(3, 5)arr[0] = valuesarrarr[:, arr[0].argsort()]array([[ 5. , 0. , 1. , 3. , 2. ], [ 0.422 , 0.1187, 1.1352, 1.4363, -1.2487], [ 0.1909, -1.0984, 0.7886, -0.5827, 1.1592]]) array([[ 0. , 1. , 2. , 3. , 5. ], [ 0.1187, 1.1352, -1.2487, 1.4363, 0.422 ], [-1.0984, 0.7886, 1.1592, -0.5827, 0.1909]]) 1234first_name = np.array(['Bob', 'Jane', 'Steve', 'Bill', 'Barbara'])last_name = np.array(['Jones', 'Arnold', 'Arnold', 'Jones', 'Walters'])sorter = np.lexsort((first_name, last_name))zip(last_name[sorter], first_name[sorter])&lt;zip at 0x1d1284f87c8&gt; 其他排序算法12345values = np.array(['2:first', '2:second', '1:first', '1:second', '1:third'])key = np.array([2, 2, 1, 1, 1])indexer = key.argsort(kind='mergesort')indexervalues.take(indexer)array([2, 3, 4, 0, 1], dtype=int64) array([&#39;1:first&#39;, &#39;1:second&#39;, &#39;1:third&#39;, &#39;2:first&#39;, &#39;2:second&#39;], dtype=&#39;&lt;U8&#39;) numpy.searchsorted: 在有序数组中查找元素12arr = np.array([0, 1, 7, 12, 15])arr.searchsorted(9)3 1arr.searchsorted([0, 8, 11, 16])array([0, 3, 3, 5], dtype=int64) 123arr = np.array([0, 0, 0, 1, 1, 1, 1])arr.searchsorted([0, 1])arr.searchsorted([0, 1], side='right')array([0, 3], dtype=int64) array([3, 7], dtype=int64) 123data = np.floor(np.random.uniform(0, 10000, size=50))bins = np.array([0, 100, 1000, 5000, 10000])dataarray([ 143., 8957., 309., 2349., 5503., 2754., 4408., 4259., 3313., 3364., 2492., 9977., 4704., 5538., 6089., 5864., 6926., 3677., 8698., 1832., 8931., 6631., 5322., 3712., 9350., 3945., 9514., 3683., 8568., 8247., 7087., 7630., 3392., 8320., 1973., 982., 1672., 7052., 6230., 3894., 1832., 9488., 755., 8522., 1858., 5417., 6162., 7517., 9827., 4458.]) 12labels = bins.searchsorted(data)labelsarray([2, 4, 2, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 2, 3, 4, 4, 3, 3, 4, 2, 4, 3, 4, 4, 4, 4, 3], dtype=int64) 1Series(data).groupby(labels).mean()2 547.250000 3 3178.550000 4 7591.038462 dtype: float64 1np.digitize(data, bins)array([2, 4, 2, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 2, 3, 4, 4, 3, 3, 4, 2, 4, 3, 4, 4, 4, 4, 3], dtype=int64) NumPy matrix class12345678X = np.array([[ 8.82768214, 3.82222409, -1.14276475, 2.04411587], [ 3.82222409, 6.75272284, 0.83909108, 2.08293758], [-1.14276475, 0.83909108, 5.01690521, 0.79573241], [ 2.04411587, 2.08293758, 0.79573241, 6.24095859]])X[:, 0] # one-dimensionaly = X[:, :1] # two-dimensional by slicingXyarray([ 8.8277, 3.8222, -1.1428, 2.0441]) array([[ 8.8277, 3.8222, -1.1428, 2.0441], [ 3.8222, 6.7527, 0.8391, 2.0829], [-1.1428, 0.8391, 5.0169, 0.7957], [ 2.0441, 2.0829, 0.7957, 6.241 ]]) array([[ 8.8277], [ 3.8222], [-1.1428], [ 2.0441]]) 1np.dot(y.T, np.dot(X, y))array([[ 1195.468]]) 12345Xm = np.matrix(X)ym = Xm[:, 0]Xmymym.T * Xm * ymmatrix([[ 8.8277, 3.8222, -1.1428, 2.0441], [ 3.8222, 6.7527, 0.8391, 2.0829], [-1.1428, 0.8391, 5.0169, 0.7957], [ 2.0441, 2.0829, 0.7957, 6.241 ]]) matrix([[ 8.8277], [ 3.8222], [-1.1428], [ 2.0441]]) matrix([[ 1195.468]]) 1Xm.I * Xmatrix([[ 1.0000e+00, 6.9616e-17, -4.0136e-17, 8.1258e-17], [ -2.3716e-17, 1.0000e+00, 2.2230e-17, -2.5721e-17], [ 1.0957e-16, 5.0783e-18, 1.0000e+00, 7.8658e-18], [ -5.7092e-17, -3.7777e-18, 6.2391e-18, 1.0000e+00]]) 高级数组输入输出内存映像文件12mmap = np.memmap('mymmap', dtype='float64', mode='w+', shape=(10000, 10000))mmapmemmap([[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.]]) 1section = mmap[:5]1234section[:] = np.random.randn(5, 10000)mmap.flush()mmapdel mmapmemmap([[-1.273 , -0.1547, 0.7817, ..., 0.3421, 1.0272, -1.8742], [-0.3544, -3.1195, 0.1256, ..., -0.4476, 0.4863, -0.8311], [-1.1117, 0.8186, 2.3934, ..., 0.1061, 1.4123, 0.6489], ..., [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ]]) 12mmap = np.memmap('mymmap', dtype='float64', shape=(10000, 10000))mmapmemmap([[-1.273 , -0.1547, 0.7817, ..., 0.3421, 1.0272, -1.8742], [-0.3544, -3.1195, 0.1256, ..., -0.4476, 0.4863, -0.8311], [-1.1117, 0.8186, 2.3934, ..., 0.1061, 1.4123, 0.6489], ..., [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ]]) 12%xdel mmap!del mymmapNameError: name &#39;mmap&#39; is not defined C:\Users\Ewan\Downloads\pydata-book-master\mymmap The process cannot access the file because it is being used by another process. ​性能建议连续内存的重要性12345arr_c = np.ones((1000, 1000), order='C')arr_f = np.ones((1000, 1000), order='F')arr_c.flagsarr_f.flagsarr_f.flags.f_contiguous C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False C_CONTIGUOUS : False F_CONTIGUOUS : True OWNDATA : True WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False True 12%timeit arr_c.sum(1)%timeit arr_f.sum(1)1000 loops, best of 3: 848 µs per loop 1000 loops, best of 3: 582 µs per loop 1arr_f.copy('C').flags C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False 12arr_c[:50].flags.contiguousarr_c[:, :50].flagsTrue C_CONTIGUOUS : False F_CONTIGUOUS : False OWNDATA : False WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False 123%xdel arr_c%xdel arr_f%cd ..C:\Users\Ewan\Downloads ​其他加速手段: Cython, f2py, C12345678910from numpy cimport ndarray, float64_tdef sum_elements(ndarray[float64_t] arr): cdef Py_ssize_t i, n = len(arr) cdef float64_t result = 0 for i in range(n): result += arr[i] return result]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GFW Break]]></title>
      <url>%2F2017%2F03%2F08%2FGFW-Break%2F</url>
      <content type="text"><![CDATA[（Beta版本， 留待以后完善）软件下载百度网盘链接：http://pan.baidu.com/s/1gftCmd1使用方法解压解压后的目录如下：​打开软件：​界面如下：​​​这里需要填一些东西：Server IPServer PortPassword具体值（sscat.txt）我放在了网盘里，链接：http://pan.baidu.com/s/1gftCmd1启动软件：​​右键这个小飞机图标（可能你的颜色看起来不一样，是暗蓝色），会出现如下界面：勾选上第一项，然后将鼠标移到第二项上：选择PAC模式（这个模式会自动检测你所进入的网站是否需要翻墙，所以选择这个模式就可以了，如果不行的话，勾选下面的Global即可）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note Ch10]]></title>
      <url>%2F2017%2F03%2F08%2Fpython-data-analysis-learning-note-Ch10%2F</url>
      <content type="text"><![CDATA[时间序列123456789from __future__ import divisionfrom pandas import Series, DataFrameimport pandas as pdfrom numpy.random import randnimport numpy as nppd.options.display.max_rows = 12np.set_printoptions(precision=4, suppress=True)import matplotlib.pyplot as pltplt.rc('figure', figsize=(12, 4))12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inline日期和时间数据类型及工具123from datetime import datetimenow = datetime.now()nowdatetime.datetime(2017, 3, 8, 14, 47, 50, 32019) 1now.year, now.month, now.day(2017, 3, 8) 返回值（天数，秒数）12delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)deltadatetime.timedelta(926, 56700) 1delta.days926 1delta.seconds56700 timedelta 天数123from datetime import timedeltastart = datetime(2011, 1, 7)start + timedelta(12)datetime.datetime(2011, 1, 19, 0, 0) 1start - 2 * timedelta(12)datetime.datetime(2010, 12, 14, 0, 0) 字符串和datatime的相互转换1stamp = datetime(2011, 1, 3)使用str直接转换1str(stamp)&#39;2011-01-03 00:00:00&#39; 格式化转换1stamp.strftime('%Y-%m-%d')&#39;2011-01-03&#39; 逆转换12value = '2011-01-03'datetime.strptime(value, '%Y-%m-%d')datetime.datetime(2011, 1, 3, 0, 0) 批量转换12datestrs = ['7/6/2011', '8/6/2011'][datetime.strptime(x, '%m/%d/%Y') for x in datestrs][datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)] 总是写格式很麻烦，直接调用parser解析12from dateutil.parser import parseparse('2011-01-03')datetime.datetime(2011, 1, 3, 0, 0) 可以解析任意格式1parse('Jan 31, 1997 10:45 PM')datetime.datetime(1997, 1, 31, 22, 45) 指定格式1parse('6/12/2011', dayfirst=True)datetime.datetime(2011, 12, 6, 0, 0) 1datestrs[&#39;7/6/2011&#39;, &#39;8/6/2011&#39;] pandas的API12pd.to_datetime(datestrs)# note: output changed (no '00:00:00' anymore)DatetimeIndex([&#39;2011-07-06&#39;, &#39;2011-08-06&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) None也可以转换，只不过会变成缺失值12idx = pd.to_datetime(datestrs + [None])idxDatetimeIndex([&#39;2011-07-06&#39;, &#39;2011-08-06&#39;, &#39;NaT&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) 1idx[2]NaT 1pd.isnull(idx)array([False, False, True], dtype=bool) 时间序列基础将行索引变成时间类型，也就是时间戳12345from datetime import datetimedates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7), datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]ts = Series(np.random.randn(6), index=dates)ts2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 12type(ts)# note: output changed to "pandas.core.series.Series"pandas.core.series.Series 拥有一个特定的类型1ts.indexDatetimeIndex([&#39;2011-01-02&#39;, &#39;2011-01-05&#39;, &#39;2011-01-07&#39;, &#39;2011-01-08&#39;, &#39;2011-01-10&#39;, &#39;2011-01-12&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) 可以直接进行加法运算，相同的时间戳会进行匹配1ts + ts[::2]2011-01-02 -0.593708 2011-01-05 NaN 2011-01-07 -0.968984 2011-01-08 NaN 2011-01-10 -0.697394 2011-01-12 NaN dtype: float64 以纳秒形式存储时间戳12ts.index.dtype# note: output changed from dtype('datetime64[ns]') to dtype('&lt;M8[ns]')dtype(&#39;&lt;M8[ns]&#39;) 行索引就会变成时间戳类型123stamp = ts.index[0]stamp# note: output changed from &lt;Timestamp: 2011-01-02 00:00:00&gt; to Timestamp('2011-01-02 00:00:00')Timestamp(&#39;2011-01-02 00:00:00&#39;) 索引、选取、子集构造时间戳索引与正常索引行为一样12stamp = ts.index[2]ts[stamp]-0.4844920247591406 可以直接通过传入与行索引相匹配的时间戳进行索引1ts['1/10/2011']-0.34869693931763396 换个格式也可以，会自动转换为datatime，只要最后转换成的时间戳是相同的，任意格式都可以1ts['20110110']-0.34869693931763396 通过periods参数来指定往后顺延的时间长短123longer_ts = Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))longer_ts2000-01-01 0.871808 2000-01-02 -0.025158 2000-01-03 0.132813 2000-01-04 -2.006494 2000-01-05 -0.988423 2000-01-06 0.775930 ... 2002-09-21 -0.186519 2002-09-22 0.881745 2002-09-23 -1.335826 2002-09-24 0.418774 2002-09-25 0.970405 2002-09-26 0.636320 Freq: D, dtype: float64 时间戳的特殊之处在于可以进行年份以及月份等的选取，相当于一个多维索引1longer_ts['2001']2001-01-01 -1.799866 2001-01-02 0.499890 2001-01-03 -0.409970 2001-01-04 -0.808111 2001-01-05 -1.220433 2001-01-06 0.581235 ... 2001-12-26 -0.312186 2001-12-27 -0.804940 2001-12-28 -0.572741 2001-12-29 -0.175605 2001-12-30 0.693675 2001-12-31 -0.196274 Freq: D, dtype: float64 1longer_ts['2001-05']2001-05-01 -2.783535 2001-05-02 1.386292 2001-05-03 0.153705 2001-05-04 -0.571590 2001-05-05 -0.933012 2001-05-06 0.579244 ... 2001-05-26 0.080809 2001-05-27 0.652650 2001-05-28 0.862616 2001-05-29 -0.967580 2001-05-30 0.907069 2001-05-31 0.551137 Freq: D, dtype: float64 同样可以进行切片，只不过是按照时间的先后度量1ts[datetime(2011, 1, 7):]2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 1ts2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 而且切片不需要进行索引匹配，只需要指定时间范围即可切片1ts['1/6/2011':'1/11/2011']2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 dtype: float64 一个可以实现同样功能的内置方法1ts.truncate(after='1/9/2011')2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 dtype: float64 这里的freq参数指定了选取的频率，这里的是每一个星期三12345dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')long_df = DataFrame(np.random.randn(100, 4), index=dates, columns=['Colorado', 'Texas', 'New York', 'Ohio'])long_df.ix['5-2001']ColoradoTexasNew YorkOhio2001-05-020.506207-1.1162180.6565750.2126062001-05-09-1.306963-0.054373-1.165053-1.3193612001-05-160.891692-0.4639001.6422670.6449722001-05-23-0.0252832.363886-0.3679880.8278822001-05-30-1.501301-2.5345530.2563690.268207带有重复索引的时间序列直接创建时间戳索引1234dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000', '1/2/2000', '1/3/2000'])dup_ts = Series(np.arange(5), index=dates)dup_ts2000-01-01 0 2000-01-02 1 2000-01-02 2 2000-01-02 3 2000-01-03 4 dtype: int32 1dup_ts.index.is_uniqueFalse 1dup_ts['1/3/2000'] # not duplicated4 如果有重复的时间索引，则会将满足条件的全部输出1dup_ts['1/2/2000'] # duplicated2000-01-02 1 2000-01-02 2 2000-01-02 3 dtype: int32 因此可以直接根据时间戳进行索引12grouped = dup_ts.groupby(level=0)grouped.mean()2000-01-01 0 2000-01-02 2 2000-01-03 4 dtype: int32 1grouped.count()2000-01-01 1 2000-01-02 3 2000-01-03 1 dtype: int64 日期的范围、频率以及移动pandas中的时间序列一般被认为是不规则的，也就是说没有固定的频率。但是有时候需要以某种相对固定的频率进行分析，比如每日、每月、每15分钟等（这样自然会在时间序列中引入缺失值）。pandas拥有一整套标准时间序列频率以及用于重采样、频率推断、生成固定频率日期范围的工具1ts2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 例如，我们可以将之前那个时间序列转换为一个具有固定频率（每日）的时间序列。只需要调用resample即可1ts.resample('D').mean()2011-01-02 -0.296854 2011-01-03 NaN 2011-01-04 NaN 2011-01-05 -1.968663 2011-01-06 NaN 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-09 NaN 2011-01-10 -0.348697 2011-01-11 NaN 2011-01-12 0.102276 Freq: D, dtype: float64 生成日期范围data_range函数， 指定始末12index = pd.date_range('4/1/2012', '6/1/2012')indexDatetimeIndex([&#39;2012-04-01&#39;, &#39;2012-04-02&#39;, &#39;2012-04-03&#39;, &#39;2012-04-04&#39;, &#39;2012-04-05&#39;, &#39;2012-04-06&#39;, &#39;2012-04-07&#39;, &#39;2012-04-08&#39;, &#39;2012-04-09&#39;, &#39;2012-04-10&#39;, &#39;2012-04-11&#39;, &#39;2012-04-12&#39;, &#39;2012-04-13&#39;, &#39;2012-04-14&#39;, &#39;2012-04-15&#39;, &#39;2012-04-16&#39;, &#39;2012-04-17&#39;, &#39;2012-04-18&#39;, &#39;2012-04-19&#39;, &#39;2012-04-20&#39;, &#39;2012-04-21&#39;, &#39;2012-04-22&#39;, &#39;2012-04-23&#39;, &#39;2012-04-24&#39;, &#39;2012-04-25&#39;, &#39;2012-04-26&#39;, &#39;2012-04-27&#39;, &#39;2012-04-28&#39;, &#39;2012-04-29&#39;, &#39;2012-04-30&#39;, &#39;2012-05-01&#39;, &#39;2012-05-02&#39;, &#39;2012-05-03&#39;, &#39;2012-05-04&#39;, &#39;2012-05-05&#39;, &#39;2012-05-06&#39;, &#39;2012-05-07&#39;, &#39;2012-05-08&#39;, &#39;2012-05-09&#39;, &#39;2012-05-10&#39;, &#39;2012-05-11&#39;, &#39;2012-05-12&#39;, &#39;2012-05-13&#39;, &#39;2012-05-14&#39;, &#39;2012-05-15&#39;, &#39;2012-05-16&#39;, &#39;2012-05-17&#39;, &#39;2012-05-18&#39;, &#39;2012-05-19&#39;, &#39;2012-05-20&#39;, &#39;2012-05-21&#39;, &#39;2012-05-22&#39;, &#39;2012-05-23&#39;, &#39;2012-05-24&#39;, &#39;2012-05-25&#39;, &#39;2012-05-26&#39;, &#39;2012-05-27&#39;, &#39;2012-05-28&#39;, &#39;2012-05-29&#39;, &#39;2012-05-30&#39;, &#39;2012-05-31&#39;, &#39;2012-06-01&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) 只指定起始， 以及长度1pd.date_range(start='4/1/2012', periods=20)DatetimeIndex([&#39;2012-04-01&#39;, &#39;2012-04-02&#39;, &#39;2012-04-03&#39;, &#39;2012-04-04&#39;, &#39;2012-04-05&#39;, &#39;2012-04-06&#39;, &#39;2012-04-07&#39;, &#39;2012-04-08&#39;, &#39;2012-04-09&#39;, &#39;2012-04-10&#39;, &#39;2012-04-11&#39;, &#39;2012-04-12&#39;, &#39;2012-04-13&#39;, &#39;2012-04-14&#39;, &#39;2012-04-15&#39;, &#39;2012-04-16&#39;, &#39;2012-04-17&#39;, &#39;2012-04-18&#39;, &#39;2012-04-19&#39;, &#39;2012-04-20&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) 只指定结尾，以及长度1pd.date_range(end='6/1/2012', periods=20)DatetimeIndex([&#39;2012-05-13&#39;, &#39;2012-05-14&#39;, &#39;2012-05-15&#39;, &#39;2012-05-16&#39;, &#39;2012-05-17&#39;, &#39;2012-05-18&#39;, &#39;2012-05-19&#39;, &#39;2012-05-20&#39;, &#39;2012-05-21&#39;, &#39;2012-05-22&#39;, &#39;2012-05-23&#39;, &#39;2012-05-24&#39;, &#39;2012-05-25&#39;, &#39;2012-05-26&#39;, &#39;2012-05-27&#39;, &#39;2012-05-28&#39;, &#39;2012-05-29&#39;, &#39;2012-05-30&#39;, &#39;2012-05-31&#39;, &#39;2012-06-01&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) 指定始末，以及采样频率， BM = business end of month1pd.date_range('1/1/2000', '12/1/2000', freq='BM')DatetimeIndex([&#39;2000-01-31&#39;, &#39;2000-02-29&#39;, &#39;2000-03-31&#39;, &#39;2000-04-28&#39;, &#39;2000-05-31&#39;, &#39;2000-06-30&#39;, &#39;2000-07-31&#39;, &#39;2000-08-31&#39;, &#39;2000-09-29&#39;, &#39;2000-10-31&#39;, &#39;2000-11-30&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;BM&#39;) 默认peroids指的是天数1pd.date_range('5/2/2012 12:56:31', periods=5)DatetimeIndex([&#39;2012-05-02 12:56:31&#39;, &#39;2012-05-03 12:56:31&#39;, &#39;2012-05-04 12:56:31&#39;, &#39;2012-05-05 12:56:31&#39;, &#39;2012-05-06 12:56:31&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) 可以省略时间戳1pd.date_range('5/2/2012 12:56:31', periods=5, normalize=True)DatetimeIndex([&#39;2012-05-02&#39;, &#39;2012-05-03&#39;, &#39;2012-05-04&#39;, &#39;2012-05-05&#39;, &#39;2012-05-06&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) 频率和日期偏移量偏移量可以采用特定单位的时间对象123from pandas.tseries.offsets import Hour, Minutehour = Hour()hour&lt;Hour&gt; 4个小时，简单粗暴12four_hours = Hour(4)four_hours&lt;4 * Hours&gt; 每隔四个小时进行采样1pd.date_range('1/1/2000', '1/3/2000 23:59', freq='4h')DatetimeIndex([&#39;2000-01-01 00:00:00&#39;, &#39;2000-01-01 04:00:00&#39;, &#39;2000-01-01 08:00:00&#39;, &#39;2000-01-01 12:00:00&#39;, &#39;2000-01-01 16:00:00&#39;, &#39;2000-01-01 20:00:00&#39;, &#39;2000-01-02 00:00:00&#39;, &#39;2000-01-02 04:00:00&#39;, &#39;2000-01-02 08:00:00&#39;, &#39;2000-01-02 12:00:00&#39;, &#39;2000-01-02 16:00:00&#39;, &#39;2000-01-02 20:00:00&#39;, &#39;2000-01-03 00:00:00&#39;, &#39;2000-01-03 04:00:00&#39;, &#39;2000-01-03 08:00:00&#39;, &#39;2000-01-03 12:00:00&#39;, &#39;2000-01-03 16:00:00&#39;, &#39;2000-01-03 20:00:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;4H&#39;) 两个半小时1Hour(2) + Minute(30)&lt;150 * Minutes&gt; 也可以直接使用这种类似于自然语言的形式1pd.date_range('1/1/2000', periods=10, freq='1h30min')DatetimeIndex([&#39;2000-01-01 00:00:00&#39;, &#39;2000-01-01 01:30:00&#39;, &#39;2000-01-01 03:00:00&#39;, &#39;2000-01-01 04:30:00&#39;, &#39;2000-01-01 06:00:00&#39;, &#39;2000-01-01 07:30:00&#39;, &#39;2000-01-01 09:00:00&#39;, &#39;2000-01-01 10:30:00&#39;, &#39;2000-01-01 12:00:00&#39;, &#39;2000-01-01 13:30:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;90T&#39;) Week of month dates （WOM日期）每月第三个星期五12rng = pd.date_range('1/1/2012', '9/1/2012', freq='WOM-3FRI')list(rng)[Timestamp(&#39;2012-01-20 00:00:00&#39;, offset=&#39;WOM-3FRI&#39;), Timestamp(&#39;2012-02-17 00:00:00&#39;, offset=&#39;WOM-3FRI&#39;), Timestamp(&#39;2012-03-16 00:00:00&#39;, offset=&#39;WOM-3FRI&#39;), Timestamp(&#39;2012-04-20 00:00:00&#39;, offset=&#39;WOM-3FRI&#39;), Timestamp(&#39;2012-05-18 00:00:00&#39;, offset=&#39;WOM-3FRI&#39;), Timestamp(&#39;2012-06-15 00:00:00&#39;, offset=&#39;WOM-3FRI&#39;), Timestamp(&#39;2012-07-20 00:00:00&#39;, offset=&#39;WOM-3FRI&#39;), Timestamp(&#39;2012-08-17 00:00:00&#39;, offset=&#39;WOM-3FRI&#39;)] 移动（超前或滞后）数据123ts = Series(np.random.randn(4), index=pd.date_range('1/1/2000', periods=4, freq='M'))ts2000-01-31 1.294798 2000-02-29 -1.907732 2000-03-31 -1.407750 2000-04-30 0.544825 Freq: M, dtype: float64 整体数据前移1ts.shift(2)2000-01-31 NaN 2000-02-29 NaN 2000-03-31 1.294798 2000-04-30 -1.907732 Freq: M, dtype: float64 整体数据后移，有点类似于位运算中的移位操作1ts.shift(-2)2000-01-31 -1.407750 2000-02-29 0.544825 2000-03-31 NaN 2000-04-30 NaN Freq: M, dtype: float64 移位之后数据对齐1ts / ts.shift(1) - 12000-01-31 NaN 2000-02-29 -2.473382 2000-03-31 -0.262082 2000-04-30 -1.387018 Freq: M, dtype: float64 加入freq之后就是在行索引上进行时间前移1ts.shift(2, freq='M')2000-03-31 1.294798 2000-04-30 -1.907732 2000-05-31 -1.407750 2000-06-30 0.544825 Freq: M, dtype: float64 在天数上进行前移1ts.shift(3, freq='D')2000-02-03 1.294798 2000-03-03 -1.907732 2000-04-03 -1.407750 2000-05-03 0.544825 dtype: float64 另一种实现方式1ts.shift(1, freq='3D')2000-02-03 1.294798 2000-03-03 -1.907732 2000-04-03 -1.407750 2000-05-03 0.544825 dtype: float64 换一个频率1ts.shift(1, freq='90T')2000-01-31 01:30:00 1.294798 2000-02-29 01:30:00 -1.907732 2000-03-31 01:30:00 -1.407750 2000-04-30 01:30:00 0.544825 Freq: M, dtype: float64 通过偏移量对日期进行位移123from pandas.tseries.offsets import Day, MonthEndnow = datetime(2011, 11, 17)now + 3 * Day()Timestamp(&#39;2011-11-20 00:00:00&#39;) 直接移位到月末，是一个相对位移1now + MonthEnd()Timestamp(&#39;2011-11-30 00:00:00&#39;) 传入的参数表示第几个月的月末1now + MonthEnd(2)Timestamp(&#39;2011-12-31 00:00:00&#39;) 换一种方式实现，“主语”不同12offset = MonthEnd()offset.rollforward(now)Timestamp(&#39;2011-11-30 00:00:00&#39;) 往回走，上一个月的月末1offset.rollback(now)Timestamp(&#39;2011-10-31 00:00:00&#39;) 对日期进行移位之后分组123ts = Series(np.random.randn(20), index=pd.date_range('1/15/2000', periods=20, freq='4d'))ts.groupby(offset.rollforward).mean()2000-01-31 -0.610639 2000-02-29 0.029121 2000-03-31 -0.089587 dtype: float64 另一种方式也可以达到相同的效果1ts.resample('M', how='mean')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: how in .resample() is deprecated the new syntax is .resample(...).mean() if __name__ == &#39;__main__&#39;: 2000-01-31 -0.610639 2000-02-29 0.029121 2000-03-31 -0.089587 Freq: M, dtype: float64 时区处理显示一些时区12import pytzpytz.common_timezones[-5:][&#39;US/Eastern&#39;, &#39;US/Hawaii&#39;, &#39;US/Mountain&#39;, &#39;US/Pacific&#39;, &#39;UTC&#39;] 显示某个时区的具体信息12tz = pytz.timezone('US/Eastern')tz&lt;DstTzInfo &#39;US/Eastern&#39; LMT-1 day, 19:04:00 STD&gt; 本地化和转换123rng = pd.date_range('3/9/2012 9:30', periods=6, freq='D')ts = Series(np.random.randn(len(rng)), index=rng)ts2012-03-09 09:30:00 0.065144 2012-03-10 09:30:00 -0.391505 2012-03-11 09:30:00 1.207495 2012-03-12 09:30:00 1.516354 2012-03-13 09:30:00 -0.253149 2012-03-14 09:30:00 -0.768138 Freq: D, dtype: float64 没有指定时区的时候默认时区为None1print(ts.index.tz)None ​指定时区1pd.date_range('3/9/2012 9:30', periods=10, freq='D', tz='UTC')DatetimeIndex([&#39;2012-03-09 09:30:00+00:00&#39;, &#39;2012-03-10 09:30:00+00:00&#39;, &#39;2012-03-11 09:30:00+00:00&#39;, &#39;2012-03-12 09:30:00+00:00&#39;, &#39;2012-03-13 09:30:00+00:00&#39;, &#39;2012-03-14 09:30:00+00:00&#39;, &#39;2012-03-15 09:30:00+00:00&#39;, &#39;2012-03-16 09:30:00+00:00&#39;, &#39;2012-03-17 09:30:00+00:00&#39;, &#39;2012-03-18 09:30:00+00:00&#39;], dtype=&#39;datetime64[ns, UTC]&#39;, freq=&#39;D&#39;) 进行时区的转换12ts_utc = ts.tz_localize('UTC')ts_utc2012-03-09 09:30:00+00:00 0.065144 2012-03-10 09:30:00+00:00 -0.391505 2012-03-11 09:30:00+00:00 1.207495 2012-03-12 09:30:00+00:00 1.516354 2012-03-13 09:30:00+00:00 -0.253149 2012-03-14 09:30:00+00:00 -0.768138 Freq: D, dtype: float64 1ts_utc.indexDatetimeIndex([&#39;2012-03-09 09:30:00+00:00&#39;, &#39;2012-03-10 09:30:00+00:00&#39;, &#39;2012-03-11 09:30:00+00:00&#39;, &#39;2012-03-12 09:30:00+00:00&#39;, &#39;2012-03-13 09:30:00+00:00&#39;, &#39;2012-03-14 09:30:00+00:00&#39;], dtype=&#39;datetime64[ns, UTC]&#39;, freq=&#39;D&#39;) 继续转换1ts_utc.tz_convert('US/Eastern')2012-03-09 04:30:00-05:00 0.065144 2012-03-10 04:30:00-05:00 -0.391505 2012-03-11 05:30:00-04:00 1.207495 2012-03-12 05:30:00-04:00 1.516354 2012-03-13 05:30:00-04:00 -0.253149 2012-03-14 05:30:00-04:00 -0.768138 Freq: D, dtype: float64 依旧是转换12ts_eastern = ts.tz_localize('US/Eastern')ts_eastern.tz_convert('UTC')2012-03-09 14:30:00+00:00 0.065144 2012-03-10 14:30:00+00:00 -0.391505 2012-03-11 13:30:00+00:00 1.207495 2012-03-12 13:30:00+00:00 1.516354 2012-03-13 13:30:00+00:00 -0.253149 2012-03-14 13:30:00+00:00 -0.768138 Freq: D, dtype: float64 转转转ts_eastern.tz_convert(‘Europe/Berlin’)转换之前必须要进行本地化1ts.index.tz_localize('Asia/Shanghai')操作时区意识型TimeStamp对象初始化时间戳，本地化，时区转换123stamp = pd.Timestamp('2011-03-12 04:00')stamp_utc = stamp.tz_localize('utc')stamp_utc.tz_convert('US/Eastern')Timestamp(&#39;2011-03-11 23:00:00-0500&#39;, tz=&#39;US/Eastern&#39;) 显式地初始化12stamp_moscow = pd.Timestamp('2011-03-12 04:00', tz='Europe/Moscow')stamp_moscowTimestamp(&#39;2011-03-12 04:00:00+0300&#39;, tz=&#39;Europe/Moscow&#39;) 自1970年1月1日起计算的纳秒数1stamp_utc.value1299902400000000000 这个值是绝对的1stamp_utc.tz_convert('US/Eastern').value1299902400000000000 1234# 30 minutes before DST transitionfrom pandas.tseries.offsets import Hourstamp = pd.Timestamp('2012-03-12 01:30', tz='US/Eastern')stampTimestamp(&#39;2012-03-12 01:30:00-0400&#39;, tz=&#39;US/Eastern&#39;) 进行时间的位移1stamp + Hour()Timestamp(&#39;2012-03-12 02:30:00-0400&#39;, tz=&#39;US/Eastern&#39;) 123# 90 minutes before DST transitionstamp = pd.Timestamp('2012-11-04 00:30', tz='US/Eastern')stampTimestamp(&#39;2012-11-04 00:30:00-0400&#39;, tz=&#39;US/Eastern&#39;) 1stamp + 2 * Hour()Timestamp(&#39;2012-11-04 01:30:00-0500&#39;, tz=&#39;US/Eastern&#39;) 不同时区之间的运算123rng = pd.date_range('3/7/2012 9:30', periods=10, freq='B')ts = Series(np.random.randn(len(rng)), index=rng)ts2012-03-07 09:30:00 -0.461750 2012-03-08 09:30:00 0.947394 2012-03-09 09:30:00 0.703239 2012-03-12 09:30:00 0.266519 2012-03-13 09:30:00 0.302334 2012-03-14 09:30:00 -0.000725 2012-03-15 09:30:00 0.305446 2012-03-16 09:30:00 -1.605358 2012-03-19 09:30:00 1.306474 2012-03-20 09:30:00 0.865511 Freq: B, dtype: float64 最终结果会变成UTC1234ts1 = ts[:7].tz_localize('Europe/London')ts2 = ts1[2:].tz_convert('Europe/Moscow')result = ts1 + ts2result.indexDatetimeIndex([&#39;2012-03-07 09:30:00+00:00&#39;, &#39;2012-03-08 09:30:00+00:00&#39;, &#39;2012-03-09 09:30:00+00:00&#39;, &#39;2012-03-12 09:30:00+00:00&#39;, &#39;2012-03-13 09:30:00+00:00&#39;, &#39;2012-03-14 09:30:00+00:00&#39;, &#39;2012-03-15 09:30:00+00:00&#39;], dtype=&#39;datetime64[ns, UTC]&#39;, freq=&#39;B&#39;) 时期及其算术运算12p = pd.Period(2007, freq='A-DEC')pPeriod(&#39;2007&#39;, &#39;A-DEC&#39;) 1p + 5Period(&#39;2012&#39;, &#39;A-DEC&#39;) 1p - 2Period(&#39;2005&#39;, &#39;A-DEC&#39;) 1pd.Period('2014', freq='A-DEC') - p7 12rng = pd.period_range('1/1/2000', '6/30/2000', freq='M')rngPeriodIndex([&#39;2000-01&#39;, &#39;2000-02&#39;, &#39;2000-03&#39;, &#39;2000-04&#39;, &#39;2000-05&#39;, &#39;2000-06&#39;], dtype=&#39;int64&#39;, freq=&#39;M&#39;) 1Series(np.random.randn(6), index=rng)2000-01 0.061389 2000-02 0.059265 2000-03 0.779627 2000-04 -0.068995 2000-05 -0.451276 2000-06 -1.531821 Freq: M, dtype: float64 123values = ['2001Q3', '2002Q2', '2003Q1']index = pd.PeriodIndex(values, freq='Q-DEC')indexPeriodIndex([&#39;2001Q3&#39;, &#39;2002Q2&#39;, &#39;2003Q1&#39;], dtype=&#39;int64&#39;, freq=&#39;Q-DEC&#39;) 时区的频率转换以十二月为结尾的一个年时期12p = pd.Period('2007', freq='A-DEC')p.asfreq('M', how='start')Period(&#39;2007-01&#39;, &#39;M&#39;) 1p.asfreq('M', how='end')Period(&#39;2007-12&#39;, &#39;M&#39;) 以六月份结尾的一个年时期12p = pd.Period('2007', freq='A-JUN')p.asfreq('M', 'start')Period(&#39;2006-07&#39;, &#39;M&#39;) 1p.asfreq('M', 'end')Period(&#39;2007-06&#39;, &#39;M&#39;) 2007年8月是属于以六月结尾的2008年的时期中12p = pd.Period('Aug-2007', 'M')p.asfreq('A-JUN')Period(&#39;2008&#39;, &#39;A-JUN&#39;) 相当于一个批量操作123rng = pd.period_range('2006', '2009', freq='A-DEC')ts = Series(np.random.randn(len(rng)), index=rng)ts2006 0.634252 2007 -0.738716 2008 0.398145 2009 -1.226529 Freq: A-DEC, dtype: float64 1ts.asfreq('M', how='start')2006-01 0.634252 2007-01 -0.738716 2008-01 0.398145 2009-01 -1.226529 Freq: M, dtype: float64 1ts.asfreq('B', how='end')2006-12-29 0.634252 2007-12-31 -0.738716 2008-12-31 0.398145 2009-12-31 -1.226529 Freq: B, dtype: float64 按季度计算的时间频率以一月为截止的第四个季度12p = pd.Period('2012Q4', freq='Q-JAN')pPeriod(&#39;2012Q4&#39;, &#39;Q-JAN&#39;) 第四个季度的起始日1p.asfreq('D', 'start')Period(&#39;2011-11-01&#39;, &#39;D&#39;) 结束日1p.asfreq('D', 'end')Period(&#39;2012-01-31&#39;, &#39;D&#39;) 截止日前一天的下午四点12p4pm = (p.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60p4pmPeriod(&#39;2012-01-30 16:00&#39;, &#39;T&#39;) 转化成时间戳对象1p4pm.to_timestamp()Timestamp(&#39;2012-01-30 16:00:00&#39;) 123rng = pd.period_range('2011Q3', '2012Q4', freq='Q-JAN')ts = Series(np.arange(len(rng)), index=rng)ts2011Q3 0 2011Q4 1 2012Q1 2 2012Q2 3 2012Q3 4 2012Q4 5 Freq: Q-JAN, dtype: int32 批量转化为时间戳123new_rng = (rng.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60ts.index = new_rng.to_timestamp()ts2010-10-28 16:00:00 0 2011-01-28 16:00:00 1 2011-04-28 16:00:00 2 2011-07-28 16:00:00 3 2011-10-28 16:00:00 4 2012-01-30 16:00:00 5 dtype: int32 将时间戳转化为时期（以及其逆过程）1234rng = pd.date_range('1/1/2000', periods=3, freq='M')ts = Series(randn(3), index=rng)pts = ts.to_period()ts2000-01-31 0.239752 2000-02-29 -0.469201 2000-03-31 2.835243 Freq: M, dtype: float64 默认以月份为单位进行转化1pts2000-01 0.239752 2000-02 -0.469201 2000-03 2.835243 Freq: M, dtype: float64 转化为月份为单位的时期123rng = pd.date_range('1/29/2000', periods=6, freq='D')ts2 = Series(randn(6), index=rng)ts2.to_period('M')2000-01 1.126773 2000-01 -0.979309 2000-01 -0.784376 2000-02 -1.490820 2000-02 1.125043 2000-02 0.421830 Freq: M, dtype: float64 12pts = ts.to_period()pts2000-01 0.239752 2000-02 -0.469201 2000-03 2.835243 Freq: M, dtype: float64 逆向转换1pts.to_timestamp(how='end')2000-01-31 0.239752 2000-02-29 -0.469201 2000-03-31 2.835243 Freq: M, dtype: float64 通过数组创建PeriodIndex12data = pd.read_csv('ch08/macrodata.csv')data.year0 1959.0 1 1959.0 2 1959.0 3 1959.0 4 1960.0 5 1960.0 ... 197 2008.0 198 2008.0 199 2008.0 200 2009.0 201 2009.0 202 2009.0 Name: year, dtype: float64 1data.quarter0 1.0 1 2.0 2 3.0 3 4.0 4 1.0 5 2.0 ... 197 2.0 198 3.0 199 4.0 200 1.0 201 2.0 202 3.0 Name: quarter, dtype: float64 将年份和季度数据统一起来转化为时期索引数据12index = pd.PeriodIndex(year=data.year, quarter=data.quarter, freq='Q-DEC')indexPeriodIndex([&#39;1959Q1&#39;, &#39;1959Q2&#39;, &#39;1959Q3&#39;, &#39;1959Q4&#39;, &#39;1960Q1&#39;, &#39;1960Q2&#39;, &#39;1960Q3&#39;, &#39;1960Q4&#39;, &#39;1961Q1&#39;, &#39;1961Q2&#39;, ... &#39;2007Q2&#39;, &#39;2007Q3&#39;, &#39;2007Q4&#39;, &#39;2008Q1&#39;, &#39;2008Q2&#39;, &#39;2008Q3&#39;, &#39;2008Q4&#39;, &#39;2009Q1&#39;, &#39;2009Q2&#39;, &#39;2009Q3&#39;], dtype=&#39;int64&#39;, length=203, freq=&#39;Q-DEC&#39;) 12data.index = indexdata.infl1959Q1 0.00 1959Q2 2.34 1959Q3 2.74 1959Q4 0.27 1960Q1 2.31 1960Q2 0.14 ... 2008Q2 8.53 2008Q3 -3.16 2008Q4 -8.79 2009Q1 0.94 2009Q2 3.37 2009Q3 3.56 Freq: Q-DEC, Name: infl, dtype: float64 重采样以及频率转换相当于进行了一次分组操作123rng = pd.date_range('1/1/2000', periods=100, freq='D')ts = Series(randn(len(rng)), index=rng)ts.resample('M', how='mean')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: how in .resample() is deprecated the new syntax is .resample(...).mean() app.launch_new_instance() 2000-01-31 -0.055153 2000-02-29 0.189412 2000-03-31 -0.075940 2000-04-30 -0.239036 Freq: M, dtype: float64 换个索引的形式1ts.resample('M', how='mean', kind='period')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: how in .resample() is deprecated the new syntax is .resample(...).mean() if __name__ == &#39;__main__&#39;: 2000-01 -0.055153 2000-02 0.189412 2000-03 -0.075940 2000-04 -0.239036 Freq: M, dtype: float64 降采样按照分钟进行采样123rng = pd.date_range('1/1/2000', periods=12, freq='T')ts = Series(np.arange(12), index=rng)ts2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 2000-01-01 00:09:00 9 2000-01-01 00:10:00 10 2000-01-01 00:11:00 11 Freq: T, dtype: int32 每5分钟降采样12ts.resample('5min').sum()# note: output changed (as the default changed from closed='right', label='right' to closed='left', label='left'2000-01-01 00:00:00 10 2000-01-01 00:05:00 35 2000-01-01 00:10:00 21 Freq: 5T, dtype: int32 1ts.resample('5min', closed='left').sum()2000-01-01 00:00:00 10 2000-01-01 00:05:00 35 2000-01-01 00:10:00 21 Freq: 5T, dtype: int32 1ts.resample('5min', closed='left', label='left').sum()2000-01-01 00:00:00 10 2000-01-01 00:05:00 35 2000-01-01 00:10:00 21 Freq: 5T, dtype: int32 加了个时间的偏移1ts.resample('5min', loffset='-1s').sum()1999-12-31 23:59:59 10 2000-01-01 00:04:59 35 2000-01-01 00:09:59 21 Freq: 5T, dtype: int32 Open-High-Low-Close (OHLC) 降采样1ts2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 2000-01-01 00:09:00 9 2000-01-01 00:10:00 10 2000-01-01 00:11:00 11 Freq: T, dtype: int32 以5分钟为单位12ts.resample('5min').ohlc()# note: output changed because of changed defaultsopenhighlowclose2000-01-01 00:00:0004042000-01-01 00:05:0059592000-01-01 00:10:0010111011通过GroupBy进行重采样123rng = pd.date_range('1/1/2000', periods=100, freq='D')ts = Series(np.arange(100), index=rng)ts.groupby(lambda x: x.month).mean()1 15 2 45 3 75 4 95 dtype: int32 1ts.groupby(lambda x: x.weekday).mean()0 47.5 1 48.5 2 49.5 3 50.5 4 51.5 5 49.0 6 50.0 dtype: float64 升采样和插值1234frame = DataFrame(np.random.randn(2, 4), index=pd.date_range('1/1/2000', periods=2, freq='W-WED'), columns=['Colorado', 'Texas', 'New York', 'Ohio'])frameColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-12-0.5871240.612993-0.796000-0.34113812df_daily = frame.resample('D').mean()df_dailyColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-06NaNNaNNaNNaN2000-01-07NaNNaNNaNNaN2000-01-08NaNNaNNaNNaN2000-01-09NaNNaNNaNNaN2000-01-10NaNNaNNaNNaN2000-01-11NaNNaNNaNNaN2000-01-12-0.5871240.612993-0.796000-0.3411381frame.resample('D').ffill()ColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-060.3607730.5064291.1664241.4023362000-01-070.3607730.5064291.1664241.4023362000-01-080.3607730.5064291.1664241.4023362000-01-090.3607730.5064291.1664241.4023362000-01-100.3607730.5064291.1664241.4023362000-01-110.3607730.5064291.1664241.4023362000-01-12-0.5871240.612993-0.796000-0.3411381frame.resample('D').ffill(limit=2)ColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-060.3607730.5064291.1664241.4023362000-01-070.3607730.5064291.1664241.4023362000-01-08NaNNaNNaNNaN2000-01-09NaNNaNNaNNaN2000-01-10NaNNaNNaNNaN2000-01-11NaNNaNNaNNaN2000-01-12-0.5871240.612993-0.796000-0.3411381frame.resample('W-THU').ffill()ColoradoTexasNew YorkOhio2000-01-060.3607730.5064291.1664241.4023362000-01-13-0.5871240.612993-0.796000-0.341138通过时期进行重采样1234frame = DataFrame(np.random.randn(24, 4), index=pd.period_range('1-2000', '12-2001', freq='M'), columns=['Colorado', 'Texas', 'New York', 'Ohio'])frame[:5]ColoradoTexasNew YorkOhio2000-01-0.2543400.401110-0.931350-0.8725522000-020.390968-0.815357-1.656213-2.2516212000-030.2062970.1973940.927518-0.6572572000-04-0.4517090.908598-0.187902-0.4980822000-05-0.215150-0.042141-0.7387332.499246以年为单位12annual_frame = frame.resample('A-DEC').mean()annual_frameColoradoTexasNew YorkOhio2000-0.0493830.037021-0.272851-0.1409842001-0.183766-0.2919930.3409410.209276以季度为单位1234# Q-DEC: Quarterly, year ending in Decemberannual_frame.resample('Q-DEC').ffill()# note: output changed, default value changed from convention='end' to convention='start' + 'start' changed to span-like# also the following cellsColoradoTexasNew YorkOhio2000Q1-0.0493830.037021-0.272851-0.1409842000Q2-0.0493830.037021-0.272851-0.1409842000Q3-0.0493830.037021-0.272851-0.1409842000Q4-0.0493830.037021-0.272851-0.1409842001Q1-0.183766-0.2919930.3409410.2092762001Q2-0.183766-0.2919930.3409410.2092762001Q3-0.183766-0.2919930.3409410.2092762001Q4-0.183766-0.2919930.3409410.2092761annual_frame.resample('Q-DEC', fill_method='ffill', convention='start')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() if __name__ == &#39;__main__&#39;: ColoradoTexasNew YorkOhio2000Q1-0.0493830.037021-0.272851-0.1409842000Q2-0.0493830.037021-0.272851-0.1409842000Q3-0.0493830.037021-0.272851-0.1409842000Q4-0.0493830.037021-0.272851-0.1409842001Q1-0.183766-0.2919930.3409410.2092762001Q2-0.183766-0.2919930.3409410.2092762001Q3-0.183766-0.2919930.3409410.2092762001Q4-0.183766-0.2919930.3409410.2092761annual_frame.resample('Q-MAR', fill_method='ffill')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() if __name__ == &#39;__main__&#39;: ColoradoTexasNew YorkOhio2000Q4-0.0493830.037021-0.272851-0.1409842001Q1-0.0493830.037021-0.272851-0.1409842001Q2-0.0493830.037021-0.272851-0.1409842001Q3-0.0493830.037021-0.272851-0.1409842001Q4-0.183766-0.2919930.3409410.2092762002Q1-0.183766-0.2919930.3409410.2092762002Q2-0.183766-0.2919930.3409410.2092762002Q3-0.183766-0.2919930.3409410.209276时间序列绘图1234close_px_all = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)close_px = close_px_all[['AAPL', 'MSFT', 'XOM']]close_px = close_px.resample('B', fill_method='ffill')close_px.info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 2292 entries, 2003-01-02 to 2011-10-14 Freq: B Data columns (total 3 columns): AAPL 2292 non-null float64 MSFT 2292 non-null float64 XOM 2292 non-null float64 dtypes: float64(3) memory usage: 71.6 KB C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() app.launch_new_instance() 按年绘图1close_px['AAPL'].plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb1f85a080&gt; 按月绘图1close_px.ix['2009'].plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb20c4d550&gt; 按天绘图1close_px['AAPL'].ix['01-2011':'03-2011'].plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21235668&gt; 按季度绘图12appl_q = close_px['AAPL'].resample('Q-DEC', fill_method='ffill')appl_q.ix['2009':].plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() if __name__ == &#39;__main__&#39;: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21346c50&gt; 移动窗口函数1close_px = close_px.asfreq('B').fillna(method='ffill')12close_px.AAPL.plot()pd.rolling_mean(close_px.AAPL, 250).plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb213b95f8&gt; C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:2: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=250,center=False).mean() from ipykernel import kernelapp as app &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb213b95f8&gt; 1plt.figure()&lt;matplotlib.figure.Figure at 0x1fb212fb550&gt; &lt;matplotlib.figure.Figure at 0x1fb212fb550&gt; 12appl_std250 = pd.rolling_std(close_px.AAPL, 250, min_periods=10)appl_std250[5:12]C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=250,min_periods=10,center=False).std() if __name__ == &#39;__main__&#39;: 2003-01-09 NaN 2003-01-10 NaN 2003-01-13 NaN 2003-01-14 NaN 2003-01-15 0.077496 2003-01-16 0.074760 2003-01-17 0.112368 Freq: B, Name: AAPL, dtype: float64 1appl_std250.plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21466ba8&gt; 12# Define expanding mean in terms of rolling_meanexpanding_mean = lambda x: rolling_mean(x, len(x), min_periods=1)1pd.rolling_mean(close_px, 60).plot(logy=True)C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pd.rolling_mean is deprecated for DataFrame and will be removed in a future version, replace with DataFrame.rolling(window=60,center=False).mean() if __name__ == &#39;__main__&#39;: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21571208&gt; 1plt.close('all')指数加权函数更好的拟合1234567891011121314fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(12, 7))aapl_px = close_px.AAPL['2005':'2009']ma60 = pd.rolling_mean(aapl_px, 60, min_periods=50)ewma60 = pd.ewma(aapl_px, span=60)aapl_px.plot(style='k-', ax=axes[0])ma60.plot(style='k--', ax=axes[0])aapl_px.plot(style='k-', ax=axes[1])ewma60.plot(style='k--', ax=axes[1])axes[0].set_title('Simple MA')axes[1].set_title('Exponentially-weighted MA')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:6: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=60,min_periods=50,center=False).mean() C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:7: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with Series.ewm(span=60,ignore_na=False,min_periods=0,adjust=True).mean() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21983b70&gt; &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21983b70&gt; &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb219c9a20&gt; &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb219c9a20&gt; &lt;matplotlib.text.Text at 0x1fb219b10b8&gt; &lt;matplotlib.text.Text at 0x1fb219efcc0&gt; 二元移动窗口函数12close_pxspx_px = close_px_all['SPX']AAPLMSFTXOM2003-01-027.4021.1129.222003-01-037.4521.1429.242003-01-067.4521.5229.962003-01-077.4321.9328.952003-01-087.2821.3128.832003-01-097.3421.9329.44............2011-10-07369.8026.2573.562011-10-10388.8126.9476.282011-10-11400.2927.0076.272011-10-12402.1926.9677.162011-10-13408.4327.1876.372011-10-14422.0027.2778.112292 rows × 3 columns1234spx_rets = spx_px / spx_px.shift(1) - 1returns = close_px.pct_change()corr = pd.rolling_corr(returns.AAPL, spx_rets, 125, min_periods=100)corr.plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: pd.rolling_corr is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=125,min_periods=100).corr(other=&lt;Series&gt;) app.launch_new_instance() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21a93438&gt; 12corr = pd.rolling_corr(returns, spx_rets, 125, min_periods=100)corr.plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pd.rolling_corr is deprecated for DataFrame and will be removed in a future version, replace with DataFrame.rolling(window=125,min_periods=100).corr(other=&lt;Series&gt;) if __name__ == &#39;__main__&#39;: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb22b21438&gt; 用户自定义移动窗口函数1234from scipy.stats import percentileofscorescore_at_2percent = lambda x: percentileofscore(x, 0.02)result = pd.rolling_apply(returns.AAPL, 250, score_at_2percent)result.plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: pd.rolling_apply is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=250,center=False).apply(kwargs=&lt;dict&gt;,args=&lt;tuple&gt;,func=&lt;function&gt;) app.launch_new_instance() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb3dbdd2e8&gt; 性能和内存使用方面的注意事项123rng = pd.date_range('1/1/2000', periods=10000000, freq='10ms')ts = Series(np.random.randn(len(rng)), index=rng)ts2000-01-01 00:00:00.000 -0.428577 2000-01-01 00:00:00.010 1.650203 2000-01-01 00:00:00.020 -0.064777 2000-01-01 00:00:00.030 -0.219433 2000-01-01 00:00:00.040 1.907433 2000-01-01 00:00:00.050 0.103347 ... 2000-01-02 03:46:39.940 0.989446 2000-01-02 03:46:39.950 2.333137 2000-01-02 03:46:39.960 0.354455 2000-01-02 03:46:39.970 0.353224 2000-01-02 03:46:39.980 -0.862868 2000-01-02 03:46:39.990 2.007468 Freq: 10L, dtype: float64 1ts.resample('15min').ohlc().info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 11112 entries, 2000-01-01 00:00:00 to 2000-04-25 17:45:00 Freq: 15T Data columns (total 4 columns): open 11112 non-null float64 high 11112 non-null float64 low 11112 non-null float64 close 11112 non-null float64 dtypes: float64(4) memory usage: 434.1 KB 1%timeit ts.resample('15min').ohlc()10 loops, best of 3: 123 ms per loop ​123rng = pd.date_range('1/1/2000', periods=10000000, freq='1s')ts = Series(np.random.randn(len(rng)), index=rng)%timeit ts.resample('15s').ohlc()1 loop, best of 3: 192 ms per loop ​]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 two layer net]]></title>
      <url>%2F2017%2F03%2F08%2Fcs231n-Assignment-1-two-layer-net%2F</url>
      <content type="text"><![CDATA[Implementing a Neural NetworkIn this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.1234567891011121314151617181920# A bit of setupimport numpy as npimport matplotlib.pyplot as pltfrom cs231n.classifiers.neural_net import TwoLayerNet%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# for auto-reloading external modules# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2def rel_error(x, y): """ returns relative error """ return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))We will use the class TwoLayerNet in the file cs231n/classifiers/neural_net.py to represent instances of our network. The network parameters are stored in the instance variable self.params where keys are string parameter names and values are numpy arrays. Below, we initialize toy data and a toy model that we will use to develop your implementation.1234567891011121314151617181920# Create a small net and some toy data to check your implementations.# Note that we set the random seed for repeatable experiments.input_size = 4hidden_size = 10num_classes = 3num_inputs = 5def init_toy_model(): np.random.seed(0) return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)def init_toy_data(): np.random.seed(1) X = 10 * np.random.randn(num_inputs, input_size) y = np.array([0, 1, 2, 2, 1]) return X, ynet = init_toy_model()X, y = init_toy_data()Forward pass: compute scoresOpen the file cs231n/classifiers/neural_net.py and look at the method TwoLayerNet.loss. This function is very similar to the loss functions you have written for the SVM and Softmax exercises: It takes the data and weights and computes the class scores, the loss, and the gradients on the parameters.Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs.1234567891011121314151617scores = net.loss(X)print 'Your scores:'print scoresprintprint 'correct scores:'correct_scores = np.asarray([ [-0.81233741, -1.27654624, -0.70335995], [-0.17129677, -1.18803311, -0.47310444], [-0.51590475, -1.01354314, -0.8504215 ], [-0.15419291, -0.48629638, -0.52901952], [-0.00618733, -0.12435261, -0.15226949]])print correct_scoresprint# The difference should be very small. We get &lt; 1e-7print 'Difference between your scores and correct scores:'print np.sum(np.abs(scores - correct_scores))Your scores: [[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]] correct scores: [[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]] Difference between your scores and correct scores: 3.68027204961e-08 Forward pass: compute lossIn the same function, implement the second part that computes the data and regularizaion loss.123456loss, _ = net.loss(X, y, reg=0.1)correct_loss = 1.30378789133# should be very small, we get &lt; 1e-12print 'Difference between your loss and correct loss:'print np.sum(np.abs(loss - correct_loss))Difference between your loss and correct loss: 1.79412040779e-13 Backward passImplement the rest of the function. This will compute the gradient of the loss with respect to the variables W1, b1, W2, and b2. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check:12345678910111213from cs231n.gradient_check import eval_numerical_gradient# Use numeric gradient checking to check your implementation of the backward pass.# If your implementation is correct, the difference between the numeric and# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.loss, grads = net.loss(X, y, reg=0.1)# these should all be less than 1e-8 or sofor param_name in grads: f = lambda W: net.loss(X, y, reg=0.1)[0] param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False) print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))W1 max relative error: 3.669858e-09 W2 max relative error: 3.440708e-09 b2 max relative error: 3.865028e-11 b1 max relative error: 2.738422e-09 Train the networkTo train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers. Look at the function TwoLayerNet.train and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the SVM and Softmax classifiers. You will also have to implement TwoLayerNet.predict, as the training process periodically performs prediction to keep track of accuracy over time while the network trains.Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.2.12345678910111213net = init_toy_model()stats = net.train(X, y, X, y, learning_rate=1e-1, reg=1e-5, num_iters=100, verbose=False)print 'Final training loss: ', stats['loss_history'][-1]# plot the loss historyplt.plot(stats['loss_history'])plt.xlabel('iteration')plt.ylabel('training loss')plt.title('Training Loss history')plt.show()Final training loss: 0.0171496079387 ​Load the dataNow that you have implemented a two-layer network that passes gradient checks and works on toy data, it’s time to load up our favorite CIFAR-10 data so we can use it to train a classifier on a real dataset.123456789101112131415161718192021222324252627282930313233343536373839404142434445from cs231n.data_utils import load_CIFAR10def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000): """ Load the CIFAR-10 dataset from disk and perform preprocessing to prepare it for the two-layer neural net classifier. These are the same steps as we used for the SVM, but condensed to a single function. """ # Load the raw CIFAR-10 data cifar10_dir = 'cs231n/datasets/cifar-10-batches-py' X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir) # Subsample the data mask = range(num_training, num_training + num_validation) X_val = X_train[mask] y_val = y_train[mask] mask = range(num_training) X_train = X_train[mask] y_train = y_train[mask] mask = range(num_test) X_test = X_test[mask] y_test = y_test[mask] # Normalize the data: subtract the mean image mean_image = np.mean(X_train, axis=0) X_train -= mean_image X_val -= mean_image X_test -= mean_image # Reshape data to rows X_train = X_train.reshape(num_training, -1) X_val = X_val.reshape(num_validation, -1) X_test = X_test.reshape(num_test, -1) return X_train, y_train, X_val, y_val, X_test, y_test# Invoke the above function to get our data.X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTrain data shape: (49000L, 3072L) Train labels shape: (49000L,) Validation data shape: (1000L, 3072L) Validation labels shape: (1000L,) Test data shape: (1000L, 3072L) Test labels shape: (1000L,) Train a networkTo train our network we will use SGD with momentum. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate.1234567891011121314input_size = 32 * 32 * 3hidden_size = 50num_classes = 10net = TwoLayerNet(input_size, hidden_size, num_classes)# Train the networkstats = net.train(X_train, y_train, X_val, y_val, num_iters=1000, batch_size=200, learning_rate=1e-4, learning_rate_decay=0.95, reg=0.5, verbose=True)# Predict on the validation setval_acc = (net.predict(X_val) == y_val).mean()print 'Validation accuracy: ', val_acciteration 0 / 1000: loss 2.302954 iteration 100 / 1000: loss 2.302550 iteration 200 / 1000: loss 2.297648 iteration 300 / 1000: loss 2.259602 iteration 400 / 1000: loss 2.204170 iteration 500 / 1000: loss 2.118565 iteration 600 / 1000: loss 2.051535 iteration 700 / 1000: loss 1.988466 iteration 800 / 1000: loss 2.006591 iteration 900 / 1000: loss 1.951473 Validation accuracy: 0.287 Debug the trainingWith the default parameters we provided above, you should get a validation accuracy of about 0.29 on the validation set. This isn’t very good.One strategy for getting insight into what’s wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized.1234567891011121314# Plot the loss function and train / validation accuraciesplt.subplot(3, 1, 1)plt.plot(stats['loss_history'])plt.title('Loss history')plt.xlabel('Iteration')plt.ylabel('Loss')plt.subplot(3, 1, 3)plt.plot(stats['train_acc_history'], label='train')plt.plot(stats['val_acc_history'], label='val')plt.title('Classification accuracy history')plt.xlabel('Epoch')plt.ylabel('Clasification accuracy')plt.show()123456789101112from cs231n.vis_utils import visualize_grid# Visualize the weights of the networkdef show_net_weights(net): W1 = net.params['W1'] W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2) plt.imshow(visualize_grid(W1, padding=3).astype('uint8')) plt.gca().axis('off') plt.show()show_net_weights(net)Tune your hyperparametersWhat’s wrong?. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.Tuning. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.Approximate results. You should be aim to achieve a classification accuracy of greater than 48% on the validation set. Our best network gets over 52% on the validation set.Experiment: You goal in this exercise is to get as good of a result on CIFAR-10 as you can, with a fully-connected Neural Network. For every 1% above 52% on the Test set we will award you with one extra bonus point. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.).123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960best_net = None # store the best model into this ################################################################################## TODO: Tune hyperparameters using the validation set. Store your best trained ## model in best_net. ## ## To help debug your network, it may help to use visualizations similar to the ## ones we used above; these visualizations will have significant qualitative ## differences from the ones we saw above for the poorly tuned network. ## ## Tweaking hyperparameters by hand can be fun, but you might find it useful to ## write code to sweep through possible combinations of hyperparameters ## automatically like we did on the previous exercises. ##################################################################################best_val = -1best_stats = Nonelearning_rates = [1e-1, 1e-2, 1e-3, 1e-4]regularization_strengths = [1e-1, 1e-2, 1e-3, 1e-4]batch_sizes = [200, 400, 800]hidden_sizes = [80, 160, 320]results = &#123;&#125;iters = 2000total_size = 144i = 0for lr in learning_rates: for rs in regularization_strengths: for bs in batch_sizes: for hs in hidden_sizes: i += 1 print i, '/', total_size net = TwoLayerNet(input_size, hs, num_classes) # Train the network stats = net.train(X_train, y_train, X_val, y_val, num_iters=iters, batch_size=bs, learning_rate=lr, learning_rate_decay=0.95, reg=rs) y_train_pred = net.predict(X_train) acc_train = np.mean(y_train == y_train_pred) y_val_pred = net.predict(X_val) acc_val = np.mean(y_val == y_val_pred) results[(lr, rs, bs, hs)] = (acc_train, acc_val) if best_val &lt; acc_val: best_stats = stats best_val = acc_val best_net = net# Print out results.# for lr, reg, bs, hs in sorted(results):# train_accuracy, val_accuracy = results[(lr, reg, bs, hs)]# print 'lr %e reg %e bs %e hs %e train accuracy: %f val accuracy: %f' % (# lr, reg, bs, hs, train_accuracy, val_accuracy)print 'best validation accuracy achieved during cross-validation: %f' % best_val################################################################################## END OF YOUR CODE ##################################################################################1 / 144 ​cs231n\classifiers\neural_net.py:104: RuntimeWarning: overflow encountered in exp exp_scores = np.exp(scores) cs231n\classifiers\neural_net.py:105: RuntimeWarning: invalid value encountered in divide a2 = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) cs231n\classifiers\neural_net.py:107: RuntimeWarning: divide by zero encountered in log correct_log_probs = -np.log(a2[range(N), y]) cs231n\classifiers\neural_net.py:81: RuntimeWarning: invalid value encountered in maximum a1 = np.maximum(0, z1) cs231n\classifiers\neural_net.py:131: RuntimeWarning: invalid value encountered in less_equal dhidden[z1 &lt;= 0] = 0 cs231n\classifiers\neural_net.py:247: RuntimeWarning: invalid value encountered in maximum a1 = np.maximum(0, z1) # pass through ReLU activation function 2 / 144 3 / 144 4 / 144 5 / 144 6 / 144 7 / 144 8 / 144 9 / 144 10 / 144 11 / 144 12 / 144 13 / 144 14 / 144 15 / 144 16 / 144 17 / 144 18 / 144 19 / 144 20 / 144 21 / 144 22 / 144 23 / 144 24 / 144 25 / 144 26 / 144 27 / 144 28 / 144 29 / 144 30 / 144 31 / 144 32 / 144 33 / 144 34 / 144 35 / 144 36 / 144 37 / 144 38 / 144 39 / 144 40 / 144 41 / 144 42 / 144 43 / 144 44 / 144 45 / 144 46 / 144 47 / 144 48 / 144 49 / 144 50 / 144 51 / 144 52 / 144 53 / 144 54 / 144 55 / 144 56 / 144 57 / 144 58 / 144 59 / 144 60 / 144 61 / 144 62 / 144 63 / 144 64 / 144 65 / 144 66 / 144 67 / 144 68 / 144 69 / 144 70 / 144 71 / 144 72 / 144 73 / 144 74 / 144 75 / 144 76 / 144 77 / 144 78 / 144 79 / 144 80 / 144 81 / 144 82 / 144 83 / 144 84 / 144 85 / 144 86 / 144 87 / 144 88 / 144 89 / 144 90 / 144 91 / 144 92 / 144 93 / 144 94 / 144 95 / 144 96 / 144 97 / 144 98 / 144 99 / 144 100 / 144 101 / 144 102 / 144 103 / 144 104 / 144 105 / 144 106 / 144 107 / 144 108 / 144 109 / 144 110 / 144 111 / 144 112 / 144 113 / 144 114 / 144 115 / 144 116 / 144 117 / 144 118 / 144 119 / 144 120 / 144 121 / 144 122 / 144 123 / 144 124 / 144 125 / 144 126 / 144 127 / 144 128 / 144 129 / 144 130 / 144 131 / 144 132 / 144 133 / 144 134 / 144 135 / 144 136 / 144 137 / 144 138 / 144 139 / 144 140 / 144 141 / 144 142 / 144 143 / 144 144 / 144 best validation accuracy achieved during cross-validation: 0.540000 12# visualize the weights of the best networkshow_net_weights(best_net)Run on the test setWhen you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%.We will give you extra bonus point for every 1% of accuracy above 52%.12test_acc = (best_net.predict(X_test) == y_test).mean()print 'Test accuracy: ', test_accTest accuracy: 0.531 ​Codes123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254import numpy as npimport matplotlib.pyplot as pltclass TwoLayerNet(object): """ A two-layer fully-connected neural network. The net has an input dimension of N, a hidden layer dimension of H, and performs classification over C classes. We train the network with a softmax loss function and L2 regularization on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture: input - fully connected layer - ReLU - fully connected layer - softmax The outputs of the second fully-connected layer are the scores for each class. """ def __init__(self, input_size, hidden_size, output_size, std=1e-4): """ Initialize the model. Weights are initialized to small random values and biases are initialized to zero. Weights and biases are stored in the variable self.params, which is a dictionary with the following keys: W1: First layer weights; has shape (D, H) b1: First layer biases; has shape (H,) W2: Second layer weights; has shape (H, C) b2: Second layer biases; has shape (C,) Inputs: - input_size: The dimension D of the input data. - hidden_size: The number of neurons H in the hidden layer. - output_size: The number of classes C. """ self.params = &#123;&#125; self.params['W1'] = std * np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden_size) self.params['W2'] = std * np.random.randn(hidden_size, output_size) self.params['b2'] = np.zeros(output_size) def loss(self, X, y=None, reg=0.0): """ Compute the loss and gradients for a two layer fully connected neural network. Inputs: - X: Input data of shape (N, D). Each X[i] is a training sample. - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it is not passed then we only return scores, and if it is passed then we instead return the loss and gradients. - reg: Regularization strength. Returns: If y is None, return a matrix scores of shape (N, C) where scores[i, c] is the score for class c on input X[i]. If y is not None, instead return a tuple of: - loss: Loss (data loss and regularization loss) for this batch of training samples. - grads: Dictionary mapping parameter names to gradients of those parameters with respect to the loss function; has the same keys as self.params. """ # Unpack variables from the params dictionary W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] N, D = X.shape # Compute the forward pass scores = None ############################################################################# # TODO: Perform the forward pass, computing the class scores for the input. # # Store the result in the scores variable, which should be an array of # # shape (N,C). # ############################################################################# # First layer pre-activation z1 = X.dot(W1) + b1 # First layer activation a1 = np.maximum(0, z1) # Second layer pre-activation z2 = a1.dot(W2) + b2 scores = z2 ############################################################################# # END OF YOUR CODE # ############################################################################# # If the targets are not given then jump out, we're done if y is None: return scores # Compute the loss loss = None ############################################################################# # TODO: Finish the forward pass, and compute the loss. This should include # # both the data loss and L2 regularization for W1 and W2. Store the result # # in the variable loss, which should be a scalar. Use the Softmax # # classifier loss. So that your results match ours, multiply the # # regularization loss by 0.5 # ############################################################################# exp_scores = np.exp(scores) a2 = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) correct_log_probs = -np.log(a2[range(N), y]) data_loss = np.sum(correct_log_probs) / N reg_loss = 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2)) loss = data_loss + reg_loss ############################################################################# # END OF YOUR CODE # ############################################################################# # Backward pass: compute gradients grads = &#123;&#125; ############################################################################# # TODO: Compute the backward pass, computing the derivatives of the weights # # and biases. Store the results in the grads dictionary. For example, # # grads['W1'] should store the gradient on W1, and be a matrix of same size # ############################################################################# dscores = a2 dscores[range(N), y] -= 1 dscores /= N grads['W2'] = np.dot(a1.T, dscores) grads['b2'] = np.sum(dscores, axis=0) dhidden = np.dot(dscores, W2.T) dhidden[z1 &lt;= 0] = 0 grads['W1'] = np.dot(X.T, dhidden) grads['b1'] = np.sum(dhidden, axis=0) grads['W2'] += reg * W2 grads['W1'] += reg * W1 ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, grads def train(self, X, y, X_val, y_val, learning_rate=1e-3, learning_rate_decay=0.95, reg=1e-5, num_iters=100, batch_size=200, verbose=False): """ Train this neural network using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) giving training data. - y: A numpy array f shape (N,) giving training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - X_val: A numpy array of shape (N_val, D) giving validation data. - y_val: A numpy array of shape (N_val,) giving validation labels. - learning_rate: Scalar giving learning rate for optimization. - learning_rate_decay: Scalar giving factor used to decay the learning rate after each epoch. - reg: Scalar giving regularization strength. - num_iters: Number of steps to take when optimizing. - batch_size: Number of training examples to use per step. - verbose: boolean; if true print progress during optimization. """ num_train = X.shape[0] iterations_per_epoch = max(num_train / batch_size, 1) # Use SGD to optimize the parameters in self.model loss_history = [] train_acc_history = [] val_acc_history = [] for it in xrange(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: Create a random minibatch of training data and labels, storing # # them in X_batch and y_batch respectively. # ######################################################################### sample_indices = np.random.choice(num_train, batch_size) X_batch = X[sample_indices] y_batch = y[sample_indices] ######################################################################### # END OF YOUR CODE # ######################################################################### # Compute loss and gradients using the current minibatch loss, grads = self.loss(X_batch, y=y_batch, reg=reg) loss_history.append(loss) ######################################################################### # TODO: Use the gradients in the grads dictionary to update the # # parameters of the network (stored in the dictionary self.params) # # using stochastic gradient descent. You'll need to use the gradients # # stored in the grads dictionary defined above. # ######################################################################### self.params['W1'] += -learning_rate * grads['W1'] self.params['b1'] += -learning_rate * grads['b1'] self.params['W2'] += -learning_rate * grads['W2'] self.params['b2'] += -learning_rate * grads['b2'] ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print 'iteration %d / %d: loss %f' % (it, num_iters, loss) # Every epoch, check train and val accuracy and decay learning rate. if it % iterations_per_epoch == 0: # Check accuracy train_acc = (self.predict(X_batch) == y_batch).mean() val_acc = (self.predict(X_val) == y_val).mean() train_acc_history.append(train_acc) val_acc_history.append(val_acc) # Decay learning rate learning_rate *= learning_rate_decay return &#123; 'loss_history': loss_history, 'train_acc_history': train_acc_history, 'val_acc_history': val_acc_history, &#125; def predict(self, X): """ Use the trained weights of this two-layer network to predict labels for data points. For each data point we predict scores for each of the C classes, and assign each data point to the class with the highest score. Inputs: - X: A numpy array of shape (N, D) giving N D-dimensional data points to classify. Returns: - y_pred: A numpy array of shape (N,) giving predicted labels for each of the elements of X. For all i, y_pred[i] = c means that X[i] is predicted to have class c, where 0 &lt;= c &lt; C. """ y_pred = None ########################################################################### # TODO: Implement this function; it should be VERY simple! # ########################################################################### z1 = X.dot(self.params['W1']) + self.params['b1'] a1 = np.maximum(0, z1) # pass through ReLU activation function scores = a1.dot(self.params['W2']) + self.params['b2'] y_pred = np.argmax(scores, axis=1) ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note 09]]></title>
      <url>%2F2017%2F03%2F07%2Fpython-data-analysis-learning-note-09%2F</url>
      <content type="text"><![CDATA[数据聚合与分组运算12345678910from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)123pd.options.display.notebook_repr_html = Falsefrom IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inlineGroupBy 机制12345df = DataFrame(&#123;'key1' : ['a', 'a', 'b', 'b', 'a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randn(5), 'data2' : np.random.randn(5)&#125;)df data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one 12grouped = df['data1'].groupby(df['key1'])grouped&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000008BAFA90&gt; 变量groupby是一个GroupBy对象。它实际还没有进行任何计算，只有进行计算之后才能显示结果1grouped.mean()key1 a 0.746672 b -0.537585 Name: data1, dtype: float64 12means = df['data1'].groupby([df['key1'], df['key2']]).mean()meanskey1 key2 a one 0.880536 two 0.478943 b one -0.519439 two -0.555730 Name: data1, dtype: float64 1means.unstack()key2 one two key1 a 0.880536 0.478943 b -0.519439 -0.555730 只要长度相同即可123states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])years = np.array([2005, 2005, 2006, 2005, 2006])df['data1'].groupby([states, years]).mean()California 2005 0.478943 2006 -0.519439 Ohio 2005 -0.380219 2006 1.965781 Name: data1, dtype: float64 只要数值型数据才会出现在结果中1df.groupby('key1').mean() data1 data2 key1 a 0.746672 0.910916 b -0.537585 0.525384 1df.groupby(['key1', 'key2']).mean() data1 data2 key1 key2 a one 0.880536 1.319920 two 0.478943 0.092908 b one -0.519439 0.281746 two -0.555730 0.769023 1df.groupby(['key1', 'key2']).size()key1 key2 a one 2 two 1 b one 1 two 1 dtype: int64 对分组进行迭代显示分组数据（要通过这种迭代的方式才能显示）1df.groupby('key1')&lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000034BC8CF8&gt; 1234dffor name, group in df.groupby('key1'): print(name) print(group) data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one a data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 4 1.965781 1.246435 a one b data1 data2 key1 key2 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 同样进行迭代才能显示结果123for (k1, k2), group in df.groupby(['key1', 'key2']): print((k1, k2)) print(group)(&#39;a&#39;, &#39;one&#39;) data1 data2 key1 key2 0 -0.204708 1.393406 a one 4 1.965781 1.246435 a one (&#39;a&#39;, &#39;two&#39;) data1 data2 key1 key2 1 0.478943 0.092908 a two (&#39;b&#39;, &#39;one&#39;) data1 data2 key1 key2 2 -0.519439 0.281746 b one (&#39;b&#39;, &#39;two&#39;) data1 data2 key1 key2 3 -0.55573 0.769023 b two 将分组结果转化成一个字典（要先转化为一个列表）12pieces = dict(list(df.groupby('key1')))pieces['b'] data1 data2 key1 key2 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 显示每一项的数据类型1df.dtypesdata1 float64 data2 float64 key1 object key2 object dtype: object 对列进行分组…按照数据类型来？！12grouped = df.groupby(df.dtypes, axis=1)dict(list(grouped)){dtype(&#39;float64&#39;): data1 data2 0 -0.204708 1.393406 1 0.478943 0.092908 2 -0.519439 0.281746 3 -0.555730 0.769023 4 1.965781 1.246435, dtype(&#39;O&#39;): key1 key2 0 a one 1 a two 2 b one 3 b two 4 a one} 选择一列或一组列12df.groupby('key1')['data1']df.groupby('key1')[['data2']]&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000034BDC2E8&gt; &lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000015EDF2B0&gt; 上述代码是以下代码的语法糖12df['data1'].groupby(df['key1'])df[['data2']].groupby(df['key1'])&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000034BDC8D0&gt; &lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000034BDC898&gt; 1df.groupby(['key1', 'key2'])[['data2']].mean() data2 key1 key2 a one 1.319920 two 0.092908 b one 0.281746 two 0.769023 12s_grouped = df.groupby(['key1', 'key2'])['data2']s_grouped&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000034BDC6A0&gt; 1s_grouped.mean()key1 key2 a one 1.319920 two 0.092908 b one 0.281746 two 0.769023 Name: data2, dtype: float64 通过字典或Series进行分组12345people = DataFrame(np.random.randn(5, 5), columns=['a', 'b', 'c', 'd', 'e'], index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])people.ix[2:3, ['b', 'c']] = np.nan # Add a few NA valuespeople a b c d e Joe 1.007189 -1.296221 0.274992 0.228913 1.352917 Steve 0.886429 -2.001637 -0.371843 1.669025 -0.438570 Wes -0.539741 NaN NaN -1.021228 -0.577087 Jim 0.124121 0.302614 0.523772 0.000940 1.343810 Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 12mapping = &#123;'a': 'red', 'b': 'red', 'c': 'blue', 'd': 'blue', 'e': 'red', 'f' : 'orange'&#125;会跳过NA值12by_column = people.groupby(mapping, axis=1)by_column.sum() blue red Joe 0.503905 1.063885 Steve 1.297183 -1.553778 Wes -1.021228 -1.116829 Jim 0.524712 1.770545 Travis -4.230992 -2.405455 上述功能同样可以通过Series实现12map_series = Series(mapping)map_seriesa red b red c blue d blue e red f orange dtype: object 1people.groupby(map_series, axis=1).count() blue red Joe 2 3 Steve 2 3 Wes 1 2 Jim 2 3 Travis 2 3 通过函数进行分组根据人名长度进行分组1people.groupby(len).sum() a b c d e 3 0.591569 -0.993608 0.798764 -0.791374 2.119639 5 0.886429 -2.001637 -0.371843 1.669025 -0.438570 6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 再加一个分组度量12key_list = ['one', 'one', 'one', 'two', 'two']people.groupby([len, key_list]).min() a b c d e 3 one -0.539741 -1.296221 0.274992 -1.021228 -0.577087 two 0.124121 0.302614 0.523772 0.000940 1.343810 5 one 0.886429 -2.001637 -0.371843 1.669025 -0.438570 6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 根据索引级别分组1234columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'], [1, 3, 5, 1, 3]], names=['cty', 'tenor'])hier_df = DataFrame(np.random.randn(4, 5), columns=columns)hier_dfcty US JP tenor 1 3 5 1 3 0 0.560145 -1.265934 0.119827 -1.063512 0.332883 1 -2.359419 -0.199543 -1.541996 -0.970736 -1.307030 2 0.286350 0.377984 -0.753887 0.331286 1.349742 3 0.069877 0.246674 -0.011862 1.004812 1.327195 1hier_df.groupby(level='cty', axis=1).count()cty JP US 0 2 3 1 2 3 2 2 3 3 2 3 数据聚合1df data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one 对分组后的数据进行相应操作12grouped = df.groupby('key1')grouped['data1'].quantile(0.9)key1 a 1.668413 b -0.523068 Name: data1, dtype: float64 通过函数进行聚合操作123def peak_to_peak(arr): return arr.max() - arr.min()grouped.agg(peak_to_peak) data1 data2 key1 a 2.170488 1.300498 b 0.036292 0.487276 列出分组后数据的一些常用属性1grouped.describe() data1 data2 key1 a count 3.000000 3.000000 mean 0.746672 0.910916 std 1.109736 0.712217 min -0.204708 0.092908 25% 0.137118 0.669671 50% 0.478943 1.246435 75% 1.222362 1.319920 max 1.965781 1.393406 b count 2.000000 2.000000 mean -0.537585 0.525384 std 0.025662 0.344556 min -0.555730 0.281746 25% -0.546657 0.403565 50% -0.537585 0.525384 75% -0.528512 0.647203 max -0.519439 0.769023 导入一个数据集用于接下来更加高级的聚合操作1234tips = pd.read_csv('ch08/tips.csv')# Add tip percentage of total billtips['tip_pct'] = tips['tip'] / tips['total_bill']tips[:6] total_bill tip sex smoker day time size_ tip_pct 0 16.99 1.01 Female No Sun Dinner 2 0.059447 1 10.34 1.66 Male No Sun Dinner 3 0.160542 2 21.01 3.50 Male No Sun Dinner 3 0.166587 3 23.68 3.31 Male No Sun Dinner 2 0.139780 4 24.59 3.61 Female No Sun Dinner 4 0.146808 5 25.29 4.71 Male No Sun Dinner 4 0.186240 面向列的多函数应用根据性别以及是否吸烟进行分类1grouped = tips.groupby(['sex', 'smoker'])算出不同类型的顾客所给的小费占总花费的比例的平均值12grouped_pct = grouped['tip_pct']grouped_pct.agg('mean')sex smoker Female No 0.156921 Yes 0.182150 Male No 0.160669 Yes 0.152771 Name: tip_pct, dtype: float64 同时算出比例的均值、标准差以及范围大小1grouped_pct.agg(['mean', 'std', peak_to_peak]) mean std peak_to_peak sex smoker Female No 0.156921 0.036421 0.195876 Yes 0.182150 0.071595 0.360233 Male No 0.160669 0.041849 0.220186 Yes 0.152771 0.090588 0.674707 起一个别名1grouped_pct.agg([('foo', 'mean'), ('bar', np.std)]) foo bar sex smoker Female No 0.156921 0.036421 Yes 0.182150 0.071595 Male No 0.160669 0.041849 Yes 0.152771 0.090588 对分组后的数据的两个属性分别做三个不同的操作123functions = ['count', 'mean', 'max']result = grouped['tip_pct', 'total_bill'].agg(functions)result tip_pct total_bill count mean max count mean max sex smoker Female No 54 0.156921 0.252672 54 18.105185 35.83 Yes 33 0.182150 0.416667 33 17.977879 44.30 Male No 97 0.160669 0.291990 97 19.791237 48.33 Yes 60 0.152771 0.710345 60 22.284500 50.81 提取出上述两个属性中的一个1result['tip_pct'] count mean max sex smoker Female No 54 0.156921 0.252672 Yes 33 0.182150 0.416667 Male No 97 0.160669 0.291990 Yes 60 0.152771 0.710345 对多个属性进行多个操作的同时进行起别名的操作12ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]grouped['tip_pct', 'total_bill'].agg(ftuples) tip_pct total_bill Durchschnitt Abweichung Durchschnitt Abweichung sex smoker Female No 0.156921 0.001327 18.105185 53.092422 Yes 0.182150 0.005126 17.977879 84.451517 Male No 0.160669 0.001751 19.791237 76.152961 Yes 0.152771 0.008206 22.284500 98.244673 对不同的列进行不同的操作1grouped.agg(&#123;'tip' : np.max, 'size_' : 'sum'&#125;) size_ tip sex smoker Female No 140 5.2 Yes 74 6.5 Male No 263 9.0 Yes 150 10.0 对不同的列进行数量不同类型不同的操作12grouped.agg(&#123;'tip_pct' : ['min', 'max', 'mean', 'std'], 'size_' : 'sum'&#125;) tip_pct size_ min max mean std sum sex smoker Female No 0.056797 0.252672 0.156921 0.036421 140 Yes 0.056433 0.416667 0.182150 0.071595 74 Male No 0.071804 0.291990 0.160669 0.041849 263 Yes 0.035638 0.710345 0.152771 0.090588 150 以无索引的形式返回聚合数据1tips.groupby(['sex', 'smoker'], as_index=False).mean() sex smoker total_bill tip size_ tip_pct 0 Female No 18.105185 2.773519 2.592593 0.156921 1 Female Yes 17.977879 2.931515 2.242424 0.182150 2 Male No 19.791237 3.113402 2.711340 0.160669 3 Male Yes 22.284500 3.051167 2.500000 0.152771 1tips.groupby(['sex', 'smoker'], as_index=True).mean() total_bill tip size_ tip_pct sex smoker Female No 18.105185 2.773519 2.592593 0.156921 Yes 17.977879 2.931515 2.242424 0.182150 Male No 19.791237 3.113402 2.711340 0.160669 Yes 22.284500 3.051167 2.500000 0.152771 分组级运算和转换1df data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one 12k1_means = df.groupby('key1').mean().add_prefix('mean_')k1_means mean_data1 mean_data2 key1 a 0.746672 0.910916 b -0.537585 0.525384 保留原索引1pd.merge(df, k1_means, left_on='key1', right_index=True) data1 data2 key1 key2 mean_data1 mean_data2 0 -0.204708 1.393406 a one 0.746672 0.910916 1 0.478943 0.092908 a two 0.746672 0.910916 4 1.965781 1.246435 a one 0.746672 0.910916 2 -0.519439 0.281746 b one -0.537585 0.525384 3 -0.555730 0.769023 b two -0.537585 0.525384 另一个例子，以更简洁的方式实现上述功能1people a b c d e Joe 1.007189 -1.296221 0.274992 0.228913 1.352917 Steve 0.886429 -2.001637 -0.371843 1.669025 -0.438570 Wes -0.539741 NaN NaN -1.021228 -0.577087 Jim 0.124121 0.302614 0.523772 0.000940 1.343810 Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 12key = ['one', 'two', 'one', 'two', 'one']people.groupby(key).mean() a b c d e one -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 two 0.505275 -0.849512 0.075965 0.834983 0.452620 将聚合后的结果放回原来数据中合适的位置（标量进行广播）1people.groupby(key).transform(np.mean) a b c d e Joe -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 Steve 0.505275 -0.849512 0.075965 0.834983 0.452620 Wes -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 Jim 0.505275 -0.849512 0.075965 0.834983 0.452620 Travis -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 同时减去均值1234def demean(arr): return arr - arr.mean()demeaned = people.groupby(key).transform(demean)demeaned a b c d e Joe 1.089221 -0.232534 1.322612 1.113271 1.381226 Steve 0.381154 -1.152125 -0.447807 0.834043 -0.891190 Wes -0.457709 NaN NaN -0.136869 -0.548778 Jim -0.381154 1.152125 0.447807 -0.834043 0.891190 Travis -0.631512 0.232534 -1.322612 -0.976402 -0.832448 检验一下1demeaned.groupby(key).mean() a b c d e one 0.000000e+00 -1.110223e-16 0.0 7.401487e-17 0.0 two -2.775558e-17 0.000000e+00 0.0 0.000000e+00 0.0 Apply: 一般性的 “拆分-应用-合并”小费数据，根据某一个属性从大到小进行排序123def top(df, n=5, column='tip_pct'): return df.sort_values(by=column)[-n:]top(tips, n=6) total_bill tip sex smoker day time size_ tip_pct 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 232 11.61 3.39 Male No Sat Dinner 2 0.291990 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 在分组后的数据集上进行上述排序操作，说明分组后的每一组都是一个DataFrame对象1tips.groupby('smoker').apply(top) total_bill tip sex smoker day time size_ tip_pct smoker No 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 1tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill') total_bill tip sex smoker day time size_ \ smoker day No Fri 94 22.75 3.25 Female No Fri Dinner 2 Sat 212 48.33 9.00 Male No Sat Dinner 4 Sun 156 48.17 5.00 Male No Sun Dinner 6 Thur 142 41.19 5.00 Male No Thur Lunch 5 Yes Fri 95 40.17 4.73 Male Yes Fri Dinner 4 Sat 170 50.81 10.00 Male Yes Sat Dinner 3 Sun 182 45.35 3.50 Male Yes Sun Dinner 3 Thur 197 43.11 5.00 Female Yes Thur Lunch 4 tip_pct smoker day No Fri 94 0.142857 Sat 212 0.186220 Sun 156 0.103799 Thur 142 0.121389 Yes Fri 95 0.117750 Sat 170 0.196812 Sun 182 0.077178 Thur 197 0.115982 获取分组后数据某一列的统计数据12result = tips.groupby('smoker')['tip_pct'].describe()resultsmoker No count 151.000000 mean 0.159328 std 0.039910 min 0.056797 25% 0.136906 50% 0.155625 75% 0.185014 max 0.291990 Yes count 93.000000 mean 0.163196 std 0.085119 min 0.035638 25% 0.106771 50% 0.153846 75% 0.195059 max 0.710345 Name: tip_pct, dtype: float64 1result.unstack('smoker')smoker No Yes count 151.000000 93.000000 mean 0.159328 0.163196 std 0.039910 0.085119 min 0.056797 0.035638 25% 0.136906 0.106771 50% 0.155625 0.153846 75% 0.185014 0.195059 max 0.291990 0.710345 grouped 根据性别以及是否吸烟进行分组12f = lambda x: x.describe()grouped.apply(f) total_bill tip size_ tip_pct sex smoker Female No count 54.000000 54.000000 54.000000 54.000000 mean 18.105185 2.773519 2.592593 0.156921 std 7.286455 1.128425 1.073146 0.036421 min 7.250000 1.000000 1.000000 0.056797 25% 12.650000 2.000000 2.000000 0.139708 50% 16.690000 2.680000 2.000000 0.149691 75% 20.862500 3.437500 3.000000 0.181630 max 35.830000 5.200000 6.000000 0.252672 Yes count 33.000000 33.000000 33.000000 33.000000 mean 17.977879 2.931515 2.242424 0.182150 std 9.189751 1.219916 0.613917 0.071595 min 3.070000 1.000000 1.000000 0.056433 25% 12.760000 2.000000 2.000000 0.152439 50% 16.270000 2.880000 2.000000 0.173913 75% 22.120000 3.500000 2.000000 0.198216 max 44.300000 6.500000 4.000000 0.416667 Male No count 97.000000 97.000000 97.000000 97.000000 mean 19.791237 3.113402 2.711340 0.160669 std 8.726566 1.489559 0.989094 0.041849 min 7.510000 1.250000 2.000000 0.071804 25% 13.810000 2.000000 2.000000 0.131810 50% 18.240000 2.740000 2.000000 0.157604 75% 22.820000 3.710000 3.000000 0.186220 max 48.330000 9.000000 6.000000 0.291990 Yes count 60.000000 60.000000 60.000000 60.000000 mean 22.284500 3.051167 2.500000 0.152771 std 9.911845 1.500120 0.892530 0.090588 min 7.250000 1.000000 1.000000 0.035638 25% 15.272500 2.000000 2.000000 0.101845 50% 20.390000 3.000000 2.000000 0.141015 75% 28.572500 3.820000 3.000000 0.191697 max 50.810000 10.000000 5.000000 0.710345 禁止分组键1tips.groupby('smoker', group_keys=True).apply(top) total_bill tip sex smoker day time size_ tip_pct smoker No 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 1tips.groupby('smoker', group_keys=False).apply(top) total_bill tip sex smoker day time size_ tip_pct 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 分位数与桶分析1234frame = DataFrame(&#123;'data1': np.random.randn(1000), 'data2': np.random.randn(1000)&#125;)factor = pd.cut(frame.data1, 4)factor[:10]0 (-1.23, 0.489] 1 (-2.956, -1.23] 2 (-1.23, 0.489] 3 (0.489, 2.208] 4 (-1.23, 0.489] 5 (0.489, 2.208] 6 (-1.23, 0.489] 7 (-1.23, 0.489] 8 (0.489, 2.208] 9 (0.489, 2.208] Name: data1, dtype: category Categories (4, object): [(-2.956, -1.23] &lt; (-1.23, 0.489] &lt; (0.489, 2.208] &lt; (2.208, 3.928]] 12345678def get_stats(group): return &#123;'min': group.min(), 'max': group.max(), 'count': group.count(), 'mean': group.mean()&#125;grouped = frame.data2.groupby(factor)grouped.apply(get_stats).unstack()#ADAPT the output is not sorted in the book while this is the case now (swap first two lines) count max mean min data1 (-2.956, -1.23] 95.0 1.670835 -0.039521 -3.399312 (-1.23, 0.489] 598.0 3.260383 -0.002051 -2.989741 (0.489, 2.208] 297.0 2.954439 0.081822 -3.745356 (2.208, 3.928] 10.0 1.765640 0.024750 -1.929776 12345# Return quantile numbersgrouping = pd.qcut(frame.data1, 10, labels=False)grouped = frame.data2.groupby(grouping)grouped.apply(get_stats).unstack() count max mean min data1 0 100.0 1.670835 -0.049902 -3.399312 1 100.0 2.628441 0.030989 -1.950098 2 100.0 2.527939 -0.067179 -2.925113 3 100.0 3.260383 0.065713 -2.315555 4 100.0 2.074345 -0.111653 -2.047939 5 100.0 2.184810 0.052130 -2.989741 6 100.0 2.458842 -0.021489 -2.223506 7 100.0 2.954439 -0.026459 -3.056990 8 100.0 2.735527 0.103406 -3.745356 9 100.0 2.377020 0.220122 -2.064111 Example: 用特定分组的值填充缺失值填一些缺失值进去123s = Series(np.random.randn(6))s[::2] = np.nans0 NaN 1 -0.125921 2 NaN 3 -0.884475 4 NaN 5 0.227290 dtype: float64 用均值填充缺失值1s.fillna(s.mean())0 -0.261035 1 -0.125921 2 -0.261035 3 -0.884475 4 -0.261035 5 0.227290 dtype: float64 同样，填一些缺失值123456states = ['Ohio', 'New York', 'Vermont', 'Florida', 'Oregon', 'Nevada', 'California', 'Idaho']group_key = ['East'] * 4 + ['West'] * 4data = Series(np.random.randn(8), index=states)data[['Vermont', 'Nevada', 'Idaho']] = np.nandataOhio 0.922264 New York -2.153545 Vermont NaN Florida -0.375842 Oregon 0.329939 Nevada NaN California 1.105913 Idaho NaN dtype: float64 计算分组均值1data.groupby(group_key).mean()East -0.535707 West 0.717926 dtype: float64 这里的g指代调用apply的主体，也就是data.groupby(group_key)分组后的结果12fill_mean = lambda g: g.fillna(g.mean())data.groupby(group_key).apply(fill_mean)Ohio 0.922264 New York -2.153545 Vermont -0.535707 Florida -0.375842 Oregon 0.329939 Nevada 0.717926 California 1.105913 Idaho 0.717926 dtype: float64 由于groupby操作后得到的结果类似于一个字典，字典key是组名，value是一个DataFrame Object1234fill_values = &#123;'East': 0.5, 'West': -1&#125;fill_func = lambda g: g.fillna(fill_values[g.name])data.groupby(group_key).apply(fill_func)Ohio 0.922264 New York -2.153545 Vermont 0.500000 Florida -0.375842 Oregon 0.329939 Nevada -1.000000 California 1.105913 Idaho -1.000000 dtype: float64 Example: 随机采样和排列构造扑克牌红桃Hearts, 黑桃Spades, 梅花Clubs, 方片Diamonds123456789# Hearts, Spades, Clubs, Diamondssuits = ['H', 'S', 'C', 'D']card_val = (list(range(1, 11)) + [10] * 3) * 4base_names = ['A'] + range(2, 11) + ['J', 'K', 'Q']cards = []for suit in ['H', 'S', 'C', 'D']: cards.extend(str(num) + suit for num in base_names)deck = Series(card_val, index=cards)1deck[:13]AH 1 2H 2 3H 3 4H 4 5H 5 6H 6 7H 7 8H 8 9H 9 10H 10 JH 10 KH 10 QH 10 dtype: int64 随机抽牌123def draw(deck, n=5): return deck.take(np.random.permutation(len(deck))[:n])draw(deck)AD 1 8C 8 5H 5 KC 10 2C 2 dtype: int64 分类抽牌12get_suit = lambda card: card[-1] # last letter is suitdeck.groupby(get_suit).apply(draw, n=2)C 2C 2 3C 3 D KD 10 8D 8 H KH 10 3H 3 S 2S 2 4S 4 dtype: int64 去掉分组键12# alternativelydeck.groupby(get_suit, group_keys=False).apply(draw, n=2)KC 10 JC 10 AD 1 5D 5 5H 5 6H 6 7S 7 KS 10 dtype: int64 Example: 分组加权平均数和相关系数1234df = DataFrame(&#123;'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], 'data': np.random.randn(8), 'weights': np.random.rand(8)&#125;)df category data weights 0 a 1.561587 0.957515 1 a 1.219984 0.347267 2 a -0.482239 0.581362 3 a 0.315667 0.217091 4 b -0.047852 0.894406 5 b -0.454145 0.918564 6 b -0.556774 0.277825 7 b 0.253321 0.955905 123grouped = df.groupby('category')get_wavg = lambda g: np.average(g['data'], weights=g['weights'])grouped.apply(get_wavg)category a 0.811643 b -0.122262 dtype: float64 stock数据集12close_px = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)close_px.info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; DatetimeIndex: 2214 entries, 2003-01-02 to 2011-10-14 Data columns (total 4 columns): AAPL 2214 non-null float64 MSFT 2214 non-null float64 XOM 2214 non-null float64 SPX 2214 non-null float64 dtypes: float64(4) memory usage: 86.5 KB 1close_px[-4:] AAPL MSFT XOM SPX 2011-10-11 400.29 27.00 76.27 1195.54 2011-10-12 402.19 26.96 77.16 1207.25 2011-10-13 408.43 27.18 76.37 1203.66 2011-10-14 422.00 27.27 78.11 1224.58 计算相关系数1234rets = close_px.pct_change().dropna()spx_corr = lambda x: x.corrwith(x['SPX'])by_year = rets.groupby(lambda x: x.year)by_year.apply(spx_corr) AAPL MSFT XOM SPX 2003 0.541124 0.745174 0.661265 1.0 2004 0.374283 0.588531 0.557742 1.0 2005 0.467540 0.562374 0.631010 1.0 2006 0.428267 0.406126 0.518514 1.0 2007 0.508118 0.658770 0.786264 1.0 2008 0.681434 0.804626 0.828303 1.0 2009 0.707103 0.654902 0.797921 1.0 2010 0.710105 0.730118 0.839057 1.0 2011 0.691931 0.800996 0.859975 1.0 lambda看来有很大用处12# Annual correlation of Apple with Microsoftby_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))2003 0.480868 2004 0.259024 2005 0.300093 2006 0.161735 2007 0.417738 2008 0.611901 2009 0.432738 2010 0.571946 2011 0.581987 dtype: float64 Example: 分组级线型回归1234567import statsmodels.api as smdef regress(data, yvar, xvars): Y = data[yvar] X = data[xvars] X['intercept'] = 1. result = sm.OLS(Y, X).fit() return result.params这样传参1by_year.apply(regress, 'AAPL', ['SPX']) SPX intercept 2003 1.195406 0.000710 2004 1.363463 0.004201 2005 1.766415 0.003246 2006 1.645496 0.000080 2007 1.198761 0.003438 2008 0.968016 -0.001110 2009 0.879103 0.002954 2010 1.052608 0.001261 2011 0.806605 0.001514 透视表和交叉表1tips[:10] total_bill tip sex smoker day time size_ tip_pct 0 16.99 1.01 Female No Sun Dinner 2 0.059447 1 10.34 1.66 Male No Sun Dinner 3 0.160542 2 21.01 3.50 Male No Sun Dinner 3 0.166587 3 23.68 3.31 Male No Sun Dinner 2 0.139780 4 24.59 3.61 Female No Sun Dinner 4 0.146808 5 25.29 4.71 Male No Sun Dinner 4 0.186240 6 8.77 2.00 Male No Sun Dinner 2 0.228050 7 26.88 3.12 Male No Sun Dinner 4 0.116071 8 15.04 1.96 Male No Sun Dinner 2 0.130319 9 14.78 3.23 Male No Sun Dinner 2 0.218539 pivot_table默认情况相当于分组后进行mean()操作1tips.pivot_table(index=['sex', 'smoker']) size_ tip tip_pct total_bill sex smoker Female No 2.592593 2.773519 0.156921 18.105185 Yes 2.242424 2.931515 0.182150 17.977879 Male No 2.711340 3.113402 0.160669 19.791237 Yes 2.500000 3.051167 0.152771 22.284500 指定分组度量12tips.pivot_table(['tip_pct', 'size_'], index=['sex', 'day'], columns='smoker') tip_pct size_ smoker No Yes No Yes sex day Female Fri 0.165296 0.209129 2.500000 2.000000 Sat 0.147993 0.163817 2.307692 2.200000 Sun 0.165710 0.237075 3.071429 2.500000 Thur 0.155971 0.163073 2.480000 2.428571 Male Fri 0.138005 0.144730 2.000000 2.125000 Sat 0.162132 0.139067 2.656250 2.629630 Sun 0.158291 0.173964 2.883721 2.600000 Thur 0.165706 0.164417 2.500000 2.300000 增加ALL列12tips.pivot_table(['tip_pct', 'size_'], index=['sex', 'day'], columns='smoker', margins=True) tip_pct size_ smoker No Yes All No Yes All sex day Female Fri 0.165296 0.209129 0.199388 2.500000 2.000000 2.111111 Sat 0.147993 0.163817 0.156470 2.307692 2.200000 2.250000 Sun 0.165710 0.237075 0.181569 3.071429 2.500000 2.944444 Thur 0.155971 0.163073 0.157525 2.480000 2.428571 2.468750 Male Fri 0.138005 0.144730 0.143385 2.000000 2.125000 2.100000 Sat 0.162132 0.139067 0.151577 2.656250 2.629630 2.644068 Sun 0.158291 0.173964 0.162344 2.883721 2.600000 2.810345 Thur 0.165706 0.164417 0.165276 2.500000 2.300000 2.433333 All 0.159328 0.163196 0.160803 2.668874 2.408602 2.569672 更换一个分组度量12tips.pivot_table('tip_pct', index=['sex', 'smoker'], columns='day', aggfunc=len, margins=True)day Fri Sat Sun Thur All sex smoker Female No 2.0 13.0 14.0 25.0 54.0 Yes 7.0 15.0 4.0 7.0 33.0 Male No 2.0 32.0 43.0 20.0 97.0 Yes 8.0 27.0 15.0 10.0 60.0 All 19.0 87.0 76.0 62.0 244.0 分组计数并填充（可能存在空组合）12tips.pivot_table('size_', index=['time', 'sex', 'smoker'], columns='day', aggfunc='sum', fill_value=0)day Fri Sat Sun Thur time sex smoker Dinner Female No 2 30 43 2 Yes 8 33 10 0 Male No 4 85 124 0 Yes 12 71 39 0 Lunch Female No 3 0 0 60 Yes 6 0 0 17 Male No 0 0 0 50 Yes 5 0 0 23 交叉表: crosstab1234567891011121314from StringIO import StringIOdata = """\Sample Gender Handedness1 Female Right-handed2 Male Left-handed3 Female Right-handed4 Male Right-handed5 Male Left-handed6 Male Right-handed7 Female Right-handed8 Female Left-handed9 Male Right-handed10 Female Right-handed"""data = pd.read_table(StringIO(data), sep='\s+')1data Sample Gender Handedness 0 1 Female Right-handed 1 2 Male Left-handed 2 3 Female Right-handed 3 4 Male Right-handed 4 5 Male Left-handed 5 6 Male Right-handed 6 7 Female Right-handed 7 8 Female Left-handed 8 9 Male Right-handed 9 10 Female Right-handed 交叉表就是在…计数…1pd.crosstab(data.Gender, data.Handedness, margins=True)Handedness Left-handed Right-handed All Gender Female 1 4 5 Male 2 3 5 All 3 7 10 1pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)smoker No Yes All time day Dinner Fri 3 9 12 Sat 45 42 87 Sun 57 19 76 Thur 1 0 1 Lunch Fri 1 6 7 Thur 44 17 61 All 151 93 244 Example: 2012 联邦选举委员会数据库这个数据库包括赞助人的姓名，职业、雇主、地址以及出资额等信息1fec = pd.read_csv('ch09/P00000001-ALL.csv')1fec.info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1001731 entries, 0 to 1001730 Data columns (total 16 columns): cmte_id 1001731 non-null object cand_id 1001731 non-null object cand_nm 1001731 non-null object contbr_nm 1001731 non-null object contbr_city 1001712 non-null object contbr_st 1001727 non-null object contbr_zip 1001620 non-null object contbr_employer 988002 non-null object contbr_occupation 993301 non-null object contb_receipt_amt 1001731 non-null float64 contb_receipt_dt 1001731 non-null object receipt_desc 14166 non-null object memo_cd 92482 non-null object memo_text 97770 non-null object form_tp 1001731 non-null object file_num 1001731 non-null int64 dtypes: float64(1), int64(1), object(14) memory usage: 122.3+ MB 1fec.ix[123456]cmte_id C00431445 cand_id P80003338 cand_nm Obama, Barack contbr_nm ELLMAN, IRA contbr_city TEMPE contbr_st AZ contbr_zip 852816719 contbr_employer ARIZONA STATE UNIVERSITY contbr_occupation PROFESSOR contb_receipt_amt 50 contb_receipt_dt 01-DEC-11 receipt_desc NaN memo_cd NaN memo_text NaN form_tp SA17A file_num 772372 Name: 123456, dtype: object 输出全部候选人名单12unique_cands = fec.cand_nm.unique()unique_candsarray([&#39;Bachmann, Michelle&#39;, &#39;Romney, Mitt&#39;, &#39;Obama, Barack&#39;, &quot;Roemer, Charles E. &#39;Buddy&#39; III&quot;, &#39;Pawlenty, Timothy&#39;, &#39;Johnson, Gary Earl&#39;, &#39;Paul, Ron&#39;, &#39;Santorum, Rick&#39;, &#39;Cain, Herman&#39;, &#39;Gingrich, Newt&#39;, &#39;McCotter, Thaddeus G&#39;, &#39;Huntsman, Jon&#39;, &#39;Perry, Rick&#39;], dtype=object) 1unique_cands[2]&#39;Obama, Barack&#39; 政党映射12345678910111213parties = &#123;'Bachmann, Michelle': 'Republican', 'Cain, Herman': 'Republican', 'Gingrich, Newt': 'Republican', 'Huntsman, Jon': 'Republican', 'Johnson, Gary Earl': 'Republican', 'McCotter, Thaddeus G': 'Republican', 'Obama, Barack': 'Democrat', 'Paul, Ron': 'Republican', 'Pawlenty, Timothy': 'Republican', 'Perry, Rick': 'Republican', "Roemer, Charles E. 'Buddy' III": 'Republican', 'Romney, Mitt': 'Republican', 'Santorum, Rick': 'Republican'&#125;1fec.cand_nm[123456:123461]123456 Obama, Barack 123457 Obama, Barack 123458 Obama, Barack 123459 Obama, Barack 123460 Obama, Barack Name: cand_nm, dtype: object 1fec.cand_nm[123456:123461].map(parties)123456 Democrat 123457 Democrat 123458 Democrat 123459 Democrat 123460 Democrat Name: cand_nm, dtype: object 根据以上创建的映射，在原数据集中添加一列party12# Add it as a columnfec['party'] = fec.cand_nm.map(parties)1fec['party'].value_counts()Democrat 593746 Republican 407985 Name: party, dtype: int64 看看出资额是正是负1(fec.contb_receipt_amt &gt; 0).value_counts()True 991475 False 10256 Name: contb_receipt_amt, dtype: int64 调整出资额为正1fec = fec[fec.contb_receipt_amt &gt; 0]筛选候选人1fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack', 'Romney, Mitt'])]根据职业和雇主统计赞助信息统计职业信息1fec.contbr_occupation.value_counts()[:10]RETIRED 233990 INFORMATION REQUESTED 35107 ATTORNEY 34286 HOMEMAKER 29931 PHYSICIAN 23432 INFORMATION REQUESTED PER BEST EFFORTS 21138 ENGINEER 14334 TEACHER 13990 CONSULTANT 13273 PROFESSOR 12555 Name: contbr_occupation, dtype: int64 筛选出一些不符合规格的信息映射到正常信息12345678910occ_mapping = &#123; 'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED', 'INFORMATION REQUESTED' : 'NOT PROVIDED', 'INFORMATION REQUESTED (BEST EFFORTS)' : 'NOT PROVIDED', 'C.E.O.': 'CEO'&#125;# If no mapping provided, return xf = lambda x: occ_mapping.get(x, x)fec.contbr_occupation = fec.contbr_occupation.map(f)以上巧妙运用了get方法12345678910emp_mapping = &#123; 'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED', 'INFORMATION REQUESTED' : 'NOT PROVIDED', 'SELF' : 'SELF-EMPLOYED', 'SELF EMPLOYED' : 'SELF-EMPLOYED',&#125;# If no mapping provided, return xf = lambda x: emp_mapping.get(x, x)fec.contbr_employer = fec.contbr_employer.map(f)根据职业以及候选人政党分组，统计出资额总和123by_occupation = fec.pivot_table('contb_receipt_amt', index='contbr_occupation', columns='party', aggfunc='sum')12over_2mm = by_occupation[by_occupation.sum(1) &gt; 2000000]over_2mmparty Democrat Republican contbr_occupation ATTORNEY 11141982.97 7.477194e+06 CEO 2074974.79 4.211041e+06 CONSULTANT 2459912.71 2.544725e+06 ENGINEER 951525.55 1.818374e+06 EXECUTIVE 1355161.05 4.138850e+06 HOMEMAKER 4248875.80 1.363428e+07 INVESTOR 884133.00 2.431769e+06 LAWYER 3160478.87 3.912243e+05 MANAGER 762883.22 1.444532e+06 NOT PROVIDED 4866973.96 2.056547e+07 OWNER 1001567.36 2.408287e+06 PHYSICIAN 3735124.94 3.594320e+06 PRESIDENT 1878509.95 4.720924e+06 PROFESSOR 2165071.08 2.967027e+05 REAL ESTATE 528902.09 1.625902e+06 RETIRED 25305116.38 2.356124e+07 SELF-EMPLOYED 672393.40 1.640253e+06 1over_2mm.plot(kind='barh')&lt;matplotlib.axes._subplots.AxesSubplot at 0x340fb4e0&gt; 12345def get_top_amounts(group, key, n=5): totals = group.groupby(key)['contb_receipt_amt'].sum() # Order totals by key in descending order return totals.sort_values()[-n:]12grouped = fec_mrbo.groupby('cand_nm')grouped.apply(get_top_amounts, 'contbr_occupation', n=7)cand_nm contbr_occupation Obama, Barack CONSULTANT 2459912.71 LAWYER 3160478.87 PHYSICIAN 3735124.94 HOMEMAKER 4248875.80 INFORMATION REQUESTED 4866973.96 ATTORNEY 11141982.97 RETIRED 25305116.38 Romney, Mitt C.E.O. 1968386.11 EXECUTIVE 2300947.03 PRESIDENT 2491244.89 ATTORNEY 5364718.82 HOMEMAKER 8147446.22 INFORMATION REQUESTED PER BEST EFFORTS 11396894.84 RETIRED 11508473.59 Name: contb_receipt_amt, dtype: float64 1grouped.apply(get_top_amounts, 'contbr_employer', n=10)cand_nm contbr_employer Obama, Barack MICROSOFT 215585.36 VOLUNTEER 257104.00 STUDENT 318831.45 SELF EMPLOYED 469290.00 SELF 1076531.20 HOMEMAKER 2605408.54 INFORMATION REQUESTED 5053480.37 NOT EMPLOYED 8586308.70 SELF-EMPLOYED 17080985.96 RETIRED 22694358.85 Romney, Mitt H.I.G. CAPITAL 139500.00 BARCLAYS CAPITAL 162750.00 GOLDMAN SACH &amp; CO. 238250.00 MORGAN STANLEY 267266.00 CREDIT SUISSE 281150.00 STUDENT 496490.94 SELF-EMPLOYED 7409860.98 HOMEMAKER 8147196.22 RETIRED 11506225.71 INFORMATION REQUESTED PER BEST EFFORTS 12059527.24 Name: contb_receipt_amt, dtype: float64 根据出资额分组不出意外果然要用到桶123bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000])labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)labels[:10]411 (10, 100] 412 (100, 1000] 413 (100, 1000] 414 (10, 100] 415 (10, 100] 416 (10, 100] 417 (100, 1000] 418 (10, 100] 419 (100, 1000] 420 (10, 100] Name: contb_receipt_amt, dtype: category Categories (8, object): [(0, 1] &lt; (1, 10] &lt; (10, 100] &lt; (100, 1000] &lt; (1000, 10000] &lt; (10000, 100000] &lt; (100000, 1000000] &lt; (1000000, 10000000]] 12grouped = fec_mrbo.groupby(['cand_nm', labels])grouped.size().unstack(0)cand_nm Obama, Barack Romney, Mitt contb_receipt_amt (0, 1] 493.0 77.0 (1, 10] 40070.0 3681.0 (10, 100] 372280.0 31853.0 (100, 1000] 153991.0 43357.0 (1000, 10000] 22284.0 26186.0 (10000, 100000] 2.0 1.0 (100000, 1000000] 3.0 NaN (1000000, 10000000] 4.0 NaN 12bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)bucket_sumscand_nm Obama, Barack Romney, Mitt contb_receipt_amt (0, 1] 318.24 77.00 (1, 10] 337267.62 29819.66 (10, 100] 20288981.41 1987783.76 (100, 1000] 54798531.46 22363381.69 (1000, 10000] 51753705.67 63942145.42 (10000, 100000] 59100.00 12700.00 (100000, 1000000] 1490683.08 NaN (1000000, 10000000] 7148839.76 NaN 计算比例12normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)normed_sumscand_nm Obama, Barack Romney, Mitt contb_receipt_amt (0, 1] 0.805182 0.194818 (1, 10] 0.918767 0.081233 (10, 100] 0.910769 0.089231 (100, 1000] 0.710176 0.289824 (1000, 10000] 0.447326 0.552674 (10000, 100000] 0.823120 0.176880 (100000, 1000000] 1.000000 NaN (1000000, 10000000] 1.000000 NaN 画个图看看1normed_sums[:-2].plot(kind='barh', stacked=True)&lt;matplotlib.axes._subplots.AxesSubplot at 0x14c4db00&gt; 根据州统计赞助信息1234grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)totals = totals[totals.sum(1) &gt; 100000]totals[:10]cand_nm Obama, Barack Romney, Mitt contbr_st AK 281840.15 86204.24 AL 543123.48 527303.51 AR 359247.28 105556.00 AZ 1506476.98 1888436.23 CA 23824984.24 11237636.60 CO 2132429.49 1506714.12 CT 2068291.26 3499475.45 DC 4373538.80 1025137.50 DE 336669.14 82712.00 FL 7318178.58 8338458.81 Mitt is so...poorly...12percent = totals.div(totals.sum(1), axis=0)percent[:10]cand_nm Obama, Barack Romney, Mitt contbr_st AK 0.765778 0.234222 AL 0.507390 0.492610 AR 0.772902 0.227098 AZ 0.443745 0.556255 CA 0.679498 0.320502 CO 0.585970 0.414030 CT 0.371476 0.628524 DC 0.810113 0.189887 DE 0.802776 0.197224 FL 0.467417 0.532583]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Lecture4 note]]></title>
      <url>%2F2017%2F03%2F06%2Fcs231n-Lecture4-note%2F</url>
      <content type="text"><![CDATA[BackPropIntroductionMotivation. In this section we will develop expertise with an intuitive understanding of backpropagation, which is a way of computing gradients of expressions through recursive application of chain rule. Understanding of this process and its subtleties is critical for you to understand, and effectively develop, design and debug Neural Networks.Problem statement. The core problem studied in this section is as follows: We are given some function $f(x)$ where $x$ is a vector of inputs and we are interested in computing the gradient of $f$ at $x$ (i.e. $\Delta f(x)$ ).Modularity: Sigmoid examplef(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}The function is made up of multiple gates. In addition to the ones described already above (add, mul, max), there are four more:f(x) = \frac{1}{x} \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = -1/x^2 \\\\ f_c(x) = c + x \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = 1 \\\\ f(x) = e^x \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = e^x \\\\ f_a(x) = ax \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = aThe full circuit then looks as follows:In the example above, we see a long chain of function applications that operates on the result of the dot product between w,x. The function that these operations implement is called the sigmoid function $\sigma (x)$. It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator):\sigma(x) = \frac{1}{1+e^{-x}} \\\\ \rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be (1 - 0.73) * 0.73 ~= 0.2, as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Lets see the backprop for this neuron in code:123456789101112w = [2,-3,-3] # assume some random weights and datax = [-1, -2]# forward passdot = w[0]*x[0] + w[1]*x[1] + w[2]f = 1.0 / (1 + math.exp(-dot)) # sigmoid function# backward pass through the neuron (backpropagation)ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivationdx = [w[0] * ddot, w[1] * ddot] # backprop into xdw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w# we're done! we have the gradients on the inputs to the circuitImplementation protip: staged backpropagation. As shown in the code above, in practice it is always helpful to break down the forward pass into stages that are easily backpropped through. For example here we created an intermediate variable dot which holds the output of the dot product between w and x. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. ddot, and ultimately dw, dx) that hold the gradients of those variables.Backprop in practice: Staged computationSuppose that we have a function of the form:f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}Here is how we would structure the forward pass of such expression:123456789101112x = 3 # example valuesy = -4# forward passsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator #(1)num = x + sigy # numerator #(2)sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # denominator #(6)invden = 1.0 / den #(7)f = num * invden # done! #(8)For each row, we also highlight which part of the forward pass it refers to:123456789101112131415161718192021# backprop f = num * invdendnum = invden # gradient on numerator #(8)dinvden = num #(8)# backprop invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# backprop den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# backprop xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# backprop xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# backprop sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# backprop num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# backprop sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# done! phewNotice a few things:Cache forward pass variables. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them.Gradients add up at forks. The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add.Patterns in backward flowIt is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (add,mul,max), all have very simple interpretations in terms of how they act during backpropagation. Consider this example circuit:Looking at the diagram above as an example, we can see that:The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged). In the example circuit above, note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged.The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values. In the example circuit above, the max operation routed the gradient of 2.00 to the z variable, which had a higher value than w, and the gradient on w remains zero.The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. In the example above, the gradient on x is -8.00, which is -4.00 x 2.00.Unintuitive effects and their consequences. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted $w^T x_i$ (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples xixi by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and you’d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases.ExtrasSome extra materials provided in here and here.Optional: [1, 2, 3]Neural Network#1Quick introIt is possible to introduce neural networks without appealing to brain analogies. In the section on linear classification we computed scores for different visual categories given the image using the formula $s=Wx$, where $W$ was a matrix and $x$ was an input column vector containing all pixel data of the image. In the case of CIFAR-10, $x$ is a [3072x1] column vector, and $W$ is a [10x3072] matrix, so that the output scores is a vector of 10 class scores.An example neural network would instead compute $s=W_2 \max(0, W_1x)$. Here, $W_1$ could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. The function $\max(0,−)$is a non-linearity that is applied elementwise. There are several choices we could make for the non-linearity (which we’ll study below), but this one is a common choice and simply thresholds all activations that are below zero to zero. Finally, the matrix $W_2$ would then be of size [10x100], so that we again get 10 numbers out that we interpret as the class scores. Notice that the non-linearity is critical computationally - if we left it out, the two matrices could be collapsed to a single matrix, and therefore the predicted class scores would again be a linear function of the input. The non-linearity is where we get the wiggle. The parameters $W_2, W_1$ are learned with stochastic gradient descent, and their gradients are derived with chain rule (and computed with backpropagation).A three-layer neural network could analogously look like $s = W_3 \max(0, W_2 \max(0, W_1 x))$, where all of $W_3, W_2, W_1$ are parameters to be learned. The sizes of the intermediate hidden vectors are hyperparameters of the network and we’ll see how we can set them later. Lets now look into how we can interpret these computations from the neuron/network perspective.Modeling one neuronAn example code for forward-propagating a single neuron might look as follows:1234567class Neuron(object): # ... def forward(inputs): """ assume inputs and weights are 1-D numpy arrays and bias is a number """ cell_body_sum = np.sum(inputs * self.weights) + self.bias firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function return firing_rateSingle neuron as a linear classifierThe mathematical form of the model Neuron’s forward computation might look familiar to you. As we saw with linear classifiers, a neuron has the capacity to “like” (activation near one) or “dislike” (activation near zero) certain linear regions of its input space. Hence, with an appropriate loss function on the neuron’s output, we can turn a single neuron into a linear classifier:Binary Softmax classifier. For example, we can interpret $\sigma(\sum_iw_ix_i + b)$ to be the probability of one of the classes $P(y_i = 1 \mid x_i; w)$. The probability of the other class would be $P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w)$, since they must sum to one. With this interpretation, we can formulate the cross-entropy loss as we have seen in the Linear Classification section, and optimizing it would lead to a binary Softmax classifier (also known as logistic regression). Since the sigmoid function is restricted to be between 0-1, the predictions of this classifier are based on whether the output of the neuron is greater than 0.5.Binary SVM classifier. Alternatively, we could attach a max-margin hinge loss to the output of the neuron and train it to become a binary Support Vector Machine.Regularization interpretation. The regularization loss in both SVM/Softmax cases could in this biological view be interpreted as gradual forgetting, since it would have the effect of driving all synaptic weights ww towards zero after every parameter update.A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers)Commonly used activation functionsEvery activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice:Sigmoid. The sigmoid non-linearity has the mathematical form $\sigma(x) = 1 / (1 + e^{-x})$ and is shown in the image. As alluded to in the previous section, it takes a real-valued number and “squashes” it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1. The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks:Sigmoids saturate and kill gradients. A very undesirable property of the sigmoid neuron is that when the neuron’s activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $x &gt; 0$ elementwise in $f = w^Tx + b$), then the gradient on the weights ww will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $f$). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.​Tanh. The tanh non-linearity is shown on the image. It squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: $\tanh(x) = 2 \sigma(2x) -1$.ReLU. The Rectified Linear Unit has become very popular in the last few years. It computes the function $f(x) = \max(0, x)$. In other words, the activation is simply thresholded at zero (see image). There are several pros and cons to using the ReLUs:(+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.(+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.(-) Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.Leaky ReLU. Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when x &lt; 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes $f(x) = \mathbb{1}(x &lt; 0) (\alpha x) + \mathbb{1}(x&gt;=0) (x)$ where $\alpha$ is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear.Maxout. Other types of units have been proposed that do not have the functional form $f(w^Tx + b)$ where a non-linearity is applied on the dot product between the weights and the data. One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. The Maxout neuron computes the function $\max(w_1^Tx+b_1, w_2^Tx + b_2)$. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have $w_1, b_1 = 0$). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters.This concludes our discussion of the most common types of neurons and their activation functions. As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.TLDR: “What neuron type should I use?” Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.Neural Network architecturesLayer-wise organizationSizing neural networks. The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with the two example networks in the above picture:The first network (left) has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters.The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters.To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing. More on this in the Convolutional Neural Networks module.Example feed-forward computation123456# forward-pass of a 3-layer neural network:f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)x = np.random.randn(3, 1) # random input vector of three numbers (3x1)h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)out = np.dot(W3, h2) + b3 # output neuron (1x1)The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function.Representational powerOne way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. A natural question that arises is: What is the representational power of this family of functions? In particular, are there functions that cannot be modeled with a Neural Network?It turns out that Neural Networks with at least one hidden layer are universal approximators. That is, it can be shown (e.g. see Approximation by Superpositions of Sigmoidal Function from 1989 (pdf), or this intuitive explanation from Michael Nielsen) that given any continuous function $f(x)$ and some $\epsilon &gt; 0$, there exists a Neural Network $g(x)$ with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that $\forall x, \mid f(x) - g(x) \mid &lt; \epsilon$. In other words, the neural network can approximate any continuous function.If one hidden layer suffices to approximate any function, why use more layers and go deeper? The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice. In one dimension, the “sum of indicator bumps” function $g(x) = \sum_i c_i \mathbb{1}(a_i &lt; x &lt; b_i)$ where $a, b, c$ are parameter vectors is also a universal approximator, but noone would suggest that we use this functional form in Machine Learning. Neural Networks work well in practice because they compactly express nice, smooth functions that fit well with the statistical properties of data we encounter in practice, and are also easy to learn using our optimization algorithms (e.g. gradient descent). Similarly, the fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal.As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.The full story is, of course, much more involved and a topic of much recent research. If you are interested in these topics we recommend for further reading:Deep Learning book in press by Bengio, Goodfellow, Courville, in practicular Chapter 6.4.Do Deep Nets Really Need to be Deep?FitNets: Hints for Thin Deep NetsSetting number of layers and their sizesWe increase the size and number of layers in a Neural Network, the capacity of the network increases:In practice, it is always better to use these methods to control overfitting instead of the number of neurons.The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: It’s clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. Since Neural Networks are non-convex, it is hard to study these properties mathematically, but some attempts to understand these objective functions have been made, e.g. in a recent paper The Loss Surfaces of Multilayer Networks. In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. On the other hand, if you train a large network you’ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network. We can look at the results achieved by three different settings:You can play with these examples in this ConvNetsJS demo.The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.Additional Referencesdeeplearning.net tutorial with TheanoConvNetJS demos for intuitionsMichael Nielsen’s tutorials]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note ch08]]></title>
      <url>%2F2017%2F03%2F05%2Fpython-data-analysis-learning-note-ch08%2F</url>
      <content type="text"><![CDATA[绘图和可视化12345678910from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inlinematplotlib API 入门1import matplotlib.pyplot as pltFigure 和 Subplot1fig = plt.figure()1ax1 = fig.add_subplot(2, 2, 1)12ax2 = fig.add_subplot(2, 2, 2)ax3 = fig.add_subplot(2, 2, 3)12from numpy.random import randnplt.plot(randn(50).cumsum(), 'k--')12_ = ax1.hist(randn(100), bins=20, color='k', alpha=0.3)ax2.scatter(np.arange(30), np.arange(30) + 3 * randn(30))1plt.close('all')12fig, axes = plt.subplots(2, 3)axes调整subplot周围的间距12plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)12345fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)for i in range(2): for j in range(2): axes[i, j].hist(randn(500), bins=50, color='k', alpha=0.5)plt.subplots_adjust(wspace=0, hspace=0)(array([ 2., 0., 3., 2., 1., 1., 0., 3., 5., 8., 9., 9., 10., 18., 34., 13., 24., 30., 24., 24., 25., 20., 34., 20., 30., 30., 19., 14., 14., 8., 19., 14., 7., 3., 7., 2., 7., 2., 2., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.]), array([-2.9493, -2.8118, -2.6743, -2.5367, -2.3992, -2.2617, -2.1241, -1.9866, -1.849 , -1.7115, -1.574 , -1.4364, -1.2989, -1.1614, -1.0238, -0.8863, -0.7487, -0.6112, -0.4737, -0.3361, -0.1986, -0.0611, 0.0765, 0.214 , 0.3516, 0.4891, 0.6266, 0.7642, 0.9017, 1.0392, 1.1768, 1.3143, 1.4519, 1.5894, 1.7269, 1.8645, 2.002 , 2.1395, 2.2771, 2.4146, 2.5522, 2.6897, 2.8272, 2.9648, 3.1023, 3.2398, 3.3774, 3.5149, 3.6525, 3.79 , 3.9275]), &lt;a list of 50 Patch objects&gt;) (array([ 1., 1., 0., 2., 0., 1., 1., 5., 7., 4., 5., 8., 12., 12., 13., 15., 17., 13., 22., 30., 21., 24., 17., 20., 20., 20., 18., 26., 16., 24., 19., 8., 14., 15., 7., 11., 5., 4., 9., 7., 6., 1., 6., 2., 4., 2., 0., 2., 1., 2.]), array([-2.595 , -2.4898, -2.3845, -2.2793, -2.1741, -2.0688, -1.9636, -1.8584, -1.7531, -1.6479, -1.5427, -1.4374, -1.3322, -1.227 , -1.1217, -1.0165, -0.9112, -0.806 , -0.7008, -0.5955, -0.4903, -0.3851, -0.2798, -0.1746, -0.0694, 0.0359, 0.1411, 0.2463, 0.3516, 0.4568, 0.562 , 0.6673, 0.7725, 0.8777, 0.983 , 1.0882, 1.1935, 1.2987, 1.4039, 1.5092, 1.6144, 1.7196, 1.8249, 1.9301, 2.0353, 2.1406, 2.2458, 2.351 , 2.4563, 2.5615, 2.6667]), &lt;a list of 50 Patch objects&gt;) (array([ 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 4., 1., 4., 5., 11., 8., 6., 11., 13., 13., 17., 18., 20., 27., 32., 29., 31., 22., 21., 31., 29., 19., 22., 18., 10., 18., 11., 12., 9., 6., 2., 3., 3., 3., 2., 1., 1., 1., 0., 1.]), array([-3.7454, -3.6052, -3.4651, -3.325 , -3.1849, -3.0448, -2.9047, -2.7646, -2.6244, -2.4843, -2.3442, -2.2041, -2.064 , -1.9239, -1.7837, -1.6436, -1.5035, -1.3634, -1.2233, -1.0832, -0.9431, -0.8029, -0.6628, -0.5227, -0.3826, -0.2425, -0.1024, 0.0377, 0.1779, 0.318 , 0.4581, 0.5982, 0.7383, 0.8784, 1.0185, 1.1587, 1.2988, 1.4389, 1.579 , 1.7191, 1.8592, 1.9994, 2.1395, 2.2796, 2.4197, 2.5598, 2.6999, 2.84 , 2.9802, 3.1203, 3.2604]), &lt;a list of 50 Patch objects&gt;) (array([ 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 2., 5., 9., 8., 6., 2., 11., 17., 10., 13., 10., 14., 12., 27., 17., 28., 27., 25., 14., 24., 25., 38., 13., 24., 15., 10., 17., 14., 13., 8., 7., 10., 3., 7., 2., 5., 2., 0., 1., 1.]), array([-3.4283, -3.3066, -3.185 , -3.0633, -2.9417, -2.8201, -2.6984, -2.5768, -2.4551, -2.3335, -2.2119, -2.0902, -1.9686, -1.847 , -1.7253, -1.6037, -1.482 , -1.3604, -1.2388, -1.1171, -0.9955, -0.8739, -0.7522, -0.6306, -0.5089, -0.3873, -0.2657, -0.144 , -0.0224, 0.0993, 0.2209, 0.3425, 0.4642, 0.5858, 0.7074, 0.8291, 0.9507, 1.0724, 1.194 , 1.3156, 1.4373, 1.5589, 1.6806, 1.8022, 1.9238, 2.0455, 2.1671, 2.2887, 2.4104, 2.532 , 2.6537]), &lt;a list of 50 Patch objects&gt;) 颜色、标记和线型1plt.figure()1plt.plot(randn(30).cumsum(), 'ko--')1plt.close('all')1234data = randn(30).cumsum()plt.plot(data, 'k--', label='Default')plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')plt.legend(loc='best')刻度、标签和图例设置标题、轴标签、刻度以及刻度标签12345678fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)ax.plot(randn(1000).cumsum())ticks = ax.set_xticks([0, 250, 500, 750, 1000])labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')ax.set_title('My first matplotlib plot')ax.set_xlabel('Stages')添加图例123456fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)ax.plot(randn(1000).cumsum(), 'k', label='one')ax.plot(randn(1000).cumsum(), 'k--', label='two')ax.plot(randn(1000).cumsum(), 'k.', label='three')ax.legend(loc='best')注解以及在subplot上绘图123456789101112131415161718192021222324252627from datetime import datetimefig = plt.figure()ax = fig.add_subplot(1, 1, 1)data = pd.read_csv('ch08/spx.csv', index_col=0, parse_dates=True)spx = data['SPX']spx.plot(ax=ax, style='k-')crisis_data = [ (datetime(2007, 10, 11), 'Peak of bull market'), (datetime(2008, 3, 12), 'Bear Stearns Fails'), (datetime(2008, 9, 15), 'Lehman Bankruptcy')]for date, label in crisis_data: ax.annotate(label, xy=(date, spx.asof(date) + 50), xytext=(date, spx.asof(date) + 200), arrowprops=dict(facecolor='black'), horizontalalignment='left', verticalalignment='top')# Zoom in on 2007-2010ax.set_xlim(['1/1/2007', '1/1/2011'])ax.set_ylim([600, 1800])ax.set_title('Important dates in 2008-2009 financial crisis')1234567891011fig = plt.figure()ax = fig.add_subplot(1, 1, 1)rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)ax.add_patch(rect)ax.add_patch(circ)ax.add_patch(pgon)将图表保存到文件1fig1fig.savefig('figpath.svg')1fig.savefig('figpath.png', dpi=400, bbox_inches='tight')1234from io import BytesIObuffer = BytesIO()plt.savefig(buffer)plot_data = buffer.getvalue()&lt;matplotlib.figure.Figure at 0xaebe550&gt; matplotlib 配置1plt.rc('figure', figsize=(10, 10))pandas中的绘图函数线型图1plt.close('all')12s = Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))s.plot()1234df = DataFrame(np.random.randn(10, 4).cumsum(0), columns=['A', 'B', 'C', 'D'], index=np.arange(0, 100, 10))df.plot()柱状图1234fig, axes = plt.subplots(2, 1)data = Series(np.random.rand(16), index=list('abcdefghijklmnop'))data.plot(kind='bar', ax=axes[0], color='k', alpha=0.7)data.plot(kind='barh', ax=axes[1], color='k', alpha=0.7)12345df = DataFrame(np.random.rand(6, 4), index=['one', 'two', 'three', 'four', 'five', 'six'], columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))dfdf.plot(kind='bar')GenusABCDone0.3016860.1563330.3719430.270731two0.7505890.5255870.6894290.358974three0.3815040.6677070.4737720.632528four0.9424080.1801860.7082840.641783five0.8402780.9095890.0100410.653207six0.0628540.5898130.8113180.0602171plt.figure()1df.plot(kind='barh', stacked=True, alpha=0.5)123456tips = pd.read_csv('ch08/tips.csv')party_counts = pd.crosstab(tips.day, tips.size_)party_counts# Not many 1- and 6-person partiesparty_counts = party_counts.ix[:, 2:5]party_countssize_123456dayFri1161100Sat253181310Sun039151831Thur1484513size_2345dayFri16110Sat5318131Sun3915183Thur4845112345# Normalize to sum to 1party_pcts = party_counts.div(party_counts.sum(1).astype(float), axis=0)party_pctsparty_pcts.plot(kind='bar', stacked=True)size_2345dayFri0.8888890.0555560.0555560.000000Sat0.6235290.2117650.1529410.011765Sun0.5200000.2000000.2400000.040000Thur0.8275860.0689660.0862070.017241直方图和密度图1plt.figure()12tips['tip_pct'] = tips['tip'] / tips['total_bill']tips['tip_pct'].hist(bins=50)1plt.figure()1tips['tip_pct'].plot(kind='kde')1plt.figure()12345comp1 = np.random.normal(0, 1, size=200) # N(0, 1)comp2 = np.random.normal(10, 2, size=200) # N(10, 4)values = Series(np.concatenate([comp1, comp2]))values.hist(bins=100, alpha=0.3, color='k', normed=True)values.plot(kind='kde', style='k--')散点图1234macro = pd.read_csv('ch08/macrodata.csv')data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]trans_data = np.log(data).diff().dropna()trans_data[-5:]cpim1tbilrateunemp198-0.0079040.045361-0.3968810.105361199-0.0219790.066753-2.2772670.1397622000.0023400.0102860.6061360.1603432010.0084190.037461-0.2006710.1273392020.0088940.012202-0.4054650.0425601plt.figure()12plt.scatter(trans_data['m1'], trans_data['unemp'])plt.title('Changes in log %s vs. log %s' % ('m1', 'unemp'))1pd.scatter_matrix(trans_data, diagonal='kde', c='k', alpha=0.3)绘制地图：图形化显示海底地震危机数据12data = pd.read_csv('ch08/Haiti.csv')data.info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 3593 entries, 0 to 3592 Data columns (total 10 columns): Serial 3593 non-null int64 INCIDENT TITLE 3593 non-null object INCIDENT DATE 3593 non-null object LOCATION 3592 non-null object DESCRIPTION 3593 non-null object CATEGORY 3587 non-null object LATITUDE 3593 non-null float64 LONGITUDE 3593 non-null float64 APPROVED 3593 non-null object VERIFIED 3593 non-null object dtypes: float64(2), int64(1), object(7) memory usage: 280.8+ KB 1data[['INCIDENT DATE', 'LATITUDE', 'LONGITUDE']][:10]INCIDENT DATELATITUDELONGITUDE005/07/2010 17:2618.233333-72.533333128/06/2010 23:0650.2260295.729886224/06/2010 16:2122.278381114.174287320/06/2010 21:5944.4070628.933989418/05/2010 16:2618.571084-72.334671526/04/2010 13:1418.593707-72.310079626/04/2010 14:1918.482800-73.638800726/04/2010 14:2718.415000-73.195000815/03/2010 10:5818.517443-72.236841915/03/2010 11:0018.547790-72.4100101data['CATEGORY'][:6]0 1. Urgences | Emergency, 3. Public Health, 1 1. Urgences | Emergency, 2. Urgences logistiqu... 2 2. Urgences logistiques | Vital Lines, 8. Autr... 3 1. Urgences | Emergency, 4 1. Urgences | Emergency, 5 5e. Communication lines down, Name: CATEGORY, dtype: object 1data.describe()SerialLATITUDELONGITUDEcount3593.0000003593.0000003593.000000mean2080.27748418.611495-72.322680std1171.1003600.7385723.650776min4.00000018.041313-74.45275725%1074.00000018.524070-72.41750050%2163.00000018.539269-72.33500075%3088.00000018.561820-72.293570max4052.00000050.226029114.174287123data = data[(data.LATITUDE &gt; 18) &amp; (data.LATITUDE &lt; 20) &amp; (data.LONGITUDE &gt; -75) &amp; (data.LONGITUDE &lt; -70) &amp; data.CATEGORY.notnull()]12345678910111213def to_cat_list(catstr): stripped = (x.strip() for x in catstr.split(',')) return [x for x in stripped if x]def get_all_categories(cat_series): cat_sets = (set(to_cat_list(x)) for x in cat_series) return sorted(set.union(*cat_sets))def get_english(cat): code, names = cat.split('.') if '|' in names: names = names.split(' | ')[1] return code, names.strip()1get_english('2. Urgences logistiques | Vital Lines')(&#39;2&#39;, &#39;Vital Lines&#39;) 12345all_cats = get_all_categories(data.CATEGORY)# Generator expressionenglish_mapping = dict(get_english(x) for x in all_cats)english_mapping['2a']english_mapping['6c']&#39;Food Shortage&#39; &#39;Earthquake and aftershocks&#39; 1234567def get_code(seq): return [x.split('.')[0] for x in seq if x]all_codes = get_code(all_cats)code_index = pd.Index(np.unique(all_codes))dummy_frame = DataFrame(np.zeros((len(data), len(code_index))), index=data.index, columns=code_index)1dummy_frame.ix[:, :6].info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 3569 entries, 0 to 3592 Data columns (total 6 columns): 1 3569 non-null float64 1a 3569 non-null float64 1b 3569 non-null float64 1c 3569 non-null float64 1d 3569 non-null float64 2 3569 non-null float64 dtypes: float64(6) memory usage: 195.2 KB 12345for row, cat in zip(data.index, data.CATEGORY): codes = get_code(to_cat_list(cat)) dummy_frame.ix[row, codes] = 1data = data.join(dummy_frame.add_prefix('category_'))1data.ix[:, 10:15].info()&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 3569 entries, 0 to 3592 Data columns (total 5 columns): category_1 3569 non-null float64 category_1a 3569 non-null float64 category_1b 3569 non-null float64 category_1c 3569 non-null float64 category_1d 3569 non-null float64 dtypes: float64(5) memory usage: 167.3 KB 1234567891011121314151617from mpl_toolkits.basemap import Basemapimport matplotlib.pyplot as pltdef basic_haiti_map(ax=None, lllat=17.25, urlat=20.25, lllon=-75, urlon=-71): # create polar stereographic Basemap instance. m = Basemap(ax=ax, projection='stere', lon_0=(urlon + lllon) / 2, lat_0=(urlat + lllat) / 2, llcrnrlat=lllat, urcrnrlat=urlat, llcrnrlon=lllon, urcrnrlon=urlon, resolution='f') # draw coastlines, state and country boundaries, edge of map. m.drawcoastlines() m.drawstates() m.drawcountries() return m123456789101112131415161718fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))fig.subplots_adjust(hspace=0.05, wspace=0.05)to_plot = ['2a', '1', '3c', '7a']lllat=17.25; urlat=20.25; lllon=-75; urlon=-71for code, ax in zip(to_plot, axes.flat): m = basic_haiti_map(ax, lllat=lllat, urlat=urlat, lllon=lllon, urlon=urlon) cat_data = data[data['category_%s' % code] == 1] # compute map proj coordinates. x, y = m(cat_data.LONGITUDE.values, cat_data.LATITUDE.values) m.plot(x, y, 'k.', alpha=0.5) ax.set_title('%s: %s' % (code, english_mapping[code]))C:\Users\Ewan\Anaconda3\envs\ipykernel_py2\lib\site-packages\mpl_toolkits\basemap\__init__.py:3260: MatplotlibDeprecationWarning: The ishold function was deprecated in version 2.0. b = ax.ishold() C:\Users\Ewan\Anaconda3\envs\ipykernel_py2\lib\site-packages\mpl_toolkits\basemap\__init__.py:3269: MatplotlibDeprecationWarning: axes.hold is deprecated. See the API Changes document (http://matplotlib.org/api/api_changes.html) for more details. ax.hold(b) 12345678910111213141516171819202122#街道数据的路径shapefilepath = 'ch08/PortAuPrince_Roads/PortAuPrince_Roads'fig = plt.figure()ax = fig.add_subplot(1,1,1)lat0 = 18.533333;lon0 = -72.333333;change = 0.13;lllat=lat0-change; urlat=lat0+change; lllon=lon0-change; urlon=lon0+change;m = basic_haiti_map(ax, lllat=lllat, urlat=urlat,lllon=lllon, urlon=urlon)m.readshapefile(shapefilepath,'roads') #添加街道数据code = '2a'cat_data = data[data['category_%s' % code] == 1]# compute map proj coordinates.x, y = m(cat_data.LONGITUDE.values, cat_data.LATITUDE.values)m.plot(x, y, 'k.', alpha=0.5)ax.set_title('Food shortages reported in Port-au-Prince')# plt.savefig('myfig.png',dpi=400,bbox_inches='tight')(1583, 3, [-72.749246, 18.409952, 0.0, 0.0], [-71.973789, 18.7147105, 0.0, 0.0],]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 softmax]]></title>
      <url>%2F2017%2F03%2F05%2Fcs231n-Assignment-1-softmax%2F</url>
      <content type="text"><![CDATA[Softmax exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website.This exercise is analogous to the SVM exercise. You will:implement a fully-vectorized loss function for the Softmax classifierimplement the fully-vectorized expression for its analytic gradientcheck your implementation with numerical gradientuse a validation set to tune the learning rate and regularization strengthoptimize the loss function with SGDvisualize the final learned weights12345678910111213import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# for auto-reloading extenrnal modules# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500): """ Load the CIFAR-10 dataset from disk and perform preprocessing to prepare it for the linear classifier. These are the same steps as we used for the SVM, but condensed to a single function. """ # Load the raw CIFAR-10 data cifar10_dir = 'cs231n/datasets/cifar-10-batches-py' X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir) # subsample the data mask = range(num_training, num_training + num_validation) X_val = X_train[mask] y_val = y_train[mask] mask = range(num_training) X_train = X_train[mask] y_train = y_train[mask] mask = range(num_test) X_test = X_test[mask] y_test = y_test[mask] mask = np.random.choice(num_training, num_dev, replace=False) X_dev = X_train[mask] y_dev = y_train[mask] # Preprocessing: reshape the image data into rows X_train = np.reshape(X_train, (X_train.shape[0], -1)) X_val = np.reshape(X_val, (X_val.shape[0], -1)) X_test = np.reshape(X_test, (X_test.shape[0], -1)) X_dev = np.reshape(X_dev, (X_dev.shape[0], -1)) # Normalize the data: subtract the mean image mean_image = np.mean(X_train, axis = 0) X_train -= mean_image X_val -= mean_image X_test -= mean_image X_dev -= mean_image # add bias dimension and transform into columns X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]) X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]) X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]) X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))]) return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev# Invoke the above function to get our data.X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeprint 'dev data shape: ', X_dev.shapeprint 'dev labels shape: ', y_dev.shapeTrain data shape: (49000L, 3073L) Train labels shape: (49000L,) Validation data shape: (1000L, 3073L) Validation labels shape: (1000L,) Test data shape: (1000L, 3073L) Test labels shape: (1000L,) dev data shape: (500L, 3073L) dev labels shape: (500L,) Softmax ClassifierYour code for this section will all be written inside cs231n/classifiers/softmax.py.1234567891011121314# First implement the naive softmax loss function with nested loops.# Open the file cs231n/classifiers/softmax.py and implement the# softmax_loss_naive function.from cs231n.classifiers.softmax import softmax_loss_naiveimport time# Generate a random softmax weight matrix and use it to compute the loss.W = np.random.randn(3073, 10) * 0.0001loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)# As a rough sanity check, our loss should be something close to -log(0.1).print 'loss: %f' % lossprint 'sanity check: %f' % (-np.log(0.1))loss: 2.395985 sanity check: 2.302585 Inline Question 1:Why do we expect our loss to be close to -log(0.1)? Explain briefly.Your answer: Because the W is selected by random, so the probability of select the true class is 1/10. That is, 0.1.1234567891011121314# Complete the implementation of softmax_loss_naive and implement a (naive)# version of the gradient that uses nested loops.loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)# As we did for the SVM, use numeric gradient checking as a debugging tool.# The numeric gradient should be close to the analytic gradient.from cs231n.gradient_check import grad_check_sparsef = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]grad_numerical = grad_check_sparse(f, W, grad, 10)# similar to SVM case, do another gradient check with regularizationloss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]grad_numerical = grad_check_sparse(f, W, grad, 10)numerical: 2.368141 analytic: 2.368141, relative error: 2.349797e-08 numerical: 1.324690 analytic: 1.324690, relative error: 7.140560e-08 numerical: 3.170412 analytic: 3.170411, relative error: 1.324741e-08 numerical: 0.249509 analytic: 0.249509, relative error: 2.647240e-08 numerical: 1.536095 analytic: 1.536095, relative error: 4.345856e-08 numerical: 1.075819 analytic: 1.075819, relative error: 3.902323e-08 numerical: -0.198098 analytic: -0.198098, relative error: 5.737134e-08 numerical: -0.089902 analytic: -0.089902, relative error: 8.604010e-07 numerical: -0.339487 analytic: -0.339487, relative error: 3.992996e-08 numerical: -4.819781 analytic: -4.819781, relative error: 3.465667e-09 numerical: 1.869922 analytic: 1.869921, relative error: 7.536693e-08 numerical: 0.783465 analytic: 0.783465, relative error: 6.960291e-08 numerical: -3.206007 analytic: -3.206007, relative error: 2.337350e-09 numerical: 0.532183 analytic: 0.532183, relative error: 1.498128e-07 numerical: 0.900500 analytic: 0.900500, relative error: 6.954913e-09 numerical: -0.353224 analytic: -0.353224, relative error: 1.836960e-07 numerical: -1.331470 analytic: -1.331470, relative error: 2.726426e-08 numerical: -0.082452 analytic: -0.082452, relative error: 7.712355e-07 numerical: -1.322133 analytic: -1.322133, relative error: 5.516628e-09 numerical: 0.345814 analytic: 0.345814, relative error: 1.251858e-07 1234567891011121314151617181920# Now that we have a naive implementation of the softmax loss function and its gradient,# implement a vectorized version in softmax_loss_vectorized.# The two versions should compute the same results, but the vectorized version should be# much faster.tic = time.time()loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)from cs231n.classifiers.softmax import softmax_loss_vectorizedtic = time.time()loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)# As we did for the SVM, we use the Frobenius norm to compare the two versions# of the gradient.grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)print 'Gradient difference: %f' % grad_differencenaive loss: 2.395985e+00 computed in 0.080000s vectorized loss: 2.395985e+00 computed in 0.003000s Loss difference: 0.000000 Gradient difference: 0.000000 1234567891011121314151617181920212223242526272829303132333435363738394041424344# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of over 0.35 on the validation set.from cs231n.classifiers import Softmaxresults = &#123;&#125;best_val = -1best_softmax = Nonelearning_rates = [1e-8, 1e-7, 2e-7]regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained softmax classifer in best_softmax. #################################################################################iters = 2000for lr in learning_rates: for rs in regularization_strengths: softmax = Softmax() softmax.train(X_train, y_train, learning_rate=lr, reg=rs, num_iters=iters) y_train_pred = softmax.predict(X_train) acc_train = np.mean(y_train == y_train_pred) y_val_pred = softmax.predict(X_val) acc_val = np.mean(y_val == y_val_pred) results[(lr, rs)] = (acc_train, acc_val) if best_val &lt; acc_val: best_val = acc_val best_softmax = softmax################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print 'lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy) print 'best validation accuracy achieved during cross-validation: %f' % best_vallr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.175633 val accuracy: 0.179000 lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.174102 val accuracy: 0.161000 lr 1.000000e-08 reg 3.000000e+04 train accuracy: 0.203490 val accuracy: 0.210000 lr 1.000000e-08 reg 4.000000e+04 train accuracy: 0.191367 val accuracy: 0.202000 lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.208000 val accuracy: 0.197000 lr 1.000000e-08 reg 6.000000e+04 train accuracy: 0.203571 val accuracy: 0.215000 lr 1.000000e-08 reg 7.000000e+04 train accuracy: 0.213551 val accuracy: 0.215000 lr 1.000000e-08 reg 8.000000e+04 train accuracy: 0.238347 val accuracy: 0.229000 lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.245102 val accuracy: 0.242000 lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.358265 val accuracy: 0.362000 lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.356306 val accuracy: 0.374000 lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.347327 val accuracy: 0.362000 lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.336347 val accuracy: 0.354000 lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.331490 val accuracy: 0.348000 lr 1.000000e-07 reg 6.000000e+04 train accuracy: 0.320163 val accuracy: 0.336000 lr 1.000000e-07 reg 7.000000e+04 train accuracy: 0.314551 val accuracy: 0.325000 lr 1.000000e-07 reg 8.000000e+04 train accuracy: 0.313082 val accuracy: 0.324000 lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.303000 val accuracy: 0.315000 lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.374163 val accuracy: 0.389000 lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.353184 val accuracy: 0.365000 lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.340265 val accuracy: 0.359000 lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.334673 val accuracy: 0.351000 lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.326531 val accuracy: 0.337000 lr 2.000000e-07 reg 6.000000e+04 train accuracy: 0.319857 val accuracy: 0.336000 lr 2.000000e-07 reg 7.000000e+04 train accuracy: 0.317878 val accuracy: 0.329000 lr 2.000000e-07 reg 8.000000e+04 train accuracy: 0.310449 val accuracy: 0.329000 lr 2.000000e-07 reg 1.000000e+05 train accuracy: 0.316286 val accuracy: 0.315000 best validation accuracy achieved during cross-validation: 0.389000 12345# evaluate on test set# Evaluate the best softmax on test sety_test_pred = best_softmax.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )softmax on raw pixels final test set accuracy: 0.375000 ​123456789101112131415# Visualize the learned weights for each classw = best_softmax.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in xrange(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])Codes123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import numpy as npfrom random import shuffledef softmax_loss_naive(W, X, y, reg): """ Softmax loss function, naive implementation (with loops) Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# num_train = X.shape[0] num_classes = W.shape[1] for i in xrange(num_train): f = X[i, :].dot(W) f -= np.max(f) correct_f = f[y[i]] denom = np.sum(np.exp(f)) p = np.exp(correct_f) / denom loss += -np.log(p) for j in xrange(num_classes): if j == y[i]: dW[:, y[i]] += (np.exp(f[j]) / denom - 1) * X[i, :] else: dW[:, j] += (np.exp(f[j]) / denom) * X[i, :] loss /= num_train loss += 0.5 * reg * np.sum(W * W) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dWdef softmax_loss_vectorized(W, X, y, reg): """ Softmax loss function, vectorized version. Inputs and outputs are the same as softmax_loss_naive. """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using no explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# num_train = X.shape[0] f = X.dot(W) f = f - np.max(f, axis=1)[:, np.newaxis] loss = -np.sum( np.log(np.exp(f[np.arange(num_train), y]) / np.sum(np.exp(f), axis=1))) loss /= num_train loss += 0.5 * reg * np.sum(W * W) ind = np.zeros_like(f) ind[np.arange(num_train), y] = 1 dW = X.T.dot(np.exp(f) / np.sum(np.exp(f), axis=1, keepdims=True) - ind) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 svm]]></title>
      <url>%2F2017%2F03%2F03%2Fcs231n-Assignment-1-svm%2F</url>
      <content type="text"><![CDATA[Multiclass Support Vector Machine exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website.In this exercise you will:​implement a fully-vectorized loss function for the SVMimplement the fully-vectorized expression for its analytic gradientcheck your implementation using numerical gradientuse a validation set to tune the learning rate and regularization strengthoptimize the loss function with SGDvisualize the final learned weights123456789101112131415161718# Run some setup code for this notebook.import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the# notebook rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2CIFAR-10 Data Loading and Preprocessing123456789# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print 'Training data shape: ', X_train.shapeprint 'Training labels shape: ', y_train.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTraining data shape: (50000L, 32L, 32L, 3L) Training labels shape: (50000L,) Test data shape: (10000L, 32L, 32L, 3L) Test labels shape: (10000L,) 12345678910111213141516# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show()1?np.random.choice1234567891011121314151617181920212223242526272829303132333435363738# Split the data into train, val, and test sets. In addition we will# create a small development set as a subset of the training data;# we can use this for development so our code runs faster.num_training = 49000num_validation = 1000num_test = 1000num_dev = 500# Our validation set will be num_validation points from the original# training set.mask = range(num_training, num_training + num_validation)X_val = X_train[mask]y_val = y_train[mask]# Our training set will be the first num_train points from the original# training set.mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]# We will also make a development set, which is a small subset of# the training set.mask = np.random.choice(num_training, num_dev, replace=False)X_dev = X_train[mask]y_dev = y_train[mask]# We use the first num_test points of the original test set as our# test set.mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask]print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTrain data shape: (49000L, 32L, 32L, 3L) Train labels shape: (49000L,) Validation data shape: (1000L, 32L, 32L, 3L) Validation labels shape: (1000L,) Test data shape: (1000L, 32L, 32L, 3L) Test labels shape: (1000L,) 1234567891011# Preprocessing: reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1))X_val = np.reshape(X_val, (X_val.shape[0], -1))X_test = np.reshape(X_test, (X_test.shape[0], -1))X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))# As a sanity check, print out the shapes of the dataprint 'Training data shape: ', X_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Test data shape: ', X_test.shapeprint 'dev data shape: ', X_dev.shapeTraining data shape: (49000L, 3072L) Validation data shape: (1000L, 3072L) Test data shape: (1000L, 3072L) dev data shape: (500L, 3072L) 1234567# Preprocessing: subtract the mean image# first: compute the image mean based on the training datamean_image = np.mean(X_train, axis=0)print mean_image[:10] # print a few of the elementsplt.figure(figsize=(4,4))plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean imageplt.show()[ 130.64189796 135.98173469 132.47391837 130.05569388 135.34804082 131.75402041 130.96055102 136.14328571 132.47636735 131.48467347] 12345# second: subtract the mean image from train and test dataX_train -= mean_imageX_val -= mean_imageX_test -= mean_imageX_dev -= mean_image12345678# third: append (at the last) the bias dimension of ones (i.e. bias trick) so that our SVM# only has to worry about optimizing a single weight matrix W.X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])print X_train.shape, X_val.shape, X_test.shape, X_dev.shape(49000L, 3073L) (1000L, 3073L) (1000L, 3073L) (500L, 3073L) ​SVM ClassifierYour code for this section will all be written inside cs231n/classifiers/linear_svm.py.As you can see, we have prefilled the function compute_loss_naive which uses for loops to evaluate the multiclass SVM loss function.123456789# Evaluate the naive implementation of the loss we provided for you:from cs231n.classifiers.linear_svm import svm_loss_naiveimport time# generate a random SVM weight matrix of small numbersW = np.random.randn(3073, 10) * 0.0001 loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.00001)print 'loss: %f' % (loss, )loss: 8.831645 ​The grad returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function svm_loss_naive. You will find it helpful to interleave your new code inside the existing function.To check that you have correctly implemented the gradient correctly, you can numerically estimate the gradient of the loss function and compare the numeric estimate to the gradient that you computed. We have provided code that does this for you:123456789101112131415161718# Once you've implemented the gradient, recompute it with the code below# and gradient check it with the function we provided for you# Compute the loss and its gradient at W.loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.0)# Numerically compute the gradient along several randomly chosen dimensions, and# compare them with your analytically computed gradient. The numbers should match# almost exactly along all dimensions.from cs231n.gradient_check import grad_check_sparsef = lambda w: svm_loss_naive(w, X_dev, y_dev, 0.0)[0]grad_numerical = grad_check_sparse(f, W, grad)# do the gradient check once again with regularization turned on# you didn't forget the regularization gradient did you?loss, grad = svm_loss_naive(W, X_dev, y_dev, 1e2)f = lambda w: svm_loss_naive(w, X_dev, y_dev, 1e2)[0]grad_numerical = grad_check_sparse(f, W, grad)numerical: -13.865929 analytic: -13.865929, relative error: 1.283977e-12 numerical: 7.842142 analytic: 7.735021, relative error: 6.876784e-03 numerical: 3.464393 analytic: 3.464393, relative error: 9.040092e-11 numerical: -23.034911 analytic: -23.034911, relative error: 6.876266e-12 numerical: -0.185311 analytic: -0.185311, relative error: 2.538774e-10 numerical: 25.825504 analytic: 25.825504, relative error: 1.336035e-11 numerical: 4.457836 analytic: 4.457836, relative error: 1.015819e-10 numerical: 3.184691 analytic: 3.184691, relative error: 8.849109e-11 numerical: 10.428446 analytic: 10.374317, relative error: 2.601982e-03 numerical: 12.479957 analytic: 12.479957, relative error: 6.825191e-12 numerical: 12.237949 analytic: 12.326308, relative error: 3.597051e-03 numerical: 4.377103 analytic: 4.377103, relative error: 3.904758e-11 numerical: -1.951930 analytic: -1.951930, relative error: 1.432276e-10 numerical: 33.752503 analytic: 33.752503, relative error: 4.254520e-12 numerical: 11.367149 analytic: 11.367149, relative error: 1.682727e-11 numerical: 16.461879 analytic: 16.461879, relative error: 4.766805e-12 numerical: 3.814562 analytic: 3.814562, relative error: 1.087469e-10 numerical: 13.931226 analytic: 13.931226, relative error: 9.578349e-12 numerical: -27.291095 analytic: -27.395406, relative error: 1.907445e-03 numerical: -7.610407 analytic: -7.610407, relative error: 1.015282e-12 Inline Question 1:It is possible that once in a while a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a gradient check could fail? Hint: the SVM loss function is not strictly speaking differentiableYour Answer: Maybe the SVM loss function is not differentiable on that dimension1?np.max1np.sum(np.maximum(0, X_dev.dot(W) - X_dev.dot(W)[np.arange(len(y_dev)), [y_dev]].T + 1))4915.822409730994 123456789101112131415# Next implement the function svm_loss_vectorized; for now only compute the loss;# we will implement the gradient in a moment.tic = time.time()loss_naive, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Naive loss: %e computed in %fs' % (loss_naive, toc - tic)from cs231n.classifiers.linear_svm import svm_loss_vectorizedtic = time.time()loss_vectorized, _ = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)# The losses should match but your vectorized implementation should be much faster.print 'difference: %f' % (loss_naive - loss_vectorized)Naive loss: 8.831645e+00 computed in 0.071000s Vectorized loss: 8.831645e+00 computed in 0.000000s difference: 0.000000 1234567891011121314151617181920# Complete the implementation of svm_loss_vectorized, and compute the gradient# of the loss function in a vectorized way.# The naive implementation and the vectorized implementation should match, but# the vectorized version should still be much faster.tic = time.time()_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Naive loss and gradient: computed in %fs' % (toc - tic)tic = time.time()_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Vectorized loss and gradient: computed in %fs' % (toc - tic)# The loss is a single number, so it is easy to compare the values computed# by the two implementations. The gradient on the other hand is a matrix, so# we use the Frobenius norm to compare them.difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')print 'difference: %f' % differenceNaive loss and gradient: computed in 0.084000s Vectorized loss and gradient: computed in 0.005000s difference: 0.000000 Stochastic Gradient DescentWe now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss.123456789# In the file linear_classifier.py, implement SGD in the function# LinearClassifier.train() and then run it with the code below.from cs231n.classifiers import LinearSVMsvm = LinearSVM()tic = time.time()loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4, num_iters=1500, verbose=True)toc = time.time()print 'That took %fs' % (toc - tic)iteration 0 / 1500: loss 791.772037 iteration 100 / 1500: loss 286.021346 iteration 200 / 1500: loss 107.673095 iteration 300 / 1500: loss 41.812791 iteration 400 / 1500: loss 18.665578 iteration 500 / 1500: loss 10.614984 iteration 600 / 1500: loss 6.664814 iteration 700 / 1500: loss 6.509693 iteration 800 / 1500: loss 5.792204 iteration 900 / 1500: loss 4.986855 iteration 1000 / 1500: loss 5.914691 iteration 1100 / 1500: loss 5.058078 iteration 1200 / 1500: loss 5.491475 iteration 1300 / 1500: loss 5.609450 iteration 1400 / 1500: loss 5.376595 That took 5.454000s 123456# A useful debugging strategy is to plot the loss as a function of# iteration number:plt.plot(loss_hist)plt.xlabel('Iteration number')plt.ylabel('Loss value')plt.show()123456# Write the LinearSVM.predict function and evaluate the performance on both the# training and validation sety_train_pred = svm.predict(X_train)print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )y_val_pred = svm.predict(X_val)print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )training accuracy: 0.364980 validation accuracy: 0.378000 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of about 0.4 on the validation set.learning_rates = [1e-8, 1e-7, 2e-7]regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]# results is dictionary mapping tuples of the form# (learning_rate, regularization_strength) to tuples of the form# (training_accuracy, validation_accuracy). The accuracy is simply the fraction# of data points that are correctly classified.results = &#123;&#125;best_val = -1 # The highest validation accuracy that we have seen so far.best_svm = None # The LinearSVM object that achieved the highest validation rate.################################################################################# TODO: ## Write code that chooses the best hyperparameters by tuning on the validation ## set. For each combination of hyperparameters, train a linear SVM on the ## training set, compute its accuracy on the training and validation sets, and ## store these numbers in the results dictionary. In addition, store the best ## validation accuracy in best_val and the LinearSVM object that achieves this ## accuracy in best_svm. ## ## Hint: You should use a small value for num_iters as you develop your ## validation code so that the SVMs don't take much time to train; once you are ## confident that your validation code works, you should rerun the validation ## code with a larger value for num_iters. #################################################################################for learning_rate in learning_rates: for regularization_strength in regularization_strengths: svm = LinearSVM() loss_hist = svm.train( X_train, y_train, learning_rate, \ regularization_strength, num_iters=1500, batch_size=200) y_train_pred = svm.predict(X_train) y_val_pred = svm.predict(X_val) training_accuracy = np.mean(y_train == y_train_pred) validation_accuracy = np.mean(y_val == y_val_pred) results[(learning_rate, regularization_strength)] = \ (training_accuracy, validation_accuracy) if validation_accuracy &gt; best_val: best_val = validation_accuracy best_svm = svm################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print 'lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy) print 'best validation accuracy achieved during cross-validation: %f' % best_vallr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.221898 val accuracy: 0.247000 lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.233653 val accuracy: 0.258000 lr 1.000000e-08 reg 3.000000e+04 train accuracy: 0.234694 val accuracy: 0.225000 lr 1.000000e-08 reg 4.000000e+04 train accuracy: 0.255959 val accuracy: 0.249000 lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.259755 val accuracy: 0.273000 lr 1.000000e-08 reg 6.000000e+04 train accuracy: 0.267408 val accuracy: 0.269000 lr 1.000000e-08 reg 7.000000e+04 train accuracy: 0.269102 val accuracy: 0.287000 lr 1.000000e-08 reg 8.000000e+04 train accuracy: 0.277102 val accuracy: 0.285000 lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.295306 val accuracy: 0.301000 lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.369388 val accuracy: 0.374000 lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.380265 val accuracy: 0.390000 lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.375490 val accuracy: 0.378000 lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.375633 val accuracy: 0.385000 lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.369694 val accuracy: 0.375000 lr 1.000000e-07 reg 6.000000e+04 train accuracy: 0.372469 val accuracy: 0.383000 lr 1.000000e-07 reg 7.000000e+04 train accuracy: 0.356000 val accuracy: 0.370000 lr 1.000000e-07 reg 8.000000e+04 train accuracy: 0.352816 val accuracy: 0.355000 lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.356796 val accuracy: 0.377000 lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.393510 val accuracy: 0.395000 lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.377020 val accuracy: 0.382000 lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.363857 val accuracy: 0.373000 lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.368714 val accuracy: 0.372000 lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.361531 val accuracy: 0.364000 lr 2.000000e-07 reg 6.000000e+04 train accuracy: 0.354714 val accuracy: 0.368000 lr 2.000000e-07 reg 7.000000e+04 train accuracy: 0.348306 val accuracy: 0.365000 lr 2.000000e-07 reg 8.000000e+04 train accuracy: 0.358082 val accuracy: 0.378000 lr 2.000000e-07 reg 1.000000e+05 train accuracy: 0.347898 val accuracy: 0.358000 best validation accuracy achieved during cross-validation: 0.395000 123456789101112131415161718192021222324# Visualize the cross-validation resultsimport mathx_scatter = [math.log10(x[0]) for x in results]y_scatter = [math.log10(x[1]) for x in results]# plot training accuracymarker_size = 100colors = [results[x][0] for x in results]plt.subplot(3, 1, 1)plt.scatter(x_scatter, y_scatter, marker_size, c=colors)plt.colorbar()plt.xlabel('log learning rate')plt.ylabel('log regularization strength')plt.title('CIFAR-10 training accuracy')# plot validation accuracycolors = [results[x][1] for x in results] # default size of markers is 20plt.subplot(3, 1, 3)plt.scatter(x_scatter, y_scatter, marker_size, c=colors)plt.colorbar()plt.xlabel('log learning rate')plt.ylabel('log regularization strength')plt.title('CIFAR-10 validation accuracy')plt.show()1234# Evaluate the best svm on test sety_test_pred = best_svm.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print 'linear SVM on raw pixels final test set accuracy: %f' % test_accuracylinear SVM on raw pixels final test set accuracy: 0.383000 ​12x = np.array([[[0], [1], [2]]])np.squeeze(x)array([0, 1, 2]) 123456789101112131415# Visualize the learned weights for each class.# Depending on your choice of learning rate and regularization strength, these may# or may not be nice to look at.w = best_svm.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in xrange(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])Inline question 2:Describe what your visualized SVM weights look like, and offer a brief explanation for why they look they way that they do.Your answer: fill this inCodeslinear_svm.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import numpy as npfrom random import shuffledef svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in xrange(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in xrange(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: dW[:, y[i]] -= X[i, :] dW[:, j] += X[i, :] loss += margin # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += 0.5 * reg * np.sum(W * W) dW += reg * W ############################################################################# # TODO: # # Compute the gradient of the loss function and store it dW. # # Rather that first computing the loss and then computing the derivative, # # it may be simpler to compute the derivative at the same time that the # # loss is being computed. As a result you may need to modify some of the # # code above to compute the gradient. # ############################################################################# return loss, dWdef svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero ############################################################################# # TODO: # # Implement a vectorized version of the structured SVM loss, storing the # # result in loss. # ############################################################################# num_train = X.shape[0] delta = 1.0 scores = X.dot(W) correct_class_score = scores[np.arange(num_train), y] margins = np.maximum( 0, scores - correct_class_score[:, np.newaxis] + delta) margins[np.arange(num_train), y] = 0 loss = np.sum(margins) loss /= num_train loss += 0.5 * reg * np.sum(W.T.dot(W)) ############################################################################# # END OF YOUR CODE # ############################################################################# ############################################################################# # TODO: # # Implement a vectorized version of the gradient for the structured SVM # # loss, storing the result in dW. # # # # Hint: Instead of computing the gradient from scratch, it may be easier # # to reuse some of the intermediate values that you used to compute the # # loss. # ############################################################################# X_mask = np.zeros(margins.shape) X_mask[margins &gt; 0] = 1 count = np.sum(X_mask, axis=1) X_mask[np.arange(num_train), y] = -count dW = X.T.dot(X_mask) dW /= num_train dW += np.multiply(W, reg) ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dWlinear_classifier123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import numpy as npfrom cs231n.classifiers.linear_svm import *from cs231n.classifiers.softmax import *class LinearClassifier(object): def __init__(self): self.W = None def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): """ Train this linear classifier using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label 0 &lt;= c &lt; C for C classes. - learning_rate: (float) learning rate for optimization. - reg: (float) regularization strength. - num_iters: (integer) number of steps to take when optimizing - batch_size: (integer) number of training examples to use at each step. - verbose: (boolean) If true, print progress during optimization. Outputs: A list containing the value of the loss function at each training iteration. """ num_train, dim = X.shape num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in xrange(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: # # Sample batch_size elements from the training data and their # # corresponding labels to use in this round of gradient descent. # # Store the data in X_batch and their corresponding labels in # # y_batch; after sampling X_batch should have shape (dim, batch_size) # # and y_batch should have shape (batch_size,) # # # # Hint: Use np.random.choice to generate indices. Sampling with # # replacement is faster than sampling without replacement. # ######################################################################### mask = np.random.choice(num_train, batch_size, replace=True) X_batch = X[mask] y_batch = y[mask] ######################################################################### # END OF YOUR CODE # ######################################################################### # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) # perform parameter update ######################################################################### # TODO: # # Update the weights using the gradient and the learning rate. # ######################################################################### self.W = self.W - learning_rate * grad ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print 'iteration %d / %d: loss %f' % (it, num_iters, loss) return loss_history def predict(self, X): """ Use the trained weights of this linear classifier to predict labels for data points. Inputs: - X: D x N array of training data. Each column is a D-dimensional point. Returns: - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional array of length N, and each element is an integer giving the predicted class. """ X = X.T y_pred = np.zeros(X.shape[1]) ########################################################################### # TODO: # # Implement this method. Store the predicted labels in y_pred. # ########################################################################### scores = X.T.dot(self.W) y_pred = np.argsort(scores, axis=1)[:, -1] ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred def loss(self, X_batch, y_batch, reg): """ Compute the loss function and its derivative. Subclasses will override this. Inputs: - X_batch: A numpy array of shape (N, D) containing a minibatch of N data points; each point has dimension D. - y_batch: A numpy array of shape (N,) containing labels for the minibatch. - reg: (float) regularization strength. Returns: A tuple containing: - loss as a single float - gradient with respect to self.W; an array of the same shape as W """ passclass LinearSVM(LinearClassifier): """ A subclass that uses the Multiclass SVM loss function """ def loss(self, X_batch, y_batch, reg): return svm_loss_vectorized(self.W, X_batch, y_batch, reg)class Softmax(LinearClassifier): """ A subclass that uses the Softmax + Cross-entropy loss function """ def loss(self, X_batch, y_batch, reg): return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 kNN]]></title>
      <url>%2F2017%2F03%2F02%2Fcs231n-Assignment-1-kNN%2F</url>
      <content type="text"><![CDATA[k-Nearest Neighbor (kNN) exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website.The kNN classifier consists of two stages:During training, the classifier takes the training data and simply remembers itDuring testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examplesThe value of k is cross-validatedIn this exercise you will implement these steps and understand the basic Image Classification pipeline, cross-validation, and gain proficiency in writing efficient, vectorized code.123456789101112131415161718# Run some setup code for this notebook.import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the notebook# rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2123456789# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print 'Training data shape: ', X_train.shapeprint 'Training labels shape: ', y_train.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTraining data shape: (50000L, 32L, 32L, 3L) Training labels shape: (50000L,) Test data shape: (10000L, 32L, 32L, 3L) Test labels shape: (10000L,) 12345678910111213141516# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show()12345678910# Subsample the data for more efficient code execution in this exercisenum_training = 5000mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]num_test = 500mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask]1234# Reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1)) # Wow~X_test = np.reshape(X_test, (X_test.shape[0], -1))print X_train.shape, X_test.shape(5000L, 3072L) (500L, 3072L) ​1234567from cs231n.classifiers import KNearestNeighbor# Create a kNN classifier instance. # Remember that training a kNN classifier is a noop: # the Classifier simply remembers the data and does no further processing classifier = KNearestNeighbor()classifier.train(X_train, y_train)We would now like to classify the test data with the kNN classifier. Recall that we can break down this process into two steps:First we must compute the distances between all test examples and all train examples.Given these distances, for each test example we find the k nearest examples and have them vote for the labelLets begin with computing the distance matrix between all training and test examples. For example, if there are Ntr training examples and Nte test examples, this stage should result in a Nte x Ntr matrix where each element (i,j) is the distance between the i-th test and j-th train example.First, open cs231n/classifiers/k_nearest_neighbor.py and implement the function compute_distances_two_loops that uses a (very inefficient) double loop over all pairs of (test, train) examples and computes the distance matrix one element at a time.123456# Open cs231n/classifiers/k_nearest_neighbor.py and implement# compute_distances_two_loops.# Test your implementation:dists = classifier.compute_distances_two_loops(X_test)print dists.shape(500L, 5000L) ​1234# We can visualize the distance matrix: each row is a single test example and# its distances to training examplesplt.imshow(dists, interpolation='none')plt.show()Inline Question #1: Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)What in the data is the cause behind the distinctly bright rows?What causes the columns?Your Answer: Maybe exists noises in test data set and train data set.12345678# Now implement the function predict_labels and run the code below:# We use k = 1 (which is Nearest Neighbor).y_test_pred = classifier.predict_labels(dists, k=1)# Compute and print the fraction of correctly predicted examplesnum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)Got 137 / 500 correct =&gt; accuracy: 0.274000 ​You should expect to see approximately 27% accuracy. Now lets try out a larger k, say k = 5:1234y_test_pred = classifier.predict_labels(dists, k=5)num_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)Got 142 / 500 correct =&gt; accuracy: 0.284000 ​You should expect to see a slightly better performance than with k = 1.1234567891011121314151617# Now lets speed up distance matrix computation by using partial vectorization# with one loop. Implement the function compute_distances_one_loop and run the# code below:dists_one = classifier.compute_distances_one_loop(X_test)# To ensure that our vectorized implementation is correct, we make sure that it# agrees with the naive implementation. There are many ways to decide whether# two matrices are similar; one of the simplest is the Frobenius norm. In case# you haven't seen it before, the Frobenius norm of two matrices is the square# root of the squared sum of differences of all elements; in other words, reshape# the matrices into vectors and compute the Euclidean distance between them.difference = np.linalg.norm(dists - dists_one, ord='fro')print 'Difference was: %f' % (difference, )if difference &lt; 0.001: print 'Good! The distance matrices are the same'else: print 'Uh-oh! The distance matrices are different'Difference was: 0.000000 Good! The distance matrices are the same 1234567891011# Now implement the fully vectorized version inside compute_distances_no_loops# and run the codedists_two = classifier.compute_distances_no_loops(X_test)# check that the distance matrix agrees with the one we computed before:difference = np.linalg.norm(dists - dists_two, ord='fro')print 'Difference was: %f' % (difference, )if difference &lt; 0.001: print 'Good! The distance matrices are the same'else: print 'Uh-oh! The distance matrices are different'Difference was: 0.000000 Good! The distance matrices are the same 123456789101112131415161718192021# Let's compare how fast the implementations aredef time_function(f, *args): """ Call a function f with args and return the time (in seconds) that it took to execute. """ import time tic = time.time() f(*args) toc = time.time() return toc - tictwo_loop_time = time_function(classifier.compute_distances_two_loops, X_test)print 'Two loop version took %f seconds' % two_loop_timeone_loop_time = time_function(classifier.compute_distances_one_loop, X_test)print 'One loop version took %f seconds' % one_loop_timeno_loop_time = time_function(classifier.compute_distances_no_loops, X_test)print 'No loop version took %f seconds' % no_loop_time# you should see significantly faster performance with the fully vectorized implementationTwo loop version took 27.001000 seconds One loop version took 59.630000 seconds No loop version took 0.205000 seconds Cross-validationWe have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]X_train_folds = []y_train_folds = []################################################################################# TODO: ## Split up the training data into folds. After splitting, X_train_folds and ## y_train_folds should each be lists of length num_folds, where ## y_train_folds[i] is the label vector for the points in X_train_folds[i]. ## Hint: Look up the numpy array_split function. #################################################################################X_train_folds = np.array_split(X_train, num_folds)y_train_folds = np.array_split(y_train, num_folds)################################################################################# END OF YOUR CODE ################################################################################## A dictionary holding the accuracies for different values of k that we find# when running cross-validation. After running cross-validation,# k_to_accuracies[k] should be a list of length num_folds giving the different# accuracy values that we found when using that value of k.k_to_accuracies = &#123;&#125;################################################################################# TODO: ## Perform k-fold cross validation to find the best value of k. For each ## possible value of k, run the k-nearest-neighbor algorithm num_folds times, ## where in each case you use all but one of the folds as training data and the ## last fold as a validation set. Store the accuracies for all fold and all ## values of k in the k_to_accuracies dictionary. #################################################################################for k in k_choices: k_to_accuracies[k] = [] for fold in xrange(num_folds): train_X = np.append( X_train_folds[:fold], X_train_folds[fold+1:]).reshape( (X_train.shape[0] - X_train.shape[0]/num_folds, -1)) train_y = np.append( y_train_folds[:fold], y_train_folds[fold+1:]).reshape( (y_train.shape[0] - y_train.shape[0]/num_folds, -1)).flatten() classifier.train(train_X, train_y) dists = classifier.compute_distances_no_loops(X_train_folds[fold]) y_test_pred = classifier.predict_labels(dists, k) num_correct = np.sum(y_test_pred == y_train_folds[fold]) accuracy = float(num_correct) / len(y_train_folds[fold]) k_to_accuracies[k].append(accuracy) ################################################################################# END OF YOUR CODE ################################################################################## Print out the computed accuraciesfor k in sorted(k_to_accuracies): for accuracy in k_to_accuracies[k]: print 'k = %d, accuracy = %f' % (k, accuracy)k = 1, accuracy = 0.263000 k = 1, accuracy = 0.257000 k = 1, accuracy = 0.264000 k = 1, accuracy = 0.278000 k = 1, accuracy = 0.266000 k = 3, accuracy = 0.241000 k = 3, accuracy = 0.249000 k = 3, accuracy = 0.243000 k = 3, accuracy = 0.273000 k = 3, accuracy = 0.264000 k = 5, accuracy = 0.258000 k = 5, accuracy = 0.273000 k = 5, accuracy = 0.281000 k = 5, accuracy = 0.290000 k = 5, accuracy = 0.272000 k = 8, accuracy = 0.263000 k = 8, accuracy = 0.288000 k = 8, accuracy = 0.278000 k = 8, accuracy = 0.285000 k = 8, accuracy = 0.277000 k = 10, accuracy = 0.265000 k = 10, accuracy = 0.296000 k = 10, accuracy = 0.278000 k = 10, accuracy = 0.284000 k = 10, accuracy = 0.286000 k = 12, accuracy = 0.260000 k = 12, accuracy = 0.294000 k = 12, accuracy = 0.281000 k = 12, accuracy = 0.282000 k = 12, accuracy = 0.281000 k = 15, accuracy = 0.255000 k = 15, accuracy = 0.290000 k = 15, accuracy = 0.281000 k = 15, accuracy = 0.281000 k = 15, accuracy = 0.276000 k = 20, accuracy = 0.270000 k = 20, accuracy = 0.281000 k = 20, accuracy = 0.280000 k = 20, accuracy = 0.282000 k = 20, accuracy = 0.284000 k = 50, accuracy = 0.271000 k = 50, accuracy = 0.288000 k = 50, accuracy = 0.278000 k = 50, accuracy = 0.269000 k = 50, accuracy = 0.266000 k = 100, accuracy = 0.256000 k = 100, accuracy = 0.270000 k = 100, accuracy = 0.263000 k = 100, accuracy = 0.256000 k = 100, accuracy = 0.263000 12345678910111213# plot the raw observationsfor k in k_choices: accuracies = k_to_accuracies[k] plt.scatter([k] * len(accuracies), accuracies)# plot the trend line with error bars that correspond to standard deviationaccuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)plt.title('Cross-validation on k')plt.xlabel('k')plt.ylabel('Cross-validation accuracy')plt.show()12345678910111213# Based on the cross-validation results above, choose the best value for k, # retrain the classifier using all the training data, and test it on the test# data. You should be able to get above 28% accuracy on the test data.best_k = 10classifier = KNearestNeighbor()classifier.train(X_train, y_train)y_test_pred = classifier.predict(X_test, k=best_k)# Compute and display the accuracynum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)Got 139 / 500 correct =&gt; accuracy: 0.278000 ​k_nearest_neighbor.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186import numpy as npimport operatorclass KNearestNeighbor(object): """ a kNN classifier with L2 distance """ def __init__(self): pass def train(self, X, y): """ Train the classifier. For k-nearest neighbors this is just memorizing the training data. Inputs: - X: A numpy array of shape (num_train, D) containing the training data consisting of num_train samples each of dimension D. - y: A numpy array of shape (N,) containing the training labels, where y[i] is the label for X[i]. """ self.X_train = X self.y_train = y def predict(self, X, k=1, num_loops=0): """ Predict labels for test data using this classifier. Inputs: - X: A numpy array of shape (num_test, D) containing test data consisting of num_test samples each of dimension D. - k: The number of nearest neighbors that vote for the predicted labels. - num_loops: Determines which implementation to use to compute distances between training points and testing points. Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. """ if num_loops == 0: dists = self.compute_distances_no_loops(X) elif num_loops == 1: dists = self.compute_distances_one_loop(X) elif num_loops == 2: dists = self.compute_distances_two_loops(X) else: raise ValueError('Invalid value %d for num_loops' % num_loops) return self.predict_labels(dists, k=k) def compute_distances_two_loops(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using a nested loop over both the training data and the test data. Inputs: - X: A numpy array of shape (num_test, D) containing test data. Returns: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] is the Euclidean distance between the ith test point and the jth training point. """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): for j in xrange(num_train): ############################################################### # TODO: # # Compute the l2 distance between the ith test point and the jth # # training point, and store the result in dists[i, j]. You should # # not use a loop over dimension. # ############################################################### dists[i, j] = np.sqrt(np.sum((X[i, :] - self.X_train[j, :]) ** 2)) ############################################################### # END OF YOUR CODE # ############################################################### return dists def compute_distances_one_loop(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using a single loop over the test data. Input / Output: Same as compute_distances_two_loops """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): ################################################################### # TODO: # # Compute the l2 distance between the ith test point and all training # # points, and store the result in dists[i, :]. # ################################################################### dists[i, :] = np.sqrt(np.sum(np.square(X[i, :] - self.X_train), axis=1)) ################################################################### # END OF YOUR CODE # ################################################################### return dists def compute_distances_no_loops(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using no explicit loops. Input / Output: Same as compute_distances_two_loops """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) ####################################################################### # TODO: # # Compute the l2 distance between all test points and all training # # points without using any explicit loops, and store the result in # # dists. # # # # You should implement this function using only basic array operations; # # in particular you should not use functions from scipy. # # # # HINT: Try to formulate the l2 distance using matrix multiplication # # and two broadcast sums. # ####################################################################### dists = np.sqrt(np.multiply(np.dot(X, self.X_train.T), -2) + np.sum(self.X_train ** 2, axis=1) + np.sum(X ** 2, axis=1)[:, np.newaxis]) ####################################################################### # END OF YOUR CODE # ####################################################################### return dists def predict_labels(self, dists, k=1): """ Given a matrix of distances between test points and training points, predict a label for each test point. Inputs: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] gives the distance betwen the ith test point and the jth training point. Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. """ num_test = dists.shape[0] y_pred = np.zeros(num_test) for i in xrange(num_test): # A list of length k storing the labels of the k nearest neighbors to # the ith test point. closest_y = [] ################################################################### # TODO: # # Use the distance matrix to find the k nearest neighbors of the ith # # testing point, and use self.y_train to find the labels of these # # neighbors. Store these labels in closest_y. # # Hint: Look up the function numpy.argsort. # ################################################################### k_nearest_index = np.argsort(dists[i, :])[:k] ################################################################### # TODO: # # Now that you have found the labels of the k nearest neighbors, you # # need to find the most common label in the list closest_y of labels. # # Store this label in y_pred[i]. Break ties by choosing the smaller # # label. # ################################################################### closest_y = self.y_train[k_nearest_index] labels_counts = &#123;&#125; for label in closest_y: if label in labels_counts.keys(): labels_counts[label] += 1 else: labels_counts[label] = 0 sorted_labels_counts = sorted( labels_counts.items(), key=operator.itemgetter(1), reverse=True) y_pred[i] = sorted_labels_counts[0][0] ################################################################### # END OF YOUR CODE # ################################################################### return y_pred]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS231n Lecture3 note]]></title>
      <url>%2F2017%2F03%2F02%2FCS231n-Lecture3-note%2F</url>
      <content type="text"><![CDATA[Loss functionSource is here.Multiclass Support Vector Machine lossThe SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$The Multiclass SVM loss for the i-th example is formalized as follows:Example$s = [13, -7, 11]$ and $\Delta = 10$, then,L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)In summary, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by at least by $\Delta$ (delta). If this is not the case, we will accumulate loss.Note that $f(x_i; W) = W x_i$, so we can also rewrite the loss function in this equivalent form:L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)A last piece of terminology we’ll mention before we finish with this section is that the threshold at zero $\max(0,−)$ function is often called the hinge loss. You’ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form $\max(0,−)^2$ that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation.The follow image shows the motivation of the SVM loss function”RegularizationThere is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters W that correctly classify every example (i.e. all scores are so that all the margins are met, and $L_i=0$ for all i). The issue is that this set of W is not necessarily unique: there might be many similar W that correctly classify the examples. One easy way to see this is that if some parameters W correctly classify all examples (so loss is zero for each example), then any multiple of these parameters $\lambda W$ where $\lambda &gt; 1$ will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30.We can avoid this by extending the loss function with a regularization penalty $R(W)$. The most common regularization penalty is the L2 norm:R(W) = \sum_k\sum_l W_{k,l}^2That is, the full Multiclass SVM loss becomes:L = \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\Or expanding this out in its full form:L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2Including the L2 penalty leads to the appealing max margin property in SVMs (See CS229 lecture notes for full details if you are interested).The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. For example, suppose that we have some input vector $x=[1,1,1,1]$ and two weight vectors $w_1=[1,0,0,0]$, $w_2=[0.25,0.25,0.25,0.25]$. Then $w^T_1x=w^T_2x=1$ so both weight vectors lead to the same dot product, but the L2 penalty of $w_1$ is 1.0 while the L2 penalty of $w_2$ is only 0.25. Therefore, according to the L2 penalty the weight vector $w_2$ would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in $w_2$ are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting.Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546def L_i(x, y, W): """ unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) """ delta = 1.0 # see notes about delta later in this section scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class correct_class_score = scores[y] D = W.shape[0] # number of classes, e.g. 10 loss_i = 0.0 for j in xrange(D): # iterate over all wrong classes if j == y: # skip for the true class to only loop over incorrect classes continue # accumulate loss for the i-th example loss_i += max(0, scores[j] - correct_class_score + delta) return loss_idef L_i_vectorized(x, y, W): """ A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) """ delta = 1.0 scores = W.dot(x) # compute the margins for all classes in one vector operation margins = np.maximum(0, scores - scores[y] + delta) # on y-th position scores[y] - scores[y] canceled and gave delta. We want # to ignore the y-th position and only consider margin on max wrong class margins[y] = 0 loss_i = np.sum(margins) return loss_idef L(X, y, W): """ fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) """ # evaluate loss over all examples in X without using any for loops # left as exercise to reader in the assignmentPractice ConsiderationsSetting DeltaNote that we brushed over the hyperparameter $\Delta$ and its setting. What value should it be set to, and do we have to cross-validate it? It turns out that this hyperparameter can safely be set to $\Delta = 1.0$ in all cases. The hyperparameters $\Delta$ and $\lambda$ seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights W has direct effect on the scores (and hence also their differences): As we shrink all values inside W the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. $\Delta = 10$, or$\Delta = 100$) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength $\lambda$).Softmax classifierIn the Softmax classifier, the function mapping $f(x_i; W) = W x_i$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form:L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}The function $f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}$ is called the softmax function.Information theory viewThe cross-entropy between a “true” distribution $p$ and an estimated distribution $q$ is defined as:H(p,q) = - \sum_x p(x) \log q(x)The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ( $q = e^{f_{y_i}} / \sum_j e^{f_j}$ as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. $p = [0, \ldots 1, \ldots, 0]$ contains a single 1 at the $y_i$ -th position.).Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q) = H(p) + D_{KL}(p||q)$, and the entropy of the delta function $p$ is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer.Probabilistic interpretationP(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }can be interpreted as the (normalized) probability assigned to the correct label $y_i$given the image $x_i$ and parameterized by W. To see this, remember that the Softmax classifier interprets the scores inside the output vector $f$ as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE). A nice feature of this view is that we can now also interpret the regularization term $R(W)$ in the full loss function as coming from a Gaussian prior over the weight matrix W, where instead of MLE we are performing the Maximum a posteriori (MAP) estimation.Numeric stabilityWhen you’re writing code for computing the Softmax function in practice, the intermediate term $e^{f_{y_i}}$ and $\sum_j e^{f_j}$ may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant C and push it into the sum, we get the following (mathematically equivalent) expression:\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}We are free to choose the value of C. This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for C is to set $\log C = -\max_j f_j$. This simply states that we should shift the values inside the vector ff so that the highest value is zero. In code:123456f = np.array([123, 456, 789]) # example with 3 classes and each having large scoresp = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup# instead: first shift the values of f so that the highest number is 0:f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answerSome tricksHow peaky or diffuse these probabilities are depends directly on the regularization strength $\lambda$ - which you are in charge of as input to the system. For example, suppose that the unnormalized log-probabilities for some three classes come out to be [1, -2, 0]. The softmax function would then compute:[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]Where the steps taken are to exponentiate and normalize to sum to one. Now, if the regularization strength $\lambda$ was higher, the weights W would be penalized more and this would lead to smaller weights. For example, suppose that the weights became one half smaller ([0.5, -1, 0]). The softmax would now compute:[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]where the probabilites are now more diffuse.Futher ReadingDeep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.OptimizationStrategy #1: A first very bad idea solution: Random search12345678910111213141516171819202122# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)# assume Y_train are the labels (e.g. 1D array of 50,000)# assume the function L evaluates the loss functionbestloss = float("inf") # Python assigns the highest possible float valuefor num in xrange(1000): W = np.random.randn(10, 3073) * 0.0001 # generate random parameters loss = L(X_train, Y_train, W) # get the loss over the entire training set if loss &lt; bestloss: # keep track of the best solution bestloss = loss bestW = W print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)# prints:# in attempt 0 the loss was 9.401632, best 9.401632# in attempt 1 the loss was 8.959668, best 8.959668# in attempt 2 the loss was 9.044034, best 8.959668# in attempt 3 the loss was 9.278948, best 8.959668# in attempt 4 the loss was 8.857370, best 8.857370# in attempt 5 the loss was 8.943151, best 8.857370# in attempt 6 the loss was 8.605604, best 8.605604# ... (trunctated: continues for 1000 lines)We can take the best weights W found by this search and try it out on the test set:1234567# Assume X_test is [3073 x 10000], Y_test [10000 x 1]scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples# find the index with max score in each column (the predicted class)Yte_predict = np.argmax(scores, axis = 0)# and calculate accuracy (fraction of predictions that are correct)np.mean(Yte_predict == Yte)# returns 0.1555With the best W this gives an accuracy of about 15.5%.Core idea: iterative refinement. Of course, it turns out that we can do much better. The core idea is that finding the best set of weights W is a very difficult or even impossible problem (especially once W contains weights for entire complex neural networks), but the problem of refining a specific set of weights W to be slightly better is significantly less difficult. In other words, our approach will be to start with a random W and then iteratively refine it, making it slightly better each time.Our strategy will be to start with random weights and iteratively refine them over time to get lower lossStrategy #2: Random Local SearchConcretely, we will start out with a random WW, generate random perturbations $\delta W$ to it and if the loss at the perturbed $W + \delta W$ is lower, we will perform an update. The code for this procedure is as follows:12345678910W = np.random.randn(10, 3073) * 0.001 # generate random starting Wbestloss = float("inf")for i in xrange(1000): step_size = 0.0001 Wtry = W + np.random.randn(10, 3073) * step_size loss = L(Xtr_cols, Ytr, Wtry) if loss &lt; bestloss: W = Wtry bestloss = loss print 'iter %d loss is %f' % (i, bestloss)This approach achieves test set classification accuracy of 21.4%.Strategy #3: Following the GradientThe mathematical expression for the derivative of a 1-D function with respect its input is:\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension.Computing the gradientThere are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone way that requires calculus (analytic gradient). We will now present both.Computing the gradient numerically with finite differences123456789101112131415161718192021222324252627def eval_numerical_gradient(f, x): """ a naive implementation of numerical gradient of f at x - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at """ fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.00001 # iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # evaluate function at x+h ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh = f(x) # evalute f(x + h) x[ix] = old_value # restore to previous value (very important!) # compute the partial derivative grad[ix] = (fxh - fx) / h # the slope it.iternext() # step to next dimension return gradPractical considerations.Note that in the mathematical formulation the gradient is defined in the limit as h goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the centered difference formula:[f(x+h) - f(x-h)] / 2 hSee wiki for details.1234567# to use the generic code above we want a function that takes a single argument# (the weights in our case) so we close over X_train and Y_traindef CIFAR10_loss_fun(W): return L(X_train, Y_train, W)W = np.random.rand(10, 3073) * 0.001 # random weight vectordf = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradientThen we can use to make an update:12345678910111213141516171819202122loss_original = CIFAR10_loss_fun(W) # the original lossprint 'original loss: %f' % (loss_original, )# lets see the effect of multiple step sizesfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]: step_size = 10 ** step_size_log W_new = W - step_size * df # new position in the weight space loss_new = CIFAR10_loss_fun(W_new) print 'for step size %f new loss: %f' % (step_size, loss_new)# prints:# original loss: 2.200718# for step size 1.000000e-10 new loss: 2.200652# for step size 1.000000e-09 new loss: 2.200057# for step size 1.000000e-08 new loss: 2.194116# for step size 1.000000e-07 new loss: 2.135493# for step size 1.000000e-06 new loss: 1.647802# for step size 1.000000e-05 new loss: 2.844355# for step size 1.000000e-04 new loss: 25.558142# for step size 1.000000e-03 new loss: 254.086573# for step size 1.000000e-02 new loss: 2539.370888# for step size 1.000000e-01 new loss: 25392.214036Effect of step sizeComputing the gradient analytically with CalculusHowever, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a gradient check.Lets use the example of the SVM loss function for a single datapoint:L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) \right) x_iwhen you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector $x_i$ scaled by this number is the gradient.\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta > 0) x_i]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note Ch07]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-Ch07%2F</url>
      <content type="text"><![CDATA[数据规整化：清理、转换、合并、重塑1234567891011121314from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandasimport pandas as pdnp.set_printoptions(precision=4, threshold=500)pd.options.display.max_rows = 100from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inline合并数据集数据库风格的DataFrame合并12345df1 = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)&#125;)df2 = DataFrame(&#123;'key': ['a', 'b', 'd'], 'data2': range(3)&#125;)df1data1key00b11b22a33c44a55a66b1df2data2key00a11b22d默认情况下根据重叠的列名进行合并1pd.merge(df1, df2)data1keydata200b111b126b132a044a055a0最好进行显式地指定1pd.merge(df1, df2, on='key')data1keydata200b111b126b132a044a055a01234df3 = DataFrame(&#123;'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)&#125;)df4 = DataFrame(&#123;'rkey': ['a', 'b', 'd'], 'data2': range(3)&#125;)1df3data1lkey00b11b22a33c44a55a66b1df4data2rkey00a11b22d如果两个对象的列名不同，那么就需要分别指定1pd.merge(df3, df4, left_on='lkey', right_on='rkey')data1lkeydata2rkey00b1b11b1b26b1b32a0a44a0a55a0a默认是进行inner连接（交集）， outer是求取并集1pd.merge(df1, df2, how='outer')data1keydata200.0b1.011.0b1.026.0b1.032.0a0.044.0a0.055.0a0.063.0cNaN7NaNd2.01234df1 = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)&#125;)df2 = DataFrame(&#123;'key': ['a', 'b', 'a', 'b', 'd'], 'data2': range(5)&#125;)1df1data1key00b11b22a33c44a55b1df2data2key00a11b22a33b44d1pd.merge(df1, df2, on='key', how='left')data1keydata200b1.010b3.021b1.031b3.042a0.052a2.063cNaN74a0.084a2.095b1.0105b3.01pd.merge(df1, df2, how='inner')data1keydata200b110b321b131b345b155b362a072a284a094a2123456left = DataFrame(&#123;'key1': ['foo', 'foo', 'bar'], 'key2': ['one', 'two', 'one'], 'lval': [1, 2, 3]&#125;)right = DataFrame(&#123;'key1': ['foo', 'foo', 'bar', 'bar'], 'key2': ['one', 'one', 'one', 'two'], 'rval': [4, 5, 6, 7]&#125;)121leftkey1key2lval0fooone11footwo22barone31rightkey1key2rval0fooone41fooone52barone63bartwo71pd.merge(left, right, on=['key1', 'key2'], how='outer')key1key2lvalrval0fooone1.04.01fooone1.05.02footwo2.0NaN3barone3.06.04bartwoNaN7.0列名重复问题1pd.merge(left, right, on='key1')key1key2_xlvalkey2_yrval0fooone1one41fooone1one52footwo2one43footwo2one54barone3one65barone3two71pd.merge(left, right, on='key1', suffixes=('_left', '_right'))key1key2_leftlvalkey2_rightrval0fooone1one41fooone1one52footwo2one43footwo2one54barone3one65barone3two7索引上的合并123left1 = DataFrame(&#123;'key': ['a', 'b', 'a', 'a', 'b', 'c'], 'value': range(6)&#125;)right1 = DataFrame(&#123;'group_val': [3.5, 7]&#125;, index=['a', 'b'])1left1keyvalue0a01b12a23a34b45c51right1group_vala3.5b7.01pd.merge(left1, right1, left_on='key', right_index=True)keyvaluegroup_val0a03.52a23.53a33.51b17.04b47.01pd.merge(left1, right1, left_on='key', right_index=True, how='outer')keyvaluegroup_val0a03.52a23.53a33.51b17.04b47.05c5NaN12345678lefth = DataFrame(&#123;'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'key2': [2000, 2001, 2002, 2001, 2002], 'data': np.arange(5.)&#125;)righth = DataFrame(np.arange(12).reshape((6, 2)), index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'], [2001, 2000, 2000, 2000, 2001, 2002]], columns=['event1', 'event2'])lefthdatakey1key200.0Ohio200011.0Ohio200122.0Ohio200233.0Nevada200144.0Nevada20021righthevent1event2Nevada200101200023Ohio200045200067200189200210111pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)datakey1key2event1event200.0Ohio20004500.0Ohio20006711.0Ohio20018922.0Ohio2002101133.0Nevada20010112pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True, how='outer')datakey1key2event1event200.0Ohio2000.04.05.000.0Ohio2000.06.07.011.0Ohio2001.08.09.022.0Ohio2002.010.011.033.0Nevada2001.00.01.044.0Nevada2002.0NaNNaN4NaNNevada2000.02.03.01234left2 = DataFrame([[1., 2.], [3., 4.], [5., 6.]], index=['a', 'c', 'e'], columns=['Ohio', 'Nevada'])right2 = DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]], index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])1left2OhioNevadaa1.02.0c3.04.0e5.06.01right2MissouriAlabamab7.08.0c9.010.0d11.012.0e13.014.01pd.merge(left2, right2, how='outer', left_index=True, right_index=True)OhioNevadaMissouriAlabamaa1.02.0NaNNaNbNaNNaN7.08.0c3.04.09.010.0dNaNNaN11.012.0e5.06.013.014.01left2.join(right2, how='outer')OhioNevadaMissouriAlabamaa1.02.0NaNNaNbNaNNaN7.08.0c3.04.09.010.0dNaNNaN11.012.0e5.06.013.014.01left1.join(right1, on='key')keyvaluegroup_val0a03.51b17.02a23.53a33.54b47.05c5NaN12another = DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]], index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])New YorkOregona7.08.0c9.010.0e11.012.0f16.017.0相当于三个表进行合并1234left2right2anotherleft2.join([right2, another])OhioNevadaa1.02.0c3.04.0e5.06.0MissouriAlabamab7.08.0c9.010.0d11.012.0e13.014.0New YorkOregona7.08.0c9.010.0e11.012.0f16.017.0OhioNevadaMissouriAlabamaNew YorkOregona1.02.0NaNNaN7.08.0c3.04.09.010.09.010.0e5.06.013.014.011.012.01left2.join([right2, another], how='outer')轴向连接之前指的都是行级别的连接操作1arr = np.arange(12).reshape((3, 4))1arrarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) 1np.concatenate([arr, arr], axis=1)array([[ 0, 1, 2, 3, 0, 1, 2, 3], [ 4, 5, 6, 7, 4, 5, 6, 7], [ 8, 9, 10, 11, 8, 9, 10, 11]]) 123s1 = Series([0, 1], index=['a', 'b'])s2 = Series([2, 3, 4], index=['c', 'd', 'e'])s3 = Series([5, 6], index=['f', 'g'])1pd.concat([s1, s2, s3])a 0 b 1 c 2 d 3 e 4 f 5 g 6 dtype: int64 1pd.concat([s1, s2, s3], axis=1)012a0.0NaNNaNb1.0NaNNaNcNaN2.0NaNdNaN3.0NaNeNaN4.0NaNfNaNNaN5.0gNaNNaN6.012s4 = pd.concat([s1 * 5, s3])s4a 0 b 5 f 5 g 6 dtype: int64 1pd.concat([s1, s4], axis=1)01a0.00b1.05fNaN5gNaN61pd.concat([s1, s4], axis=1, join='inner')01a00b151pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])01a0.00.0cNaNNaNb1.05.0eNaNNaN在连接轴上建立一个层次化索引123s1s3result = pd.concat([s1, s1, s3], keys=['one', 'two', 'three'])a 0 b 1 dtype: int64 f 5 g 6 dtype: int64 1resultone a 0 b 1 two a 0 b 1 three f 5 g 6 dtype: int64 12# Much more on the unstack function laterresult.unstack()abfgone0.01.0NaNNaNtwo0.01.0NaNNaNthreeNaNNaN5.06.01pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three'])onetwothreea0.0NaNNaNb1.0NaNNaNcNaN2.0NaNdNaN3.0NaNeNaN4.0NaNfNaNNaN5.0gNaNNaN6.012345678df1 = DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'], columns=['one', 'two'])df2 = DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'], columns=['three', 'four'])df1df2pd.concat([df1, df2], keys=['level1', 'level2'])pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])onetwoa01b23c45threefoura56c78fouronethreetwolevel1aNaN0.0NaN1.0bNaN2.0NaN3.0cNaN4.0NaN5.0level2a6.0NaN5.0NaNc8.0NaN7.0NaNlevel1level2onetwothreefoura015.06.0b23NaNNaNc457.08.012pd.concat(&#123;'level1': df1, 'level2': df2&#125;, axis=0)pd.concat(&#123;'level1': df1, 'level2': df2&#125;, axis=1)fouronethreetwolevel1aNaN0.0NaN1.0bNaN2.0NaN3.0cNaN4.0NaN5.0level2a6.0NaN5.0NaNc8.0NaN7.0NaNlevel1level2onetwothreefoura015.06.0b23NaNNaNc457.08.012pd.concat([df1, df2], axis=1, keys=['level1', 'level2'], names=['upper', 'lower'])upperlevel1level2loweronetwothreefoura015.06.0b23NaNNaNc457.08.012df1 = DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])df2 = DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])1df1abcd0-0.2047080.478943-0.519439-0.55573011.9657811.3934060.0929080.28174620.7690231.2464351.007189-1.2962211df2bda00.2749920.2289131.35291710.886429-2.001637-0.371843去除无关的行索引12pd.concat([df1, df2], ignore_index=False)pd.concat([df1, df2], ignore_index=True)abcd0-0.2047080.478943-0.519439-0.55573011.9657811.3934060.0929080.28174620.7690231.2464351.007189-1.29622101.3529170.274992NaN0.2289131-0.3718430.886429NaN-2.001637abcd0-0.2047080.478943-0.519439-0.55573011.9657811.3934060.0929080.28174620.7690231.2464351.007189-1.29622131.3529170.274992NaN0.2289134-0.3718430.886429NaN-2.001637合并重叠数据12345a = Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan], index=['f', 'e', 'd', 'c', 'b', 'a'])b = Series(np.arange(len(a), dtype=np.float64), index=['f', 'e', 'd', 'c', 'b', 'a'])b[-1] = np.nan1af NaN e 2.5 d NaN c 3.5 b 4.5 a NaN dtype: float64 1bf 0.0 e 1.0 d 2.0 c 3.0 b 4.0 a NaN dtype: float64 1np.where(pd.isnull(a), b, a)array([ 0. , 2.5, 2. , 3.5, 4.5, nan]) combine_first, 重叠值合并，且进行数据对其1b[:-2].combine_first(a[2:])a NaN b 4.5 c 3.0 d 2.0 e 1.0 f 0.0 dtype: float64 12345678df1 = DataFrame(&#123;'a': [1., np.nan, 5., np.nan], 'b': [np.nan, 2., np.nan, 6.], 'c': range(2, 18, 4)&#125;)df2 = DataFrame(&#123;'a': [5., 4., np.nan, 3., 7.], 'b': [np.nan, 3., 4., 6., 8.]&#125;)df1df2df1.combine_first(df2)abc01.0NaN21NaN2.0625.0NaN103NaN6.014ab05.0NaN14.03.02NaN4.033.06.047.08.0abc01.0NaN2.014.02.06.025.04.010.033.06.014.047.08.0NaN重塑和轴向旋转重塑层次化索引1234data = DataFrame(np.arange(6).reshape((2, 3)), index=pd.Index(['Ohio', 'Colorado'], name='state'), columns=pd.Index(['one', 'two', 'three'], name='number'))datanumberonetwothreestateOhio012Colorado345stack将列旋转为行12result = data.stack()resultstate number Ohio one 0 two 1 three 2 Colorado one 3 two 4 three 5 dtype: int32 unstack将行旋转为列，默认操作最内层1result.unstack()numberonetwothreestateOhio012Colorado3451result.unstack(0)stateOhioColoradonumberone03two14three251result.unstack('state')stateOhioColoradonumberone03two14three251234567s1 = Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])s2 = Series([4, 5, 6], index=['c', 'd', 'e'])s1s2data2 = pd.concat([s1, s2], keys=['one', 'two'])data2data2.unstack()a 0 b 1 c 2 d 3 dtype: int64 c 4 d 5 e 6 dtype: int64 one a 0 b 1 c 2 d 3 two c 4 d 5 e 6 dtype: int64 abcdeone0.01.02.03.0NaNtwoNaNNaN4.05.06.0stack默认会滤除缺失值，因此两者可逆1data2.unstack().stack()one a 0.0 b 1.0 c 2.0 d 3.0 two c 4.0 d 5.0 e 6.0 dtype: float64 1data2.unstack().stack(dropna=False)one a 0.0 b 1.0 c 2.0 d 3.0 e NaN two a NaN b NaN c 4.0 d 5.0 e 6.0 dtype: float64 1234resultdf = DataFrame(&#123;'left': result, 'right': result + 5&#125;, columns=pd.Index(['left', 'right'], name='side'))dfstate number Ohio one 0 two 1 three 2 Colorado one 3 two 4 three 5 dtype: int32 sideleftrightstatenumberOhioone05two16three27Coloradoone38two49three510DataFrame作为旋转轴的级别将成为结果中的最低级别（axis=MAX）1df.unstack('state')sideleftrightstateOhioColoradoOhioColoradonumberone0358two1469three25710stack操作将axis-1?1df.unstack('state').stack('side')stateOhioColoradonumbersideoneleft03right58twoleft14right69threeleft25right71012df.unstack('state')df.unstack('state').stack('state')sideleftrightstateOhioColoradoOhioColoradonumberone0358two1469three25710sideleftrightnumberstateoneOhio05Colorado38twoOhio16Colorado49threeOhio27Colorado510将长格式旋转为宽格式12345678910111213data = pd.read_csv('ch07/macrodata.csv')data[:10]periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name='date')periods[:10]data = DataFrame(data.to_records(), columns=pd.Index(['realgdp', 'infl', 'unemp'], name='item'), index=periods.to_timestamp('D', 'end'))data[:10]data.to_records()[:10]data.stack()[:10]ldata = data.stack().reset_index().rename(columns=&#123;0: 'value'&#125;)wdata = ldata.pivot('date', 'item', 'value')yearquarterrealgdprealconsrealinvrealgovtrealdpicpim1tbilrateunemppopinflrealint01959.01.02710.3491707.4286.898470.0451886.928.98139.72.825.8177.1460.000.0011959.02.02778.8011733.7310.859481.3011919.729.15141.73.085.1177.8302.340.7421959.03.02775.4881751.8289.226491.2601916.429.35140.53.825.3178.6572.741.0931959.04.02785.2041753.7299.356484.0521931.329.37140.04.335.6179.3860.274.0641960.01.02847.6991770.5331.722462.1991955.529.54139.63.505.2180.0072.311.1951960.02.02834.3901792.9298.152460.4001966.129.55140.22.685.2180.6710.142.5561960.03.02839.0221785.8296.375474.6761967.829.75140.92.365.6181.5282.70-0.3471960.04.02802.6161788.2259.764476.4341966.629.84141.12.296.3182.2871.211.0881961.01.02819.2641787.7266.405475.8541984.529.81142.12.376.8182.992-0.402.7791961.02.02872.0051814.3286.246480.3282014.429.92142.92.297.0183.6911.470.81PeriodIndex([&#39;1959Q1&#39;, &#39;1959Q2&#39;, &#39;1959Q3&#39;, &#39;1959Q4&#39;, &#39;1960Q1&#39;, &#39;1960Q2&#39;, &#39;1960Q3&#39;, &#39;1960Q4&#39;, &#39;1961Q1&#39;, &#39;1961Q2&#39;], dtype=&#39;int64&#39;, name=&#39;date&#39;, freq=&#39;Q-DEC&#39;) itemrealgdpinflunempdate1959-03-312710.3490.005.81959-06-302778.8012.345.11959-09-302775.4882.745.31959-12-312785.2040.275.61960-03-312847.6992.315.21960-06-302834.3900.145.21960-09-302839.0222.705.61960-12-312802.6161.216.31961-03-312819.264-0.406.81961-06-302872.0051.477.0rec.array([(datetime.datetime(1959, 3, 31, 0, 0), 2710.349, 0.0, 5.8), (datetime.datetime(1959, 6, 30, 0, 0), 2778.801, 2.34, 5.1), (datetime.datetime(1959, 9, 30, 0, 0), 2775.488, 2.74, 5.3), (datetime.datetime(1959, 12, 31, 0, 0), 2785.204, 0.27, 5.6), (datetime.datetime(1960, 3, 31, 0, 0), 2847.699, 2.31, 5.2), (datetime.datetime(1960, 6, 30, 0, 0), 2834.39, 0.14, 5.2), (datetime.datetime(1960, 9, 30, 0, 0), 2839.022, 2.7, 5.6), (datetime.datetime(1960, 12, 31, 0, 0), 2802.616, 1.21, 6.3), (datetime.datetime(1961, 3, 31, 0, 0), 2819.264, -0.4, 6.8), (datetime.datetime(1961, 6, 30, 0, 0), 2872.005, 1.47, 7.0)], dtype=[(&#39;date&#39;, &#39;O&#39;), (&#39;realgdp&#39;, &#39;&lt;f8&#39;), (&#39;infl&#39;, &#39;&lt;f8&#39;), (&#39;unemp&#39;, &#39;&lt;f8&#39;)]) date item 1959-03-31 realgdp 2710.349 infl 0.000 unemp 5.800 1959-06-30 realgdp 2778.801 infl 2.340 unemp 5.100 1959-09-30 realgdp 2775.488 infl 2.740 unemp 5.300 1959-12-31 realgdp 2785.204 dtype: float64 1ldata[:10]dateitemvalue01959-03-31realgdp2710.34911959-03-31infl0.00021959-03-31unemp5.80031959-06-30realgdp2778.80141959-06-30infl2.34051959-06-30unemp5.10061959-09-30realgdp2775.48871959-09-30infl2.74081959-09-30unemp5.30091959-12-31realgdp2785.20412pivoted = ldata.pivot('date', 'item', 'value')pivoted.head()iteminflrealgdpunempdate1959-03-310.002710.3495.81959-06-302.342778.8015.11959-09-302.742775.4885.31959-12-310.272785.2045.61960-03-312.312847.6995.212ldata['value2'] = np.random.randn(len(ldata))ldata[:10]dateitemvaluevalue201959-03-31realgdp2710.349-0.20470811959-03-31infl0.0000.47894321959-03-31unemp5.800-0.51943931959-06-30realgdp2778.801-0.55573041959-06-30infl2.3401.96578151959-06-30unemp5.1001.39340661959-09-30realgdp2775.4880.09290871959-09-30infl2.7400.28174681959-09-30unemp5.3000.76902391959-12-31realgdp2785.2041.24643512pivoted = ldata.pivot('date', 'item')pivoted[:5]valuevalue2iteminflrealgdpunempinflrealgdpunempdate1959-03-310.002710.3495.80.478943-0.204708-0.5194391959-06-302.342778.8015.11.965781-0.5557301.3934061959-09-302.742775.4885.30.2817460.0929080.7690231959-12-310.272785.2045.61.0071891.246435-1.2962211960-03-312.312847.6995.20.2289130.2749921.3529171pivoted['value'][:5]iteminflrealgdpunempdate1959-03-310.002710.3495.81959-06-302.342778.8015.11959-09-302.742775.4885.31959-12-310.272785.2045.61960-03-312.312847.6995.2123ldata.set_index(['date', 'item'])[:9]unstacked = ldata.set_index(['date', 'item']).unstack('item')unstacked[:7]valuevalue2dateitem1959-03-31realgdp2710.349-0.204708infl0.0000.478943unemp5.800-0.5194391959-06-30realgdp2778.801-0.555730infl2.3401.965781unemp5.1001.3934061959-09-30realgdp2775.4880.092908infl2.7400.281746unemp5.3000.769023valuevalue2iteminflrealgdpunempinflrealgdpunempdate1959-03-310.002710.3495.80.478943-0.204708-0.5194391959-06-302.342778.8015.11.965781-0.5557301.3934061959-09-302.742775.4885.30.2817460.0929080.7690231959-12-310.272785.2045.61.0071891.246435-1.2962211960-03-312.312847.6995.20.2289130.2749921.3529171960-06-300.142834.3905.2-2.0016370.886429-0.3718431960-09-302.702839.0225.6-0.4385701.669025-0.539741数据转换移除重复值123data = DataFrame(&#123;'k1': ['one'] * 3 + ['two'] * 4, 'k2': [1, 1, 2, 3, 3, 4, 4]&#125;)datak1k20one11one12one23two34two35two46two41data.duplicated()0 False 1 True 2 False 3 False 4 True 5 False 6 True dtype: bool 1data.drop_duplicates()k1k20one12one23two35two4123data['v1'] = range(7)datadata.drop_duplicates(['k1'])k1k2v10one101one112one223two334two345two456two46k1k2v10one103two3312data.drop_duplicates(['k1', 'k2'], keep='last')data.drop_duplicates(['k1', 'k2'], keep='first')k1k2v11one112one224two346two46k1k2v10one102one223two335two45利用函数或映射进行数据转换12345data = DataFrame(&#123;'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', 'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'], 'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]&#125;)datafoodounces0bacon4.01pulled pork3.02bacon12.03Pastrami6.04corned beef7.55Bacon8.06pastrami3.07honey ham5.08nova lox6.012345678meat_to_animal = &#123; 'bacon': 'pig', 'pulled pork': 'pig', 'pastrami': 'cow', 'corned beef': 'cow', 'honey ham': 'pig', 'nova lox': 'salmon'&#125;12data['animal'] = data['food'].map(str.lower).map(meat_to_animal)datafoodouncesanimal0bacon4.0pig1pulled pork3.0pig2bacon12.0pig3Pastrami6.0cow4corned beef7.5cow5Bacon8.0pig6pastrami3.0cow7honey ham5.0pig8nova lox6.0salmon1data['food'].map(lambda x: meat_to_animal[x.lower()])0 pig 1 pig 2 pig 3 cow 4 cow 5 pig 6 cow 7 pig 8 salmon Name: food, dtype: object 替换值12data = Series([1., -999., 2., -999., -1000., 3.])data0 1.0 1 -999.0 2 2.0 3 -999.0 4 -1000.0 5 3.0 dtype: float64 1data.replace(-999, np.nan)0 1.0 1 NaN 2 2.0 3 NaN 4 -1000.0 5 3.0 dtype: float64 1data.replace([-999, -1000], np.nan)0 1.0 1 NaN 2 2.0 3 NaN 4 NaN 5 3.0 dtype: float64 1data.replace([-999, -1000], [np.nan, 0])0 1.0 1 NaN 2 2.0 3 NaN 4 0.0 5 3.0 dtype: float64 1data.replace(&#123;-999: np.nan, -1000: 0&#125;)0 1.0 1 NaN 2 2.0 3 NaN 4 0.0 5 3.0 dtype: float64 重命名轴索引1234data = DataFrame(np.arange(12).reshape((3, 4)), index=['Ohio', 'Colorado', 'New York'], columns=['one', 'two', 'three', 'four'])dataonetwothreefourOhio0123Colorado4567New York8910111data.index.map(str.upper)array([&#39;OHIO&#39;, &#39;COLORADO&#39;, &#39;NEW YORK&#39;], dtype=object) 12data.index = data.index.map(str.upper)dataonetwothreefourOHIO0123COLORADO4567NEW YORK8910111data.rename(index=str.title, columns=str.upper)ONETWOTHREEFOUROhio0123Colorado4567New York89101112data.rename(index=&#123;'OHIO': 'INDIANA'&#125;, columns=&#123;'three': 'peekaboo'&#125;)onetwopeekaboofourINDIANA0123COLORADO4567NEW YORK891011123# Always returns a reference to a DataFrame_ = data.rename(index=&#123;'OHIO': 'INDIANA'&#125;, inplace=True)dataonetwothreefourINDIANA0123COLORADO4567NEW YORK891011离散化和面元划分1ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]123bins = [18, 25, 35, 60, 100]cats = pd.cut(ages, bins)cats[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]] Length: 12 Categories (4, object): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]] 1cats.codesarray([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8) 1cats.categoriesIndex([&#39;(18, 25]&#39;, &#39;(25, 35]&#39;, &#39;(35, 60]&#39;, &#39;(60, 100]&#39;], dtype=&#39;object&#39;) 1pd.value_counts(cats)(18, 25] 5 (35, 60] 3 (25, 35] 3 (60, 100] 1 dtype: int64 1pd.cut(ages, [18, 26, 36, 61, 100], right=False)[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)] Length: 12 Categories (4, object): [[18, 26) &lt; [26, 36) &lt; [36, 61) &lt; [61, 100)] 12group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']pd.cut(ages, bins, labels=group_names)[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult] Length: 12 Categories (4, object): [Youth &lt; YoungAdult &lt; MiddleAged &lt; Senior] 12data = np.random.rand(20)pd.cut(data, 4, precision=2)[(0.25, 0.49], (0.25, 0.49], (0.73, 0.98], (0.25, 0.49], (0.25, 0.49], ..., (0.25, 0.49], (0.73, 0.98], (0.49, 0.73], (0.49, 0.73], (0.49, 0.73]] Length: 20 Categories (4, object): [(0.0032, 0.25] &lt; (0.25, 0.49] &lt; (0.49, 0.73] &lt; (0.73, 0.98]] 123data = np.random.randn(1000) # Normally distributedcats = pd.qcut(data, 4) # Cut into quartilescats[(0.636, 3.26], [-3.745, -0.648], (0.636, 3.26], (-0.022, 0.636], (-0.648, -0.022], ..., (0.636, 3.26], (-0.022, 0.636], [-3.745, -0.648], (-0.022, 0.636], (-0.022, 0.636]] Length: 1000 Categories (4, object): [[-3.745, -0.648] &lt; (-0.648, -0.022] &lt; (-0.022, 0.636] &lt; (0.636, 3.26]] 1pd.value_counts(cats)(0.636, 3.26] 250 (-0.022, 0.636] 250 (-0.648, -0.022] 250 [-3.745, -0.648] 250 dtype: int64 1pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])[(-0.022, 1.298], [-3.745, -1.274], (-0.022, 1.298], (-0.022, 1.298], (-1.274, -0.022], ..., (-0.022, 1.298], (-0.022, 1.298], [-3.745, -1.274], (-0.022, 1.298], (-0.022, 1.298]] Length: 1000 Categories (4, object): [[-3.745, -1.274] &lt; (-1.274, -0.022] &lt; (-0.022, 1.298] &lt; (1.298, 3.26]] 检测和过滤异常值123np.random.seed(12345)data = DataFrame(np.random.randn(1000, 4))data.describe()0123count1000.0000001000.0000001000.0000001000.000000mean-0.0676840.0679240.025598-0.002298std0.9980350.9921061.0068350.996794min-3.428254-3.548824-3.184377-3.74535625%-0.774890-0.591841-0.641675-0.64414450%-0.1164010.1011430.002073-0.01361175%0.6163660.7802820.6803910.654328max3.3666262.6536563.2603833.92752812col = data[3]col[np.abs(col) &gt; 3]97 3.927528 305 -3.399312 400 -3.745356 Name: 3, dtype: float64 1data[(np.abs(data) &gt; 3).any(1)]01235-0.5397410.4769853.248944-1.02122897-0.7743630.5529360.1060613.927528102-0.655054-0.5652303.1768730.959533305-2.3155550.457246-0.025907-3.3993123240.0501881.9513123.2603830.9633014000.1463260.508391-0.196713-3.745356499-0.293333-0.242459-3.0569901.918403523-3.428254-0.296336-0.439938-0.8671655860.2751441.179227-3.1843771.369891808-0.362528-3.5488241.553205-2.1863019003.366626-2.3722140.8510101.33284612data[np.abs(data) &gt; 3] = np.sign(data) * 3data.describe()0123count1000.0000001000.0000001000.0000001000.000000mean-0.0676230.0684730.025153-0.002081std0.9954850.9902531.0039770.989736min-3.000000-3.000000-3.000000-3.00000025%-0.774890-0.591841-0.641675-0.64414450%-0.1164010.1011430.002073-0.01361175%0.6163660.7802820.6803910.654328max3.0000002.6536563.0000003.000000排列和随机采样123df = DataFrame(np.arange(5 * 4).reshape((5, 4)))sampler = np.random.permutation(5)samplerarray([1, 0, 2, 3, 4]) 1df0123001231456728910113121314154161718191df.take(sampler)0123145670012328910113121314154161718191df.take(np.random.permutation(len(df))[:3])0123145670012341617181912bag = np.array([5, 7, -1, 6, 4])sampler = np.random.randint(0, len(bag), size=10)1samplerarray([3, 0, 4, 1, 1, 2, 3, 0, 1, 2]) 12draws = bag.take(sampler)drawsarray([ 6, 5, 4, 7, 7, -1, 6, 5, 7, -1]) 计算指标 / 哑变量1234df = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)&#125;)dfpd.get_dummies(df['key'])data1key00b11b22a33c44a55babc00.01.00.010.01.00.021.00.00.030.00.01.041.00.00.050.01.00.01234dummies = pd.get_dummies(df['key'], prefix='key')dummiesdf_with_dummy = df[['data1']].join(dummies)df_with_dummykey_akey_bkey_c00.01.00.010.01.00.021.00.00.030.00.01.041.00.00.050.01.00.0data1key_akey_bkey_c000.01.00.0110.01.00.0221.00.00.0330.00.01.0441.00.00.0550.01.00.01234mnames = ['movie_id', 'title', 'genres']movies = pd.read_table('ch02/movielens/movies.dat', sep='::', header=None, names=mnames)movies[:10]C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: ParserWarning: Falling back to the &#39;python&#39; engine because the &#39;c&#39; engine does not support regex separators (separators &gt; 1 char and different from &#39;\s+&#39; are interpreted as regex); you can avoid this warning by specifying engine=&#39;python&#39;. app.launch_new_instance() movie_idtitlegenres01Toy Story (1995)Animation|Children's|Comedy12Jumanji (1995)Adventure|Children's|Fantasy23Grumpier Old Men (1995)Comedy|Romance34Waiting to Exhale (1995)Comedy|Drama45Father of the Bride Part II (1995)Comedy56Heat (1995)Action|Crime|Thriller67Sabrina (1995)Comedy|Romance78Tom and Huck (1995)Adventure|Children's89Sudden Death (1995)Action910GoldenEye (1995)Action|Adventure|Thriller123genre_iter = (set(x.split('|')) for x in movies.genres)genres = sorted(set.union(*genre_iter))genres[&#39;Action&#39;, &#39;Adventure&#39;, &#39;Animation&#39;, &quot;Children&#39;s&quot;, &#39;Comedy&#39;, &#39;Crime&#39;, &#39;Documentary&#39;, &#39;Drama&#39;, &#39;Fantasy&#39;, &#39;Film-Noir&#39;, &#39;Horror&#39;, &#39;Musical&#39;, &#39;Mystery&#39;, &#39;Romance&#39;, &#39;Sci-Fi&#39;, &#39;Thriller&#39;, &#39;War&#39;, &#39;Western&#39;] 12dummies = DataFrame(np.zeros((len(movies), len(genres))), columns=genres)dummies[:10].ix[:, :5]ActionAdventureAnimationChildren'sComedy00.00.00.00.00.010.00.00.00.00.020.00.00.00.00.030.00.00.00.00.040.00.00.00.00.050.00.00.00.00.060.00.00.00.00.070.00.00.00.00.080.00.00.00.00.090.00.00.00.00.0123for i, gen in enumerate(movies.genres): dummies.ix[i, gen.split('|')] = 1dummies[:10].ix[:, :5]ActionAdventureAnimationChildren'sComedy00.00.01.01.01.010.01.00.01.00.020.00.00.00.01.030.00.00.00.01.040.00.00.00.01.051.00.00.00.00.060.00.00.00.01.070.01.00.01.00.081.00.00.00.00.091.01.00.00.00.012movies_windic = movies.join(dummies.add_prefix('Genre_'))movies_windic.ix[0]movie_id 1 title Toy Story (1995) genres Animation|Children&#39;s|Comedy Genre_Action 0 Genre_Adventure 0 Genre_Animation 1 Genre_Children&#39;s 1 Genre_Comedy 1 Genre_Crime 0 Genre_Documentary 0 Genre_Drama 0 Genre_Fantasy 0 Genre_Film-Noir 0 Genre_Horror 0 Genre_Musical 0 Genre_Mystery 0 Genre_Romance 0 Genre_Sci-Fi 0 Genre_Thriller 0 Genre_War 0 Genre_Western 0 Name: 0, dtype: object 1np.random.seed(12345)12values = np.random.rand(10)valuesarray([ 0.9296, 0.3164, 0.1839, 0.2046, 0.5677, 0.5955, 0.9645, 0.6532, 0.7489, 0.6536]) 12bins = [0, 0.2, 0.4, 0.6, 0.8, 1]pd.get_dummies(pd.cut(values, bins))(0, 0.2](0.2, 0.4](0.4, 0.6](0.6, 0.8](0.8, 1]00.00.00.00.01.010.01.00.00.00.021.00.00.00.00.030.01.00.00.00.040.00.01.00.00.050.00.01.00.00.060.00.00.00.01.070.00.00.01.00.080.00.00.01.00.090.00.00.01.00.0字符串操作字符串对象方法12val = 'a,b, guido'val.split(',')[&#39;a&#39;, &#39;b&#39;, &#39; guido&#39;] 12pieces = [x.strip() for x in val.split(',')]pieces[&#39;a&#39;, &#39;b&#39;, &#39;guido&#39;] 12first, second, third = piecesfirst + '::' + second + '::' + third&#39;a::b::guido&#39; Surprise :P1'::'.join(pieces)&#39;a::b::guido&#39; 1'guido' in valTrue 1val.index(',')1 1val.find(':')-1 1val.index(':')--------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-110-280f8b2856ce&gt; in &lt;module&gt;() ----&gt; 1 val.index(&#39;:&#39;) ValueError: substring not found 1val.count(',')2 1val.replace(',', '::')&#39;a::b:: guido&#39; 1val.replace(',', '')&#39;ab guido&#39; 正则表达式123import retext = "foo bar\t baz \tqux"re.split('\s+', text)[&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;, &#39;qux&#39;] 12regex = re.compile('\s+')regex.split(text)[&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;, &#39;qux&#39;] 1regex.findall(text)[&#39; &#39;, &#39;\t &#39;, &#39; \t&#39;] 123456789text = """Dave dave@google.comSteve steve@gmail.comRob rob@gmail.comRyan ryan@yahoo.com"""pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]&#123;2,4&#125;'# re.IGNORECASE makes the regex case-insensitiveregex = re.compile(pattern, flags=re.IGNORECASE)1regex.findall(text)[&#39;dave@google.com&#39;, &#39;steve@gmail.com&#39;, &#39;rob@gmail.com&#39;, &#39;ryan@yahoo.com&#39;] Search只返回第一项12m = regex.search(text)m&lt;_sre.SRE_Match object; span=(5, 20), match=&#39;dave@google.com&#39;&gt; 1text[m.start():m.end()]&#39;dave@google.com&#39; 只匹配出现在字符串开头的模式1print(regex.match(text))None ​替换1print(regex.sub('REDACTED', text))Dave REDACTED Steve REDACTED Rob REDACTED Ryan REDACTED ​12pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]&#123;2,4&#125;)'regex = re.compile(pattern, flags=re.IGNORECASE)12m = regex.match('wesm@bright.net')m.groups()(&#39;wesm&#39;, &#39;bright&#39;, &#39;net&#39;) 1regex.findall(text)[(&#39;dave&#39;, &#39;google&#39;, &#39;com&#39;), (&#39;steve&#39;, &#39;gmail&#39;, &#39;com&#39;), (&#39;rob&#39;, &#39;gmail&#39;, &#39;com&#39;), (&#39;ryan&#39;, &#39;yahoo&#39;, &#39;com&#39;)] 1print(regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text))Dave Username: dave, Domain: google, Suffix: com Steve Username: steve, Domain: gmail, Suffix: com Rob Username: rob, Domain: gmail, Suffix: com Ryan Username: ryan, Domain: yahoo, Suffix: com ​123456regex = re.compile(r""" (?P&lt;username&gt;[A-Z0-9._%+-]+) @ (?P&lt;domain&gt;[A-Z0-9.-]+) \. (?P&lt;suffix&gt;[A-Z]&#123;2,4&#125;)""", flags=re.IGNORECASE|re.VERBOSE)12m = regex.match('wesm@bright.net')m.groupdict(){&#39;domain&#39;: &#39;bright&#39;, &#39;suffix&#39;: &#39;net&#39;, &#39;username&#39;: &#39;wesm&#39;} pandas中矢量化的字符串函数123data = &#123;'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan&#125;data = Series(data)1dataDave dave@google.com Rob rob@gmail.com Steve steve@gmail.com Wes NaN dtype: object 1data.isnull()Dave False Rob False Steve False Wes True dtype: bool 1data.str.contains('gmail')Dave False Rob True Steve True Wes NaN dtype: object 1pattern&#39;([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})&#39; 1data.str.findall(pattern, flags=re.IGNORECASE)Dave [(dave, google, com)] Rob [(rob, gmail, com)] Steve [(steve, gmail, com)] Wes NaN dtype: object 12matches = data.str.match(pattern, flags=re.IGNORECASE)matchesC:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: In future versions of pandas, match will change to always return a bool indexer. if __name__ == &#39;__main__&#39;: Dave (dave, google, com) Rob (rob, gmail, com) Steve (steve, gmail, com) Wes NaN dtype: object 1matches.str.get(1)Dave google Rob gmail Steve gmail Wes NaN dtype: object 1matches.str[0]Dave dave Rob rob Steve steve Wes NaN dtype: object 1data.str[:5]Dave dave@ Rob rob@g Steve steve Wes NaN dtype: object Example: USDA Food Database123456789101112131415161718192021222324252627&#123; "id": 21441, "description": "KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,Wing, meat and skin with breading", "tags": ["KFC"], "manufacturer": "Kentucky Fried Chicken", "group": "Fast Foods", "portions": [ &#123; "amount": 1, "unit": "wing, with skin", "grams": 68.0 &#125;, ... ], "nutrients": [ &#123; "value": 20.8, "units": "g", "description": "Protein", "group": "Composition" &#125;, ... ]&#125;123import jsondb = json.load(open('ch07/foods-2011-10-03.json'))len(db)6636 1db[0].keys()dict_keys([&#39;id&#39;, &#39;tags&#39;, &#39;portions&#39;, &#39;nutrients&#39;, &#39;description&#39;, &#39;group&#39;, &#39;manufacturer&#39;]) 1db[0]['nutrients'][0]{&#39;description&#39;: &#39;Protein&#39;, &#39;group&#39;: &#39;Composition&#39;, &#39;units&#39;: &#39;g&#39;, &#39;value&#39;: 25.18} 12nutrients = DataFrame(db[0]['nutrients'])nutrients[:7]descriptiongroupunitsvalue0ProteinCompositiong25.181Total lipid (fat)Compositiong29.202Carbohydrate, by differenceCompositiong3.063AshOtherg3.284EnergyEnergykcal376.005WaterCompositiong39.286EnergyEnergykJ1573.0012info_keys = ['description', 'group', 'id', 'manufacturer']info = DataFrame(db, columns=info_keys)1info[:5]descriptiongroupidmanufacturer0Cheese, carawayDairy and Egg Products10081Cheese, cheddarDairy and Egg Products10092Cheese, edamDairy and Egg Products10183Cheese, fetaDairy and Egg Products10194Cheese, mozzarella, part skim milkDairy and Egg Products10281pd.value_counts(info.group)[:10]Vegetables and Vegetable Products 812 Beef Products 618 Baked Products 496 Breakfast Cereals 403 Legumes and Legume Products 365 Fast Foods 365 Lamb, Veal, and Game Products 345 Sweets 341 Pork Products 328 Fruits and Fruit Juices 328 Name: group, dtype: int64 12345678nutrients = []for rec in db: fnuts = DataFrame(rec['nutrients']) fnuts['id'] = rec['id'] nutrients.append(fnuts)nutrients = pd.concat(nutrients, ignore_index=True)1nutrients[:10]descriptiongroupunitsvalueid0ProteinCompositiong25.1810081Total lipid (fat)Compositiong29.2010082Carbohydrate, by differenceCompositiong3.0610083AshOtherg3.2810084EnergyEnergykcal376.0010085WaterCompositiong39.2810086EnergyEnergykJ1573.0010087Fiber, total dietaryCompositiong0.0010088Calcium, CaElementsmg673.0010089Iron, FeElementsmg0.6410081nutrients.duplicated().sum()14179 1nutrients = nutrients.drop_duplicates()1234col_mapping = &#123;'description' : 'food', 'group' : 'fgroup'&#125;info = info.rename(columns=col_mapping, copy=False)info[:10]foodfgroupidmanufacturer0Cheese, carawayDairy and Egg Products10081Cheese, cheddarDairy and Egg Products10092Cheese, edamDairy and Egg Products10183Cheese, fetaDairy and Egg Products10194Cheese, mozzarella, part skim milkDairy and Egg Products10285Cheese, mozzarella, part skim milk, low moistureDairy and Egg Products10296Cheese, romanoDairy and Egg Products10387Cheese, roquefortDairy and Egg Products10398Cheese spread, pasteurized process, american, ...Dairy and Egg Products10489Cream, fluid, half and halfDairy and Egg Products10491234col_mapping = &#123;'description' : 'nutrient', 'group' : 'nutgroup'&#125;nutrients = nutrients.rename(columns=col_mapping, copy=False)nutrients[:10]nutrientnutgroupunitsvalueid0ProteinCompositiong25.1810081Total lipid (fat)Compositiong29.2010082Carbohydrate, by differenceCompositiong3.0610083AshOtherg3.2810084EnergyEnergykcal376.0010085WaterCompositiong39.2810086EnergyEnergykJ1573.0010087Fiber, total dietaryCompositiong0.0010088Calcium, CaElementsmg673.0010089Iron, FeElementsmg0.6410081ndata = pd.merge(nutrients, info, on='id', how='outer')1ndata[:10]nutrientnutgroupunitsvalueidfoodfgroupmanufacturer0ProteinCompositiong25.181008Cheese, carawayDairy and Egg Products1Total lipid (fat)Compositiong29.201008Cheese, carawayDairy and Egg Products2Carbohydrate, by differenceCompositiong3.061008Cheese, carawayDairy and Egg Products3AshOtherg3.281008Cheese, carawayDairy and Egg Products4EnergyEnergykcal376.001008Cheese, carawayDairy and Egg Products5WaterCompositiong39.281008Cheese, carawayDairy and Egg Products6EnergyEnergykJ1573.001008Cheese, carawayDairy and Egg Products7Fiber, total dietaryCompositiong0.001008Cheese, carawayDairy and Egg Products8Calcium, CaElementsmg673.001008Cheese, carawayDairy and Egg Products9Iron, FeElementsmg0.641008Cheese, carawayDairy and Egg Products1ndata.ix[30000]nutrient Glycine nutgroup Amino Acids units g value 0.04 id 6158 food Soup, tomato bisque, canned, condensed fgroup Soups, Sauces, and Gravies manufacturer Name: 30000, dtype: object 123result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)result[:10]result['Zinc, Zn'].sort_values().plot(kind='barh')nutrient fgroup Adjusted Protein Sweets 12.900 Vegetables and Vegetable Products 2.180 Alanine Baby Foods 0.085 Baked Products 0.248 Beef Products 1.550 Beverages 0.003 Breakfast Cereals 0.311 Cereal Grains and Pasta 0.373 Dairy and Egg Products 0.271 Ethnic Foods 1.290 Name: value, dtype: float64 &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba08c9e780&gt; 123456789by_nutrient = ndata.groupby(['nutgroup', 'nutrient'])get_maximum = lambda x: x.xs(x.value.idxmax())get_minimum = lambda x: x.xs(x.value.idxmin())max_foods = by_nutrient.apply(get_maximum)[['value', 'food']]# make the food a little smallermax_foods.food = max_foods.food.str[:50]1max_foods.ix['Amino Acids']['food']nutrient Alanine Gelatins, dry powder, unsweetened Arginine Seeds, sesame flour, low-fat Aspartic acid Soy protein isolate Cystine Seeds, cottonseed flour, low fat (glandless) Glutamic acid Soy protein isolate Glycine Gelatins, dry powder, unsweetened Histidine Whale, beluga, meat, dried (Alaska Native) Hydroxyproline KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINA... Isoleucine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Leucine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Lysine Seal, bearded (Oogruk), meat, dried (Alaska Na... Methionine Fish, cod, Atlantic, dried and salted Phenylalanine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Proline Gelatins, dry powder, unsweetened Serine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Threonine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Tryptophan Sea lion, Steller, meat with fat (Alaska Native) Tyrosine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Valine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Name: food, dtype: object]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note Ch06]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-Ch06%2F</url>
      <content type="text"><![CDATA[Data loading, storage, and file formats12345678910111213from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport sysimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%pwd&#39;C:\\Users\\Ewan\\Downloads\\pydata-book-master&#39; Reading and Writing Data in Text Format1!more ch06\ex1.csva,b,c,d,message 1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo 12df = pd.read_csv('ch06/ex1.csv')dfabcdmessage01234hello15678world29101112foo1pd.read_table('ch06/ex1.csv', sep=',')abcdmessage01234hello15678world29101112foo1!more ch06\ex2.csv1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo 12pd.read_csv('ch06/ex2.csv', header=None)pd.read_csv('ch06/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])0123401234hello15678world29101112fooabcdmessage01234hello15678world29101112foo12names = ['a', 'b', 'c', 'd', 'message']pd.read_csv('ch06/ex2.csv', names=names, index_col='message')abcdmessagehello1234world5678foo9101112123!more ch06\csv_mindex.csvparsed = pd.read_csv('ch06/csv_mindex.csv', index_col=['key1', 'key2'])parsedkey1,key2,value1,value2 one,a,1,2 one,b,3,4 one,c,5,6 one,d,7,8 two,a,9,10 two,b,11,12 two,c,13,14 two,d,15,16 value1value2key1key2onea12b34c56d78twoa910b1112c1314d15161list(open('ch06/ex3.txt'))[&#39; A B C\n&#39;, &#39;aaa -0.264438 -1.026059 -0.619500\n&#39;, &#39;bbb 0.927272 0.302904 -0.032399\n&#39;, &#39;ccc -0.264273 -0.386314 -0.217601\n&#39;, &#39;ddd -0.871858 -0.348382 1.100491\n&#39;] 采用正则表达式作为分隔符12result = pd.read_table('ch06/ex3.txt', sep='\s+')resultABCaaa-0.264438-1.026059-0.619500bbb0.9272720.302904-0.032399ccc-0.264273-0.386314-0.217601ddd-0.871858-0.3483821.10049112!more ch06\ex4.csvpd.read_csv('ch06/ex4.csv', skiprows=[0, 2, 3])# hey! a,b,c,d,message # just wanted to make things more difficult for you # who reads CSV files with computers, anyway? 1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo abcdmessage01234hello15678world29101112foo1234!more ch06\ex5.csvresult = pd.read_csv('ch06/ex5.csv')resultpd.isnull(result)something,a,b,c,d,message one,1,2,3,4,NA two,5,6,,8,world three,9,10,11,12,foo somethingabcdmessage0one123.04NaN1two56NaN8world2three91011.012foosomethingabcdmessage0FalseFalseFalseFalseFalseTrue1FalseFalseFalseTrueFalseFalse2FalseFalseFalseFalseFalseFalse12result = pd.read_csv('ch06/ex5.csv', na_values=['NULL'])resultsomethingabcdmessage0one123.04NaN1two56NaN8world2three91011.012foo12sentinels = &#123;'message': ['foo', 'NA'], 'something': ['two']&#125;pd.read_csv('ch06/ex5.csv', na_values=sentinels)somethingabcdmessage0one123.04NaN1NaN56NaN8world2three91011.012NaN逐块读取文本文件12result = pd.read_csv('ch06/ex6.csv')resultonetwothreefourkey00.467976-0.038649-0.295344-1.824726L1-0.3588931.4044530.704965-0.200638B2-0.5018400.659254-0.421691-0.057688G30.2048861.0741341.388361-0.982404R40.354628-0.1331160.283763-0.837063Q51.8174800.7422730.419395-2.251035Q6-0.7767640.935518-0.332872-1.875641U7-0.9131351.530624-0.5726570.477252K80.358480-0.497572-0.3670160.507702S9-1.740877-1.160417-1.6378302.172201G100.240564-0.3282491.2521551.0727968110.7640181.165476-0.6395441.495258R120.571035-0.3105370.582437-0.2987651132.3176580.430710-1.3342160.199679P141.547771-1.119753-2.2776340.329586J15-1.3106080.401719-1.0009871.156708E16-0.0884960.6347120.1533240.415335B17-0.018663-0.247487-1.4465220.750938A18-0.070127-1.5790970.1208920.671432F19-0.194678-0.4920392.3596050.319810H20-0.2486180.868707-0.492226-0.717959W21-1.091549-0.867110-0.647760-0.832562C220.641404-0.138822-0.621963-0.284839C231.2164080.9926870.165162-0.069619V24-0.5644740.7928320.7470530.571675I251.759879-0.515666-0.2304811.362317S260.1262660.3092810.382820-0.239199L271.334360-0.100152-0.840731-0.643967628-0.7376200.278087-0.053235-0.950972J29-1.148486-0.986292-0.1449630.124362Y..................99700.633495-0.1865240.9276270.143164499710.308636-0.1128570.762842-1.07297719972-1.627051-0.9781510.154745-1.229037Z99730.3148470.0979890.1996080.955193P99741.6669070.9920050.496128-0.686391S99750.0106030.708540-1.2587110.226541K99760.118693-0.714455-0.501342-0.254764K99770.302616-2.011527-0.6280850.768827H9978-0.0985721.769086-0.215027-0.053076A9979-0.0190581.9649940.738538-0.883776F9980-0.5953490.001781-1.423355-1.458477M99811.392170-1.396560-1.425306-0.847535H9982-0.896029-0.1522871.9244830.36518469983-2.274642-0.9018741.5003520.996541N9984-0.3018981.0199061.1021602.624526I9985-2.548389-0.5853741.496201-0.718815D9986-0.0645880.759292-1.568415-0.420933E9987-0.143365-1.111760-1.8155810.43527429988-0.070412-1.0559210.338017-0.440763X99890.6491480.994273-1.3842270.485120Q9990-0.3707690.404356-1.051628-1.05089989991-0.4099800.155627-0.8189901.277350W99920.301214-1.1112030.6682580.671922A99931.8211170.4164450.1738740.505118X99940.0688041.3227590.8023460.223618H99952.311896-0.417070-1.409599-0.515821L9996-0.479893-0.6504190.745152-0.646038E99970.5233310.7871120.4860661.093156K9998-0.3625590.598894-1.8432010.887292G9999-0.096376-1.012999-0.657431-0.573315010000 rows × 5 columns1pd.read_csv('ch06/ex6.csv', nrows=5)onetwothreefourkey00.467976-0.038649-0.295344-1.824726L1-0.3588931.4044530.704965-0.200638B2-0.5018400.659254-0.421691-0.057688G30.2048861.0741341.388361-0.982404R40.354628-0.1331160.283763-0.837063Q12chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)chunker&lt;pandas.io.parsers.TextFileReader at 0x2035229de80&gt; 1234567chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)tot = Series([])for piece in chunker: tot = tot.add(piece['key'].value_counts(), fill_value=0)tot = tot.sort_values(ascending=False)1tot[:10]E 368.0 X 364.0 L 346.0 O 343.0 Q 340.0 M 338.0 J 337.0 F 335.0 K 334.0 H 330.0 dtype: float64 将数据写出到文本格式12data = pd.read_csv('ch06/ex5.csv')datasomethingabcdmessage0one123.04NaN1two56NaN8world2three91011.012foo12data.to_csv('ch06/out.csv')!more ch06\out.csv,something,a,b,c,d,message 0,one,1,2,3.0,4, 1,two,5,6,,8,world 2,three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, sep='|')|something|a|b|c|d|message 0|one|1|2|3.0|4| 1|two|5|6||8|world 2|three|9|10|11.0|12|foo 1data.to_csv(sys.stdout, na_rep='NULL'),something,a,b,c,d,message 0,one,1,2,3.0,4,NULL 1,two,5,6,NULL,8,world 2,three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, index=False, header=False)one,1,2,3.0,4, two,5,6,,8,world three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])a,b,c 1,2,3.0 5,6, 9,10,11.0 123456dates = pd.date_range('1/1/2000', periods=7)datests = Series(np.arange(7), index=dates)tsts.to_csv('ch06/tseries.csv')!more ch06\tseries.csvDatetimeIndex([&#39;2000-01-01&#39;, &#39;2000-01-02&#39;, &#39;2000-01-03&#39;, &#39;2000-01-04&#39;, &#39;2000-01-05&#39;, &#39;2000-01-06&#39;, &#39;2000-01-07&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) 2000-01-01 0 2000-01-02 1 2000-01-03 2 2000-01-04 3 2000-01-05 4 2000-01-06 5 2000-01-07 6 Freq: D, dtype: int32 2000-01-01,0 2000-01-02,1 2000-01-03,2 2000-01-04,3 2000-01-05,4 2000-01-06,5 2000-01-07,6 1Series.from_csv('ch06/tseries.csv', parse_dates=True)2000-01-01 0 2000-01-02 1 2000-01-03 2 2000-01-04 3 2000-01-05 4 2000-01-06 5 2000-01-07 6 dtype: int64 手动处理分隔符格式1!more ch06\ex7.csv&quot;a&quot;,&quot;b&quot;,&quot;c&quot; &quot;1&quot;,&quot;2&quot;,&quot;3&quot; &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot; 1234import csvf = open('ch06/ex7.csv')reader = csv.reader(f)12for line in reader: print(line)[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;] [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;] [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;] 12345678910lines = list(csv.reader(open('ch06/ex7.csv')))header, values = lines[0], lines[1:]for item in zip(*values): print(item) for h, v in zip(header, zip(*values)): print(h, v)data_dict = &#123;h: v for h, v in zip(header, zip(*values))&#125;data_dict(&#39;1&#39;, &#39;1&#39;) (&#39;2&#39;, &#39;2&#39;) (&#39;3&#39;, &#39;3&#39;) a (&#39;1&#39;, &#39;1&#39;) b (&#39;2&#39;, &#39;2&#39;) c (&#39;3&#39;, &#39;3&#39;) {&#39;a&#39;: (&#39;1&#39;, &#39;1&#39;), &#39;b&#39;: (&#39;2&#39;, &#39;2&#39;), &#39;c&#39;: (&#39;3&#39;, &#39;3&#39;)} 12345class my_dialect(csv.Dialect): lineterminator = '\n' delimiter = ';' quotechar = '"' quoting = csv.QUOTE_MINIMAL123456with open('mydata.csv', 'w') as f: writer = csv.writer(f, dialect=my_dialect) writer.writerow(('one', 'two', 'three')) writer.writerow(('1', '2', '3')) writer.writerow(('4', '5', '6')) writer.writerow(('7', '8', '9'))14 6 6 6 1!more mydata.csvone;two;three 1;2;3 4;5;6 7;8;9 JSON数据12345678obj = """&#123;"name": "Wes", "places_lived": ["United States", "Spain", "Germany"], "pet": null, "siblings": [&#123;"name": "Scott", "age": 25, "pet": "Zuko"&#125;, &#123;"name": "Katie", "age": 33, "pet": "Cisco"&#125;]&#125;"""123import jsonresult = json.loads(obj)result{&#39;name&#39;: &#39;Wes&#39;, &#39;pet&#39;: None, &#39;places_lived&#39;: [&#39;United States&#39;, &#39;Spain&#39;, &#39;Germany&#39;], &#39;siblings&#39;: [{&#39;age&#39;: 25, &#39;name&#39;: &#39;Scott&#39;, &#39;pet&#39;: &#39;Zuko&#39;}, {&#39;age&#39;: 33, &#39;name&#39;: &#39;Katie&#39;, &#39;pet&#39;: &#39;Cisco&#39;}]} 1asjson = json.dumps(result) # convert to json12siblings = DataFrame(result['siblings'], columns=['name', 'age'])siblingsnameage0Scott251Katie33XML和HTML： Web信息收集NB. The Yahoo! Finance API has changed and this example no longer works123456from lxml.html import parsefrom urllib.request import urlopenparsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options'))doc = parsed.getroot()12links = doc.findall('.//a')links[15:20][&lt;Element a at 0x20352cad598&gt;, &lt;Element a at 0x20352cad5e8&gt;, &lt;Element a at 0x20352cad638&gt;, &lt;Element a at 0x20352cad688&gt;, &lt;Element a at 0x20352cad6d8&gt;] 1234lnk = links[28]lnklnk.get('href')lnk.text_content()&lt;Element a at 0x20352cad9a8&gt; &#39;/quote/NFLX?p=NFLX&#39; &#39;NFLX&#39; 12urls = [lnk.get('href') for lnk in doc.findall('.//a')]urls[-10:][&#39;//finance.yahoo.com/broker-comparison?bypass=true&#39;, &#39;https://help.yahoo.com/kb/index?page=content&amp;y=PROD_MAIL_ML&amp;locale=en_US&amp;id=SLN2310&amp;actp=productlink&#39;, &#39;http://help.yahoo.com/l/us/yahoo/finance/&#39;, &#39;https://yahoo.uservoice.com/forums/382977&#39;, &#39;http://info.yahoo.com/privacy/us/yahoo/&#39;, &#39;http://info.yahoo.com/relevantads/&#39;, &#39;http://info.yahoo.com/legal/us/yahoo/utos/utos-173.html&#39;, &#39;http://twitter.com/YahooFinance&#39;, &#39;http://facebook.com/yahoofinance&#39;, &#39;http://yahoofinance.tumblr.com&#39;] 123tables = doc.findall('.//table')len(tables)calls = tables[0]1 1rows = calls.findall('.//tr')123def _unpack(row, kind='td'): elts = row.findall('.//%s' % kind) return [val.text_content() for val in elts]12_unpack(rows[0], kind='th')_unpack(rows[1], kind='td')[] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-87-7d371ed47023&gt; in &lt;module&gt;() 1 _unpack(rows[0], kind=&#39;th&#39;) ----&gt; 2 _unpack(rows[1], kind=&#39;td&#39;) IndexError: list index out of range 1234567from pandas.io.parsers import TextParserdef parse_options_data(table): rows = table.findall('.//tr') header = _unpack(rows[0], kind='th') data = [_unpack(r) for r in rows[1:]] return TextParser(data, names=header).get_chunk()123call_data = parse_options_data(calls)put_data = parse_options_data(puts)call_data[:10]Parsing XML with lxml.objectify12345from lxml import objectifypath = '.\ch06\mta_perf\Performance_MNR.xml'parsed = objectify.parse(open(path))root = parsed.getroot()12345678910111213data = []skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ', 'DESIRED_CHANGE', 'DECIMAL_PLACES']for elt in root.INDICATOR: el_data = &#123;&#125; for child in elt.getchildren(): if child.tag in skip_fields: continue el_data[child.tag] = child.pyval data.append(el_data)12perf = DataFrame(data)perf.ix[:10, 5:]INDICATOR_UNITMONTHLY_ACTUALMONTHLY_TARGETPERIOD_MONTHPERIOD_YEARYTD_ACTUALYTD_TARGET0%96.9951200896.9951%95952200896952%96.9953200896.3953%98.3954200896.8954%95.8955200896.6955%94.4956200896.2956%96957200896.2957%96.4958200896.2958%93.7959200895.9959%96.495102008969510%96.99511200896.195二进制数据格式123frame = pd.read_csv('ch06/ex1.csv')frameframe.to_pickle('ch06/frame_pickle')abcdmessage01234hello15678world29101112foo1pd.read_pickle('ch06/frame_pickle')abcdmessage01234hello15678world29101112foo使用HDF5格式1234store = pd.HDFStore('mydata.h5')store['obj1'] = framestore['obj1_col'] = frame['a']store&lt;class &#39;pandas.io.pytables.HDFStore&#39;&gt; File path: mydata.h5 /obj1 frame (shape-&gt;[3,5]) /obj1_col series (shape-&gt;[3]) 1store['obj1']abcdmessage01234hello15678world29101112foo12store.close()os.remove('mydata.h5')使用数据库1234567891011import sqlite3query = """CREATE TABLE test(a VARCHAR(20), b VARCHAR(20), c REAL, d INTEGER);"""con = sqlite3.connect(':memory:')con.execute(query)con.commit()&lt;sqlite3.Cursor at 0x2035487c880&gt; 1234567data = [('Atlanta', 'Georgia', 1.25, 6), ('Tallahassee', 'Florida', 2.6, 3), ('Sacramento', 'California', 1.7, 5)]stmt = "INSERT INTO test VALUES(?, ?, ?, ?)"con.executemany(stmt, data)con.commit()&lt;sqlite3.Cursor at 0x2035487c810&gt; 123cursor = con.execute('select * from test')rows = cursor.fetchall()rows[(&#39;Atlanta&#39;, &#39;Georgia&#39;, 1.25, 6), (&#39;Tallahassee&#39;, &#39;Florida&#39;, 2.6, 3), (&#39;Sacramento&#39;, &#39;California&#39;, 1.7, 5)] 12import pandas.io.sql as sqlsql.read_sql('select * from test', con)abcd0AtlantaGeorgia1.2561TallahasseeFlorida2.6032SacramentoCalifornia1.705]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note ch05]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-ch05%2F</url>
      <content type="text"><![CDATA[pandas入门按照以下约定引用相关package12from pandas import Series, DataFrameimport pandas as pd123456789101112from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"pandas数据结构介绍SeriesSeries是一种类似于一维数组的对象，由一组数据以及一组与之相关的数据标签（类似于字典的键）组成，所以可以看成是一个有序的字典12obj = Series([4, 7, -5, 3])obj0 4 1 7 2 -5 3 3 dtype: int64 12obj.valuesobj.indexarray([ 4, 7, -5, 3], dtype=int64) RangeIndex(start=0, stop=4, step=1) 12obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])obj2d 4 b 7 a -5 c 3 dtype: int64 1obj2.indexIndex([&#39;d&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;], dtype=&#39;object&#39;) 1obj2['a']-5 12obj2['d'] = 6obj2[['c', 'a', 'd']]c 3 a -5 d 6 dtype: int64 各种Numpy运算都是作用在数据上，同时索引与数据的链接会一直保持1obj2[obj2 &gt; 0]d 6 b 7 c 3 dtype: int64 1obj2 * 2d 12 b 14 a -10 c 6 dtype: int64 1np.exp(obj2)d 403.428793 b 1096.633158 a 0.006738 c 20.085537 dtype: float64 1'b' in obj2True 1'e' in obj2False 因此可以直接根据Numpy的Dict来创建Series123sdata = &#123;'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000&#125;obj3 = Series(sdata)obj3Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 如果传入了index参数的话，那么就会与传入的Dict做键匹配，没有匹配上的就设为NaN123states = ['California', 'Ohio', 'Oregon', 'Texas']obj4 = Series(sdata, index=states)obj4California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64 1pd.isnull(obj4)California True Ohio False Oregon False Texas False dtype: bool 1pd.notnull(obj4)California False Ohio True Oregon True Texas True dtype: bool 1obj4.isnull()California True Ohio False Oregon False Texas False dtype: bool Series有一个非常重要的数据对齐的功能1obj3Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 1obj4California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64 1obj3 + obj4California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 Series本身以及其索引都有一个叫做name的属性，这个属性十分重要，以后很多高级功能都会用到123obj4.name = 'population'obj4.index.name = 'state'obj4state California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 Name: population, dtype: float64 可以通过直接赋值的方式修改index属性12obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']objBob 4 Steve 7 Jeff -5 Ryan 3 dtype: int64 DataFrameDataFrame是一个表格型的数据结构，可以看成由Series组成的字典，只不过这些Series共用一套索引1234data = &#123;'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2002, 2001, 2002], 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]&#125;frame = DataFrame(data)数据会被排序1framepopstateyear01.5Ohio200011.7Ohio200123.6Ohio200232.4Nevada200142.9Nevada20021DataFrame(data, columns=['year', 'state', 'pop'])yearstatepop02000Ohio1.512001Ohio1.722002Ohio3.632001Nevada2.442002Nevada2.9123frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'], index=['one', 'two', 'three', 'four', 'five'])frame2yearstatepopdebtone2000Ohio1.5NaNtwo2001Ohio1.7NaNthree2002Ohio3.6NaNfour2001Nevada2.4NaNfive2002Nevada2.9NaN1frame2.columnsIndex([&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;, &#39;debt&#39;], dtype=&#39;object&#39;) DataFrame每一个Key对应的Value都是一个Series1frame2['state']one Ohio two Ohio three Ohio four Nevada five Nevada Name: state, dtype: object 1frame2.yearone 2000 two 2001 three 2002 four 2001 five 2002 Name: year, dtype: int64 注意到name属性也已经被设置好了ix相当于一个行索引？1frame2.ix['three']year 2002 state Ohio pop 3.6 debt NaN Name: three, dtype: object 可以利用Numpy的广播功能12frame2['debt'] = 16.5frame2yearstatepopdebtone2000Ohio1.516.5two2001Ohio1.716.5three2002Ohio3.616.5four2001Nevada2.416.5five2002Nevada2.916.5也可以赋值一个列表，但是长度必须匹配12frame2['debt'] = np.arange(5.)frame2yearstatepopdebtone2000Ohio1.50.0two2001Ohio1.71.0three2002Ohio3.62.0four2001Nevada2.43.0five2002Nevada2.94.0如果是赋值一个Series，则会匹配上索引，没有匹配上的就是置为NaN123val = Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])frame2['debt'] = valframe2yearstatepopdebtone2000Ohio1.5NaNtwo2001Ohio1.7-1.2three2002Ohio3.6NaNfour2001Nevada2.4-1.5five2002Nevada2.9-1.7也可以进行逻辑操作12frame2['eastern'] = frame2.state == 'Ohio'frame2yearstatepopdebteasternone2000Ohio1.5NaNTruetwo2001Ohio1.7-1.2Truethree2002Ohio3.6NaNTruefour2001Nevada2.4-1.5Falsefive2002Nevada2.9-1.7Falsedel用于删除一列12del frame2['eastern']frame2.columnsIndex([&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;, &#39;debt&#39;], dtype=&#39;object&#39;) 只要是通过索引方式进行的操作，都是直接在原数据上进行的操作，不是一个副本嵌套的字典也可直接生成DataFrame，只不过内层的键被当作index，外层的键被当作colums12pop = &#123;'Nevada': &#123;2001: 2.4, 2002: 2.9&#125;, 'Ohio': &#123;2000: 1.5, 2001: 1.7, 2002: 3.6&#125;&#125;12frame3 = DataFrame(pop)frame3NevadaOhio2000NaN1.520012.41.720022.93.6同样可以进行转置，这样的话index和column就会互换1frame3.T200020012002NevadaNaN2.42.9Ohio1.51.73.6显式地指定索引，不匹配的会置为NaN1DataFrame(pop, index=[2001, 2002, 2003])NevadaOhio20012.41.720022.93.62003NaNNaN1frame3NevadaOhio2000NaN1.520012.41.720022.93.6还可以这样构建123pdata = &#123;'Ohio': frame3['Ohio'][:-1], 'Nevada': frame3['Nevada'][:2]&#125;DataFrame(pdata)NevadaOhio2000NaN1.520012.41.71frame3NevadaOhio2000NaN1.520012.41.720022.93.6name属性也会在表格中显示出来12frame3.index.name = 'year'; frame3.columns.name = 'state'frame3stateNevadaOhioyear2000NaN1.520012.41.720022.93.6values方法只返回数据，不返回index以及key1frame3.valuesarray([[ nan, 1.5], [ 2.4, 1.7], [ 2.9, 3.6]]) 1frame2yearstatepopdebtone2000Ohio1.5NaNtwo2001Ohio1.7-1.2three2002Ohio3.6NaNfour2001Nevada2.4-1.5five2002Nevada2.9-1.71frame2.valuesarray([[2000, &#39;Ohio&#39;, 1.5, nan], [2001, &#39;Ohio&#39;, 1.7, -1.2], [2002, &#39;Ohio&#39;, 3.6, nan], [2001, &#39;Nevada&#39;, 2.4, -1.5], [2002, &#39;Nevada&#39;, 2.9, -1.7]], dtype=object) 索引对象Index是一个可以单独提取出来的对象123obj = Series(range(3), index=['a', 'b', 'c'])index = obj.indexindexIndex([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], dtype=&#39;object&#39;) 1index[1:]Index([&#39;b&#39;, &#39;c&#39;], dtype=&#39;object&#39;) 不可修改~！1index[1] = 'd'--------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-52-676fdeb26a68&gt; in &lt;module&gt;() ----&gt; 1 index[1] = &#39;d&#39; C:\Users\Ewan\Anaconda3\lib\site-packages\pandas\indexes\base.py in __setitem__(self, key, value) 1243 1244 def __setitem__(self, key, value): -&gt; 1245 raise TypeError(&quot;Index does not support mutable operations&quot;) 1246 1247 def __getitem__(self, key): TypeError: Index does not support mutable operations 直接创建Index对象123index = pd.Index(np.arange(3))obj2 = Series([1.5, -2.5, 0], index=index)obj2.index is indexTrue 1frame3stateNevadaOhioyear2000NaN1.520012.41.720022.93.61'Ohio' in frame3.columnsTrue 12003 in frame3.indexFalse 基本功能重新索引12obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])objd 4.5 b 7.2 a -5.3 c 3.6 dtype: float64 重排索引形成新对象12obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])obj2a -5.3 b 7.2 c 3.6 d 4.5 e NaN dtype: float64 1obj.reindex(['a', 'b', 'c', 'd', 'e'], fill_value=0)a -5.3 b 7.2 c 3.6 d 4.5 e 0.0 dtype: float64 12obj3 = Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])obj3.reindex(range(6), method='ffill') # 前向填充0 blue 1 blue 2 purple 3 purple 4 yellow 5 yellow dtype: object 123frame = DataFrame(np.arange(9).reshape((3, 3)), index=['a', 'c', 'd'], columns=['Ohio', 'Texas', 'California'])frameOhioTexasCaliforniaa012c345d67812frame2 = frame.reindex(['a', 'b', 'c', 'd'])frame2OhioTexasCaliforniaa0.01.02.0bNaNNaNNaNc3.04.05.0d6.07.08.012states = ['Texas', 'Utah', 'California']frame.reindex(columns=states)TexasUtahCaliforniaa1NaN2c4NaN5d7NaN8插值只能按行12frame.reindex(index=['a', 'b', 'c', 'd'], method='ffill', columns=states)TexasUtahCaliforniaa1NaN2b1NaN2c4NaN5d7NaN8用ix方法进行重新索引操作会使得代码很简洁1frame.ix[['a', 'b', 'c', 'd'], states]TexasUtahCaliforniaa1.0NaN2.0bNaNNaNNaNc4.0NaN5.0d7.0NaN8.0丢弃指定轴上的项1frame.ix[['a', 'b', 'c', 'd'], states]TexasUtahCaliforniaa1.0NaN2.0bNaNNaNNaNc4.0NaN5.0d7.0NaN8.0123obj = Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])new_obj = obj.drop('c')new_obja 0.0 b 1.0 d 3.0 e 4.0 dtype: float64 1obj.drop(['d', 'c'])a 0.0 b 1.0 e 4.0 dtype: float64 1234data = DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])dataonetwothreefourOhio0123Colorado4567Utah891011New York121314151data.drop(['Colorado', 'Ohio'])onetwothreefourUtah891011New York121314151data.drop('two', axis=1)onethreefourOhio023Colorado467Utah81011New York1214151data.drop(['two', 'four'], axis=1)onethreeOhio02Colorado46Utah810New York1214索引，选取和过滤多种索引方式123obj = Series(np.arange(4.), index=['a', 'b', 'c', 'd'])objobj['b']a 0.0 b 1.0 c 2.0 d 3.0 dtype: float64 1.0 1obj[1]1.0 1obj[2:4]c 2.0 d 3.0 dtype: float64 1obj[['b', 'a', 'd']]b 1.0 a 0.0 d 3.0 dtype: float64 1obj[[1, 3]]b 1.0 d 3.0 dtype: float64 1obj[obj &lt; 2] # 直接对data进行操作a 0.0 b 1.0 dtype: float64 这种切片方式…末端包含1obj['b':'c']b 1.0 c 2.0 dtype: float64 12obj['b':'c'] = 5obja 0.0 b 5.0 c 5.0 d 3.0 dtype: float64 1234data = DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])dataonetwothreefourOhio0123Colorado4567Utah891011New York121314151data['two']Ohio 1 Colorado 5 Utah 9 New York 13 Name: two, dtype: int32 1data[['three', 'one']]threeoneOhio20Colorado64Utah108New York14121data[:2] # axis=0onetwothreefourOhio0123Colorado45671data[data['three'] &gt; 5]onetwothreefourColorado4567Utah891011New York121314151data &lt; 5onetwothreefourOhioTrueTrueTrueTrueColoradoTrueFalseFalseFalseUtahFalseFalseFalseFalseNew YorkFalseFalseFalseFalse1data[data &lt; 5] = 01dataonetwothreefourOhio0000Colorado0567Utah891011New York12131415索引的另外一种选择1data.ix['Colorado', ['two', 'three']]two 5 three 6 Name: Colorado, dtype: int32 1data.ix[['Colorado', 'Utah'], [3, 0, 1]]fouronetwoColorado705Utah11891data.ix[2]one 8 two 9 three 10 four 11 Name: Utah, dtype: int32 1data.ix[:'Utah', 'two']Ohio 0 Colorado 5 Utah 9 Name: two, dtype: int32 1data.ix[data.three &gt; 5, :3]onetwothreeColorado056Utah8910New York121314总结一下就是说，DataFrame是一个二维的数组，只不过每一维的索引方式除了序号之外，还可以用name属性来进行索引，且一切行为与序号无异算术运算和数据对齐12s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])1s1a 7.3 c -2.5 d 3.4 e 1.5 dtype: float64 1s2a -2.1 c 3.6 e -1.5 f 4.0 g 3.1 dtype: float64 数据对齐操作就是一种特殊的并集操作1s1 + s2a 5.2 c 1.1 d NaN e 0.0 f NaN g NaN dtype: float64 12345df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'), index=['Ohio', 'Texas', 'Colorado'])df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])df1bcdOhio0.01.02.0Texas3.04.05.0Colorado6.07.08.01df2bdeUtah0.01.02.0Ohio3.04.05.0Texas6.07.08.0Oregon9.010.011.0并且数据对齐操作是在所有维度上同时进行的1df1 + df2bcdeColoradoNaNNaNNaNNaNOhio3.0NaN6.0NaNOregonNaNNaNNaNNaNTexas9.0NaN12.0NaNUtahNaNNaNNaNNaN在算术方法中填充词下面这种定义column的方式值得注意123df1 = DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))df2 = DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))df1abcd00.01.02.03.014.05.06.07.028.09.010.011.01df2abcde00.01.02.03.04.015.06.07.08.09.0210.011.012.013.014.0315.016.017.018.019.01df1 + df2abcde00.02.04.06.0NaN19.011.013.015.0NaN218.020.022.024.0NaN3NaNNaNNaNNaNNaN要想填充值必须使用add方法1df1.add(df2, fill_value=0)abcde00.02.04.06.04.019.011.013.015.09.0218.020.022.024.014.0315.016.017.018.019.0reindex方法与add方法还是存在差异的1df1.reindex(columns=df2.columns, fill_value=0)abcde00.01.02.03.0014.05.06.07.0028.09.010.011.00DataFrame和Series之间的运算12arr = np.arange(12.).reshape((3, 4))arrarray([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) 1arr[0]array([ 0., 1., 2., 3.]) 广播操作1arr - arr[0]array([[ 0., 0., 0., 0.], [ 4., 4., 4., 4.], [ 8., 8., 8., 8.]]) 1234frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])series = frame.ix[0]framebdeUtah0.01.02.0Ohio3.04.05.0Texas6.07.08.0Oregon9.010.011.0Series的name等于DataFrame的切片属性1seriesb 0.0 d 1.0 e 2.0 Name: Utah, dtype: float64 默认情况下，DataFrame和Series之间的算术运算会将Series的index匹配到DataFrame的column, 然后沿着行向下广播1frame - seriesbdeUtah0.00.00.0Ohio3.03.03.0Texas6.06.06.0Oregon9.09.09.0如果Series的index与DataFrame的column不匹配，则进行数据对齐12series2 = Series(range(3), index=['b', 'e', 'f'])frame + series2bdefUtah0.0NaN3.0NaNOhio3.0NaN6.0NaNTexas6.0NaN9.0NaNOregon9.0NaN12.0NaN12series3 = frame['d']framebdeUtah0.01.02.0Ohio3.04.05.0Texas6.07.08.0Oregon9.010.011.01series3Utah 1.0 Ohio 4.0 Texas 7.0 Oregon 10.0 Name: d, dtype: float64 匹配行并且在列上进行广播， 就必须要指定axis12frame.sub(series3, axis=0)frame.sub(series3, axis=1)bdeUtah-1.00.01.0Ohio-1.00.01.0Texas-1.00.01.0Oregon-1.00.01.0OhioOregonTexasUtahbdeUtahNaNNaNNaNNaNNaNNaNNaNOhioNaNNaNNaNNaNNaNNaNNaNTexasNaNNaNNaNNaNNaNNaNNaNOregonNaNNaNNaNNaNNaNNaNNaN函数应用和映射12frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])1framebdeUtah-0.2047080.478943-0.519439Ohio-0.5557301.9657811.393406Texas0.0929080.2817460.769023Oregon1.2464351.007189-1.296221Numpy的元素级方法也可以应用到DataFrame上，直接把DataFrame当作二维的Numpy.array即可1np.abs(frame)bdeUtah0.2047080.4789430.519439Ohio0.5557301.9657811.393406Texas0.0929080.2817460.769023Oregon1.2464351.0071891.2962211f = lambda x: x.max() - x.min()apply方法将函数映射到由各行或者各列形成的一维数组上1frame.apply(f) # axis=0b 1.802165 d 1.684034 e 2.689627 dtype: float64 1frame.apply(f, axis=1)Utah 0.998382 Ohio 2.521511 Texas 0.676115 Oregon 2.542656 dtype: float64 1234def f(x): return Series([x.min(), x.max()], index=['min', 'max'])frame.apply(f)frame.apply(f, axis=1)bdemin-0.5557300.281746-1.296221max1.2464351.9657811.393406minmaxUtah-0.5194390.478943Ohio-0.5557301.965781Texas0.0929080.769023Oregon-1.2962211.246435元素级的函数映射applymap， 之所以叫这个名字是因为Series有一个元素级的映射函数map12format = lambda x: '%.2f' % xframe.applymap(format)bdeUtah-0.200.48-0.52Ohio-0.561.971.39Texas0.090.280.77Oregon1.251.01-1.301frame['e'].map(format)Utah -0.52 Ohio 1.39 Texas 0.77 Oregon -1.30 Name: e, dtype: object 排序和排名对索引或者column进行（字典）排序12obj = Series(range(4), index=['d', 'a', 'b', 'c'])obj.sort_index()a 1 b 2 c 3 d 0 dtype: int32 123frame = DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'], columns=['d', 'a', 'b', 'c'])frame.sort_index()dabcone4567three01231frame.sort_index(axis=1)abcdthree1230one5674降序1frame.sort_index(axis=1, ascending=False)dcbathree0321one4765按照data进行排序12obj = Series([4, 7, -3, 2])obj.sort_values()2 -3 3 2 0 4 1 7 dtype: int64 在排序时，任何缺失值默认都会被放到Series的末尾12obj = Series([4, np.nan, 7, np.nan, -3, 2])obj.sort_values()4 -3.0 5 2.0 0 4.0 2 7.0 1 NaN 3 NaN dtype: float64 12frame = DataFrame(&#123;'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]&#125;)frameab00411720-3312对指定index或者column进行排序1frame.sort_values(by='b')ab20-3312004117或者根据multi-index亦或multi-column进行排序1frame.sort_values(by=['a', 'b'])ab20-3004312117默认情况下，rank方法通过“为各组分配一个平均排名”的方式破坏平级关系。也就是说，如果有多个相同的值，则这些值的rank就是这些相同值rand的算术平均。12obj = Series([7, -5, 7, 4, 2, 0, 4, 4])obj.rank()0 7.5 1 1.0 2 7.5 3 5.0 4 3.0 5 2.0 6 5.0 7 5.0 dtype: float64 如果想按照一般方式排名1obj.rank(method='first')0 7.0 1 1.0 2 8.0 3 4.0 4 3.0 5 2.0 6 5.0 7 6.0 dtype: float64 使用每个分组的最大排名1obj.rank(ascending=False, method='max')0 2.0 1 8.0 2 2.0 3 5.0 4 6.0 5 7.0 6 5.0 7 5.0 dtype: float64 123frame = DataFrame(&#123;'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1], 'c': [-2, 5, 8, -2.5]&#125;)frameabc004.3-2.0117.05.020-3.08.0312.0-2.5指定维度1frame.rank(axis=1)abc02.03.01.011.03.02.022.01.03.032.03.01.0带有重复值的轴索引12obj = Series(range(5), index=['a', 'a', 'b', 'b', 'c'])obja 0 a 1 b 2 b 3 c 4 dtype: int32 1obj.index.is_uniqueFalse 返回一个Series1obj['a']a 0 a 1 dtype: int32 1obj['c']4 12df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])df012a0.2749920.2289131.352917a0.886429-2.001637-0.371843b1.669025-0.438570-0.539741b0.4769853.248944-1.0212281df.ix['b']012b1.669025-0.438570-0.539741b0.4769853.248944-1.021228汇总和计算描述统计12345df = DataFrame([[1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]], index=['a', 'b', 'c', 'd'], columns=['one', 'two'])dfonetwoa1.40NaNb7.10-4.5cNaNNaNd0.75-1.31df.sum() # axis=0 skipna=Trueone 9.25 two -5.80 dtype: float64 1df.sum(axis=1) # skipna=Truea 1.40 b 2.60 c 0.00 d -0.55 dtype: float64 1df.mean(axis=1, skipna=False)a NaN b 1.300 c NaN d -0.275 dtype: float64 返回的是索引1df.idxmax()one b two d dtype: object 1df.cumsum()onetwoa1.40NaNb8.50-4.5cNaNNaNd9.25-5.8describe对于数值型和非数值型数据的行为不一样1df.describe()C:\Users\Ewan\Anaconda3\lib\site-packages\numpy\lib\function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile RuntimeWarning) onetwocount3.0000002.000000mean3.083333-2.900000std3.4936852.262742min0.750000-4.50000025%NaNNaN50%NaNNaN75%NaNNaNmax7.100000-1.300000123obj = Series(['a', 'a', 'b', 'c'] * 4)objobj.describe()0 a 1 a 2 b 3 c 4 a 5 a 6 b 7 c 8 a 9 a 10 b 11 c 12 a 13 a 14 b 15 c dtype: object count 16 unique 3 top a freq 8 dtype: object 相关系数和xi12345678910import pandas_datareader.data as weball_data = &#123;&#125;for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']: all_data[ticker] = web.get_data_yahoo(ticker)price = DataFrame(&#123;tic: data['Adj Close'] for tic, data in all_data.items()&#125;)volume = DataFrame(&#123;tic: data['Volume'] for tic, data in all_data.items()&#125;)1price[:10]AAPLGOOGIBMMSFTDate2010-01-0427.727039313.062468111.40500025.5554852010-01-0527.774976311.683844110.05923225.5637412010-01-0627.333178303.826685109.34428325.4068592010-01-0727.282650296.753749108.96578625.1426342010-01-0827.464034300.709808110.05923225.3160312010-01-1127.221758300.255255108.90690324.9940072010-01-1226.912110294.945572109.77324524.8288662010-01-1327.291720293.252243109.53773525.0600642010-01-1427.133657294.630868111.28724525.5637412010-01-1526.680198289.710772110.84145825.4811721volume[:10]AAPLGOOGIBMMSFTDate2010-01-0412343240039270006155300384091002010-01-0515047620060319006841400497496002010-01-0613804000079871005605300581824002010-01-07119282800128766005840600505597002010-01-0811190270094839004197200511974002010-01-11115557400144798005730400687547002010-01-1214861490097429008081500659121002010-01-13151473000130418006455400518635002010-01-1410822350085119007111800632281002010-01-1514851690010909600849440079913200price.pct_changeSignature: price.pct_change(periods=1, fill_method=’pad’, limit=None, freq=None, **kwargs)Docstring:Percent change over given number of periods.Parametersperiods : int, default 1Periods to shift for forming percent changefill_method : str, default ‘pad’How to handle NAs before computing percent changeslimit : int, default NoneThe number of consecutive NAs to fill before stoppingfreq : DateOffset, timedelta, or offset alias string, optionalIncrement to use from time series API (e.g. ‘M’ or BDay())Returnschg : NDFrameNotesBy default, the percentage change is calculated along the stataxis: 0, or Index, for DataFrame and 1, or minor forPanel. You can change this with the axis keyword argument.12returns = price.pct_change()returns.tail()AAPLGOOGIBMMSFTDate2017-02-210.0072210.004335-0.002269-0.0020122017-02-220.002999-0.0010820.004937-0.0020162017-02-23-0.0042300.0006860.0027600.0040402017-02-240.000952-0.003236-0.0016510.0000002017-02-270.0019760.000772-0.010753-0.0060351returns.MSFT.corr(returns.IBM)0.49525655865062668 1returns.MSFT.cov(returns.IBM)8.5880535146740545e-05 1returns.corr()AAPLGOOGIBMMSFTAAPL1.0000000.4095230.3813740.388875GOOG0.4095231.0000000.4027810.470781IBM0.3813740.4027811.0000000.495257MSFT0.3888750.4707810.4952571.0000001returns.cov()AAPLGOOGIBMMSFTAAPL0.0002690.0001050.0000750.000092GOOG0.0001050.0002440.0000750.000106IBM0.0000750.0000750.0001440.000086MSFT0.0000920.0001060.0000860.0002091returns.corrwith(returns.IBM)AAPL 0.381374 GOOG 0.402781 IBM 1.000000 MSFT 0.495257 dtype: float64 1returns.corrwith(volume)AAPL -0.074055 GOOG -0.009543 IBM -0.194107 MSFT -0.090724 dtype: float64 唯一值， 值计数以及成员资格1obj = Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])12uniques = obj.unique()uniquesarray([&#39;c&#39;, &#39;a&#39;, &#39;d&#39;, &#39;b&#39;], dtype=object) 1obj.value_counts()c 3 a 3 b 2 d 1 dtype: int64 1pd.value_counts(obj.values, sort=False)a 3 d 1 b 2 c 3 dtype: int64 12mask = obj.isin(['b', 'c'])mask0 True 1 False 2 False 3 False 4 False 5 True 6 True 7 True 8 True dtype: bool 1obj[mask]0 c 5 b 6 b 7 c 8 c dtype: object 1234data = DataFrame(&#123;'Qu1': [1, 3, 4, 3, 4], 'Qu2': [2, 3, 1, 2, 3], 'Qu3': [1, 5, 2, 4, 4]&#125;)dataQu1Qu2Qu30121133524123324443412result = data.apply(pd.value_counts).fillna(0)resultQu1Qu2Qu311.01.01.020.02.01.032.02.00.042.00.02.050.00.01.0处理缺失数据12string_data = Series(['aardvark', 'artichoke', np.nan, 'avocado'])string_data0 aardvark 1 artichoke 2 NaN 3 avocado dtype: object 1string_data.isnull()0 False 1 False 2 True 3 False dtype: bool 12string_data[0] = Nonestring_data.isnull()0 True 1 False 2 True 3 False dtype: bool 滤除缺失数据123from numpy import nan as NAdata = Series([1, NA, 3.5, NA, 7])data.dropna()0 1.0 2 3.5 4 7.0 dtype: float64 1data[data.notnull()]0 1.0 2 3.5 4 7.0 dtype: float64 1234data = DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])cleaned = data.dropna()data01201.06.53.011.0NaNNaN2NaNNaNNaN3NaN6.53.01cleaned01201.06.53.01data.dropna(how='all')01201.06.53.011.0NaNNaN3NaN6.53.012data[4] = NAdata012401.06.53.0NaN11.0NaNNaNNaN2NaNNaNNaNNaN3NaN6.53.0NaN1data.dropna(axis=1, how='all')01201.06.53.011.0NaNNaN2NaNNaNNaN3NaN6.53.0123df = DataFrame(np.random.randn(7, 3))df.ix[:4, 1] = NA; df.ix[:2, 2] = NAdf0120-0.204708NaNNaN1-0.555730NaNNaN20.092908NaNNaN31.246435NaN-1.29622140.274992NaN1.35291750.886429-2.001637-0.37184361.669025-0.438570-0.5397411df.dropna(thresh=2)01231.246435NaN-1.29622140.274992NaN1.35291750.886429-2.001637-0.37184361.669025-0.438570-0.539741填充缺失数据1df.fillna(0)0120-0.2047080.0000000.0000001-0.5557300.0000000.00000020.0929080.0000000.00000031.2464350.000000-1.29622140.2749920.0000001.35291750.886429-2.001637-0.37184361.669025-0.438570-0.5397411df.fillna(&#123;1: 0.5, 3: -1&#125;)0120-0.2047080.500000NaN1-0.5557300.500000NaN20.0929080.500000NaN31.2464350.500000-1.29622140.2749920.5000001.35291750.886429-2.001637-0.37184361.669025-0.438570-0.539741fillna默认返回新对象，但也可以对现有对象就地修改123# always returns a reference to the filled object_ = df.fillna(0, inplace=True)df0120-0.2047080.0000000.0000001-0.5557300.0000000.00000020.0929080.0000000.00000031.2464350.000000-1.29622140.2749920.0000001.35291750.886429-2.001637-0.37184361.669025-0.438570-0.539741123df = DataFrame(np.random.randn(6, 3))df.ix[2:, 1] = NA; df.ix[4:, 2] = NAdf01200.4769853.248944-1.0212281-0.5770870.1241210.30261420.523772NaN1.3438103-0.713544NaN-2.3702324-1.860761NaNNaN5-1.265934NaNNaN1df.fillna(method='ffill')01200.4769853.248944-1.0212281-0.5770870.1241210.30261420.5237720.1241211.3438103-0.7135440.124121-2.3702324-1.8607610.124121-2.3702325-1.2659340.124121-2.3702321df.fillna(method='ffill', limit=2)01200.4769853.248944-1.0212281-0.5770870.1241210.30261420.5237720.1241211.3438103-0.7135440.124121-2.3702324-1.860761NaN-2.3702325-1.265934NaN-2.37023212data = Series([1., NA, 3.5, NA, 7])data.fillna(data.mean())0 1.000000 1 3.833333 2 3.500000 3 3.833333 4 7.000000 dtype: float64 层次索引1234data = Series(np.random.randn(10), index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd', 'd'], [1, 2, 3, 1, 2, 3, 1, 2, 2, 3]])dataa 1 0.332883 2 -2.359419 3 -0.199543 b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 d 2 -0.753887 3 0.331286 dtype: float64 1data.indexMultiIndex(levels=[[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;], [1, 2, 3]], labels=[[0, 0, 0, 1, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 1, 2]]) 1data['b']1 -1.541996 2 -0.970736 3 -1.307030 dtype: float64 1data['b':'c']b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 dtype: float64 1data.ix[['b', 'd']]b 1 -1.541996 2 -0.970736 3 -1.307030 d 2 -0.753887 3 0.331286 dtype: float64 1data[:, 2]a -2.359419 b -0.970736 c 0.377984 d -0.753887 dtype: float64 1data.unstack()123a0.332883-2.359419-0.199543b-1.541996-0.970736-1.307030c0.2863500.377984NaNdNaN-0.7538870.3312861data.unstack().stack()a 1 0.332883 2 -2.359419 3 -0.199543 b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 d 2 -0.753887 3 0.331286 dtype: float64 12345frame = DataFrame(np.arange(12).reshape((4, 3)), index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]], columns=[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])frameOhioColoradoGreenRedGreena10122345b1678291011123frame.index.names = ['key1', 'key2']frame.columns.names = ['state', 'color']framestateOhioColoradocolorGreenRedGreenkey1key2a10122345b16782910111frame['Ohio']colorGreenRedkey1key2a101234b1672910创建MultiIndex对象复用12MultiIndex.from_arrays([[&apos;Ohio&apos;, &apos;Ohio&apos;, &apos;Colorado&apos;], [&apos;Green&apos;, &apos;Red&apos;, &apos;Green&apos;]], names=[&apos;state&apos;, &apos;color&apos;])重排分级顺序1frame.swaplevel('key1', 'key2')stateOhioColoradocolorGreenRedGreenkey2key11a0122a3451b6782b910111frame.sortlevel(1)stateOhioColoradocolorGreenRedGreenkey1key2a1012b1678a2345b2910111frame.swaplevel(0, 1).sortlevel(0)stateOhioColoradocolorGreenRedGreenkey2key11a012b6782a345b91011根据级别汇总统计1frame.sum(level='key2')stateOhioColoradocolorGreenRedGreenkey21681021214161frame.sum(level='color', axis=1)colorGreenRedkey1key2a121284b114722010使用DataFrame的列1234frame = DataFrame(&#123;'a': range(7), 'b': range(7, 0, -1), 'c': ['one', 'one', 'one', 'two', 'two', 'two', 'two'], 'd': [0, 1, 2, 0, 1, 2, 3]&#125;)frameabcd007one0116one1225one2334two0443two1552two2661two312frame2 = frame.set_index(['c', 'd'])frame2abcdone007116225two0341432523611frame.set_index(['c', 'd'], drop=False)abcdcdone007one0116one1225one2two034two0143two1252two2361two31frame2.reset_index()cdab0one0071one1162one2253two0344two1435two2526two361拓展话题整数索引12ser = Series(np.arange(3.))ser.iloc[-1]2.0 1ser0 0.0 1 1.0 2 2.0 dtype: float64 12ser2 = Series(np.arange(3.), index=['a', 'b', 'c'])ser2[-1]2.0 1ser.ix[:1]0 0.0 1 1.0 dtype: float64 12ser3 = Series(range(3), index=[-5, 1, 3])ser3.iloc[2]2 123frame = DataFrame(np.arange(6).reshape((3, 2)), index=[2, 0, 1])frameframe.iloc[0]012010231450 0 1 1 Name: 2, dtype: int32 面板数据1234import pandas_datareader.data as webpdata = pd.Panel(dict((stk, web.get_data_yahoo(stk)) for stk in ['AAPL', 'GOOG', 'MSFT', 'DELL']))1pdata&lt;class &#39;pandas.core.panel.Panel&#39;&gt; Dimensions: 4 (items) x 1820 (major_axis) x 6 (minor_axis) Items axis: AAPL to MSFT Major_axis axis: 2010-01-04 00:00:00 to 2017-02-27 00:00:00 Minor_axis axis: Open to Adj Close 12pdata = pdata.swapaxes('items', 'minor')pdata['Adj Close'].iloc[:10]AAPLDELLGOOGMSFTDate2010-01-0427.72703914.06528313.06246825.5554852010-01-0527.77497614.38450311.68384425.5637412010-01-0627.33317814.10397303.82668525.4068592010-01-0727.28265014.23940296.75374925.1426342010-01-0827.46403414.36516300.70980825.3160312010-01-1127.22175814.37483300.25525524.9940072010-01-1226.91211014.56830294.94557224.8288662010-01-1327.29172014.57797293.25224325.0600642010-01-1427.13365714.22005294.63086825.5637412010-01-1526.68019813.92985289.71077225.4811721pdata.ix[:, '6/1/2012', :]OpenHighLowCloseVolumeAdj CloseAAPL569.159996572.650009560.520012560.989983130246900.072.681610DELL12.15000012.30000012.04500012.07000019397600.011.675920GOOG571.790972572.650996568.350996570.9810006138700.0285.205295MSFT28.76000028.95999928.44000128.45000156634300.024.9422391pdata.ix['Adj Close', '5/22/2012':, :].iloc[:10]AAPLDELLGOOGMSFTDate2012-05-2272.16078614.58765300.10041226.0907212012-05-2373.92149412.08221304.42610625.5208642012-05-2473.24260712.04351301.52897825.4857952012-05-2572.85003812.05319295.47005025.4770282012-05-28NaN12.05319NaNNaN2012-05-2974.14304112.24666296.87364525.9153802012-05-3075.03700512.14992293.82167425.7225052012-05-3174.85044211.92743290.14035425.5910002012-06-0172.68161011.67592285.20529524.9422392012-06-0473.10915611.60821289.00648025.02990812stacked = pdata.ix[:, '5/30/2012':, :].to_frame()stackedOpenHighLowCloseVolumeAdj CloseDateminor2012-05-30AAPL569.199997579.989990566.559990579.169998132357400.075.037005DELL12.59000012.70000012.46000012.56000019787800.012.149920GOOG588.161028591.901014583.530999588.2309923827600.0293.821674MSFT29.35000029.48000029.12000129.34000041585500.025.7225052012-05-31AAPL580.740021581.499985571.460022577.730019122918600.074.850442DELL12.53000012.54000012.33000012.33000019955600.011.927430GOOG588.720982590.001032579.001013580.8609905958800.0290.140354MSFT29.29999929.42000028.94000129.19000139134000.025.5910002012-06-01AAPL569.159996572.650009560.520012560.989983130246900.072.681610DELL12.15000012.30000012.04500012.07000019397600.011.675920GOOG571.790972572.650996568.350996570.9810006138700.0285.205295MSFT28.76000028.95999928.44000128.45000156634300.024.9422392012-06-04AAPL561.500008567.499985548.499977564.289978139248900.073.109156DELL12.11000012.11250011.80000012.00000017015700.011.608210GOOG570.220958580.491016570.011006578.5909734883500.0289.006480MSFT28.62000128.78000128.32000028.54999947926300.025.0299082012-06-05AAPL561.269989566.470001558.330002562.83002597053600.072.920005DELL11.95000012.24000011.95000012.16000015620900.011.762980GOOG575.451008578.131003566.470986570.4109994697200.0284.920579MSFT28.51000028.75000028.38999928.51000045715400.024.9948412012-06-06AAPL567.770004573.849983565.499992571.460022100363900.074.038104DELL12.21000012.28000012.09000012.21500020779900.011.816190GOOG576.480979581.970971573.611004580.5709664207200.0289.995487MSFT28.87999929.37000128.80999929.35000046860500.025.7312732012-06-07AAPL577.290009577.320023570.500000571.72000194941700.074.071787DELL12.32000012.41000012.12000012.13000020074000.011.733960GOOG587.601014587.891038577.251006578.2309863530100.0288.826666MSFT29.63999929.70000129.17000029.23000037792800.025.6260672012-06-08AAPL571.599998580.580017568.999992580.31998486879100.075.185997DELL12.13000012.22500012.02000012.12000018155600.011.724290........................2017-02-13AAPL133.080002133.820007132.750000133.28999323035400.0133.289993GOOG816.000000820.958984815.489990819.2399901198100.0819.239990MSFT64.23999864.86000164.12999764.72000122920100.064.3300002017-02-14AAPL133.470001135.089996133.250000135.02000432815500.0135.020004GOOG819.000000823.000000816.000000820.4500121053600.0820.450012MSFT64.41000464.72000164.01999764.57000023065900.064.5700002017-02-15AAPL135.520004136.270004134.619995135.50999535501600.0135.509995GOOG819.359985823.000000818.469971818.9799801304000.0818.979980MSFT64.50000064.57000064.16000464.52999916917000.064.5299992017-02-16AAPL135.669998135.899994134.839996135.35000622118000.0135.350006GOOG819.929993824.400024818.979980824.1599731281700.0824.159973MSFT64.73999865.23999864.44000264.51999720524700.064.5199972017-02-17AAPL135.100006135.830002135.100006135.72000122084500.0135.720001GOOG823.020020828.070007821.655029828.0700071597800.0828.070007MSFT64.47000164.69000264.30000364.62000321234600.064.6200032017-02-21AAPL136.229996136.750000135.979996136.69999724265100.0136.699997GOOG828.659973833.450012828.349976831.6599731247700.0831.659973MSFT64.61000164.94999764.44999764.48999819384900.064.4899982017-02-22AAPL136.429993137.119995136.110001137.11000120745300.0137.110001GOOG828.659973833.250000828.640015830.760010982900.0830.760010MSFT64.33000264.38999964.05000364.36000119259700.064.3600012017-02-23AAPL137.380005137.479996136.300003136.52999920704100.0136.529999GOOG830.119995832.460022822.880005831.3300171470100.0831.330017MSFT64.41999864.73000364.19000264.62000320235200.064.6200032017-02-24AAPL135.910004136.660004135.279999136.66000421690900.0136.660004GOOG827.729980829.000000824.200012828.6400151386600.0828.640015MSFT64.52999964.80000364.13999964.62000321705200.064.6200032017-02-27AAPL137.139999137.440002136.279999136.92999320196400.0136.929993GOOG824.549988830.500000824.000000829.2800291099500.0829.280029MSFT64.54000164.54000164.05000364.23000315850400.064.2300033952 rows × 6 columns1stacked.to_panel()&lt;class &#39;pandas.core.panel.Panel&#39;&gt; Dimensions: 6 (items) x 1207 (major_axis) x 4 (minor_axis) Items axis: Open to Adj Close Major_axis axis: 2012-05-30 00:00:00 to 2017-02-27 00:00:00 Minor_axis axis: AAPL to MSFT]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis-Learning note-Ch04]]></title>
      <url>%2F2017%2F02%2F26%2FPython-data-analysis-Learning-note-Ch04%2F</url>
      <content type="text"><![CDATA[Numpy基础：数组和矢量计算1%matplotlib inline1234from __future__ import divisionfrom numpy.random import randnimport numpy as npnp.set_printoptions(precision=4, suppress=True)12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"NumPy ndarray: 一种多维数组对象1data = randn(2, 3)123datadata * 10data + dataarray([[ 0.1584, 0.299 , -0.2555], [ 0.3277, -0.6934, 1.3191]]) array([[ 1.5842, 2.9896, -2.5545], [ 3.2767, -6.9342, 13.1913]]) array([[ 0.3168, 0.5979, -0.5109], [ 0.6553, -1.3868, 2.6383]]) 12data.shapedata.dtype(2, 3) dtype(&#39;float64&#39;) 创建ndarray123data1 = [6, 7.5, 8, 0, 1]arr1 = np.array(data1)arr1array([ 6. , 7.5, 8. , 0. , 1. ]) 12345data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]arr2 = np.array(data2)arr2arr2.ndimarr2.shapearray([[1, 2, 3, 4], [5, 6, 7, 8]]) 2 (2, 4) 除非显示说明，np.array会尝试为新建的数组选择一个合适的类型12arr1.dtypearr2.dtypedtype(&#39;float64&#39;) dtype(&#39;int32&#39;) 123np.zeros(10)np.zeros((3, 6))np.empty((2, 3, 2))array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) array([[ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.]]) array([[[ 0., 0.], [ 0., 0.], [ 0., 0.]], [[ 0., 0.], [ 0., 0.], [ 0., 0.]]]) 1np.arange(15)array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) ones_like, zeros_like, empty_like这三个方法接受一个数组为对象，创建和这个数组形状和dtype一样的全1， 全0和分配的初始空间ndarray的数据类型1234arr1 = np.array([1, 2, 3], dtype=np.float64)arr2 = np.array([1, 2, 3], dtype=np.int32)arr1.dtypearr2.dtypedtype(&#39;float64&#39;) dtype(&#39;int32&#39;) 当需要控制数据在内存和磁盘中的存储方式时（尤其是对大数据集），那就得了解如何控制存储类型1234arr = np.array([1, 2, 3, 4, 5])arr.dtypefloat_arr = arr.astype(np.float64) float_arr.dtypedtype(&#39;int32&#39;) dtype(&#39;float64&#39;) 123arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])arrarr.astype(np.int32)array([ 3.7, -1.2, -2.6, 0.5, 12.9, 10.1]) array([ 3, -1, -2, 0, 12, 10]) 12numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_)numeric_strings.astype(float)array([ 1.25, -9.6 , 42. ]) 123int_array = np.arange(10)calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)int_array.astype(calibers.dtype)array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) 12empty_uint32 = np.empty(8, dtype='u4')empty_uint32array([1, 2, 3, 4, 5, 6, 7, 8], dtype=uint32) 调用astype会创建原数组的一份拷贝数组和标量之间的运算1234arr = np.array([[1., 2., 3.], [4., 5., 6.]])arrarr * arrarr - arrarray([[ 1., 2., 3.], [ 4., 5., 6.]]) array([[ 1., 4., 9.], [ 16., 25., 36.]]) array([[ 0., 0., 0.], [ 0., 0., 0.]]) 121 / arrarr ** 0.5array([[ 1. , 0.5 , 0.3333], [ 0.25 , 0.2 , 0.1667]]) array([[ 1. , 1.4142, 1.7321], [ 2. , 2.2361, 2.4495]]) 基本的索引和切片123456arr = np.arange(10)arrarr[5]arr[5:8]arr[5:8] = 12arrarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 5 array([5, 6, 7]) array([ 0, 1, 2, 3, 4, 12, 12, 12, 8, 9]) 切片直接在原数组上操作如果想要得到一个复制的版本，需要显示地调用copy()方法12345arr_slice = arr[5:8]arr_slice[1] = 12345arrarr_slice[:] = 64arrarray([ 0, 1, 2, 3, 4, 12, 12345, 12, 8, 9]) array([ 0, 1, 2, 3, 4, 64, 64, 64, 8, 9]) 12arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])arr2d[2]array([7, 8, 9]) 注意下面这种索引方式12arr2d[0][2]arr2d[0, 2]3 3 12arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])arr3darray([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[0]array([[1, 2, 3], [4, 5, 6]]) 12345old_values = arr3d[0].copy()arr3d[0] = 42arr3darr3d[0] = old_valuesarr3darray([[[42, 42, 42], [42, 42, 42]], [[ 7, 8, 9], [10, 11, 12]]]) array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[1, 0]array([7, 8, 9]) 切片索引1arr[1:6]array([ 1, 2, 3, 4, 64]) 12arr2darr2d[:2]array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) array([[1, 2, 3], [4, 5, 6]]) 1arr2d[:2, 1:]array([[2, 3], [5, 6]]) 12arr2d[1, :2]arr2d[2, :1]array([4, 5]) array([7]) 1arr2d[:, :1]array([[1], [4], [7]]) 1arr2d[:2, 1:] = 0布尔型索引1234names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])data = randn(7, 4)namesdataarray([&#39;Bob&#39;, &#39;Joe&#39;, &#39;Will&#39;, &#39;Bob&#39;, &#39;Will&#39;, &#39;Joe&#39;, &#39;Joe&#39;], dtype=&#39;&lt;U4&#39;) array([[-2.9033, 1.4721, 0.9512, 1.7727], [ 2.2303, -1.0259, 1.0664, 0.534 ], [-0.9725, 0.2226, -0.1538, -0.4994], [-1.4289, 0.1665, -1.2874, -1.0817], [ 1.3581, -1.0734, -0.1387, 0.1673], [ 1.2816, 1.8883, 0.5699, -0.5843], [-0.0464, -0.9633, 0.2855, -0.6473]]) 1names == 'Bob'array([ True, False, False, True, False, False, False], dtype=bool) 1data[names == 'Bob']array([[-2.9033, 1.4721, 0.9512, 1.7727], [-1.4289, 0.1665, -1.2874, -1.0817]]) 12data[names == 'Bob', 2:]data[names == 'Bob', 3]array([[ 0.9512, 1.7727], [-1.2874, -1.0817]]) array([ 1.7727, -1.0817]) 12names != 'Bob'data[~(names == 'Bob')]array([False, True, True, False, True, True, True], dtype=bool) array([[ 2.2303, -1.0259, 1.0664, 0.534 ], [-0.9725, 0.2226, -0.1538, -0.4994], [ 1.3581, -1.0734, -0.1387, 0.1673], [ 1.2816, 1.8883, 0.5699, -0.5843], [-0.0464, -0.9633, 0.2855, -0.6473]]) 123mask = (names == 'Bob') | (names == 'Will')maskdata[mask]array([ True, False, True, True, True, False, False], dtype=bool) array([[-2.9033, 1.4721, 0.9512, 1.7727], [-0.9725, 0.2226, -0.1538, -0.4994], [-1.4289, 0.1665, -1.2874, -1.0817], [ 1.3581, -1.0734, -0.1387, 0.1673]]) Python关键字and和or在布尔型数组中无效12data[data &lt; 0] = 0dataarray([[ 0. , 1.4721, 0.9512, 1.7727], [ 2.2303, 0. , 1.0664, 0.534 ], [ 0. , 0.2226, 0. , 0. ], [ 0. , 0.1665, 0. , 0. ], [ 1.3581, 0. , 0. , 0.1673], [ 1.2816, 1.8883, 0.5699, 0. ], [ 0. , 0. , 0.2855, 0. ]]) 12data[names != 'Joe'] = 7dataarray([[ 7. , 7. , 7. , 7. ], [ 2.2303, 0. , 1.0664, 0.534 ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 1.2816, 1.8883, 0.5699, 0. ], [ 0. , 0. , 0.2855, 0. ]]) 花式索引花式索引创建新的数组1234arr = np.empty((8, 4))for i in range(8): arr[i] = iarrarray([[ 0., 0., 0., 0.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], [ 3., 3., 3., 3.], [ 4., 4., 4., 4.], [ 5., 5., 5., 5.], [ 6., 6., 6., 6.], [ 7., 7., 7., 7.]]) 1arr[[4, 3, 0, 6]]array([[ 4., 4., 4., 4.], [ 3., 3., 3., 3.], [ 0., 0., 0., 0.], [ 6., 6., 6., 6.]]) 1arr[[-3, -5, -7]]array([[ 5., 5., 5., 5.], [ 3., 3., 3., 3.], [ 1., 1., 1., 1.]]) 1234# more on reshape in Chapter 12arr = np.arange(32).reshape((8, 4))arrarr[[1, 5, 7, 2], [0, 3, 1, 2]]array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]) array([ 4, 23, 29, 10]) 根据以上可知，传入两个索引数组相当于进行了同位置组合注意以下这种方式1arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) np.ix_方法将两个一维数组转换为一个矩形区域的索引选择器1arr[np.ix_([1, 5, 7, 2], [0, 3, 1, 2])]array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) 数组转置和轴对换123arr = np.arange(15).reshape((3, 5))arrarr.Tarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]]) 12arr = np.random.randn(6, 3)np.dot(arr.T, arr)array([[ 3.6804, 0.0133, 1.0388], [ 0.0133, 1.6074, 0.1836], [ 1.0388, 0.1836, 3.5281]]) 123arr = np.arange(16).reshape((2, 2, 4))arrarr.transpose((1, 0, 2))array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) Refered from here.In short: transposing an array means that NumPy just needs to permute the stride and shape information for each axis:&gt;&gt;&gt; arr.strides (64, 32, 8) &gt;&gt;&gt; arr.transpose(1, 0, 2).strides (32, 64, 8) Notice that the strides for the first and second axes were swapped here. This means that no data needs to be copied; NumPy can simply change how it looks at the memory to construct the array.What are strides?The values in a 3D array arr are stored in a contiguous block of memory like this:[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] In the case of arr, each integer takes up 8 bytes of memory (i.e. we’re using the int64 dtype).A stride tells NumPy how many bytes to skip in order to move to the next value along an axis. For example, to get the next value in a row in arr (axis 2), we just need to move 8 bytes (1 number).The strides for arr.transpose(1, 0, 2) are (32, 64, 8). To move along the first axis, instead of 64 bytes (8 numbers) NumPy will now only skip 32 bytes (4 numbers) each time:[[[0 ...] [... ...]] [[4 ...] [... ...]]] Similarly, NumPy will now skip 64 bytes (8 numbers) in order to move along axis 1:[[[0 ...] [8 ...]] [[4 ...] [12 ...]]] The actual code that does the transposing is written in C and can be found here.也可以使用swapaxes方法123arrarr.swapaxes(1, 2)arrarray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) array([[[ 0, 4], [ 1, 5], [ 2, 6], [ 3, 7]], [[ 8, 12], [ 9, 13], [10, 14], [11, 15]]]) array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) 通用函数：快速的元素级数组函数123arr = np.arange(10)np.sqrt(arr)np.exp(arr)array([ 0. , 1. , 1.4142, 1.7321, 2. , 2.2361, 2.4495, 2.6458, 2.8284, 3. ]) array([ 1. , 2.7183, 7.3891, 20.0855, 54.5982, 148.4132, 403.4288, 1096.6332, 2980.958 , 8103.0839]) 12345x = randn(8)y = randn(8)xynp.maximum(x, y) # 对应元素进行比较array([ 0.811 , -0.0214, -0.3702, -0.4856, 1.1449, -0.4246, 0.9396, 0.0382]) array([-1.223 , 0.3271, -1.7197, -2.2636, -0.1154, -1.4122, -0.0989, 0.4477]) array([ 0.811 , 0.3271, -0.3702, -0.4856, 1.1449, -0.4246, 0.9396, 0.4477]) modf函数挺有意思123arr = randn(7) * 5arrnp.modf(arr)array([ 10.3171, -4.733 , -6.3358, 3.2457, -7.3823, 2.7036, -2.6173]) (array([ 0.3171, -0.733 , -0.3358, 0.2457, -0.3823, 0.7036, -0.6173]), array([ 10., -4., -6., 3., -7., 2., -2.])) 利用数组进行数据处理meshgrid产生两个二维数组，对应points中所有的二元组1234points = np.arange(-5, 5, 0.01) # 1000 equally spaced pointsxs, ys = np.meshgrid(points, points)xsysarray([[-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], ..., [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99]]) array([[-5. , -5. , -5. , ..., -5. , -5. , -5. ], [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99], [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98], ..., [ 4.97, 4.97, 4.97, ..., 4.97, 4.97, 4.97], [ 4.98, 4.98, 4.98, ..., 4.98, 4.98, 4.98], [ 4.99, 4.99, 4.99, ..., 4.99, 4.99, 4.99]]) 1from matplotlib.pyplot import imshow, title12345import matplotlib.pyplot as pltz = np.sqrt(xs ** 2 + ys ** 2)zplt.imshow(z, cmap=plt.cm.gray); plt.colorbar()plt.title("Image plot of $\sqrt&#123;x^2 + y^2&#125;$ for a grid of values")array([[ 7.0711, 7.064 , 7.0569, ..., 7.0499, 7.0569, 7.064 ], [ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569], [ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499], ..., [ 7.0499, 7.0428, 7.0357, ..., 7.0286, 7.0357, 7.0428], [ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499], [ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569]]) &lt;matplotlib.image.AxesImage at 0x23400a22b38&gt; &lt;matplotlib.colorbar.Colorbar at 0x23400a7c7b8&gt; &lt;matplotlib.text.Text at 0x23400a03da0&gt; 1plt.draw()&lt;matplotlib.figure.Figure at 0x23401396eb8&gt; 将条件逻辑表述为数组运算123xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])cond = np.array([True, False, True, True, False])注意下面列表生成式的写法123result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]result[1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5] 上述方法具有一些缺点：大数组处理速度慢（纯Python实现）无法处理多维数组所以可以使用下面这种方法：12result = np.where(cond, xarr, yarr)resultarray([ 1.1, 2.2, 1.3, 1.4, 2.5]) 1234arr = randn(4, 4)arrnp.where(arr &gt; 0, 2, -2)np.where(arr &gt; 0, 2, arr) # set only positive values to 2array([[-0.7355, -0.3188, -0.2358, 0.3137], [-0.6196, -0.5803, -0.5504, -1.1508], [ 0.1719, -1.1599, -0.7115, 1.7869], [-0.2306, 0.2068, 1.5366, 1.6154]]) array([[-2, -2, -2, 2], [-2, -2, -2, -2], [ 2, -2, -2, 2], [-2, 2, 2, 2]]) array([[-0.7355, -0.3188, -0.2358, 2. ], [-0.6196, -0.5803, -0.5504, -1.1508], [ 2. , -1.1599, -0.7115, 2. ], [-0.2306, 2. , 2. , 2. ]]) 显然where还可以应用于更复杂的操作。考虑下面这种逻辑：12345678910result = []for i in range(n): if cond1[i] and cond2[i]: result.append(0) elif cond1[i]: result.append(1) elif cond2[i]: result.append(2) else: result.append(3)用where可以这样实现：123np.where(cond1 &amp; cond2, 0, np.where(cond1, 1, np.where(cond2, 2, 3)))更加magic一点：1result = 1 * cond1 + 2 * cond2 + 3 * -(cond1 | cond2)数学和统计方法123456arr = np.random.randn(5, 4) # 正态分布arr# 下面两种方式都可以使用arr.mean()np.mean(arr)arr.sum()array([[ 1.4513, -0.8225, 0.7011, -0.617 ], [ 1.5872, 1.2937, 1.0151, 0.7123], [-0.2012, -0.0168, -0.3847, 0.5274], [-0.6312, -0.2762, 0.4869, 0.0462], [-0.5268, -1.1071, 1.8642, 0.2282]]) 0.26650934393195791 0.26650934393195791 5.3301868786391582 12arr.mean(axis=1)arr.sum(0) # axis=0array([ 0.1782, 1.1521, -0.0188, -0.0936, 0.1146]) array([ 1.6793, -0.9289, 3.6826, 0.8971]) 1234arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])arrarr.cumsum(0) # axis=0arr.cumprod(1) # axis=1array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) array([[ 0, 1, 2], [ 3, 5, 7], [ 9, 12, 15]], dtype=int32) array([[ 0, 0, 0], [ 3, 12, 60], [ 6, 42, 336]], dtype=int32) 用于布尔数组的方法123arr = randn(100)arr(arr &gt; 0).sum() # Number of positive valuesarray([ 0.7828, 0.1372, -0.6264, 1.8927, -0.2104, 0.2822, -0.3672, -0.3601, 0.5918, 0.9285, 0.1808, -0.4021, 0.4086, -0.2949, 0.5633, -0.7462, -0.1635, 0.1482, -0.3226, -1.2127, -0.9821, 0.0536, -0.1772, -0.4714, -0.9002, -0.0037, 0.7352, 0.5675, -1.1612, 0.5288, 0.3319, 0.7315, 0.6841, -0.6881, 1.5654, -0.4605, -0.5423, 0.0184, -0.8153, -0.1313, 0.4594, 0.0228, 0.255 , -2.2361, 0.8703, -1.5153, -0.9458, 0.2769, 0.9986, 0.7699, -0.7948, -1.2508, 1.7059, 0.1805, -1.0265, -0.0181, -0.9415, 0.1265, -0.2576, 0.6791, 0.3969, 0.8027, -0.6792, -0.7487, -1.9949, -0.9595, 0.5706, -0.5727, -1.0204, 0.1521, -0.9755, -0.4094, 0.67 , 0.212 , 0.4081, -0.1435, 0.3964, -0.1865, -0.6018, -2.6185, -0.5073, -0.6328, -0.2631, 0.6637, -0.5586, 1.3346, -0.5317, 0.8572, 1.1159, 0.9563, -0.0434, -1.0534, 0.5869, 0.0502, -0.0479, -0.8673, 0.1531, 1.0646, -0.2624, -0.3726]) 47 1234bools = np.array([False, False, True, False])boolsbools.any()bools.all()array([False, False, True, False], dtype=bool) True False 排序1234arr = randn(8)arrarr.sort()arrarray([ 1.0584, 1.9062, -0.2923, 0.7169, 0.5186, -0.6089, -2.0444, -0.5661]) array([-2.0444, -0.6089, -0.5661, -0.2923, 0.5186, 0.7169, 1.0584, 1.9062]) 1234arr = randn(5, 3)arrarr.sort(1) # axis=1arrarray([[ 0.0118, -2.8916, -0.4477], [-1.9768, 1.859 , -1.128 ], [-2.6262, 0.5791, 0.7594], [-0.5254, -0.9059, 0.0203], [-1.4029, -1.8566, 0.1892]]) array([[-2.8916, -0.4477, 0.0118], [-1.9768, -1.128 , 1.859 ], [-2.6262, 0.5791, 0.7594], [-0.9059, -0.5254, 0.0203], [-1.8566, -1.4029, 0.1892]]) 12345large_arr = randn(1000)large_arrlarge_arr.sort()large_arrlarge_arr[int(0.05 * len(large_arr))] # 5% quantilearray([ 1.2296, 0.3794, -0.1526, 2.1223, -0.0675, 0.6867, -0.5742, -1.4291, 0.6856, 0.1364, -0.3966, -0.7793, 0.4965, 0.2447, -0.7487, 0.7695, 0.5358, -0.4813, 0.9949, -0.6489, -0.3656, 1.9551, 0.8327, 1.497 , -0.4431, -0.8357, -0.821 , -0.7348, 1.9294, -0.3144, 0.1396, -0.9111, 0.0943, 0.8043, 1.067 , 0.9362, -2.2574, 0.7475, -1.0152, -1.1234, -0.3774, 1.076 , 0.8743, 1.1864, 0.0801, 0.3995, 0.2536, -0.9371, -1.669 , -2.2444, 1.2544, 1.0539, -0.7579, 0.2963, 0.7496, -1.3655, 0.1552, -0.6259, 0.2621, -1.5415, -0.1036, -0.5794, 1.2098, 1.3388, 0.3159, 1.0998, 0.5109, -0.3927, 1.4797, -1.4891, 0.3624, 0.966 , 0.0756, -0.4703, 0.1859, 1.6091, 0.662 , -0.4808, 0.8744, 0.4738, 1.1351, 0.0251, -1.017 , -0.849 , -0.1602, -1.5392, 0.0601, 1.7323, 1.1837, 0.4657, 0.8858, -0.211 , 0.1865, 0.673 , 0.3086, -1.2527, -0.7802, 0.407 , -1.118 , -0.2058, 0.7921, 0.5284, -2.3038, -0.4038, -1.1087, -0.827 , -2.6518, 0.3711, -0.0244, 1.1103, 0.2748, -0.7962, 1.9456, 0.5347, 0.1862, -0.3734, -0.3036, 0.6831, -0.9419, 1.4848, -0.1247, -0.4138, -0.601 , 0.6138, 1.1334, 0.4386, 0.0466, -0.0588, 0.6883, -1.2912, -0.2381, 0.3934, 0.2132, -0.4143, 1.0844, -0.5258, -0.9944, 1.0977, 0.3528, 1.9928, 1.421 , 0.8634, 0.1973, -1.1799, -2.9433, 2.697 , 0.4778, 0.6464, 0.049 , -0.2339, 1.6945, -0.6568, -0.5972, -0.8324, -0.6443, 0.0882, -0.3686, 0.0419, 0.5119, -0.641 , 1.1545, 1.0735, -0.5329, -0.1126, 0.0375, -1.0699, -1.3153, -1.6097, 2.5671, -0.9516, -0.388 , -0.0129, -0.0171, -1.0763, -0.7125, 0.767 , 0.2254, -0.7638, -0.2065, 1.2797, 0.0784, -0.7762, 1.7106, -0.0136, -0.4435, 1.2946, -2.5489, 0.4241, 0.5675, -0.7596, 0.6128, 1.1161, -1.2456, -0.131 , -0.2684, 1.6461, -0.2497, -0.4294, 1.122 , 0.5969, 0.3335, -0.0453, 1.1567, 0.0216, -0.7277, -2.5465, -2.4542, -1.5895, 0.4607, -0.8303, 0.0263, 0.0301, -1.2365, -0.146 , -0.8632, 0.6449, 0.1958, -0.6914, -0.3223, 0.4037, 0.9918, -0.3542, 0.8442, 0.7751, -1.6248, 2.6081, 0.3524, 1.5298, 0.4421, 1.5228, -1.5263, -1.3994, 0.0285, -0.5389, 1.4047, -2.1117, -1.0397, 0.6495, 0.9073, 1.8738, 0.2913, -1.069 , -0.7835, -0.6437, 0.6739, 0.3272, -0.8483, -0.2971, 0.2882, 0.1778, -0.6705, -1.4129, -0.1935, 0.6615, -0.4423, -1.2472, -0.9816, 0.927 , -2.2774, 0.5736, 1.3996, 1.1653, -0.3253, -0.2074, -0.2447, 0.4925, 1.8415, -1.1551, -0.5131, -0.6407, 0.5033, -0.817 , 0.0479, -0.9106, 1.4391, -1.5824, -0.4652, 1.253 , -0.6051, 0.6699, 0.3803, 1.0767, 1.5449, 0.106 , -0.7215, -0.354 , 0.1016, -1.3191, -0.6596, -0.9632, -0.3655, 0.8411, -0.2314, 1.9493, -0.6966, -1.2598, 0.4023, 0.1704, -0.452 , 1.5924, 0.381 , -0.4731, -1.2467, -0.4264, -0.2298, -0.1792, -0.5009, -1.0032, 1.0126, 0.5436, 1.1366, -1.0318, 1.3289, 0.3218, -0.2828, 0.5597, -0.0213, -0.078 , 0.7667, -0.3984, -1.0263, -0.5557, -2.0724, -0.9343, -0.6877, 1.0567, -0.605 , 1.7923, 0.6351, -1.769 , 0.4175, 0.8266, 0.3767, -0.1508, -0.4301, -0.3397, 0.7248, 0.188 , 1.1632, -1.0831, -0.5726, -0.475 , 0.092 , -0.1566, 1.9074, -1.4261, 1.8589, -0.7534, -1.0767, -0.2704, 0.7567, 0.5903, -1.5612, -1.1097, 0.3504, -0.9086, -0.1691, 0.6714, -0.6033, 1.8315, -0.8141, 0.5968, -0.408 , -1.1843, 0.5146, 0.6201, 0.4293, 0.9797, 1.066 , -1.3325, -1.733 , 0.8545, 0.3993, -0.2041, -0.4624, 0.0272, -0.005 , 0.9237, -0.5523, 0.9975, -0.4374, 0.1351, -0.6148, 0.3185, 0.0572, -0.3002, 0.0889, -0.0894, -0.5617, -2.0553, -0.2923, 0.7227, 0.604 , -0.6623, -0.6126, -0.4991, 0.0923, -0.6982, 0.2099, -0.6853, -0.4752, -1.625 , 0.0443, 2.5507, -1.1597, 0.3504, -0.7654, -1.4366, -1.3755, 0.3702, -1.7853, -0.7326, -1.2803, -0.6089, -0.4472, 0.462 , 0.7799, 0.3141, 0.8064, -1.0487, 0.7317, -0.2446, 0.3061, 0.1384, -0.572 , -0.0311, 0.3572, -0.6371, -0.2236, 0.0806, 0.6648, -0.148 , -0.2547, 1.3649, -0.1595, 1.3632, -0.8858, 1.1801, 0.5533, 2.3306, 0.2724, 0.7073, -0.5605, -0.8849, 0.9533, 0.3683, -0.2901, -0.0453, 0.1064, 1.3342, -0.7036, 0.7127, 1.2156, 0.9017, 1.2378, -1.1017, 1.0558, 1.4273, 0.7003, 1.1649, 0.0334, 0.3433, -0.3997, -0.1195, 1.3725, -0.3746, 0.8444, 0.961 , -0.2644, 0.3245, -1.3583, 0.387 , 1.2944, 0.0274, -0.5057, 0.15 , 0.6 , -0.5752, 0.3746, 1.7114, -0.0026, -0.1221, -0.8084, -0.9521, -0.6332, 0.7254, 1.7032, -0.0879, 0.3329, -1.9525, -0.7083, -0.4113, 1.163 , 0.9018, -0.3667, 0.8419, 0.4417, 0.2904, 0.1666, 1.3722, -0.4455, -1.4876, 0.4103, 2.3672, 0.3569, -0.8546, 0.5152, 0.9623, 1.1777, 1.6789, -1.7793, -0.7797, -1.0923, 0.07 , -0.8974, -0.3151, -0.3675, -1.9851, -2.3352, 0.3566, 1.1929, 1.5275, 1.4349, -1.4742, -0.1913, 1.5874, -0.7264, -0.5594, 0.3166, -0.9377, -0.6452, 0.394 , -0.2238, -1.1239, -0.0324, 1.3866, -0.6174, -0.1301, -0.0328, -0.92 , 1.8067, 0.2576, -0.5248, 0.4114, 0.1655, -0.1674, 0.2743, 0.0835, -0.145 , 1.1658, 1.2624, 0.0404, 2.0929, 0.6047, 1.0317, -0.4956, -1.5666, 1.1729, 0.484 , 0.955 , 1.0546, 0.0106, 0.5062, 0.3211, 0.8503, 0.4706, 1.9953, -0.9362, 0.6326, -0.3154, 1.4987, -0.1695, 1.0906, -0.686 , 0.2501, -0.316 , 0.3032, 0.4873, 0.6402, -0.1209, -0.1857, -0.3707, -0.3082, -0.4769, -0.858 , -0.1521, -0.3403, -0.9853, -0.5049, 0.3338, -0.3197, -0.5789, -0.7124, -0.8867, -0.0228, -1.5519, 1.8517, 0.5229, 0.7613, -0.5586, 0.4827, -1.3011, -0.5284, -0.3806, -0.7719, 1.6304, 0.0375, -0.9122, -0.1006, 0.382 , 0.0969, 1.7784, 0.1831, -1.8866, 0.2996, 0.4778, -0.2491, -1.6537, 0.022 , -0.101 , 0.5912, -0.2249, -1.1422, -0.6436, -1.4096, -0.7446, 0.8055, 1.0727, 0.2426, -0.8079, -1.4692, 0.062 , -0.4466, 0.3786, -2.0461, 0.7238, -1.6195, 1.4005, 0.4881, -0.8161, -0.582 , 0.3456, 1.2922, 0.2469, 1.9035, 0.9072, -0.0729, -0.9424, -1.1129, 0.8922, -0.5628, 1.6215, -0.7022, 0.8395, -0.3423, 0.6048, -0.248 , 0.7411, 0.3546, 0.6176, -0.8221, -0.338 , -2.1051, -1.0049, -0.0659, 0.0917, -0.6661, -0.5234, 0.9574, -0.6316, -0.0047, -0.4773, 0.1562, -0.116 , -1.6255, -0.9108, -1.4767, -0.7765, -1.7101, 0.0557, 0.8112, 0.7382, 1.8806, 0.9239, 1.8638, 0.8426, 0.0359, 0.2743, 1.9204, 1.2223, 0.4575, -0.3408, 0.3727, 0.5036, 0.5392, -1.3331, -0.4008, -0.1341, -1.5197, 0.1923, 0.2128, 1.1533, -1.4284, -0.7483, -0.4092, 1.2843, -0.4489, -0.6624, 0.9255, -0.0895, 0.3199, -0.2564, -0.1166, -1.4701, 1.1799, -1.6238, 0.0508, 0.2312, 0.7322, -1.3623, -0.232 , -0.2206, 0.566 , 1.2411, -1.1563, 1.1777, -1.1481, -0.6716, 0.4596, -0.2422, -0.8654, 0.4441, 0.1869, 1.4626, 0.7621, 0.4249, 0.252 , 0.632 , 0.5626, -0.7925, 1.1995, 1.5665, 0.6096, 0.4821, -0.7324, -0.7624, 1.858 , -0.8434, -0.4408, 0.2011, 0.7552, -0.8955, -1.3255, 0.7022, 0.1507, 0.662 , -1.2229, 0.5199, 0.9837, -0.3947, -0.5262, -1.0424, -1.4582, 0.5126, -0.3606, 0.4427, -2.3922, 1.2784, -0.8382, -0.0198, 1.2136, -0.4212, -0.7798, -1.3387, -0.7141, 0.9581, -0.8575, -0.2255, 0.8436, -2.2162, 0.0742, 0.9683, -0.3633, -0.0227, -1.2176, 1.1482, -0.6697, 0.9643, -1.2802, -0.3651, -1.29 , 0.851 , 1.0167, 1.0011, -1.3014, -0.7205, 1.3621, -0.692 , 1.0637, 0.5637, 0.0851, 2.1514, -0.272 , 0.3136, 0.2179, 0.7035, -1.3028, -0.1032, 0.0611, 1.2002, -0.7346, 0.9991, -0.3747, 0.7908, -0.9573, -0.5114, -0.8607, -0.6711, 1.3335, -0.6671, -0.1687, 0.4601, 0.5747, -0.0767, -0.8428, 0.3372, -1.7756, -2.5264, -1.503 , -0.5669, 0.0167, -1.961 , 0.8861, 1.1902, 2.239 , 0.2481, 0.7361, -1.1103, 0.8368, -1.0434, 0.6809, -0.0839, -0.6972, -1.5492, -1.4129, 0.5889, 0.2138, 1.7689, -0.4861, -0.1124, 0.2032, 1.0664, -0.369 , 2.3793, 0.4406, -1.1741, 1.0812, 1.3965, -0.149 , 0.8793, 1.3494, 1.2159, -0.0001, 1.1929, -0.1966, -0.1666, 1.7097, -0.4273, 0.4831, -0.2411, -1.4517, -0.7317, 0.099 , 1.7922, 0.2313, -0.5031, -0.0849, 0.7331, -0.1483, -0.8003, 1.1897, 0.031 , -0.3624, -1.1133, 1.4647, 2.5653, -1.9536, -0.4528, -1.693 , 0.4847, 0.1368, 0.6859, -0.9872, 0.8425, -0.1492, -0.1335, -0.0229, -0.0903, -0.4381, 1.2552, 1.5763, 0.2375, -0.7597, 0.0845, 0.0894, -1.6022, -0.1988, 0.3095, -1.0785, -1.6044, -0.4922, 0.4583, 0.3168, -2.0485, -1.2147, -0.2803, -0.2071, 0.0767, 1.9544, -1.7648, 0.2873, -0.4029, -0.8128, -0.1081, 0.0332, 2.5288, 0.9933, 0.4378, -0.8208, -0.2451, 0.3472, -0.2917, 2.0775, 1.7381, -0.467 , -0.8943, -1.4171, -0.3905, 0.2591, 0.8118, -0.643 , 1.0387, 0.0049, 1.7299, -0.6882, -1.4132, -1.0893, 0.4606, -1.546 , -2.87 , 0.3492, -1.5968, 0.9858, 0.1384, -0.6016, -0.9632, -0.9088, 0.3711, 1.3509, 0.4601, -1.4963, -0.043 , 0.5588, 0.2638, -1.1118, -0.9376, -0.9139, 0.6551, 0.4876, -1.7039, -0.2915, 0.3867, -0.1795, 1.2298, 0.0893, -0.6019, 1.4109, -1.1918, 0.5009, 0.0157, -1.1307, 1.0407, 1.9742, -1.0377, -0.6151, -0.8398, 1.4096, -0.012 , -1.5323, 0.3323, 0.0539, 0.2383, -0.4059, 2.285 , 0.1536, 0.3838, 0.3623, -0.4326, -0.0975, -1.8119]) array([-2.9433, -2.87 , -2.6518, -2.5489, -2.5465, -2.5264, -2.4542, -2.3922, -2.3352, -2.3038, -2.2774, -2.2574, -2.2444, -2.2162, -2.1117, -2.1051, -2.0724, -2.0553, -2.0485, -2.0461, -1.9851, -1.961 , -1.9536, -1.9525, -1.8866, -1.8119, -1.7853, -1.7793, -1.7756, -1.769 , -1.7648, -1.733 , -1.7101, -1.7039, -1.693 , -1.669 , -1.6537, -1.6255, -1.625 , -1.6248, -1.6238, -1.6195, -1.6097, -1.6044, -1.6022, -1.5968, -1.5895, -1.5824, -1.5666, -1.5612, -1.5519, -1.5492, -1.546 , -1.5415, -1.5392, -1.5323, -1.5263, -1.5197, -1.503 , -1.4963, -1.4891, -1.4876, -1.4767, -1.4742, -1.4701, -1.4692, -1.4582, -1.4517, -1.4366, -1.4291, -1.4284, -1.4261, -1.4171, -1.4132, -1.4129, -1.4129, -1.4096, -1.3994, -1.3755, -1.3655, -1.3623, -1.3583, -1.3387, -1.3331, -1.3325, -1.3255, -1.3191, -1.3153, -1.3028, -1.3014, -1.3011, -1.2912, -1.29 , -1.2803, -1.2802, -1.2598, -1.2527, -1.2472, -1.2467, -1.2456, -1.2365, -1.2229, -1.2176, -1.2147, -1.1918, -1.1843, -1.1799, -1.1741, -1.1597, -1.1563, -1.1551, -1.1481, -1.1422, -1.1307, -1.1239, -1.1234, -1.118 , -1.1133, -1.1129, -1.1118, -1.1103, -1.1097, -1.1087, -1.1017, -1.0923, -1.0893, -1.0831, -1.0785, -1.0767, -1.0763, -1.0699, -1.069 , -1.0487, -1.0434, -1.0424, -1.0397, -1.0377, -1.0318, -1.0263, -1.017 , -1.0152, -1.0049, -1.0032, -0.9944, -0.9872, -0.9853, -0.9816, -0.9632, -0.9632, -0.9573, -0.9521, -0.9516, -0.9424, -0.9419, -0.9377, -0.9376, -0.9371, -0.9362, -0.9343, -0.92 , -0.9139, -0.9122, -0.9111, -0.9108, -0.9106, -0.9088, -0.9086, -0.8974, -0.8955, -0.8943, -0.8867, -0.8858, -0.8849, -0.8654, -0.8632, -0.8607, -0.858 , -0.8575, -0.8546, -0.849 , -0.8483, -0.8434, -0.8428, -0.8398, -0.8382, -0.8357, -0.8324, -0.8303, -0.827 , -0.8221, -0.821 , -0.8208, -0.817 , -0.8161, -0.8141, -0.8128, -0.8084, -0.8079, -0.8003, -0.7962, -0.7925, -0.7835, -0.7802, -0.7798, -0.7797, -0.7793, -0.7765, -0.7762, -0.7719, -0.7654, -0.7638, -0.7624, -0.7597, -0.7596, -0.7579, -0.7534, -0.7487, -0.7483, -0.7446, -0.7348, -0.7346, -0.7326, -0.7324, -0.7317, -0.7277, -0.7264, -0.7215, -0.7205, -0.7141, -0.7125, -0.7124, -0.7083, -0.7036, -0.7022, -0.6982, -0.6972, -0.6966, -0.692 , -0.6914, -0.6882, -0.6877, -0.686 , -0.6853, -0.6716, -0.6711, -0.6705, -0.6697, -0.6671, -0.6661, -0.6624, -0.6623, -0.6596, -0.6568, -0.6489, -0.6452, -0.6443, -0.6437, -0.6436, -0.643 , -0.641 , -0.6407, -0.6371, -0.6332, -0.6316, -0.6259, -0.6174, -0.6151, -0.6148, -0.6126, -0.6089, -0.6051, -0.605 , -0.6033, -0.6019, -0.6016, -0.601 , -0.5972, -0.582 , -0.5794, -0.5789, -0.5752, -0.5742, -0.5726, -0.572 , -0.5669, -0.5628, -0.5617, -0.5605, -0.5594, -0.5586, -0.5557, -0.5523, -0.5389, -0.5329, -0.5284, -0.5262, -0.5258, -0.5248, -0.5234, -0.5131, -0.5114, -0.5057, -0.5049, -0.5031, -0.5009, -0.4991, -0.4956, -0.4922, -0.4861, -0.4813, -0.4808, -0.4773, -0.4769, -0.4752, -0.475 , -0.4731, -0.4703, -0.467 , -0.4652, -0.4624, -0.4528, -0.452 , -0.4489, -0.4472, -0.4466, -0.4455, -0.4435, -0.4431, -0.4423, -0.4408, -0.4381, -0.4374, -0.4326, -0.4301, -0.4294, -0.4273, -0.4264, -0.4212, -0.4143, -0.4138, -0.4113, -0.4092, -0.408 , -0.4059, -0.4038, -0.4029, -0.4008, -0.3997, -0.3984, -0.3966, -0.3947, -0.3927, -0.3905, -0.388 , -0.3806, -0.3774, -0.3747, -0.3746, -0.3734, -0.3707, -0.369 , -0.3686, -0.3675, -0.3667, -0.3656, -0.3655, -0.3651, -0.3633, -0.3624, -0.3606, -0.3542, -0.354 , -0.3423, -0.3408, -0.3403, -0.3397, -0.338 , -0.3253, -0.3223, -0.3197, -0.316 , -0.3154, -0.3151, -0.3144, -0.3082, -0.3036, -0.3002, -0.2971, -0.2923, -0.2917, -0.2915, -0.2901, -0.2828, -0.2803, -0.272 , -0.2704, -0.2684, -0.2644, -0.2564, -0.2547, -0.2497, -0.2491, -0.248 , -0.2451, -0.2447, -0.2446, -0.2422, -0.2411, -0.2381, -0.2339, -0.232 , -0.2314, -0.2298, -0.2255, -0.2249, -0.2238, -0.2236, -0.2206, -0.211 , -0.2074, -0.2071, -0.2065, -0.2058, -0.2041, -0.1988, -0.1966, -0.1935, -0.1913, -0.1857, -0.1795, -0.1792, -0.1695, -0.1691, -0.1687, -0.1674, -0.1666, -0.1602, -0.1595, -0.1566, -0.1526, -0.1521, -0.1508, -0.1492, -0.149 , -0.1483, -0.148 , -0.146 , -0.145 , -0.1341, -0.1335, -0.131 , -0.1301, -0.1247, -0.1221, -0.1209, -0.1195, -0.1166, -0.116 , -0.1126, -0.1124, -0.1081, -0.1036, -0.1032, -0.101 , -0.1006, -0.0975, -0.0903, -0.0895, -0.0894, -0.0879, -0.0849, -0.0839, -0.078 , -0.0767, -0.0729, -0.0675, -0.0659, -0.0588, -0.0453, -0.0453, -0.043 , -0.0328, -0.0324, -0.0311, -0.0244, -0.0229, -0.0228, -0.0227, -0.0213, -0.0198, -0.0171, -0.0136, -0.0129, -0.012 , -0.005 , -0.0047, -0.0026, -0.0001, 0.0049, 0.0106, 0.0157, 0.0167, 0.0216, 0.022 , 0.0251, 0.0263, 0.0272, 0.0274, 0.0285, 0.0301, 0.031 , 0.0332, 0.0334, 0.0359, 0.0375, 0.0375, 0.0404, 0.0419, 0.0443, 0.0466, 0.0479, 0.049 , 0.0508, 0.0539, 0.0557, 0.0572, 0.0601, 0.0611, 0.062 , 0.07 , 0.0742, 0.0756, 0.0767, 0.0784, 0.0801, 0.0806, 0.0835, 0.0845, 0.0851, 0.0882, 0.0889, 0.0893, 0.0894, 0.0917, 0.092 , 0.0923, 0.0943, 0.0969, 0.099 , 0.1016, 0.106 , 0.1064, 0.1351, 0.1364, 0.1368, 0.1384, 0.1384, 0.1396, 0.15 , 0.1507, 0.1536, 0.1552, 0.1562, 0.1655, 0.1666, 0.1704, 0.1778, 0.1831, 0.1859, 0.1862, 0.1865, 0.1869, 0.188 , 0.1923, 0.1958, 0.1973, 0.2011, 0.2032, 0.2099, 0.2128, 0.2132, 0.2138, 0.2179, 0.2254, 0.2312, 0.2313, 0.2375, 0.2383, 0.2426, 0.2447, 0.2469, 0.2481, 0.2501, 0.252 , 0.2536, 0.2576, 0.2591, 0.2621, 0.2638, 0.2724, 0.2743, 0.2743, 0.2748, 0.2873, 0.2882, 0.2904, 0.2913, 0.2963, 0.2996, 0.3032, 0.3061, 0.3086, 0.3095, 0.3136, 0.3141, 0.3159, 0.3166, 0.3168, 0.3185, 0.3199, 0.3211, 0.3218, 0.3245, 0.3272, 0.3323, 0.3329, 0.3335, 0.3338, 0.3372, 0.3433, 0.3456, 0.3472, 0.3492, 0.3504, 0.3504, 0.3524, 0.3528, 0.3546, 0.3566, 0.3569, 0.3572, 0.3623, 0.3624, 0.3683, 0.3702, 0.3711, 0.3711, 0.3727, 0.3746, 0.3767, 0.3786, 0.3794, 0.3803, 0.381 , 0.382 , 0.3838, 0.3867, 0.387 , 0.3934, 0.394 , 0.3993, 0.3995, 0.4023, 0.4037, 0.407 , 0.4103, 0.4114, 0.4175, 0.4241, 0.4249, 0.4293, 0.4378, 0.4386, 0.4406, 0.4417, 0.4421, 0.4427, 0.4441, 0.4575, 0.4583, 0.4596, 0.4601, 0.4601, 0.4606, 0.4607, 0.462 , 0.4657, 0.4706, 0.4738, 0.4778, 0.4778, 0.4821, 0.4827, 0.4831, 0.484 , 0.4847, 0.4873, 0.4876, 0.4881, 0.4925, 0.4965, 0.5009, 0.5033, 0.5036, 0.5062, 0.5109, 0.5119, 0.5126, 0.5146, 0.5152, 0.5199, 0.5229, 0.5284, 0.5347, 0.5358, 0.5392, 0.5436, 0.5533, 0.5588, 0.5597, 0.5626, 0.5637, 0.566 , 0.5675, 0.5736, 0.5747, 0.5889, 0.5903, 0.5912, 0.5968, 0.5969, 0.6 , 0.604 , 0.6047, 0.6048, 0.6096, 0.6128, 0.6138, 0.6176, 0.6201, 0.632 , 0.6326, 0.6351, 0.6402, 0.6449, 0.6464, 0.6495, 0.6551, 0.6615, 0.662 , 0.662 , 0.6648, 0.6699, 0.6714, 0.673 , 0.6739, 0.6809, 0.6831, 0.6856, 0.6859, 0.6867, 0.6883, 0.7003, 0.7022, 0.7035, 0.7073, 0.7127, 0.7227, 0.7238, 0.7248, 0.7254, 0.7317, 0.7322, 0.7331, 0.7361, 0.7382, 0.7411, 0.7475, 0.7496, 0.7552, 0.7567, 0.7613, 0.7621, 0.7667, 0.767 , 0.7695, 0.7751, 0.7799, 0.7908, 0.7921, 0.8043, 0.8055, 0.8064, 0.8112, 0.8118, 0.8266, 0.8327, 0.8368, 0.8395, 0.8411, 0.8419, 0.8425, 0.8426, 0.8436, 0.8442, 0.8444, 0.8503, 0.851 , 0.8545, 0.8634, 0.8743, 0.8744, 0.8793, 0.8858, 0.8861, 0.8922, 0.9017, 0.9018, 0.9072, 0.9073, 0.9237, 0.9239, 0.9255, 0.927 , 0.9362, 0.9533, 0.955 , 0.9574, 0.9581, 0.961 , 0.9623, 0.9643, 0.966 , 0.9683, 0.9797, 0.9837, 0.9858, 0.9918, 0.9933, 0.9949, 0.9975, 0.9991, 1.0011, 1.0126, 1.0167, 1.0317, 1.0387, 1.0407, 1.0539, 1.0546, 1.0558, 1.0567, 1.0637, 1.066 , 1.0664, 1.067 , 1.0727, 1.0735, 1.076 , 1.0767, 1.0812, 1.0844, 1.0906, 1.0977, 1.0998, 1.1103, 1.1161, 1.122 , 1.1334, 1.1351, 1.1366, 1.1482, 1.1533, 1.1545, 1.1567, 1.163 , 1.1632, 1.1649, 1.1653, 1.1658, 1.1729, 1.1777, 1.1777, 1.1799, 1.1801, 1.1837, 1.1864, 1.1897, 1.1902, 1.1929, 1.1929, 1.1995, 1.2002, 1.2098, 1.2136, 1.2156, 1.2159, 1.2223, 1.2296, 1.2298, 1.2378, 1.2411, 1.253 , 1.2544, 1.2552, 1.2624, 1.2784, 1.2797, 1.2843, 1.2922, 1.2944, 1.2946, 1.3289, 1.3335, 1.3342, 1.3388, 1.3494, 1.3509, 1.3621, 1.3632, 1.3649, 1.3722, 1.3725, 1.3866, 1.3965, 1.3996, 1.4005, 1.4047, 1.4096, 1.4109, 1.421 , 1.4273, 1.4349, 1.4391, 1.4626, 1.4647, 1.4797, 1.4848, 1.497 , 1.4987, 1.5228, 1.5275, 1.5298, 1.5449, 1.5665, 1.5763, 1.5874, 1.5924, 1.6091, 1.6215, 1.6304, 1.6461, 1.6789, 1.6945, 1.7032, 1.7097, 1.7106, 1.7114, 1.7299, 1.7323, 1.7381, 1.7689, 1.7784, 1.7922, 1.7923, 1.8067, 1.8315, 1.8415, 1.8517, 1.858 , 1.8589, 1.8638, 1.8738, 1.8806, 1.9035, 1.9074, 1.9204, 1.9294, 1.9456, 1.9493, 1.9544, 1.9551, 1.9742, 1.9928, 1.9953, 2.0775, 2.0929, 2.1223, 2.1514, 2.239 , 2.285 , 2.3306, 2.3672, 2.3793, 2.5288, 2.5507, 2.5653, 2.5671, 2.6081, 2.697 ]) -1.5519406239259821 唯一化以及其他的集合逻辑1234names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])np.unique(names)ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])np.unique(ints)array([&#39;Bob&#39;, &#39;Joe&#39;, &#39;Will&#39;], dtype=&#39;&lt;U4&#39;) array([1, 2, 3, 4]) 1sorted(set(names))[&#39;Bob&#39;, &#39;Joe&#39;, &#39;Will&#39;] in1d感觉挺有用12values = np.array([6, 0, 0, 3, 2, 5, 6])np.in1d(values, [2, 3, 6])array([ True, False, False, True, True, False, True], dtype=bool) 用于数组的文件输入输出将数组以二进制的形式保存到磁盘12arr = np.arange(10)np.save('some_array', arr)1np.load('some_array.npy')array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 压缩存储，并且可以存储多个1np.savez('array_archive.npz', a=arr[:4], b=arr)123arch = np.load('array_archive.npz')arch['a']arch['b']array([0, 1, 2, 3]) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 存取文本文件1!more ch04\array_ex.txt0.580052,0.186730,1.040717,1.134411 0.194163,-0.636917,-0.938659,0.124094 -0.126410,0.268607,-0.695724,0.047428 -1.484413,0.004176,-0.744203,0.005487 2.302869,0.200131,1.670238,-1.881090 -0.193230,1.047233,0.482803,0.960334 12arr = np.loadtxt('.\\ch04\\array_ex.txt', delimiter=',')arrarray([[ 0.5801, 0.1867, 1.0407, 1.1344], [ 0.1942, -0.6369, -0.9387, 0.1241], [-0.1264, 0.2686, -0.6957, 0.0474], [-1.4844, 0.0042, -0.7442, 0.0055], [ 2.3029, 0.2001, 1.6702, -1.8811], [-0.1932, 1.0472, 0.4828, 0.9603]]) 线性代数12345x = np.array([[1., 2., 3.], [4., 5., 6.]])y = np.array([[6., 23.], [-1, 7], [8, 9]])xyx.dot(y) # equivalently np.dot(x, y)array([[ 1., 2., 3.], [ 4., 5., 6.]]) array([[ 6., 23.], [ -1., 7.], [ 8., 9.]]) array([[ 28., 64.], [ 67., 181.]]) 1np.dot(x, np.ones(3))array([ 6., 15.]) 1np.random.seed(12345)123456789from numpy.linalg import inv, qrX = randn(5, 5)Xmat = X.T.dot(X)matinv(mat)mat.dot(inv(mat))q, r = qr(mat) # QR分解rarray([[-0.5031, -0.6223, -0.9212, -0.7262, 0.2229], [ 0.0513, -1.1577, 0.8167, 0.4336, 1.0107], [ 1.8249, -0.9975, 0.8506, -0.1316, 0.9124], [ 0.1882, 2.1695, -0.1149, 2.0037, 0.0296], [ 0.7953, 0.1181, -0.7485, 0.585 , 0.1527]]) array([[ 4.2538, -1.0645, 1.4407, 0.9898, 1.7318], [-1.0645, 7.4431, -1.5585, 4.4972, -2.1367], [ 1.4407, -1.5585, 2.8126, 0.243 , 1.2786], [ 0.9898, 4.4972, 0.243 , 5.0897, 0.305 ], [ 1.7318, -2.1367, 1.2786, 0.305 , 1.928 ]]) array([[ 0.4057, -0.1875, -0.0764, 0.1229, -0.541 ], [-0.1875, 2.462 , 0.2537, -2.3367, 3.0984], [-0.0764, 0.2537, 0.5435, -0.2369, 0.0268], [ 0.1229, -2.3367, -0.2369, 2.4239, -2.9264], [-0.541 , 3.0984, 0.0268, -2.9264, 4.8837]]) array([[ 1., 0., -0., -0., -0.], [ 0., 1., -0., -0., -0.], [ 0., 0., 1., 0., -0.], [ 0., -0., 0., 1., 0.], [ 0., 0., -0., 0., 1.]]) array([[-5.0281, 2.7734, -2.8428, -1.0619, -3.0078], [ 0. , -8.7212, 1.2925, -6.5614, 1.622 ], [ 0. , 0. , -2.0873, -1.0487, -0.6291], [ 0. , 0. , 0. , -1.408 , -0.955 ], [ 0. , 0. , 0. , 0. , 0.1537]]) 随机数生成12samples = np.random.normal(size=(4, 4))samplesarray([[-0.5196, 1.297 , 0.9062, 0.5809], [ 1.2233, -1.3301, 1.0483, 0.357 ], [-0.7935, -0.406 , -0.0096, -0.596 ], [ 1.3833, -0.2029, -1.0547, -0.9795]]) 1234from random import normalvariateN = 1000000%timeit samples = [normalvariate(0, 1) for _ in range(N)]%timeit np.random.normal(size=N)1 loop, best of 3: 814 ms per loop 10 loops, best of 3: 28.4 ms per loop 可以看出numpy确实要快很多Example: 随机游走123456789import randomposition = 0walk = [position]steps = 1000for i in range(steps): step = 1 if random.randint(0, 1) else -1 position += step walk.append(position)通过numpy来实现上述过程1np.random.seed(12345)1234nsteps = 1000draws = np.random.randint(0, 2, size=nsteps)steps = np.where(draws &gt; 0, 1, -1)walk = steps.cumsum()1234import matplotlib.pyplot as pltindex = [x + 1 for x in range(len(walk))]plt.plot(index, walk)[&lt;matplotlib.lines.Line2D at 0x234014e9f98&gt;] 12walk.min()walk.max()-3 31 1(np.abs(walk) &gt;= 10).argmax() # the first index37 一次模拟多次随机漫步123456nwalks = 5000 # 5000 random walknsteps = 1000draws = np.random.randint(0, 2, size=(nwalks, nsteps)) # 0 or 1steps = np.where(draws &gt; 0, 1, -1)walks = steps.cumsum(1)walksarray([[ -1, 0, -1, ..., 24, 23, 22], [ -1, 0, -1, ..., -36, -37, -36], [ 1, 2, 3, ..., -42, -41, -40], ..., [ 1, 0, -1, ..., 48, 49, 50], [ -1, -2, -3, ..., -38, -39, -40], [ -1, 0, 1, ..., -48, -47, -48]], dtype=int32) 12walks.max()walks.min()130 -117 123hits30 = (np.abs(walks) &gt;= 30).any(1)hits30hits30.sum() # Number that hit 30 or -30array([ True, True, True, ..., True, True, True], dtype=bool) 3412 12crossing_times = (np.abs(walks[hits30]) &gt;= 30).argmax(1)crossing_times.mean()497.04103165298943]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Naive Bayes]]></title>
      <url>%2F2017%2F02%2F25%2FNaive-Bayes%2F</url>
      <content type="text"><![CDATA[利用朴素贝叶斯进行文本分类准备数据从文本中构建词向量下面写一个词表到向量的转换函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def LoadDataSet(): """ Load a vector-like data set tranfered by a data set list that generated by artifical. Returns: return_vec: the vector-like data set. class_vec: the class label corresponds to the data items. """ posting_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] class_vec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not return posting_list, class_vecdef CreateVocabList(data_set): """ Create vocabulary list from vector-like data set. Arguments: data_set: the data source. Returns: vocab_list: the vocabulary list. """ vocab_set = set([]) for document in data_set: vocab_set = vocab_set | set(document) return list(vocab_set)def SetOfWords2Vec(vocab_list, input_set): """ Transfer the words list to vector for each posting. Arguments: vocab_list: The vocabulary list. input_set: The posting that ready to transfer to vector. Returns: return_vec: the result vector. """ # Initialize return_vec = [0] * len(vocab_list) for word in input_set: if word in vocab_list: return_vec[vocab_list.index(word)] = 1 else: print("the word: %s is not in my Vocabulary" % word) return return_vec下面对函数的功能进行测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110In [13]: import bayesIn [14]: list_of_posts, list_classes = bayes.LoadDataSet()In [15]: my_vocab_list = bayes.CreateVocabList(list_of_posts)In [16]: my_vocab_listOut[16]:['help', 'worthless', 'I', 'take', 'love', 'maybe', 'stupid', 'to', 'not', 'please', 'quit', 'park', 'posting', 'dog', 'dalmation', 'steak', 'my', 'how', 'food', 'so', 'stop', 'is', 'garbage', 'flea', 'problems', 'has', 'buying', 'ate', 'him', 'licks', 'mr', 'cute']In [17]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[0])Out[17]:[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]In [18]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[3])Out[18]:[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]看上去一切都work，可以进入下一步了。训练从词向量计算概率我们来看看贝叶斯公式：p(c_i | w) = \frac{p(w | c_i) p(c_i)}{p(w)}这里$w$代表词向量， 可以看出$c_i$的计算十分简单，值得注意的是，根据朴素贝叶斯的假设，有：p(w | c_i) = p(w_0, w_1, w_2, \cdots, w_N | c_i) = p(w_0 | c_i)p(w_1 | c_i)p(w_2 | c_i) \cdots p(w_N | c_i)当需要预测新样本的类别时：这样一切就很清楚了，下面给出伪代码：12345678910计算每个类别的文档数目for 每一篇文档： for 每一个类别： if 词条出现在文档中： 增加该词条的计数值 增加总词条数的计数值 for 每一个类别： for 每一个词条： 将该词条的数目除以总词条数目得到条件概率 返回每个类别的条件概率注意，这里的$p(w_j | c_i)$是要根据整个训练集来算，代码实现如下：123456789101112131415161718192021222324252627282930313233def TrainNaiveBayes0(train_matrix, train_category): """ the training method. Arguments: train_matrix: The train data. train_category: The train label. Returns: p0_vect: The conditional probability of w by c0 p1_vect: The conditional probability of w by c1 p_abusive: The conditional probability of c1 """ num_train_docs = len(train_matrix) num_words = len(train_matrix[0]) p_abusive = sum(train_category) / num_train_docs p0_num = zeros(num_words) p1_num = zeros(num_words) p0_denom = 0.0 p1_denom = 0.0 for i in range(num_train_docs): if train_category[i] == 1: p1_num += train_matrix[i] p1_denom += sum(train_matrix[i]) else: p0_num += train_matrix[i] p0_denom += sum(train_matrix[i]) p1_vect = p1_num / p1_denom p0_vect = p0_num / p0_denom return p0_vect, p1_vect, p_abusive同样进行一下测试：123456789101112131415161718192021222324252627282930In [25]: train_mat = []In [26]: for post_in_doc in list_of_posts: ...: train_mat.append(bayes.SetOfWords2Vec(my_vocab_list, post_in_doc)) ...:In [27]: p0_v, p1_v, p_ab = bayes.TrainNaiveBayes0(train_mat, list_classes)In [28]: p_abOut[28]: 0.5In [29]: p0_vOut[29]:array([ 0.04166667, 0. , 0.04166667, 0. , 0.04166667, 0. , 0. , 0.04166667, 0. , 0.04166667, 0. , 0. , 0. , 0.04166667, 0.04166667, 0.04166667, 0.125 , 0.04166667, 0. , 0.04166667, 0.04166667, 0.04166667, 0. , 0.04166667, 0.04166667, 0.04166667, 0. , 0.04166667, 0.08333333, 0.04166667, 0.04166667, 0.04166667])In [30]: p1_vOut[30]:array([ 0. , 0.10526316, 0. , 0.05263158, 0. , 0.05263158, 0.15789474, 0.05263158, 0.05263158, 0. , 0.05263158, 0.05263158, 0.05263158, 0.10526316, 0. , 0. , 0. , 0. , 0.05263158, 0. , 0.05263158, 0. , 0.05263158, 0. , 0. , 0. , 0.05263158, 0. , 0.05263158, 0. , 0. , 0. ])但是上述代码存在一些缺陷，首先，计算$p(w_j | c_i)$可能会出现结果为0的情况，那么最后的结果就会为0，那么需要进行一些修改 (为什么是2？)：1234p0_num = ones(num_words)p1_num = ones(num_words)p0_denom = 2.0p1_denom = 2.0另外一个就是下溢的问题，所以要改用log函数12p1_vect = log(p1_num / p1_denom)p0_vect = log(p0_num / p0_denom)测试最后写一个分类和测试函数：123456789101112131415161718192021222324252627282930313233343536373839404142def ClassifyNaiveBayes(vec_to_classify, p0_vect, p1_vect, p_abusive): """ Classify. Arguments: vec_to_classify: The vector to classify. p0_vect: The conditional probability of w by c0. p1_vect: The conditional probability of w by c1. p_abusive: The conditional probability of c1. Returns: 0: The predict class is 0 1: The predict class is 1 """ p1 = sum(vec_to_classify * p1_vect) + log(p_abusive) p0 = sum(vec_to_classify * p0_vect) + log(1 - p_abusive) if p1 &gt; p0: return 1 else: return 0def TestNaiveBayes(): """ A test method. """ list_of_posts, list_of_classes = LoadDataSet() my_vocab_list = CreateVocabList(list_of_posts) train_mat = [] for post_in_doc in list_of_posts: train_mat.append(SetOfWords2Vec(my_vocab_list, post_in_doc)) p0_v, p1_v, p_ab = TrainNaiveBayes0( array(train_mat), array(list_of_classes)) test_entry = ['love', 'my', 'dalmation'] this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry)) print(test_entry, 'classified as: ', ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab)) test_entry = ['stupid', 'garbage'] this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry)) print(test_entry, 'classified as: ', ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab))Test123In [32]: bayes.TestNaiveBayes()['love', 'my', 'dalmation'] classified as: 0['stupid', 'garbage'] classified as: 1Ok, bravo!词袋模型现在有一个问题， 到目前为止，我们将每个词是否出现作为特征，这被称为词集模型。但是如果有一个词在文档中不止出现一次，那么就需要词袋模型进行建模。1234567891011121314151617181920def BagOfWords2Vec(vocab_list, input_set): """ Transfer the words list to vector for each posting. Arguments: vocab_list: The vocabulary list. input_set: The posting that ready to transfer to vector. Returns: return_vec: the result vector. """ # Initialize return_vec = [0] * len(vocab_list) for word in input_set: if word in vocab_list: return_vec[vocab_list.index(word)] += 1 else: print("the word: %s is not in my Vocabulary" % word) return return_vec高斯朴素贝叶斯一般的朴素贝叶斯算法的输入特征为离散值，那么当输入变量为连续值时就不能处理了，一般这时候假设输入变量服从一个正态分布，这样$p(w_j | c_i)$就可以计算了，所以整个的流程如下：采用sk-learn进行下实验123456789In [33]: from sklearn import datasets ...: iris = datasets.load_iris() ...: from sklearn.naive_bayes import GaussianNB ...: gnb = GaussianNB() ...: y_pred = gnb.fit(iris.data, iris.target).predict(iris.data) ...: print("Number of mislabeled points out of a total %d points : %d" ...: % (iris.data.shape[0],(iris.target != y_pred).sum())) ...:Number of mislabeled points out of a total 150 points : 6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS231n Lecture2 note]]></title>
      <url>%2F2017%2F02%2F24%2FCS231n-Lecture2-note%2F</url>
      <content type="text"><![CDATA[图像分类目标给一张输入图片赋予一个标签， 这个标签属于事先定义好的类别集合中地位计算机视觉的核心问题例子难点拍摄点的视角多样拍摄点的远近距离多样物体的变形物体部分遮挡光线背景相似品种多样方法数据驱动（即包含训练数据）pipeline输入 -&gt; 学习 -&gt; 评估最近邻分类器距离度量L1 distanced_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|L2 distanced_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}示例代码数据读取1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide# flatten out all images to be one-dimensionalXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072预测及评估123456nn = NearestNeighbor() # create a Nearest Neighbor classifier classnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labelsYte_predict = nn.predict(Xte_rows) # predict labels on the test images# and now print the classification accuracy, which is the average number# of examples that are correctly predicted (i.e. label matches)print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )基本方法123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypred实验结果L1-distance 38.6% on CIFAR-10L2-distance 35.4% on CIFAR-10L1 vs. L2 L2比L1的差异容忍度更小K近邻分类器采用验证集进行超参调参实例代码123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc))交叉验证最近邻分类器的优缺点优点训练速度快缺点测试速度很慢​ 解决方法：1. ANN 2. FANN距离度量不合适​(http://cs231n.github.io/assets/pixels_embed_cifar10_big.jpg)接下来…t-SNE http://lvdmaaten.github.io/tsne/random projection http://scikit-learn.org/stable/modules/random_projection.htmlINTUITION FAILS IN HIGH DIMENSIONS http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdfRecognizing and Learning Object Categories http://people.csail.mit.edu/torralba/shortCourseRLOC/index.html线性分类一个从图像到标签的映射函数f(x_i, W, b) = W x_i + bx shape is [D x 1], W shape is [K x D], b shape is [K x 1]注意点：W代表K个分类器的参数放在一起，因此整个模型是K个分类器的一个整合向量化能够大大提升计算速度线性分类器的解释权重W表示不同的标签对于图像不同位置不同颜色的重视程度。比如太阳可能对于圆形的区域以及黄颜色比较看重将图像看成高维空间中的点将线性分类器看成模板匹配将W的每一行看成一个模板，通过内积计算，每一张图片张成的列向量都与每一个模板作比较，最后选出最匹配的，这也是一种最近邻算法。从上图可以看出，这里的模板是各个图像的一种折中。将bias项放入W中数据预处理​ 数据中心化]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Qiniu cloud images batch upload and directory synchronization]]></title>
      <url>%2F2017%2F02%2F24%2FQiniu-cloud-images-batch-upload-and-directory-synchronization%2F</url>
      <content type="text"><![CDATA[最近写博客的时候会用到图片，因此用了七牛云的图片外链功能，但是其内容管理不能创建目录，所以图片的命名以及上传都很麻烦，然后去网上查了一下，也看了一下官方文档，发现官方有一个批量上传的工具挺好用，所以记录一下，下面把官方文档中的一些东西贴出来，方便日后查阅。​]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Correlation Analysis]]></title>
      <url>%2F2017%2F02%2F24%2FCorrelation-Analysis%2F</url>
      <content type="text"><![CDATA[问题描述最近有一个关于关联分析的小作业，问题描述如下：孩子的情感状况、品行症状、亲社会行为等等与很多因素相关，这个作业主要着眼点在于孩子每天看视频的时间是否对于以上这些指标有着重要的影响，因此需要对输入特征与输出标签之间的相关关系做一个分析。属性描述如下：由以上属性描述表可以看出，属性类型同时包括类别以及数值，因此进行相关分析时针对这两种情况需要采用不同的方法，这里采用的策略如下：对于离散-离散的情况，采用卡方检验对于连续-离散的情况，采用ANOVA方差检验下面对于这两种检验方法进行介绍。卡方检验一维情况假设一条河里有三种鱼，gumpies, sticklebarbs, 以及spotheads。如果这条河的生态环境没有遭到干扰，那么这三种鱼的数量是相等的（下表第三行）。现在从河里进行300次抽样，最后抽样得到的结果如下表第二行所示：gumpiessticklebarbsspothheadsTotals观察到的频数8912091300期望的频数100100100300现在需要解决的问题是，是否这条河的生态环境收到了干扰？我们假设生态环境正常，这是我们的原假设。（注意，期望的频数是根据实际情况自行定义的）很容易想到的是，我们可以建立一种度量来衡量现实情况与原假设的偏离程度，例如：\frac{观察值 - 期望值}{期望值}把实际的数据带入，可以得到以下的结果：gumpies: $\frac{89 - 100}{100} = -0.11$sticklebarbs: $\frac{120 - 100}{100} = +0.20$spothheads: $\frac{91 - 100}{100} = -0.09$发现结果还不错，但是这只能用来衡量单个类别的偏差程度，而不能用来衡量整体的偏差程度，因为这三者的加和为零。既然这样，很容易想到可以对之前的度量进行简单的修改，变成这样：\frac{(观察值 - 期望值)^2}{期望值}再把数据带入看看：gumpies: $\frac{(89 - 100)^2}{100} = 1.21$sticklebarbs: $\frac{(120 - 100)^2}{100} = 4.0$spothheads: $\frac{(91 - 100)}{100} = 0.81$sum: $1.21 + 4.0 + 0.81 = 6.02$这样一来问题便得到了解决，而这也正是卡方检验所采取的方式。而这个sum值就是卡方（chi-square），记作$\chi^2$， 为了更加形式化地表示，我们把观察值记为$O$， 期望值记为$E$， 那么有如下等式成立：\chi^2 = \sum \frac{(O - E) ^ 2}{E}下面的问题是，我们达到了卡方值，但是这个值到底好还是不好呢？是更加支持原假设还是拒绝原假设呢？可以设想这样一种情况，我们假设河里的鱼服从原假设的分布，也就是三种鱼出现的概率相等。然后我们把三百次采样看作一次实验，每一次实验完毕之后记录下采样出的300条鱼中每一种鱼的频数，然后计算卡方值。在进行很多次这样的实验之后，我们可以画一个柱状图，这个图记录下了卡方值的分布情况。然后再把实际的观察值（也就是上面表格的第二行）计算的卡方值（6.02）带入进去，看看大于或者等于这个值在柱状图中所有卡方值中占有多少比例。如果占有的比例很大，说明这个值是由跟原假设很近似的假设得出的，这就证明了原假设是对的；反之。如果这个比例很小，说明如果分布服从原假设，那么所计算出的卡方值基本不可能包含这个观测出的卡方值，表明原假设是不对的，我们就可以拒绝原假设。其实统计检验的基本思想就是这样。但是存在一个问题，我们不可能进行真实的采样（从河里抓鱼），所以一般采用计算机模拟的方式，具体步骤如下所示：等概率的产生a（代表gumpies）, b（代表sticklebarbs）, c（代表spothheads）的序列计算一个大小为300的序列中a, b, c三者的频数，作为观察值，然后将a = b = c = 100作为期望值，计算并记录下算出的卡方值重复1~2步10000次， 画出柱状图如下：可以看出只有5%左右的值大于6.02，说明我们可以以95%的置信度拒绝原假设。二维情况对于二维的情况，卡方检验又被称为卡方关联度检验，也就是检验两个变量之间的相关程度（独立程度），考虑下面这个数据表$O$：Alzhemer’s onset -during 5-year periodnoyesrecieved-yes1479156estrogenno8101589689571671124这是观察值，为了计算卡方值，很明显我们需要计算期望值。为了方便表示，把上表变成如下形式：（A）Alzhemer’s onset -during 5-year periodnoyes(R) recieved-yes[cell a][cell b]156estrogenno[cell c][cell d]9689571671124拿a做例子：E_a = \frac{156}{1124} \times \frac{957}{1124} \times 1124这个式子如何解释呢？如果这两个变量是独立的，那么$A$变量取$no$值与$R$变量取$yes$值这两个事件之间就是独立的，那么$[cell_a]$事件发生的概率就是两者相乘，也就是上述等式右边前两个数相乘，最后的期望值自然就是概率乘以实验总数。上面的解释比较不正式，换一种较为正式的表达方式。假设我们要求$cell$的期望值，设$R$为$cell$所在列的边缘事件总数，$C$为$cell$所在行的边缘事件总数，$N$为实验总数目，这样就有：E_{cell} =\frac{R}{N} \times \frac{C}{N} \times N所以就有期望值$E$数据表如下：（A）Alzhemer’s onset -during 5-year periodnoyes(R) recieved-yes$E_a = \frac{156 \times 957}{1124} = 132.82$$E_b = \frac{156 \times 167}{1124} = 23.18$156estrogenno$E_c = \frac{968 \times 957}{1124} = 824.18$$E_d = \frac{968 \times 167}{1124} = 143.82$9689571671124这样就可以调用公式：\chi^2 = \sum \frac{(O - E) ^ 2}{E}特别地，当行数以及列数都为2时，上述公式需要进行一下修改：\chi^2 = \sum \frac{(|O - E| - 0.5) ^ 2}{E}这样算出来的卡方值为$11.01$最后涉及到自由度的问题，因为比较简单，所以只写出结论：df = (r - 1)(c - 1)r = number of rowsc = number of columnsANOVA方差检验为了方便比较，同样采用上述阿尔兹海默病的例子。研究表明，老年痴呆症患者患病之后会经常经历情绪非常不稳定地阶段，原因是因为患者患病之前的生活中经常有恐惧或者焦虑的体验，正是这些一直存在于脑海中的记忆出发了患病后的不稳定情绪的产生。现在我们假设有一个实验团队发明了一种药物，可以缓解这种情绪问题，他们对小白鼠进行了实验。实验设计如下：将小白鼠随机分为四组A， B， C， DA组作为参照组，不给药；B， C， D三组分别注射一个单位，两个单位，三个单位的药剂记录实验结果，数值越低表明实验效果越好实验结果如下：ABCDTotal27.022.821.923.526.223.123.419.628.827.720.123.733.527.627.820.828.824.019.323.9$M_a = 28.86$$M_b = 25.04$$M_c = 22.50$$M_d = 22.30$$M_T = 24.68$下面需要进行一下相关性分析，判断药物是否对症状的缓解产生作用。和卡方检验一样，ANOVA检验最后也有一个衡量指标，记为$F$，定义如下：F = \frac{MS_{bg}}{MS_{wg}} = \frac{组间相似度}{组内相似度}具体的计算步骤如下所示（推导过程省略）（1） 首先计算出如下值：ABCDTotal$N_A = 5$$N_B = 5$$N_C = 5$$N_D = 5$$N_T = 20$$\sum X_{Ai} = 144.30$$\sum X_{Bi} = 125.20$$\sum X_{Ci} = 112.50$$\sum X_{Di} = 111.50$$\sum X_{Ti} = 493.50$$\sum X^2_{Ai} = 4196.57$$\sum X^2_{Bi} = 3158.50$$\sum X^2_{Ci} = 2576.51$$\sum X^2_{Di} = 2501.95$$\sum X^2_{Ti} = 12433.53$$SS_A = 32.07$$SS_B = 23.49$$SS_C = 45.26$$SS_D = 15.50$$SS_T = 256.42$其中：SS = \sum X^2_i - \frac{(\sum X_i)^2}{N}（2）计算$SS{wg}$以及$SS{bg}$SS_{wg} = SS_A + SS_B + SS_C + SS_DSS_{bg} = SS_T - SS_{wg}（4）计算相关自由度df_{bg} = k - 1 = 4 - 1 = 3df_{wg} = (N_A - 1) + (N_B - 1) + (N_C - 1) + (N_D - 1)（5）计算 $MS{bg}$以及$MS{wg}$MS_{bg} = \frac{SS_{bg}}{df_{bg}}MS_{wg} = \frac{SS_{wg}}{df_{wg}}（6）计算$F$最后得出F = 6.42 （df = 3, 16）代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269import osimport xlrdimport numpy as npimport scipy.stats as statsdef GetChildDetailLab(combine=True): """ Get the detail labs of each child data. Returns: child_detail_lab: the detail labs. """ CHILDS_FEATURE_LAB = ['tid', 'age_m', 'female', 'onlychild', 'divorce', 'medu_newcat', 'income_newcat', 'scr_ave', 'edu_ave', 'edu_of_scr', 'scr_h_cat', 'mediacoview', 'mediacontact'] CHILDS_CAT_LAB = ['emo_cat', 'con_cat', 'hyp_cat', 'pee_cat', 'difficulties_cat', 'pro_cat'] if combine: CHILDS_DETAIL_LAB = CHILDS_FEATURE_LAB CHILDS_DETAIL_LAB.extend(CHILDS_CAT_LAB) return CHILDS_DETAIL_LAB else: return CHILDS_FEATURE_LAB, CHILDS_CAT_LABdef ReadChildInfoFromExcel( file_name='屏幕暴露与SDQ.xlsx', sheet_name='data'): """ Read the screen-exposed vs.SDQ detail information of each child from the excel-type file. Arguments: file_name: the name of the excel-type file. sheet_name: the name of the sheet of the excel file. Returns: child_scr_exp_sdq: A list that contains the detail information of each child. labs: The lab corresponds to each colume of the data. """ CHILDS_FILE_NAME = 'child_scr_exp_sdq.npy' CHILDS_DETAIL_LAB = GetChildDetailLab() # print(CHILDS_DETAIL_LAB) NOT_INT_LAB_INDEIES = [CHILDS_DETAIL_LAB.index('age_m'), CHILDS_DETAIL_LAB.index('scr_ave'), CHILDS_DETAIL_LAB.index('edu_ave'), CHILDS_DETAIL_LAB.index('edu_of_scr')] child_scr_exp_sdq = [] if (os.path.isfile(CHILDS_FILE_NAME)): with open(CHILDS_FILE_NAME, 'rb') as f: child_scr_exp_sdq = np.load(f) else: workBook = xlrd.open_workbook(file_name) bookSheet = workBook.sheet_by_name(sheet_name) # read from second row because of the first row has tabs for row in range(1, bookSheet.nrows): child_row = [] for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = str(cel.value) # tolerant the value error if val == '': val = '-1.0' except Exception as e: print(e) # because of the type is different try: if col in NOT_INT_LAB_INDEIES: val = float(val) else: # in Excel, if cel.value is 1, then str(cel.value) is # '1.0' val = val.split('.')[0] val = int(val) except Exception as e: print(e) val = -1 child_row.append(val) child_scr_exp_sdq.append(child_row) child_scr_exp_sdq = np.array(child_scr_exp_sdq) with open(CHILDS_FILE_NAME, 'wb') as f: np.save(f, child_scr_exp_sdq) return child_scr_exp_sdq, CHILDS_DETAIL_LABdef SplitDataSet(data_set, feature, cat): """ Split the data set to two column, that is feature and cat Arguments: data_set: the source data set. feature: the input vector cat: the correspond category. Returns: splited_data_set: self-explation. """ CHILDS_DETAIL_LAB = GetChildDetailLab() feature_index = CHILDS_DETAIL_LAB.index(feature) cat_index = CHILDS_DETAIL_LAB.index(cat) # print(feature_index, cat_index) return data_set[:, (feature_index, cat_index)]def CalChi2(data_set): """ Calculate the chi-square value and p-value corresponds to the data set. Arguments: data_set: the object data set. Returns: chi2: the chi-square value. p: the p-value. """ rows_number = len(set(data_set[:, -1])) columns_number = len(set(data_set[:, 0])) # print(rows_number, columns_number) counts = np.zeros((rows_number, columns_number)) for row in data_set: if row[-1] != -1: if row[0] != -1: try: counts[int(row[-1])][int(row[0])] += 1 except: pass # drop the row that all item is 0 del_row_index = [] for index, count in enumerate(counts): if not count.any(): del_row_index.append(index) counts = np.delete(counts, tuple(del_row_index), axis=0) # drop the column that all item is 0 del_col_index = [] for index, count in enumerate(counts.T): if not count.any(): del_col_index.append(index) counts = np.delete(counts, tuple(del_col_index), axis=1) # print(counts) # calculate the chi-square value and correspond p-value chi2, p, dof, excepted = stats.chi2_contingency(counts) return chi2, pdef ANOVATest(data_set): """ Implement a ANOVA test on the data set. Arguments: data_set: the object data set. Return: f: The computed F-value of the test. p: The associated p-value from the F-distribution. """ # Initial the three categories normal = [] critical = [] abnormal = [] for data in data_set: if data[0] != -1: if data[-1] == 0: normal.append(data[0]) elif data[-1] == 1: critical.append(data[0]) elif data[-1] == 2: abnormal.append(data[0]) f, p = stats.f_oneway(normal, critical, abnormal) return f, pdef GenerateCoffMatrix(data_set): """ Calculate the chi-square and p-value of each feature-category pair. Arguments: data_set: the source data set. Returns: coff_matrix: the final cofficient matrix. """ coff_matrix = &#123;&#125; CHILDS_FEATURE_LAB, CHILDS_CAT_LAB = GetChildDetailLab(combine=False) NOT_KEEP_FEATURE_LAB = ['tid', 'age_m', 'scr_ave', 'edu_ave', 'edu_of_scr'] for feature in CHILDS_FEATURE_LAB: if feature in NOT_KEEP_FEATURE_LAB: if feature != NOT_KEEP_FEATURE_LAB[0]: for cat in CHILDS_CAT_LAB: splited_data_set = SplitDataSet(data_set, feature, cat) # print(feature, cat) f, p = ANOVATest(splited_data_set) key = feature + '-' + cat coff_matrix[key] = (f, p) else: for cat in CHILDS_CAT_LAB: splited_data_set = SplitDataSet(data_set, feature, cat) # print(feature, cat) chi2, p = CalChi2(splited_data_set) key = feature + '-' + cat coff_matrix[key] = (chi2, p) return coff_matrixdef SiftRelativeFeature(coff_matrix, conf=1e-5): """ Sift the feature that satisfy the caonfident condition. Arguemnts: coff_matrix: the calculated cofficient matrix for all features and categories. conf: the confident. Returns: relative_feature_matrix: the satisfied feature and correspond category and chi2 and p-value. """ relative_feature_matrix = &#123;&#125; for key in coff_matrix.keys(): if coff_matrix[key][-1] &lt;= conf: relative_feature_matrix[key] = coff_matrix[key] return relative_feature_matrixdef WriteResult(coff_matrix, file_name='result.txt'): """ Write the result to file. Arguments: relative_feature_matrix: the satisfied feature and correspond category and chi2 and p-value. file_name: the result file name. """ # Sort sorted_coff_matrix = sorted(coff_matrix.items(), key=lambda item: item[1][-1], reverse=False) # print(sorted_coff_matrix) with open(file_name, 'w') as f: for item in sorted_coff_matrix: f.write(str(item)) f.write('\n')if __name__ == '__main__': data_set, labels = ReadChildInfoFromExcel() # splited_data_set = SplitDataSet(data_set, 'female', 'difficulties_cat') # chi2, p = CalChi2(splited_data_set) coff_matrix = GenerateCoffMatrix(data_set) # relative_feature = SiftRelativeFeature(coff_matrix, 1) WriteResult(coff_matrix)因子分析先说说因子分析与主成分分析的区别，下面的话引自知乎(https://www.zhihu.com/question/24524693)具体实现使用SPSS软件进行实现：操作步骤如下(引自 SPSS数据分析从入门到精通-陈胜可)：实验结果如下：2017.2.27 Updates决策树1234567import pandas as pdfrom sklearn import treeimport numpy as npfrom sklearn.model_selection import cross_val_scorefrom IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"Read data1data = pd.read_excel('./scr_SDQ.xlsx', sheetname='data', index_col=0).dropna().sort_index()Convert the data format1234float_columns = ['scr_ave', 'edu_ave','scr_of_edu']for column in data.columns: if column not in float_columns: data[column] = data[column].astype(int)Split the data123data_feature_with_edu_ave = data.ix[:, :'mediacontact'].drop('scr_of_edu', axis=1)data_feature_with_scr_of_edu = data.ix[:, :'mediacontact'].drop('edu_ave', axis=1)data_classes = data.ix[:, 'emo_cat':]12# dtree = tree.DecisionTreeClassifier(min_samples_leaf=500)# cross_val_score(dtree,data_feature_with_edu_ave, data_classes['difficulties_cat'], cv=10)array([ 0.64356436, 0.64356436, 0.64391855, 0.64391855, 0.64407713, 0.64407713, 0.64407713, 0.64407713, 0.64407713, 0.64407713]) difficulties_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['difficulties_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['difficulties_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_diff.pngwith open('dtree_with_edu_ave_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_diff.pngwith open('dtree_with_scr_of_edu_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.553782medu_newcat0.217218age_m0.085820income_newcat0.048912edu_ave0.029740female0.026161onlychild0.021336mediacontact0.017030divorce0.000000scr_h_cat0.000000mediacoview0.000000Impscr_ave0.525587medu_newcat0.213067age_m0.084708scr_of_edu0.080575mediacontact0.029232female0.025661onlychild0.020929income_newcat0.020240divorce0.000000scr_h_cat0.000000mediacoview0.000000emo_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['emo_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['emo_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_emo.pngwith open('dtree_with_edu_ave_emo.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_emo.pngwith open('dtree_with_scr_of_edu_emo.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.521694medu_newcat0.159798income_newcat0.147978edu_ave0.083233age_m0.040750female0.023781mediacontact0.019127mediacoview0.003639onlychild0.000000divorce0.000000scr_h_cat0.000000Impscr_ave0.520711income_newcat0.155111medu_newcat0.146762scr_of_edu0.115745age_m0.023767female0.021924mediacontact0.011410mediacoview0.004571onlychild0.000000divorce0.000000scr_h_cat0.000000con_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['con_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['con_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_con.pngwith open('dtree_with_edu_ave_con.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_con.pngwith open('dtree_with_scr_of_edu_con.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.562325medu_newcat0.098819mediacontact0.095089edu_ave0.077572female0.064213income_newcat0.056334age_m0.023696onlychild0.013513mediacoview0.008440divorce0.000000scr_h_cat0.000000Impscr_ave0.511282scr_of_edu0.110155medu_newcat0.097473mediacontact0.093793female0.082843income_newcat0.055566age_m0.035552onlychild0.013335divorce0.000000scr_h_cat0.000000mediacoview0.000000hyp_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['hyp_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['hyp_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_hyp.pngwith open('dtree_with_edu_ave_hyp.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_hyp.pngwith open('dtree_with_scr_of_edu_hyp.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impmedu_newcat0.343448scr_ave0.265652onlychild0.165487edu_ave0.076194income_newcat0.054466age_m0.046774mediacontact0.020992mediacoview0.015532female0.011454divorce0.000000scr_h_cat0.000000Impmedu_newcat0.335192scr_ave0.257218onlychild0.161509scr_of_edu0.105471age_m0.065618income_newcat0.033129mediacontact0.022542mediacoview0.012901female0.006421divorce0.000000scr_h_cat0.000000pee_cat1234567891011121314151617dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['pee_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['pee_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_pee.pngwith open('dtree_with_edu_ave_pee.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_pee.pngwith open('dtree_with_scr_of_edu_pee.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.441246income_newcat0.210646female0.152568age_m0.151888medu_newcat0.033530mediacoview0.009625edu_ave0.000498onlychild0.000000divorce0.000000scr_h_cat0.000000mediacontact0.000000Impscr_ave0.430946income_newcat0.200188age_m0.151286female0.139104medu_newcat0.033289scr_of_edu0.033259mediacoview0.011927onlychild0.000000divorce0.000000scr_h_cat0.000000mediacontact0.000000pro_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['pro_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['pro_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_pro.pngwith open('dtree_with_edu_ave_pro.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_pro.pngwith open('dtree_with_scr_of_edu_pro.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impfemale0.295389scr_ave0.252091mediacontact0.145010age_m0.137453income_newcat0.085421edu_ave0.062697medu_newcat0.021938onlychild0.000000divorce0.000000scr_h_cat0.000000mediacoview0.000000Impfemale0.299316scr_ave0.204496mediacontact0.131972age_m0.125169scr_of_edu0.111909income_newcat0.080072medu_newcat0.047066onlychild0.000000divorce0.000000scr_h_cat0.000000mediacoview0.000000Convert continuous variables to categorical variables1234data_category = data.copy()for column in data_category.columns: if column in float_columns: data_category[column] = pd.cut(data_category[column], 10, labels=np.arange(10))123data_cate_feature_with_edu_ave = data_category.ix[:, :'mediacontact'].drop('scr_of_edu', axis=1)data_cate_feature_with_scr_of_edu = data_category.ix[:, :'mediacontact'].drop('edu_ave', axis=1)data_cate_classes = data.ix[:, 'emo_cat':]12dtree_cate = tree.DecisionTreeClassifier(min_samples_leaf=50)cross_val_score(dtree_cate,data_cate_feature_with_edu_ave, data_cate_classes['difficulties_cat'], cv=10).sum() / 100.63948514409153423 12345678910111213141516dtree_cate_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_cate_with_edu_ave = dtree_with_edu_ave.fit(data_cate_feature_with_edu_ave, data_cate_classes['difficulties_cat'])pd.DataFrame(dtree_cate_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_cate_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_cate_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_cate_with_scr_of_edu = dtree_cate_with_scr_of_edu.fit(data_cate_feature_with_scr_of_edu, data_cate_classes['difficulties_cat'])pd.DataFrame(dtree_cate_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_cate_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_cate_with_edu_ave_diff.pngwith open('dtree_cate_with_edu_ave_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_cate_with_edu_ave, out_file=dot_file, feature_names=data_cate_feature_with_edu_ave.columns)# dtree_cate_with_scr_of_edu_diff.pngwith open('dtree_cate_with_scr_of_edu_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_cate_with_scr_of_edu, out_file=dot_file, feature_names=data_cate_feature_with_scr_of_edu.columns)Impscr_h_cat0.528976medu_newcat0.221779age_m0.092168income_newcat0.057032onlychild0.034724mediacontact0.027193female0.021036scr_ave0.011653mediacoview0.005439divorce0.000000edu_ave0.000000Impscr_h_cat0.507678medu_newcat0.215210age_m0.089438scr_of_edu0.042832mediacontact0.041463income_newcat0.035742onlychild0.033695female0.020413scr_ave0.013527divorce0.000000mediacoview0.000000]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Get Pois uses God-map apis]]></title>
      <url>%2F2017%2F02%2F21%2FGet-Pois-uses-God-map-apis%2F</url>
      <content type="text"><![CDATA[伪代码如下：12345从Excel文件中读出数据对于每一个house: 提取出其location字段（经纬度） 将location字段作为输入参数传给map api 将返回值进行适当筛选最后存入原数据集中代码实现如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import osimport xlrdimport pickleimport requestsdef ReadHousesInfoFromExcel( file_name='houses_nadrop.xls', sheet_name='小区信息'): """ Read the houses detail information from the excel-type file. Arguments: file_name: the name of the excel-type file. sheet_name: the name of the sheet of the excel file. Returns: houses: A dict that contains the detail information of each house. """ HOUSES_FILE_NAME = 'houses.pkl' HOUSES_DETAIL_TAB = ['name', 'address', 'property_category', 'area', 'avg_price', 'location', 'property_costs', 'volume_rate', 'green_rate'] houses = [] if (os.path.isfile(HOUSES_FILE_NAME)): with open(HOUSES_FILE_NAME, 'rb') as f: houses = pickle.load(f) else: workBook = xlrd.open_workbook(file_name) bookSheet = workBook.sheet_by_name(sheet_name) # read from second row because of the first row has tabs for row in range(1, bookSheet.nrows): house = &#123;&#125; for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = cel.value except: pass val = str(val) house[HOUSES_DETAIL_TAB[col]] = val houses.append(house) with open(HOUSES_FILE_NAME, 'wb') as f: pickle.dump(houses, f) return housesdef Geocode(location, poi_type): """ A tool that call the God-Map api. Arguments: location: The location of house. poi_type: The poi type. Returns: answer: The JSON-type data that contains pois infomation. """ location = str(location).strip() parameters = &#123;'location': location, 'key': 'e798a5bfb344a09977b79552ae415974', 'types': poi_type, 'offset': 10, 'page': 1, 'extensions': 'base'&#125; base = 'http://restapi.amap.com/v3/place/around' try: response = requests.get(base, parameters) answer = response.json() except Exception as e: print('error!', e) answer = 'null' finally: pass return answerdef GetPOI(houses): """ Get the pois information of the houses according to the location. Arguments: houses: The house detail information. Returns: houses_with_pois: The house detail information that contains the pois information. """ POI_TYPE_LAB = ['subway_station', 'bus_station', 'parking_lot', 'primary_school', 'secondary_school', 'university', 'mall', 'park'] POI_TYPE_CODE = ['150500', '150700', '150904', '141203', '141202', '141201', '060100', '110101'] KEEP_INFO_LAB = ['name', 'location', 'distance'] NO_INFO_NOW = '-' SIZE = len(houses) houses_with_pois = houses.copy() count = 0 for house in houses_with_pois: count = count + 1 if count % 100 == 0: print(count, '', SIZE) house['pois'] = &#123;&#125; for poi_type_index in range(len(POI_TYPE_LAB)): poi_info_json = Geocode(house['location'], POI_TYPE_CODE[poi_type_index]) if poi_info_json == 'null' or poi_info_json['pois'] is None: house['pois'][POI_TYPE_LAB[poi_type_index]] = NO_INFO_NOW else: house['pois'][POI_TYPE_LAB[poi_type_index]] = [] for poi in poi_info_json['pois']: pois_without_useless = &#123;&#125; for key in poi.keys(): if key in KEEP_INFO_LAB: pois_without_useless[key] = poi[key] house['pois'][POI_TYPE_LAB[poi_type_index]].append( pois_without_useless) # return houses_with_pois return houses_with_poisif __name__ == '__main__': houses = ReadHousesInfoFromExcel() # answer = Geocode(houses[0]['location'], '150905') houses_with_pois = GetPOI(houses)总结一下有几个注意点：传给parameters的location参数的格式一定要规范，前后都不能有空格for循环中不能改变字典的大小，这里的大小不仅指其元素的数目，也包括其总占用空间的大小注意pickle的用法从Excel中读出的内容要转成str格式整个过程十分清晰明了，值得注意的是细节问题]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis-Learning note-Ch02]]></title>
      <url>%2F2017%2F02%2F19%2FPython%20data%20analysis-Learning%20notes-ch02%2F</url>
      <content type="text"><![CDATA[利用Python内置的JSON模块对数据进行解析并转化为字典数据如下：123456'&#123; "a": "Mozilla\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\/535.11 (KHTML, like Gecko)Chrome\\/17.0.963.78 Safari\\/535.11", "c": "US", "nk": 1, "tz": "America\\/New_York", "gr":"MA", "g": "A6qOVH", "h": "wfLQtf", "l": "orofrog", "al": "en-US,en;q=0.8", "hh": "1.usa.gov","r": "http:\\/\\/www.facebook.com\\/l\\/7AQEFzjSi\\/1.usa.gov\\/wfLQtf", "u":"http:\\/\\/www.ncbi.nlm.nih.gov\\/pubmed\\/22415991", "t": 1331923247, "hc": 1331822918,"cy": "Danvers", "ll": [ 42.576698, -70.954903 ] &#125;\n'核心代码：123import jsonpath = 'ch02/usagov_bitly_data2012-03-16-1331923249.txt'records = [json.loads(line) for line in open(path)]123456789101112131415161718records[0]-----------------------------------&#123;'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11', 'al': 'en-US,en;q=0.8', 'c': 'US', 'cy': 'Danvers', 'g': 'A6qOVH', 'gr': 'MA', 'h': 'wfLQtf', 'hc': 1331822918, 'hh': '1.usa.gov', 'l': 'orofrog', 'll': [42.576698, -70.954903], 'nk': 1, 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf', 't': 1331923247, 'tz': 'America/New_York', 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991'&#125;对时区字段进行计数（pure python vs. pandas）首先从记录中提取时区字段并且放入一个列表中1time_zones = [rec['tz'] for rec in records if 'tz' in rec]123456789101112time_zones[:10]-----------------------------------['America/New_York', 'America/Denver', 'America/New_York', 'America/Sao_Paulo', 'America/New_York', 'America/New_York', 'Europe/Warsaw', '', '', '']使用纯粹的python进行计数12345678def get_counts(sequence): counts = &#123;&#125; for x in sequence: if x in counts: counts[x] += 1 else: counts[x] = 1 return counts使用下列方法更加简洁1234567from collections import defaultdictdef get_counts2(sequence): counts = defaultdict(int) # values will initialize to 0 for x in sequence: counts[x] += 1 return counts如果需要返回前十位的时区及其计数值1234def top_counts(count_dict, n=10): value_key_pairs = [(count, tz) for tz, count in count_dict.items()] value_key_pairs.sort() return value_key_pairs[-n:]123456789101112top_counts(counts)--------------------------------------[(33, 'America/Sao_Paulo'), (35, 'Europe/Madrid'), (36, 'Pacific/Honolulu'), (37, 'Asia/Tokyo'), (74, 'Europe/London'), (191, 'America/Denver'), (382, 'America/Los_Angeles'), (400, 'America/Chicago'), (521, ''), (1251, 'America/New_York')]可以使用python自带的库1from collections import Counter1counts = Counter(time_zones)123456789101112counts.most_common(10)--------------------------------[('America/New_York', 1251), ('', 521), ('America/Chicago', 400), ('America/Los_Angeles', 382), ('America/Denver', 191), ('Europe/London', 74), ('Asia/Tokyo', 37), ('Pacific/Honolulu', 36), ('Europe/Madrid', 35), ('America/Sao_Paulo', 33)]使用pandas进行相同的任务pandas中主要的数据结构是DataFrame， 作用是将数据表示成表格12345from pandas import DataFrame, Seriesimport pandas as pdframe = DataFrame(records)frame12345678910111213frame['tz'][:10]-------------------------------0 America/New_York1 America/Denver2 America/New_York3 America/Sao_Paulo4 America/New_York5 America/New_York6 Europe/Warsaw7 8 9 Name: tz, dtype: object计数·1234567891011121314tz_counts = frame['tz'].value_counts()tz_counts[:10]--------------------------------------------------America/New_York 1251 521America/Chicago 400America/Los_Angeles 382America/Denver 191Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35America/Sao_Paulo 33Name: tz, dtype: int64填补缺失值以及未知值12345678910111213141516clean_tz = frame['tz'].fillna('Missing')clean_tz[clean_tz == ''] = 'Unknown'tz_counts = clean_tz.value_counts()tz_counts[:10]----------------------------------------------America/New_York 1251Unknown 521America/Chicago 400America/Los_Angeles 382America/Denver 191Missing 120Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35Name: tz, dtype: int64画个图展示一下12plt.figure(figsize=(10, 4))tz_counts[:10].plot(kind='barh', rot=0)下面我们对用户使用的浏览器的信息做一些操作Series应该代表的是DataFrame中的一列123456789results = Series([x.split()[0] for x in frame.a.dropna()])results[:5]---------------------------------------------------0 Mozilla/5.01 GoogleMaps/RochesterNY2 Mozilla/4.03 Mozilla/5.04 Mozilla/5.0dtype: object同样可以进行计数1234567891011results.value_counts()[:8]-----------------------------------------Mozilla/5.0 2594Mozilla/4.0 601GoogleMaps/RochesterNY 121Opera/9.80 34TEST_INTERNET_AGENT 24GoogleProducer 21Mozilla/6.0 5BlackBerry8520/5.0.0.681 4dtype: int64根据Windows和Non-Windows用户进行时区的分组操作1cframe = frame[frame.a.notnull()]123456operating_system = np.where(cframe['a'].str.contains('Windows'), 'Windows', 'Not Windows')operating_system[:5]-----------------------------------------------------------------array(['Windows', 'Not Windows', 'Windows', 'Not Windows', 'Windows'], dtype='&lt;U11')1by_tz_os = cframe.groupby(['tz', operating_system])来看看这个by_tz_os长什么样1by_tz_os.size()再来看看unstack()的炫酷效果排下序， 看看排名多少12345678910111213141516# Use to sort in ascending orderindexer = agg_counts.sum(1).argsort()indexer[:10]------------------------------------------------tz 24Africa/Cairo 20Africa/Casablanca 21Africa/Ceuta 92Africa/Johannesburg 87Africa/Lusaka 53America/Anchorage 54America/Argentina/Buenos_Aires 57America/Argentina/Cordoba 26America/Argentina/Mendoza 55dtype: int64取出前十的来看看12count_subset = agg_counts.take(indexer)[-10:]count_subset同样画个图1count_subset.plot(kind='barh', stacked=True)看看两个类别所占的比例是多少12normed_subset = count_subset.div(count_subset.sum(1), axis=0)normed_subset.plot(kind='barh', stacked=True)电影评分数据表连接操作123456789101112131415import pandas as pdimport osencoding = 'latin1'upath = os.path.expanduser('ch02/movielens/users.dat')rpath = os.path.expanduser('ch02/movielens/ratings.dat')mpath = os.path.expanduser('ch02/movielens/movies.dat')unames = ['user_id', 'gender', 'age', 'occupation', 'zip']rnames = ['user_id', 'movie_id', 'rating', 'timestamp']mnames = ['movie_id', 'title', 'genres']users = pd.read_csv(upath, sep='::', header=None, names=unames, encoding=encoding)ratings = pd.read_csv(rpath, sep='::', header=None, names=rnames, encoding=encoding)movies = pd.read_csv(mpath, sep='::', header=None, names=mnames, encoding=encoding)看看数据长什么样1users[:5]1ratings[:5]1movies[:5]多表连接12data = pd.merge(pd.merge(ratings, users), movies)data12345678910111213data.ix[0]--------------------------------------------user_id 1movie_id 1193rating 5timestamp 978300760gender Fage 1occupation 10zip 48067title One Flew Over the Cuckoo's Nest (1975)genres DramaName: 0, dtype: object根据性别计算每部电影的平均评分123mean_ratings = data.pivot_table('rating', index='title', columns='gender', aggfunc='mean')mean_ratings[:5]过滤掉评分数小于250的电影1ratings_by_title = data.groupby('title').size()123456789ratings_by_title[:5]-----------------------------------------title$1,000,000 Duck (1971) 37'Night Mother (1986) 70'Til There Was You (1997) 52'burbs, The (1989) 303...And Justice for All (1979) 199dtype: int641active_titles = ratings_by_title.index[ratings_by_title &gt;= 250]12345678active_titles[:10]-----------------------------------------Index([''burbs, The (1989)', '10 Things I Hate About You (1999)', '101 Dalmatians (1961)', '101 Dalmatians (1996)', '12 Angry Men (1957)', '13th Warrior, The (1999)', '2 Days in the Valley (1996)', '20,000 Leagues Under the Sea (1954)', '2001: A Space Odyssey (1968)', '2010 (1984)'], dtype='object', name='title')ix应该是一个交集操作12mean_ratings = mean_ratings.ix[active_titles]mean_ratings按照女性最喜欢的电影进行降序排序12top_female_ratings = mean_ratings.sort_values(by='F', ascending=False)top_female_ratings[:10]​US Baby Names 1880-2010123import pandas as pdnames1880 = pd.read_csv('ch02/names/yob1880.txt', names=['name', 'sex', 'births'])names1880把所有年份的数据合并一下123456789101112131415# 2010 is the last available year right nowyears = range(1880, 2011)pieces = []columns = ['name', 'sex', 'births']for year in years: path = 'ch02/names/yob%d.txt' % year frame = pd.read_csv(path, names=columns) frame['year'] = year pieces.append(frame)# Concatenate everything into a single DataFramenames = pd.concat(pieces, ignore_index=True)进行聚合操作12total_births = names.pivot_table('births', index='year', columns='sex', aggfunc=sum)1total_births.tail()计算一下每个名字的出生比例1234567def add_prop(group): # Integer division floors births = group.births.astype(float) group['prop'] = births / births.sum() return groupnames = names.groupby(['year', 'sex']).apply(add_prop)1names进行一下有效性检查123np.allclose(names.groupby(['year', 'sex']).prop.sum(), 1)--------------------------------------------True筛选出每一对year/sex下总数前1000的名字1234def get_top1000(group): return group.sort_values(by='births', ascending=False)[:1000]grouped = names.groupby(['year', 'sex'])top1000 = grouped.apply(get_top1000)加个索引，结合了numpy1top1000.index = np.arange(len(top1000))Analyzing naming trends将数据分为男女12boys = top1000[top1000.sex == 'M']girls = top1000[top1000.sex == 'F']计算每一年每个名字的出生总数123total_births = top1000.pivot_table('births', index='year', columns='name', aggfunc=sum)total_births选出几个名字看看总数随年份的变化情况123subset = total_births[['John', 'Harry', 'Mary', 'Marilyn']]subset.plot(subplots=True, figsize=(12, 10), grid=False, title="Number of births per year")Measuring the increase in naming diversity通过统计前1000项名字所占的比例来判断多样性的变化1234table = top1000.pivot_table('prop', index='year', columns='sex', aggfunc=sum)table.plot(title='Sum of table1000.prop by year and sex', yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10))另一种方法，计算占出生人数50%的名字的数量也即从开始累加，看加到第几个名字时所占比例为50%先来看看2010年的男孩1df = boys[boys.year == 2010]1234567891011121314prop_cumsum = df.sort_index(by='prop', ascending=False).prop.cumsum()prop_cumsum[:10]--------------------------------------------------260877 0.011523260878 0.020934260879 0.029959260880 0.038930260881 0.047817260882 0.056579260883 0.065155260884 0.073414260885 0.081528260886 0.089621Name: prop, dtype: float64看来是第116个，不过序号从0开始，应该是117123prop_cumsum.values.searchsorted(0.5)---------------------------------------------------116再来看看1900年的男孩儿12345df = boys[boys.year == 1900]in1900 = df.sort_index(by='prop', ascending=False).prop.cumsum()in1900.values.searchsorted(0.5) + 1---------------------------------------------------25所以这样做是可行的把相同的操作赋予整个数据集123456def get_quantile_count(group, q=0.5): group = group.sort_values(by='prop', ascending=False) return group.prop.cumsum().values.searchsorted(q) + 1diversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)diversity = diversity.unstack('sex')diversity.head()1diversity.plot(title="Number of popular names in top 50%")The “Last letter” Revolution取出每个名字对应的最后一个字母，同时序号对应1234567# extract last letter from name columnget_last_letter = lambda x: x[-1]last_letters = names.name.map(get_last_letter)last_letters.name = 'last_letter'table = names.pivot_table('births', index=last_letters, columns=['sex', 'year'], aggfunc=sum)单独取出三年的来看看12subtable = table.reindex(columns=[1910, 1960, 2010], level='year')subtable.head()计算一下字母比例12345678910subtable.sum()-------------------------------------sex yearF 1910 396416.0 1960 2022062.0 2010 1759010.0M 1910 194198.0 1960 2132588.0 2010 1898382.0dtype: float641letter_prop = subtable / subtable.sum().astype(float)123456import matplotlib.pyplot as pltfig, axes = plt.subplots(2, 1, figsize=(10, 8))letter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')letter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female', legend=False)最后看一下所有的年份并生成一个趋势图12letter_prop = table / table.sum().astype(float)dny_ts = letter_prop.ix[['d', 'n', 'y'], 'M'].T1dny_ts.plot()Boy names that became girl names (and vice versa)以lesl开头的名字为例123456all_names = top1000.name.unique()mask = np.array(['lesl' in x.lower() for x in all_names])lesley_like = all_names[mask]lesley_like----------------------------------------------array(['Leslie', 'Lesley', 'Leslee', 'Lesli', 'Lesly'], dtype=object)从原数据集中筛选出来12345678910filtered = top1000[top1000.name.isin(lesley_like)]filtered.groupby('name').births.sum()----------------------------------------------nameLeslee 1082Lesley 35022Lesli 929Leslie 370429Lesly 10067Name: births, dtype: int64做一下聚合操作并计算比例1234table = filtered.pivot_table('births', index='year', columns='sex', aggfunc='sum')table = table.div(table.sum(1), axis=0)table.tail()看一下趋势1table.plot(style=&#123;'M': 'k-', 'F': 'k--'&#125;)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spider the house infomation and save to excel file]]></title>
      <url>%2F2017%2F02%2F18%2FSpider-the-house-infomation-and-save-to-excel-file%2F</url>
      <content type="text"><![CDATA[数据来源http://sh.fang.com/项目目标爬取二手房信息中的小区信息实现步骤【1】爬取小区信息（核心代码，下同）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150""" &lt;A spider to crawl the house information.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;. """class HouseSpider(scrapy.Spider): name = "house" head = "http://esf.sh.fang.com" allowed_domains = ["sh.fang.com"] start_urls = [ "http://esf.sh.fang.com/housing/" ] # 各区对应的编号（由于丧心病狂的url） area_map = &#123;25: 'pudong', 18: 'minhang', 19: 'xuhui', 30: 'baoshan', 28: 'putuo', 20: 'changning', 26: 'yangpu', 586: 'songjiang', 29: 'jiading', 23: 'hongkou', 27: 'zhabei', 21: 'jingan', 24: 'huangpu', 22: 'luwan', 31: 'qingpu', 32: 'fengxian', 35: 'jinshan', 996: 'chongming'&#125; estate_to_area_map = &#123;&#125; seperator = '=\n' def __init__(self): for key, value in self.area_map.items(): self.estate_to_area_map[key] = [] # print(self.estate_to_area_map) def parse(self, response): # 解析出上海市各区的地址 area_lis = response.xpath('//*[@id="houselist_B03_02"]/div[1]') for a in area_lis.xpath('./a'): # areas = items.AreaItem() # areas['name'] = a.xpath('text()').extract()[0] yield Request(self.head + a.xpath('@href').extract()[0], callback=self.parse_area) # print(a.xpath('text()').extract()[0]) def parse_area(self, response): # 确定response来源于哪一个区 area_index = str(response).split('/')[-2].split('_')[0] if area_index == '': return else: # 解析出各区中小区的详情页面地址 detail_str = 'xiangqing' estate_list = response.xpath('/html/body/div[4]/div[5]/div[4]') for a in estate_list.xpath('.//a[@class="plotTit"]'): estate_url = a.xpath('@href').extract()[0] if estate_url.find('esf') != -1: estate_url = estate_url.replace('esf', detail_str) else: estate_url = estate_url + detail_str if estate_url.find('http') != -1: # print(estate_url) self.estate_to_area_map[int(area_index)].append(estate_url) # print(len(self.estate_to_area_map[int(area_index)])) next_page = response.xpath('//*[@id="PageControl1_hlk_next"]') if len(next_page) != 0: yield Request(self.head + next_page.xpath('@href').extract()[0], callback=self.parse_area) else: # print(len(self.estate_to_area_map[int(area_index)])) for url in self.estate_to_area_map[int(area_index)]: request = Request(url, callback=self.parse_house, dont_filter=True) request.meta['index'] = int(area_index) yield request def parse_house(self, response): flag = 0 area_index = response.meta['index'] area_name = self.area_map[area_index] filename = area_name + '.txt' # print(response.xpath('/html')) # 详情页面存在两种，因此分情况讨论 house_name = response.xpath( '/html/body/div[4]/div[2]/div[2]/h1/a/text()') if len(house_name) == 0: # house_name = response.xpath( # '/html/body/div[1]/div[3]/div[2]/h1/a/text()') # flag = 1 return house_name = house_name.extract()[0] # 清洁小区名 house_name = re.sub(r'小区网', '', house_name) result_str = '【小区名称】' + house_name + '\n' if flag == 0: avg_price_xpath = response.xpath( '/html/body/div[4]/div[4]/div[1]/div[1]/dl[1]/dd/span/text()') avg_price = avg_price_xpath.extract()[0] result_str = result_str + '【平均价格】' + avg_price + '\n' detail_block_list = response.xpath( '/html/body/div[4]/div[4]/div[1]') for headline in detail_block_list.xpath('.//h3'): head_str = headline.xpath('./text()').extract()[0] if head_str == '基本信息': result_str = result_str + \ '【' + \ head_str + '】\n' for item in headline.xpath( '../../div[@class="inforwrap clearfix"]/dl/dd'): if len(item.xpath('./strong/text()')) != 0: if len(item.xpath('./text()')) != 0: result_str = result_str + \ item.xpath( './strong/text()').extract()[0] result_str = result_str + \ item.xpath('./text()').extract()[0] + '\n' # print(result_str) # elif head_str == '交通状况': # result_str = result_str + \ # '【' + \ # head_str + '】\n' # tempstr = headline.xpath( # '../../div[@class="inforwrap clearfix"]/dl/dt/text()').extract()[0] # result_str = result_str + tempstr + '\n' # # print(result_str) # elif head_str == '周边信息': # result_str = result_str + \ # '【' + \ # head_str + '】\n' # for item in headline.xpath( # '../../div[@class="inforwrap clearfix"]/dl/dt'): # result_str = result_str + \ # item.xpath('./text()').extract()[0] + '\n' # # print(result_str) elif head_str == '就近楼群': result_str = result_str + \ '【' + \ head_str + '】\n' for item in headline.xpath( '../../div[@class="inforwrap clearfix"]/dl/dd'): result_str = result_str + \ item.xpath('./a/text()').extract()[0] + '\n' result_str = result_str + self.seperator # print(result_str) with open(filename, 'a', errors='ignore') as f: f.write(result_str)【2】格式化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136""" &lt;A formatter&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;."""def GetDataFileList(path='.'): """ Get the houses file list. Arguments: path: Dir path. Returns: file_list: the list of data file that find houses data. """ file_list = [x for x in os.listdir(path) if os.path.isfile( x) and os.path.splitext(x)[1] == '.txt'] return file_listdef Parse(file_list): """ Parse the txt file that find houses data. Extract some import infomation such as house name, avarage price, address and so on. Arguments: file_list: the list of data file that find houses data. Returns: houses_dict_list: the list that each item find the detail dict of each house. """ HOUSE_NAME = '小区名称' HOUSE_NAME_SPLITOR = '】' HOUSE_ADDRESS = '小区地址' HOUSE_ADDRESS_SPLITOR = '：' HOUSE_AVG_PRICE = '平均价格' HOUSE_AVG_PRICE_SPLITOR = '】' AREA_OF_HOUSE_BELONGS_TO = '所属区域' AREA_OF_HOUSE_BELONGS_TO_SPLITOR_1 = '：' AREA_OF_HOUSE_BELONGS_TO_SPLITOR_2 = ' ' PROPERTY_CATEGORY = '物业类别' PROPERTY_CATEGORY_SPLITOR = '：' GREEN_RATE = '绿 化 率' GREEN_RATE_SPLITOR = '：' VOLUME_RATE = '容 积 率' VOLUME_RATE_SPLITOR = '：' PROPERTY_COSTS = '物 业 费' PROPERTY_COSTS_SPLITOR = '：' NO_INFO_NOW = '暂无信息' DETAIL_LIST = [HOUSE_NAME, HOUSE_AVG_PRICE, HOUSE_ADDRESS, AREA_OF_HOUSE_BELONGS_TO, PROPERTY_CATEGORY, GREEN_RATE, VOLUME_RATE, PROPERTY_COSTS] houses_dict_list = [] for file_name in file_list: raw_houses_string = '' # read all lines as a string with open(file_name, 'r', errors='ignore') as f: for line in f.readlines(): raw_houses_string += line # split the string to the houses raw info list raw_houses_list = raw_houses_string.split('=\n') raw_houses_details_list = [] for raw_house in raw_houses_list: # format house raw info to lines raw_houses_details = raw_house.split('\n')[:-1] if len(raw_houses_details) == 0: continue # combine the all formated house raw info to a list raw_houses_details_list.append(raw_houses_details) for raw_house_details in raw_houses_details_list: house_details_dict = &#123;&#125; for raw_detail in raw_house_details: # search house name if re.search(HOUSE_NAME, raw_detail): house_details_dict[HOUSE_NAME] = raw_detail.split( HOUSE_NAME_SPLITOR)[-1] # search house avarage price elif re.search(HOUSE_AVG_PRICE, raw_detail): # print(raw_detail) house_details_dict[HOUSE_AVG_PRICE] = raw_detail.split( HOUSE_AVG_PRICE_SPLITOR)[-1] # search house address elif re.search(HOUSE_ADDRESS, raw_detail): house_details_dict[HOUSE_ADDRESS] = raw_detail.split( HOUSE_ADDRESS_SPLITOR)[-1] # search the area of house belongs to elif re.search(AREA_OF_HOUSE_BELONGS_TO, raw_detail): temp_detail_value = raw_detail.split( AREA_OF_HOUSE_BELONGS_TO_SPLITOR_1)[-1] detail_value = temp_detail_value.split( AREA_OF_HOUSE_BELONGS_TO_SPLITOR_2)[0] house_details_dict[AREA_OF_HOUSE_BELONGS_TO] = detail_value # search the property category of house elif re.search(PROPERTY_CATEGORY, raw_detail): house_details_dict[PROPERTY_CATEGORY] = raw_detail.split( PROPERTY_CATEGORY_SPLITOR)[-1] # search the green rate elif re.search(GREEN_RATE, raw_detail): house_details_dict[GREEN_RATE] = raw_detail.split( GREEN_RATE_SPLITOR)[-1] # search the volume rate elif re.search(VOLUME_RATE, raw_detail): house_details_dict[VOLUME_RATE] = raw_detail.split( VOLUME_RATE_SPLITOR)[-1] # search the property costs elif re.search(PROPERTY_COSTS, raw_detail): house_details_dict[PROPERTY_COSTS] = raw_detail.split( PROPERTY_COSTS_SPLITOR)[-1] # Judge if all details are contained. # If not, set to null. house_details_dict_keys = house_details_dict.keys() for detail_name in DETAIL_LIST: if detail_name not in house_details_dict_keys: house_details_dict[detail_name] = NO_INFO_NOW houses_dict_list.append(house_details_dict) return houses_dict_list【3】通过高德地图api获取经纬度信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100""" &lt;A toolto transfer position.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;."""def Geocode(address): """ A tool that call the God-Map api. Arguments: address: the address to transfer. Returns: location: the transfered location. """ CITY_NAME = '上海' parameters = &#123;'address': address, 'key': 'your key', 'city': CITY_NAME&#125; base = 'http://restapi.amap.com/v3/geocode/geo' try: response = requests.get(base, parameters) except Exception as e: print('error!', e) finally: pass answer = response.json() return answerdef GETGodMapLocation(houses): """ Get the location that corresponds to the house name. Use the God-Map api to get the corresponding location. Arguments: houses_dict_list: the houses info. Returns: houses_dict_list_contains_loc: the houses info that contains the location info. """ HOUSE_NAME = '小区名称' HOUSE_LOCATION = '经纬度' NO_INFO_NOW = '暂无信息' houses_dict_list = houses.copy() error_count = 0 count = 0 size = len(houses) for house_dict in houses_dict_list: # Count count = count + 1 # Loading needs if count % 1000 == 0: print(count, '/', size) address = house_dict[HOUSE_NAME] answer = Geocode(address) # print(answer) # If find if len(answer['geocodes']) != 0: # print(address + "的经纬度：", answer['geocodes'][0]['location']) house_dict[HOUSE_LOCATION] = answer['geocodes'][0]['location'] else: # remaking the invalid address # print('address remaking...') if re.search(r'别墅', address): re.sub(r'别墅', '', address) else: address = address + '小区' # print('retransfering...') # transfer again answer = Geocode(address) if len(answer['geocodes']) != 0: # print(address + "的经纬度：", answer['geocodes'][0]['location']) house_dict[HOUSE_LOCATION] = answer['geocodes'][0]['location'] else: # print(address) error_count += 1 house_dict[HOUSE_LOCATION] = NO_INFO_NOW print('error counts: ', error_count) return houses_dict_list【4】存储成excel文件1234567891011121314151617181920212223242526272829303132333435363738394041424344""" &lt;A tool to save the excel file.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;.""" def Save2ExcelFile(houses): """ Save the python based list file to excel file. Arguments: houses: the houses list. """ houses_dict_list = houses.copy() house_list = [] # format the source data to fit the xlwt package keys = houses[0].keys() for key in keys: house = [] house.append(key) for house_dict in houses_dict_list: house.append(house_dict[key]) house_list.append(house) # return house_list xls = ExcelWrite.Workbook() sheet = xls.add_sheet('小区信息') for i in range(len(house_list)): for j in range(len(house_list[0])): sheet.write(j, i, house_list[i][j]) xls.save('houses.xls')结果展示]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python_god_web_api]]></title>
      <url>%2F2017%2F02%2F17%2Fpython-god-web-api%2F</url>
      <content type="text"><![CDATA[http://lbs.amap.com/api/webservice/guide/api/search123456789101112131415161718#!/usr/bin/env python3#-*- coding:utf-8 -*-'''利用高德地图api实现地址和经纬度的转换'''import requestsdef geocode(address): parameters = &#123;'address': address, 'key': 'e798a5bfb344a09977b79552ae415974'&#125; base = 'http://restapi.amap.com/v3/geocode/geo' response = requests.get(base, parameters) answer = response.json() print(address + "的经纬度：", answer['geocodes'][0]['location'])if __name__=='__main__': #address = input("请输入地址:") address = '北京市海淀区' geocode(address)12345678910111213141516171819202122232425262728293031import xlrddef readXlsx(self, filename='CenterBottom2013.xlsx', sheetname='Sheet1'): rawData = [] if (os.path.isfile(self.fn_rawDat)): with open(self.fn_rawDat, 'rb') as f: self.rawDat = np.load(f) else: workBook = xlrd.open_workbook(filename) bookSheet = workBook.sheet_by_name(sheetname) # 从第二行开始读取，因为第一行有标签 for row in range(1, bookSheet.nrows): rowData = [] for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = cel.value except: pass if type(val) == float: val = float(val) else: val = str(val) rowData.append(val) rawData.append(rowData) self.rawDat = np.array(rawData) with open(self.fn_rawDat, 'wb') as f: np.save(f, self.rawDat) return self.rawDatRead Excel filesTransfer the address to locaion infoPut back]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Decision tree]]></title>
      <url>%2F2017%2F02%2F15%2FDecision-tree%2F</url>
      <content type="text"><![CDATA[决策树（ID3）决策树的构建构造决策树时，所需要解决的第一个问题就是，每划分一个分支时，应该根据哪一维特征进行划分。这时候我们需要定义某种指标，然后对每一维特征进行该指标的评估，最后选择指标值最高的特征进行划分。划分完毕之后，原始数据集就被划分为几个数据子集。如果某一个下的数据属于同一类型，则算法停止；否则，重复划分过程。伪代码（创建分支）1234567891011createbranch()检测数据集中的每个子项是否属于同一分类: If so return 类标签; Else 寻找划分数据集的最好特征 划分数据集 创建分支节点 for 每个划分的子集 调用函数createBranch并增加返回结果到分支节点中 return 分支节点那么接下来的重点便是如何寻找划分数据集的最好特征，在这里我们使用ID3算法中使用的划分数据集的方法，也即根据熵来划分。信息增益熵划分数据的核心思想是将无序的数据变得更加有序。而一个数据有序程度可以进行量化表示，也就是信息，其度量方式就是熵。其显然，数据集划分前后其所含的信息会发生变化，这个变化便称为信息增益。熵定义为信息的期望，其中信息的定义如下，信息一般针对的对象为多个类别中的某一个类别：l(x_i) = -log_{2}p(x_i)其中$x_i$表示某一类别，$p(x_i)$表示从多个类别中选择该类别的概率。接下来，熵的定义如下：H = \sum_{i=1}^{n}p(x_i)l(x_i)=-\sum_{i=1}^{n}p(x_i)log_{2}p(x_i)信息增益定义如下：IG(S|T) = H(S) - \sum_{value(T)} \frac{|S_v|}{|S|} H(S_v)其中$S$ 为全部样本集合，$value(T) $是属性 $T$所有取值的集合，$v$ 是 $T$ 的其中一个属性值，$S_v$是 $S$ 中属性 $T$ 的值为 $v$ 的样例集合，$|S_v|$ 为 $S_v$ 中所含样例数，$|S|$ 为 $S$ 中所含样例数。代码实现：123456789101112131415161718192021222324252627from math import logdef CalcShannonEnt(data_set): """ Calculate the Shannon Entropy. Arguments: data_set: The object dataset. Returns: shannon_ent: The Shannon entropy of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Shannon entropy shannon_ent = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries shannon_ent -= prob * log(prob, 2) return shannon_ent为了进行测试，以及之后的算法运行，我们写一个十分naive的数据生成方法：123456789101112131415def CreateDataSet(): """ A naive data generation method. Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] return data_set, labels注意，这里的labels并不代表分类标签，yes以及no才是，labels代表特征名。下面进行一个简单的demo:123456789In [22]: import treesIn [23]: my_dat, labels = trees.CreateDataSet()In [24]: my_datOut[24]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [25]: trees.CalcShannonEnt(my_dat)Out[25]: 0.9709505944546686熵越高，表明数据集中类别数越多。另一个度量无序程度的方法是基尼不纯度（Gini impurity）。基尼不纯度基尼不纯度的定义为，对于每一个节点，从所有类别标签中随机选择一个，选择出来的类别标签与其本身的类别标签不一致的概率之和。形式化地定义如下：G = \sum_{i \ne j}p(x_i)p(x_j) = \sum_{i}p(x_i)\sum_{j \ne i}p(x_j) = \sum_{i}p(x_i)(1-p(x_i)) = \sum_{i}p(x_i) - \sum_{i}(p(x_i))^2 = 1 - \sum_{i}(p(x_i))^2代码实现如下：1234567891011121314151617181920212223242526def CalcGiniImpurity(data_set): """ Calculate the Gini impurity. Arguments: data_set: The object dataset. Returns: gini_impurity: The Gini impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Gini impurity gini_impurity = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries gini_impurity += pow(prob, 2) gini_impurity = 1 - gini_impurity return gini_impurity同样进行一个简单的demo：123456789In [4]: import treesIn [5]: my_dat, labels = trees.CreateDataSet()In [6]: my_datOut[6]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [7]: trees.CalcGiniImpurity(my_dat)Out[7]: 0.48最后再介绍一种度量无序程度的方式，误分类不纯度。误分类不纯度定义如下：M = 1 - \max_{i}(p(x_i))代码实现如下：123456789101112131415161718192021222324def CalcMisClassifyImpurity(data_set): """ Calculate the misclassification impurity. Arguments: data_set: The object dataset. Returns: mis_classify_impurity: The misclassification impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the misclassification impurity mis_classify_impurity = 0.0 max_prob = max(label_counts.values()) / num_entries mis_classify_impurity = 1 - max_prob return mis_classify_impurity进行一个简单的demo:12345678910In [25]: reload(trees)Out[25]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [26]: my_dat, labels = trees.CreateDataSet()In [27]: my_datOut[27]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [28]: trees.CalcMisClassifyImpurity(my_dat)Out[28]: 0.4最后用一个图来总结一下这三种不纯度度量的函数图像（以二类情况为例）[Ref: http://www.cse.msu.edu/~cse802/DecisionTrees.pdf]：数据划分根据以上，数据划分的思路是，基于每一维特征的每一个值进行划分，并计算划分前后的信息增益，最后选取增益最大的特征及其所对应的值进行划分，由于这里运用的是ID3算法，因此选择的信息度量方式是熵。代码实现如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def SplitDataSet(data_set, axis, value): """ Split the data set according to the given axis and correspond value. Arguments: data_set: Object data set. axis: The split-feature index. value: The value of the split-feature. Returns: ret_data_set: The splited data set. """ ret_data_set = [] for feat_vec in data_set: if feat_vec[axis] == value: reduced_feat_vec = feat_vec[:axis] reduced_feat_vec.extend(feat_vec[axis + 1:]) ret_data_set.append(reduced_feat_vec) return ret_data_setdef ChooseBestFeatureToSplit(data_set): """ Choose the best feature to split. Arguments: data_set: Object data set. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannonEnt(data_set) best_info_gain = 0.0 best_feature = -1 for i in range(num_features): # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) new_entropy = 0.0 # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannonEnt(sub_data_set) # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_feature由以上代码（ID3算法）可以看出，其计算熵的依据是根据最后一个特征，是否这种naive的选取方式能够达到平均的最好结果？另外，其划分依据仅仅根据划分一次后的子数据集的熵之和，属于一种贪心策略，这样是否可以达到最优解？递归构建决策树既然是递归算法，那么必须设定递归结束条件：遍历完所有属性每个分支下的数据都属于相同的分类这里存在一个问题，如果遍历完所有属性后，某些分支下还是存在多个分类，这种情况下一般采用多数表决的方式，代码实现方式如下：12345678910111213141516171819202122def majority_cnt(class_list): """ Decided the final class. When the splited data is not belongs to the same class while all feature is handled, the final class is decided by the majority class. Arguments: class_list: The class list of the splited data set. Returns: sorted_class_count[0][0]: The majority class. """ class_count = &#123;&#125; for vote in class_list: if vote not in class_count.keys(): class_count[vote] = 0 class_count[vote] += 1 sorted_class_count = sorted( class_count.iteritems(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0]下面进行树的创建：1234567891011121314151617181920212223242526272829303132333435def CreateTree(data_set, labels): """ Create decision tree. Arguments: data_set: The object data set. labels: The feature labels in the data_set. Returns: my_tree: A dict that represents the decision tree. """ class_list = [example[-1] for example in data_set] # If the classes are fully same if class_list.count(class_list[0]) == len(class_list): return class_list[0] # If all feature is handled if len(data_set[0]) == 1: return majority_cnt(class_list) # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set) best_feat_label = labels[best_feat] # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Get the next step labels parameter del(labels[best_feat]) # Next step start feat_values = [example[best_feat] for example in data_set] unique_vals = set(feat_values) for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels) return my_tree下面进行一下简单的测试：123456789In [27]: reload(trees)Out[27]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [28]: my_dat, labels = trees.CreateDataSet()In [29]: my_tree = trees.CreateTree(my_dat, labels)In [30]: my_treeOut[30]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;可见决策树已经构造成功（图形化如下所示），但是这显然不够，我们需要的是用决策树进行分类。决策树分类demo如下：123456789101112131415161718192021In [63]: reload(trees)Out[63]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [64]: my_dat, labels = trees.CreateDataSet()In [65]: labelsOut[65]: ['no surfacing', 'flippers']In [66]: my_tree = trees.CreateTree(my_dat, labels)In [67]: labelsOut[67]: ['no surfacing', 'flippers']In [68]: my_treeOut[68]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;In [69]: trees.Classify(my_tree, labels, [1, 0])Out[69]: 'no'In [70]: trees.Classify(my_tree, labels, [1, 1])Out[70]: 'yes'C4.5C4.5算法是由ID3算法引申而来，主要改进有以下两点：选取最优分裂属性时根据信息增益率 (IGR)使算法对连续变量兼容下面分别对分裂信息以及信息增益率进行定义：IGR = \frac{IG}{IV}因此只需对ID3算法的代码做一些改动即可，为了兼容ID3， 具体实现如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def ChooseBestFeatureToSplit(data_set, flag='ID3'): """ Choose the best feature to split. Arguments: data_set: Object data set. flag: Decide if use the infomation gain rate or not. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannon(data_set) method = 'ID3' best_feature = -1 best_info_gain = 0.0 best_info_gain_rate = 0.0 for i in range(num_features): new_entropy = 0.0 # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) if len(unique_vals) &gt; 3: method = 'C4.5' if method == 'ID3': # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannon(sub_data_set) else: data_set = np.array(data_set) sorted_feat = np.argsort(feat_list) for index in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [index + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; new_entropy: new_entropy = iv # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if flag == 'C4.5': info_gain_rate = info_gain / new_entropy # print('index', i, 'info_gain_rate', info_gain_rate) if info_gain_rate &gt; best_info_gain_rate: best_info_gain_rate = info_gain_rate best_feature = i if flag == 'ID3': if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_feature下面需要解决的问题是连续变量的问题，为了实验的方便，我们更改一下naive的数据生成方法（Ref: http://blog.csdn.net/lemon_tree12138/article/details/51840361 ）：1234567891011121314151617181920212223242526272829303132333435363738def CreateDataSet(method='ID3'): """ A naive data generation method. Arguments: method: The algorithm class Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) if method == 'ID3': data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] else: data_set = [[85, 85, 'no'], [80, 90, 'yes'], [83, 78, 'no'], [70, 96, 'no'], [68, 80, 'no'], [65, 70, 'yes'], [64, 65, 'yes'], [72, 95, 'no'], [69, 70, 'no'], [75, 80, 'no'], [75, 70, 'yes'], [72, 90, 'yes'], [81, 75, 'no'], [71, 80, 'yes']] labels = ['temperature', 'humidity'] return data_set, labels假设我们选择了温度属性，则被提取的关键数据为：[[85, No], [80, No], [83, Yes], [70, Yes], [68, Yes], [65, No], [64, Yes], [72, No], [69, Yes], [75, Yes], [75, Yes], [72, Yes], [81, Yes], [71, No]]现在我们对这批数据进行从小到大进行排序，排序后数据集就变成：[[64, Yes], [65, No], [68, Yes], [69, Yes], [70, Yes], [71, No], [72, No], [72, Yes], [75, Yes], [75, Yes], [80, No], [81, Yes], [83, Yes], [85, No]]绘制成如下图例：当我们拿到一个已经排好序的（温度，结果）的列表之后，分别计算被某个单元分隔的左边和右边的分裂信息，比如现在计算 index = 4 时的分裂信息。则：IV(v_4) = IV([4, 1], [5, 4]) = \frac{5}{14}IV([4, 1]) + \frac{9}{14}IV([5, 4])IV(v_4) = \frac{5}{14}(-\frac{4}{5} \log_{2} \frac{4}{5} - \frac{1}{5} \log_{2} \frac{1}{5}) + \frac{9}{14}(-\frac{5}{9} \log_{2} \frac{5}{9} - \frac{4}{9} \log_{2} \frac{4}{9})下图表示了不同分裂位置所得到的分裂信息：最后给出完整的代码实现 (最后的Classify方法还需修改)：trees.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383from math import logimport operatorimport numpy as npdef CalcShannon(data_set): """ Calculate the Shan0n Entropy. Arguments: data_set: The object dataset. Returns: shan0n_ent: The Shan0n entropy of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # print(label_counts) # Calculates the Shan0n entropy shan0n_ent = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries shan0n_ent -= prob * log(prob, 2) return shan0n_entdef CalcGiniImpurity(data_set): """ Calculate the Gini impurity. Arguments: data_set: The object dataset. Returns: gini_impurity: The Gini impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Gini impurity gini_impurity = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries gini_impurity += pow(prob, 2) gini_impurity = 1 - gini_impurity return gini_impuritydef CalcMisClassifyImpurity(data_set): """ Calculate the misclassification impurity. Arguments: data_set: The object dataset. Returns: mis_classify_impurity: The misclassification impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the misclassification impurity mis_classify_impurity = 0.0 max_prob = max(label_counts.values()) / num_entries mis_classify_impurity = 1 - max_prob return mis_classify_impuritydef CreateDataSet(method='ID3'): """ A naive data generation method. Arguments: method: The algorithm class Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) if method == 'ID3': data_set = [[1, 1, 1], [1, 1, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0]] labels = ['0 surfacing', 'flippers'] else: data_set = [[1, 85, 85, 0, 0], [1, 80, 90, 1, 0], [2, 83, 78, 0, 1], [3, 70, 96, 0, 1], [3, 68, 80, 0, 1], [3, 65, 70, 1, 0], [2, 64, 65, 1, 1], [1, 72, 95, 0, 0], [1, 69, 70, 0, 1], [3, 75, 80, 0, 1], [1, 75, 70, 1, 1], [2, 72, 90, 1, 1], [2, 81, 75, 0, 1], [3, 71, 80, 1, 0]] labels = ['outlook', 'temperature', 'humidity', 'windy'] return data_set, labelsdef SplitDataSet(data_set, axis, value): """ Split the data set according to the given axis and correspond value. Arguments: data_set: Object data set. axis: The split-feature index. value: The value of the split-feature. Returns: ret_data_set: The splited data set. """ ret_data_set = [] for feat_vec in data_set: if feat_vec[axis] == value: reduced_feat_vec = feat_vec[:axis] reduced_feat_vec.extend(feat_vec[axis + 1:]) ret_data_set.append(reduced_feat_vec) return ret_data_setdef ChooseBestFeatureToSplit(data_set, flag='ID3'): """ Choose the best feature to split. Arguments: data_set: Object data set. flag: Decide if use the infomation gain rate or not. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannon(data_set) method = 'ID3' best_feature = -1 best_info_gain = 0.0 best_info_gain_rate = 0.0 for i in range(num_features): new_entropy = 0.0 # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) if len(unique_vals) &gt; 3: method = 'C4.5' if method == 'ID3': # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannon(sub_data_set) else: data_set = np.array(data_set) sorted_feat = np.argsort(feat_list) for index in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [index + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; new_entropy: new_entropy = iv # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if flag == 'C4.5': info_gain_rate = info_gain / new_entropy # print('index', i, 'info_gain_rate', info_gain_rate) if info_gain_rate &gt; best_info_gain_rate: best_info_gain_rate = info_gain_rate best_feature = i if flag == 'ID3': if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_featuredef majority_cnt(class_list): """ Decided the final class. When the splited data is 0t belongs to the same class while all feature is handled, the final class is decided by the majority class. Arguments: class_list: The class list of the splited data set. Returns: sorted_class_count[0][0]: The majority class. """ class_count = &#123;&#125; for vote in class_list: if vote not in class_count.keys(): class_count[vote] = 0 class_count[vote] += 1 sorted_class_count = sorted( class_count. items(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0]def CreateTree(data_set, feat_labels, method='ID3'): """ Create decision tree. Arguments: data_set: The object data set. labels: The feature labels in the data_set. method: The algorithm class. Returns: my_tree: A dict that represents the decision tree. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) labels = feat_labels.copy() class_list = [example[-1] for example in data_set] # print(class_list) # If the classes are fully same print('class_list', class_list) if class_list.count(class_list[0]) == len(class_list): return class_list[0] # If all feature is handled if len(data_set[0]) == 1: return majority_cnt(class_list) if method == 'ID3': # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set) best_feat_label = labels[best_feat] # print(best_feat_label) # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Next step start feat_values = [example[best_feat] for example in data_set] # Get the next step labels parameter del(labels[best_feat]) unique_vals = set(feat_values) for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels) return my_tree else: flag = 'ID3' # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set, 'C4.5') best_feat_label = labels[best_feat] print(best_feat_label) # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Next step start feat_values = [example[best_feat] for example in data_set] del(labels[best_feat]) unique_vals = set(feat_values) if len(unique_vals) &gt; 3: flag = 'C4.5' if flag == 'ID3': for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels, 'C4.5') return my_tree else: data_set = np.array(data_set) best_iv = 0.0 best_split_value = -1 sorted_feat = np.argsort(feat_values) for i in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [i + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; best_iv: best_iv = iv best_split_value = feat_values[sorted_feat[i]] print(best_feat, best_split_value) # print(best_split_value) left_data_set = data_set[ data_set[:, best_feat] &lt;= best_split_value] left_data_set = np.delete(left_data_set, best_feat, axis=1) # if len(left_data_set) == 1: # return left_data_set[0][-1] right_data_set = data_set[ data_set[:, best_feat] &gt; best_split_value] right_data_set = np.delete(right_data_set, best_feat, axis=1) # if len(right_data_set) == 1: # return right_data_set[0][-1] sub_labels = labels[:] my_tree[best_feat_label][ '&lt;=' + str(best_split_value)] = CreateTree( left_data_set.tolist(), sub_labels, 'C4.5') my_tree[best_feat_label][ '&gt;' + str(best_split_value)] = CreateTree( right_data_set.tolist(), sub_labels, 'C4.5') # print('continious tree', my_tree) return my_treedef Classify(input_tree, feat_labels, test_vec): """ Classify that uses the given decision tree. Arguments: input_tree: The Given decision tree. feat_labels: The labels of correspond feature. test_vec: The test data. Returns: class_label: The class label that corresponds to the test data. """ # Get the start feature label to split first_str = list(input_tree.keys())[0] # Get the sub-tree that corresponds to the start feature to split second_dict = input_tree[first_str] # Get the feature index that the label is the start feature label feat_index = feat_labels.index(first_str) # Start recurrence search for key in second_dict.keys(): if test_vec[feat_index] == key: if type(second_dict[key]).__name__ == 'dict': # Recurrence calls class_label = Classify(second_dict[key], feat_labels, test_vec) else: class_label = second_dict[key] return class_label一个小demo：123456789101112131415161718192021222324252627282930313233In [108]: reload(trees)Out[108]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [109]: my_dat, labels = trees.CreateDataSet('C4.5')In [110]: my_tree_c = trees.CreateTree(my_dat, labels, 'C4.5')class_list [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]outlookclass_list [0, 0, 0, 1, 1]humidity1 90class_list [0, 0, 1, 1]temperature0 69class_list [1]class_list [0, 0, 1]windyclass_list [0]class_list [0, 1]class_list [0]class_list [1, 1, 1, 1]class_list [1, 1, 0, 1, 0]windyclass_list [1, 1, 1]class_list [0, 0]In [111]: my_tree_cOut[111]:&#123;'outlook': &#123;1: &#123;'humidity': &#123;'&lt;=90': &#123;'temperature': &#123;'&lt;=69': 1, '&gt;69': &#123;'windy': &#123;0: 0, 1: 0&#125;&#125;&#125;&#125;, '&gt;90': 0&#125;&#125;, 2: 1, 3: &#123;'windy': &#123;0: 1, 1: 0&#125;&#125;&#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kNN and kd-tree]]></title>
      <url>%2F2017%2F02%2F15%2FkNN-and-kd-tree%2F</url>
      <content type="text"><![CDATA[k-近邻算法工作原理存在一组带标签的训练集[1]，每当有新的不带标签的样本[2]出现时，将训练集中数据的特征与测试集的特征逐个比较，通过某种测度来提取出与测试集最相似的k个训练集样本，然后将这k个样本中占大多数[4]的标签赋予测试集样本。伪代码对测试集中的每个点依次执行如下操作：计算训练集中的每个点与当前点的距离按照距离递增次序排序在排序好的点中选取前k个点统计出k个点中不同类别的出现频率选择频率最高的类别为当前点的预测分类​代码实现首先创建测试数据集1from numpy import *1234def createDataset(): group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labels返回预测分类123456789101112131415def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] diffMat = tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 sortedDistIndices = distances.argsort() classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndices[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 # the return of sorted() is a list and its item is a tuple sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0] # returns the predict class label进一步探索k-近邻算法的缺点在于当数据量很大时，拥有不可接受的空间复杂度以及时间复杂度其次该算法最关键的地方在与超参k的选取。当k选取的过小时容易造成过拟合，反之容易造成欠拟合。考虑两个极端情况，当k=1时，该算法又叫最近邻算法；当k=N[3]时，表示直接从原始数据中选取占比最大的类别，显然这个算法太naive了。为了解决kNN算法时间复杂度的问题，最关键的便是在于如何对数据进行快速的k近邻搜索，一种解决方法是引入kd树来进行加速。kd树简介以二维空间为例，假设有6个二维数据点{(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2)}，可以用下图来表明kd树所能达到的效果:kd树算法主要分为两个部分：kd树数据结构的建立在kd树上进行查找kd树是一种对k维空间上的数据点进行存储以便进行高效查找的树形数据结构，属于二叉树。构造kd树相当于不断用垂直于坐标轴的超平面对k维空间进行划分，构成一系列k维超矩形区域。kd树的每一个结点对应一个超矩形区域，表示一个空间范围。数据结构下表给出每个结点主要包含的数据结构:域名数据类型描述Node-data数据矢量数据集中的某个数据点，k维矢量Split整数垂直于分割超平面的方向轴序号Leftkd树由位于该节点分割超平面左子空间内所有数据点构成的kd树Rightkd树由位于该节点分割超平面右子空间内所有数据点构成的kd树建立树伪代码下面给出构建kd树的伪代码：算法：构建k-d树（createKDTree）输入：数据点集Data-set输出：Kd，类型为k-d tree1. If Data-set为空，则返回空的k-d tree2. 调用节点生成程序： （1）确定split域：对于所有描述子数据（特征矢量），统计它们在每个维上的数据方差。以SURF特征为例，描述子为64维，可计算64个方差。挑选出最大值，对应的维就是split域的值。数据方差大表明沿该坐标轴方向上的数据分散得比较开，在这个方向上进行数据分割有较好的分辨率； （2）确定Node-data域：数据点集Data-set按其第split域的值排序。位于正中间的那个数据点被选为Node-data。此时新的Data-set’ = Data-set\Node-data（除去其中Node-data这一点）。3. dataleft = {d属于Data-set’ &amp;&amp; d[split] ≤ Node-data[split]} dataright = {d属于Data-set’ &amp;&amp; d[split] &gt; Node-data[split]}4. left = 由（dataleft）建立的k-d tree，即递归调用createKDTree（dataleft）并设置left的parent域为Kd； right = 由（dataright）建立的k-d tree，即调用createKDTree（dataleft）并设置right的parent域为Kd。实例用最开始的6个二维数据点的例子，来具体化这个过程：确定split域的首先该取的值。分别计算x，y方向上数据的方差得知x方向上的方差最大，所以split域值首先取0，也就是x轴方向；确定Node-data的域值。根据x轴方向的值2,5,9,4,8,7排序选出中值为7，所以Node-data = (7, 2)。这样，该节点的分割超平面就是通过(7, 2)并垂直于split = 0（x轴）的直线x = 7；确定左子空间和右子空间。分割超平面x = 7将整个空间分为两部分，如下图所示。x &lt; = 7的部分为左子空间，包含3个节点{(2, 3), (5, 4), (4, 7)}；另一部分为右子空间，包含2个节点{(9, 6), (8, 1)}。如算法所述，k-d树的构建是一个递归的过程。然后对左子空间和右子空间内的数据重复根节点的过程就可以得到下一级子节点（5,4）和（9,6）（也就是左右子空间的’根’节点），同时将空间和数据集进一步细分。如此反复直到空间中只包含一个数据点，如图1所示。最后生成的k-d树如下图所示。注意：每一级节点旁边的’x’和’y’表示以该节点分割左右子空间时split所取的值。这里进行一点补充说明，kd树其实就是二叉树，其与普通的二叉查找树不同之处在于，其每一层根据split的维度进行二叉拆分。具体来说，根据上图，第一层的拆分是根据x，那么其左孩子的x值就小于根结点的x值，右孩子则反之。y值则没有规定（这里出现的左大右小只是纯粹的巧合）。第二层是根据y值来进行split，因此第三层的规律显而易见。代码实现运行环境：Windows 10 Pro 64-bit x64-based(Ver. 10.0.14393), Python 3.5.2, Anaconda 4.1.1(64-bit), IPython 5.0.0, Windows CMD,kdTreeCreate.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import numpy as npfrom kdTreeNode import *def createDataSet(): """ Create the test dataset. Returns: A numpy array that contains the test data. """ dataSet = np.array([[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]) return dataSetdef split(dataSet): """ Split the given dataset. Returns: LeftDataSet: A kdTreeNode object. RightDataSet: A kdTreeNode object. NodeData: A tuple. """ # Ensure the dimension to split dimenIndex = np.var(dataSet, axis=0).argmax() partitionDataSet = dataSet[:, dimenIndex] # print(partitionDataSet) # Ensure the position to split partitionDataSetArgSort = partitionDataSet.argsort() # print(partitionDataSetArgSort) lenOfPartitionDataSetArgSort = len(partitionDataSetArgSort) if lenOfPartitionDataSetArgSort % 2 == 0: posIndex = lenOfPartitionDataSetArgSort // 2 splitIndex = partitionDataSetArgSort[posIndex] else: posIndex = (lenOfPartitionDataSetArgSort - 1) // 2 splitIndex = partitionDataSetArgSort[posIndex] # print(splitIndex) # Split nodeData = dataSet[splitIndex] leftIndeies = partitionDataSetArgSort[:posIndex] rightIndeies = partitionDataSetArgSort[posIndex + 1:] leftDataSet = dataSet[leftIndeies] rightDataSet = dataSet[rightIndeies] return nodeData, dimenIndex, leftDataSet, rightDataSetdef createKDTree(dataSet): """ Create the KD tree. Returns: A kdTreeNode object. """ if len(dataSet) == 0: return nodeData, dimenIndex, leftDataSet, rightDataSet = split(dataSet) # print(nodeData, dimenIndex, leftDataSet, rightDataSet) node = kdTreeNode(nodeData, dimenIndex) node.setLeft(createKDTree(leftDataSet)) node.setRight(createKDTree(rightDataSet)) return nodedef midTravel(node): if node is None: return midTravel(node.getLeft()) print(node.getData()) midTravel(node.getRight())if __name__ == "__main__": dataSet = createDataSet() node = createKDTree(dataSet) midTravel(node)kdTreeNode.py123456789101112131415161718192021222324252627282930313233class kdTreeNode(object): """ Class of k-d tree nodes """ def __init__(self, data=None, split=None, left=None, right=None): self.__data = data self.__split = split self.__left = left self.__right = right def getData(self): return self.__data def setData(self, data): self.__data = data def getSplit(self): return self.__split def setSplit(self, split): self.__split = split def getLeft(self): return self.__left def setLeft(self, left): self.__left = left def getRight(self): return self.__right def setRight(self, right): self.__right = right运行结果：12345678In [1]: run kdTreeCreate.py-------------------------------------[2 3][5 4][4 7][7 2][8 1][9 6]时间复杂度：N个K维数据进行查找操作时时间复杂度为 $t=O(KN^{2})$下面就要在已经建立好的kd树上进行查找操作。查找kd树中进行的查找与普通的查找操作存在较大的差异，其目的是为了找出与查询点距离最近的点。星号表示要查询的点(2.1, 3.1)。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点(2, 3)。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行’回溯’操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从(7, 2)点开始进行二叉查找，然后到达(5, 4)，最后到达(2, 3)，此时搜索路径中的节点为&lt;(7, 2), (5, 4), (2, 3)&gt;，首先以(2, 3)作为当前最近邻点，计算其到查询点(2.1, 3.1)的距离为0.1414，然后回溯到其父节点(5, 4)，并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以(2.1, 3.1)为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入(5, 4)节点右子空间中去搜索。再回溯到(7, 2)，以(2.1, 3.1)为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入(7, 2)右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点(2, 3)，最近距离为0.1414。一个复杂点了例子如查找点为(2, 4.5)。同样先进行二叉查找，先从(7, 2)查找到(5, 4)节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到(4, 7)，形成搜索路径&lt;(7, 2), (5, 4), (4, 7)&gt;，取(4, 7)为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到(5, 4)，计算其与查找点之间的距离为3.041。以(2, 4.5)为圆心，以3.041为半径作圆。可见该圆和y = 4超平面交割，所以需要进入(5, 4)左子空间进行查找。此时需将(2, 3)节点加入搜索路径中得&lt;(7, 2), (2, 3)&gt;。回溯至(2, 3)叶子节点，(2, 3)距离(2, 4.5)比(5, 4)要近，所以最近邻点更新为(2, 3)，最近距离更新为1.5。回溯至(7, 2)，以(2, 4.5)为圆心1.5为半径作圆，并不和x = 7分割超平面交割。至此，搜索路径回溯完。返回最近邻点(2, 3)，最近距离1.5。k-d树查询算法的伪代码如下所示。查找伪代码算法： k-d树最邻近查找输入：Kd， //k-d tree类型target //查询数据点输出：nearest， //最邻近数据点dist //最邻近数据点和查询点间的距离123456789101112131415161718192021222324252627282930311. If Kd为NULL，则设dist为infinite并返回2. //进行二叉查找，生成搜索路径 Kd_point = &amp;Kd； //Kd-point中保存k-d tree根节点地址 nearest = Kd_point -&gt; Node-data； //初始化最近邻点 while（Kd_point） push（Kd_point）到search_path中； //search_path是一个堆栈结构，存储着搜索路径节点指针 /*** If Dist（nearest，target） &gt; Dist（Kd_point -&gt; Node-data，target） nearest = Kd_point -&gt; Node-data； //更新最近邻点 Max_dist = Dist(Kd_point，target）； //更新最近邻点与查询点间的距离 ***/ s = Kd_point -&gt; split； //确定待分割的方向 If target[s] &lt;= Kd_point -&gt; Node-data[s] //进行二叉查找 Kd_point = Kd_point -&gt; left； else Kd_point = Kd_point -&gt;right； nearest = search_path中最后一个叶子节点； //注意：二叉搜索时不比计算选择搜索路径中的最邻近点，这部分已被注释 Max_dist = Dist（nearest，target）； //直接取最后叶子节点作为回溯前的初始最近邻点 3. //回溯查找 while（search_path != NULL） back_point = 从search_path取出一个节点指针； //从search_path堆栈弹栈 s = back_point -&gt; split； //确定分割方向 If Dist（target[s]，back_point -&gt; Node-data[s]） &lt; Max_dist //判断还需进入的子空间 If target[s] &lt;= back_point -&gt; Node-data[s] Kd_point = back_point -&gt; right； //如果target位于左子空间，就应进入右子空间 else Kd_point = back_point -&gt; left; //如果target位于右子空间，就应进入左子空间 将Kd_point压入search_path堆栈； If Dist（nearest，target） &gt; Dist（Kd_Point -&gt; Node-data，target） nearest = Kd_point -&gt; Node-data； //更新最近邻点 Min_dist = Dist（Kd_point -&gt; Node-data,target）； //更新最近邻点与查询点间的距离代码实现kdTreeSearch.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import numpy as npdef cal_dist(node, target): """ Calculate the distance between the node and the target. Arguments: node: The kd-tree's one node. target: Search target. Returns: dist: The distance between the two nodes. """ node_data = np.array(node) target_data = np.array(target) square_dist_vector = (node_data - target_data) ** 2 square_dist = np.sum(square_dist_vector) dist = square_dist ** 0.5 return distdef search(root_node, target): """ Search the nearest node of the target node in the kd-tree that root node is the root_node Arguments: root_node: The kd-tree's root node. target: Search target. Returns: nearest: The nearest node of the target node in the kd-tree. min_dist: The nearest distance. """ if root_node is None: min_dist = float('inf') return min_dist # Two-fork search kd_point = root_node # Save the root node nearest = kd_point.getData() # Initial the nearest node search_path = [] # Initial the search stack while kd_point: search_path.append(kd_point) split_index = kd_point.getSplit() # Ensure the split path if target[split_index] &lt;= kd_point.getData()[split_index]: kd_point = kd_point.getLeft() else: kd_point = kd_point.getRight() nearest = search_path.pop().getData() min_dist = cal_dist(nearest, target) # Retrospect search while search_path: back_point = search_path.pop() # Ensure the back-split path back_split_index = back_point.getSplit() # Judge if needs to enter the subspace if cal_dist(target[back_split_index], back_point.getData()[back_split_index]) &lt; min_dist: # If the target is in the left subspace, then enter the right if target[back_split_index] &lt;= back_point.getData()[back_split_index]: kd_point = back_point.getRight() # Otherwise enter the left else: kd_point = back_point.getLeft() # Add the node to the search path if kd_point is not None: search_path.append(kd_point) if cal_dist(nearest, target) &gt; cal_dist(kd_point.getData(), target): # Update the nearest node nearest = kd_point.getData() # Update the maximum distance min_dist = cal_dist(kd_point.getData(), target) return nearest, min_dist运行结果：12345678910111213141516171819202122232425262728293031323334In [1]: run kdTreeCreate.py-------------------------------------[2 3][5 4][4 7][7 2][8 1][9 6]In [2]: node-------------------------------------Out [2]: &lt;kdTreeNode.kdTreeNode at 0x26bff22f160&gt;In [3]: import kdTreeSearchIn [4]: nearest, min_dist = kdTreeSearch.search(node, [2.1, 3.1])In [5]: nearest-------------------------------------Out [5]: array([2, 3])In [6]: min_dist-------------------------------------Out [6]: 0.14142135623730964In [7]: nearest, min_dist = kdTreeSearch.search(node, [2, 4.5])In [8]: nearest-------------------------------------Out [8]: array([2, 3])In [9]: min_dist-------------------------------------Out [9]: 1.5时间复杂度：N个结点的K维kd树进行查找操作时最坏时间复杂度为 $t_{worst}=O(KN^{1-1/k})$根据相关研究，当数据维度为K时，只有当数据量N满足 $N&gt;&gt;2^K$ 时，才能达到高效的搜索（K&lt;20，超过20维时可采用ball-tree算法），所以引出了一系列的改进算法（BBF算法，和一系列M树、VP树、MVP树等高维空间索引树），留待后续补充。采用kd树的k-近邻算法接下来便是将两者相结合。[1] 说是训练集其实是不准确的，因为k-近邻算法是一个无参数方法，只存在一个超参k，因此其不存在一个训练的过程[2] 测试集[3] N代表训练集的数目[4] 多数表决]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F02%2F15%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new "My New Post"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment]]></content>
    </entry>

    
  
  
</search>
