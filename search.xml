<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[How to make an optimal decision in the case of knowing the environment model(CS294 lecture notes) ?]]></title>
      <url>%2F2019%2F03%2F25%2FHow-to-make-an-optimal-decision-in-the-case-of-knowing-the-environment-model-CS294-lecture-notes%2F</url>
      <content type="text"><![CDATA[é¦–å…ˆæˆ‘ä»¬å‡è®¾çŽ¯å¢ƒæ˜¯ç¡®å®šæ€§çš„ï¼Œå³åœ¨æŸä¸ªçŠ¶æ€æ‰§è¡ŒæŸä¸ªåŠ¨ä½œä¹‹åŽï¼Œè½¬ç§»åˆ°çš„ä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯ç¡®å®šçš„ï¼Œä¸å­˜åœ¨ä»»ä½•éšæœºæ€§ã€‚è€Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æƒ³åšçš„æ˜¯åœ¨çŽ¯å¢ƒç»™äº†æˆ‘ä»¬ä¸€ä¸ªåˆå§‹çŠ¶æ€çš„æ¡ä»¶ä¸‹ï¼Œæ ¹æ®æˆ‘ä»¬éœ€è¦å®Œæˆçš„ä»»åŠ¡ä»¥åŠçŽ¯å¢ƒæ¨¡åž‹ï¼Œç›´æŽ¥å¾—å‡ºä»Žåˆå§‹çŠ¶æ€åˆ°ä»»åŠ¡å®ŒæˆçŠ¶æ€ä¸­é—´æœ€ä¼˜çš„åŠ¨ä½œåºåˆ—ã€‚å› ä¸ºçŽ¯å¢ƒæ˜¯ç¡®å®šçš„ï¼Œè€Œæˆ‘ä»¬åˆå·²çŸ¥çŽ¯å¢ƒæ¨¡åž‹ï¼Œå› è€Œä»¥ä¸Šæƒ³æ³•æ˜¯è‡ªç„¶ä¸”å¯è¡Œçš„ã€‚ä¸‹å›¾å±•ç¤ºäº†æˆ‘ä»¬æƒ³åšçš„äº‹æƒ…ï¼šçŽ°åœ¨æˆ‘ä»¬å°†ä»¥ä¸Šé—®é¢˜æŠ½è±¡æˆä¸€ä¸ªæ­£å¼çš„ä¼˜åŒ–é—®é¢˜ï¼šå…¶ä¸­$f$å°±ä»£è¡¨çŽ¯å¢ƒæ¨¡åž‹ã€‚ä½†æ˜¯ä¸€æ—¦çŽ¯å¢ƒä¸å†æ˜¯ç¡®å®šçš„ï¼Œå³æ­£åœ¨æŸä¸ªçŠ¶æ€æ‰§è¡ŒæŸä¸ªåŠ¨ä½œä¹‹åŽï¼Œè½¬ç§»åˆ°çš„ä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯ä»Žä¸€ä¸ªçŠ¶æ€åˆ†å¸ƒä¸­éšæœºé‡‡æ ·çš„ã€‚å¯¹äºŽè¿™ç§æƒ…å†µï¼Œä¸Šè¿°çš„ä¼˜åŒ–é—®é¢˜å°±ä¼šè½¬å˜ä¸ºæœ€å¤§åŒ–å¦‚ä¸‹å½¢å¼çš„æœŸæœ›å€¼ï¼šä½†æ˜¯åœ¨éšæœºçŽ¯å¢ƒä¸‹ï¼Œè§£å†³ä¸Žç¡®å®šçŽ¯å¢ƒçš„ç±»ä¼¼çš„å¦‚ä¸Šä¼˜åŒ–é—®é¢˜å¹¶ä¸èƒ½å¾—åˆ°ä¸Žç¡®å®šçŽ¯å¢ƒä¸€æ ·çš„æœ€ä¼˜è§£ã€‚åŽŸå› åœ¨äºŽæˆ‘ä»¬åªæŽ¥å—çŽ¯å¢ƒåé¦ˆå›žæ¥çš„åˆå§‹çŠ¶æ€ï¼ŒæŽ¥ç€ä¾¿å‡­å€Ÿç€æˆ‘ä»¬æŽŒæ¡çš„çŽ¯å¢ƒæ¨¡åž‹åœ¨è„‘æµ·ä¸­è¿›è¡Œè§„åˆ’ã€‚è¿™ç§æ–¹æ³•åœ¨ç¡®å®šçŽ¯å¢ƒä¸‹æ²¡æœ‰ä»»ä½•é—®é¢˜ï¼Œä½†åœ¨éšæœºçŽ¯å¢ƒä¸‹ï¼Œæ™ºèƒ½ä½“å®žé™…ä¼šè½¬ç§»åˆ°çš„çŠ¶æ€å¯èƒ½å¹¶ä¸ç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸï¼Œå› ä¸ºå®ƒæ˜¯ä»Žä¸€ä¸ªæ¡ä»¶çŠ¶æ€åˆ†å¸ƒä¸­éšæœºé‡‡æ ·çš„ã€‚è€Œä¸€æ—¦ä»ŽæŸä¸€ä¸ªçŠ¶æ€å¼€å§‹ä¸Žæˆ‘ä»¬çš„é¢„æœŸäº§ç”Ÿåå·®ï¼Œé‚£ä¹ˆåŽç»­çš„æ‰€æœ‰çŠ¶æ€éƒ½ä¼šäº§ç”Ÿåå·®ï¼Œè€Œæˆ‘ä»¬è®¾æƒ³çš„æœ€ä¼˜åŠ¨ä½œåºåˆ—ä¾¿ä¸æ˜¯æœ€ä¼˜äº†ã€‚ä»Žä¼˜åŒ–å‡½æ•°çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ä¼˜åŒ–çš„åªæ˜¯ä¸€ä¸ªæœŸæœ›å€¼ï¼Œè€Œä¸æ˜¯æŸä¸€æ¬¡éšæœºé‡‡æ ·çš„å€¼ã€‚æˆ‘ä»¬æŠŠä¸Šè¿°è§£å†³é—®é¢˜çš„æ–¹æ³•å«åšå¼€çŽ¯æ–¹æ³•ï¼Œä¸Žä¹‹å¯¹åº”çš„å«åšé—­çŽ¯æ–¹æ³•ã€‚é‚£ä¹ˆè¿™ä¸ªâ€œçŽ¯â€å…·ä½“æ˜¯æŒ‡ä»€ä¹ˆå‘¢ï¼Ÿå…·ä½“ç¤ºæ„å›¾å¦‚ä¸‹ï¼šå¼€çŽ¯æ–¹æ³•æ˜¯åªåœ¨æœ€å¼€å§‹æ—¶æŽ¥æ”¶çŽ¯å¢ƒåé¦ˆçš„åˆå§‹çŠ¶æ€ï¼Œç„¶åŽå¼€å§‹è§„åˆ’ä»Žå¼€å§‹åˆ°ä»»åŠ¡å®Œæˆçš„è¿‡ç¨‹ä¸­æ‰€ç»åŽ†çš„æ‰€æœ‰çŠ¶æ€å¯¹åº”çš„æœ€ä¼˜åŠ¨ä½œï¼Œå¹¶ä¸éœ€è¦ä¸€ä¸ªåŸºäºŽçŠ¶æ€äº§ç”ŸåŠ¨ä½œçš„ç­–ç•¥ï¼›åä¹‹ï¼Œé—­çŽ¯æ–¹æ³•åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥éƒ½ä¼šæŽ¥æ”¶çŽ¯å¢ƒåé¦ˆçš„çŠ¶æ€ï¼Œç„¶åŽåˆ©ç”¨ä¸€ä¸ªæ ¹æ®çŠ¶æ€è¾“å‡ºåŠ¨ä½œçš„ç­–ç•¥æ¥äº§ç”Ÿä¸€ä¸ªåŠ¨ä½œã€‚æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œå¯¹äºŽä¸€ä¸ªéšæœºçŽ¯å¢ƒï¼Œé—­çŽ¯æ–¹æ³•æ˜¾ç„¶æ¯”å¼€çŽ¯æ–¹æ³•æ›´å…·ä¼˜åŠ¿ï¼Œå› ä¸ºå…¶å¯ä»¥æ ¹æ®æ‰€å¤„çš„çŠ¶æ€éšæ—¶è°ƒæ•´è‡ªå·±çš„åŠ¨ä½œã€‚ä½†æŽ¥ä¸‹æ¥æˆ‘ä»¬è¿˜æ˜¯é¦–å…ˆå‡å®šä¸€ä¸ªç¡®å®šæ€§çš„çŽ¯å¢ƒï¼Œå› è€Œé‡‡ç”¨å¼€çŽ¯æ–¹æ³•æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚ä¸‹é¢å°†ä»‹ç»ä¸‰ç§ä¼˜åŒ–æ–¹æ³•ï¼šéšæœºä¼˜åŒ–æ–¹æ³•ã€è’™ç‰¹å¡æ´›æ ‘æœç´¢æ³•ä»¥åŠè½¨è¿¹ä¼˜åŒ–æ–¹æ³•ã€‚éšæœºä¼˜åŒ–æ–¹æ³•å¯¹äºŽéšæœºä¼˜åŒ–æ–¹æ³•æ¥è®²ï¼Œä¸Šè¿°ä¼˜åŒ–é—®é¢˜å¯ä»¥ç®€åŒ–ä¸ºå¦‚ä¸‹ç­‰ä»·ä¼˜åŒ–é—®é¢˜ï¼šéšæœºä¼˜åŒ–æ–¹æ³•å®Œå…¨ä¸å…³ç³»ä¼˜åŒ–ç›®æ ‡çš„ç‰¹æ®Šç»“æž„ç­‰ä¿¡æ¯ï¼Œè€Œæ˜¯æŠŠä»»ä½•ä¼˜åŒ–é—®é¢˜éƒ½å½“ä½œä¸Šå›¾å³åŠè¾¹è¿™æ ·çš„ä¸€èˆ¬ä¼˜åŒ–é—®é¢˜ã€‚éšå³å‘å°„æ–¹æ³•æœ€ç®€å•çš„éšæœºä¼˜åŒ–æ–¹æ³•å°±æ˜¯éšæœºçžŽçŒœï¼Œå³éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œåºåˆ—ï¼Œç„¶åŽè¯„ä¼°å…¶ç´¯ç§¯çš„ä»£ä»·ã€‚å¦‚ä¸Šè¿‡ç¨‹ä¸æ–­è¿›è¡Œï¼Œæœ€åŽé€‰æ‹©ä¸€ä¸ªç´¯ç§¯ä»£ä»·æœ€å°çš„åŠ¨ä½œåºåˆ—ä½œä¸ºä¸Šè¿°ä¼˜åŒ–é—®é¢˜çš„æœ€ä¼˜è§£ï¼Œå› è€Œè¿™ç±»æ–¹æ³•ä¹Ÿå«åšâ€œéšæœºå‘å°„æ–¹æ³•â€ï¼šCMAç®—æ³•ä½†æ˜¯è¿™ç§æ–¹æ³•åœ¨ç›¸å¯¹é«˜ç»´çš„æƒ…å†µä¸‹æ•ˆçŽ‡ä¼šå¾ˆä½Žï¼Œå› ä¸ºæœç´¢ç©ºé—´å¤ªå¤§ä½†æ˜¯ç›®æ ‡åŒºåŸŸæ¯”è¾ƒå°ã€‚å›žé¡¾ä¸€ä¸‹ä¸Šè¿°æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é‡‡æ ·åˆ†å¸ƒä¸Šåšäº›æ–‡ç« ã€‚å‡è®¾ç¬¬ä¸€æ¬¡ä»Žä¸€ä¸ªå‡åŒ€åˆ†å¸ƒé‡‡æ ·ä¸€äº›åŠ¨ä½œåºåˆ—ä¹‹åŽï¼Œå¾—åˆ°çš„ç´¯ç§¯ä»£ä»·åˆ†åˆ«ä¸ºå¦‚ä¸‹æƒ…å†µï¼šé‚£ä¹ˆä¸‹ä¸€æ¬¡æˆ‘ä»¬å¯ä»¥ä¸å†ç»§ç»­ä»Žä¸€ä¸ªå‡åŒ€åˆ†å¸ƒä¸­é‡‡æ ·äº†ï¼Œæˆ‘ä»¬å¯ä»¥èšç„¦äºŽç´¯ç§¯ä»£ä»·è¾ƒå°ï¼ˆç´¯ç§¯å›žæŠ¥è¾ƒå¤§ï¼‰çš„åŒºåŸŸï¼Œç„¶åŽä¼°è®¡é‚£ä¸ªåŒºåŸŸçš„åˆ†å¸ƒï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬å‡è®¾åˆ†å¸ƒæ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼šæŽ¥ä¸‹æ¥çš„é‡‡æ ·æˆ‘ä»¬å°±ä»Žè¿™ä¸ªæ–°çš„åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼šç„¶åŽåœ¨ä¸‹ä¸€æ¬¡é‡‡æ ·ä¹‹å‰ï¼Œæˆ‘ä»¬å†æ¬¡èšç„¦äºŽæ€§èƒ½æ›´å¥½çš„åŒºåŸŸç„¶åŽä¼°è®¡å…¶åˆ†å¸ƒï¼šå°±è¿™æ ·ä¸æ–­è¿­ä»£ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ã€‚ä»¥ä¸Šæ–¹æ³•å°±æ˜¯Cross-Entropy Method(CEM)ç®—æ³•ï¼Œå…¶ä¼ªä»£ç å¦‚ä¸‹ï¼šè¯¥æ–¹æ³•è¿˜æœ‰ä¸ªè¿›é˜¶ç‰ˆçš„ç®—æ³•å«åšCMA-ESç®—æ³•ï¼ŒåŽè€…å¯ä»¥çœ‹ä½œæ˜¯CMAç®—æ³•å¸¦åŠ¨é‡çš„ç‰ˆæœ¬ã€‚CMAç®—æ³•ä¼šç›´æŽ¥èˆå¼ƒä¹‹å‰é‡‡æ ·çš„æ•°æ®ç‚¹ï¼Œä½†æ˜¯CMA-ESç®—æ³•ä¼šä¿ç•™éƒ¨åˆ†ä¹‹å‰é‡‡æ ·çš„æ•°æ®ç‚¹çš„ç›¸å…³ä¿¡æ¯ï¼Œç”¨æ¥æŒ‡å¯¼åŽç»­çš„é‡‡æ ·ã€‚å¯ä»¥ç±»æ¯”ä¸€ä¸‹æ¢¯åº¦ä¸‹é™æ³•ä»¥åŠå¸¦åŠ¨é‡çš„æ¢¯åº¦ä¸‹é™æ³•ã€‚æ€»ç»“éšæœºä¼˜åŒ–æ–¹æ³•å…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼šå¹¶è¡ŒåŒ–åŽæ•ˆçŽ‡æžé«˜å®žçŽ°èµ·æ¥ååˆ†ç®€å•ä½†æ˜¯ä¹Ÿå­˜åœ¨å¦‚ä¸‹ä¸å¯é¿å…åœ°ç¼ºç‚¹ï¼šæžæ˜“å—åˆ°ç»´åº¦ç¾éš¾çš„å½±å“åªé€‚ç”¨äºŽå¼€çŽ¯æƒ…å½¢éšå³ä¼˜åŒ–æ–¹æ³•è™½ç„¶å¯ä»¥åŒæ—¶é€‚ç”¨äºŽè¿žç»­å˜é‡ä»¥åŠç¦»æ•£å˜é‡çš„æƒ…å†µï¼Œä½†ä¸æ˜¯ä¸“é—¨ä¸ºç¦»æ•£æƒ…å†µè®¾è®¡çš„ã€‚ä¸‹é¢æˆ‘ä»¬å°†ä»‹ç»ä¸€ç§ä¸“é—¨ä¸ºç¦»æ•£åŠ¨ä½œç©ºé—´è®¾è®¡çš„å¼ºå¤§çš„ä¼˜åŒ–æ–¹æ³•ï¼ˆä¸¥æ ¼æ¥è®²å«åšå¯å‘å¼æœç´¢ç®—æ³•ï¼‰ï¼šè’™ç‰¹å¡æ´›æ ‘æœç´¢MCTSæ–¹æ³•ã€‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•MCTSæ–¹æ³•æœ¬è´¨æ˜¯ä¸€ä¸ªæœç´¢ç®—æ³•ï¼šå‡è®¾æˆ‘ä»¬æƒ³è¦è®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿè‡ªåŠ¨åŽ»çŽ©ä¸Šé¢è¿™ä¸ªæ¸¸æˆï¼ˆå‡»æ²‰æ•Œæ–¹æ½œæ°´è‰‡å°†ä¼šèŽ·å¾—åˆ†æ•°ï¼Œä½†æ˜¯æ½œæ°´è‰‡è‡ªèº«çš„æ°§æ°”å‚¨å­˜é‡æ˜¯é€æ¸å‡å°‘çš„ï¼Œéœ€è¦ä¸æ—¶åœ°åŽ»æµ®å‡ºæ°´é¢è¡¥å……æ°§æ°”ã€‚è¢«æ•Œæ–¹æ½œæ°´è‰‡æ’žä¸Šä¼šæŸå¤±ç”Ÿå‘½å€¼ï¼Œæ¸¸æˆç›®æ ‡å°±æ˜¯èŽ·å¾—å°½å¯èƒ½å¤šçš„åˆ†æ•°ï¼‰ã€‚ä¸€ä¸ªç®€å•çš„æš´åŠ›æœç´¢ç®—æ³•å¯èƒ½ä¼šåŒ…å«ä¸Šå›¾å³è¾¹çš„è¿‡ç¨‹ï¼Œå‡è®¾ä¸€æ®µæœ€ä¼˜åŠ¨ä½œåºåˆ—ä»…ä»…åŒ…å«åä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸ªçŠ¶æ€ä¸‹ä»…ä»…åŒ…å«ä¸¤ä¸ªå¯èƒ½åŠ¨ä½œï¼Œé‚£ä¹ˆæœ€åŽä¸€ä¸ªæ—¶é—´æ­¥å°±åŒ…å«1024ä¸ªå¯èƒ½æ€§ã€‚ä½†å¯¹äºŽå¤§å¤šæ•°é—®é¢˜æ¥è¯´ï¼Œåä¸ªæ—¶é—´æ­¥è¿œè¿œä¸è¶³ä»¥å®Œæˆç›®æ ‡ï¼Œå› è€Œæš´åŠ›æœç´¢ç®—æ³•æ˜¯ä¸å¯è¡Œçš„ã€‚é‚£ä¹ˆè’™ç‰¹å¡æ´›ç®—æ³•æ˜¯å¦‚ä½•åœ¨ä¸ç©·ä¸¾æ‰€æœ‰å¯èƒ½æ€§ç›´åˆ°åˆ°è¾¾ç»ˆç‚¹çš„æƒ…å†µä¸‹å¯¹ä¸€ä¸ªåŠ¨ä½œåºåˆ—è¿›è¡Œè¯„ä¼°çš„å‘¢ï¼Ÿè€ƒè™‘æ½œæ°´è‰‡æ¸¸æˆï¼Œåœ¨æ½œæ°´è‰‡åšå‡ºæ”»å‡»æŒ‡ä»¤åŽï¼Œç”±äºŽç‚®å¼¹çš„è¿è¡Œéœ€è¦æ—¶é—´ï¼Œå› è€Œå‡ ä¸ªæ—¶é—´æ­¥ä¹‹åŽæ•Œæ–¹æ½œæ°´è‰‡æ‰ä¼šè¢«å‡»æ²‰ä»Žè€Œå—åˆ°å¥–åŠ±ï¼Œåœ¨æ½œæ°´è‰‡åšå‡ºæ”»å‡»æŒ‡ä»¤é‚£ä¸ªæ—¶é—´æ­¥æ˜¯æ²¡æœ‰ä»»ä½•å¥–åŠ±çš„ï¼Œå› è€Œæ™ºèƒ½ä½“å¯èƒ½è®¤ä¸ºè¿™ä¸ªåŠ¨ä½œå¹¶ä¸æ˜¯ä¸€ä¸ªä¼˜ç§€çš„åŠ¨ä½œã€‚å¯¹äºŽä»¥ä¸Šæƒ…å†µï¼Œå…¶å®žæˆ‘ä»¬åªéœ€è¦åœ¨åšå‡ºæ”»å‡»æŒ‡ä»¤åŽï¼Œå¦‚æžœè¦è¯„ä¼°è¿™ä¸ªåŠ¨ä½œçš„ä¼˜åŠ£ï¼Œâ€œç­‰å¾…â€å‡ ä¸ªæ—¶é—´æ­¥ å³å¯ã€‚è’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•æ­£æ˜¯é‡‡ç”¨è¿™ç§æ€æƒ³ï¼ŒåŒæ ·ç”¨ä¸Šå›¾å³è¾¹çš„è¿‡ç¨‹ä¸¾ä¾‹ï¼Œå½“åŠ¨ä½œæ‰§è¡Œåˆ°ç¬¬ä¸‰å±‚æ—¶ï¼Œå¦‚ä½•è¯„ä¼°è¿™å››ä¸ªåŠ¨ä½œåºåˆ—çš„æ€§èƒ½å¥½åå‘¢ï¼Ÿç®—æ³•è¿›è¡Œäº†æŸç§â€œç­‰å¾…â€ï¼Œå³ä»Žç¬¬ä¸‰å±‚å¼€å§‹ï¼Œä¸å†æŠŠæ ‘è¿›è¡Œå®Œå…¨çš„æ‰©å±•äº†ï¼Œè€Œæ˜¯é‡‡ç”¨ä¸€ä¸ªéšæœºç­–ç•¥éšæœºæ‰§è¡ŒåŠ¨ä½œç›´åˆ°æ¸¸æˆç»“æŸæˆ–è€…åˆ°è¾¾æŸä¸ªè®¾å®šçš„æ—¶é—´æ­¥ã€‚è¿™å°±ç±»ä¼¼äºŽåœ¨æ½œæ°´è‰‡æ¸¸æˆä¸­ï¼Œæ½œæ°´è‰‡åœ¨å‘å‡ºç‚®å¼¹åŽï¼Œéšæœºæ‰§è¡Œä¸€äº›åŠ¨ä½œï¼Œç›´åˆ°ç‚®å¼¹å‡»ä¸­æ•Œæ–¹æ½œæ°´è‰‡ã€‚è€Œè’™ç‰¹å¡æ´›ç®—æ³•æ­£æ˜¯é€šè¿‡è¿™ç§è¯„ä¼°æ–¹æ³•æ¥é¿å…æš´åŠ›æœç´¢ï¼Œå…·ä½“æ¥è¯´ï¼Œè’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•ä¼šæ ¹æ®è¯„ä¼°ç»“æžœçš„å¥½åä»¥åŠè®¿é—®æ¬¡æ•°æ¥å†³å®šä¸‹ä¸€æ­¥åº”è¯¥æœç´¢å“ªä¸€æ¡è·¯å¾„ï¼šå¯èƒ½ä»¥ä¸Šæè¿°æœ‰ç‚¹éš¾ä»¥ç†è§£ï¼Œé‚£ä¹ˆä¸‹é¢æˆ‘ä»¬è¿‡ä¸€éè’™ç‰¹å¡æ´›æ ‘æœç´¢æ–¹æ³•çš„æœç´¢è¿‡ç¨‹ã€‚æˆ‘ä»¬é¦–å…ˆç»™å‡ºç®—æ³•çš„æ‰§è¡Œæ­¥éª¤ï¼šé¦–å…ˆæˆ‘ä»¬å¤„äºŽä¸€ä¸ªåˆå§‹çŠ¶æ€ï¼šç„¶åŽæˆ‘ä»¬è¿›è¡Œç®—æ³•ç¬¬ä¸€æ­¥ï¼Œæ ¹æ®ä¸€ä¸ªâ€œæ ‘ç­–ç•¥â€æ‰¾åˆ°ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œæ³¨æ„è¿™é‡Œæ‰¾åˆ°ä¸€ä¸ªå¶èŠ‚ç‚¹çš„æ„æ€æ˜¯æ‰¾åˆ°ä¸€ä¸ª*æ–°çš„å¶èŠ‚ç‚¹ã€‚æ ‘ç­–ç•¥çš„å…·ä½“å½¢å¼å¦‚ä¸‹ï¼šæ ¹æ®ä»¥ä¸Šç­–ç•¥ï¼Œç”±äºŽåˆå§‹çŠ¶æ€æ²¡æœ‰è¢«å®Œå…¨æ‰©å±•ï¼Œå› è€Œéšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œå¹¶æ‰§è¡Œç¬¬äºŒæ­¥ä½¿ç”¨é»˜è®¤ç­–ç•¥æ¥è¯„ä¼°æ‰§è¡Œè¿™ä¸ªåŠ¨ä½œçš„å¥½åï¼Œè¿™é‡Œé»˜è®¤ç­–ç•¥ä½¿ç”¨çš„æ˜¯éšæœºé‡‡æ ·ç­–ç•¥ï¼šå‡è®¾è¯„ä¼°ç»“æžœå¦‚ä¸‹ï¼šè¿™é‡ŒQä»£è¡¨çŽ¯å¢ƒå®šä¹‰çš„å›žæŠ¥ï¼ŒNä»£è¡¨è®¿é—®è¿™ä¸ªçŠ¶æ€çš„æ¬¡æ•°ã€‚è¿™é‡Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒNè®°å½•çš„å¹¶ä¸æ˜¯æŸä¸ªå…·ä½“çš„çŠ¶æ€çš„è®¿é—®æ¬¡æ•°ï¼Œè€Œæ˜¯æ‰§è¡ŒæŸä¸ªåŠ¨ä½œçš„æ¬¡æ•°ï¼Œæ‰§è¡Œè¿™ä¸ªåŠ¨ä½œåŽåœ¨éšæœºçŽ¯å¢ƒä¸‹å¯èƒ½è½¬ç§»åˆ°å¾ˆå¤šä¸ªä¸åŒçš„çŠ¶æ€ï¼Œä½†åœ¨æ ‘ä¸­å‡æ˜¾ç¤ºä¸ºä¸€ä¸ªèŠ‚ç‚¹ã€‚è¯„ä¼°å®Œä¹‹åŽï¼Œæˆ‘ä»¬éœ€è¦æ›´æ–°æ ¹èŠ‚ç‚¹åˆ°è¿™ä¸ªæ–°åŠ å…¥çš„å¶èŠ‚ç‚¹ä¹‹é—´æ‰€æœ‰èŠ‚ç‚¹çš„Qå€¼ä»¥åŠNå€¼ã€‚ç”±äºŽè¿™é‡Œä¸¤è€…ä¹‹é—´å¹¶æ²¡æœ‰å…¶ä»–çš„èŠ‚ç‚¹ï¼Œå› è€Œè·³è¿‡è¿™ä¸€æ­¥ã€‚ç„¶åŽä»¥ä¸Šè¿‡ç¨‹å¼€å§‹å¾ªçŽ¯ï¼Œæˆ‘ä»¬å†å°†çŠ¶æ€è·³å›žåˆ°åˆå§‹çŠ¶æ€ï¼Œéµå¾ªæ ‘ç­–ç•¥ï¼Œæ‰¾åˆ°ä¸‹ä¸€ä¸ªæ–°çš„å¶èŠ‚ç‚¹ã€‚ç”±äºŽåˆå§‹çŠ¶æ€è¿˜æ˜¯æ²¡æœ‰æ‰©å±•å®Œæ¯•ï¼Œå› æ­¤è¿™ä¸€æ¬¡æ‰§è¡Œä¸‹ä¸€ä¸ªæœªè¢«æ‰§è¡Œè¿‡çš„çŠ¶æ€ï¼šå†é‡‡ç”¨é»˜è®¤ç­–ç•¥å¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼Œå‡è®¾æˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹ç»“æžœï¼šç”±äºŽæ ¹èŠ‚ç‚¹ä¸Žæ–°çš„å¶èŠ‚ç‚¹ä¹‹é—´çš„è·¯å¾„å¹¶æ²¡æœ‰å…¶ä»–èŠ‚ç‚¹ï¼Œå› è€Œæ›´æ–°æ­¥éª¤ç•¥è¿‡ã€‚å†æ¬¡é‡å¤ä»¥ä¸Šè¿‡ç¨‹ï¼Œå°†çŠ¶æ€è·³å›žåˆ°åˆå§‹çŠ¶æ€ï¼Œæ‰§è¡Œæ ‘ç­–ç•¥æ‰¾åˆ°ä¸€ä¸ªæ–°çš„å¶èŠ‚ç‚¹ã€‚é¦–å…ˆæ ¹æ®æ ‘ç­–ç•¥ï¼Œåˆå§‹çŠ¶æ€å·²ç»è¢«å®Œå…¨æ‰©å±•å¼€äº†ï¼ˆå³æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œå‡å·²ç»æ‰§è¡Œè¿‡ï¼‰ï¼Œè¿™ä¸ªæ—¶å€™æˆ‘ä»¬æ ¹æ®æ ‘ç­–ç•¥ä¸­çš„å…¬å¼è®¡ç®—æ¯ä¸€æ¡è·¯å¾„çš„ä¸€ä¸ªåˆ†æ•°ã€‚ä»Žåˆ†æ•°è®¡ç®—å…¬å¼å¯ä»¥çœ‹å‡ºï¼Œè¿™ä¸ªåˆ†æ•°æ˜¯åŒæ—¶è€ƒè™‘åŠ¨ä½œçš„å›žæŠ¥ä»¥åŠåŠ¨ä½œçš„æ‰§è¡Œæ¬¡æ•°ï¼Œæ›´åŠ å€¾å‘äºŽæ‰§è¡Œè¢«æ‰§è¡Œæ¬¡æ•°å°‘çš„å›žæŠ¥é«˜çš„åŠ¨ä½œã€‚åœ¨è¿™é‡Œï¼Œç”±äºŽä¸¤ä¸ªåŠ¨ä½œè¢«æ‰§è¡Œæ¬¡æ•°å‡ä¸º1ï¼Œå› è€Œæˆ‘ä»¬é€‰æ‹©å›žæŠ¥æ›´é«˜çš„ç¬¬äºŒä¸ªåŠ¨ä½œæ‰§è¡Œï¼Œç„¶åŽå†æ ¹æ®æ ‘ç­–ç•¥ï¼ˆåœ¨æ²¡æœ‰æ‰¾åˆ°æ–°çš„å¶èŠ‚ç‚¹ä¹‹å‰ï¼Œå¾ªçŽ¯æ‰§è¡Œæ ‘ç­–ç•¥ï¼‰ï¼Œç¬¬äºŒå±‚çš„çŠ¶æ€æ²¡æœ‰è¢«å®Œå…¨æ‰©å±•ï¼Œå› è€Œéšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œæ‰§è¡Œï¼šä¾æ®é»˜è®¤ç­–ç•¥è¿›è¡Œè¯„ä¼°ï¼šæ³¨æ„ï¼Œåˆ°äº†è¿™ä¸€æ­¥ï¼Œæ ¹èŠ‚ç‚¹åˆ°æ–°çš„å¶èŠ‚ç‚¹ä¹‹é—´çš„è·¯å¾„å­˜åœ¨å…¶ä»–èŠ‚ç‚¹äº†ï¼Œæˆ‘ä»¬å°±è¦ç”¨æœ€æ–°çš„å¶èŠ‚ç‚¹çš„è¯„ä¼°å€¼ä»¥åŠè®¿é—®æ¬¡æ•°åŠ åˆ°è¿™äº›ä¸­é—´èŠ‚ç‚¹çš„è¯„ä¼°å€¼ä»¥åŠè®¿é—®æ¬¡æ•°ä¸Šï¼šå†æ¬¡é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œå°†çŠ¶æ€è·³å›žåˆ°åˆå§‹çŠ¶æ€ï¼Œè°ƒç”¨æ ‘ç­–ç•¥ï¼Œè¿™æ—¶å€™æ ¹æ®åˆ†æ•°è®¡ç®—å…¬å¼ï¼Œåœ¨å‡è®¾ä¸€äº›è¶…å‚æ•°çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‡å®šè¿™ä¸ªæ—¶å€™æ›´åŠ ä¾§é‡äºŽæ‰§è¡Œè¢«æ‰§è¡Œæ¬¡æ•°æ›´å°çš„åŠ¨ä½œå¹¶è¯„ä¼°ï¼šç„¶åŽå†æ›´æ–°å†è·³å›žâ€¦â€¦å¦‚æžœæƒ³è¯¦ç»†äº†è§£è’™ç‰¹å¡æ´›æ ‘æœç´¢ç®—æ³•çš„æ‰©å±•ä»¥åŠåº”ç”¨ï¼Œå¯ä»¥å‚è€ƒä¸‹é¢è¿™ç¯‡ç»¼è¿°ï¼šè¿™é‡Œè®²ä¸€ä¸ªæ¯”è¾ƒæœ‰æ„æ€çš„æ¡ˆä¾‹ï¼šå…¶æ€æƒ³å…¶å®žæ˜¯å°†DAggerç®—æ³•ä¸ŽMCTSç®—æ³•è¿›è¡Œç»“åˆã€‚ç”±äºŽDAggerç®—æ³•éœ€è¦äººå·¥çš„ä¸æ–­å‚ä¸Žè¿›è¡Œæ–°æ•°æ®çš„æ ‡æ³¨ï¼Œä»¥ä¸Šæ¡ˆä¾‹å°†ä¸“å®¶æ ‡æ³¨çš„è¿‡ç¨‹ç”¨MCTSç®—æ³•è¿›è¡Œæ›¿ä»£ï¼Œå­¦ä¹ ä¸€ä¸ªMCTSçš„ç­–ç•¥ä¼°è®¡å™¨ï¼šé‚£ä¹ˆä¸ºä»€ä¹ˆä¸ç›´æŽ¥ä½¿ç”¨MCTSç®—æ³•å‘¢ï¼Ÿå…¶å®žæ˜¯åŸºäºŽä»¥ä¸‹ä¸¤ç‚¹è€ƒè™‘çš„ï¼šå®žæ—¶æ€§è¦æ±‚è¾ƒé«˜çš„ä»»åŠ¡ä¸­MCTSå¤ªæ…¢äº†é‡‡ç”¨ç±»ä¼¼ç¥žç»ç½‘ç»œçš„ç­–ç•¥ä¼°è®¡å™¨å…·æœ‰æ›´å¥½çš„æ³›åŒ–æ€§è·¯å¾„ä¼˜åŒ–ç®—æ³•è®©æˆ‘ä»¬å†æ¬¡å›žé¡¾ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜ï¼šç›´æŽ¥ä¸¢å¼ƒæŽ‰ä»¥ä¸Šä¼˜åŒ–é—®é¢˜ä¸­çš„ç‰¹æ®Šç»“æž„æ˜¾ç„¶ä¸æ˜¯ååˆ†æ°å½“çš„ï¼ŒæŽ¥ä¸‹æ¥è®©æˆ‘ä»¬å›žåˆ°ä¸€èˆ¬è§£å†³ä»¥ä¸Šä¼˜åŒ–é—®é¢˜çš„æ€è·¯ã€‚æˆ‘ä»¬ä¸€çœ‹åˆ°ä»¥ä¸Šé—®é¢˜ï¼Œå°±ä¼šé¦–å…ˆæƒ³åˆ°èƒ½ä¸èƒ½åˆ©ç”¨ç±»ä¼¼æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•å‘¢ï¼Ÿä¸ºäº†ä¸Žæœ€ä¼˜æŽ§åˆ¶ä¸­è·¯å¾„ä¼˜åŒ–ç®—æ³•çš„ä¸€èˆ¬ç¬¦å·è®°æ³•ä¸€è‡´ï¼Œæˆ‘ä»¬å°†ä»¥ä¸Šé—®é¢˜é‡å†™ä¸ºä»¥ä¸‹å½¢å¼ï¼šæˆ‘ä»¬å¯ä»¥å°†çº¦æŸéƒ¨åˆ†æ”¾è¿›ä¼˜åŒ–å‡½æ•°ä¸­ä»Žè€Œå°†ä»¥ä¸Šé—®é¢˜è½¬å˜ä¸ºä¸€ä¸ªæ— çº¦æŸé—®é¢˜ï¼šå¯¹äºŽä»¥ä¸Šé—®é¢˜ï¼Œåªè¦æˆ‘ä»¬çŸ¥æ™“ä»¥ä¸‹å››é¡¹ï¼Œå³å¯æ ¹æ®é“¾å¼æ³•åˆ™å¾—å‡ºå…¶æœ€ä¼˜è§£ï¼šLQRç®—æ³•ç¡®å®šæ€§çŽ¯å¢ƒä¸ºäº†è§£å†³ä»¥ä¸Šä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬æŽ¥ä¸‹æ¥å°†ä»‹ç»ä¸€ç§è·¯å¾„ä¼˜åŒ–ç®—æ³•LQRï¼Œæ­¤ç®—æ³•å‡è®¾çŽ¯å¢ƒæ¨¡åž‹æ˜¯çº¿æ€§çš„ï¼Œå¹¶ä¸”ä»£ä»·å‡½æ•°æ˜¯äºŒæ¬¡çš„ï¼šä¸ºäº†è§£å†³è¿™ç§ç‰¹æ®Šå½¢å¼çš„ä»¥ä¸Šä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨åŠ¨æ€è§„åˆ’çš„æ€æƒ³ï¼Œå…ˆæ‰¾å‡ºæœ€ä¼˜çš„æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œã€‚ä¹‹æ‰€ä»¥è¿™æ ·åšï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬å¯ä»¥å‘çŽ°ï¼Œä»¥ä¸Šè¿žåŠ é¡¹ä¸­åªæœ‰æœ€åŽä¸€é¡¹æ˜¯ä¸Žæœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œç›¸å…³çš„ã€‚å¦‚æžœæˆ‘ä»¬é¦–å…ˆè§£å†³ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œï¼Œé‚£ä¹ˆè¿žåŠ é¡¹çš„æ‰€æœ‰é¡¹éƒ½ä¸Žç¬¬ä¸€ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œç›¸å…³ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠŠæœ€åŽä¸€é¡¹ä¸­è¿žç»­çš„å‡½æ•°æ±‚å€¼ç®€å†™ä¸º$x_{T}$ï¼Œæ³¨æ„è¿™ä¸ªå€¼æ˜¯æœªçŸ¥çš„ã€‚è¿›è¡Œäº†ä»¥ä¸Šçš„å‡†å¤‡å·¥ä½œåŽï¼Œæ±‚è§£æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œå¯¹åº”çš„ä¼˜åŒ–ç›®æ ‡å¦‚ä¸‹ï¼Œæˆ‘ä»¬æŠŠå…¶è®°ä¸º$Q(x_T,u_T)$ï¼šç„¶åŽæˆ‘ä»¬å°†çº¿æ€§é¡¹ç³»æ•°ä»¥åŠäºŒæ¬¡é¡¹ç³»æ•°å±•å¼€ï¼šç„¶åŽï¼Œä¸ºäº†å¾—å‡ºæœ€ä¼˜åŠ¨ä½œï¼Œæˆ‘ä»¬ä»¤è¿™ä¸ªä¼˜åŒ–ç›®æ ‡å…³äºŽæœ€åŽä¸€ä¸ªæ—¶é—´æ­¥åŠ¨ä½œçš„æ¢¯åº¦ç­‰äºŽ0ï¼šæ±‚è§£ä»¥ä¸Šçº¿æ€§æ–¹ç¨‹ï¼Œå¯ä»¥å¾—å‡ºæœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œä¸ºï¼šå°†å…¶è¿›è¡Œç®€å•çš„è½¬åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œæœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œæ˜¯æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çŠ¶æ€ï¼ˆçŽ°åœ¨è¿˜æ˜¯æœªçŸ¥é¡¹ï¼‰çš„çº¿æ€§å‡½æ•°ï¼ˆä»¥ä¸Šå…³ç³»é€‚ç”¨äºŽæ‰€æœ‰æ—¶é—´æ­¥ï¼‰ï¼šå…¶ä¸­ï¼šç”±äºŽæœ€åŽä¸€æ­¥çš„æœ€ä¼˜åŠ¨ä½œå®Œå…¨å¯ä»¥ç”¨æœ€åŽä¸€æ­¥çš„çŠ¶æ€è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºæœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„æœ€ä¼˜çš„Qå€¼ï¼Œè¿™é‡Œæˆ‘ä»¬å°†å…¶è®°ä¸ºVï¼šè¿™é‡Œçš„Qå€¼ä»¥åŠVå€¼å…¶å®žæ˜¯å’Œå¼ºåŒ–å­¦ä¹ ä¸­çš„å®šä¹‰æ˜¯ä¸€è‡´çš„ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸Šå¼å±•å¼€ï¼šå°†ä¸Šå¼åˆå¹¶åŒç±»é¡¹å¯å¾—ï¼šå…¶ä¸­ï¼šå› è€Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°å¦ä¸€ä¸ªå…³ç³»ï¼Œæœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„Vå€¼ï¼ˆæœ€ä¼˜Qå€¼ï¼‰æ˜¯æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çŠ¶æ€çš„äºŒæ¬¡å‡½æ•°ï¼ˆä»¥ä¸Šå…³ç³»é€‚ç”¨äºŽæ‰€æœ‰æ—¶é—´æ­¥ï¼‰ã€‚è¿›è¡Œåˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»è§£å‡ºæœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œäº†ã€‚æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦åœ¨æ­¤åŸºç¡€ä¸Šè§£å‡ºå€’æ•°ç¬¬äºŒä¸ªæ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œã€‚é¦–å…ˆæˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå€’æ•°ç¬¬äºŒä¸ªæ—¶é—´æ­¥çš„Qå€¼å‡½æ•°å¯ä»¥è®°ä¸ºï¼šå°†çŽ¯å¢ƒæ¨¡åž‹å¼•å…¥å¯å°†Vå€¼å±•å¼€ï¼šæˆ‘ä»¬å°†å±•å¼€åŽçš„Vå€¼ä»£å…¥å€’æ•°ç¬¬äºŒæ—¶é—´æ­¥çš„Qå€¼å‡½æ•°ä¸­ï¼šå…¶ä¸­ï¼šåŒæ ·ï¼Œä¸ºäº†æ±‚å‡ºå€’æ•°ç¬¬äºŒä¸ªæ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œï¼Œæˆ‘ä»¬ä»¤ç›¸å…³æ¢¯åº¦ä¸ºé›¶ï¼šè§£å¾—å€’æ•°ç¬¬äºŒä¸ªæ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œä¸ºï¼šå…¶ä¸­ï¼šè®©æˆ‘ä»¬ä¸æ–­åœ°é‡å¤ä»¥ä¸Šè¿‡ç¨‹ï¼Œç›´åˆ°ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºŽæ¯ä¸€æ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œä¸Žé‚£ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€æœ‰å…³ï¼Œä½†æ˜¯çŠ¶æ€æ˜¯æœªçŸ¥çš„ã€‚å½“æ•´ä¸ªè¿‡ç¨‹å›žæº¯åˆ°åˆå§‹æ—¶é—´æ­¥æ—¶ï¼Œæƒ…å†µå‘ç”Ÿäº†å˜åŒ–ï¼Œåˆå§‹çŠ¶æ€æˆ‘ä»¬æ˜¯å·²çŸ¥çš„ï¼å› è€Œï¼Œæˆ‘ä»¬å°±å¯ä»¥ç®—æ³•åˆå§‹æ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œã€‚åˆ©ç”¨çŽ¯å¢ƒæ¨¡åž‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—çŸ¥ç¬¬äºŒä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€ï¼Œå¦‚æ­¤å¾ªçŽ¯ä¸‹åŽ»ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—çŸ¥æ‰€æœ‰æ—¶é—´æ­¥çš„æœ€ä¼˜åŠ¨ä½œï¼šä»¥ä¸Šå°±æ˜¯æ•´ä¸ªLQRç®—æ³•çš„æ‰§è¡Œè¿‡ç¨‹ã€‚éžç¡®å®šæ€§çŽ¯å¢ƒ ï¼ˆæœªå®Œæˆï¼‰å¯¹äºŽéžç¡®å®šæ€§çŽ¯å¢ƒï¼Œå‡è®¾æˆ‘ä»¬çš„çŽ¯å¢ƒæ¨¡åž‹å¦‚ä¸‹ï¼šé‚£ä¹ˆLQRç®—æ³•ä¾æ—§æ˜¯å®Œå…¨å¯è¡Œçš„ã€‚iLQRç®—æ³•LQRç®—æ³•ç”±äºŽå‡è®¾çŽ¯å¢ƒæ¨¡åž‹ä»¥åŠä»£ä»·å‡½æ•°æ˜¯çº¿æ€§ä»¥åŠäºŒæ¬¡çš„ï¼Œè¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œå¯¹äºŽæ›´åŠ å¤æ‚çš„ä»»åŠ¡æ˜¾ç„¶ä¸èƒ½å¾ˆå¥½çš„ä¼°è®¡ã€‚å› è€Œï¼Œè§£å†³è¿™ä¸ªé—®é¢˜çš„iLQRç®—æ³•åº”è¿è€Œç”Ÿã€‚å…¶åŸºæœ¬æ€æƒ³å¾ˆç®€å•ï¼Œæ—¢ç„¶çº¿æ€§ä»¥åŠäºŒæ¬¡å‡½æ•°ä¸è¶³ä»¥ä¼°è®¡å…¨å±€çš„çœŸå®žå‡½æ•°ï¼Œé‚£ä¹ˆä¼°è®¡å±€éƒ¨çš„æ€»æ˜¯è¶³å¤Ÿçš„ã€‚å› è€Œæˆ‘ä»¬å¯ä»¥å¯¹çŽ¯å¢ƒæ¨¡åž‹ä»¥åŠä»£ä»·å‡½æ•°åˆ†åˆ«åšä¸€é˜¶ä»¥åŠäºŒé˜¶çš„æ³°å‹’å±•å¼€ï¼ï¼šé‚£ä¹ˆæˆ‘ä»¬çš„é—®é¢˜å…¶å®žåˆè½¬å˜å›žäº†åŽŸå§‹çš„LQRè®¾å®šï¼šå…¶ä¸­ï¼šiLQRç®—æ³•çš„å…·ä½“æ¡†æž¶å¦‚ä¸‹ï¼šè¯¥ç®—æ³•ä¹‹æ‰€ä»¥é‡‡ç”¨è¿­ä»£çš„å½¢å¼ï¼Œæ˜¯å› ä¸ºå…¶éœ€è¦ä¸æ–­åœ°ç”¨çœŸå®žæ ·æœ¬æ¥åŽ»â€çŸ«æ­£â€œå…¶å¯¹äºŽçŽ¯å¢ƒæ¨¡åž‹ä»¥åŠä»£ä»·å‡½æ•°çš„ä¼°è®¡ã€‚æ›´ä¸¥æ ¼æ¥è®²ï¼Œè¯¥ç®—æ³•ä¹‹æ‰€ä»¥èƒ½å¤Ÿè¾¾åˆ°å¾ˆå¥½çš„æ•ˆæžœï¼Œæ˜¯å› ä¸ºå®ƒå’Œç‰›é¡¿æ–¹æ³•çš„æœ¬è´¨æ˜¯ä¸€æ ·çš„ï¼ˆé€šè¿‡æ³°å‹’å±•ç¤ºæ¥ä¼°è®¡ä¸€ä¸ªå¤æ‚çš„éžçº¿æ€§å‡½æ•°çš„å±€éƒ¨ç‰¹æ€§ï¼‰ï¼šè€Œå¦‚æžœæˆ‘ä»¬å¯¹çŽ¯å¢ƒæ¨¡åž‹ä¼°è®¡æ—¶ä¹Ÿè¿›è¡ŒäºŒé˜¶æ³°å‹’å±•å¼€ï¼šé‚£ä¹ˆæˆ‘ä»¬çš„ç®—æ³•å°±å˜ä¸ºå¾®åˆ†åŠ¨æ€è§„åˆ’ç®—æ³•ï¼ˆDDPï¼‰ã€‚ä½†æ˜¯åœ¨å®žé™…æƒ…å†µä¸­ï¼Œä»£ä»·å‡½æ•°çš„å½¢å¼ä¸€èˆ¬æ¯”è¾ƒç®€å•ï¼Œå› è€Œè¿›è¡ŒäºŒé˜¶æ³°å‹’å±•å¼€ä»£ä»·ä¸å¤§ã€‚ä½†æ˜¯çŽ¯å¢ƒæ¨¡åž‹ä¸€èˆ¬æ˜¯ååˆ†å¤æ‚çš„ï¼Œä¸€é˜¶å±•å¼€è¿˜å¥½ï¼Œä¸€æ—¦è¿›è¡ŒäºŒé˜¶å±•å¼€å…¶å¤æ‚æ€§å°†ä¼šå¤§å¤§å¢žåŠ ã€‚äº‹å®žè¡¨æ˜Žä¸€é˜¶å±•å¼€å…¶å®žæ˜¯è¶³å¤Ÿçš„ã€‚ä½†æ˜¯ä»¥ä¸Šç®—æ³•è¿˜å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œè€ƒè™‘ä»¥ä¸‹ä¼°è®¡è¯¯å·®ï¼šå¯¹äºŽè¿™ç§æƒ…å†µï¼Œå…¶å®žæˆ‘ä»¬åªè¦ç®€å•çš„åœ¨åŽŸå§‹iLQRç®—æ³•ä¸­åŠ ä¸€ä¸ªline searchè¿‡ç¨‹å³å¯ï¼šæœ€åŽæˆ‘ä»¬çœ‹ä¸€ä¸ªiLQRç®—æ³•åœ¨å®žé™…æƒ…å†µåº”ç”¨çš„å®žä¾‹ï¼šä¸ºäº†ä¿è¯iLQRæ›´åŠ ç¨³å®šï¼Œè¿™ä¸ªå·¥ä½œé‡‡ç”¨äº†å¦‚ä¸‹å½¢å¼çš„æ”¹è¿›ï¼šå³åœ¨æ¯ä¸€æ­¥éƒ½è¿›è¡Œä¸€ä¸ªå®Œæ•´çš„è§„åˆ’ï¼Œä½†æ˜¯è€ƒè™‘åˆ°iLQRçš„ä¼°è®¡è¯¯å·®éšç€æ—¶é—´ä¼šäº§ç”Ÿç´¯ç§¯ï¼Œå› è€Œæ¯æ¬¡åªæ‰§è¡Œè§„åˆ’çš„ç¬¬ä¸€æ­¥ã€‚]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TD-VAE [ICLR 2019]]]></title>
      <url>%2F2019%2F03%2F20%2FTD-VAE%2F</url>
      <content type="text"><![CDATA[ç®€ä»‹ã€ç¬”è®°ç‰ˆã€‘ä»Šå¤©è¦è®²çš„æ˜¯ICLR2019ä¸­DeepMindçš„ä¸€ä¸ªå·¥ä½œï¼ŒTD-VAEï¼Œä¸€ä¸ªåºåˆ—ç”Ÿæˆæ¨¡åž‹ã€‚é€šè¿‡å¼•å…¥å¼ºåŒ–å­¦ä¹ ä¸­æ—¶åºå·®åˆ†ä»¥åŠå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼Œæ¥å®žçŽ°ä»Žå½“å‰æ—¶é—´æ­¥åˆ°æœªæ¥æ—¶é—´æ­¥çš„é¢„æµ‹ã€‚è¿™é‡Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTD-VAEå¹¶ä¸æ˜¯ä¸€ä¸ªå›ºå®šæ—¶é—´æ­¥çš„åºåˆ—ç”Ÿæˆæ¨¡åž‹ï¼ˆå½“ç„¶å¦‚æžœè®­ç»ƒæ—¶å–‚çš„è®­ç»ƒæ•°æ®æ˜¯ä¸€ä¸ªæ—¶é—´é—´éš”å›ºå®šçš„åºåˆ—æ•°æ®ï¼Œé‚£ä¹ˆè®­ç»ƒå‡ºçš„æ¨¡åž‹å°±æ˜¯å›ºå®šæ—¶é—´æ­¥çš„åºåˆ—ç”Ÿæˆæ¨¡åž‹ï¼‰ï¼Œå³å…¶ç”Ÿæˆçš„æ•°æ®æ—¶é—´é—´éš”ä¸æ˜¯ä¸€ä¸ªå›ºå®šçš„æ—¶é—´æ­¥ï¼Œè€Œæ˜¯éšæœºçš„ã€‚å¦‚æžœæƒ³ç”Ÿæˆæ•°æ®çš„æ—¶é—´é—´éš”å¯æŽ§ï¼Œé‚£ä¹ˆå¯ä»¥åœ¨å‰å‘æ¨¡åž‹çš„å»ºæ¨¡ä¸­æ˜¾å¼åœ°å°†æ—¶é—´æ­¥ä½œä¸ºå˜é‡å³å¯ã€‚è¿™ç¯‡è®ºæ–‡çš„ä½œè€…è®¤ä¸ºï¼Œä¸€ä¸ªåºåˆ—ç”Ÿæˆæ¨¡åž‹éœ€è¦å…·å¤‡ä»¥ä¸‹ä¸‰ç‚¹å±žæ€§ï¼šè¿™ä¸ªæ¨¡åž‹åº”è¯¥å­¦ä¹ ä¸€ä¸ªæ•°æ®çš„æŠ½è±¡çŠ¶æ€è¡¨ç¤ºå¹¶ä¸”åœ¨çŠ¶æ€ç©ºé—´ä¸­è¿›è¡Œé¢„æµ‹ï¼Œè€Œä¸æ˜¯åœ¨è§‚å¯Ÿç©ºé—´è¿›è¡Œé¢„æµ‹ã€‚è¿™ä¸ªæ¨¡åž‹åº”è¯¥å­¦ä¹ ä¸€ä¸ªç½®ä¿¡çŠ¶æ€ï¼Œè¿™ä¸ªçŠ¶æ€éœ€è¦åŒ…å«ç›®å‰ä¸ºæ­¢æ™ºèƒ½ä½“å¯¹äºŽå‘¨å›´çŽ¯å¢ƒçš„æ‰€æœ‰æ„ŸçŸ¥ä¿¡æ¯ã€‚ç½®ä¿¡çŠ¶æ€ç›¸å½“äºŽçŠ¶æ€è¡¨ç¤ºçš„éšå˜é‡ã€‚è¿™ä¸ªæ¨¡åž‹åº”è¯¥è¡¨çŽ°å‡ºæ—¶åºæŠ½è±¡ï¼Œæ—¢èƒ½å¤Ÿç›´æŽ¥é¢„æµ‹å¤šä¸ªæ—¶é—´æ­¥ä¹‹åŽçš„çŠ¶æ€ï¼Œä¹Ÿèƒ½å¤Ÿåªé€šè¿‡ä¸¤ä¸ªç‹¬ç«‹çš„æ—¶é—´ç‚¹è¿›è¡Œè®­ç»ƒè€Œä¸éœ€è¦ä¸­é—´æ‰€æœ‰æ—¶é—´ç‚¹çš„ä¿¡æ¯ã€‚ä¼˜åŒ–ç›®æ ‡TD-VAEçš„ç›®æ ‡ä¾¿æ˜¯ä¼˜åŒ–ä»¥ä¸‹å¯¹æ•°æ¡ä»¶ä¼¼ç„¶ï¼š$$\log p(x_t|x_{&lt;t})$$è¿™é‡Œå‡è®¾$x_t$å¯ä»¥é€šè¿‡è¯¥æ—¶é—´æ­¥ä»¥åŠä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€è¡¨ç¤º$z_t$å’Œ$z_{t-1}$æŽ¨æ–­å¾—å‡ºï¼Œç±»ä¼¼äºŽVAEä¸­æŸå¤±å‡½æ•°çš„æŽ¨å¯¼è¿‡ç¨‹ï¼Œè¿™é‡ŒåŒæ ·å¼•å…¥ELBOï¼Œå…·ä½“æŽ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹å›¾ï¼šæœ€åŽçš„æŸå¤±å‡½æ•°åŒ…å«ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼šç„¶åŽæˆ‘ä»¬æŠŠä¸¤ä¸ªè¿žç»­æ—¶é—´æ­¥çš„çŠ¶æ€è¡¨ç¤ºæ¢ä¸ºä¸¤ä¸ªä»»æ„æ—¶åˆ»çš„çŠ¶æ€è¡¨ç¤ºï¼šè¿™å®žè´¨ä¸Šæ˜¯å¦‚ä¸‹VAEçš„æŸå¤±å‡½æ•°ï¼šå…¶ä¸­$t2&gt;t1$ã€‚æ•´ä¸ªæŸå¤±å‡½æ•°å¯ä»¥ç›´è§‚åœ°è§£é‡Šä¸ºä»¥ä¸‹å››ä¸ªéƒ¨åˆ†ç»„æˆï¼šè®­ç»ƒæ—¶çš„è®¡ç®—å›¾å¦‚ä¸‹æ‰€ç¤ºï¼šæœ€åŽåœ¨ä¸‰ä¸ªä¸åŒä»»åŠ¡ä¸Šçš„å®žéªŒç»“æžœï¼š]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Survey of Sim2Real: Part I]]></title>
      <url>%2F2019%2F03%2F20%2FSurvey-of-Sim2Real-Part-I%2F</url>
      <content type="text"><![CDATA[æœ€è¿‘surveyäº†ä¸€ä¸‹sim2realé¢†åŸŸæœ€è¿‘çš„ç›¸å…³å·¥ä½œï¼Œå…ˆæ•´ç†ä¸ªç¬¬ä¸€ç‰ˆï¼ˆå…±æœ‰ä¸ƒç¯‡è®ºæ–‡ï¼‰çš„æ€»ç»“ã€‚æ•´ç¯‡æ€»ç»“åˆ†ä¸ºä»¥ä¸‹å››ä¸ªéƒ¨åˆ†ï¼šé—®é¢˜çš„å®šä¹‰ä»¥åŠå·¥ä½œçš„å‡ºå‘ç‚¹æ–¹æ³•çš„åˆ†ç±»å…·ä½“ç®—æ³•ä¸€ä¸ªå®žä¾‹é—®é¢˜çš„å®šä¹‰ä»¥åŠå·¥ä½œçš„å‡ºå‘ç‚¹sim2realçš„å…¨ç§°æ˜¯simulation to realityï¼Œæ˜¯å¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼ŒåŒæ—¶ä¹Ÿå±žäºŽtransfer learningçš„ä¸€ç§ã€‚ä¸»è¦è§£å†³çš„é—®é¢˜æ˜¯æœºå™¨äººé¢†åŸŸä¸­ï¼Œç›´æŽ¥è®©æœºå™¨äººæˆ–è€…æœºæ¢°è‡‚åœ¨çŽ°å®žçŽ¯å¢ƒä¸­ä¸ŽçŽ¯å¢ƒè¿›è¡Œäº¤äº’ã€é‡‡æ ·æ—¶ï¼Œä¼šå‡ºçŽ°ä»¥ä¸‹ä¸¤ä¸ªæ¯”è¾ƒä¸¥é‡çš„é—®é¢˜ï¼šé‡‡æ ·æ•ˆçŽ‡å¤ªä½Žï¼ˆåœ¨ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•è§£å†³æœºå™¨äººç›¸å…³é—®é¢˜æ—¶ï¼Œæ‰€éœ€è¦çš„æ ·æœ¬é‡ä¸€èˆ¬ä¼šè¾¾åˆ°ä¸Šåƒä¸‡ï¼Œåœ¨çŽ°å®žçŽ¯å¢ƒä¸­é‡‡é›†å¦‚æ­¤æ•°é‡çº§çš„æ ·æœ¬è¦è€—è´¹å‡ ä¸ªæœˆçš„æ—¶é—´ï¼‰å®‰å…¨é—®é¢˜ ï¼ˆç”±äºŽå¼ºåŒ–å­¦ä¹ éœ€è¦é€šè¿‡æ™ºèƒ½ä½“åœ¨çŽ¯å¢ƒä¸­è¿›è¡Œå¤§èŒƒå›´çš„éšæœºé‡‡æ ·æ¥è¿›è¡Œè¯•é”™ï¼Œå› è€Œåœ¨æŸäº›æ—¶åˆ»å…¶åšå‡ºçš„è¡Œä¸ºå¯èƒ½ä¼šæŸä¼¤æœºå™¨äººè‡ªèº«ï¼Œä¾‹å¦‚æ‰‹è‡‚è½¬åŠ¨è§’åº¦è¿‡å¤§æˆ–è€…é¿éšœä»»åŠ¡ä¸­ç”±äºŽç¢°æ’žé€ æˆçš„ä¸å¯é€†æŸä¼¤ç­‰ç­‰ï¼›ä¹Ÿå¯èƒ½ä¼šæŸå®³å‘¨å›´çš„çŽ¯å¢ƒç”šè‡³ç”Ÿç‰©ï¼‰ä½†æ˜¯å¦‚æžœæˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå™¨ä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„è®­ç»ƒï¼Œä»¥ä¸Šä¸¤ä¸ªé—®é¢˜å‡å¯è¿Žåˆƒè€Œè§£ã€‚ä½†æ˜¯ï¼Œè¿™é‡ŒåŒæ ·ä¼šå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œç”±äºŽæ¨¡æ‹Ÿå™¨å¯¹äºŽç‰©ç†çŽ¯å¢ƒçš„å»ºæ¨¡éƒ½æ˜¯å­˜åœ¨è¯¯å·®çš„ï¼Œå› è€Œåœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­å­¦ä¹ åˆ°çš„æœ€ä¼˜ç­–ç•¥æ˜¯å¦å¯ä»¥ç›´æŽ¥åœ¨çŽ°å®žçŽ¯å¢ƒä¸­åº”ç”¨å‘¢ï¼Ÿç­”æ¡ˆå¾€å¾€æ˜¯å¦å®šçš„ï¼Œæˆ‘ä»¬æŠŠè¿™ä¸ªé—®é¢˜ç§°ä¸º â€œreality gapâ€ã€‚è€Œsim2realçš„å·¥ä½œå°±æ˜¯åŽ»å°è¯•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™é‡Œå€¼å¾—æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œè™½ç„¶è¿™ä¸ªæ–¹å‘å«åšsim2realï¼Œå…¶å®žå…¶ä¸­çš„æ‰€æœ‰çš„ç®—æ³•éƒ½å¯ä»¥ç›´æŽ¥åº”ç”¨åœ¨sim2simï¼Œreal2realç­‰çš„ä»»åŠ¡ä¸­ã€‚æ–¹æ³•çš„åˆ†ç±»sim2realä¸­çš„å…¸åž‹å·¥ä½œå¤§è‡´å¯ä»¥åˆ†ä¸ºä»¥ä¸‹äº”ç±»ï¼šDomain Adaption ä¸»è¦æ˜¯é€šè¿‡å­¦ä¹ ä¸€ä¸ªæ¨¡æ‹ŸçŽ¯å¢ƒä»¥åŠçŽ°å®žçŽ¯å¢ƒå…±åŒçš„çŠ¶æ€åˆ°éšå˜é‡ç©ºé—´çš„æ˜ å°„ï¼Œåœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­ï¼Œä½¿ç”¨æ˜ å°„åŽçš„çŠ¶æ€ç©ºé—´è¿›è¡Œç®—æ³•çš„è®­ç»ƒï¼›å› è€Œåœ¨è¿ç§»åˆ°çŽ°å®žçŽ¯å¢ƒä¸­æ—¶ï¼ŒåŒæ ·å°†çŠ¶æ€æ˜ å°„åˆ°éšå«ç©ºé—´åŽï¼Œå°±å¯ä»¥ç›´æŽ¥åº”ç”¨åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒè®­ç»ƒå¥½çš„æ¨¡åž‹äº†ã€‚Progressive Network åˆ©ç”¨ä¸€ç±»ç‰¹æ®Šçš„Progressive Neural Networkæ¥è¿›è¡Œsim2realã€‚å…¶ä¸»è¦æ€æƒ³ç±»ä¼¼äºŽcumulative learningï¼Œä»Žç®€å•ä»»åŠ¡é€æ­¥è¿‡æ¸¡åˆ°å¤æ‚ä»»åŠ¡ï¼ˆè¿™é‡Œå¯ä»¥è®¤ä¸ºæ¨¡æ‹Ÿå™¨ä¸­çš„ä»»åŠ¡æ€»æ˜¯è¦æ¯”çŽ°å®žä»»åŠ¡ç®€å•çš„ï¼‰ã€‚Inverse Dynamic Model é€šè¿‡åœ¨çŽ°å®žçŽ¯å¢ƒä¸­å­¦ä¹ ä¸€ä¸ªé€†è½¬ç§»æ¦‚çŽ‡çŸ©é˜µæ¥ç›´æŽ¥åœ¨çŽ°å®žçŽ¯å¢ƒä¸­åº”ç”¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­è®­ç»ƒå¥½çš„æ¨¡åž‹ã€‚Domain Randomization å¯¹æ¨¡æ‹ŸçŽ¯å¢ƒä¸­çš„è§†è§‰ä¿¡æ¯æˆ–è€…ç‰©ç†å‚æ•°è¿›è¡ŒéšæœºåŒ–ï¼Œä¾‹å¦‚å¯¹äºŽé¿éšœä»»åŠ¡ï¼Œæ™ºèƒ½ä½“åœ¨ä¸€ä¸ªå¢™å£é¢œè‰²ã€åœ°æ¿é¢œè‰²ç­‰ç­‰æˆ–è€…æ‘©æ“¦åŠ›ã€å¤§æ°”åŽ‹å¼ºä¼šéšæœºå˜åŒ–çš„æ¨¡æ‹ŸçŽ¯å¢ƒä¸­è¿›è¡Œå­¦ä¹ ã€‚å…·ä½“ç®—æ³•è¿™ä¸€éƒ¨åˆ†å°†å¯¹ä»¥ä¸‹å…­ç¯‡è®ºæ–‡è¿›è¡Œè¯¦ç»†çš„è¯´æ˜Žï¼šTowards Adapting Deep Visuomotor Representations from Simulated to Real Environments[arXiv 2015] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao Peng, Sergey Levine, Kate Saenko, Trevor DarrellLearning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning [arXiv 2017] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, Sergey LevineSim-to-Real Robot Learning from Pixels with Progressive Nets [arXiv 2016] Andrei A. Rusu Deepmind.Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model[arXiv 2016] Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter Abbeel, and Wojciech ZarembaSim-to-Real Transfer of Robotic Control with Dynamics Randomization [ICRA 2018] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter AbbeelDomain Randomization for Transferring Deep Neural Networks from Simulation to the Real World [IROS 2017] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter AbbeelTowards Adapting Deep Visuomotor Representations from Simulated to Real Environmentsè¯¥è®ºæ–‡å±žäºŽ Domain Adaption ç±»åˆ«ã€‚å¦‚ä¸Šå›¾ï¼Œæœ¬æ–‡çš„åŸºæœ¬æ€æƒ³æ˜¯ï¼Œæ— è®ºæ˜¯åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒè¿˜æ˜¯åœ¨çŽ°å®žçŽ¯å¢ƒæ™ºèƒ½ä½“æ”¶é›†çš„å›¾åƒä¸­ï¼Œå¯¹äºŽä»»åŠ¡æ¯”è¾ƒé‡è¦çš„ä¾¿æ˜¯ä¸€äº›å¯æŽ§åˆ¶ç‰©ä½“æˆ–è€…ç›®æ ‡çš„ä½ç½®ã€‚å› è€Œå¸Œæœ›å­¦åˆ°çš„éšå«è¡¨ç¤ºèƒ½å¤Ÿä¿ç•™è¿™éƒ¨åˆ†ç‰©ä½“çš„ä½ç½®ä¿¡æ¯ã€‚ä»¥ä¸Šæ˜¯é’ˆå¯¹å›¾åƒå±€éƒ¨ä¿¡æ¯çš„çº¦æŸã€‚è€Œå¯¹äºŽæ•´ä½“å›¾åƒæ¥è¯´ï¼Œæœ¬æ–‡å¸Œæœ›æ¨¡æ‹ŸçŽ¯å¢ƒä»¥åŠçŽ°å®žçŽ¯å¢ƒåœ¨è¿™ä¸ªå…¬å…±çš„éšå«è¡¨ç¤ºç©ºé—´ä¸­çš„éšå«è¡¨ç¤ºæ— æ³•è¢«ä¸€ä¸ªäºŒåˆ†ç±»å™¨æ‰€åˆ†è¾¨å‡ºæ¥ã€‚å¦å¤–ï¼Œå¯¹äºŽä¸€å¯¹å›¾ç‰‡ï¼Œä¾‹å¦‚ä¸Šå›¾ï¼Œæœ¬æ–‡å¸Œæœ›è¿™ä¸€å¯¹å›¾ç‰‡çš„éšå«è¡¨ç¤ºçš„æ¬§æ°è·ç¦»èƒ½å¤Ÿå°½å¯èƒ½æŽ¥è¿‘ã€‚æ ¹æ®ä»¥ä¸Šä¸‰ä¸ªçº¦æŸï¼Œå¯ä»¥å¾—åˆ°ä»¥ä¸‹ä¸‰ä¸ªæŸå¤±å‡½æ•°ï¼šPose Estimation Loss:Domain Confusion Loss:å…¶ä¸­Contrastive Loss:å…¶ä¸­ï¼šè€Œæ±‚è§£æ•´ä¸ªé—®é¢˜çš„æœ€ç»ˆä¼˜åŒ–ç›®æ ‡å³ä»¥ä¸Šä¸‰ä¸ªæŸå¤±å‡½æ•°çš„åŠ æƒæ±‚å’Œï¼šç»™å‡ºä¸€ä¸ªæ›´åŠ å®¹æ˜“ç†è§£çš„æ¡†æž¶å›¾ï¼šä½†æ˜¯è¿™ç§æ–¹æ³•å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œåœ¨è®¡ç®—contrastive lossæ—¶éœ€è¦ä½¿ç”¨ä¸€å¯¹åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä»¥åŠçŽ°å®žçŽ¯å¢ƒä¸­èƒ½å¯¹åº”ä¸Šçš„å›¾ç‰‡ã€‚è¿™ç§å¯¹åº”å…³ç³»å¦‚æžœéœ€è¦äººå·¥å®Œæˆå·¥ä½œé‡å¾ˆå¤§è€Œä¸”å¦‚ä½•åŽ»åˆ†è¾¨ä¸¤å¼ å›¾æ˜¯å¦æ˜¯å¯¹åº”å…³ç³»ä¹Ÿæ²¡æœ‰ä¸€ä¸ªç»å¯¹çš„æ ‡å‡†ã€‚å› è€Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— ç›‘ç£æ–¹æ³•æ¥è‡ªåŠ¨ä»Žæ•°æ®é›†ä¸­æ‰¾å‡ºè¿™ç§å¯¹åº”å…³ç³»ï¼Œå…·ä½“æ¥è¯´åˆ†ä¸ºä»¥ä¸‹äº”ä¸ªæ­¥éª¤ï¼šåªä½¿ç”¨è™šæ‹ŸçŽ¯å¢ƒä¸­æ”¶é›†çš„å›¾ç‰‡ï¼ˆè¿›è¡Œä½ç½®æ ‡è®°ï¼‰å¹¶åªæ˜¯ç”¨pose estimation lossè®­ç»ƒä¸€ä¸ªè¡¨ç¤ºå­¦ä¹ ç½‘ç»œã€‚ä½¿ç”¨ä¸Šä¸€æ­¥è®­ç»ƒå¥½çš„è¡¨ç¤ºå­¦ä¹ ç½‘ç»œæŠ½å–æ•°æ®é›†ä¸­æ‰€æœ‰å›¾ç‰‡ï¼ˆåŒ…æ‹¬ä»¿çœŸçŽ¯å¢ƒä»¥åŠçœŸå®žçŽ¯å¢ƒï¼‰çš„ç¬¬ä¸€ä¸ªå·ç§¯ç‰¹å¾å›¾ã€‚å¯¹ä»¥ä¸Šç‰¹å¾å›¾é‡‡ç”¨5x5çš„æœ€å¤§æ± åŒ–ã€‚ä¸ºæ¯ä¸€ä¸ªä»¿çœŸ-çŽ°å®žå›¾ç‰‡å¯¹è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå³è®¡ç®—å…¶æ‹‰ç›´åŽçš„ç‰¹å¾å›¾çš„å†…ç§¯ã€‚æ¯ä¸€å¼ çœŸå®žçŽ¯å¢ƒä¸­çš„å›¾ç‰‡å¯¹åº”çš„è™šæ‹ŸçŽ¯å¢ƒçš„å›¾ç‰‡ä¸ºç›¸ä¼¼åº¦æœ€é«˜çš„é‚£ä¸€å¼ ã€‚Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learningè¿™ç¯‡è®ºæ–‡åŒæ ·å±žäºŽ Domain Adaption é¢†åŸŸï¼Œå³å­¦ä¹ ä¸€ä¸ªè™šæ‹ŸçŽ¯å¢ƒä»¥åŠçœŸå®žçŽ¯å¢ƒçš„çŠ¶æ€ï¼ˆstateï¼‰çš„å…¬å…±çš„éšå«è¡¨ç¤ºç©ºé—´ã€‚å…¶æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹åˆ†ä¸ºä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥è¿›è¡Œè¡¨ç¤ºå­¦ä¹ ï¼Œç¬¬äºŒæ­¥é‡‡ç”¨å­¦ä¹ åˆ°çš„è¡¨ç¤ºåœ¨çŽ°å®žçŽ¯å¢ƒä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚é¦–å…ˆæœ¬æ–‡å¯¹éœ€è¦è§£å†³çš„é—®é¢˜æœ‰å¦‚ä¸‹å‡è®¾ï¼šAssume that the reward functions share some structural similarity, in that the state distribution of an optimal policy in the source domain will resemble the state distribution of an optimal policy in the target domain when projected into some common feature space.å³å½“ä»¿çœŸçŽ¯å¢ƒä»¥åŠçœŸå®žçŽ¯å¢ƒçš„çŠ¶æ€åŒæ—¶æ˜ å°„åˆ°ä¸€ä¸ªå…¬å…±çš„éšå«è¡¨ç¤ºç©ºé—´ä¸­ï¼Œè¿™ä¸¤ä¸ªçŽ¯å¢ƒæ‰€éœ€è¦è§£å†³çš„é—®é¢˜çš„å›žæŠ¥å‡½æ•°å…·æœ‰ä¸€å®šçš„ç›¸ä¼¼æ€§ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬åœ¨ä»¿çœŸçŽ¯å¢ƒä¸­æž„å»ºä¸€ä¸ªæ‹¥æœ‰ä¸¤ä¸ªå…³èŠ‚çš„æœºæ¢°è‡‚å¸Œæœ›å®ƒèƒ½å¤Ÿå°†ä¸€ä¸ªå†°çƒæŽ¨åˆ°æŒ‡å®šä½ç½®ï¼Œå›žæŠ¥å‡½æ•°è®¾è®¡ä¸ºå†°çƒä¸Žç›®æ ‡ä½ç½®çš„è·ç¦»çš„è´Ÿå€¼ï¼›ç„¶åŽåœ¨çŽ°å®žçŽ¯å¢ƒä¸­ï¼Œæˆ‘ä»¬æ‹¥æœ‰ä¸€ä¸ªæœ‰ä¸‰ä¸ªå…³èŠ‚çš„æœºæ¢°è‡‚åŽ»å®ŒæˆåŒæ ·çš„ä»»åŠ¡ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè™½ç„¶ä»Žæ™ºèƒ½ä½“èŽ·å¾—çš„å›¾åƒè¡¨ç¤ºå®Œå…¨ä¸åŒï¼ˆä¸€ä¸ªä¸¤å…³èŠ‚ä¸€ä¸ªä¸‰å…³èŠ‚ï¼‰ï¼Œä½†æ˜¯å›žæŠ¥å‡½æ•°å…¶å®žæ˜¯ä¸€æ ·çš„ï¼Œä¸Žå…³èŠ‚æ•°ç›®æ²¡æœ‰å…³ç³»ã€‚å½“ç„¶è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒæžç«¯çš„ä¾‹å­ï¼Œå›žæŠ¥å‡½æ•°å¯ä»¥ä¸å®Œå…¨ä¸€æ ·ã€‚æ‰€ä»¥æœ¬æ–‡çš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸¤ä¸ªæ˜ å°„å‡½æ•°ï¼Œèƒ½å¤Ÿå°†ä¸¤ä¸ªçŽ¯å¢ƒä¸­çš„çŠ¶æ€æ˜ å°„åˆ°ä¸€ä¸ªå…±åŒçš„éšå«è¡¨ç¤ºç©ºé—´ï¼Œè¿™ä¸Žä¸Šä¸€ç¯‡è®ºæ–‡åªæœ‰ä¸€ä¸ªå…¬å…±çš„æ˜ å°„å‡½æ•°ä¸åŒï¼šè€Œè¦èƒ½é€šè¿‡è¿™ä¸ªç›®æ ‡æ¥æ±‚å‡ºä¸¤ä¸ªæ˜ å°„å‡½æ•°ï¼Œè¿˜éœ€è¦åšå‡ºä»¥ä¸‹å‡è®¾:ä»¿çœŸçŽ¯å¢ƒä»¥åŠçœŸå®žçŽ¯å¢ƒçš„æ™ºèƒ½ä½“éœ€è¦å­¦ä¼šå®ŒæˆåŒä¸€ä¸ªä»»åŠ¡åŠ¨ä½œç©ºé—´ä¸€è‡´ï¼ŒçŠ¶æ€ç©ºé—´çš„ç»´åº¦ä¸€è‡´ç¬¬ä¸€ä¸ªå‡è®¾å¿…é¡»å­˜åœ¨æ˜¯ç”±äºŽéœ€è¦ä»Žè¿™ä¸ªå…±åŒçš„ä»»åŠ¡ä¸­åŽ»å­¦ä¹ è¿™ä¸¤ä¸ªæ˜ å°„å‡½æ•°ã€‚è¿™ä¸¤ä¸ªå‡è®¾å…¶å®žä¸ç®—å¾ˆå¼ºçƒˆçš„å‡è®¾ï¼Œå¯¹äºŽç¬¬ä¸€ä¸ªå‡è®¾æ¥è¯´ï¼Œè¿™ä¸ªå…±åŒä»»åŠ¡å¯ä»¥æ˜¯ä¸€äº›æ¯”è¾ƒç®€å•çš„ä»»åŠ¡ï¼Œä½¿å¾—è®­ç»ƒæˆæœ¬è¾ƒå°ï¼›å¦å¤–å¯¹äºŽç¬¬äºŒä¸ªå‡è®¾ï¼Œå¦‚æžœä»¿çœŸçŽ¯å¢ƒä»¥åŠçŽ°å®žçŽ¯å¢ƒä¸­ä½¿ç”¨çš„æ˜¯åŒä¸€æ¬¾æœºå™¨äººæˆ–è€…æœºæ¢°è‡‚ï¼ŒåŠ¨ä½œç©ºé—´ä¸€è‡´ä»¥åŠçŠ¶æ€ç©ºé—´çš„ç»´åº¦ä¸€è‡´æ˜¯ä¸€ä¸ªéžå¸¸è‡ªç„¶çš„å‡è®¾ã€‚è¡¨ç¤ºå­¦ä¹ è¦è¿›è¡Œå¦‚ä¸Šå…¬å¼æ‰€ç¤ºçš„è¡¨ç¤ºå­¦ä¹ ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å¯¹ä¸¤ä¸ªçŽ¯å¢ƒä¸­çš„çŠ¶æ€ï¼ˆå­¦ä¼šçš„å…±åŒä»»åŠ¡ä¸­çš„çŠ¶æ€ï¼‰è¿›è¡Œå¯¹é½ï¼ˆä¸Žä¸Šä¸€ç¯‡è®ºæ–‡é‡Œçš„å¯¹é½æ„ä¹‰æ˜¯ä¸€æ ·çš„ï¼‰ï¼Œè¿™é‡Œå­˜åœ¨ä¸¤ç§æ–¹æ³•è¿›è¡Œå¯¹é½ï¼šTime-bases AlignmentDynamic Time Wrappingç¬¬ä¸€ç§æ–¹æ³•éžå¸¸ç®€å•ï¼Œå¯¹äºŽä¸¤ä¸ªçŽ¯å¢ƒä¸­çš„æ™ºèƒ½ä½“éƒ½å­¦ä¼šè§£å†³çš„å…±åŒä»»åŠ¡ï¼Œå¦‚æžœæ™ºèƒ½ä½“åœ¨ä»¿çœŸçŽ¯å¢ƒä»¥åŠçŽ°å®žçŽ¯å¢ƒä¸­åŠ¨ä½œæ‰§è¡Œçš„æ—¶é’Ÿæ˜¯å¤§è‡´ç›¸åŒçš„ï¼Œé‚£ä¹ˆåªè¦è®©ä¸¤ä¸ªçŽ¯å¢ƒä¸­çš„æ™ºèƒ½ä½“åŒæ—¶å¼€å§‹æ‰§è¡Œè¿™ä¸ªå…±åŒçš„ä»»åŠ¡ï¼Œå…¶åˆ†åˆ«äº§ç”Ÿçš„çŠ¶æ€åºåˆ—ä¸€å®šæ˜¯å¯¹é½çš„ï¼›ä½†æ˜¯æ—¶é’Ÿç›¸ç­‰çš„å‡è®¾è¿‡äºŽå¼ºçƒˆäº†ï¼Œå› è€Œç¬¬äºŒç§æ–¹æ³•æ˜¯ä¸€ä¸ªå¯è¡Œæ€§æ›´é«˜çš„æ–¹æ³•ã€‚å®ƒæ˜¯ä¸€ä¸ªè¿­ä»£çš„æ–¹æ³•ï¼Œå®ƒéœ€è¦ä¸€ä¸ªè®¡ç®—ä¸¤ä¸ªåºåˆ—ç›¸ä¼¼åº¦çš„è·ç¦»å‡½æ•°ï¼Œæ ¹æ®è¿™ä¸ªè·ç¦»å‡½æ•°æ¥æ‰¾å‡ºä½¿å¾—ä¸¤ä¸ªåºåˆ—è·ç¦»æœ€è¿‘çš„å¯¹é½æ–¹å¼ï¼›å¯¹é½åŽï¼Œå†æ ¹æ®æ–°çš„å¯¹é½æ–¹å¼æ›´æ–°è·ç¦»å‡½æ•°ï¼Œå¦‚æ­¤ä¸æ–­è¿­ä»£ç›´è‡³æ”¶æ•›æˆ–è€…åˆ°è¾¾åœæ­¢æ¡ä»¶ã€‚è¿™ä¸ªæ–¹æ³•ä¸»è¦åœ¨äºŽå¦‚ä½•åŽ»é€‰æ‹©è¿™ä¸ªè·ç¦»å‡½æ•°ï¼Œæœ¬æ–‡çš„åšæ³•æ˜¯é¦–å…ˆç”¨time-bases alignmentæ–¹æ³•å¾—åˆ°ä¸€ä¸ªåˆå§‹çš„å¯¹é½æ–¹å¼ï¼Œå†ä½¿ç”¨ä¸‹é¢è¦è®²åˆ°çš„è¡¨ç¤ºå­¦ä¹ çš„æ–¹æ³•å­¦ä¹ ä¸¤ä¸ªæ˜ å°„å‡½æ•°ï¼Œå°†æ•´ä¸ªåºåˆ—æ¯ä¸€å¯¹å¯¹é½çŠ¶æ€æ˜ å°„åŽéšå«è¡¨ç¤ºå‘é‡çš„æ¬§æ°è·ç¦»çš„å’Œä½œä¸ºdynamic time warppingæ–¹æ³•ä¸­åºåˆ—ç›¸ä¼¼åº¦çš„è·ç¦»å‡½æ•°ã€‚ä»¥ä¸Šå°±æ˜¯çŠ¶æ€å¯¹é½æ­¥éª¤ï¼Œä¸‹é¢å°±è¦è¿›è¡Œæ­£å¼çš„è¡¨ç¤ºå­¦ä¹ äº†ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå¯¹äºŽä»¥ä¸Šå…¬å¼ï¼Œå…¶å®žæœ‰ä¸ªéžå¸¸ç®€å•çš„è§£ï¼Œå³è¿™ä¸¤ä¸ªæ˜ å°„å‡½æ•°çš„æ˜¯ä¸ªè¾“å‡ºæ°¸è¿œä¸º0çš„å¸¸æ•°å‡½æ•°ã€‚è¿™æ ·ä¸€ä¸ªè§£æ˜¾ç„¶ä¸æ˜¯æˆ‘ä»¬éœ€è¦çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥åŠ ä¸Šä¸€ä¸ªçº¦æŸï¼Œå³å­¦ä¹ åˆ°çš„éšå«è¡¨ç¤ºèƒ½å¤Ÿå°½å¯èƒ½å¤šçš„ä¿ç•™åŽŸè¡¨ç¤ºçš„ä¿¡æ¯ï¼Œå³å­¦ä¹ åˆ°çš„éšå«è¡¨ç¤ºæ˜¯ä¸€ä¸ªauto encoderçš„éšå‘é‡ã€‚æ ¹æ®ä»¥ä¸Šå‡è®¾ä»¥åŠæˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡ï¼Œå¯ä»¥å¾—åˆ°å¦‚ä¸‹è¡¨ç¤ºå­¦ä¹ æŸå¤±å‡½æ•°ï¼šåŒæ ·ç»™å‡ºä¸€ä¸ªæ›´å®¹æ˜“ç†è§£çš„æ¡†æž¶å›¾ï¼šçŸ¥è¯†è¿ç§»åœ¨è¿›è¡Œäº†ç¬¬ä¸€æ­¥çš„è¡¨ç¤ºå­¦ä¹ åŽï¼Œæˆ‘ä»¬éœ€è¦åˆ©ç”¨å­¦ä¹ åˆ°çš„è¡¨ç¤ºåœ¨çŽ°å®žçŽ¯å¢ƒä¸­è¿›è¡Œæ–°ä»»åŠ¡çš„è®­ç»ƒã€‚ä½†æ˜¯æ³¨æ„ï¼Œæˆ‘ä»¬å­¦ä¹ çš„è¡¨ç¤ºæ˜¯ç»è¿‡å¦‚ä¸‹ä¸¤ä¸ªçº¦æŸå­¦åˆ°çš„ï¼Œç¬¬ä¸€ä¸ªçº¦æŸå¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªauto encoderçš„é™ç»´ï¼›ç¬¬äºŒä¸ªçº¦æŸæ˜¯èƒ½å¤Ÿä¸Žæ¨¡æ‹ŸçŽ¯å¢ƒæœ€ä¼˜ç­–ç•¥äº§ç”Ÿçš„çŠ¶æ€æ¦‚çŽ‡åˆ†å¸ƒç›¸åŒçš„ä¸€ä¸ªéšå«çŠ¶æ€è¡¨ç¤ºç©ºé—´ã€‚å› è€Œæˆ‘ä»¬ä¸èƒ½å•å•åªåˆ©ç”¨å­¦ä¹ åˆ°éšå«è¡¨ç¤ºåŽ»åœ¨çŽ°å®žä¸–ç•Œä¸­è®­ç»ƒï¼Œè¿™æ ·åœ¨æ¨¡æ‹ŸçŽ¯å¢ƒä¸­è®­ç»ƒå¥½çš„ç­–ç•¥æ²¡æœ‰åŠžæ³•å¯¹çŽ°å®žä»»åŠ¡çš„è®­ç»ƒé€ æˆä»»ä½•å½±å“ï¼Œè¿™ä¸ªå½±å“å¿…é¡»é€šè¿‡å°†çŽ°å®žä»»åŠ¡çš„çŠ¶æ€åºåˆ—ä¸Žæ¨¡æ‹ŸçŽ¯å¢ƒä¸­æœ€ä¼˜ç­–ç•¥äº§ç”Ÿçš„çŠ¶æ€åºåˆ—å¯¹é½åŽæ‰èƒ½å¤Ÿå®žçŽ°ã€‚æœ¬æ–‡é€šè¿‡å¯¹çŽ°å®žçŽ¯å¢ƒä¸­æ™ºèƒ½ä½“éœ€è¦è§£å†³çš„ä»»åŠ¡çš„å›žæŠ¥å‡½æ•°çš„åŸºç¡€ä¸ŠåŠ ä¸Šå¦‚ä¸‹é™„åŠ é¡¹æ¥å®žçŽ°çŸ¥è¯†è¿ç§»ï¼šè¿™é‡Œçš„ä¸Šæ ‡$t$è¡¨æ˜Žï¼Œåœ¨è¿›è¡ŒçŽ°å®žçŽ¯å¢ƒä¸­æ™ºèƒ½ä½“çš„è®­ç»ƒæ—¶ï¼Œæ¨¡æ‹ŸçŽ¯å¢ƒå¿…é¡»åŒæ­¥è¿è¡Œã€‚Sim-to-Real Robot Learning from Pixels with Progressive Netsæœ¬æ–¹æ³•å±žäºŽ Progressive Network ç±»åˆ«æ–¹æ³•ï¼Œå…¶ä½¿ç”¨çš„Progressive Nerual Networkæ˜¯è¿ç§»å­¦ä¹ é¢†åŸŸæå‡ºçš„ä¸€ç§ç½‘ç»œç»“æž„ï¼Œå…¶å…·ä½“å½¢å¼å¦‚ä¸‹å›¾ï¼ˆå·¦ï¼‰æ‰€ç¤ºï¼šå·¦å›¾ä¸­æ¯ä¸€åˆ—ï¼ˆcolumnï¼‰ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„ä»»åŠ¡ï¼Œä»»åŠ¡è®­ç»ƒé¡ºåºä»Žå·¦åˆ°å³ã€‚è™½ç„¶ä»»åŠ¡è®­ç»ƒé¡ºåºä»Žç®€å•åˆ°å¤æ‚ä»Žç›´è§‰ä¸Šæ¥çœ‹æ˜¯æ¯”è¾ƒåˆç†çš„ï¼Œä½†æ˜¯PNNå¹¶ä¸ä¸€å®šè¦æ»¡è¶³è¿™ä¸ªè§„å¾‹ï¼Œ å…¶ä»»åŠ¡è®­ç»ƒé¡ºåºå¯ä»¥æ˜¯ä»»æ„çš„ã€‚ç”±äºŽæˆ‘ä»¬ä»¥ç¬¬ä¸‰åˆ—ï¼ˆç¬¬ä¸‰ä¸ªä»»åŠ¡ï¼‰ä¸ºä¸­å¿ƒæ¥è€ƒè™‘ï¼Œå› è€Œæœ‰å®žçº¿ä¸Žè™šçº¿çš„å·®å¼‚ã€‚å¯ä»¥çœ‹åˆ°PNNçš„æ€æƒ³éžå¸¸ç®€å•ï¼Œåœ¨åŽé¢ä»»åŠ¡çš„æ¯ä¸€å±‚è®¡ç®—æ—¶ï¼Œè¾“å…¥ç«¯å¹¶ä¸Šä¹‹å‰ä»»åŠ¡å‰ä¸€å±‚çš„è¾“å‡ºå³å¯ã€‚ä½†æ˜¯PNNæ‰©å±•åˆ°å¼ºåŒ–å­¦ä¹ ä¸­è¿›è¡Œäº†å¦‚ä¸‹ä¸‰ä¸ªæ”¹å˜ï¼šçŽ°å®žçŽ¯å¢ƒä¸­ä½¿ç”¨çš„ç¥žç»ç½‘ç»œè¦æ¯”æ¨¡æ‹ŸçŽ¯å¢ƒä¸­è¦å°ã€‚åŽŸå› ä¸»è¦æ˜¯ç”±äºŽåŽŸPNNè®ºæ–‡å‘çŽ°ï¼Œå½“åˆ—æ•°è¶Šå¤šæ—¶æ¯ä¸€åˆ—ç½‘ç»œçš„å‚æ•°éƒ½å¾ˆç¨€ç–ï¼Œå®Œå…¨å¯ä»¥è¿›è¡Œç½‘ç»œåŽ‹ç¼©æˆ–è€…å‰ªæžã€‚è¾“å‡ºå±‚ä¸å†æŽ¥å—å‰ç½®ä»»åŠ¡çš„è¾“å…¥ã€‚ç”±äºŽæ¨¡æ‹ŸçŽ¯å¢ƒä¸ŽçŽ°å®žçŽ¯å¢ƒåœ¨åŠ¨ä½œç©ºé—´ä¸Šå¯èƒ½å­˜åœ¨å·®å¼‚ï¼Œå› è€Œåœ¨è¾“å‡ºå±‚å€Ÿé‰´å‰é¢ä»»åŠ¡çš„çŸ¥è¯†åè€Œå®¹æ˜“äº§ç”Ÿè¯¯å¯¼ã€‚ä¸ºäº†è®©æ™ºèƒ½ä½“åœ¨çŽ°å®žçŽ¯å¢ƒä¸­è®­ç»ƒæ‰€éœ€çš„æ ·æœ¬é‡æ›´å°ï¼Œå› è€Œè¾“å‡ºå±‚çš„å‚æ•°ç›´æŽ¥å¤åˆ¶ä¹‹å‰ä»»åŠ¡çš„å‚æ•°ç”¨æ¥åˆå§‹åŒ–ï¼Œç”¨ä»¥æå‡ç®—æ³•è®­ç»ƒåˆæœŸçš„æŽ¢ç´¢åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè®ºæ–‡çš„ç»“æžœè¿˜è¡¨æ˜Žä½¿ç”¨LSTMè¿›è¡Œç­–ç•¥ç½‘ç»œçš„å»ºæ¨¡è¦æ¯”ä½¿ç”¨MLPæ•ˆæžœæ›´å¥½ã€‚å…¶å®žè¿˜æœ‰å¾ˆå¤šå…¶ä»–å·¥ä½œä¹ŸåŒæ ·å‘çŽ°äº†è¿™ä¸€ç‚¹ï¼Œä¸»è¦è¿˜æ˜¯å› ä¸ºå¤§éƒ¨åˆ†çŽ°å®žä¸­çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜éƒ½æ˜¯éƒ¨åˆ†è§‚å¯Ÿçš„ï¼Œä¸æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨ã€‚Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Modelæœ¬æ–‡å±žäºŽ Inverse Dynamic Model ç±»åˆ«ã€‚å…¶ä¸»è¦åŸºäºŽçš„å‡è®¾æ˜¯å³ä½¿è™šæ‹ŸçŽ¯å¢ƒæ— æ³•å¯¹çŽ°å®žä¸–ç•Œè¿›è¡Œå®Œå…¨å‡†ç¡®çš„å»ºæ¨¡ï¼Œä½†æ˜¯å…¶çŠ¶æ€çš„å˜åŒ–è¿˜æ˜¯åˆç†çš„ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽä¸€ä¸ªå°†ç‰©ä½“æŽ¨åˆ°æŒ‡å®šç›®æ ‡ä½ç½®çš„ä»»åŠ¡æ¥è¯´ï¼Œä¸€ä¸ªæœºæ¢°è‡‚å°†å†°çƒå¾€å‰æŽ¨é‚£ä¹ˆä¸‹ä¸€ä¸ªçŠ¶æ€å°±æ˜¯å†°çƒå¾€æŽ¨åŠ¨æ–¹å‘å‰è¿›ä¸€äº›ï¼Œä½†æ˜¯ä¸ä¼šå¾€ç›¸åçš„æ–¹å‘ç§»åŠ¨ã€‚åŸºäºŽè¿™ä¸ªå‡è®¾ï¼Œé¦–å…ˆåœ¨è™šæ‹ŸçŽ¯å¢ƒè®­ç»ƒå¥½ä¸€ä¸ªç­–ç•¥ï¼Œå…¶è¾“å…¥æ˜¯å‰nä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€ï¼ˆè¿™é‡ŒåŒæ ·è€ƒè™‘åˆ°éƒ¨åˆ†è§‚å¯Ÿçš„é—®é¢˜ï¼‰ï¼Œå°†è¾“å‡ºçš„åŠ¨ä½œè¾“å…¥åˆ°è™šæ‹ŸçŽ¯å¢ƒæ¨¡åž‹ä¸­ï¼Œå°±ä¼šè½¬ç§»åˆ°è™šæ‹ŸçŽ¯å¢ƒä¸­çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚å°†è¿™ä¸ªçŠ¶æ€ä¸ŽçŽ°å®žçŽ¯å¢ƒä¸­çš„å‰nä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€è¾“å…¥åˆ°çœŸå®žçŽ¯å¢ƒä¸­å­¦ä¹ åˆ°çš„é€†åŠ¨æ€æ¨¡åž‹ä¸­ï¼Œå°±ä¼šå¾—å‡ºèƒ½å¤Ÿè¾“å‡ºè¿™ä¸‹ä¸€ä¸ªçŠ¶æ€æ‰€éœ€è¦é‡‡å–çš„åŠ¨ä½œã€‚å…·ä½“è§ä¸‹å›¾ï¼šä»¥ä¸Šè¿‡ç¨‹å”¯ä¸€éœ€è¦è¯¦ç»†è¯´æ˜Žçš„ä¾¿æ˜¯å¦‚ä½•åœ¨çŽ°å®žçŽ¯å¢ƒä¸­å­¦ä¹ ä¸€ä¸ªé€†åŠ¨æ€æ¨¡åž‹ï¼Œå…¶å®žéžå¸¸ç®€å•ï¼šä½†æ˜¯è¿™ä¸ªæ¨¡åž‹çš„å¥½åå–å†³äºŽåœ¨çŽ°å®žçŽ¯å¢ƒä¸­æ”¶é›†çš„æ ·æœ¬çš„è´¨é‡ï¼Œå³æ ·æœ¬æ˜¯å¦å…·æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§ä»Žè€Œè¦†ç›–è¶³å¤Ÿå¤§çš„çŠ¶æ€ç©ºé—´ã€‚ä¸€ä¸ªç®€å•ä½†æœ‰æ•ˆçš„åšæ³•æ˜¯åœ¨æŽ¢ç´¢æ—¶çš„åŠ¨ä½œä¸Šå¢žåŠ ä¸€å®šçš„å™ªå£°ï¼Œä½†æ˜¯åŠ å…¥å™ªå£°çš„é¢‘çŽ‡ç­‰éœ€è¦ä»”ç»†è€ƒé‡å¦åˆ™å°±ä¼šä½¿å¾—æ”¶é›†åˆ°çš„æ•°æ®è´¨é‡ä¸‹é™ï¼Œè®ºæ–‡ç»è¿‡å®žé™…çš„å®žéªŒå¾—å‡ºä»¥ä¸‹ä¸¤ç‚¹ç»éªŒï¼šä¸éœ€è¦æ¯ä¸ªæ—¶é—´æ­¥éƒ½åŠ å…¥å™ªå£°ã€‚å½“çŽ°å®žçŽ¯å¢ƒä¸­æ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œå‘ç”ŸçŠ¶æ€è½¬ç§»æ—¶è½¬ç§»åˆ°ä¸€ä¸ªä¸Žè™šæ‹ŸçŽ¯å¢ƒå·®åˆ«å¾ˆå¤§çš„çŠ¶æ€æ—¶ï¼Œå°±åº”å½“å³æ—¶åœæ­¢è¿™ä¸€è½®çš„é‡‡æ ·ã€‚Sim-to-Real Transfer of Robotic Control with Dynamics Randomizationæœ¬æ–‡å±žäºŽ Domain Randomization ç±»åˆ«ã€‚æœ¬æ–‡å‡ºå‘ç‚¹åœ¨äºŽæ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•å…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼šDeepRL policies are prone to exploiting idiosyncrasies of the simulator to realize behaviours that are infeasible in the real world.å³å¼ºåŒ–å­¦ä¹ ç®—æ³•åœ¨ä¸€ä¸ªç‰¹å®šçŽ¯å¢ƒä¸­è¿›è¡Œå­¦ä¹ æ—¶ï¼Œä¼šå°è¯•åŽ»æŒ–æŽ˜æŸäº›ä¸“å±žäºŽè¿™ä¸ªçŽ¯å¢ƒçš„ç‰¹æ€§ä»Žè€Œä½¿å¾—ç®—æ³•çš„æ³›åŒ–èƒ½åŠ›å¾ˆå·®ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å°†å¼ºåŒ–å­¦ä¹ çš„ä¼˜åŒ–ç›®æ ‡æ›´æ”¹å¦‚ä¸‹ï¼šè¿™é‡Œçš„$\mu$ä»£è¡¨å†³å®šçŽ¯å¢ƒçš„ç‰©ç†å‚æ•°ã€‚å¦‚æžœæ™ºèƒ½ä½“ä¼˜åŒ–çš„æ˜¯åœ¨å¤§é‡ä¸åŒç‰©ç†å‚æ•°ç¡®å®šçš„è™šæ‹ŸçŽ¯å¢ƒä¸­ç´¯ç§¯å›žæŠ¥çš„æœŸæœ›å€¼çš„è¯ï¼Œè®­ç»ƒå‡ºçš„ç­–ç•¥å°±ä¼šæ›´åŠ é²æ£’ã€‚å¯¹äºŽä¸€ä¸ªç‰¹å®šçš„çŽ¯å¢ƒï¼Œæœ¬æ–‡é‡‡ç”¨HER+RDPGç®—æ³•è¿›è¡Œæœ€ä¼˜ç­–ç•¥çš„è®­ç»ƒï¼šå…¶ä¸­çš„ç­–ç•¥ç½‘ç»œä»¥åŠå€¼å‡½æ•°ç½‘ç»œé‡‡ç”¨å¦‚ä¸‹æ–¹å¼è¿›è¡Œå»ºæ¨¡ï¼šDomain Randomization for Transferring Deep Neural Networks from Simulation to the Real Worldæœ¬æ–‡åŒæ ·å±žäºŽ Domain Randomization ç±»åˆ«ï¼Œåªä¸è¿‡ä¸åŒäºŽä¸Šä¸€ç¯‡è®ºæ–‡æ˜¯éšæœºåŒ–ç‰©ç†å‚æ•°ï¼Œæœ¬æ–‡æ˜¯éšæœºåŒ–çŽ¯å¢ƒçš„è§†è§‰è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæœ¬æ–‡æ˜¯æƒ³å­¦ä¹ ä¸€ä¸ªå®šä½å™¨ï¼Œé€šè¿‡è¾“å…¥ä¸€å¼ å›¾ç‰‡æ¥å®šä½å…¶ä¸­æ‰€æœ‰ç›®æ ‡ç‰©ä½“çš„ä¸‰ç»´åæ ‡ï¼šå…·ä½“éœ€è¦éšæœºåŒ–çš„è§†è§‰ä¿¡æ¯åŒ…æ‹¬ï¼šæ¡Œå­ä¸Šæ‰€æœ‰ç›®æ ‡ç‰©ä½“çš„ä½ç½®ä»¥åŠçº¹ç†æ¡Œå­ã€åœ°æ¿ã€èƒŒæ™¯ä»¥åŠæœºæ¢°è‡‚çš„çº¹ç†æ‘„åƒæœºçš„ä½ç½®ã€æœå‘ä»¥åŠå¯è§†èŒƒå›´åœºæ™¯ä¸­å…‰æºçš„æ•°é‡åœºæ™¯ä¸­å…‰æºçš„ä½ç½®ã€æœå‘ä»¥åŠå…‰è°±ç‰¹å¾åŠ å…¥åˆ°å›¾åƒä¸­å™ªå£°çš„ç±»åž‹ä»¥åŠæ•°é‡ä¸€ä¸ªå®žä¾‹åœ¨è¿™ä¸€éƒ¨åˆ†æˆ‘å°†ä»‹ç»ä¸€ä¸‹OpenAIåœ¨sim2realé¢†åŸŸåšå‡ºçš„ä¸€ä¸ªå·¥ä½œï¼Œå…¶åœ°ä½ç±»ä¼¼äºŽå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ é¢†åŸŸçš„OpenAI Fiveã€‚Learning Dexterous In-Hand Manipulationè¿™ä¸ªä¾‹å­ä¸»è¦ç”¨åˆ°çš„æŠ€æœ¯åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼šè™šæ‹ŸçŽ¯å¢ƒçš„éšæœºåŒ–ã€å¤§è§„æ¨¡åˆ†å¸ƒå¼é‡‡æ ·ä»¥åŠç²¾ç¡®çš„è™šæ‹ŸçŽ¯å¢ƒæ­å»ºã€‚å…¶æ‰€éœ€è¦å®Œæˆçš„ä»»åŠ¡æ˜¯ï¼šä½¿ç”¨ä¸€ä¸ªå…·æœ‰20ä¸ªè‡ªç”±åº¦çš„æœºæ¢°æ‰‹ï¼Œå°†å…¶æ‰‹æŽŒä¸­çš„ç«‹æ–¹ä½“ä»Žåˆå§‹æœå‘åˆ©ç”¨æ‰‹æŒ‡ç¿»è½¬åˆ°ç›®æ ‡æœå‘ï¼šä¸ºäº†å»ºç«‹ä¸€ä¸ªè¶³å¤Ÿç²¾ç»†ï¼ˆä½†æ˜¯ä¾æ—§å­˜åœ¨æ— æ³•å»ºæ¨¡çš„ç‰©ç†é‡ï¼‰çš„è™šæ‹ŸçŽ¯å¢ƒï¼ŒOpenAIä»¥æœºæ¢°æ‰‹ä¸ºçƒå¿ƒåŠå¾„ä¸º80åŽ˜ç±³çš„çƒé¢ä¸Šå‡åŒ€åˆ†å¸ƒäº†16ä¸ªç²¾åº¦ä¸º20å¾®ç±³çš„è¿½è¸ªå™¨ï¼Œèƒ½å¤Ÿå®šä½æœºæ¢°æ‰‹ä»»æ„ä½ç½®çš„å¾®å°ä½ç§»ã€‚ä¹‹æ‰€ä»¥é‡‡ç”¨å¦‚æ­¤é«˜ç²¾åº¦çš„è¿½è¸ªå™¨æ˜¯ä¸ºäº†å°½å¯èƒ½å‡†ç¡®åœ°å¯¹æœºæ¢°æ‰‹çš„ç›¸å…³ç‰©ç†å‚æ•°ï¼Œä¾‹å¦‚æ‰‹æŒ‡å…³èŠ‚å¤„çš„é˜»å°¼ç­‰ç­‰ï¼Œè¿™æ ·çš„ç‰©ç†å‚æ•°æœ‰å°†è¿‘500ä¸ªã€‚æˆ‘è®¤ä¸ºè¿™ä¸ªå·¥ä½œä¹‹æ‰€ä»¥èƒ½å¤Ÿç›´æŽ¥å°†è™šæ‹ŸçŽ¯å¢ƒä¸­å­¦ä¹ åˆ°çš„æœ€ä¼˜ç­–ç•¥ç›´æŽ¥åº”ç”¨åˆ°çŽ°å®žçŽ¯å¢ƒä¸­ï¼Œè¿™ä¸ªé«˜ç²¾åº¦çš„è™šæ‹ŸçŽ¯å¢ƒåŠŸä¸å¯æ²¡ï¼šæ•´ä¸ªç³»ç»Ÿçš„è®­ç»ƒæ­¥éª¤å¤§è‡´å¯åˆ†ä¸ºä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼ˆæœ€åŽæ˜¯è®­ç»ƒå®Œæ¯•çš„æ‰§è¡Œéƒ¨åˆ†ï¼‰ï¼šä¸‹é¢å°†è¯¦ç»†å¯¹æ¯ä¸€ä¸ªéƒ¨åˆ†è¿›è¡Œè¯´æ˜Žã€‚é¦–å…ˆæ˜¯ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼ŒåŒ…æ‹¬æ¨¡æ‹ŸçŽ¯å¢ƒä¸­æ•°æ®çš„å¹¶è¡Œé‡‡æ ·ä»¥åŠæ•´ä¸ªå¼ºåŒ–å­¦ä¹ çš„å‚æ•°æ›´æ–°æ¡†æž¶ï¼šä¸Šå›¾ä¸­é‡‡ç”¨çš„åˆ†å¸ƒå¼æ•°æ®æ”¶é›†ä»¥åŠæ¨¡åž‹è®­ç»ƒæ¡†æž¶åŒæ ·ä¹Ÿæ˜¯OpenAI Fiveæ‰€é‡‡ç”¨çš„ã€‚ä»Žå·¦ä¸‹æ–¹å¼€å§‹ï¼Œå¤šä¸ªå¹¶è¡Œé‡‡æ ·çš„workerä¼šå°†è‡ªå·±æ ¹æ®å½“å‰ç­–ç•¥é‡‡é›†çš„æ ·æœ¬å‘é€ç»™ä¸Žè‡ªå·±ç›¸å…³è”çš„RedisæœåŠ¡å™¨ä¸Šï¼Œæ¨¡åž‹æ›´æ–°æ¨¡å—ä¸­çš„Pullerå°†ä¼šå®šæœŸå¼‚æ­¥åœ°ä»ŽRedisæœåŠ¡å™¨ä¸­æ‹‰å–ä¸€ä¸ªbatchçš„æ•°æ®å¹¶æ”¾åˆ°RAMä¸­ï¼Œä¹‹åŽStagerä»ŽRAMä¸­æ‹‰å–ä¸€ä¸ªmini-batchæ”¾åˆ°GPUä¸Šï¼Œä¸Žå…¶ä»–é‡‡ç”¨MPIåè®®è”ç³»çš„GPUä¸€èµ·å¯¹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚æ›´æ–°åŽçš„å‚æ•°å°†æ¯ä¸ªOptimizeréƒ½ä¿å­˜ä¸€ä»½ã€‚ä¹‹åŽOptimizeræ²¿ç€ä¹‹å‰ç›¸åçš„è·¯å¾„å°†æ›´æ–°åŽçš„å‚æ•°å­˜å‚¨åˆ°RedisæœåŠ¡å™¨ä¸Šï¼Œworkerså°†å®šæœŸå¼‚æ­¥åœ°ä»ŽRedisæœåŠ¡å™¨ä¸Šæ‹‰å–æœ€æ–°çš„ç­–ç•¥å‚æ•°è¿›è¡Œé‡‡æ ·ã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å°±æ˜¯ä»¥ä¸Šè¿‡ç¨‹çš„è¿­ä»£ã€‚ä¸‹å›¾è¡¨ç¤ºäº†ä¸åŒè§„æ¨¡çš„å¹¶è¡Œå¯¹äºŽæœ€ç»ˆç®—æ³•æ€§èƒ½çš„å½±å“ï¼šç¬¬äºŒéƒ¨åˆ†ï¼Œå…·ä½“çš„å¼ºåŒ–ç®—æ³•é€‰ç”¨çš„æ˜¯PPOç®—æ³•ï¼Œå…¶ç­–ç•¥ç½‘ç»œä»¥åŠå€¼å‡½æ•°ç½‘ç»œå»ºæ¨¡å¦‚ä¸‹ï¼šå…¶ä¸­è¾“å…¥éƒ¨åˆ†å·¦è¾¹ä»£è¡¨æœºæ¢°æ‰‹çš„çŠ¶æ€ï¼Œå³è¾¹ä»£è¡¨ç‰©ä½“çš„æœå‘ï¼Œå…·ä½“çš„ç»´åº¦å¦‚ä¸‹æ‰€ç¤ºï¼šç”±äºŽPPOç®—æ³•çš„å€¼å‡½æ•°ç½‘ç»œåªä¼šåœ¨è®­ç»ƒæ—¶ä½¿ç”¨åˆ°ï¼Œå› è€Œé‡‡ç”¨å®Œæ•´ä¿¡æ¯å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚åœ¨è®­ç»ƒæ—¶ç®—æ³•é‡‡ç”¨äº†å¦‚ä¸‹ä¸‰ç§éšæœºæ³¨å…¥æ–¹æ³•ï¼šDynamic RandomizationDomain RandomizationUnmodeled Effects Randomizationå…·ä½“è§ä¸‹é¢ä¸‰å¼ å›¾ï¼šæœ€åŽä¸€éƒ¨åˆ†ï¼Œç”±äºŽæœºæ¢°æ‰‹çš„ä»»åŠ¡ä¸åº”è¯¥å±€é™äºŽè½¬åŠ¨æ–¹å—ï¼Œè¿˜åº”è¯¥åŒ…æ‹¬æ“çºµå…¶ä»–ç‰©ä½“ã€‚è€Œä¸”ç”±ç¬¬äºŒéƒ¨åˆ†å¯çŸ¥ç­–ç•¥ç½‘ç»œä»¥åŠå€¼å‡½æ•°ç½‘ç»œçš„è¾“å…¥å¯çŸ¥éœ€è¦ç«‹æ–¹ä½“çš„æœå‘ä»¥åŠä½ç½®ä¿¡æ¯ï¼Œç›®å‰æ˜¯é€šè¿‡16ä¸ªé«˜ç²¾åº¦è¿½è¸ªå™¨ç¡®å®šçš„ã€‚OpenAIä¸ºäº†æé«˜æ•´ä¸ªç³»ç»Ÿçš„é€šç”¨æ€§ï¼Œå› è€Œåœ¨æ–¹å—çš„å‘¨å›´ç›¸éš”ä¸€å®šé«˜åº¦120åº¦è§’å‡åŒ€æ”¾ç½®äº†ä¸‰ä¸ªæ‘„åƒå¤´ï¼Œå°è¯•å­¦ä¹ ä¸€ä¸ªæ¨¡åž‹ï¼Œè¾“å…¥æ˜¯ä¸‰å¼ ä¸åŒè§’åº¦çš„å›¾ç‰‡ï¼Œè¾“å‡ºæ˜¯ç«‹æ–¹ä½“çš„ä½ç½®ä»¥åŠæœå‘ã€‚è¿™æ ·å°±å¯ä»¥æŠŠç«‹æ–¹ä½“æ¢ä¸ºä»»æ„çš„ç‰©ä½“ã€‚å…¶å…·ä½“çš„ç½‘ç»œç»“æž„å¦‚ä¸‹ï¼š]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Variational Discreminator Bottlenect [ICLR 2019 Peng et al.]]]></title>
      <url>%2F2019%2F03%2F20%2FVariational-Discreminator-Bottlenect%2F</url>
      <content type="text"><![CDATA[ç®€ä»‹å¯¹æŠ—å­¦ä¹ æ–¹æ³•ä»Šå¹´æ¥è¢«å¹¿æ³›åº”ç”¨äºŽå„é¢†åŸŸä¸­ï¼Œä½†å…¶è®­ç»ƒè¿‡ç¨‹æžä¸ç¨³å®šã€‚ç”±äºŽåˆ¤åˆ«å™¨è¿‡äºŽå‡†ç¡®å°†ä¼šä½¿å¾—å…¶äº§ç”Ÿçš„æ¢¯åº¦åŒ…å«çš„ä¿¡æ¯è¿‡å°‘ä»Žè€Œä¸èƒ½æœ‰æ•ˆåœ°å¯¹ç”Ÿæˆå™¨è¿›è¡Œè®­ç»ƒï¼Œå› è€Œæœ‰æ•ˆåœ°å¹³è¡¡åˆ¤åˆ«å™¨ä»¥åŠç”Ÿæˆå™¨çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªç®€å•ä½†é€šç”¨çš„æ–¹å¼æ¥å¯¹åˆ¤åˆ«å™¨æŽ¥æ”¶åˆ°çš„ä¿¡æ¯æµé‡‡ç”¨ä¿¡æ¯ç“¶é¢ˆè¿›è¡Œçº¦æŸã€‚é€šè¿‡å¯¹åˆ¤åˆ«å™¨çš„å†…éƒ¨çŠ¶æ€ä»¥åŠè¾“å…¥çš„åŽŸå§‹æ•°æ®ä¹‹é—´çš„äº’ä¿¡æ¯æ–½åŠ çº¦æŸå¯ä»¥æœ‰æ•ˆåœ°æŽ§åˆ¶åˆ¤åˆ«å™¨çš„å‡†ç¡®åº¦ä»Žè€Œä½¿å¾—å…¶äº§ç”Ÿçš„æ¢¯åº¦èƒ½å¤ŸåŒ…å«å¯¹ç”Ÿæˆå™¨è®­ç»ƒæ›´åŠ å…·æœ‰æŒ‡å¯¼æ„ä¹‰çš„ä¿¡æ¯ã€‚ä½œè€…æå‡ºçš„å˜åˆ†åˆ¤åˆ«å™¨ç“¶é¢ˆèƒ½å¤Ÿæ˜¾è‘—æå‡æ¨¡ä»¿å­¦ä¹ ä»¥åŠé€†å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç‰¹æ€§ï¼Œå½“ç„¶ç”±äºŽå…¶é€šç”¨æ€§ï¼Œä»»ä½•å¯¹æŠ—ç”Ÿæˆæ¨¡åž‹å‡å¯ä»Žä¸­å—ç›Šã€‚å˜åˆ†ä¿¡æ¯ç“¶é¢ˆæˆ‘ä»¬ä»Žç›‘ç£å­¦ä¹ ä¸­çš„å˜åˆ†ä¿¡æ¯ç“¶é¢ˆå‡ºå‘ã€‚å¯¹äºŽç›‘ç£å­¦ä¹ ä¸­ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œå­˜åœ¨ä»¥ä¸‹ä¼˜åŒ–ç›®æ ‡ï¼š$$\min_q \mathbb{E}_{x,y\sim p(x,y)}\left[ -\log q(y|x) \right].$$ç„¶è€Œï¼Œä¼˜åŒ–ä¸Šè¿°ç›®æ ‡å®¹æ˜“ä½¿å¾—è®­ç»ƒå‡ºçš„æ¨¡åž‹è¿‡æ‹Ÿåˆã€‚å¼•å…¥ä¿¡æ¯ç“¶é¢ˆå¯ä»¥ä½¿å¾—æ¨¡åž‹åªå…³æ³¨äºŽè¾“å…¥æ•°æ®ä¸­æ›´åŠ å…·æœ‰åˆ¤åˆ«æ€§çš„ç‰¹å¾ã€‚é¦–å…ˆæˆ‘ä»¬ä¸€å¦‚ä¸€ä¸ªç¼–ç å™¨$\mathbb{E}(z|x)$å°†è¾“å…¥æ•°æ®$x$æ˜ å°„åˆ°ä¸€ä¸ªéšå«åˆ†å¸ƒä¸­ï¼Œç„¶åŽå¯¹äºŽç¼–ç åŽçš„æ•°æ®ä»¥åŠåŽŸæ•°æ®ä¹‹é—´çš„äº’ä¿¡æ¯$I(X,Z)$çš„ä¸Šç•Œæ–½åŠ çº¦æŸï¼Œå³å¯å¾—åˆ°ä¸‹é¢çš„ä¼˜åŒ–ç›®æ ‡ï¼š$$\begin{align}J(q,E)=&amp;\min_{q,E} \;\;\mathbb{E}_{x,y \sim p(x,y)} \left[ \mathbb{E}_{z \sim E(z|x)} \left[ -\log q(y|z) \right] \right] \nonumber \\&amp;\text{s.t.}\;\;\;\;I(X,Z) \leq I_c.\end{align}$$æˆ‘ä»¬å¯ä»¥é€šè¿‡å˜åˆ†æ–¹æ³•å¼•å…¥äº’ä¿¡æ¯çš„ä¸Šç•Œï¼Œä»Žè€ŒæŽ¨å¯¼å‡ºä¸Šè¿°ä¼˜åŒ–ç›®æ ‡çš„ä¸Šç•Œï¼Œæœ€åŽé€šè¿‡æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•å°†ä¸Šè¿°å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜è½¬å˜ä¸ºä¸€ä¸ªæ— çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ï¼Œå…·ä½“æŽ¨å¯¼è¿‡ç¨‹è§ä¸‹å›¾ï¼šå˜åˆ†åˆ¤åˆ«å™¨ç“¶é¢ˆæŽ¥ç€æˆ‘ä»¬å°†ä¸Šè¿°å˜åˆ†ä¿¡æ¯ç“¶é¢ˆå¼•å…¥åˆ°ä¸€ä¸ªæ ‡å‡†çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„åˆ¤åˆ«å™¨æŸå¤±å‡½æ•°ä¸Š:ç”±äºŽä¸€äº›ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ä»¥åŠå¯¹æŠ—é€†å¼ºåŒ–å­¦ä¹ ç®—æ³•å‡é‡‡ç”¨ä»¥ä¸Šçš„ç”Ÿæˆå¯¹æŠ—æ¡†æž¶ï¼Œå› è€Œå¯ä»¥å¼•å…¥ä»¥ä¸Šå˜åˆ†åˆ¤åˆ«å™¨ç“¶é¢ˆæ¥å¢žå¼ºæ€§èƒ½ã€‚è®¨è®ºä¸ºä½•å¼•å…¥å˜åˆ†åˆ¤åˆ«å™¨ç“¶é¢ˆå¯ä»¥æé«˜ç”Ÿæˆå¯¹æŠ—æ¨¡åž‹çš„æ€§èƒ½å‘¢ï¼Ÿåœ¨ç”Ÿæˆå¯¹æŠ—å­¦ä¹ ä¸­ï¼Œå¦‚æžœçœŸå®žæ•°æ®åˆ†å¸ƒä¸Žç”Ÿæˆæ•°æ®åˆ†å¸ƒå…·æœ‰ä¸ç›¸äº¤çš„æ”¯æ’‘é›†æ—¶ï¼Œä¸€ä¸ªæœ€ä¼˜çš„åˆ¤åˆ«å™¨èƒ½å¤Ÿå®Œç¾Žåˆ†è¾¨ä¸¤ä¸ªåˆ†å¸ƒå¹¶ä¸”å…¶æ¢¯åº¦å‡ ä¹Žå¤„å¤„ä¸ºé›¶ã€‚å› è€Œï¼Œå½“åˆ¤åˆ«å™¨æ”¶æ•›åˆ°æœ€ä¼˜æ€§èƒ½æ—¶ï¼Œç”¨ä»¥è®­ç»ƒç”Ÿæˆå™¨çš„æ¢¯åº¦ä¼šå› æ­¤æ¶ˆå¤±ã€‚ç›®å‰ä¸€ç§è§£å†³æ­¤é—®é¢˜çš„æ–¹æ³•æ˜¯å¯¹åˆ¤åˆ«å™¨çš„è¾“å…¥æ•°æ®å¢žåŠ ä¸€äº›è¿žç»­çš„å™ªå£°ï¼Œå› è€Œä½¿å¾—ä¸¤ä¸ªåˆ†å¸ƒåœ¨ä»»ä½•åœ°æ–¹éƒ½æ‹¥æœ‰è¿žç»­çš„æ”¯æ’‘é›†ã€‚ä½†æ˜¯å®žé™…ä¸Šï¼Œå¦‚æžœä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»å¾ˆå¤§æ—¶ï¼Œå¢žåŠ å™ªå£°å‡ ä¹Žæ²¡æœ‰å½±å“ã€‚è€Œå¼•å…¥å˜åˆ†åˆ¤åˆ«å™¨ç“¶é¢ˆæ—¶ï¼Œé¦–å…ˆç¼–ç å™¨å°†è¾“å…¥æ˜ å°„åˆ°ä¸€ä¸ªåµŒå…¥ç©ºé—´ä¸­å¹¶å¯¹åµŒå…¥è¡¨ç¤ºæ–½åŠ ä¿¡æ¯ç“¶é¢ˆçº¦æŸï¼Œä½¿å¾—ä¸¤ä¸ªåˆ†å¸ƒä¸ä»…å…·æœ‰å…±äº«çš„æ”¯æ’‘é›†è€Œä¸”åˆ†å¸ƒä¹‹é—´å­˜åœ¨æ˜Žæ˜¾çš„é‡åˆï¼ˆè·ç¦»ä¸å¤§ï¼‰ï¼ŒåŒæ—¶ç”±äºŽå¼•å…¥ä¿¡æ¯ç“¶é¢ˆä¸Žå¼•å…¥å™ªå£°éƒ¨åˆ†ç­‰åŒï¼Œä½¿å¾—ä¸Šè¿°é—®é¢˜å¾—ä»¥è§£å†³ã€‚]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Accelerate your pandas workflows by changing one line of code]]></title>
      <url>%2F2018%2F10%2F26%2FAccelerate-your-pandas-workflows-by-changing-one-line-of-code%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Curiosity-Driven Learning made easy Part I (Repost)]]></title>
      <url>%2F2018%2F10%2F17%2FCuriosity-Driven-Learning-made-easy-Part-I-Repost%2F</url>
      <content type="text"><![CDATA[Curiosity-Driven Learning made easy Part IThis article is part of Deep Reinforcement Learning Course with Tensorflow ðŸ•¹ï¸. Check the syllabus here.OpenAI Five contestIn the recent years, weâ€™ve seen a lot of innovations in Deep Reinforcement Learning. From DeepMind and the Deep Q learning architecture in 2014 to OpenAI playing Dota2 with OpenAI five in 2018, we live in an exciting and promising moment.And today weâ€™ll learn about Curiosity-Driven Learning, one of the most exciting and promising strategy in Deep Reinforcement Learning.Reinforcement Learning is based on the reward hypothesis, which is the idea that each goal can be described as the maximization of the rewards. However, the current problem of extrinsic rewards (aka rewards given by the environment) is that this function is hard coded by a human, which is not scalable.The idea of Curiosity-Driven learning, is to build a reward function that is intrinsic to the agent (generated by the agent itself). It means that the agent will be a self-learner since he will be the student but also the feedback master.Sounds crazy? Yes but thatâ€™s a genius idea that was introduced in the 2017 paper Curiosity-driven Exploration by Self-supervised Prediction. The results were then improved with the second paper Large-Scale Study of Curiosity-Driven Learning.They discovered that curiosity driven learning agents perform as good as if they had extrinsic rewards, and were able to generalize better with unexplored environments.In this first article weâ€™ll talk about the theory and explain how works Curiosity Driven Learning in theory.Then, in a second article, weâ€™ll implement a Curiosity driven PPO agent playing Super Mario Bros.Sounds fun? Letâ€™s dive on in !Two main problems in Reinforcement LearningFirst, the problem of sparse rewards, which is the time difference between an action and its feedback (its reward). An agent learns fast if each of its action has a reward, so that he gets a rapid feedback.For instance, if you play Space Invaders, you shoot and kill an enemy, you get a reward. Consequently youâ€™ll understand that this action at that state was good.Thanks the reward our agent knows that this action at that state was goodHowever, in complex games such as real time strategy games, you will not have a direct reward for each of your actions. Therefore, a bad decision will not have a feedback until hours later.For example, if we take Age Of Empires II, we can see on the first image that agent decided to build one barrack and focus on collecting resources. Thus in the second picture (some hours after) the enemies destroyed our barrack consequently we have ton of resources but we canâ€™t create an army so weâ€™re dead.Enemies destroyed our barrackThe second big problem is that extrinsic rewards are not scalable. Since in each environment, a human implemented a reward function. But how we can scale that in big and complex environments?The solution is to develop a reward function that is intrinsic to the agent (generated by the agent itself). This reward function will be called curiosity.A new reward function: curiosityCuriosity is an intrinsic reward that is equal to the error of our agent to predict the consequence of its own actions given its current state (aka to predict the next state given current state and action taken).Why? Because the idea of curiosity is to encourage our agent to perform actions that reduce the uncertainty in the agentâ€™s ability to predict the consequence of its own action (uncertainty will be higher in areas where the agent has spent less time, or in areas with complex dynamics).Consequently measuring error requires building a model of environmental dynamics that predicts the next state given the current state and the action a.The question that we can ask here is how we can calculate this error?To calculate curiosity, we will use a module introduced in the first paper called Intrinsic Curiosity module.Introducing the Intrinsic Curiosity ModuleThe need of a good feature spaceBefore diving into the description of the module, we must ask ourselves how our agent can predict the next state given our current state and our action?We know that we can define the curiosity as the error between the predicted new state (st+1) given our state st and action at and the real new state.But, remember that most of the time, our state is a stack of 4 frames (pixels). It means that we need to find a way to predict the next stack of frames which is really hard for two reasons:First of all, itâ€™s hard to predict the pixels directly, imagine youâ€™re in Doom you move left, you need to predict 248*248 = 61504 pixels!Second, the researchers think thatâ€™s not the right thing to do and take a good example to prove it.Imagine you need to study the movement of the tree leaves in a breeze. First of all, itâ€™s already hard to model breeze, consequently it is much harder to predict the pixel location of each leaves at each time step.The problem, is that because youâ€™ll always have a big pixel prediction error, the agent will always be curious even if the movement of the leaves is not the consequence of the agent actions therefore its continued curiosity is undesirable.Trying to predict the movement of each pixel at each timeframe is really hardSo instead of making prediction in the raw sensory space (pixels), we need to transform the raw sensory input (array of pixels) into a feature space with only relevant information.We need to define what rules must respect a good feature space, there are 3:Needs to model things that can be controlled by the agent.Needs also to model things that canâ€™t be controlled by the agent but that can affect an agent.Needs to not model (and consequently be unaffected) by things that are not in agentâ€™s control and have no effect on him.Letâ€™s take this example, your agent is a car, if we want to create a good feature representation we need to model:The yellow boxes are the important elementsOur car (controlled by our agent), the other cars (we canâ€™t control it but that can affect the agent) but we donâ€™t need to model the leaves (not affect the agent and we canâ€™t control it). This way we will have a feature representation with less noise.The desired embedding space should:Be compact in terms of dimensional (remove irrelevant parts of the observation space).Preserve sufficient information about the observation.Stable: because non-stationary rewards make it difficult for reinforcement agents to learn.Intrinsic Curiosity Module (ICM)ICM Taken from the PaperThe Intrinsic Curiosity Module is the system that helps us to generate curiosity. It is composed of two neural networks.Remember, we want to only predict changes in the environment that could possibly be due to the actions of our agent or affect the agent and ignore the rest. It means, we need instead of making predictions from a raw sensory space (pixels), transform the sensory input into a feature vector where only the information relevant to the action performed by the agent is represented.To learn this feature space: we use self-supervision, training a neural network on a proxy inverse dynamics task of predicting the agent action (Ã¢t) given its current and next states (st and st+1).Inverse Model PartSince the neural network is only required to predict the action, it has no incentive to represent within its feature embedding space, the factors of variation in the environment that does not affect the agent itself.Forward Model PartThen we use this feature space to train a forward dynamics model that predicts the future representation of the next state phi(st+1), given the feature representation of the current state phi(st) and the action at.And we provide the prediction error of the forward dynamics model to the agent as an intrinsic reward to encourage its curiosity.Curiosity = predicted_phi(st+1) â€” phi(st+1)So, we have two models in ICM:Inverse Model (Blue): Encode the states st and st+1 into the feature vectors phi(st) and phi(st+1) that are trained to predict action Ã¢t.Inverse Loss function that measures the difference between the real action and our predicted actionForward Model (Red): Takes as input phi(st) and at and predict the feature representation phi(st+1) of st+1.Forward Model Loss functionThen mathematically speaking, curiosity will be the difference between our predicted feature vector of the next state and the real feature vector of the next state.Finally the overall optimization problem of this module is a composition of Inverse Loss, Forward Loss.Thatâ€™s was a lot of information and mathematics!To recap:Because of extrinsic rewards implementation and sparse rewards problems, we want to create a reward that is intrinsic to the agent.To do that we created curiosity, which is the agentâ€™s error in predicting the consequence of its action given its current state.Using curiosity will push our agent to favor transitions with high prediction error (which will be higher in areas where the agent has spent less time, or in areas with complex dynamics) and consequently better explore our environment.But because we canâ€™t predict the next state by predicting the next frame (too much complicated), we use a better feature representation that will keep only elements that can be controlled by our agent or affect our agent.To generate curiosity, we use Intrinsic Curiosity module that is composed of two models: Inverse Model that is used to learn the feature representation of state and next state and Forward Dynamics model used to generate the predicted feature representation of the next state.Curiosity will be equal to the difference between predicted_phi(st+1) (Forward Dynamics model) and phi(st+1) (Inverse Dynamics model)Thatâ€™s all for today! Now that you understood the theory, you should read the two papers experiments results Curiosity-driven Exploration by Self-supervised Prediction and Large-Scale Study of Curiosity-Driven Learning.Next time, weâ€™ll implement a PPO agent using curiosity as intrinsic reward to play Super Mario Bros.Source address is here,]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Validation Checklist in Kaggle Competition]]></title>
      <url>%2F2018%2F10%2F16%2FValidation-Checklist-in-Kaggle-Competition%2F</url>
      <content type="text"><![CDATA[Data Splitting StrategiesRandomTimewiseBy id (maybe hidden)CombinedNoticesMake sure the strategy used by train/val splitting is same as train/test splitting.Different models trained from different data splitting strategies have much performance gap.Logic of feature generation depends on the data splitting strategy.Validation problemsValidation stageCauses of different scores and optimal parametersToo little dataToo diverse and inconsistent dataSolutionsAverage scores from different K-Fold splitsTune model on one split and evaluate score on the otherSubmission stageWe can observe thatLB score is consistently higher/lower than validation scoreLB score is not correlated with validation score at allCausesWe may already have quite different scores in K-Foldmake sure split train/validation correcttoo litter data in public LBJust trust your validation scorestrain and test data are from different distributionsclasses show in the test set not show in the train setmake a shift to your prediction (mean of train minus mean of test) â€“ LB probingclasses ratio is not samemake the validation classes ratio is same as test classes ratioExpect LB shuffle because ofRandomnessLitter amount of dataDifferent public/private distributions]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Coursera-dl plugin issues on Windows 10]]></title>
      <url>%2F2018%2F10%2F16%2FCoursera-dl-plugin-issues-on-Windows-10%2F</url>
      <content type="text"><![CDATA[I can&#x27;t download the video ~state: closed opened by: jenkey2011 on: 2017-05-04I can&#x27;t download the video ~Your environmentwin10Python version : 3.6coursera-dl version:0.8PSï¼šI&#x27;m from Chinaâ€¦â€¦Steps to reproduce&gt; coursera-dl -u xxx -p xxxx -b html-css-javascriptThen it works , but only the subtitles were downloaded ; And the cmd shows &quot;The following URLs (64) could not be downloaded:&quot; , they&#x27;re all video links;Commentsfrom: wanghoppe on: 2017-05-17Me too have the issueâ€¦. Could someone help??from: lvhuiyang on: 2017-05-18I met the same problem yesterday.I use the command &#x60;coursera-dl -u xx@xxx.com -p xxx course_name â€“wget&#x60; to solve it. (env: Mac OS, wget, python3.6)from: balta2ar on: 2017-05-18Are you guys all from China? Did you try downloading over a VPN or Tor? If it&#x27;s your government&#x27;s firewall, we can&#x27;t do anything about it, use proxies, VPNs and Tor.from: jenkey2011 on: 2017-05-19I guess soâ€¦â€¦ What a pity.from: FBryce on: 2017-06-02Hi, If you are from China, adding &quot;52.84.246.72 d3c33hcgiwev3.cloudfront.net&quot; in the host file and fresh dns with &quot; ipconfig/flushdns&quot; may workfrom: wanghoppe on: 2017-06-02@FBryce It worked! Thank you so much.from: XiangBicheng on: 2017-07-28@FBryce Nice! Thank you for useful information.from: techlarry on: 2017-08-23Good job @FBryce, I downloaded several courses manually because of the issue. It wasted me at least half day to download filesâ€¦.from: balta2ar on: 2017-08-27&gt; Hi, If you are from China, adding &quot;52.84.246.72 d3c33hcgiwev3.cloudfront.net&quot; in the host file and fresh dns with &quot; ipconfig/flushdns&quot; may workI&#x27;ve added your comment to the readme. Thanks! Closing.from: wenxingxing on: 2017-12-26It works, thanks~~from: 1c7 on: 2018-09-22Still work in 2018, Thanks!from: shuoooo on: 2018-10-07&gt; Hi, If you are from China, adding &quot;52.84.246.72 d3c33hcgiwev3.cloudfront.net&quot; in the host file and fresh dns with &quot; ipconfig/flushdns&quot; may workWorks beautifully, thanks a lotCould not authenticate: Cannot login on coursera.org: 400 Client Error: Bad Request for url: https://api.coursera.org/api/login/v3state: closed opened by: jeet-parekh on: 2018-06-15Subject of the issueRunning coursera-dl gives an error 400.Your environmentOperating System (name/version): Windows 8.1Python version: 3.6.5coursera-dl version: 0.11.2Steps to reproduceI ran the command &#x60;coursera-dl -u myusername -p mypassword machine-learning&#x60;.Is the problem happening with the latest version of the script?Yes.Do you have all the recommended versions of the modules? See them in thefile &#x60;requirements.txt&#x60;.Yes. I did a &#x60;pip install&#x60;.What is the course that you are trying to access?machine-learningWhat is the precise command line that you are using (don&#x27;t forget to obfuscateyour username and password, but leave all other information untouched).&#x60;coursera-dl -u myusername -p mypassword machine-learning&#x60;What are the precise messages that you get? Please, use the &#x60;â€“debug&#x60;option before posting the messages as a bug report. Please, copy and pastethem. Don&#x27;t reword/paraphrase the messages.&#x60;&#x60;&#x60;root[main] coursera_dl version 0.11.2root[main] Downloading class: machine-learning (1 / 1)root[download_class] Downloading new style (on demand) class machine-learningroot[login] Initiating login.root[login] There were no .coursera.org cookies to be cleared.root[prepape_auth_headers] Forging cookie header: csrftoken&#x3D;rgrpC7s9fPPIdLTaWeGA; csrf2_token_doUFgKoj&#x3D;G39Y5Rvw4XFBwlb8W8cN5SEM.urllib3.connectionpool[_new_conn] Starting new HTTPS connection (1): api.coursera.orgurllib3.connectionpool[_make_request] https://api.coursera.org:443 &quot;POST /api/login/v3 HTTP/1.1&quot; 400 Noneroot[main] Could not authenticate: Cannot login on coursera.org: 400 Client Error: Bad Request for url: https://api.coursera.org/api/login/v3&#x60;&#x60;&#x60;Expected behaviourThere would be no errors and the course would download.Actual behaviourThe output given above.Commentsfrom: jeet-parekh on: 2018-06-17It worked when I used my email id as the user name.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[EDA Example I (Springleaf competition)]]></title>
      <url>%2F2018%2F10%2F16%2FEDA-Example-I-Springleaf-competition%2F</url>
      <content type="text"><![CDATA[This is a notebook, used in the screencast video. Note, that the data files are not present here in Jupyter hub and you will not be able to run it. But you can always download the notebook to your local machine as well as the competition data and make it interactive.1234567891011import osimport numpy as npimport pandas as pd from tqdm import tqdm_notebookimport matplotlib.pyplot as plt%matplotlib inlineimport warningswarnings.filterwarnings('ignore')import seaborn123456789101112131415161718192021222324252627282930313233def autolabel(arrayA): ''' label each colored square with the corresponding data value. If value &gt; 20, the text is in black, else in white. ''' arrayA = np.array(arrayA) for i in range(arrayA.shape[0]): for j in range(arrayA.shape[1]): plt.text(j,i, "%.2f"%arrayA[i,j], ha='center', va='bottom',color='w')def hist_it(feat): plt.figure(figsize=(16,4)) feat[Y==0].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.8) feat[Y==1].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.5) plt.ylim((0,1)) def gt_matrix(feats,sz=16): a = [] for i,c1 in enumerate(feats): b = [] for j,c2 in enumerate(feats): mask = (~train[c1].isnull()) &amp; (~train[c2].isnull()) if i&gt;=j: b.append((train.loc[mask,c1].values&gt;=train.loc[mask,c2].values).mean()) else: b.append((train.loc[mask,c1].values&gt;train.loc[mask,c2].values).mean()) a.append(b) plt.figure(figsize = (sz,sz)) plt.imshow(a, interpolation = 'None') _ = plt.xticks(range(len(feats)),feats,rotation = 90) _ = plt.yticks(range(len(feats)),feats,rotation = 0) autolabel(a)12345def hist_it1(feat): plt.figure(figsize=(16,4)) feat[Y==0].hist(bins=100,range=(feat.min(),feat.max()),normed=True,alpha=0.5) feat[Y==1].hist(bins=100,range=(feat.min(),feat.max()),normed=True,alpha=0.5) plt.ylim((0,1))Read the data12train = pd.read_csv('train.csv.zip')Y = train.target12test = pd.read_csv('test.csv.zip')test_ID = test.IDData overviewProbably the first thing you check is the shapes of the train and test matrices and look inside them.12print 'Train shape', train.shapeprint 'Test shape', test.shapeTrain shape (145231, 1934) Test shape (145232, 1933) 1train.head().dataframe thead tr:only-child th{ text-align:right}.dataframe thead th{ text-align:left}.dataframe tbody tr th{ vertical-align:top}IDVAR_0001VAR_0002VAR_0003VAR_0004VAR_0005VAR_0006VAR_0007VAR_0008VAR_0009â€¦VAR_1926VAR_1927VAR_1928VAR_1929VAR_1930VAR_1931VAR_1932VAR_1933VAR_1934target02H22404300C0.00.0FalseFalseâ€¦989899899999999899899899989998IAPS014H7534448B1.00.0FalseFalseâ€¦989899899999999899899899989998IAPS025H11633464C0.00.0FalseFalseâ€¦989899899999999899899899989998IAPS037H2403003200C0.00.0FalseFalseâ€¦989899899999999899899899989998RCC048R722612000N0.00.0FalseFalseâ€¦989899899999999899899899989998BRANCH15 rows Ã— 1934 columns1test.head().dataframe thead tr:only-child th{ text-align:right}.dataframe thead th{ text-align:left}.dataframe tbody tr th{ vertical-align:top}IDVAR_0001VAR_0002VAR_0003VAR_0004VAR_0005VAR_0006VAR_0007VAR_0008VAR_0009â€¦VAR_1925VAR_1926VAR_1927VAR_1928VAR_1929VAR_1930VAR_1931VAR_1932VAR_1933VAR_193401R360252251B2.02.0FalseFalseâ€¦0989899899999999899899899989998IAPS13R741923274C2.03.0FalseFalseâ€¦0989899899999999899899899989998IAPS26R21363500C1.01.0FalseFalseâ€¦0989899899999999899899899989998IAPS39R821500B0.00.0FalseFalseâ€¦0989899899999999899899899989998IAPS410H913984500C8.03.0FalseFalseâ€¦0989899899999999899899899989998IAPS5 rows Ã— 1933 columnsThere are almost 2000 anonymized variables! Itâ€™s clear, some of them are categorical, some look like numeric. Some numeric feateures are integer typed, so probably they are event conters or dates. And others are of float type, but from the first few rows they look like integer-typed too, since fractional part is zero, but pandas treats them as float since there are NaN values in that features.From the first glance we see train has one more column target which we should not forget to drop before fitting a classifier. We also see ID column is shared between train and test, which sometimes can be succesfully used to improve the score.It is also useful to know if there are any NaNs in the data. You should pay attention to columns with NaNs and the number of NaNs for each row can serve as a nice feature later.12# Number of NaNs for each objecttrain.isnull().sum(axis=1).head(15)0 25 1 19 2 24 3 24 4 24 5 24 6 24 7 24 8 16 9 24 10 22 11 24 12 17 13 24 14 24 dtype: int64 12# Number of NaNs for each columntrain.isnull().sum(axis=0).head(15)ID 0 VAR_0001 0 VAR_0002 0 VAR_0003 0 VAR_0004 0 VAR_0005 0 VAR_0006 56 VAR_0007 56 VAR_0008 56 VAR_0009 56 VAR_0010 56 VAR_0011 56 VAR_0012 56 VAR_0013 56 VAR_0014 56 dtype: int64 Just by reviewing the head of the lists we immediately see the patterns, exactly 56 NaNs for a set of variables, and 24 NaNs for objects.Dataset cleaningRemove constant featuresAll 1932 columns are anonimized which makes us to deduce the meaning of the features ourselves. We will now try to clean the dataset.It is usually convenient to concatenate train and test into one dataframe and do all feature engineering using it.1traintest = pd.concat([train, test], axis = 0)First we schould look for a constant features, such features do not provide any information and only make our dataset larger.12# `dropna = False` makes nunique treat NaNs as a distinct valuefeats_counts = train.nunique(dropna = False)1feats_counts.sort_values()[:10]VAR_0213 1 VAR_0207 1 VAR_0840 1 VAR_0847 1 VAR_1428 1 VAR_1165 2 VAR_0438 2 VAR_1164 2 VAR_1163 2 VAR_1162 2 dtype: int64 We found 5 constant features. Letâ€™s remove them.12345constant_features = feats_counts.loc[feats_counts==1].index.tolist()print (constant_features)traintest.drop(constant_features,axis = 1,inplace=True)[&apos;VAR_0207&apos;, &apos;VAR_0213&apos;, &apos;VAR_0840&apos;, &apos;VAR_0847&apos;, &apos;VAR_1428&apos;] Remove duplicated featuresFill NaNs with something we can find later if needed.1traintest.fillna('NaN', inplace=True)Now letâ€™s encode each feature, as we discussed.1234train_enc = pd.DataFrame(index = train.index)for col in tqdm_notebook(traintest.columns): train_enc[col] = train[col].factorize()[0]â€‹We could also do something like this:1# train_enc[col] = train[col].map(train[col].value_counts())The resulting data frame is very very large, so we cannot just transpose it and use .duplicated. That is why we will use a simple loop.123456dup_cols = &#123;&#125;for i, c1 in enumerate(tqdm_notebook(train_enc.columns)): for c2 in train_enc.columns[i + 1:]: if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]): dup_cols[c2] = c1â€‹1dup_cols{&apos;VAR_0009&apos;: &apos;VAR_0008&apos;, &apos;VAR_0010&apos;: &apos;VAR_0008&apos;, &apos;VAR_0011&apos;: &apos;VAR_0008&apos;, &apos;VAR_0012&apos;: &apos;VAR_0008&apos;, &apos;VAR_0013&apos;: &apos;VAR_0006&apos;, &apos;VAR_0018&apos;: &apos;VAR_0008&apos;, &apos;VAR_0019&apos;: &apos;VAR_0008&apos;, &apos;VAR_0020&apos;: &apos;VAR_0008&apos;, &apos;VAR_0021&apos;: &apos;VAR_0008&apos;, &apos;VAR_0022&apos;: &apos;VAR_0008&apos;, &apos;VAR_0023&apos;: &apos;VAR_0008&apos;, &apos;VAR_0024&apos;: &apos;VAR_0008&apos;, &apos;VAR_0025&apos;: &apos;VAR_0008&apos;, &apos;VAR_0026&apos;: &apos;VAR_0008&apos;, &apos;VAR_0027&apos;: &apos;VAR_0008&apos;, &apos;VAR_0028&apos;: &apos;VAR_0008&apos;, &apos;VAR_0029&apos;: &apos;VAR_0008&apos;, &apos;VAR_0030&apos;: &apos;VAR_0008&apos;, &apos;VAR_0031&apos;: &apos;VAR_0008&apos;, &apos;VAR_0032&apos;: &apos;VAR_0008&apos;, &apos;VAR_0038&apos;: &apos;VAR_0008&apos;, &apos;VAR_0039&apos;: &apos;VAR_0008&apos;, &apos;VAR_0040&apos;: &apos;VAR_0008&apos;, &apos;VAR_0041&apos;: &apos;VAR_0008&apos;, &apos;VAR_0042&apos;: &apos;VAR_0008&apos;, &apos;VAR_0043&apos;: &apos;VAR_0008&apos;, &apos;VAR_0044&apos;: &apos;VAR_0008&apos;, &apos;VAR_0181&apos;: &apos;VAR_0180&apos;, &apos;VAR_0182&apos;: &apos;VAR_0180&apos;, &apos;VAR_0189&apos;: &apos;VAR_0188&apos;, &apos;VAR_0190&apos;: &apos;VAR_0188&apos;, &apos;VAR_0196&apos;: &apos;VAR_0008&apos;, &apos;VAR_0197&apos;: &apos;VAR_0008&apos;, &apos;VAR_0199&apos;: &apos;VAR_0008&apos;, &apos;VAR_0201&apos;: &apos;VAR_0051&apos;, &apos;VAR_0202&apos;: &apos;VAR_0008&apos;, &apos;VAR_0203&apos;: &apos;VAR_0008&apos;, &apos;VAR_0210&apos;: &apos;VAR_0208&apos;, &apos;VAR_0211&apos;: &apos;VAR_0208&apos;, &apos;VAR_0215&apos;: &apos;VAR_0008&apos;, &apos;VAR_0216&apos;: &apos;VAR_0008&apos;, &apos;VAR_0221&apos;: &apos;VAR_0008&apos;, &apos;VAR_0222&apos;: &apos;VAR_0008&apos;, &apos;VAR_0223&apos;: &apos;VAR_0008&apos;, &apos;VAR_0228&apos;: &apos;VAR_0227&apos;, &apos;VAR_0229&apos;: &apos;VAR_0008&apos;, &apos;VAR_0238&apos;: &apos;VAR_0089&apos;, &apos;VAR_0239&apos;: &apos;VAR_0008&apos;, &apos;VAR_0357&apos;: &apos;VAR_0260&apos;, &apos;VAR_0394&apos;: &apos;VAR_0246&apos;, &apos;VAR_0438&apos;: &apos;VAR_0246&apos;, &apos;VAR_0446&apos;: &apos;VAR_0246&apos;, &apos;VAR_0512&apos;: &apos;VAR_0506&apos;, &apos;VAR_0527&apos;: &apos;VAR_0246&apos;, &apos;VAR_0528&apos;: &apos;VAR_0246&apos;, &apos;VAR_0529&apos;: &apos;VAR_0526&apos;, &apos;VAR_0530&apos;: &apos;VAR_0246&apos;, &apos;VAR_0672&apos;: &apos;VAR_0670&apos;, &apos;VAR_1036&apos;: &apos;VAR_0916&apos;} Donâ€™t forget to save them, as it takes long time to find these.12import cPickle as picklepickle.dump(dup_cols, open('dup_cols.p', 'w'), protocol=pickle.HIGHEST_PROTOCOL)Drop from traintest.1traintest.drop(dup_cols.keys(), axis = 1,inplace=True)Determine typesLetâ€™s examine the number of unique values.12nunique = train.nunique(dropna=False)nuniqueID 145231 VAR_0001 3 VAR_0002 820 VAR_0003 588 VAR_0004 7935 VAR_0005 4 VAR_0006 38 VAR_0007 36 VAR_0008 2 VAR_0009 2 VAR_0010 2 VAR_0011 2 VAR_0012 2 VAR_0013 38 VAR_0014 38 VAR_0015 27 VAR_0016 30 VAR_0017 26 VAR_0018 2 VAR_0019 2 VAR_0020 2 VAR_0021 2 VAR_0022 2 VAR_0023 2 VAR_0024 2 VAR_0025 2 VAR_0026 2 VAR_0027 2 VAR_0028 2 VAR_0029 2 ... VAR_1907 41 VAR_1908 37 VAR_1909 41 VAR_1910 37 VAR_1911 107 VAR_1912 16370 VAR_1913 25426 VAR_1914 14226 VAR_1915 1148 VAR_1916 8 VAR_1917 10 VAR_1918 86 VAR_1919 383 VAR_1920 22 VAR_1921 18 VAR_1922 6798 VAR_1923 2445 VAR_1924 573 VAR_1925 11 VAR_1926 6 VAR_1927 10 VAR_1928 30 VAR_1929 591 VAR_1930 8 VAR_1931 10 VAR_1932 74 VAR_1933 363 VAR_1934 5 target 2 VAR_0004_mod50 50 Length: 1935, dtype: int64 and build a histogram of those values12plt.figure(figsize=(14,6))_ = plt.hist(nunique.astype(float)/train.shape[0], bins=100)Letâ€™s take a looks at the features with a huge number of unique values:12mask = (nunique.astype(float)/train.shape[0] &gt; 0.8)train.loc[:, mask].dataframe thead tr:only-child th{ text-align:right}.dataframe thead th{ text-align:left}.dataframe tbody tr th{ vertical-align:top}IDVAR_0212VAR_022702NaN311951149.20713e+102.76949e+06252.65477e+10654127377.75753e+103.01509e+06486.04238e+101186785147.73796e+101.76557e+066169.70303e+10801517203.10981e+108536418217.82124e+101.40254e+069221.94014e+102.2187e+0610233.71295e+102.77679e+0611243.01203e+1043430012251.80185e+101.48914e+0613269.83358e+1068666614289.33087e+101.4847e+0615302.01715e+1088371416314.15638e+102.6707e+0617329.17617e+102.65485e+0618353.81344e+104877211936NaN2.54705e+0620373.27144e+101.74684e+0621381.82142e+102.5813e+0622407.70153e+102.59396e+0623424.69701e+101.02977e+0624439.84442e+101.45101e+062546NaN2.37136e+0626509.25094e+1066593027513.09094e+1049768628526.06105e+101.95816e+0629543.78768e+101.62591e+06â€¦â€¦â€¦â€¦1452012904098.80126e+101.83053e+061452022904124.6152e+101.02024e+061452032904149.33055e+101.88151e+061452042904154.63509e+106693511452052904172.36028e+106557971452062904243.73293e+101.45626e+061452072904262.38892e+101.9503e+061452082904276.38632e+105963651452092904293.00602e+105721191452102904314.33429e+10161201452112904323.86543e+102.08375e+061452122904349.21391e+101.89779e+061452132904363.07472e+102.94532e+061452142904397.83326e+102.54726e+06145215290440NaN6003181452162904412.78561e+106025051452172904431.90952e+102.44184e+061452182904454.62035e+102.87349e+06145219290447NaN1.53493e+061452202904487.54282e+101.60102e+061452212904494.30768e+102.08415e+061452222904507.81325e+102.85367e+061452232904524.51061e+101.56506e+061452242904534.62223e+101.46815e+061452252904547.74507e+102.92811e+061452262904577.05088e+102.03657e+061452272904589.02492e+101.68013e+061452282904599.17224e+102.41922e+061452292904614.51033e+101.53960e+061452302904639.14114e+102.6609e+06145231 rows Ã— 3 columnsThe values are not float, they are integer, so these features are likely to be even counts. Letâ€™s look at another pack of features.12mask = (nunique.astype(float)/train.shape[0] &lt; 0.8) &amp; (nunique.astype(float)/train.shape[0] &gt; 0.4)train.loc[:25, mask].dataframe thead tr:only-child th{ text-align:right}.dataframe thead th{ text-align:left}.dataframe tbody tr th{ vertical-align:top}VAR_0541VAR_0543VAR_0899VAR_1081VAR_1082VAR_1087VAR_1179VAR_1180VAR_118104946311678311287176857768571167837685776857768571303472346196346375341365341365346196341365341365176604294990122601121501107267107267121501107267107267587143205935949061890457944756859490457944756847568410071357083478720475236473470820475236472364751887728055284552113921139280552113921139206276321783333565886886327744327744333565327744327744163944729615181110844326432651814326432643268203593011433434249692712830114249692712827128981513007677119711971300119711971197106088152331548370777077152337077707740331143214572000621621757621621621123835398607521158539752115811581314359475624756217706177064756217706177061770614145391218067214836176627176627216307175273175273910191510040121191726310399103991211910399103995379164880960796079165916596079165916591651712900355903578126096260963559026096260961964618104442139605150505136419142218139605136419142218142218191389825566266852012220122255662012220122201222035241003310133583858381003358385838583821129873204072206946183049183049204072183049183049967362235911140017680556555651140055655565556523999999999999999999-9999999999999999999999999999999999999999999999999999999999924127049551220124902490495524902490249025201524582458201520152458201520151008These look like counts too. First thing to notice is the 23th line: 99999.., -99999 values look like NaNs so we should probably built a related feature. Second: the columns are sometimes placed next to each other, so the columns are probably grouped together and we can disentangle that.Our conclusion: there are no floating point variables, there are some counts variables, which we will treat as numeric.And finally, letâ€™s pick one variable (in this case â€˜VAR_0015â€™) from the third group of features.1train['VAR_0015'].value_counts() 0.0 102382 1.0 28045 2.0 8981 3.0 3199 4.0 1274 5.0 588 6.0 275 7.0 166 8.0 97 -999.0 56 9.0 51 10.0 39 11.0 18 12.0 16 13.0 9 14.0 8 15.0 8 16.0 6 22.0 3 21.0 3 19.0 1 35.0 1 17.0 1 29.0 1 18.0 1 32.0 1 23.0 1 Name: VAR_0015, dtype: int64 12cat_cols = list(train.select_dtypes(include=['object']).columns)num_cols = list(train.select_dtypes(exclude=['object']).columns)Go throughLetâ€™s replace NaNs with something first.1train.replace('NaN', -999, inplace=True)Letâ€™s calculate how many times one feature is greater than the other and create cross tabel out of it.12345# select first 42 numeric featuresfeats = num_cols[:42]# build 'mean(feat1 &gt; feat2)' plotgt_matrix(feats,16)Indeed, we see interesting patterns here. There are blocks of geatures where one is strictly greater than the other. So we can hypothesize, that each column correspondes to cumulative counts, e.g. feature number one is counts in first month, second â€“ total count number in first two month and so on. So we immediately understand what features we should generate to make tree-based models more efficient: the differences between consecutive values.VAR_0002, VAR_00031234567hist_it(train['VAR_0002'])plt.ylim((0,0.05))plt.xlim((-10,1010))hist_it(train['VAR_0003'])plt.ylim((0,0.03))plt.xlim((-10,1010))(-10, 1010) 1train['VAR_0002'].value_counts()12 5264 24 4763 36 3499 60 2899 6 2657 13 2478 72 2243 48 2222 3 2171 4 1917 2 1835 84 1801 120 1786 1 1724 7 1671 26 1637 5 1624 14 1572 18 1555 8 1513 999 1510 25 1504 96 1445 30 1438 9 1306 144 1283 15 1221 27 1186 38 1146 37 1078 ... 877 1 785 1 750 1 653 1 784 1 764 1 751 1 797 1 926 1 691 1 808 1 774 1 902 1 755 1 656 1 814 1 813 1 685 1 739 1 935 1 906 1 807 1 550 1 933 1 804 1 675 1 674 1 745 1 778 1 851 1 Name: VAR_0002, Length: 820, dtype: int64 1train['VAR_0003'].value_counts()0 17436 24 3469 12 3271 60 3054 36 2498 72 2081 48 2048 6 1993 1 1797 3 1679 84 1553 2 1459 999 1428 4 1419 120 1411 7 1356 13 1297 18 1296 96 1253 14 1228 8 1216 5 1189 9 1182 30 1100 25 1100 144 1090 15 1047 61 1008 26 929 42 921 ... 560 1 552 1 550 1 804 1 543 1 668 1 794 1 537 1 531 1 664 1 632 1 709 1 597 1 965 1 852 1 648 1 596 1 466 1 592 1 521 1 533 1 636 1 975 1 973 1 587 1 523 1 584 1 759 1 583 1 570 1 Name: VAR_0003, Length: 588, dtype: int64 We see there is something special about 12, 24 and so on, sowe can create another feature x mod 12.VAR_0004123train['VAR_0004_mod50'] = train['VAR_0004'] % 50hist_it(train['VAR_0004_mod50'])plt.ylim((0,0.6))(0, 0.6) Categorical featuresLetâ€™s take a look at categorical features we have.1train.loc[:,cat_cols].head().T.dataframe thead tr:only-child th{ text-align:right}.dataframe thead th{ text-align:left}.dataframe tbody tr th{ vertical-align:top}01234VAR_0001HHHHRVAR_0005CBCCNVAR_0008FalseFalseFalseFalseFalseVAR_0009FalseFalseFalseFalseFalseVAR_0010FalseFalseFalseFalseFalseVAR_0011FalseFalseFalseFalseFalseVAR_0012FalseFalseFalseFalseFalseVAR_0043FalseFalseFalseFalseFalseVAR_0044[][][][][]VAR_0073NaT2012-09-04 00:00:00NaTNaTNaTVAR_00752011-11-08 00:00:002011-11-10 00:00:002011-12-13 00:00:002010-09-23 00:00:002011-10-15 00:00:00VAR_0156NaTNaTNaTNaTNaTVAR_0157NaTNaTNaTNaTNaTVAR_0158NaTNaTNaTNaTNaTVAR_0159NaTNaTNaTNaTNaTVAR_0166NaTNaTNaTNaTNaTVAR_0167NaTNaTNaTNaTNaTVAR_0168NaTNaTNaTNaTNaTVAR_0169NaTNaTNaTNaTNaTVAR_0176NaTNaTNaTNaTNaTVAR_0177NaTNaTNaTNaTNaTVAR_0178NaTNaTNaTNaTNaTVAR_0179NaTNaTNaTNaTNaTVAR_0196FalseFalseFalseFalseFalseVAR_0200FT LAUDERDALESANTEEREEDSVILLELIBERTYFRANKFORTVAR_0202BatchInquiryBatchInquiryBatchInquiryBatchInquiryBatchInquiryVAR_02042014-01-29 21:16:002014-02-01 00:11:002014-01-30 15:11:002014-02-01 00:07:002014-01-29 19:31:00VAR_0214NaNNaNNaNNaNNaNVAR_0216DSDSDSDSDSVAR_02172011-11-08 02:00:002012-10-02 02:00:002011-12-13 02:00:002012-11-01 02:00:002011-10-15 02:00:00VAR_0222C6C6C6C6C6VAR_0226FalseFalseFalseFalseFalseVAR_0229FalseFalseFalseFalseFalseVAR_0230FalseFalseFalseFalseFalseVAR_0232TrueFalseTrueFalseTrueVAR_0236TrueTrueTrueTrueTrueVAR_0237FLCAWVTXILVAR_0239FalseFalseFalseFalseFalseVAR_0274FLMIWVTXILVAR_0283SSSSSVAR_0305SSPPPVAR_0325-1HRHSVAR_0342CFECUU-1-1VAR_0352OORRRVAR_0353URRRUVAR_0354OR-1-1OVAR_0404CHIEF EXECUTIVE OFFICER-1-1-1-1VAR_0466-1I-1-1-1VAR_0467-1Discharged-1-1-1VAR_0493COMMUNITY ASSOCIATION MANAGER-1-1-1-1VAR_1934IAPSIAPSIAPSRCCBRANCHVAR_0200, VAR_0237, VAR_0274 look like some georgraphical data thus one could generate geography related features, we will talk later in the course.There are some features, that are hard to identify, but look, there a date columns VAR_0073 â€“ VAR_0179, VAR_0204, VAR_0217. It is useful to plot one date against another to find relationships.12345678910date_cols = [u'VAR_0073','VAR_0075', u'VAR_0156',u'VAR_0157',u'VAR_0158','VAR_0159', u'VAR_0166', u'VAR_0167',u'VAR_0168',u'VAR_0169', u'VAR_0176',u'VAR_0177',u'VAR_0178',u'VAR_0179', u'VAR_0204', u'VAR_0217']for c in date_cols: train[c] = pd.to_datetime(train[c],format = '%d%b%y:%H:%M:%S') test[c] = pd.to_datetime(test[c], format = '%d%b%y:%H:%M:%S')12345678c1 = 'VAR_0217'c2 = 'VAR_0073'# mask = (~test[c1].isnull()) &amp; (~test[c2].isnull())# sc2(test.ix[mask,c1].values,test.ix[mask,c2].values,alpha=0.7,c = 'black')mask = (~train[c1].isnull()) &amp; (~train[c2].isnull())sc2(train.loc[mask,c1].values,train.loc[mask,c2].values,c=train.loc[mask,'target'].values)We see that one date is strictly greater than the other, so the difference between them can be a good feature. Also look at horizontal line there â€“ it also looks like NaN, so I would rather create a new binary feature which will serve as an idicator that our time feature is NaN.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[EDA check list]]></title>
      <url>%2F2018%2F10%2F16%2FEDA-check-list%2F</url>
      <content type="text"><![CDATA[Get domain knowledgeCheck if the data is intuitive (abnormal detection)add a feature is_incorrectUnderstand how the data was generatedIt is crucial to understand the generation process to set up a proper validation schemeTwo things to do with anonymized featuresTry to decode the featuresGuess the true meaning of the featureGuess the feature typesEach type need its own preprocessingVisualizationTools for individual features explorationHistograms plt.hist(x)Plot (index versus value) plt.plot(x, something)Statistics df.describe() or x.mean() or x.var()Other tools x.value_counts() or x.isnull()Tools for feature relationshipsPairsplt.scatter(x1, x2)pd.scatter_matrix(df)df.corr() or plt.matshow()Groups:ClusteringPlot (index vs feature statistics) df.mean().sort_values().plot()Data Cleanremove duplicated and constant featurestraintest.nunique(axis=1) == 1traintest.T.drop_duplicates()for f in categorical_feats: traintest[f] = traintest[f].factorize then traintest.T.drop_duplicates()check if same rows have same labelcheck if dataset is shuffled]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Processing Anonymized Features]]></title>
      <url>%2F2018%2F10%2F16%2FProcessing-anonymized-features%2F</url>
      <content type="text"><![CDATA[IMPORTANT: You will not be able to run this notebook at coursera platform, as the dataset is not there. The notebook is in read-only mode.But you can run the notebook locally and download the dataset using this link to explore the data interactively.1pd.set_option('max_columns', 100)Load the data12train = pd.read_csv('./train.csv')train.head().dataframe thead tr:only-child th{ text-align:right}.dataframe thead th{ text-align:left}.dataframe tbody tr th{ vertical-align:top}x0x1x2x3x4x5x6x7x8x9x10x11x12x13x14x15x16x17x18x19x20x21x22x23x24x25x26x27x28x29x30x31x32x33x34x35x36x37x38x39x40x41x42x43x44x45x46x47x48x49x50x51x52x53x54x55x56x57x58x59x60x61y0b4d8a653ea16a14a2d1706330986edca63304de0a62168d6261746600cb011-0.6887067e5c97705ae5df3eff9b91bb549494e33c63cf353694.06e40247e69617a4ad3f9718c61545bc26d08129a634e3cf3acdd9c9e0da217c99905b6513a3e3f369aba4d7f5140.579612-0.112693-0.1721911.1666671.6745380.63088937.0000001.29492255.00.16666710.00.00.0000001.09.00.01.023.03.670.121.9352.20.6250.2500.1250.0000.8130.0740.6340.5480.2353330.2649520.0000000.3333330.3333330.3333330.0000000.0000009.021467f9617a316a14a2d1706330986edca63304de0b7584c2d521746600cb0110.8708715624b8f759fa0b797a92669ea3d319f17880307418156.001ede04b4b617a4ad3f9718c61545bd342e2765fbb20e1ca068a6c8cef831b02793146992153ed659aba4d7f5128.7655032.6122852.1590914.0000001.7107141.7135380.1666670.027669109.00.00000031.00.00.0000001.0244.01.01.068.017.250.573.4524.00.4090.6190.5790.2480.3460.5410.5220.0001.7823461.3224090.0116470.3976710.2396010.2495840.0682200.033278601.042190436e52816a14a2d1706330986edca63304de0b7584c2d521746600cb0110.4376555624b8f759152af2cb2f91bb549494e33c63cf351178.0cc69cbe29a617a4ad3f9e8a040423ac82c3dbd33ee3501282b199ce7c4845f17dedd5c5c5025bd0a9aba4d7f5124.943933-0.814660-0.7083081.500000-0.512422-0.7339670.33333314.83772811.00.00000024.00.00.0000001.029.00.03.011.04.420.150.1610.21.0001.0001.0001.0001.0000.5200.5330.835-0.5865400.6724360.0000000.6060610.1212120.2121210.0606060.00000033.03343859085bc16a14a2d1706330986edca63304de0a62168d6261746600cb0110.004439f67f142e40c4dd2197c391bb549494e33c63cf3514559.06e40247e69617a4ad3f9718c61545bc26d08129a9e166b965d466f8951b0fde72a6d5cacfadc5c019aba4d7f5141.576860-0.907833-0.7617360.500000-0.627525-0.8058011.1666670.0043950.00.5000000.00.00.0000007.07.00.03.015.08.920.290.2260.80.0000.0000.0000.0000.0001.0000.0000.000-1.600326-1.8386800.0000001.0000000.0000000.0000000.0000000.0000001.044a4c3095b7516a14a2d1706330986edca63304de0b7584c2d521746600cb0110.4809777e5c97705ae071d01df591bb549494e33c63cf355777.06e40247e69617a4ad3f94b9480aa42e84655292c527b6ca8ccdd9c9e0da217c99905b60fc56ea1f09aba4d7f5131.080282-0.371787-0.3676161.6666670.2713070.01311217.3333331713.43912833.00.0000006.01.00.6666678.0108.01.04.086.01.580.052.0322.40.3480.7620.5500.3920.4890.5171.0000.6420.9609910.7909900.0201610.6451610.2580650.0362900.0403230.000000248.03Build a quick baseline123456789101112131415161718from sklearn.ensemble import RandomForestClassifier# Create a copy to work withX = train.copy()# Save and drop labelsy = train.yX = X.drop('y', axis=1)# fill NANs X = X.fillna(-999)# Label encoderfor c in train.columns[train.dtypes == 'object']: X[c] = X[c].factorize()[0] rf = RandomForestClassifier()rf.fit(X,y)RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&apos;gini&apos;, max_depth=None, max_features=&apos;auto&apos;, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=None, verbose=0, warm_start=False) 12plt.plot(rf.feature_importances_)plt.xticks(np.arange(X.shape[1]), X.columns.tolist(), rotation=90);/home/dulyanov/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family [u&apos;serif&apos;] not found. Falling back to DejaVu Sans (prop.get_family(), self.defaultFamily[fontext])) There is something interesting about x8.123# we see it was standard scaled, most likely, if we concat train and test, we will get exact mean=1, and std 1 print 'Mean:', train.x8.mean()print 'std:', train.x8.std()Mean: -0.000252352028622 std: 1.02328163601 12# And we see that it has a lot of repeated valuestrain.x8.value_counts().head(15)-2.984750 2770 0.480977 2569 0.610941 1828 0.654263 1759 0.567620 1746 0.697585 1691 0.524298 1639 0.740906 1628 0.394333 1610 0.437655 1513 0.351012 1450 0.264369 1429 0.307690 1401 0.221047 1372 0.784228 1293 Name: x8, dtype: int64 1234567# It's very hard to work with scaled feature, so let's try to scale them back# Let's first take a look at difference between neighbouring values in x8x8_unique = train.x8.unique()x8_unique_sorted = np.sort(x8_unique) np.diff(x8_unique_sorted)array([ 43.27826527, 38.98942817, 0.21660793, 0.04332159, 0.17328635, 0.21660793, 0.08664317, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.12996476, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.04332159, 0.21660793, 1.16968285, 0.04332159, 0.38989428, nan]) 1234567# The most of the diffs are 0.04332159! # The data is scaled, so we don't know what was the diff value for the original feature# But let's assume it was 1.0# Let's devide all the numbers by 0.04332159 to get the right scaling# note, that feature will still have zero meannp.diff(x8_unique_sorted/0.04332159)array([ 998.99992752, 899.9999347 , 4.99999964, 0.99999993, 3.99999971, 4.99999964, 1.99999985, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 2.99999978, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 0.99999993, 4.99999964, 26.99999804, 0.99999993, 8.99999935, nan]) 1(train.x8/0.04332159).head(10)0 -15.897530 1 20.102468 2 10.102468 3 0.102469 4 11.102468 5 -68.897526 6 10.102468 7 15.102468 8 9.102468 9 -68.897526 Name: x8, dtype: float64 1234# Ok, now we see .102468 in every value# this looks like a part of a mean that was subtracted during standard scaling# If we subtract it, the values become almost integers(train.x8/0.04332159 - .102468).head(10)0 -15.999998 1 20.000000 2 10.000000 3 0.000001 4 11.000000 5 -68.999994 6 10.000000 7 15.000000 8 9.000000 9 -68.999994 Name: x8, dtype: float64 123# let's round them x8_int = (train.x8/0.04332159 - .102468).round()x8_int.head(10)0 -16.0 1 20.0 2 10.0 3 0.0 4 11.0 5 -69.0 6 10.0 7 15.0 8 9.0 9 -69.0 Name: x8, dtype: float64 123# Ok, what's next? In fact it is not obvious how to find shift parameter, # and how to understand what the data this feature actually store# But ...1x8_int.value_counts()-69.0 2770 11.0 2569 14.0 1828 15.0 1759 13.0 1746 16.0 1691 12.0 1639 17.0 1628 9.0 1610 10.0 1513 8.0 1450 6.0 1429 7.0 1401 5.0 1372 18.0 1293 1.0 1290 4.0 1276 2.0 1250 3.0 1213 -1.0 1085 0.0 1080 -2.0 1006 -4.0 995 -3.0 976 -5.0 954 -8.0 923 -9.0 921 -6.0 906 19.0 893 -7.0 881 ... 26.0 3 -40.0 3 -41.0 3 25.0 2 -59.0 2 31.0 2 34.0 2 -46.0 2 -49.0 2 33.0 2 -42.0 2 32.0 2 37.0 2 30.0 2 -45.0 2 -54.0 1 36.0 1 -51.0 1 27.0 1 79.0 1 -47.0 1 69.0 1 70.0 1 -50.0 1 -1968.0 1 42.0 1 -63.0 1 -48.0 1 -64.0 1 35.0 1 Name: x8, Length: 99, dtype: int64 123# do you see this -1968? Doesn't it look like a year? ... So my hypothesis is that this feature is a year of birth! # Maybe it was a textbox where users enter their year of birth, and someone entered 0000 instead# The hypothesis looks plausible, isn't it?1(x8_int + 1968.0).value_counts().sort_index()0.0 1 999.0 4 1899.0 2770 1904.0 1 1905.0 1 1909.0 2 1914.0 1 1916.0 3 1917.0 1 1918.0 1 1919.0 2 1920.0 1 1921.0 1 1922.0 2 1923.0 2 1924.0 4 1925.0 4 1926.0 2 1927.0 3 1928.0 3 1929.0 4 1930.0 4 1931.0 12 1932.0 10 1933.0 7 1934.0 13 1935.0 28 1936.0 35 1937.0 35 1938.0 45 ... 1978.0 1513 1979.0 2569 1980.0 1639 1981.0 1746 1982.0 1828 1983.0 1759 1984.0 1691 1985.0 1628 1986.0 1293 1987.0 893 1988.0 624 1989.0 434 1990.0 233 1991.0 110 1992.0 31 1993.0 2 1994.0 3 1995.0 1 1998.0 2 1999.0 2 2000.0 2 2001.0 2 2002.0 2 2003.0 1 2004.0 1 2005.0 2 2010.0 1 2037.0 1 2038.0 1 2047.0 1 Name: x8, Length: 99, dtype: int64 1# After the competition ended the organisers told it was really a year of birth]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Exploratory Data Analysis]]></title>
      <url>%2F2018%2F10%2F16%2FExploratory-Data-Analysis%2F</url>
      <content type="text"><![CDATA[This is a detailed EDA of the data, shown in the second video of â€œExploratory data analysisâ€ lecture (week 2).PLEASE NOTE: the dataset cannot be published, so this notebook is read-only.Load dataIn this competition hosted by solutions.se, the task was to predict the advertisement cost for a particular ad.12345678import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinedata_path = './data'train = pd.read_csv('%s/train.csv.gz' % data_path, parse_dates=['Date'])test = pd.read_csv('%s/test.csv.gz' % data_path, parse_dates=['Date'])Letâ€™s look at the data (notice that the table is transposed, so we can see all feature names).1train.head().T01234AdGroupId78db03413668a0110c6921af1035aff63fda0c33cd868ebdccAdGroupName6d91d 25866 9c5942657d cb2d0 6d91d6d91d e33a0 9a99b59991 9c5946d91d 25866 9a99bAdNetworkType2sssssAveragePosition1.22111.1CampaignId273823cb71273823cb71273823cb71273823cb71273823cb71CampaignName2657d 16cb2 74532 b4842 0136e 35aca f140d2657d 16cb2 74532 b4842 0136e 35aca f140d2657d 16cb2 74532 b4842 0136e 35aca f140d2657d 16cb2 74532 b4842 0136e 35aca f140d2657d 16cb2 74532 b4842 0136e 35aca f140dClicks00003Conversions00000ConversionsManyPerClick00000Cost00000.94Date2014-01-01 00:00:002014-01-01 00:00:002014-01-01 00:00:002014-01-01 00:00:002014-01-01 00:00:00DestinationUrl98035d60fcc25f23cd0801f87f76395c0e89f5328888b55ddeDevicetttddFirstPageCpc1.062.940.421.750.17Impressions3214122KeywordMatchTypebbbbbKeywordTextjze 10 +uxsgk+jze +dznvgyhjclrjze 100 +gzpxykjze 10 +uxsgk 1950kjze 10 mykj +gzpxykMaxCpc0.2810.220.540.12QualityScore11111Slots_2s_2s_1s_2s_1TopOfPageCpc1.075.020.4240.25KeywordId7d20d63df9a617d4f0376e0b7024d29c2ea0cdf84c8ba7affdWe see a lot of features with not obvious names. If you search for the CampaignId, AdGroupName, AdNetworkType2 using any web search engine, you will find this dataset was exported from Google AdWords. So what is the required domain knowledge here? The knowledge of how web advertisement and Google AdWords work! After you have learned it, the features will make sense to you and you can proceed.For the sake of the story I will briefly describe Google AdWords system now. Basically every time a user queries a search engine, Google AdWords decides what ad will be shown along with the actual search results. On the other side of AdWords, the advertisers manage the ads â€“ they can set a multiple keywords, that a user should query in order to their ad to be shown. If the keywords are set properly and are relevant to the ad, then the ad will be shown to relevant users and the ad will get clicked. Advertisers pay to Google for some type of events, happened with their ad: for example for a click event, i.e. the user saw this ad and clicked it. AdWords uses complex algorithms to decide which ad to show to a particular user with a particular search query. The advertisers can only indirectly influence AdWords decesion process by changing keywords and several other parameters. So at a high level, the task is to predict what will be the costs for the advertiser (how much he will pay to Google, column Cost) when the parameters (e.g. keywords) are changed.The ads are grouped in groups, there are features AdGroupId AdGroupName describing them. A campaign corresponds to some specific parameters that an advertiser sets. Similarly, there are ID and name features CampaignId, CampaignName. And finally there is some information about keywords: KeywordId and KeywordText. Slot is $1$ when ad is shown on top of the page, and $2$ when on the side. Device is a categorical variable and can be either â€œtabletâ€, â€œmobileâ€ or â€œpcâ€. And finally the Date is just the date, for which clicks were aggregated.1test.head().T01234Id01234AdGroupId00096e761100096e761100096e761100096e761100096e7611AdGroupNamec8037 75b01 9a99b 3b678 52ba4 2657dc8037 75b01 9a99b 3b678 52ba4 2657dc8037 75b01 9a99b 3b678 52ba4 2657dc8037 75b01 9a99b 3b678 52ba4 2657dc8037 75b01 9a99b 3b678 52ba4 2657dAdNetworkType2sssssAveragePosition11111CampaignIde62b4bc4c3e62b4bc4c3e62b4bc4c3e62b4bc4c3e62b4bc4c3CampaignName2657d 16cb2 74532 06feb 0136e 3a15d2657d 16cb2 74532 06feb 0136e 3a15d2657d 16cb2 74532 06feb 0136e 3a15d2657d 16cb2 74532 06feb 0136e 3a15d2657d 16cb2 74532 06feb 0136e 3a15dDate2014-06-01 00:00:002014-06-01 00:00:002014-06-01 00:00:002014-06-01 00:00:002014-06-01 00:00:00DestinationUrlf5aad09031f5aad09031f5aad09031f5aad09031f5aad09031DevicetdmtdKeywordId539778bb80539778bb80539778bb80539778bb80539778bb80KeywordMatchTypeeeeeeKeywordTexttcjnw gzpxyk nyss ewzhytcjnw gzpxyk nyss ewzhytcjnw gzpxyk nyss ewzhytcjnw gzpxyk nyss ewzhytcjnw gzpxyk nyss ewzhySlots_1s_1s_1s_2s_2Notice there is diffrent number of columns in test and train â€“ our target is Cost column, but it is closly related to several other features, e.g. Clicks, Conversions. All of the related columns were deleted from the test set to avoid data leakages.Letâ€™s analyzeAre we ready to modeling? Not yet. Take a look at this statistic:12345678print 'Train min/max date: %s / %s' % (train.Date.min().date(), train.Date.max().date())print 'Test min/max date: %s / %s' % ( test.Date.min().date(), test.Date.max().date())print ''print 'Number of days in train: %d' % ((train.Date.max() - train.Date.min()).days + 1)print 'Number of days in test: %d' % (( test.Date.max() - test.Date.min()).days + 1)print ''print 'Train shape: %d rows' % train.shape[0]print 'Test shape: %d rows' % test.shape[0]Train min/max date: 2014-01-01 / 2014-05-31 Test min/max date: 2014-06-01 / 2014-06-14 Number of days in train: 151 Number of days in test: 14 Train shape: 3493820 rows Test shape: 8951040 rows Train period is more than 10 times larger than the test period, but train set has fewer rows, how could that happen?At this point I suggest you to stop and think yourself, what could be a reason, why this did happen. Unfortunately we cannot share the data for this competition, but the information from above should be enough to get a right idea.Alternatively, you can go along for the explanation, if you want.InvestigationLetâ€™s take a look how many rows with each date we have in train and test.1test.Date.value_counts()2014-06-02 639360 2014-06-12 639360 2014-06-09 639360 2014-06-14 639360 2014-06-01 639360 2014-06-11 639360 2014-06-08 639360 2014-06-05 639360 2014-06-10 639360 2014-06-07 639360 2014-06-04 639360 2014-06-06 639360 2014-06-03 639360 2014-06-13 639360 Name: Date, dtype: int64 12# print only first 10train.Date.value_counts().head(10)2014-01-01 36869 2014-01-04 36427 2014-01-05 36137 2014-01-02 34755 2014-01-03 34693 2014-01-06 31349 2014-04-07 30950 2014-02-09 30101 2014-01-26 29830 2014-02-08 29187 Name: Date, dtype: int64 Interesting, for the test set we have the same number of rows for every date, while in train set the number of rows is different for each day. It looks like that for each day in the test set a loop through some kind of IDs had been run. But what about train set? So far we donâ€™t know, but letâ€™s find the test IDs first.TestSo now we know, that there is $639360$ different IDs. It should be easy to find the columns, that form ID, because if the ID is [â€˜col1â€™, â€˜col2â€™], then to compute the number of combinations we should just multiply the number of unique elements in each.12test_nunique = test.nunique()test_nuniqueId 8951040 AdGroupId 13548 AdGroupName 2281 AdNetworkType2 2 AveragePosition 131 CampaignId 252 CampaignName 252 Date 14 DestinationUrl 52675 Device 3 KeywordId 12285 KeywordMatchType 3 KeywordText 11349 Slot 4 dtype: int64 12345678910111213141516import itertools# This function looks for a combination of elements # with product of 639360 def find_prod(data): # combinations of not more than 5 features for n in range(1, 5): # iterate through all combinations for c in itertools.combinations(range(len(data)), n): if data[list(c)].prod() == 639360: print test_nunique.index[c] return print 'Nothing found' find_prod(test_nunique.values)Nothing found Hmm, nothing found! The problem is that some features are tied, and the number of their combinations does not equal to product of individual unique number of elements. For example it does not make sense to create all possible combinations of DestinationUrl and AdGroupId as DestinationUrl belong to exactly one AdGroupId.1test.groupby('DestinationUrl').AdGroupId.nunique()DestinationUrl 00010d62df 1 000249f717 1 00054cf3f8 1 000684bf0b 1 00072a9fa7 1 00077a6729 1 0007cc191f 1 0009388900 1 001144cae4 1 00115f6477 1 00141a299f 1 00169dc49b 1 0018b27e06 1 001b0b3d06 1 001ef8368e 1 00205e056a 1 002082ab8b 1 0020c585ea 1 0021419f7e 1 00225519cc 1 002498dc88 1 0026171436 1 00265dc4bb 1 0026833e5c 1 0027ffbad9 1 002b1deb25 1 002c55ccef 1 002e44290f 1 0030ca870e 1 0032b64beb 1 .. ffda377018 1 ffda3c412a 1 ffda5b53d6 1 ffda8c0d8c 1 ffdbf5d179 1 ffdc872fcf 1 ffde114af5 1 ffde41a800 1 ffe2fb7007 1 ffe4a040d4 1 ffe685e937 1 ffe8c3da53 1 ffe8f82e08 1 ffeb9fda9d 1 ffebd1d253 1 ffebea724f 1 ffecf398b1 1 ffecf3e7d4 1 ffed185438 1 fff02d7269 1 fff10adcb0 1 fff12e5f19 1 fff132d5bd 1 fff19836a0 1 fff3539204 1 fff4c5d255 1 fff55db78a 1 fff8c11ad9 1 fff90ea351 1 fffb248bf0 1 Name: AdGroupId, Length: 52675, dtype: int64 So, now letâ€™s try to find ID differently. Letâ€™s try to find a list of columns, such that threre is exazctly $639360$ unique combinations of their values in the test set (not overall). So, we want to find columns, such that:1test[columns].drop_duplicates().shape[0] == 639360We could do it with a similar loop.123456789101112131415import itertoolsdef find_ncombinations(data): # combinations of not more than 5 features for n in range(1, 5): for c in itertools.combinations(range(data.shape[1]), n): print c columns = test.columns[list(c)] if test[columns].drop_duplicates().shape[0] == 639360: print columns return print 'Nothing found' find_ncombinations(test)But it will take forever to compute. So it is easier to find the combination manually.So after some time of trials and errors I figured out, that the four features KeywordId, AdGroupId, Device, Slot form the index. The number of unique rows is exactly 639360 as we wanted to find.12columns = ['KeywordId', 'AdGroupId', 'Device', 'Slot']test[columns].drop_duplicates().shape(639360, 4) Looks reasonable. For each AdGroupId there is a distinct set of possible KeywordIdâ€™s, but Device and Slot variants are the same for each ad. And the target is to predict what will be the daily cost for using different KeywordIdâ€™s, Device type, Slot type to advertise ads from AdGroups.TrainTo this end, we found how test set was constructed, but what about the train set? Let us plot something, probably we will find it out.123import seaborn as snssns.set(palette='pastel')sns.set(font_scale=2)12# from absolute dates to relativetrain['date_diff'] = (train.Date - train.Date.min()).dt.days12345678910# group by the index, that we've foundg= train.groupby(['KeywordId', 'AdGroupId', 'Device', 'Slot'])# and for each index show average relative date versus # the number of rows with that indexplt.figure(figsize=(12,12))plt.scatter(g.date_diff.mean(),g.size(),edgecolor = 'none',alpha = 0.2, s=20, c='b')plt.xlabel('Group mean relative date')plt.ylabel('Group size')plt.title('Train');Looks interesting, isnâ€™t it? That is something we need to explain! How the same plot looks for the test set?12# from absolute dates to relativetest['date_diff'] = (test.Date - test.Date.min()).dt.days1234567891011# group by the index, that we've foundg= test.groupby(['KeywordId', 'AdGroupId', 'Device', 'Slot'])# and for each index show average relative date versus # the number of rows with that indexplt.figure(figsize=(12,12))plt.scatter(g.date_diff.mean(),g.size(),edgecolor = 'none',alpha = 0.2, s=20, c='b')plt.xlabel('Group mean relative date')plt.ylabel('Group size')plt.ylim(-2, 30)plt.title('Test');Just a dot!Now letâ€™s think, what we actually plotted? We grouped the data by the ID that weâ€™ve found previously and we plotted average Date in the group versus the size of each group. We found that ID is an aggregation index â€“ so for each date the Cost is aggreagated for each possible index. So group size shows for how many days we have Const information for each ID and mean relative date shows some information about these days.For test set it is expectable that both average date and the size of the groups are the same for each group: the size of each group is $14$ (as we have $14$ test days) and mean date is $6.5$, because for each group (index) we have $14$ different days, and $\frac{0 + 1 + \dots + 13}{14} = 6.5$.And now we can explain everything for the train set. Look at the top of the triangle: for those points (groups) we have Cost information for all the days in the train period, while on the sides we see groups, for which we have very few rows.But why for some groups we have smaller number of rows, than number of days? Letâ€™s look at the Impressions column.1train.Impressions.value_counts()1 1602929 2 565896 3 287128 4 175197 5 119092 6 86651 7 66443 8 53007 9 42984 10 35731 11 30248 12 25950 13 22629 14 20126 15 17503 16 15682 17 14100 18 12848 19 11597 20 10724 21 9864 22 8931 23 8316 24 7953 25 7168 26 6684 27 6196 28 5863 29 5556 30 5223 ... 4978 1 15210 1 9076 1 13174 1 116535 1 4979 1 17273 1 90974 1 4976 1 5906 1 7023 1 60282 1 7955 1 13881 1 2921 1 4970 1 7019 1 17249 1 23394 1 28210 1 11116 1 15929 1 7017 1 95761 1 2923 1 15213 1 9070 1 5692 1 13162 1 13922 1 Name: Impressions, Length: 8135, dtype: int64 We never have $0$ value in Imressions column. But in reality, of course, some ads with some combination of keyword, slot, device were never shown. So this looks like a nice explanation for the data: in the train set we only have information about ads (IDs, groups) which were shown at least once. And for the test set, we, of course, want to predict Cost for every possible ID.What it means for competitors, is that if one would just fit a model on the train set as is, the predictions for the test set will be biased by a lot. The predictions will be much higher than they should be, as we are only given a specific subset of rows as train.csv file.So, before modeling we should first extend the trainset and inject rows with 0 impressions. Such change will make train set very similar to the test set and the models will generalize nicely.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[æ•°æ®é¢„å¤„ç†ç›¸å…³æŠ€æœ¯]]></title>
      <url>%2F2018%2F10%2F15%2F%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF%2F</url>
      <content type="text"><![CDATA[æ•°å€¼åž‹æ•°æ® (non-tree based model)ç‰¹å¾é¢„å¤„ç†MinMaxScalar ä¸ä¼šæ”¹å˜æ•°æ®åˆ†å¸ƒStandardScalarscipy.stats.rankdatalog transform np.log(1+x)raising to the power &lt; 1 np.sqrt(x + 2/3)drop outlierï¼ˆwinsorizationï¼Œspecify upper and lower boundï¼‰èžåˆä¸åŒé¢„å¤„ç†æ–¹æ³•å¾—åˆ°çš„ç‰¹å¾è®­ç»ƒä¸€ä¸ªæ¨¡åž‹æˆ–è€…æ¯ä¸€ç§ç‰¹å¾è®­ç»ƒå‡ºä¸€ä¸ªæ¨¡åž‹æœ€åŽåšæ¨¡åž‹èžåˆç‰¹å¾ç”Ÿæˆä¸»è¦ä¾æ®å…ˆéªŒç»éªŒä»¥åŠå¯¹æ•°æ®çš„æ·±åˆ»ç†è§£ä¾‹å¦‚ï¼Œæµ®ç‚¹æ•°çš„å°æ•°éƒ¨åˆ†å•ç‹¬æå–å‡ºæ¥ä½œä¸ºç‰¹å¾ç±»åˆ«æ•°æ®ä»¥åŠæœ‰åºç±»åˆ«æ•°æ®ç‰¹å¾é¢„å¤„ç†Label encoding (tree(or non-tree)-based model)alphabetical sorted sklearn.preprocessing.LabelEncoderorder of appearance Pandas.factorizefrequency encoding ï¼ˆéžå¸¸é€‚ç”¨äºŽæµ‹è¯•æ•°æ®ä¸­åŒ…å«è®­ç»ƒæ•°æ®æœªåŒ…å«çš„ç±»åˆ«ï¼‰Label encoding (non-tree(or tree)-based model)one-hot encoding (sparse matrix)ç‰¹å¾ç”Ÿæˆæžšä¸¾ä¸åŒçš„ç±»åˆ«ç‰¹å¾çš„ç»„åˆå½¢æˆæ–°çš„ç±»åˆ«ç‰¹å¾ (linear models and KNN)æ—¥æœŸæ•°æ®ä»¥åŠåæ ‡æ•°æ®æ—¥æœŸæ•°æ®ç‰¹å¾ç”Ÿæˆå‘¨æœŸæ€§æ•°æ®Day number in week, month, season, yearsecond, minute, secondè‡ªä»€ä¹ˆæ—¶å€™ä»¥æ¥é—®é¢˜æ— å…³ï¼Œ æ¯”å¦‚è‡ª1970å¹´1æœˆ1æ—¥ä»¥æ¥é—®é¢˜ç›¸å…³ï¼Œæ¯”å¦‚è·ç¦»ä¸‹ä¸€ä¸ªèŠ‚å‡æ—¥è¿˜æœ‰å¤šå°‘å¤©ç­‰ç­‰ä¸¤ä¸ªæ—¥æœŸç‰¹å¾ä¹‹é—´çš„å·®å€¼åæ ‡æ•°æ®ç‰¹å¾ç”Ÿæˆè·ç¦»æŸäº›å…³é”®åæ ‡çš„è·ç¦»ç­‰ç­‰ï¼ˆéœ€è¦å¤–éƒ¨æ•°æ®æ”¯æŒï¼‰å¯¹åæ ‡è¿›è¡Œç½‘æ ¼åŒ–æˆ–è€…èšç±»ï¼Œç„¶åŽè®¡ç®—æ¯ä¸ªç½‘æ ¼ä¸­çš„ç‚¹è·ç¦»é€‰å®šç‚¹çš„è·ç¦»æˆ–è€…æ¯ä¸ªç°‡ä¸­çš„ç‚¹è·ç¦»èšç±»ä¸­å¿ƒçš„è·ç¦»ç‚¹çš„å¯†åº¦ï¼ˆæŸä¸€é™å®šèŒƒå›´ä¹‹å†…ï¼‰åŒºåŸŸä»·å€¼ï¼Œä¾‹å¦‚ç‰©ä»·æˆ¿ä»·ç­‰ï¼ˆæŸä¸€é™å®šèŒƒå›´ä¹‹å†…ï¼‰ç‰¹å¾é¢„å¤„ç†åæ ‡æ—‹è½¬ï¼ˆä¾‹å¦‚45Â°ï¼‰ç¼ºå¤±å€¼å¤„ç†æ‰¾å‡ºéšå«çš„NaNï¼Œé€šè¿‡å¯è§†åŒ–æ•°æ®åˆ†å¸ƒå¡«å……æ–¹æ³•-999, -1ç­‰ä¸­å€¼ï¼Œå‡å€¼ç­‰å°è¯•æ¢å¤ç¼ºå¤±æ•°æ®ï¼ˆçº¿æ€§å›žå½’ï¼‰ç‰¹å¾ç”Ÿæˆå¢žåŠ ä¸€ä¸ªç‰¹å¾ï¼Œæ˜¯å¦æœ‰ç¼ºå¤±å€¼é‡‡ç”¨å¡«å……çš„ç¼ºå¤±å€¼è¿›è¡Œç‰¹å¾ç”Ÿæˆè¦ç‰¹åˆ«å°å¿ƒï¼Œä¸€èˆ¬æ¥è¯´è‹¥è¦è¿›è¡Œç‰¹å¾ç”Ÿæˆï¼Œåˆ™æœ€å¥½ä¸è¦åœ¨ä¹‹å‰è¿›è¡Œç¼ºå¤±å€¼å¡«å……xgboostå¯¹äºŽç¼ºå¤±å€¼ä¸æ•æ„Ÿæ–‡æœ¬æ•°æ®ç‰¹å¾ç”Ÿæˆè¯è¢‹ skearn.feature_extraction.text.CountVectorizerTF-IDF skearn.feature_extraction.text.TfidfVectorizerN-grams ngramç‰¹å¾é¢„å¤„ç†lowercaselemmatization (å•è¯æœ€åŽŸå§‹çš„å½¢å¼)stemmingstopwords nltkWord2Vec, Doc2vec, Glove, FastText, etcPipelineé¢„å¤„ç†Ngrams then TF-IDFor Word2Vec, etcå›¾åƒæ•°æ®å¯ä»¥ç»“åˆä¸åŒå±‚çš„ç‰¹å¾å›¾]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[About Feature Scaling and Normalization]]></title>
      <url>%2F2018%2F10%2F15%2FAbout-Feature-Scaling-and-Normalization%2F</url>
      <content type="text"><![CDATA[About standardizationThe result of standardization (or Z-score normalization) is that the features will be rescaled so that theyâ€™ll have the properties of a standard normal distribution with $\mu=0$ and $\sigma=1$where $\mu$ is the mean (average) and $\sigma$ is the standard deviation from the mean; standard scores (also called z scores) of the samples are calculated as follows:$$z = \frac{x - \mu}{\sigma}$$Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. Intuitively, we can think of gradient descent as a prominent example (an optimization algorithm often used in logistic regression, SVMs, perceptrons, neural networks etc.); with features being on different scales, certain weights may update faster than others since the feature values $x_j$ play a role in the weight updates$$\Delta w_j = - \eta \frac{\partial J}{\partial w_j} = \eta \sum_i (t^{(i)} - o^{(i)}) x_j^{(i)},$$so that$w_j := w_j + \Delta w_j$, where $\eta$ is the learning rate, $t$ the target class label, and $o$ the actual output. Other intuitive examples include K-Nearest Neighbor algorithms and clustering algorithms that use, for example, Euclidean distance measures â€“ in fact, tree-based classifier are probably the only classifiers where feature scaling doesnâ€™t make a difference.In fact, the only family of algorithms that I could think of being scale-invariant are tree-based methods. Letâ€™s take the general CART decision tree algorithm. Without going into much depth regarding information gain and impurity measures, we can think of the decision as â€œis feature x_i &gt;= some_val?â€ Intuitively, we can see that it really doesnâ€™t matter on which scale this feature is (centimeters, Fahrenheit, a standardized scale â€“ it really doesnâ€™t matter).Some examples of algorithms where feature scaling matters are:k-nearest neighbors with an Euclidean distance measure if want all features to contribute equallyk-means (see k-nearest neighbors)logistic regression, SVMs, perceptrons, neural networks etc. if you are using gradient descent/ascent-based optimization, otherwise some weights will update much faster than otherslinear discriminant analysis, principal component analysis, kernel principal component analysis since you want to find directions of maximizing the variance (under the constraints that those directions/eigenvectors/principal components are orthogonal); you want to have features on the same scale since youâ€™d emphasize variables on â€œlarger measurement scalesâ€ more. There are many more cases than I can possibly list here â€¦ I always recommend you to think about the algorithm and what itâ€™s doing, and then it typically becomes obvious whether we want to scale your features or not.In addition, weâ€™d also want to think about whether we want to â€œstandardizeâ€ or â€œnormalizeâ€ (here: scaling to [0, 1] range) our data. Some algorithms assume that our data is centered at 0. For example, if we initialize the weights of a small multi-layer perceptron with tanh activation units to 0 or small random values centered around zero, we want to update the model weights â€œequally.â€ As a rule of thumb Iâ€™d say: When in doubt, just standardize the data, it shouldnâ€™t hurt.About Min-Max scalingAn alternative approach to Z-score normalization (or standardization) is the so-called Min-Max scaling(often also simply called â€œnormalizationâ€ - a common cause for ambiguities).In this approach, the data is scaled to a fixed range - usually 0 to 1.The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.A Min-Max scaling is typically done via the following equation:$$X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}$$Z-score standardization or Min-Max scaling?â€œStandardization or Min-Max scaling?â€ - There is no obvious answer to this question: it really depends on the application.For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; but more about PCA in my previous article).However, this doesnâ€™t mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale.Standardizing and normalizing - how it can be done using scikit-learnOf course, we could make use of NumPyâ€™s vectorization capabilities to calculate the z-scores for standardization and to normalize the data using the equations that were mentioned in the previous sections. However, there is an even more convenient approach using the preprocessing module from one of Pythonâ€™s open-source machine learning library scikit-learn.For the following examples and discussion, we will have a look at the free â€œWineâ€ Dataset that is deposited on the UCI machine learning repository(http://archive.ics.uci.edu/ml/datasets/Wine).Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy.Bache, K. &amp; Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.The Wine dataset consists of 3 different classes where each row correspond to a particular wine sample.The class labels (1, 2, 3) are listed in the first column, and the columns 2-14 correspond to 13 different attributes (features):1) Alcohol2) Malic acidâ€¦Loading the wine dataset123456789101112import pandas as pdimport numpy as npdf = pd.io.parsers.read_csv( 'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', header=None, usecols=[0,1,2] )df.columns=['Class label', 'Alcohol', 'Malic acid']df.head()Class labelAlcoholMalic acid0114.231.711113.201.782113.162.363114.371.954113.242.59As we can see in the table above, the features Alcohol (percent/volumne) and Malic acid (g/l) are measured on different scales, so that Feature Scaling is necessary important prior to any comparison or combination of these data.Standardization and Min-Max scaling1234567from sklearn import preprocessingstd_scale = preprocessing.StandardScaler().fit(df[['Alcohol', 'Malic acid']])df_std = std_scale.transform(df[['Alcohol', 'Malic acid']])minmax_scale = preprocessing.MinMaxScaler().fit(df[['Alcohol', 'Malic acid']])df_minmax = minmax_scale.transform(df[['Alcohol', 'Malic acid']])1234print('Mean after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_std[:,0].mean(), df_std[:,1].mean()))print('\nStandard deviation after standardization:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_std[:,0].std(), df_std[:,1].std()))12345Mean after standardization:Alcohol=0.00, Malic acid=0.00Standard deviation after standardization:Alcohol=1.00, Malic acid=1.001234print('Min-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_minmax[:,0].min(), df_minmax[:,1].min()))print('\nMax-value after min-max scaling:\nAlcohol=&#123;:.2f&#125;, Malic acid=&#123;:.2f&#125;' .format(df_minmax[:,0].max(), df_minmax[:,1].max()))12345Min-value after min-max scaling:Alcohol=0.00, Malic acid=0.00Max-value after min-max scaling:Alcohol=1.00, Malic acid=1.00Plotting1%matplotlib inline123456789101112131415161718192021222324from matplotlib import pyplot as pltdef plot(): plt.figure(figsize=(8,6)) plt.scatter(df['Alcohol'], df['Malic acid'], color='green', label='input scale', alpha=0.5) plt.scatter(df_std[:,0], df_std[:,1], color='red', label='Standardized [$$N (\mu=0, \; \sigma=1)$$]', alpha=0.3) plt.scatter(df_minmax[:,0], df_minmax[:,1], color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3) plt.title('Alcohol and Malic Acid content of the wine dataset') plt.xlabel('Alcohol') plt.ylabel('Malic Acid') plt.legend(loc='upper left') plt.grid() plt.tight_layout()plot()plt.show()The plot above includes the wine datapoints on all three different scales: the input scale where the alcohol content was measured in volume-percent (green), the standardized features (red), and the normalized features (blue). In the following plot, we will zoom in into the three different axis-scales.123456789101112131415161718192021222324fig, ax = plt.subplots(3, figsize=(6,14))for a,d,l in zip(range(len(ax)), (df[['Alcohol', 'Malic acid']].values, df_std, df_minmax), ('Input scale', 'Standardized [$$N (\mu=0, \; \sigma=1)$$]', 'min-max scaled [min=0, max=1]') ): for i,c in zip(range(1,4), ('red', 'blue', 'green')): ax[a].scatter(d[df['Class label'].values == i, 0], d[df['Class label'].values == i, 1], alpha=0.5, color=c, label='Class %s' %i ) ax[a].set_title(l) ax[a].set_xlabel('Alcohol') ax[a].set_ylabel('Malic Acid') ax[a].legend(loc='upper left') ax[a].grid()plt.tight_layout()plt.show()Bottom-up approachesOf course, we can also code the equations for standardization and 0-1 Min-Max scaling â€œmanuallyâ€. However, the scikit-learn methods are still useful if you are working with test and training data sets and want to scale them equally.E.g.,123std_scale = preprocessing.StandardScaler().fit(X_train)X_train = std_scale.transform(X_train)X_test = std_scale.transform(X_test)Below, we will perform the calculations using â€œpureâ€ Python code, and an more convenient NumPy solution, which is especially useful if we attempt to transform a whole matrix.Vanilla Python1234567891011# Standardizationx = [1,4,5,6,6,2,3]mean = sum(x)/len(x)std_dev = (1/len(x) * sum([ (x_i - mean)**2 for x_i in x]))**0.5z_scores = [(x_i - mean)/std_dev for x_i in x]# Min-Max scalingminmax = [(x_i - min(x)) / (max(x) - min(x)) for x_i in x]NumPy12345678910import numpy as np# Standardizationx_np = np.asarray(x)z_scores_np = (x_np - x_np.mean()) / x_np.std()# Min-Max scalingnp_minmax = (x_np - x_np.min()) / (x_np.max() - x_np.min())VisualizationJust to make sure that our code works correctly, let us plot the results via matplotlib.12345678910111213141516171819202122232425from matplotlib import pyplot as pltfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,5))y_pos = [0 for i in range(len(x))]ax1.scatter(z_scores, y_pos, color='g')ax1.set_title('Python standardization', color='g')ax2.scatter(minmax, y_pos, color='g')ax2.set_title('Python Min-Max scaling', color='g')ax3.scatter(z_scores_np, y_pos, color='b')ax3.set_title('Python NumPy standardization', color='b')ax4.scatter(np_minmax, y_pos, color='b')ax4.set_title('Python NumPy Min-Max scaling', color='b')plt.tight_layout()for ax in (ax1, ax2, ax3, ax4): ax.get_yaxis().set_visible(False) ax.grid()plt.show()The effect of standardization on PCA in a pattern classification taskEarlier, I mentioned the Principal Component Analysis (PCA) as an example where standardization is crucial, since it is â€œanalyzingâ€ the variances of the different features.Now, let us see how the standardization affects PCA and a following supervised classification on the whole wine dataset.In the following section, we will go through the following steps:Reading in the datasetDividing the dataset into a separate training and test datasetStandardization of the featuresPrincipal Component Analysis (PCA) to reduce the dimensionalityTraining a naive Bayes classifierEvaluating the classification accuracy with and without standardizationReading in the dataset123456import pandas as pddf = pd.io.parsers.read_csv( 'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', header=None, )Dividing the dataset into a separate training and test datasetIn this step, we will randomly divide the wine dataset into a training dataset and a test dataset where the training dataset will contain 70% of the samples and the test dataset will contain 30%, respectively.1234567from sklearn.cross_validation import train_test_splitX_wine = df.values[:,1:]y_wine = df.values[:,0]X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, test_size=0.30, random_state=12345)Feature Scaling - Standardization12345from sklearn import preprocessingstd_scale = preprocessing.StandardScaler().fit(X_train)X_train_std = std_scale.transform(X_train)X_test_std = std_scale.transform(X_test)Dimensionality reduction via Principal Component Analysis (PCA)Now, we perform a PCA on the standardized and the non-standardized datasets to transform the dataset onto a 2-dimensional feature subspace.In a real application, a procedure like cross-validation would be done in order to find out what choice of features would yield a optimal balance between â€œpreserving informationâ€ and â€œoverfittingâ€ for different classifiers. However, we will omit this step since we donâ€™t want to train a perfect classifier here, but merely compare the effects of standardization.123456789101112from sklearn.decomposition import PCA# on non-standardized datapca = PCA(n_components=2).fit(X_train)X_train = pca.transform(X_train)X_test = pca.transform(X_test)# om standardized datapca_std = PCA(n_components=2).fit(X_train_std)X_train_std = pca_std.transform(X_train_std)X_test_std = pca_std.transform(X_test_std)Let us quickly visualize how our new feature subspace looks like (note that class labels are not considered in a PCA - in contrast to a Linear Discriminant Analysis - but I will add them in the plot for clarity).123456789101112131415161718192021222324252627282930313233from matplotlib import pyplot as pltfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,4))for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')): ax1.scatter(X_train[y_train==l, 0], X_train[y_train==l, 1], color=c, label='class %s' %l, alpha=0.5, marker=m )for l,c,m in zip(range(1,4), ('blue', 'red', 'green'), ('^', 's', 'o')): ax2.scatter(X_train_std[y_train==l, 0], X_train_std[y_train==l, 1], color=c, label='class %s' %l, alpha=0.5, marker=m )ax1.set_title('Transformed NON-standardized training dataset after PCA') ax2.set_title('Transformed standardized training dataset after PCA') for ax in (ax1, ax2): ax.set_xlabel('1st principal component') ax.set_ylabel('2nd principal component') ax.legend(loc='upper right') ax.grid()plt.tight_layout()plt.show()Training a naive Bayes classifierWe will use a naive Bayes classifier for the classification task. If you are not familiar with it, the term â€œnaiveâ€ comes from the assumption that all features are â€œindependentâ€.All in all, it is a simple but robust classifier based on Bayesâ€™ ruleI donâ€™t want to get into more detail about Bayesâ€™ rule in this article, but if you are interested in a more detailed collection of examples, please have a look at the Statistical Patter Classification in my pattern classification repository.123456789from sklearn.naive_bayes import GaussianNB# on non-standardized datagnb = GaussianNB()fit = gnb.fit(X_train, y_train)# on standardized datagnb_std = GaussianNB()fit_std = gnb_std.fit(X_train_std, y_train)Evaluating the classification accuracy with and without standardization1234567891011from sklearn import metricspred_train = gnb.predict(X_train)print('\nPrediction accuracy for the training dataset')print('&#123;:.2%&#125;'.format(metrics.accuracy_score(y_train, pred_train)))pred_test = gnb.predict(X_test)print('\nPrediction accuracy for the test dataset')print('&#123;:.2%&#125;\n'.format(metrics.accuracy_score(y_test, pred_test)))12345Prediction accuracy for the training dataset81.45%Prediction accuracy for the test dataset64.81%123456789pred_train_std = gnb_std.predict(X_train_std)print('\nPrediction accuracy for the training dataset')print('&#123;:.2%&#125;'.format(metrics.accuracy_score(y_train, pred_train_std)))pred_test_std = gnb_std.predict(X_test_std)print('\nPrediction accuracy for the test dataset')print('&#123;:.2%&#125;\n'.format(metrics.accuracy_score(y_test, pred_test_std)))12345Prediction accuracy for the training dataset96.77%Prediction accuracy for the test dataset98.15%As we can see, the standardization prior to the PCA definitely led to an decrease in the empirical error rate on classifying samples from test dataset.Appendix A: The effect of scaling and mean centering of variables prior to PCALet us think about whether it matters or not if the variables are centered for applications such as Principal Component Analysis (PCA) if the PCA is calculated from the covariance matrix (i.e., the kkprincipal components are the eigenvectors of the covariance matrix that correspond to the kk largest eigenvalues.1. Mean centering does not affect the covariance matrixHere, the rational is: If the covariance is the same whether the variables are centered or not, the result of the PCA will be the same.Letâ€™s assume we have the 2 variables $x$ and $y$ Then the covariance between the attributes is calculated as$$\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})$$Let us write the centered variables as$$xâ€™ = x - \bar{x} \text{ and } yâ€™ = y - \bar{y}$$The centered covariance would then be calculated as follows:$$\sigma_{xy}â€™ = \frac{1}{n-1} \sum_{i}^{n} (x_iâ€™ - \bar{x}â€™)(y_iâ€™ - \bar{y}â€™)$$But since after centering, $\bar{x}â€™ = 0$ and $\bar{y}â€™ = 0$ we have$\sigma_{xy}â€™ = \frac{1}{n-1} \sum_{i}^{n} x_iâ€™ y_iâ€™$ which is our original covariance matrix if we resubstitute back the terms $xâ€™ = x - \bar{x} \text{ and } yâ€™ = y - \bar{y}$.Even centering only one variable, e.g., xx wouldnâ€™t affect the covariance:$$\sigma_{\text{xy}} = \frac{1}{n-1} \sum_{i}^{n} (x_iâ€™ - \bar{x}â€™)(y_i - \bar{y})$$2. Scaling of variables does affect the covariance matrixIf one variable is scaled, e.g, from pounds into kilogram (1 pound = 0.453592 kg), it does affect the covariance and therefore influences the results of a PCA.Let cc be the scaling factor for $x$Given that the â€œoriginalâ€ covariance is calculated as$$\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})$$the covariance after scaling would be calculated as:$$\sigma_{xy}â€™ = \frac{1}{n-1} \sum_{i}^{n} (c \cdot x_i - c \cdot \bar{x})(y_i - \bar{y}) = \frac{c}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y}) \Rightarrow \sigma_{xy}â€™ = c \cdot \sigma_{xy}$$Therefore, the covariance after scaling one attribute by the constant $c$ will result in a rescaled covariance $c \sigma_{xy}$ So if weâ€™d scaled $x$ from pounds to kilograms, the covariance between $x$ and $y$ will be 0.453592 times smaller.3. Standardizing affects the covarianceStandardization of features will have an effect on the outcome of a PCA (assuming that the variables are originally not standardized). This is because we are scaling the covariance between every pair of variables by the product of the standard deviations of each pair of variables.The equation for standardization of a variable is written as$$z = \frac{x_i - \bar{x}}{\sigma}$$The â€œoriginalâ€ covariance matrix:$$\sigma_{xy} = \frac{1}{n-1} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})$$And after standardizing both variables:$$xâ€™ = \frac{x - \bar{x}}{\sigma_x} \text{ and } yâ€™ =\frac{y - \bar{y}}{\sigma_y}$$$$\sigma_{xy}â€™ = \frac{1}{n-1} \sum_{i}^{n} (x_iâ€™ - 0)(y_iâ€™ - 0) = \frac{1}{n-1} \sum_{i}^{n} \bigg(\frac{x - \bar{x}}{\sigma_x}\bigg)\bigg(\frac{y - \bar{y}}{\sigma_y}\bigg) = \frac{1}{(n-1) \cdot \sigma_x \sigma_y} \sum_{i}^{n} (x_i - \bar{x})(y_i - \bar{y})$$$$\Rightarrow \sigma_{xy}â€™ = \frac{\sigma_{xy}}{\sigma_x \sigma_y}$$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Install SerpentAI on Windows 10]]></title>
      <url>%2F2018%2F06%2F07%2FInstall-SerpentAI-on-Windows-10%2F</url>
      <content type="text"><![CDATA[Python EnvironmentPython 3.6+ (with Anaconda)Serpent.AI was developed taking full advantage of Python 3.6 so it is only natural that the Python requirement be for versions 3.6 and up.Installing regular Python 3.6+ isnâ€™t exactly difficult but Serpent.AI relies on a good amount of scientific computing libraries that are extremely difficult / impossible to compile on your own on Windows. Thankfully, the Anaconda Distribution exists and takes this huge weight off our collective shoulders.Installing Anaconda 5.2.0 (Python 3.6)Download the Python 3.6 version of Anaconda 5.2.0 and run the graphical installer.The following commands are to be performed in an Anaconda Prompt with elevated privileges (Right click and Run as Administrator). It is recommended to create a shortcut to this prompt because every Python and Serpent command will have to be performed from there starting now.Creating a Conda Env for Serpent.AIconda create --name serpent python=3.6 (â€˜serpentâ€™ can be replaced with another name)Creating a directory for your Serpent.AI projectsmkdir SerpentAI &amp;&amp; cd SerpentAIActivating the Conda Envconda activate serpent3rd-Party DependenciesRedisRedis is used in the framework as the in-memory store for the captured frame buffers as well as the temporary storage of analytics events. It is not meant to be compatible with Windows! Microsoft used to maintain a port but itâ€™s been abandoned since. This being said, that Redis version is sufficient and it outperforms stuff like running it in WSL on Windows 10. It will install as a Windows Service. Make sure you set it to start automatically.Install Windows Subsystem for Linux (WSL)From Start, search for Turn Windows features on or off (type turn)Select Windows Subsystem for Linux (beta)Once installed you can run bash on Ubuntu by typing bash from a Windows Command Prompt. To install the latest version of Redis weâ€™ll need to use a repository that maintains up-to-date packages for Ubuntu and Debian servers like https://www.dotdeb.org which you can add to Ubuntuâ€™s apt-get sources with:1234$ echo deb http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list$ echo deb-src http://packages.dotdeb.org wheezy all &gt;&gt; dotdeb.org.list$ sudo mv dotdeb.org.list /etc/apt/sources.list.d$ wget -q -O - http://www.dotdeb.org/dotdeb.gpg | sudo apt-key add -Then after updating our APT cache we can install Redis with:12$ sudo apt-get update$ sudo apt-get install redis-serverYouâ€™ll then be able to launch redis with:1$ redis-server --daemonize yesWhich will run redis in the background freeing your shell so you can play with it using the redis client:12345$ redis-cli$ 127.0.0.1:6379&gt; SET foo barOK$ 127.0.0.1:6379&gt; GET foo&quot;bar&quot;Which you can connect to from within bash or from your Windows desktop using the redis-cli native Windows binary from MSOpenTech.Build Tools for Visual Studio 2017Some of the packages that will be installed alongside Serpent.AI are not pre-compiled binaries and will be need to be built from source. This is a little more problematic for Windows but with the correct C++ Build Tools for Visual Studio it all goes down smoothly.You can get the proper installer by visiting https://www.visualstudio.com/downloads/ and scrolling down to the Build Tools for Visual Studio 2017 download. Download, run, select the Visual C++ build tools section and make sure the following components are checked (VSs are not installed):Visual C++ Build Tools core featuresVC++ 2017 version 15.7 v14.14 latest v141 toolsVisual C++ 2017 Redistributable UpdateVC++ 2015.3 v14.00 (v140) toolset for desktopWindows 10 SDK (10.0.17134.0)Windows Universal CRT SDKInstalling Serpent.AIOnce all of the above had been installed and set up, you are ready to install the framework. Remember that PATH changes in Windows are not reflected in your command prompts that were opened while you made the changes. Open a fresh Anaconda prompt before continuing to avoid installation issues.Go back to the directory you created earlier for your Serpent.AI projects. Make sure you are scoped in your Conda Env.Run pip install SerpentAIThen run serpent setup to install the remaining dependencies automatically.Installing Optional ModulesIn the spirit of keeping the initial installation on the light side, some specialized / niche components with extra dependencies have been isolated from the core. It is recommended to only focus on installing them once you reach a point where you actually need them. The framework will provide a warning when a feature you are trying to use requires one of those modules.OCRA module to provide OCR functionality in your game agents.TesseractSerpent.AI leverages Tesseract for its OCR functionality. You can install Tesseract for Windows by following these steps:Visit https://github.com/UB-Mannheim/tesseract/wikiDownload the .exe for version 3Run the graphical installer (Remember the install path!)Add the path to tesseract.exe to your %PATH% environment variableYou can test your Tesseract installation by opening an Anaconda Prompt and executing tesseract --list-langs.InstallationOnce youâ€™ve validated that Tesseract has been properly set up, you can install the module with serpent setup ocrGUIA module to allow Serpent.AI desktop app to run.KivyKivy is the GUI framework used in the framework.Once you are ready to test your Kivy, you can install the module with serpent setup gui and try to run serpent visual_debugger]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Matching Networks for One Shot Learning]]></title>
      <url>%2F2018%2F06%2F02%2FMatching-Networks-for-One-Shot-Learning%2F</url>
      <content type="text"><![CDATA[By DeepMind crew: Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan WierstraThis is a paper on one-shot learning, where weâ€™d like to learn a class based on very few (or indeed, 1) training examples. E.g. it suffices to show a child a single giraffe, not a few hundred thousands before it can recognize more giraffes.This paper falls into a category of â€œduh of courseâ€ kind of paper, something very interesting, powerful, but somehow obvious only in retrospect. I like it.Suppose youâ€™re given a single example of some class and would like to label it in test images.Observation 1: a standard approach might be to train an Exemplar SVM for this one (or few) examples vs. all the other training examples - i.e. a linear classifier. But this requires optimization.Observation 2: known non-parameteric alternatives (e.g. k-Nearest Neighbor) donâ€™t suffer from this problem. E.g. I could immediately use a Nearest Neighbor to classify the new class without having to do any optimization whatsoever. However, NN is gross because it depends on an (arbitrarily-chosen) metric, e.g. L2 distance. Ew.Core idea: lets train a fully end-to-end nearest neighbor classifer!The training protocolAs the authors amusingly point out in the conclusion (and this is the duh of course part), â€œone-shot learning is much easier if you train the network to do one-shot learningâ€. Therefore, we want the test-time protocol (given N novel classes with only k examples each (e.g. k = 1 or 5), predict new instances to one of N classes) to exactly match the training time protocol.To create each â€œepisodeâ€ of training from a dataset of examples then:Sample a task T from the training data, e.g. select 5 labels, and up to 5 examples per label (i.e. 5-25 examples).To form one episode sample a label set L (e.g. {cats, dogs}) and then use L to sample the support set S and a batch B of examples to evaluate loss on.The idea on high level is clear but the writing here is a bit unclear on details, of exactly how the sampling is done.The modelI find the paperâ€™s model description slightly wordy and unclear, but basically weâ€™re building a differentiable nearest neighbor++. The output \hat{y} for a test example \hat{x} is computed very similar to what you might see in Nearest Neighbors:where a acts as a kernel, computing the extent to which \hat{x} is similar to a training example x_i, and then the labels from the training examples (y_i) are weight-blended together accordingly. The paper doesnâ€™t mention this but I assume for classification y_i would presumbly be one-hot vectors.Now, weâ€™re going to embed both the training examples x_i and the test example \hat{x}, and weâ€™ll interpret their inner products (or here a cosine similarity) as the â€œmatchâ€, and pass that through a softmax to get normalized mixing weights so they add up to 1. No surprises here, this is quite natural:Here c() is cosine distance, which I presume is implemented by normalizing the two input vectors to have unit L2 norm and taking a dot product. I assume the authors tried skipping the normalization too and it did worse? Anyway, now all thatâ€™s left to define is the function f (i.e. how do we embed the test example into a vector) and the function g (i.e. how do we embed each training example into a vector?).Embedding the training examples. This (the function g) is a bidirectional LSTM over the examples:i.e. encoding of iâ€™th example x_i is a function of its â€œrawâ€ embedding gâ€™(x_i) and the embedding of its friends, communicated through the bidirectional networkâ€™s hidden states. i.e. each training example is a function of not just itself but all of its friends in the set. This is part of the ++ above, because in a normal nearest neighbor you wouldnâ€™t change the representation of an example as a function of the other data points in the training set.Itâ€™s odd that the order is not mentioned, I assume itâ€™s random? This is a bit gross because order matters to a bidirectional LSTM; youâ€™d get different embeddings if you permute the examples.Embedding the test example. This (the function f) is a an LSTM that processes for a fixed amount (K time steps) and at each point also attends over the examples in the training set. The encoding is the last hidden state of the LSTM. Again, this way weâ€™re allowing the network to change its encoding of the test example as a function of the training examples. Nifty: That looks scary at first but itâ€™s really just a vanilla LSTM with attention where the input at each time step is constant (fâ€™(\hat{x}), an encoding of the test example all by itself) and the hidden state is a function of previous hidden state but also a concatenated readout vector r, which we obtain by attending over the encoded training examples (encoded with g from above).Oh and I assume there is a typo in equation (5), it should say r_k = â€¦ without the -1 on LHS.ExperimentsTask: N-way k-shot learning task. i.e. weâ€™re given k (e.g. 1 or 5) labelled examples for N classes that we have not previously trained on and asked to classify new instances into he N classes.Baselines: an â€œobviousâ€ strategy of using a pretrained ConvNet and doing nearest neighbor based on the codes. An option of finetuning the network on the new examples as well (requires training and careful and strong regularization!).MANN of Santoro et al. [21]: Also a DeepMind paper, a fun NTM-like Meta-Learning approach that is fed a sequence of examples and asked to predict their labels.Siamese network of Koch et al. [11]: A siamese network that takes two examples and predicts whether they are from the same class or not with logistic regression. A test example is labeled with a nearest neighbor: with the class it matches best according to the siamese net (requires iteration over all training examples one by one). Also, this approach is less end-to-end than the one here because it requires the ad-hoc nearest neighbor matching, while here the exact end task is optimized for. Itâ€™s beautiful.Omniglot experimentsOmniglot of Lake et al. [14] is a MNIST-like scribbles dataset with 1623 characters with 20 examples each.Image encoder is a CNN with 4 modules of [3x3 CONV 64 filters, batchnorm, ReLU, 2x2 max pool]. The original image is claimed to be so resized from original 28x28 to 1x1x64, which doesnâ€™t make sense because factor of 2 downsampling 4 times is reduction of 16, and 28/16 is a non-integer &gt;1. Iâ€™m assuming they use VALID convs?Results: Matching nets do best. Fully Conditional Embeddings (FCE) by which I mean they the â€œFull Context Embeddingsâ€ of Section 2.1.2 instead are not used here, mentioned to not work much better. Finetuning helps a bit on baselines but not with Matching nets (weird).The comparisons in this table are somewhat confusing:I canâ€™t find the MANN numbers of 82.8% and 94.9% in their paper [21]; not clear where they come from. E.g. for 5 classes and 5-shot they seem to report 88.4% not 94.9% as seen here. I must be missing something.I also canâ€™t find the numbers reported here in the Siamese Net [11] paper. As far as I can tell in their Table 2 they report one-shot accuracy, 20-way classification to be 92.0, while here it is listed as 88.1%?The results of Lake et al. [14] who proposed Omniglot are also missing from the table. If Iâ€™m understanding this correctly they report 95.2% on 1-shot 20-way, while matching nets here show 93.8%, and humans are estimated at 95.5%. That is, the results here appear weaker than those of Lake et al., but one should keep in mind that the method here is significantly more generic and does not make any assumptions about the existence of strokes, etc., and itâ€™s a simple, single fully-differentiable blob of neural stuff.(skipping ImageNet/LM experiments as there are few surprises)ConclusionsGood paper, effectively develops a differentiable nearest neighbor trained end-to-end. Itâ€™s something new, I like it!A few concerns:A bidirectional LSTMs (not order-invariant compute) is applied over sets of training examples to encode them. The authors donâ€™t talk about the order actually used, which presumably is random, or mention this potentially unsatisfying feature. This can be solved by using a recurrent attentional mechanism instead, as the authors are certainly aware of and as has been discussed at length in ORDER MATTERS: SEQUENCE TO SEQUENCE FOR SETS, where Oriol is also the first author. I wish there was a comment on this point in the paper somewhere.The approach also gets quite a bit slower as the number of training examples grow, but once this number is large one would presumable switch over to a parameteric approach.Itâ€™s also potentially concerning that during training the method uses a specific number of examples, e.g. 5-25, so this is the number of that must also be used at test time. What happens if we want the size of our training set to grow online? It appears that we need to retrain the network because the encoder LSTM for the training data is not â€œused toâ€ seeing inputs of more examples? That is unless you fall back to iteratively subsampling the training data, doing multiple inference passes and averaging, or something like that. If we donâ€™t use FCE it can still be that the attention mechanism LSTM can still not be â€œused toâ€ attending over many more examples, but itâ€™s not clear how much this matters. An interesting experiment would be to not use FCE and try to use 100 or 1000 training examples, while only training on up to 25 (with and fithout FCE). Discussion surrounding this point would be interesting.Not clear what happened with the Omniglot experiments, with incorrect numbers for [11], [21], and the exclusion of Lake et al. [14] comparison.A baseline that is missing would in my opinion also include training of an Exemplar SVM, which is a much more powerful approach than encode-with-a-cnn-and-nearest-neighbor.â€‹]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[One Shot Learning and Siamese Networks in Keras]]></title>
      <url>%2F2018%2F06%2F02%2FOne-Shot-Learning-and-Siamese-Networks-in-Keras%2F</url>
      <content type="text"><![CDATA[Background:Conventional wisdom says that deep neural networks are really good at learning from high dimensional data like images or spoken language, but only when they have huge amounts of labelled examples to train on. Humans on the other hand, are capable of one-shot learning - if you take a human whoâ€™s never seen a spatula before, and show them a single picture of a spatula, they will probably be able to distinguish spatulas from other kitchen utensils with astoundingly high precision.Never been inside a kitchen before? Nowâ€™s your chance to test your one shot learning ability! which of the images on the right is of the same type as the big image? Email me for the correct answer...Yet another one of the things humans can do that seemed trivial to us right up until we tried to make an algorithm do it.This ability to rapidly learn from very little data seems like itâ€™s obviously desirable for machine learning systems to have because collecting and labelling data is expensive. I also think this is an important step on the long road towards general intelligence.Recently there have been many interesting papers about one-shot learning with neural nets and theyâ€™ve gotten some good results. This is a new area that really excites me, so I wanted to make a gentle introduction to make it more accessible to fellow newcomers to deep learning.In this post, I want to:Introduce and formulate the problem of one-shot learningDescribe benchmarks for one-shot classification and give a baseline for performanceGive an example of deep one-shot learning by partially reimplementing the model in this paper with keras.Hopefully point out some small insights that arenâ€™t obvious to everyoneFormulating the Problem - N-way One-Shot LearningBefore we try to solve any problem, we should first precisely state what the problem actually is, so here is the problem of one-shot classification expressed symbolically:Our model is given a tiny labelled training set SS, which has N examples, each vectors of the same dimension with a distinct label yy.$$S={(x_1,y_1),â€¦,(x_N,y_N)}$$It is also given $\hat{x}$, the test example it has to classify. Since exactly one example in the support set has the right class, the aim is to correctly predict which $y \in S$ is the same as $\hat{x}$ â€˜s label, $\hat{y}$.There are fancier ways of defining the problem, but this one is ours. Here are some things to make note of:Real world problems might not always have the constraint that exactly one image has the correct classItâ€™s easy to generalize this to k-shot learning by having there be k examples for each yiyirather than just one.When N is higher, there are more possible classes that $\hat{x}$ can belong to, so itâ€™s harder to predict the correct one.Random guessing will average $\frac{100}{n}\%$ accuracy.Here are some examples of one-shot learning tasks on the Omniglot dataset, which Iâ€™ll describe in the next section.9, 25 and 36 way one-shot learnng tasks.About the data - Omniglot! :The Omniglot dataset is a collection of 1623 hand drawn characters from 50 alphabets. For every character there are just 20 examples, each drawn by a different person at resolution 105x105.A few of the alphabets from the omniglot dataset. As you can see, thereâ€™s a huge variety of different symbols.If you like machine learning, youâ€™ve probably heard of the MNIST dataset. Omniglot is sometimes referred to as the transpose of mnist, since it has 1623 types of character with only 20 examples each, in contrast to MNIST having thousands of examples for only 10 digits. There is also data about the strokes used to create each character, but we wonâ€™t be using that. Usually, itâ€™s split into 30 training alphabets and 20 evaluation alphabets. All those different characters make for lots of possible one-shot tasks, so itâ€™s a really good benchmark for one-shot learning algorithms.A One-Shot Learning Baseline / 1 Nearest NeighbourThe simplest way of doing classification is with k-nearest neighbours, but since there is only one example per class we have to do 1 nearest neighbour. This is very simple, just calculate the Euclidean distance of the test example from each training example and pick the closest one:$$C(\hat{x})={\arg \min}_{c\in S}||\hat{x} âˆ’ x_c||$$According to Koch et al, 1-nn gets ~28% accuracy in 20 way one shot classification on omniglot. 28% doesnâ€™t sound great, but itâ€™s nearly six times more accurate than random guessing(5%). This is a good baseline or â€œsanity checkâ€ to compare future one-shot algorithms with.Hierarchical Bayesian Program Learning from Lake et al gets 95.2% - very impressive! The ~30% of this paper which I understood was very interesting. Comparing it with deep learning results that train on raw pixels is kind of â€œapples and orangesâ€ though, because:HBPL used data about the strokes, not just the raw pixelsHBPL on omniglot involved learning a generative model for strokes. The algorithm requires data with more complicated annotation, so unlike deep learning it canâ€™t easily be tweaked to one-shot learn from raw pixels of dogs/trucks/brain scans/spatulas and other objects that arenâ€™t made up of brushstrokes.Lake et al also says that humans get 95.5% accuracy in 20 way classification on omniglot, only beating HBPL by a tiny margin. In the spirit of nullius in verba, I tried testing myself on the 20 way tasks and managed to average 97.2%. I wasnâ€™t always doing true one-shot learning though - I saw several symbols I recognised, since Iâ€™m familiar with the greek alphabet, hiragana and katakana. I removed those alphabets and tried again but still managed 96.7%. My hypothesis is that having to read my own terrible handwriting has endowed me with superhuman symbol recognition ability.Ways to use deep networks for one shot learning?!If we naively train a neural network on a one-shot as a vanilla cross-entropy-loss softmax classifier, it will severely overfit. Heck, even if it was a hundred shot learning a modern neural net would still probably overfit. Big neural networks have millions of parameters to adjust to their data and so they can learn a huge space of possible functions. (More formally, they have a high VC dimension, which is part of why they do so well at learning from complex data with high dimensionality.) Unfortunately this strength also appears to be their undoing for one-shot learning. When there are millions of parameters to gradient descend upon, and a staggeringly huge number of possible mappings that can be learned, how can we make a network learn one that generalizes when thereâ€™s just a single example to learn from?Itâ€™s easier for humans to one-shot learn the concept of a spatula or the letter Î˜Î˜ because they have spent a lifetime observing and learning from similar objects. Itâ€™s not really fair to compare the performance of a human whoâ€™s spent a lifetime having to classify objects and symbols with that of a randomly initialized neural net, which imposes a very weak prior about the structure of the mapping to be learned from the data. This is why most of the one-shot learning papers Iâ€™ve seen take the approach of knowledge transfer from other tasks.Neural nets are really good at extracting useful features from structurally complex/high dimensional data, such as images. If a neural network is given training data that is similar to (but not the same as) that in the one-shot task, it might be able to learn useful features which can be used in a simple learning algorithm that doesnâ€™t require adjusting these parameters. It still counts as one-shot learning as long as the training examples are of different classes to the examples used for one-shot testing.(NOTE: Here a feature means a â€œtransformation of the data that is useful for learningâ€.)So now an interesting problem is how do we get a neural network to learn the features? The most obvious way of doing this (if thereâ€™s labelled data) is just vanilla transfer learning - train a softmax classifier on the training set, then fine-tune the weights of the last layer on the support set of the one-shot task. In practice, neural net classifiers donâ€™t work too well for data like omniglot where there are few examples per class, and even fine tuning only the weights in the last layer is enough to overfit the support set. Still works quite a lot better than L2 distance nearest neighbour though! (See Matching Networks for One Shot learning for a comparison table of various deep one-shot learning methods and their accuracy.)Thereâ€™s a better way of doing it though! Remember 1 nearest neighbour? This simple, non-parametric one-shot learner just classifies the test example with the same class of whatever support example is the closest in L2 distance. This works ok, but L2 Distance suffers from the ominous sounding curse of dimensionality and so wonâ€™t work well for data with thousands of dimensions like omniglot. Also, if you have two nearly identical images and move one over a few pixels to the right the L2 distance can go from being almost zero to being really high. L2 distance is a metric that is just woefully inadequate for this task. Deep learning to the rescue? We can use a deep convolutional network to learn some kind of similarity function that a non-parametric classifer like nearest neighbor can use.Siamese networksI originally planned to have craniopagus conjoined twins as the accompanying image for this section but ultimately decided that siamese cats would go over better..This wonderful paper is what I will be implementing in this tutorial. Koch et alâ€™s approach to getting a neural net to do one-shot classification is to give it two images and train it to guess whether they have the same category. Then when doing a one-shot classification task described above, the network can compare the test image to each image in the support set, and pick which one it thinks is most likely to be of the same category. So we want a neural net architecture that takes two images as input and outputs the probability they share the same class.Say x1 and x2 are two images in our dataset, and let x1âˆ˜x2 mean â€œx1 and x2 are images with the same classâ€. Note that x1âˆ˜x2 is the same as x2âˆ˜x1 - this means that if we reverse the order of the inputs to the neural network, the output should be the same - p(x1âˆ˜x2) should equal p(x2âˆ˜x1). This property is called symmetry and siamese nets are designed around having it.Symmetry is important because itâ€™s required for learning a distance metric - the distance from x1 to x2 should equal the distance x2 to x1.If we just concatenate two examples together and use them as a single input to a neural net, each example will be matrix multiplied(or convolved) with a different set of weights, which breaks symmetry. Sure itâ€™s possible it will eventually manage to learn the exact same weights for each input, but it would be much easier to learn a single set of weights applied to both inputs. So we could propagate both inputs through identical twin neural nets with shared parameters, then use the absolute difference as the input to a linear classifier - this is essentially what a siamese net is. Two identical twins, joined at the head, hence the name.Network architectureUnfortunately, properly explaining how and why a convolutional neural net work would make this post twice as long. If you want to understand convnets work, I suggest checking out cs231n and then colah. For any non-dl people who are reading this, the best summary I can give of a CNN is this: An image is a 3D array of pixels. A convolutional layer is where you have a neuron connected to a tiny subgrid of pixels or neurons, and use copies of that neuron across all parts of the image/block to make another 3d array of neuron activations. A max pooling layer makes a block of activations spatially smaller. Lots of these stacked on top of one another can be trained with gradient descent and are really good at learning from images.Iâ€™m going to describe the architecture pretty briefly because itâ€™s not the important part of the paper. Koch et al uses a convolutional siamese network to classify pairs of omniglot images, so the twin networks are both convolutional neural nets(CNNs). The twins each have the following architecture: convolution with 64 10x10 filters, relu -&gt; max pool -&gt; convolution with 128 7x7 filters, relu -&gt; max pool -&gt; convolution with 128 4x4 filters, relu -&gt; max pool -&gt; convolution with 256 4x4 filters. The twin networks reduce their inputs down to smaller and smaller 3d tensors, finally their is a fully connected layer with 4096 units. The absolute difference between the two vectors is used as input to a linear classifier. All up, the network has 38,951,745 parameters - 96% of which belong to the fully connected layer. This is quite a lot, so the network has high capacity to overfit, but as I show below, pairwse training means the dataset size is huge so this wonâ€™t be a problem.Hastily made architecture diagram.The output is squashed into [0,1] with a sigmoid function to make it a probability. We use the target t=1t=1 when the images have the same class and t=0t=0 for a different class. Itâ€™s trained with logistic regression. This means the loss function should be binary cross entropy between the predictions and targets. There is also a L2 weight decay term in the loss to encourage the network to learn smaller/less noisy weights and possibly improve generalization:$$L(x_1,x_2,t)=tâ‹…\log(p(x_1âˆ˜x_2))+(1âˆ’t)â‹…\log(1âˆ’p(x_1âˆ˜x_2))+Î»â‹…||w||^2$$When it does a one-shot task, the siamese net simply classifies the test image as whatever image in the support set it thinks is most similar to the test image:$$C(\hat{x},S) = {\arg\max}_c P(\hat{x}âˆ˜x_c),x_câˆˆS$$This uses an argmax unlike nearest neighbour which uses an argmin, because a metric like L2 is higher the more â€œdifferentâ€ the examples are, but this models outputs p(x1âˆ˜x2), so we want the highest. This approach has one flaw thatâ€™s obvious to me: for any xaxa in the support set,the probability $\hat{x}âˆ˜x_a$ is independent of every other example in the support set! This means the probabilities wonâ€™t sum to 1, ignores important information, namely that the test image will be the same type as exactly one xâˆˆSâ€¦Observation: effective dataset size in pairwise trainingEDIT: After discussing this with a PhD student at UoA, I think this bit might be overstated or even just wrong. Emperically, my implementation did overfit, even though it wasnâ€™t trained for enough iterations to sample every possible pair, which kind of contradicts this section. Iâ€™m leaving it up in the spirit of being wrong loudly.One cool thing I noticed about training on pairs is that there are quadratically many possible pairs of images to train the model on, making it hard to overfit. Say we have CC examples each of EE classes. Since there are Câ‹…ECâ‹…E images total, the total number of possible pairs is given by$$Npairs=(Câ‹…E 2)=(Câ‹…E)!/2!(Câ‹…Eâˆ’2)!$$For omniglot with its 20 examples of 964 training classes, this leads to 185,849,560 possible pairs, which is huge! However, the siamese network needs examples of both same and different class pairs. There are E examples per class, so there will be (E 2) pairs for every class, which means there are Nsame=(E 2)â‹…C possible pairs with the same class - 183,160 pairs for omniglot. Even though 183,160 example pairs is plenty, itâ€™s only a thousandth of the possible pairs, and the number of same-class pairs increases quadratically with E but only linearly with C. This is important because the siamese network should be given a 1:1 ratio of same-class and different-class pairs to train on - perhaps it implies that pairwise training is easier on datasets with lots of examples per class.The Code:Prefer to just play with a jupyter notebook? I got you famHere is the model definition, it should be pretty easy to follow if youâ€™ve seen keras before. I only define the twin networkâ€™s architecture once as a Sequential() model and then call it with respect to each of two input layers, this way the same parameters are used for both inputs. Then merge them together with absolute distance and add an output layer, and compile the model with binary cross entropy loss.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2Dfrom keras.models import Model, Sequentialfrom keras.regularizers import l2from keras import backend as Kfrom keras.optimizers import SGD,Adamfrom keras.losses import binary_crossentropyimport numpy.random as rngimport numpy as npimport osimport dill as pickleimport matplotlib.pyplot as pltfrom sklearn.utils import shuffledef W_init(shape,name=None): """Initialize weights as in paper""" values = rng.normal(loc=0,scale=1e-2,size=shape) return K.variable(values,name=name)#//TODO: figure out how to initialize layer biases in keras.def b_init(shape,name=None): """Initialize bias as in paper""" values=rng.normal(loc=0.5,scale=1e-2,size=shape) return K.variable(values,name=name)input_shape = (105, 105, 1)left_input = Input(input_shape)right_input = Input(input_shape)#build convnet to use in each siamese 'leg'convnet = Sequential()convnet.add(Conv2D(64,(10,10),activation='relu',input_shape=input_shape, kernel_initializer=W_init,kernel_regularizer=l2(2e-4)))convnet.add(MaxPooling2D())convnet.add(Conv2D(128,(7,7),activation='relu', kernel_regularizer=l2(2e-4),kernel_initializer=W_init,bias_initializer=b_init))convnet.add(MaxPooling2D())convnet.add(Conv2D(128,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))convnet.add(MaxPooling2D())convnet.add(Conv2D(256,(4,4),activation='relu',kernel_initializer=W_init,kernel_regularizer=l2(2e-4),bias_initializer=b_init))convnet.add(Flatten())convnet.add(Dense(4096,activation="sigmoid",kernel_regularizer=l2(1e-3),kernel_initializer=W_init,bias_initializer=b_init))#encode each of the two inputs into a vector with the convnetencoded_l = convnet(left_input)encoded_r = convnet(right_input)#merge two encoded inputs with the l1 distance between themL1_distance = lambda x: K.abs(x[0]-x[1])both = merge([encoded_l,encoded_r], mode = L1_distance, output_shape=lambda x: x[0])prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(both)siamese_net = Model(input=[left_input,right_input],output=prediction)#optimizer = SGD(0.0004,momentum=0.6,nesterov=True,decay=0.0003)optimizer = Adam(0.00006)#//TODO: get layerwise learning rates and momentum annealing scheme described in paperworkingsiamese_net.compile(loss="binary_crossentropy",optimizer=optimizer)siamese_net.count_params()The original paper used layerwise learning rates and momentum - I skipped this because it; was kind of messy to implement in keras and the hyperparameters arenâ€™t the interesting part of the paper. Koch et al adds examples to the dataset by distorting the images and runs experiments with a fixed training set of up to 150,000 pairs. Since that wonâ€™t fit in my computers memory, I decided to just randomly sample pairs. Loading image pairs was probably the hardest part of this to implement. Since there were 20 examples for every class, I reshaped the data into N_classes x 20 x 105 x 105 arrays, to make it easier to index by category.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Siamese_Loader: """For loading batches and testing tasks to a siamese net""" def __init__(self,Xtrain,Xval): self.Xval = Xval self.Xtrain = Xtrain self.n_classes,self.n_examples,self.w,self.h = Xtrain.shape self.n_val,self.n_ex_val,_,_ = Xval.shape def get_batch(self,n): """Create batch of n pairs, half same class, half different class""" categories = rng.choice(self.n_classes,size=(n,),replace=False) pairs=[np.zeros((n, self.h, self.w,1)) for i in range(2)] targets=np.zeros((n,)) targets[n//2:] = 1 for i in range(n): category = categories[i] idx_1 = rng.randint(0,self.n_examples) pairs[0][i,:,:,:] = self.Xtrain[category,idx_1].reshape(self.w,self.h,1) idx_2 = rng.randint(0,self.n_examples) #pick images of same class for 1st half, different for 2nd category_2 = category if i &gt;= n//2 else (category + rng.randint(1,self.n_classes)) % self.n_classes pairs[1][i,:,:,:] = self.Xtrain[category_2,idx_2].reshape(self.w,self.h,1) return pairs, targets def make_oneshot_task(self,N): """Create pairs of test image, support set for testing N way one-shot learning. """ categories = rng.choice(self.n_val,size=(N,),replace=False) indices = rng.randint(0,self.n_ex_val,size=(N,)) true_category = categories[0] ex1, ex2 = rng.choice(self.n_examples,replace=False,size=(2,)) test_image = np.asarray([self.Xval[true_category,ex1,:,:]]*N).reshape(N,self.w,self.h,1) support_set = self.Xval[categories,indices,:,:] support_set[0,:,:] = self.Xval[true_category,ex2] support_set = support_set.reshape(N,self.w,self.h,1) pairs = [test_image,support_set] targets = np.zeros((N,)) targets[0] = 1 return pairs, targets def test_oneshot(self,model,N,k,verbose=0): """Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks""" pass n_correct = 0 if verbose: print("Evaluating model on &#123;&#125; unique &#123;&#125; way one-shot learning tasks ...".format(k,N)) for i in range(k): inputs, targets = self.make_oneshot_task(N) probs = model.predict(inputs) if np.argmax(probs) == 0: n_correct+=1 percent_correct = (100.0*n_correct / k) if verbose: print("Got an average of &#123;&#125;% &#123;&#125; way one-shot learning accuracy".format(percent_correct,N)) return percent_correct..And now the training loop. Nothing unusual here, except for that I monitor one-shot tasks validation accuracy to test performance, rather than loss on the validation set.12345678910111213141516171819evaluate_every = 7000loss_every=300batch_size = 32N_way = 20n_val = 550siamese_net.load_weights("PATH")best = 76.0for i in range(900000): (inputs,targets)=loader.get_batch(batch_size) loss=siamese_net.train_on_batch(inputs,targets) if i % evaluate_every == 0: val_acc = loader.test_oneshot(siamese_net,N_way,n_val,verbose=True) if val_acc &gt;= best: print("saving") siamese_net.save('PATH') best=val_acc if i % loss_every == 0: print("iteration &#123;&#125;, training loss: &#123;:.2f&#125;,".format(i,loss))ResultsOnce the learning curve flattened out, I used the weights which got the best validation 20 way accuracy for testing. My network averaged ~83% accuracy for tasks from the evaluation set, compared to 93% in the original paper. Probably this difference is because I didnâ€™t implement many of the performance enhancing tricks from the original paper, like layerwise learning rates/momentum, data augmentation with distortions, bayesian hyperparemeter optimization and I also probably trained for less epochs. Iâ€™m not too worried about this because this tutorial was more about introducing one-shot learning in general, than squeezing the last few % performance out of a classifier. There is no shortage of resources on that!I was curious to see how accuracy varied over different values of â€œNâ€ in N way one shot learning, so I plotted it, with comparisons to 1 nearest neighbours, random guessing and training set performance.results.As you can see, it performs worse on tasks from the validaiton set than the train set, especially for high values of N, so there must be overfitting. It would be interesting to see how well traditional regularization methods like dropout work when the validation set is made of completely different classes to the training set. It works better than I expected for large N, still averaging above 65% accuracy for 50-60 way tasks.DiscussionWeâ€™ve just trained a neural network trained to do same-different pairwise classification on symbols. More importantly, weâ€™ve shown that it can then get reasonable accuracy in 20 way one-shot learning on symbols from unseen alphabets. Of course, this is not the only way to use deep networks for one-shot learning.As I touched on earlier, I think a major flaw of this siamese approach is that it only compares the test image to every support image individualy, when it should be comparing it to the support set as a whole. When the network compares the test image to any image x1, p(x^âˆ˜x1) is the same no matter what else is the support set. This is silly. Say youâ€™re doing a one-shot task and you see an image that looks similar to the test image. You should be much less confident they have the same class if there is another image in the support set that also looks similar to the test image. The training objective is different to the test objective. It might work better to have a model that can compare the test image to the support set as a whole and use the constraint that only one support image has the same class.Matching Networks for One Shot learning does exactly that. Rather than learning a similarity function, they have a deep model learn a full nearest neighbour classifier end to end, training directly on oneshot tasks rather than on image pairs. Andrej Karpathyâ€™s notes explain it much better than I can. Since you are learning a machine classifier, this can be seen as a kind of meta-learning. One-shot Learning with Memory-Augmented Neural Networks explores the connection between one-shot learning and meta learning and trains a memory augmented network on omniglot, though I confess I had trouble understanding this paper.What next?The omniglot dataset has been around since 2015, and already there are scalable ML algorithms getting within the ballpark of human level performance on certain one-shot learning tasks. Hopefully one day it will be seen as a mere â€œsanity checkâ€ for one-shot classification algorithms much like MNIST is for supervised learning now.Image classification is cool but I donâ€™t think itâ€™s the most interesting problem in machine learning. Now that we know deep one-shot learning can work pretty good, I think it would be cool to see attempts at one-shot learning for other, more exotic tasks.Ideas from one-shot learning could be used for more sample efficient reinforcement learning, especially for problems like OpenAIâ€™s Universe, where there are lots of MDPs/environments that have similar visual features and dynamics. - It would be cool to have an RL agent that could efficiently explore a new environment after learning in similar MDPs.OpenAIâ€™s world of bits environments.One-shot Imitation learning is one of my favourite one-shot learning papers. The goal is to have an agent learn a robust policy for solving a task from a single human demonstration of that task.This is done by:Having a neural net map from the current state and a sequence of states(the human demonstration) to an actionTraining it on pairs of human demonstrations on slightly different variants of the same task, with the goal of reproducing the second demonstration based on the first.This strikes me as a really promising path to one day having broadly applicable, learning based robots!Bringing one-shot learning to NLP tasks is a cool idea too. Matching Networks for One-Shot learning has an attempt at one-shot language modeling, filling a missing word in a test sentence given a small set of support sentences, and it seems to work pretty well. Exciting!ConclusionAnyway, thanks for reading! I hope youâ€™ve managed to one-shot learn the concept of one-shot learning :) If not, Iâ€™d love to hear feedback or answer any questions you have!]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Anaconda uses socket proxy on Windows 10]]></title>
      <url>%2F2018%2F05%2F17%2FAnaconda-uses-socket-proxy-on-Windows-10%2F</url>
      <content type="text"><![CDATA[you need to create a .condarc file in you Windows user area:1C:\Users\&lt;username&gt;\The file should contain (if you are using shadowsocks):12345678910111213channels:- defaults# Show channel URLs when displaying what is going to be downloaded and# in 'conda list'. The default is False.show_channel_urls: Trueallow_other_channels: Trueproxy_servers: http: socks5://127.0.0.1:1080 https: socks5://127.0.0.1:1080ssl_verify: FalseNoticed that you cannot create a file that begins with a dot in Windows directly.To create/rename on windows explorer, just rename to .name. - The additional dot at the end is necessary, and will be removed by Windows Explorer.To create a new file begins with a dot, on command prompt:1echo testing &gt; .name]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Paper Note: Selective Search for Object Recognition]]></title>
      <url>%2F2018%2F05%2F03%2FPaper-Note-Selective-Search-for-Object-Recognition%2F</url>
      <content type="text"><![CDATA[ä¸Ž Selective Search åˆæ¬¡è§é¢æ˜¯åœ¨è‘—åçš„ç‰©ä½“æ£€æµ‹è®ºæ–‡ Rich feature hierarchies for accurate object detection and semantic segmentation ï¼Œå› æ­¤ï¼Œè¿™ç¯‡è®ºæ–‡ç®—æ˜¯é˜…è¯» R-CNN çš„å‡†å¤‡ã€‚è¿™ç¯‡è®ºæ–‡çš„æ ‡é¢˜è™½ç„¶ä¹Ÿæåˆ°äº† Object Recognition ï¼Œä½†å°±åˆ›æ–°ç‚¹è€Œè¨€ï¼Œå…¶å®žåœ¨ Selective Search ã€‚æ‰€ä»¥ï¼Œè¿™é‡Œåªç®€å•ä»‹ç» Selective Search çš„æ€æƒ³å’Œç®—æ³•è¿‡ç¨‹ï¼Œå¯¹äºŽ Object Recognition åˆ™ä¸å†èµ˜è¿°ã€‚ä»€ä¹ˆæ˜¯ Selective SearchSelective Searchï¼Œè¯´çš„ç®€å•ç‚¹ï¼Œå°±æ˜¯ä»Žå›¾ç‰‡ä¸­æ‰¾å‡ºç‰©ä½“å¯èƒ½å­˜åœ¨çš„åŒºåŸŸã€‚resultä¸Šé¢è¿™å¹…å®‡èˆªå‘˜çš„å›¾ç‰‡ä¸­ï¼Œé‚£äº›çº¢è‰²çš„æ¡†å°±æ˜¯ Selective Search æ‰¾å‡ºæ¥çš„å¯èƒ½å­˜åœ¨ç‰©ä½“çš„åŒºåŸŸã€‚åœ¨è¿›ä¸€æ­¥æŽ¢è®¨å®ƒçš„åŽŸç†ä¹‹å‰ï¼Œæˆ‘ä»¬åˆ†æžä¸€ä¸‹ï¼Œå¦‚ä½•åˆ¤åˆ«å“ªäº› region å±žäºŽä¸€ä¸ªç‰©ä½“ï¼Ÿimage segä½œè€…åœ¨è®ºæ–‡ä¸­ç”¨ä»¥ä¸Šå››å¹…å›¾ï¼Œåˆ†åˆ«æè¿°äº†å››ç§å¯èƒ½çš„æƒ…å†µï¼šå›¾ a ï¼Œç‰©ä½“ä¹‹é—´å¯èƒ½å­˜åœ¨å±‚çº§å…³ç³»ï¼Œæ¯”å¦‚ï¼šç¢—é‡Œæœ‰ä¸ªå‹ºï¼›å›¾ bï¼Œæˆ‘ä»¬å¯ä»¥ç”¨é¢œè‰²æ¥åˆ†å¼€ä¸¤åªçŒ«ï¼Œå´æ²¡æ³•ç”¨çº¹ç†æ¥åŒºåˆ†ï¼›å›¾ cï¼Œæˆ‘ä»¬å¯ä»¥ç”¨çº¹ç†æ¥åŒºåˆ†å˜è‰²é¾™ï¼Œå´æ²¡æ³•ç”¨é¢œè‰²æ¥åŒºåˆ†ï¼›å›¾ dï¼Œè½®èƒŽæ˜¯è½¦çš„ä¸€éƒ¨åˆ†ï¼Œä¸æ˜¯å› ä¸ºå®ƒä»¬é¢œè‰²ç›¸è¿‘ã€çº¹ç†ç›¸è¿‘ï¼Œè€Œæ˜¯å› ä¸ºè½®èƒŽåŒ…å«åœ¨è½¦ä¸Šã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬æ²¡æ³•ç”¨å•ä¸€çš„ç‰¹å¾æ¥å®šä½ç‰©ä½“ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘å¤šç§ç­–ç•¥ï¼Œè¿™ä¸€ç‚¹æ˜¯ Selective Search ç²¾è¦æ‰€åœ¨ã€‚éœ€è¦è€ƒè™‘çš„é—®é¢˜åœ¨å­¦ä¹  Selective Search ç®—æ³•ä¹‹å‰ï¼Œæˆ‘æ›¾åœ¨è®¡ç®—æœºè§†è§‰è¯¾ä¸Šå­¦åˆ°è¿‡å…³äºŽç‰©ä½“ï¼ˆä¸»è¦æ˜¯äººè„¸ï¼‰æ£€æµ‹çš„æ–¹æ³•ã€‚é€šå¸¸æ¥è¯´ï¼Œæœ€å¸¸è§„ä¹Ÿæ˜¯æœ€ç®€å•ç²—æš´çš„æ–¹æ³•ï¼Œå°±æ˜¯ç”¨ä¸åŒå°ºå¯¸çš„çŸ©å½¢æ¡†ï¼Œä¸€è¡Œä¸€è¡Œåœ°æ‰«ææ•´å¼ å›¾åƒï¼Œé€šè¿‡æå–çŸ©å½¢æ¡†å†…çš„ç‰¹å¾åˆ¤æ–­æ˜¯å¦æ˜¯å¾…æ£€æµ‹ç‰©ä½“ã€‚è¿™ç§æ–¹æ³•çš„å¤æ‚åº¦æžé«˜ï¼Œæ‰€ä»¥åˆè¢«ç§°ä¸º exhaustive searchã€‚åœ¨äººè„¸è¯†åˆ«ä¸­ï¼Œç”±äºŽä½¿ç”¨äº† Haar ç‰¹å¾ï¼Œå› æ­¤å¯ä»¥å€ŸåŠ© Paul Viola å’Œ Michael Jones ä¸¤ä½å¤§ç‰›æå‡ºçš„ç§¯åˆ†å›¾ï¼Œä½¿æ£€æµ‹åœ¨å¸¸è§„æ—¶é—´å†…å®Œæˆã€‚ä½†å¹¶ä¸æ˜¯æ¯ç§ç‰¹å¾éƒ½é€‚ç”¨äºŽç§¯åˆ†å›¾ï¼Œå°¤å…¶åœ¨ç¥žç»ç½‘ç»œä¸­ï¼Œç§¯åˆ†å›¾è¿™ç§åŠ¨æ€è§„åˆ’çš„æ€è·¯å°±æ²¡ä»€ä¹ˆä½œç”¨äº†ã€‚é’ˆå¯¹ä¼ ç»Ÿæ–¹æ³•çš„ä¸è¶³ï¼ŒSelective Search ä»Žä¸‰ä¸ªè§’åº¦æå‡ºäº†æ”¹è¿›ï¼šæˆ‘ä»¬æ²¡æ³•äº‹å…ˆå¾—çŸ¥ç‰©ä½“çš„å¤§å°ï¼Œåœ¨ä¼ ç»Ÿæ–¹æ³•ä¸­éœ€è¦ç”¨ä¸åŒå°ºå¯¸çš„çŸ©å½¢æ¡†æ£€æµ‹ç‰©ä½“ï¼Œé˜²æ­¢é—æ¼ã€‚è€Œ Selective Search é‡‡ç”¨äº†ä¸€ç§å…·å¤‡å±‚æ¬¡ç»“æž„çš„ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›æ£€æµ‹çš„æ—¶é—´å¤æ‚åº¦å¯èƒ½ä¼šå¾ˆé«˜ã€‚Selective Search éµå¾ªç®€å•å³æ˜¯ç¾Žçš„åŽŸåˆ™ï¼Œåªè´Ÿè´£å¿«é€Ÿåœ°ç”Ÿæˆå¯èƒ½æ˜¯ç‰©ä½“çš„åŒºåŸŸï¼Œè€Œä¸åšå…·ä½“çš„æ£€æµ‹ï¼›å¦å¤–ï¼Œç»“åˆä¸Šä¸€èŠ‚æå‡ºçš„ï¼Œé‡‡ç”¨å¤šç§å…ˆéªŒçŸ¥è¯†æ¥å¯¹å„ä¸ªåŒºåŸŸè¿›è¡Œç®€å•çš„åˆ¤åˆ«ï¼Œé¿å…ä¸€äº›æ— ç”¨çš„æœç´¢ï¼Œæé«˜é€Ÿåº¦å’Œç²¾åº¦ã€‚ç®—æ³•æ¡†æž¶algorithmè®ºæ–‡ä¸­ç»™å‡ºçš„è¿™ä¸ªç®—æ³•æ¡†æž¶è¿˜æ˜¯å¾ˆè¯¦ç»†çš„ï¼Œè¿™é‡Œå†ç®€å•ç¿»è¯‘ä¸€ä¸‹ã€‚è¾“å…¥ï¼šå½©è‰²å›¾ç‰‡ã€‚è¾“å‡ºï¼šç‰©ä½“å¯èƒ½çš„ä½ç½®ï¼Œå®žé™…ä¸Šæ˜¯å¾ˆå¤šçš„çŸ©å½¢åæ ‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ç¯‡è®ºæ–‡çš„æ–¹æ³•å°†å›¾ç‰‡åˆå§‹åŒ–ä¸ºå¾ˆå¤šå°åŒºåŸŸ $R=r_i, \cdots, r_n$ã€‚ç”±äºŽæˆ‘ä»¬çš„é‡ç‚¹æ˜¯ Selective Searchï¼Œå› æ­¤æˆ‘ç›´æŽ¥å°†è¯¥è®ºæ–‡çš„ç®—æ³•å½“æˆä¸€ä¸ªé»‘ç›’å­ã€‚åˆå§‹åŒ–ä¸€ä¸ªç›¸ä¼¼é›†åˆä¸ºç©ºé›†ï¼š $S=âˆ…$ã€‚è®¡ç®—æ‰€æœ‰ç›¸é‚»åŒºåŸŸä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆç›¸ä¼¼åº¦å‡½æ•°ä¹‹åŽä¼šé‡ç‚¹åˆ†æžï¼‰ï¼Œæ”¾å…¥é›†åˆ S ä¸­ï¼Œé›†åˆ S ä¿å­˜çš„å…¶å®žæ˜¯ä¸€ä¸ªåŒºåŸŸå¯¹ä»¥åŠå®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚æ‰¾å‡º S ä¸­ç›¸ä¼¼åº¦æœ€é«˜çš„åŒºåŸŸå¯¹ï¼Œå°†å®ƒä»¬åˆå¹¶ï¼Œå¹¶ä»Ž S ä¸­åˆ é™¤ä¸Žå®ƒä»¬ç›¸å…³çš„æ‰€æœ‰ç›¸ä¼¼åº¦å’ŒåŒºåŸŸå¯¹ã€‚é‡æ–°è®¡ç®—è¿™ä¸ªæ–°åŒºåŸŸä¸Žå‘¨å›´åŒºåŸŸçš„ç›¸ä¼¼åº¦ï¼Œæ”¾å…¥é›†åˆ S ä¸­ï¼Œå¹¶å°†è¿™ä¸ªæ–°åˆå¹¶çš„åŒºåŸŸæ”¾å…¥é›†åˆ R ä¸­ã€‚é‡å¤è¿™ä¸ªæ­¥éª¤ç›´åˆ° S ä¸ºç©ºã€‚ä»Ž R ä¸­æ‰¾å‡ºæ‰€æœ‰åŒºåŸŸçš„ bounding boxï¼ˆå³åŒ…å›´è¯¥åŒºåŸŸçš„æœ€å°çŸ©å½¢æ¡†ï¼‰ï¼Œè¿™äº› box å°±æ˜¯ç‰©ä½“å¯èƒ½çš„åŒºåŸŸã€‚å¦å¤–ï¼Œä¸ºäº†æé«˜é€Ÿåº¦ï¼Œæ–°åˆå¹¶åŒºåŸŸçš„ feature å¯ä»¥é€šè¿‡ä¹‹å‰çš„ä¸¤ä¸ªåŒºåŸŸèŽ·å¾—ï¼Œè€Œä¸å¿…é‡æ–°éåŽ†æ–°åŒºåŸŸçš„åƒç´ ç‚¹è¿›è¡Œè®¡ç®—ã€‚è¿™ä¸ª feature ä¼šè¢«ç”¨äºŽè®¡ç®—ç›¸ä¼¼åº¦ã€‚ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•å°†ç›´æŽ¥å½±å“åˆå¹¶åŒºåŸŸçš„é¡ºåºï¼Œè¿›è€Œå½±å“åˆ°æ£€æµ‹ç»“æžœçš„å¥½åã€‚è®ºæ–‡ä¸­æ¯”è¾ƒäº†å…«ç§é¢œè‰²ç©ºé—´çš„ç‰¹ç‚¹ï¼Œåœ¨å®žé™…æ“ä½œä¸­ï¼Œåªé€‰æ‹©ä¸€ä¸ªé¢œè‰²ç©ºé—´ï¼ˆæ¯”å¦‚ï¼šRGB ç©ºé—´ï¼‰è¿›è¡Œè®¡ç®—ã€‚æ­£å¦‚ä¸€å¼€å§‹æå‡ºçš„é‚£æ ·ï¼Œæˆ‘ä»¬éœ€è¦ç»¼åˆå¤šç§ä¿¡æ¯æ¥åˆ¤æ–­ã€‚ä½œè€…å°†ç›¸ä¼¼åº¦åº¦é‡å…¬å¼åˆ†ä¸ºå››ä¸ªå­å…¬å¼ï¼Œç§°ä¸ºäº’è¡¥ç›¸ä¼¼åº¦æµ‹é‡(Complementary Similarity Measures) ã€‚è¿™å››ä¸ªå­å…¬å¼çš„å€¼éƒ½è¢«å½’ä¸€åŒ–åˆ°åŒºé—´ [0, 1] å†…ã€‚1. é¢œè‰²ç›¸ä¼¼åº¦scolor (ri,rj)scolor (ri,rj)æ­£å¦‚æœ¬æ–‡ä¸€å¼€å§‹æåˆ°çš„ï¼Œé¢œè‰²æ˜¯ä¸€ä¸ªå¾ˆé‡è¦çš„åŒºåˆ†ç‰©ä½“çš„å› ç´ ã€‚è®ºæ–‡ä¸­å°†æ¯ä¸ª region çš„åƒç´ æŒ‰ä¸åŒé¢œè‰²é€šé“ç»Ÿè®¡æˆç›´æ–¹å›¾ï¼Œå…¶ä¸­ï¼Œæ¯ä¸ªé¢œè‰²é€šé“çš„ç›´æ–¹å›¾ä¸º 25 bins ï¼ˆæ¯”å¦‚ï¼Œå¯¹äºŽ 0 ï½ž 255 çš„é¢œè‰²é€šé“æ¥è¯´ï¼Œå°±æ¯éš” 9(255/25=9) ä¸ªæ•°å€¼ç»Ÿè®¡åƒç´ æ•°é‡ï¼‰ã€‚è¿™æ ·ï¼Œä¸‰ä¸ªé€šé“å¯ä»¥å¾—åˆ°ä¸€ä¸ª 75 ç»´çš„ç›´æ–¹å›¾å‘é‡ $C_i={c_{i}^{1}, â€¦, c_{i}^{n}}$ï¼Œå…¶ä¸­ n = 75ã€‚ä¹‹åŽï¼Œæˆ‘ä»¬ç”¨ L1 èŒƒæ•°ï¼ˆç»å¯¹å€¼ä¹‹å’Œï¼‰å¯¹ç›´æ–¹å›¾è¿›è¡Œå½’ä¸€åŒ–ã€‚ç”±ç›´æ–¹å›¾æˆ‘ä»¬å°±å¯ä»¥è®¡ç®—ä¸¤ä¸ªåŒºåŸŸçš„é¢œè‰²ç›¸ä¼¼åº¦ï¼š$$s_{color}(r_i, r_j) =\sum_{k=1}^{n}{min(c_{i}^{k}, c_{j}^{k})}$$è¿™ä¸ªé¢œè‰²ç›´æ–¹å›¾å¯ä»¥åœ¨åˆå¹¶åŒºåŸŸçš„æ—¶å€™ï¼Œå¾ˆæ–¹ä¾¿åœ°ä¼ é€’ç»™ä¸‹ä¸€çº§åŒºåŸŸã€‚å³å®ƒä»¬åˆå¹¶åŽçš„åŒºåŸŸçš„ç›´æ–¹å›¾å‘é‡ä¸ºï¼š$$C_t=\frac{size(r_i)C_i+size(r_j)C_j}{size(r_i)+size(r_j)}$$ï¼Œå…¶ä¸­$size(r_i)$ è¡¨ç¤ºåŒºåŸŸ $r_i$ çš„é¢ç§¯ï¼Œåˆå¹¶åŽçš„åŒºåŸŸä¸º $size(r_t)=size(r_i)+size(r_j)$ã€‚2. çº¹ç†ç›¸ä¼¼åº¦$s_{texture}(r_i,r_j)$å¦ä¸€ä¸ªéœ€è¦è€ƒè™‘çš„å› ç´ æ˜¯çº¹ç†ï¼Œå³å›¾åƒçš„æ¢¯åº¦ä¿¡æ¯ã€‚è®ºæ–‡ä¸­å¯¹çº¹ç†çš„è®¡ç®—é‡‡ç”¨äº† SIFT-like ç‰¹å¾ï¼Œè¯¥ç‰¹å¾å€Ÿé‰´äº† SIFT çš„è®¡ç®—æ€è·¯ï¼Œå¯¹æ¯ä¸ªé¢œè‰²é€šé“çš„åƒç´ ç‚¹ï¼Œæ²¿å‘¨å›´ 8 ä¸ªæ–¹å‘è®¡ç®—é«˜æ–¯ä¸€é˜¶å¯¼æ•°(Ïƒ=1Ïƒ=1)ï¼Œæ¯ä¸ªæ–¹å‘ç»Ÿè®¡ä¸€ä¸ªç›´æ–¹å›¾ï¼ˆbin = 10ï¼‰ï¼Œè¿™æ ·ï¼Œä¸€ä¸ªé¢œè‰²é€šé“ç»Ÿè®¡å¾—åˆ°çš„ç›´æ–¹å›¾å‘é‡ä¸º 80 ç»´ï¼Œä¸‰ä¸ªé€šé“å°±æ˜¯ 240 ç»´ï¼š$T_i={t_i^{(1)}, â€¦, t_i^{(n)}}$ï¼Œå…¶ä¸­ n = 240ã€‚æ³¨æ„è¿™ä¸ªç›´æ–¹å›¾è¦ç”¨ L1 èŒƒæ•°å½’ä¸€åŒ–ã€‚ç„¶åŽï¼Œæˆ‘ä»¬æŒ‰ç…§é¢œè‰²ç›¸ä¼¼åº¦çš„è®¡ç®—æ€è·¯è®¡ç®—ä¸¤ä¸ªåŒºåŸŸçš„çº¹ç†ç›¸ä¼¼åº¦ï¼š$$s_{texture}(r_i, r_j) =\sum_{k=1}^{n}{min(t_{i}^{k}, t_{j}^{k})}$$3. å°ºå¯¸ç›¸ä¼¼åº¦$s_{size} (r_i,r_j)$åœ¨åˆå¹¶åŒºåŸŸçš„æ—¶å€™ï¼Œè®ºæ–‡ä¼˜å…ˆè€ƒè™‘å°åŒºåŸŸçš„åˆå¹¶ï¼Œè¿™ç§åšæ³•å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šä¿è¯æ¯æ¬¡åˆå¹¶çš„åŒºåŸŸé¢ç§¯éƒ½æ¯”è¾ƒç›¸ä¼¼ï¼Œé˜²æ­¢å¤§åŒºåŸŸå¯¹å°åŒºåŸŸçš„é€æ­¥èš•é£Ÿã€‚è¿™ä¹ˆåšçš„ç†ç”±ä¹Ÿå¾ˆç®€å•ï¼Œæˆ‘ä»¬è¦å‡åŒ€åœ°åœ¨å›¾ç‰‡çš„æ¯ä¸ªè§’è½ç”Ÿæˆä¸åŒå°ºå¯¸çš„åŒºåŸŸï¼Œä½œç”¨ç›¸å½“äºŽ exhaustive search ä¸­ç”¨ä¸åŒå°ºå¯¸çš„çŸ©å½¢æ‰«æå›¾ç‰‡ã€‚å…·ä½“çš„ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼ä¸ºï¼š$$s_{size}(r_i, r_j)=1-\frac{size(r_i) + size(r_j)}{size(im)}$$å…¶ä¸­ï¼Œ$size(im)$ è¡¨ç¤ºåŽŸå›¾ç‰‡çš„åƒç´ æ•°é‡ã€‚4. å¡«å……ç›¸ä¼¼åº¦$s_{fill}(r_i,r_j)$å¡«å……ç›¸ä¼¼åº¦ä¸»è¦ç”¨æ¥æµ‹é‡ä¸¤ä¸ªåŒºåŸŸä¹‹é—´ fit çš„ç¨‹åº¦ï¼Œä¸ªäººè§‰å¾—è¿™ä¸€ç‚¹æ˜¯è¦è§£å†³æ–‡ç« æœ€å¼€å§‹æå‡ºçš„ç‰©ä½“ä¹‹é—´çš„åŒ…å«å…³ç³»ï¼ˆæ¯”å¦‚ï¼šè½®èƒŽåŒ…å«åœ¨æ±½è½¦ä¸Šï¼‰ã€‚åœ¨ç»™å‡ºå¡«å……ç›¸ä¼¼åº¦çš„å…¬å¼å‰ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªçŸ©å½¢åŒºåŸŸ $BB_{ij}$ï¼Œå®ƒè¡¨ç¤ºåŒ…å« $r_i$ å’Œ $r_j$ çš„æœ€å°çš„ bounding boxã€‚åŸºäºŽæ­¤ï¼Œæˆ‘ä»¬ç»™å‡ºç›¸ä¼¼åº¦è®¡ç®—å…¬å¼ä¸ºï¼š$$s_{fill}(r_i, r_j)=1-\frac{size(BB_{ij})-size(r_i)-size(r_j)}{size(im)}$$ä¸ºäº†é«˜æ•ˆåœ°è®¡ç®— $BB_{ij}$ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è®¡ç®—æ¯ä¸ª region çš„æ—¶å€™ï¼Œéƒ½ä¿å­˜å®ƒä»¬çš„ bounding box çš„ä½ç½®ï¼Œè¿™æ ·ï¼Œ$BB_{ij}$ å°±å¯ä»¥å¾ˆå¿«åœ°ç”±ä¸¤ä¸ªåŒºåŸŸçš„ bounding box æŽ¨å‡ºæ¥ã€‚5. ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼ç»¼åˆä¸Šé¢å››ä¸ªå­å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°è®¡ç®—ç›¸ä¼¼åº¦çš„æœ€ç»ˆå…¬å¼ï¼š$$s(r_i, r_j) = a_1 s_{color}(r_i, r_j) +a_2s_{texture}(r_i, r_j) \\\\ +a_3s_{size}(r_i, r_j)+a_4s_{fill}(r_i, r_j)$$å…¶ä¸­ï¼Œ$a_i$çš„å–å€¼ä¸º 0 æˆ– 1ï¼Œè¡¨ç¤ºæŸä¸ªç›¸ä¼¼åº¦æ˜¯å¦è¢«é‡‡çº³ã€‚Combining Locationså‰é¢æˆ‘ä»¬åŸºæœ¬å®Œæˆäº† Selective Search çš„æµç¨‹ï¼Œä»Žå›¾ç‰‡ä¸­æå–å‡ºäº†ç‰©ä½“å¯èƒ½çš„ä½ç½®ã€‚çŽ°åœ¨ï¼Œæˆ‘ä»¬æƒ³å®Œå–„æœ€åŽä¸€ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯ç»™è¿™äº›ä½ç½®æŽ’ä¸ªåºã€‚å› ä¸ºæå–å‡ºæ¥çš„çŸ©å½¢æ¡†æ•°é‡å·¨å¤§ï¼Œè€Œç”¨æˆ·å¯èƒ½åªéœ€è¦å…¶ä¸­çš„å‡ ä¸ªï¼Œè¿™ä¸ªæ—¶å€™æˆ‘ä»¬å°±å¾ˆæœ‰å¿…è¦å¯¹è¿™äº›çŸ©å½¢æ¡†èµ‹äºˆä¼˜å…ˆçº§ï¼ŒæŒ‰ç…§ä¼˜å…ˆçº§é«˜ä½Žè¿”å›žç»™ç”¨æˆ·ã€‚åŽŸæ–‡ä¸­ä½œè€…ç§°è¿™ä¸€æ­¥ä¸º Combining Locationsï¼Œæˆ‘æ‰¾ä¸å‡ºåˆé€‚çš„ç¿»è¯‘ï¼Œå°±å§‘ä¸”ä¿ç•™è‹±æ–‡åŽŸæ–‡ã€‚è¿™ä¸ªæŽ’åºçš„æ–¹æ³•ä¹Ÿå¾ˆç®€å•ã€‚ä½œè€…å…ˆç»™å„ä¸ª region ä¸€ä¸ªåºå·ï¼Œå‰é¢è¯´äº†ï¼ŒSelective Search æ˜¯ä¸€ä¸ªé€æ­¥åˆå¹¶çš„å±‚çº§ç»“æž„ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å°†è¦†ç›–æ•´ä¸ªåŒºåŸŸçš„ region çš„åºå·æ ‡è®°ä¸º 1ï¼Œåˆæˆè¿™ä¸ªåŒºåŸŸçš„ä¸¤ä¸ªå­åŒºåŸŸçš„åºå·ä¸º 2ï¼Œä»¥æ­¤ç±»æŽ¨ã€‚ä½†å¦‚æžœä»…æŒ‰åºå·æŽ’åºï¼Œä¼šå­˜åœ¨ä¸€ä¸ªæ¼æ´žï¼Œé‚£å°±æ˜¯åŒºåŸŸé¢ç§¯å¤§çš„ä¼šæŽ’åœ¨å‰é¢ï¼Œä¸ºäº†é¿å…è¿™ä¸ªæ¼æ´žï¼Œä½œè€…åˆåœ¨æ¯ä¸ªåºå·å‰ä¹˜ä¸Šä¸€ä¸ªéšæœºæ•° $RNDâˆˆ[0,1]$ï¼Œé€šè¿‡è¿™ä¸ªæ–°è®¡ç®—å‡ºæ¥çš„æ•°å€¼ï¼ŒæŒ‰ä»Žå°åˆ°å¤§çš„é¡ºåºå¾—å‡º region æœ€ç»ˆçš„æŽ’åºç»“æžœã€‚å‚è€ƒSelective Search for Object Recognition(é˜…è¯»)Efficient Graph-Based Image Segmentationæœ¬æ–‡ä½œè€…ï¼š Jermmyæœ¬æ–‡é“¾æŽ¥ï¼š https://jermmy.github.io/2017/05/04/2017-5-4-paper-notes-selective-search/ç‰ˆæƒå£°æ˜Žï¼š æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜Žå¤–ï¼Œå‡é‡‡ç”¨ CC BY-NC-SA 3.0 è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜Žå‡ºå¤„ï¼]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Micro- and Macro-average of Precision, Recall and F-Score]]></title>
      <url>%2F2018%2F04%2F27%2FMicro-and-Macro-average-of-Precision-Recall-and-F-Score%2F</url>
      <content type="text"><![CDATA[Micro-average MethodIn Micro-average method, you sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics. For example, for a set of data, the systemâ€™sTrue positive (TP1)= 12False positive (FP1)=9False negative (FN1)=3Then precision (P1) and recall (R1) will be 57.14 and 80and for a different set of data, the systemâ€™sTrue positive (TP2)= 50False positive (FP2)=23False negative (FN2)=9Then precision (P2) and recall (R2) will be 68.49 and 84.75Now, the average precision and recall of the system using the Micro-average method isMicro-average of precision = (TP1+TP2)/(TP1+TP2+FP1+FP2) = (12+50)/(12+50+9+23) = 65.96Micro-average of recall = (TP1+TP2)/(TP1+TP2+FN1+FN2) = (12+50)/(12+50+3+9) = 83.78The Micro-average F-Score will be simply the harmonic mean of these two figures.Macro-average MethodThe method is straight forward. Just take the average of the precision and recall of the system on different sets. For example, the macro-average precision and recall of the system for the given example isMacro-average precision = (P1+P2)/2 = (57.14+68.49)/2 = 62.82Macro-average recall = (R1+R2)/2 = (80+84.75)/2 = 82.25The Macro-average F-Score will be simply the harmonic mean of these two figures.SuitabilityMacro-average method can be used when you want to know how the system performs overall across the sets of data. You should not come up with any specific decision with this average.On the other hand, micro-average can be a useful measure when your dataset varies in size.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Get financial data from Tushare]]></title>
      <url>%2F2018%2F04%2F11%2FGet-financial-data-from-Tushare%2F</url>
      <content type="text"><![CDATA[IntroductionTuShare is a famous free, open source python financial data interface package. Its official home page is: TuShare - financial data interface package. The interface package now provides a large amount of financial data covering a wide range of data such as stocks, fundamentals, macros, news, etc. (Please check the official website for details) and keep updating. At present, the length of the stockâ€™s data is three years. Although it is a bit short, it can basically meet the needs of quantitative beginners for testing.TutorialInstall and ImportYou need to install first:PandaslxmlTwo way to install tushare:pip install tusharevisit https://pypi.python.org/pypi/Tushare/, download and installHow to update:pip install tushare --upgradeImport package and view package version:123import tushareprint(tushare.__version__)Use some simple functionStock dataupdateï¼šMany of the quotes returned by the get_hist_data function are wrong, but both get_h_data and get_k_data can be usedWe should still master how to use tushare to obtain stock market data, using the ts.get_hist_data() function whose input parameters are:code: Stock code, ie 6-digit code, or index code (sh = Shanghai index sz = Shenzhen index hs300 = CSI 300 index sz50 = SSE 50 zxb = small and medium board cyb = board)start: Start date, format YYYY-MM-DDend: End date, format YYYY-MM-DDktype: Data type, D = day k line W = week M = month 5 = 5 minutes 15 = 15 minutes 30 = 30 minutes 60 = 60 minutes, the default is Dretry_count: The number of retries after the network is abnormal. The default is 3pause: Pause seconds when retrying, default is 0Return values:dateï¼šdateopenï¼šOpening pricehighï¼šHighest pricecloseï¼šClosing pricelowï¼šLowest pricevolumeï¼šVolumeprice_changeï¼šprice fluncuationp_changeï¼šQuote changema5ï¼š5-day average pricema10ï¼š10-day average pricema20: 20-day average pricev_ma5: 5-day average volumev_ma10: 10-day average volumev_ma20: 20-day average volumeturnover: Change in hand rate [Note: Index does not have this item]Specific examples:12345678910111213141516171819202122ts.get_hist_data('600848') date open high close low volume p_change ma5 2012-01-11 6.880 7.380 7.060 6.880 14129.96 2.62 7.0602012-01-12 7.050 7.100 6.980 6.900 7895.19 -1.13 7.0202012-01-13 6.950 7.000 6.700 6.690 6611.87 -4.01 6.9132012-01-16 6.680 6.750 6.510 6.480 2941.63 -2.84 6.8132012-01-17 6.660 6.880 6.860 6.460 8642.57 5.38 6.8222012-01-18 7.000 7.300 6.890 6.880 13075.40 0.44 6.7882012-01-19 6.690 6.950 6.890 6.680 6117.32 0.00 6.7702012-01-20 6.870 7.080 7.010 6.870 6813.09 1.74 6.832date ma10 ma20 v_ma5 v_ma10 v_ma20 turnover2012-01-11 7.060 7.060 14129.96 14129.96 14129.96 0.482012-01-12 7.020 7.020 11012.58 11012.58 11012.58 0.272012-01-13 6.913 6.913 9545.67 9545.67 9545.67 0.232012-01-16 6.813 6.813 7894.66 7894.66 7894.66 0.102012-01-17 6.822 6.822 8044.24 8044.24 8044.24 0.302012-01-18 6.833 6.833 7833.33 8882.77 8882.77 0.452012-01-19 6.841 6.841 7477.76 8487.71 8487.71 0.212012-01-20 6.863 6.863 7518.00 8278.38 8278.38 0.23You can also set the start time and end time of historical data:12345678910111213ts.get_hist_data('600848',start='2015-01-05',end='2015-01-09') date open high close low volume p_change ma5 ma102015-01-05 11.160 11.390 11.260 10.890 46383.57 1.26 11.156 11.2122015-01-06 11.130 11.660 11.610 11.030 59199.93 3.11 11.182 11.1552015-01-07 11.580 11.990 11.920 11.480 86681.38 2.67 11.366 11.2512015-01-08 11.700 11.920 11.670 11.640 56845.71 -2.10 11.516 11.3492015-01-09 11.680 11.710 11.230 11.190 44851.56 -3.77 11.538 11.363 date ma20 v_ma5 v_ma10 v_ma20 turnover2015-01-05 11.198 58648.75 68429.87 97141.81 1.592015-01-06 11.382 54854.38 63401.05 98686.98 2.032015-01-07 11.543 55049.74 61628.07 103010.58 2.972015-01-08 11.647 57268.99 61376.00 105823.50 1.95Others:123456789101112ts.get_hist_data('600848', ktype='W') # Get weekly k-line datats.get_hist_data('600848', ktype='M') # Get monthly k-line datats.get_hist_data('600848', ktype='5') # Get 5 minutes k-line datats.get_hist_data('600848', ktype='15') # Get 15 minutes k-line datats.get_hist_data('600848', ktype='30') # Get 30 minutes k-line datats.get_hist_data('600848', ktype='60') # Get 60 minutes k-line datats.get_hist_data('sh'ï¼‰# Get data on the Shanghai index k-line, other parameters consistent with the stocks, the same belowts.get_hist_data('sz'ï¼‰# Get Shenzhen Chengzhi k line datats.get_hist_data('hs300'ï¼‰# Get the CSI 300 k line datats.get_hist_data('sz50'ï¼‰# Get SSE 50 Index k-line datats.get_hist_data('zxb'ï¼‰# Get the k-line data of small and medium board indicests.get_hist_data('cyb'ï¼‰# Get GEM Index k-line dataFundamental dataWith tushare we can also get fundamental data through ts.get_stock_basics() (shown in the results section):1234567891011121314ts.get_stock_basics()code name industry area pe outstanding totals totalAssets 300563 Nç¥žå®‡ é€šä¿¡è®¾å¤‡ æ±Ÿè‹ 26.73 2000.00 8000.00 4.216000e+04 601882 æµ·å¤©ç²¾å·¥ æœºåºŠåˆ¶é€  æµ™æ±Ÿ 26.83 5220.00 52200.00 1.877284e+05 601880 å¤§è¿žæ¸¯ æ¸¯å£ è¾½å® 76.40 773582.00 1289453.63 3.263012e+06 300556 ä¸è·¯è§†è§‰ è½¯ä»¶æœåŠ¡ æ·±åœ³ 101.38 2780.00 11113.33 4.448248e+04 600528 ä¸­é“äºŒå±€ å»ºç­‘æ–½å·¥ å››å· 149.34 145920.00 145920.00 5.709568e+06 002495 ä½³éš†è‚¡ä»½ é£Ÿå“ å¹¿ä¸œ 202.12 66611.13 93562.56 1.169174e+05 600917 é‡åº†ç‡ƒæ°” ä¾›æ°”ä¾›çƒ­ é‡åº† 76.87 15600.00 155600.00 8.444600e+05 002752 æ˜‡å…´è‚¡ä»½ å¹¿å‘ŠåŒ…è£… ç¦å»º 75.14 12306.83 63000.00 2.387493e+05 002346 æŸ˜ä¸­è‚¡ä»½ ç”µæ°”è®¾å¤‡ ä¸Šæµ· 643.97 7980.00 44157.53 2.263010e+05 000680 å±±æŽ¨è‚¡ä»½ å·¥ç¨‹æœºæ¢° å±±ä¸œ 0.00 105694.97 124078.75 9.050701e+05...Macro dataWe use the resident consumer index as an example, which can be obtained through the ts.get_cpi() function (it will get 322 items at a time, some of them will be displayed):123456789101112131415print ts.get_cpi() month cpi0 2016.10 102.101 2016.9 101.902 2016.8 101.343 2016.7 101.774 2016.6 101.885 2016.5 102.046 2016.4 102.337 2016.3 102.308 2016.2 102.289 2016.1 101.7510 2015.12 101.64...Recent newsThe tushare package can use the ts.get_latest_news() function to view the latest news, and it will return 80. For reasons of space, we only show the first 15 here. We can see that it is all Sina Financeâ€™s news data.12345678910111213141516171819202122232425262728293031323334353637print ts.get_latest_news(); classify title time \0 ç¾Žè‚¡ â€œç‰¹æœ—æ™®é€šèƒ€â€é¢„æœŸå‡æ¸© ç¾Žå›½å›½å€ºä¸‹æŒ« 11-14 23:10 1 ç¾Žè‚¡ ç‰¹æœ—æ™®ï¼šè„¸ä¹¦ã€æŽ¨ç‰¹ç­‰ç¤¾äº¤åª’ä½“åŠ©æˆ‘å…¥ä¸»ç™½å®« 11-14 23:10 2 è¯åˆ¸ 11æœˆ14æ—¥æ™šå¢žå‡æŒæ¯æ—¥é€Ÿè§ˆ 11-14 22:54 3 ç¾Žè‚¡ è´¢ç»è§‚å¯Ÿï¼šæ—¥æœ¬ä¸ºä½•æ€¥äºŽæŽ¨åŠ¨TPPæ‰¹å‡†ç¨‹åº 11-14 22:54 4 ç¾Žè‚¡ æ–°æ€»ç»Ÿè°œé¢˜ï¼šç‰¹æœ—æ™®ä¼šè¿žç»­åŠ æ¯å—ï¼Ÿ 11-14 22:52 5 è¯åˆ¸ ç¥žå·žä¸“è½¦è´¢æŠ¥é­è´¨ç–‘ å¢žå‘100äº¿è‚¡ä¸œé€€å‡ºéœ€50å¹´ 11-14 22:41 6 è¯åˆ¸ æ’å¤§é—ªç”µæ€å›žé©¬æžªé”ä»“åŠå¹´ æˆ’çŸ­ç‚’äº†å—ï¼Ÿ 11-14 22:38 7 å›½å†…è´¢ç» æ¥¼ç»§ä¼ŸåŠ›æŽ¨æ”¹é©åšæ´¾ æˆ–åŠ å¿«å›½æœ‰èµ„æœ¬åˆ’æ‹¨ç¤¾ä¿ 11-14 22:36 8 ç¾Žè‚¡ å¼€ç›˜ï¼šç¾Žè‚¡å‘¨ä¸€å°å¹…é«˜å¼€ å»¶ç»­ä¸Šå‘¨æ¶¨åŠ¿ 11-14 22:32 9 ç¾Žè‚¡ å–œè¾¾å±‹åˆ›å§‹äººï¼šå½“å¥½æ€»ç»Ÿå°±è¦èµ°ä¸­åº¸ä¹‹é“ 11-14 22:24 10 è¯åˆ¸ åŒ—äº¬é«˜åŽï¼šå°†ä¹è§†ç½‘è¯„çº§ä¸‹è°ƒè‡³ä¸­æ€§ 11-14 22:09 11 ç¾Žè‚¡ 11æœˆ14æ—¥22ç‚¹äº¤æ˜“å‘˜æ­£å…³æ³¨è¦é—» 11-14 22:02 12 ç¾Žè‚¡ æ‘©æ ¹å¤§é€šï¼šæ–°å…´å¸‚åœºè‚¡å¸‚ã€è´§å¸çš„å‰æ™¯æ‚²è§‚ 11-14 21:55 13 å›½å†…è´¢ç» äººæ°‘æ—¥æŠ¥åˆŠæ–‡è°ˆå…¨é¢æ·±åŒ–æ”¹é©è¿™ä¸‰å¹´ï¼šå•ƒä¸‹ç¡¬éª¨å¤´ 11-14 21:46 14 è¯åˆ¸ æ³½å¹³å®è§‚ï¼šç»æµŽLåž‹å»¶ç»­ åœ°äº§é”€é‡å›žè½æŠ•èµ„è¶…é¢„æœŸ 11-14 21:43 15 è¯åˆ¸ é»„ç‡•é“­ç­‰äº”å¤§åˆ¸å•†å¤§ä½¬å‘Šè¯‰ä½  2017å¹´ä¹°ç‚¹å•¥ï¼Ÿ 11-14 21:41 url 0 http://finance.sina.com.cn/stock/usstock/c/201... 1 http://finance.sina.com.cn/stock/usstock/c/201... 2 http://finance.sina.com.cn/stock/y/2016-11-14/... 3 http://finance.sina.com.cn/stock/usstock/c/201... 4 http://finance.sina.com.cn/stock/usstock/c/201... 5 http://finance.sina.com.cn/stock/marketresearc... 6 http://finance.sina.com.cn/stock/marketresearc... 7 http://finance.sina.com.cn/china/gncj/2016-11-... 8 http://finance.sina.com.cn/stock/usstock/c/201... 9 http://finance.sina.com.cn/stock/usstock/c/201... 10 http://finance.sina.com.cn/stock/s/2016-11-14/... 11 http://finance.sina.com.cn/stock/usstock/c/201... 12 http://finance.sina.com.cn/stock/usstock/c/201... 13 http://finance.sina.com.cn/china/gncj/2016-11-... 14 http://finance.sina.com.cn/stock/marketresearc... 15 http://finance.sina.com.cn/stock/marketresearc...]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Some Paper Summaries of Semantic Segmentation with Deep Learning]]></title>
      <url>%2F2018%2F04%2F10%2FSome-Paper-Summaries-of-Semantic-Segmentation-with-Deep-Learning%2F</url>
      <content type="text"><![CDATA[What exactly is semantic segmentation?Semantic segmentation is understanding an image at pixel level i.e, we want to assign each pixel in the image an object class. For example, check out the following images. Left: Input image. Right: Itâ€™s semantic segmentation. Source.Apart from recognizing the bike and the person riding it, we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models.VOC2012 and MSCOCO are the most important datasets for semantic segmentation.What are the different approaches?Before deep learning took over computer vision, people used approaches like TextonForest and Random Forest based classifiers for semantic segmentation. As with image classification, convolutional neural networks (CNN) have had enormous success on segmentation problems.One of the popular initial deep learning approaches was patch classification where each pixel was separately classified into classes using a patch of image around it. Main reason to use patches was that classification networks usually have full connected layers and therefore required fixed size images.In 2014, Fully Convolutional Networks (FCN) by Long et al. from Berkeley, popularized CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for image of any size and was also much faster compared to the patch classification approach. Almost all the subsequent state of the art approaches on semantic segmentation adopted this paradigm.Apart from fully connected layers, one of the main problems with using CNNs for segmentation is pooling layers. Pooling layers increase the field of view and are able to aggregate the context while discarding the â€˜whereâ€™ information. However, semantic segmentation requires the exact alignment of class maps and thus, needs the â€˜whereâ€™ information to be preserved. Two different classes of architectures evolved in the literature to tackle this issue.First one is encoder-decoder architecture. Encoder gradually reduces the spatial dimension with pooling layers and decoder gradually recovers the object details and spatial dimension. There are usually shortcut connections from encoder to decoder to help decoder recover the object details better. U-Net is a popular architecture from this class.U-Net: An encoder-decoder architecture. Source.Architectures in the second class use what are called as dilated/atrous convolutionsand do away with pooling layers.Dilated/atrous convolutions. rate=1 is same as normal convolutions. Source.Conditional Random Field (CRF) postprocessing are usually used to improve the segmentation. CRFs are graphical models which â€˜smoothâ€™ segmentation based on the underlying image intensities. They work based on the observation that similar intensity pixels tend to be labeled as the same class. CRFs can boost scores by 1-2%.CRF illustration. (b) Unary classifiers is the segmentation input to the CRF. (c, d, e) are variants of CRF with (e) being the widely used one. Source.In the next section, Iâ€™ll summarize a few papers that represent the evolution of segmentation architectures starting from FCN. All these architectures are benchmarked on VOC2012 evaluation server.SummariesFollowing papers are summarized (in chronological order):FCNSegNetDilated ConvolutionsDeepLab (v1 &amp; v2)RefineNetPSPNetLarge Kernel MattersDeepLab v3For each of these papers, I list down their key contributions and explain them. I also show their benchmark scores (mean IOU) on VOC2012 test dataset.FCNFully Convolutional Networks for Semantic SegmentationSubmitted on 14 Nov 2014Arxiv LinkKey Contributions:Popularize the use of end to end convolutional networks for semantic segmentationRe-purpose imagenet pretrained networks for segmentationUpsample using deconvolutional layersIntroduce skip connections to improve over the coarseness of upsamplingExplanation:Key observation is that fully connected layers in classification networks can be viewed as convolutions with kernels that cover their entire input regions. This is equivalent to evaluating the original classification network on overlapping input patches but is much more efficient because computation is shared over the overlapping regions of patches. Although this observation is not unique to this paper (see overfeat, this post), it improved the state of the art on VOC2012 significantly.Fully connected layers as a convolution. Source.After convolutionalizing fully connected layers in a imagenet pretrained network like VGG, feature maps still need to be upsampled because of pooling operations in CNNs. Instead of using simple bilinear interpolation, deconvolutional layers can learn the interpolation. This layer is also known as upconvolution, full convolution, transposed convolution or fractionally-strided convolution.However, upsampling (even with deconvolutional layers) produces coarse segmentation maps because of loss of information during pooling. Therefore, shortcut/skip connections are introduced from higher resolution feature maps.Benchmarks (VOC2012):ScoreCommentSource62.2-leaderboard67.2More momentum. Not described in paperleaderboardMy Comments:This was an important contribution but state of the art has improved a lot by now though.SegNetSegNet: A Deep Convolutional Encoder-Decoder Architecture for Image SegmentationSubmitted on 2 Nov 2015Arxiv LinkKey Contributions:Maxpooling indices transferred to decoder to improve the segmentation resolution.Explanation:FCN, despite upconvolutional layers and a few shortcut connections produces coarse segmentation maps. Therefore, more shortcut connections are introduced. However, instead of copying the encoder features as in FCN, indices from maxpooling are copied. This makes SegNet more memory efficient than FCN.Segnet Architecture. Source.Benchmarks (VOC2012):ScoreCommentSource59.9-leaderboardMy comments:FCN and SegNet are one of the first encoder-decoder architectures.Benchmarks for SegNet are not good enough to be used anymore.Dilated ConvolutionsMulti-Scale Context Aggregation by Dilated ConvolutionsSubmitted on 23 Nov 2015Arxiv LinkKey Contributions:Use dilated convolutions, a convolutional layer for dense predictions.Propose â€˜context moduleâ€™ which uses dilated convolutions for multi scale aggregation.Explanation:Pooling helps in classification networks because receptive field increases. But this is not the best thing to do for segmentation because pooling decreases the resolution. Therefore, authors use dilated convolution layer which works like this:Dilated/Atrous Convolutions. SourceDilated convolutional layer (also called as atrous convolution in DeepLab) allows for exponential increase in field of view without decrease of spatial dimensions.Last two pooling layers from pretrained classification network (here, VGG) are removed and subsequent convolutional layers are replaced with dilated convolutions. In particular, convolutions between the pool-3 and pool-4 have dilation 2 and convolutions after pool-4 have dilation 4. With this module (called frontend module in the paper), dense predictions are obtained without any increase in number of parameters.A module (called context module in the paper) is trained separately with the outputs of frontend module as inputs. This module is a cascade of dilated convolutions of different dilations so that multi scale context is aggregated and predictions from frontend are improved.Benchmarks (VOC2012):ScoreCommentSource71.3frontendreported in the paper73.5frontend + contextreported in the paper74.7frontend + context + CRFreported in the paper75.3frontend + context + CRF-RNNreported in the paperMy comments:Note that predicted segmentation mapâ€™s size is 1/8th of that of the image. This is the case with almost all the approaches. They are interpolated to get the final segmentation map.DeepLab (v1 &amp; v2)v1 : Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFsSubmitted on 22 Dec 2014Arxiv Linkv2 : DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFsSubmitted on 2 Jun 2016Arxiv LinkKey Contributions:Use atrous/dilated convolutions.Propose atrous spatial pyramid pooling (ASPP)Use Fully connected CRFExplanation:Atrous/Dilated convolutions increase the field of view without increasing the number of parameters. Net is modified like in dilated convolutions paper.Multiscale processing is achieved either by passing multiple rescaled versions of original images to parallel CNN branches (Image pyramid) and/or by using multiple parallel atrous convolutional layers with different sampling rates (ASPP).Structured prediction is done by fully connected CRF. CRF is trained/tuned separately as a post processing step.DeepLab2 Pipeline. Source.Benchmarks (VOC2012):ScoreCommentSource79.7ResNet-101 + atrous Convolutions + ASPP + CRFleaderboardRefineNetRefineNet: Multi-Path Refinement Networks for High-Resolution Semantic SegmentationSubmitted on 20 Nov 2016Arxiv LinkKey Contributions:Encoder-Decoder architecture with well thought-out decoder blocksAll the components follow residual connection designExplanation:Approach of using dilated/atrous convolutions are not without downsides. Dilated convolutions are computationally expensive and take a lot of memory because they have to be applied on large number of high resolution feature maps. This hampers the computation of high-res predictions. DeepLabâ€™s predictions, for example are 1/8th the size of original input.So, the paper proposes to use encoder-decoder architecture. Encoder part is ResNet-101 blocks. Decoder has RefineNet blocks which concatenate/fuse high resolution features from encoder and low resolution features from previous RefineNet block.RefineNet Architecture. Source.Each RefineNet block has a component to fuse the multi resolution features by upsampling the lower resolution features and a component to capture context based on repeated 5 x 5 stride 1 pool layers. Each of these components employ the residual connection design following the identity map mindset.RefineNet Block. Source.Benchmarks (VOC2012):ScoreCommentSource84.2Uses CRF, Multiscale inputs, COCO pretrainingleaderboardPSPNetPyramid Scene Parsing NetworkSubmitted on 4 Dec 2016Arxiv LinkKey Contributions:Propose pyramid pooling module to aggregate the context.Use auxiliary lossExplanation:Global scene categories matter because it provides clues on the distribution of the segmentation classes. Pyramid pooling module captures this information by applying large kernel pooling layers.Dilated convolutions are used as in dilated convolutions paper to modify Resnet and a pyramid pooling module is added to it. This module concatenates the feature maps from ResNet with upsampled output of parallel pooling layers with kernels covering whole, half of and small portions of image.An auxiliary loss, additional to the loss on main branch, is applied after the fourth stage of ResNet (i.e input to pyramid pooling module). This idea was also called as intermediate supervision elsewhere.PSPNet Architecture. Source.Benchmarks (VOC2012):ScoreCommentSource85.4MSCOCO pretraining, multi scale input, no CRFleaderboard82.6no MSCOCO pretraining, multi scale input, no CRFreported in the paperLarge Kernel MattersLarge Kernel Matters â€“ Improve Semantic Segmentation by Global Convolutional NetworkSubmitted on 8 Mar 2017Arxiv LinkKey Contributions:Propose a encoder-decoder architecture with very large kernels convolutionsExplanation:Semantic segmentation requires both segmentation and classification of the segmented objects. Since fully connected layers cannot be present in a segmentation architecture, convolutions with very large kernels are adopted instead.Another reason to adopt large kernels is that although deeper networks like ResNet have very large receptive field, studies show that the network tends to gather information from a much smaller region (valid receptive filed).Larger kernels are computationally expensive and have a lot of parameters. Therefore, k x k convolution is approximated with sum of 1 x k + k x 1 and k x 1 and 1 x k convolutions. This module is called as Global Convolutional Network (GCN) in the paper.Coming to architecture, ResNet(without any dilated convolutions) forms encoder part of the architecture while GCNs and deconvolutions form decoder. A simple residual block called Boundary Refinement (BR) is also used.GCN Architecture. Source.Benchmarks (VOC2012):ScoreCommentSource82.2-reported in the paper83.6Improved training, not described in the paperleaderboardDeepLab v3Rethinking Atrous Convolution for Semantic Image SegmentationSubmitted on 17 Jun 2017Arxiv LinkKey Contributions:Improved atrous spatial pyramid pooling (ASPP)Module which employ atrous convolutions in cascadeExplanation:ResNet model is modified to use dilated/atrous convolutions as in DeepLabv2 and dilated convolutions. Improved ASPP involves concatenation of image-level features, a 1x1 convolution and three 3x3 atrous convolutions with different rates. Batch normalization is used after each of the parallel convolutional layers.Cascaded module is a resnet block except that component convolution layers are made atrous with different rates. This module is similar to context module used in dilated convolutions paper but this is applied directly on intermediate feature maps instead of belief maps (belief maps are final CNN feature maps with channels equal to number of classes).Both the proposed models are evaluated independently and attempt to combine the both did not improve the performance. Both of them performed very similarly on val set with ASPP performing slightly better. CRF is not used.Both these models outperform the best model from DeepLabv2. Authors note that the improvement comes from the batch normalization and better way to encode multi scale context.DeepLabv3 ASPP (used for submission). Source.Benchmarks (VOC2012):ScoreCommentSource85.7used ASPP (no cascaded modules)leaderboardReblog from here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN]]></title>
      <url>%2F2018%2F04%2F10%2FA-Brief-History-of-CNNs-in-Image-Segmentation-From-R-CNN-to-Mask-R-CNN%2F</url>
      <content type="text"><![CDATA[Ever since Alex Krizhevsky, Geoff Hinton, and Ilya Sutskever won ImageNet in 2012, Convolutional Neural Networks(CNNs) have become the gold standard for image classification. In fact, since then, CNNs have improved to the point where they now outperform humans on the ImageNet challenge!CNNs now outperform humans on the ImageNet challenge. The y-axis in the above graph is the error rate on ImageNet.While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.An example of an image used in the classification challenge. Note how the image is well framed and has just one object.In classification, thereâ€™s generally an image with a single object as the focus and the task is to say what that image is (see above). But when we look at the world around us, we carry out far more complex tasks.Sights in real life are often composed of a multitude of different, overlapping objects, backgrounds, and actions.We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!In image segmentation, our goal is to classify the different objects in the image, and identify their boundaries. Source: Mask R-CNN paper.Can CNNs help us with such complex tasks? Namely, given a more complicated image, can we use CNNs to identify the different objects in the image, and their boundaries? As has been shown by Ross Girshick and his peers over the last few years, the answer is conclusively yes.Goals of this PostThrough this post, weâ€™ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how theyâ€™ve evolved from one implementation to the next. In particular, weâ€™ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Finally, weâ€™ll cover Mask R-CNN, a paper released recently by Facebook Research that extends such object detection techniques to provide pixel level segmentation. Here are the papers referenced in this post:R-CNN: https://arxiv.org/abs/1311.2524Fast R-CNN: https://arxiv.org/abs/1504.08083Faster R-CNN: https://arxiv.org/abs/1506.01497Mask R-CNN: https://arxiv.org/abs/1703.068702014: R-CNN - An Early Application of CNNs to Object DetectionObject detection algorithms such as R-CNN take in an image and identify the locations and classifications of the main objects in the image. Source: https://arxiv.org/abs/1311.2524.Inspired by the research of Hintonâ€™s lab at the University of Toronto, a small team at UC Berkeley, led by Professor Jitendra Malik, asked themselves what today seems like an inevitable question:To what extent do [Krizhevsky et. alâ€™s results] generalize to object detection?Object detection is the task of finding the different objects in an image and classifying them (as seen in the image above). The team, comprised of Ross Girshick (a name weâ€™ll see again), Jeff Donahue, and Trevor Darrel found that this problem can be solved with Krizhevskyâ€™s results by testing on the PASCAL VOC Challenge, a popular object detection challenge akin to ImageNet. They write,This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features.Letâ€™s now take a moment to understand how their architecture, Regions With CNNs (R-CNN) works.Understanding R-CNNThe goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image.Inputs: ImageOutputs: Bounding boxes + labels for each object in the image.But how do we find out where these bounding boxes are? R-CNN does what we might intuitively do as well - propose a bunch of boxes in the image and see if any of them actually correspond to an object.Selective Search looks through windows of multiple scales and looks for adjacent pixels that share textures, colors, or intensities. Image source: https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdfR-CNN creates these bounding boxes, or region proposals, using a process called Selective Search which you can read about here. At a high level, Selective Search (shown in the image above) looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.After creating a set of region proposals, R-CNN passes the image through a modified version of AlexNet to determine whether or not it is a valid region. Source: https://arxiv.org/abs/1311.2524.Once the proposals are created, R-CNN warps the region to a standard square size and passes it through to a modified version of AlexNet (the winning submission to ImageNet 2012 that inspired R-CNN), as shown above.On the final layer of the CNN, R-CNN adds a Support Vector Machine (SVM) that simply classifies whether this is an object, and if so what object. This is step 4 in the image above.Improving the Bounding BoxesNow, having found the object in the box, can we tighten the box to fit the true dimensions of the object? We can, and this is the final step of R-CNN. R-CNN runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result. Here are the inputs and outputs of this regression model:Inputs: sub-regions of the image corresponding to objects.Outputs: New bounding box coordinates for the object in the sub-region.So, to summarize, R-CNN is just the following steps:Generate a set of proposals for bounding boxes.Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is.Run the box through a linear regression model to output tighter coordinates for the box once the object has been classified.2015: Fast R-CNN - Speeding up and Simplifying R-CNNRoss Girshick wrote both R-CNN and Fast R-CNN. He continues to push the boundaries of Computer Vision at Facebook Research.R-CNN works really well, but is really quite slow for a few simple reasons:It requires a forward pass of the CNN (AlexNet) for every single region proposal for every single image (thatâ€™s around 2000 forward passes per image!).It has to train three different models separately - the CNN to generate image features, the classifier that predicts the class, and the regression model to tighten the bounding boxes. This makes the pipeline extremely hard to train.In 2015, Ross Girshick, the first author of R-CNN, solved both these problems, leading to the second algorithm in our short history - Fast R-CNN. Letâ€™s now go over its main insights.Fast R-CNN Insight 1: RoI (Region of Interest) PoolingFor the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). His insight was simple â€” Why not run the CNN just once per image and then find a way to share that computation across the ~2000 proposals?In RoIPool, a full forward pass of the image is created and the conv features for each region of interest are extracted from the resulting forward pass. Source: Stanfordâ€™s CS231N slides by Fei Fei Li, Andrei Karpathy, and Justin Johnson.This is exactly what Fast R-CNN does using a technique known as RoIPool (Region of Interest Pooling). At its core, RoIPool shares the forward pass of a CNN for an image across its subregions. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNNâ€™s feature map. Then, the features in each region are pooled (usually using max pooling). So all it takes us is one pass of the original image as opposed to ~2000!Fast R-CNN Insight 2: Combine All Models into One NetworkFast R-CNN combined the CNN, classifier, and bounding box regressor into one, single network. Source: https://www.slideshare.net/simplyinsimple/detection-52781995.The second insight of Fast R-CNN is to jointly train the CNN, classifier, and bounding box regressor in a single model. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), Fast R-CNN instead used a single network to compute all three.You can see how this was done in the image above. Fast R-CNN replaced the SVM classifier with a softmax layer on top of the CNN to output a classification. It also added a linear regression layer parallel to the softmax layer to output bounding box coordinates. In this way, all the outputs needed came from one single network! Here are the inputs and outputs to this overall model:Inputs: Images with region proposals.Outputs: Object classifications of each region along with tighter bounding boxes.2016: Faster R-CNN - Speeding Up Region ProposalEven with all these advancements, there was still one remaining bottleneck in the Fast R-CNN process â€” the region proposer. As we saw, the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test. In Fast R-CNN, these proposals were created using Selective Search, a fairly slow process that was found to be the bottleneck of the overall process.Jian Sun, a principal researcher at Microsoft Research, led the team behind Faster R-CNN. Source: https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdpIn the middle 2015, a team at Microsoft Research composed of Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, found a way to make the region proposal step almost cost free through an architecture they (creatively) named Faster R-CNN.The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). So why not reuse those same CNN results for region proposals instead of running a separate selective search algorithm?In Faster R-CNN, a single CNN is used for region proposals, and classifications. Source: https://arxiv.org/abs/1506.01497.Indeed, this is just what the Faster R-CNN team achieved. In the image above, you can see how a single CNN is used to both carry out region proposals and classification. This way, only one CNN needs to be trained and we get region proposals almost for free! The authors write:Our observation is that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region proposals [thus enabling nearly cost-free region proposals].Here are the inputs and outputs of their model:Inputs: Images (Notice how region proposals are not needed).Outputs: Classifications and bounding box coordinates of objects in the images.How the Regions are GeneratedLetâ€™s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Faster R-CNN adds a Fully Convolutional Network on top of the features of the CNN creating whatâ€™s known as the Region Proposal Network.The Region Proposal Network slides a window over the features of the CNN. At each window location, the network outputs a score and a bounding box per anchor (hence 4k box coordinates where k is the number of anchors). Source: https://arxiv.org/abs/1506.01497.The Region Proposal Network works by passing a sliding window over the CNN feature map and at each window, outputting k potential bounding boxes and scores for how good each of those boxes is expected to be. What do these k boxes represent?We know that the bounding boxes for people tend to be rectangular and vertical. We can use this intuition to guide our Region Proposal networks through creating an anchor of such dimensions. Image Source: http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignments/assignment1/bbox0062.jpg.Intuitively, we know that objects in an image should fit certain common aspect ratios and sizes. For instance, we know that we want some rectangular boxes that resemble the shapes of humans. Likewise, we know we wonâ€™t see many boxes that are very very thin. In such a way, we create k such common aspect ratios we call anchor boxes. For each such anchor box, we output one bounding box and score per position in the image.With these anchor boxes in mind, letâ€™s take a look at the inputs and outputs to this Region Proposal Network:Inputs: CNN Feature Map.Outputs: A bounding box per anchor. A score representing how likely the image in that bounding box will be an object.We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes.2017: Mask R-CNN - Extending Faster R-CNN for Pixel Level SegmentationThe goal of image instance segmentation is to identify, at a pixel level, what the different objets in a scene are. Source: https://arxiv.org/abs/1703.06870.So far, weâ€™ve seen how weâ€™ve been able to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes.Can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes? This problem, known as image segmentation, is what Kaiming He and a team of researchers, including Girshick, explored at Facebook AI using an architecture known as Mask R-CNN.Kaiming He, a researcher at Facebook AI, is lead author of Mask R-CNN and also a coauthor of Faster R-CNN.Much like Fast R-CNN, and Faster R-CNN, Mask R-CNNâ€™s underlying intuition is straight forward. Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel level segmentation?In Mask R-CNN, a Fully Convolutional Network (FCN) is added on top of the CNN features of Faster R-CNN to generate a mask (segmentation output). Notice how this is in parallel to the classification and bounding box regression network of Faster R-CNN. Source: https://arxiv.org/abs/1703.06870.Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch (in white in the above image), as before, is just a Fully Convolutional Network on top of a CNN based feature map. Here are its inputs and outputs:Inputs: CNN Feature Map.Outputs: Matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a binary mask).But the Mask R-CNN authors had to make one small adjustment to make this pipeline work as expected.RoiAlign - Realigning RoIPool to be More AccurateInstead of RoIPool, the image gets passed through RoIAlign so that the regions of the feature map selected by RoIPool correspond more precisely to the regions of the original image. This is needed because pixel level segmentation requires more fine-grained alignment than bounding boxes. Source: https://arxiv.org/abs/1703.06870.When run without modifications on the original Faster R-CNN architecture, the Mask R-CNN authors realized that the regions of the feature map selected by RoIPool were slightly misaligned from the regions of the original image. Since image segmentation requires pixel level specificity, unlike bounding boxes, this naturally led to inaccuracies.The authors were able to solve this problem by cleverly adjusting RoIPool to be more precisely aligned using a method known as RoIAlign.How do we accurately map a region of interest from the original image onto the feature map?Imagine we have an image of size 128x128 and a feature map of size 25x25. Letâ€™s imagine we want features the region corresponding to the top-left 15x15pixels in the original image (see above). How might we select these pixels from the feature map?We know each pixel in the original image corresponds to ~ 25/128 pixels in the feature map. To select 15 pixels from the original image, we just select 15 25/128 ~= *2.93 pixels.In RoIPool, we would round this down and select 2 pixels causing a slight misalignment. However, in RoIAlign, we avoid such rounding. Instead, we use bilinear interpolation to get a precise idea of what would be at pixel 2.93. This, at a high level, is what allows us to avoid the misalignments caused by RoIPool.Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:Mask R-CNN is able to segment as well as classify the objects in an image. Source: https://arxiv.org/abs/1703.06870.CodeIf youâ€™re interested in trying out these algorithms yourselves, here are relevant repositories:Faster R-CNNCaffe: https://github.com/rbgirshick/py-faster-rcnnPyTorch: https://github.com/longcw/faster_rcnn_pytorchMatLab: https://github.com/ShaoqingRen/faster_rcnnMask R-CNNPyTorch: https://github.com/felixgwu/mask_rcnn_pytorchTensorFlow: https://github.com/CharlesShang/FastMaskRCNNReblog from here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Understanding nested list comprehension syntax in Python]]></title>
      <url>%2F2018%2F03%2F27%2FUnderstanding-nested-list-comprehension-syntax-in-Python%2F</url>
      <content type="text"><![CDATA[List comprehensions are one of the really nice and powerful features of Python. It is actually a smart way to introduce new users to functional programming concepts (after all a list comprehension is just a combination of map and filter) and compact statements.However, one thing that always troubled me when using list comprehensions is their non intuitive syntax when nesting was needed. For example, letâ€™s say that we just want to flatten a list of lists using a nested list comprehension:1non_flat = [ [1,2,3], [4,5,6], [7,8] ]To write that, somebody would think: For a simple list comprehension I need to write [ x for x in non_flat ] to get all its items - however I want to retrieve each element of the x list so Iâ€™ll write something like this:12&gt;&gt;&gt; [y for y in x for x in non_flat][7, 7, 7, 8, 8, 8]Well duh! At this time Iâ€™d need research google for a working list comprehension syntax and adjust it to my needs (or give up and write it as a double for loop).Hereâ€™s the correct nested list comprehension people wondering:12&gt;&gt;&gt; [y for x in non_flat for y in x][1, 2, 3, 4, 5, 6, 7, 8]What if I wanted to add a third level of nesting or an if? Well Iâ€™d just bite the bullet and use for loops!However, if you take a look at the document describing list comprehensions in python (PEP202) youâ€™ll see the following phrase:It is proposed to allow conditional construction of list literals using for and if clauses. They would nest in the same way for loops and if statements nest now.This statement explains everything! Just think in for-loops syntax. So, If I used for loops for the previous flattening, Iâ€™d do something like:123for x in non_flat: for y in x: ywhich, if y is moved to the front and joined in one line would be the correct nested list comprehension!So thatâ€™s the wayâ€¦ What If I wanted to include only lists with more than 2 elements in the flattening (so [7,8] should not be included)? Iâ€™ll write it with for loops first:1234for x in non_flat: if len(x) &gt; 2 for y in x: yso by convering this to list comprehension we get:12&gt;&gt;&gt; [ y for x in non_flat if len(x) &gt; 2 for y in x ][1, 2, 3, 4, 5, 6]Success!One final, more complex example: Letâ€™s say that we have a list of lists of words and we want to get a list of all the letters of these words along with the index of the list they belong to but only for words with more than two characters. Using the same for-loop syntax for the nested list comprehensions weâ€™ll get:123&gt;&gt;&gt; strings = [ ['foo', 'bar'], ['baz', 'taz'], ['w', 'koko'] ]&gt;&gt;&gt; [ (letter, idx) for idx, lst in enumerate(strings) for word in lst if len(word)&gt;2 for letter in word][('f', 0), ('o', 0), ('o', 0), ('b', 0), ('a', 0), ('r', 0), ('b', 1), ('a', 1), ('z', 1), ('t', 1), ('a', 1), ('z', 1), ('k', 2), ('o', 2), ('k', 2), ('o', 2)]source blog is here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Pythonå¤šæ ¸ç¼–ç¨‹mpi4pyå®žè·µ]]></title>
      <url>%2F2018%2F03%2F19%2FPython%E5%A4%9A%E6%A0%B8%E7%BC%96%E7%A8%8Bmpi4py%E5%AE%9E%E8%B7%B5%2F</url>
      <content type="text"><![CDATA[è½¬è½½è‡ªè¿™ç¯‡åšæ–‡.æ¦‚è¿°â€‹ CPUä»Žä¸‰åå¤šå¹´å‰çš„8086ï¼Œåˆ°åå¹´å‰çš„å¥”è…¾ï¼Œå†åˆ°å½“ä¸‹çš„å¤šæ ¸i7ã€‚ä¸€å¼€å§‹ï¼Œä»¥å•æ ¸cpuçš„ä¸»é¢‘ä¸ºç›®æ ‡ï¼Œæž¶æž„çš„æ”¹è‰¯å’Œé›†æˆç”µè·¯å·¥è‰ºçš„è¿›æ­¥ä½¿å¾—cpuçš„æ€§èƒ½é«˜é€Ÿä¸Šå‡ï¼Œå•æ ¸cpuçš„ä¸»é¢‘ä»Žè€çˆ·è½¦çš„MHzé˜¶æ®µä¸€åº¦æŽ¥è¿‘4GHzé«˜åœ°ã€‚ç„¶è€Œï¼Œä¹Ÿå› ä¸ºå·¥è‰ºå’ŒåŠŸè€—ç­‰çš„é™åˆ¶ï¼Œå•æ ¸cpué‡åˆ°äº†äººç”Ÿçš„å¤©èŠ±æ¿ï¼Œæ€¥éœ€è½¬æ¢æ€ç»´ï¼Œä»¥æ»¡è¶³æ— æ­¢å¢ƒçš„æ€§èƒ½éœ€æ±‚ã€‚å¤šæ ¸cpuåœ¨æ­¤ç™»ä¸ŠåŽ†å²èˆžå°ã€‚ç»™ä½ çš„è€çˆ·è½¦å¤šåŠ ä¸¤ä¸ªå¼•æ“Žï¼Œè®©ä½ æœ‰æ³•æ‹‰åˆ©çš„æ„Ÿè§‰ã€‚çŽ°æ—¶ä»£ï¼Œè¿žæ‰‹æœºéƒ½åˆ°å¤„å«åš£è‡ªå·±æœ‰4æ ¸8æ ¸å¤„ç†å™¨çš„æ—¶ä»£ï¼ŒPCå°±æ›´ä¸ç”¨è¯´äº†ã€‚â€‹ æ‰¯è¿œäº†ï¼Œanywayï¼Œå¯¹äºŽä¿ºä»¬ç¨‹åºå‘˜æ¥è¯´ï¼Œå¦‚ä½•åˆ©ç”¨å¦‚æ­¤å¼ºå¤§çš„å¼•æ“Žå®Œæˆæˆ‘ä»¬çš„ä»»åŠ¡æ‰æ˜¯æˆ‘ä»¬è¦è€ƒè™‘çš„ã€‚éšç€å¤§è§„æ¨¡æ•°æ®å¤„ç†ã€å¤§è§„æ¨¡é—®é¢˜å’Œå¤æ‚ç³»ç»Ÿæ±‚è§£éœ€æ±‚çš„å¢žåŠ ï¼Œä»¥å‰çš„å•æ ¸ç¼–ç¨‹å·²ç»æœ‰å¿ƒæ— åŠ›äº†ã€‚å¦‚æžœç¨‹åºä¸€è·‘å°±å¾—å‡ ä¸ªå°æ—¶ï¼Œç”šè‡³ä¸€å¤©ï¼Œæƒ³æƒ³éƒ½æ— æ³•åŽŸè°…è‡ªå·±ã€‚é‚£å¦‚ä½•è®©è‡ªå·±æ›´å¿«çš„è¿‡åº¦åˆ°é«˜å¤§ä¸Šçš„å¤šæ ¸å¹¶è¡Œç¼–ç¨‹ä¸­åŽ»å‘¢ï¼Ÿå“ˆå“ˆï¼Œå¹¿å¤§äººæ°‘çš„åŠ›é‡ï¼â€‹ ç›®å‰å·¥ä½œä¸­æˆ‘æ‰€æŽ¥è§¦åˆ°çš„å¹¶è¡Œå¤„ç†æ¡†æž¶ä¸»è¦æœ‰MPIã€OpenMPå’ŒMapReduce(Hadoop)ä¸‰ä¸ªï¼ˆCUDAå±žäºŽGPUå¹¶è¡Œç¼–ç¨‹ï¼Œè¿™é‡Œä¸æåŠï¼‰ã€‚MPIå’ŒHadoopéƒ½å¯ä»¥åœ¨é›†ç¾¤ä¸­è¿è¡Œï¼Œè€ŒOpenMPå› ä¸ºå…±äº«å­˜å‚¨ç»“æž„çš„å…³ç³»ï¼Œä¸èƒ½åœ¨é›†ç¾¤ä¸Šè¿è¡Œï¼Œåªèƒ½å•æœºã€‚å¦å¤–ï¼ŒMPIå¯ä»¥è®©æ•°æ®ä¿ç•™åœ¨å†…å­˜ä¸­ï¼Œå¯ä»¥ä¸ºèŠ‚ç‚¹é—´çš„é€šä¿¡å’Œæ•°æ®äº¤äº’ä¿å­˜ä¸Šä¸‹æ–‡ï¼Œæ‰€ä»¥èƒ½æ‰§è¡Œè¿­ä»£ç®—æ³•ï¼Œè€ŒHadoopå´ä¸å…·æœ‰è¿™ä¸ªç‰¹æ€§ã€‚å› æ­¤ï¼Œéœ€è¦è¿­ä»£çš„æœºå™¨å­¦ä¹ ç®—æ³•å¤§å¤šä½¿ç”¨MPIæ¥å®žçŽ°ã€‚å½“ç„¶äº†ï¼Œéƒ¨åˆ†æœºå™¨å­¦ä¹ ç®—æ³•ä¹Ÿæ˜¯å¯ä»¥é€šè¿‡è®¾è®¡ä½¿ç”¨Hadoopæ¥å®Œæˆçš„ã€‚ï¼ˆæµ…è§ï¼Œå¦‚æžœé”™è¯¯ï¼Œå¸Œæœ›å„ä½ä¸åæŒ‡å‡ºï¼Œè°¢è°¢ï¼‰ã€‚â€‹ æœ¬æ–‡ä¸»è¦ä»‹ç»PythonçŽ¯å¢ƒä¸‹MPIç¼–ç¨‹çš„å®žè·µåŸºç¡€ã€‚MPIä¸Žmpi4pyâ€‹ MPIæ˜¯Message Passing Interfaceçš„ç®€ç§°ï¼Œä¹Ÿå°±æ˜¯æ¶ˆæ¯ä¼ é€’ã€‚æ¶ˆæ¯ä¼ é€’æŒ‡çš„æ˜¯å¹¶è¡Œæ‰§è¡Œçš„å„ä¸ªè¿›ç¨‹å…·æœ‰è‡ªå·±ç‹¬ç«‹çš„å †æ ˆå’Œä»£ç æ®µï¼Œä½œä¸ºäº’ä¸ç›¸å…³çš„å¤šä¸ªç¨‹åºç‹¬ç«‹æ‰§è¡Œï¼Œè¿›ç¨‹ä¹‹é—´çš„ä¿¡æ¯äº¤äº’å®Œå…¨é€šè¿‡æ˜¾ç¤ºåœ°è°ƒç”¨é€šä¿¡å‡½æ•°æ¥å®Œæˆã€‚â€‹ Mpi4pyæ˜¯æž„å»ºåœ¨mpiä¹‹ä¸Šçš„pythonåº“ï¼Œä½¿å¾—pythonçš„æ•°æ®ç»“æž„å¯ä»¥åœ¨è¿›ç¨‹ï¼ˆæˆ–è€…å¤šä¸ªcpuï¼‰ä¹‹é—´è¿›è¡Œä¼ é€’ã€‚MPIçš„å·¥ä½œæ–¹å¼â€‹ å¾ˆç®€å•ï¼Œå°±æ˜¯ä½ å¯åŠ¨äº†ä¸€ç»„MPIè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æ˜¯æ‰§è¡ŒåŒæ ·çš„ä»£ç ï¼ç„¶åŽæ¯ä¸ªè¿›ç¨‹éƒ½æœ‰ä¸€ä¸ªIDï¼Œä¹Ÿå°±æ˜¯rankæ¥æ ‡è®°æˆ‘æ˜¯è°ã€‚ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿå‡è®¾ä¸€ä¸ªCPUæ˜¯ä½ è¯·çš„ä¸€ä¸ªå·¥äººï¼Œå…±æœ‰10ä¸ªå·¥äººã€‚ä½ æœ‰100å—ç –å¤´è¦æ¬ï¼Œç„¶åŽå¾ˆå…¬å¹³ï¼Œè®©æ¯ä¸ªå·¥äººæ¬10å—ã€‚è¿™æ—¶å€™ï¼Œä½ æŠŠä»»åŠ¡å†™åˆ°ä¸€ä¸ªä»»åŠ¡å¡é‡Œé¢ï¼Œè®©10ä¸ªå·¥äººéƒ½æ‰§è¡Œè¿™ä¸ªä»»åŠ¡å¡ä¸­çš„ä»»åŠ¡ï¼Œä¹Ÿå°±æ˜¯æ¬ç –ï¼è¿™ä¸ªä»»åŠ¡å¡ä¸­çš„â€œæ¬ç –â€å°±æ˜¯ä½ å†™çš„ä»£ç ã€‚ç„¶åŽ10ä¸ªCPUæ‰§è¡ŒåŒä¸€æ®µä»£ç ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä»£ç é‡Œé¢çš„æ‰€æœ‰å˜é‡éƒ½æ˜¯æ¯ä¸ªè¿›ç¨‹ç‹¬æœ‰çš„ï¼Œè™½ç„¶åå­—ç›¸åŒã€‚â€‹ ä¾‹å¦‚ï¼Œä¸€ä¸ªè„šæœ¬test.pyï¼Œé‡Œé¢åŒ…å«ä»¥ä¸‹ä»£ç ï¼š123from mpi4py import MPI print("hello world'') print("my rank is: %d" %MPI.rank)â€‹ ç„¶åŽæˆ‘ä»¬åœ¨å‘½ä»¤è¡Œé€šè¿‡ä»¥ä¸‹æ–¹å¼è¿è¡Œï¼šâ€‹ mpirun â€“np 5 python test.pyâ€‹ -np5 æŒ‡å®šå¯åŠ¨5ä¸ªmpiè¿›ç¨‹æ¥æ‰§è¡ŒåŽé¢çš„ç¨‹åºã€‚ç›¸å½“äºŽå¯¹è„šæœ¬æ‹·è´äº†5ä»½ï¼Œæ¯ä¸ªè¿›ç¨‹è¿è¡Œä¸€ä»½ï¼Œäº’ä¸å¹²æ‰°ã€‚åœ¨è¿è¡Œçš„æ—¶å€™ä»£ç é‡Œé¢å”¯ä¸€çš„ä¸åŒï¼Œå°±æ˜¯å„è‡ªçš„rankä¹Ÿå°±æ˜¯IDä¸ä¸€æ ·ã€‚æ‰€ä»¥è¿™ä¸ªä»£ç å°±ä¼šæ‰“å°5ä¸ªhello worldå’Œ5ä¸ªä¸åŒçš„rankå€¼ï¼Œä»Ž0åˆ°4.ç‚¹å¯¹ç‚¹é€šä¿¡â€‹ ç‚¹å¯¹ç‚¹é€šä¿¡ï¼ˆPoint-to-PointCommunicationï¼‰çš„èƒ½åŠ›æ˜¯ä¿¡æ¯ä¼ é€’ç³»ç»Ÿæœ€åŸºæœ¬çš„è¦æ±‚ã€‚æ„æ€å°±æ˜¯è®©ä¸¤ä¸ªè¿›ç¨‹ç›´æŽ¥å¯ä»¥ä¼ è¾“æ•°æ®ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªå‘é€æ•°æ®ï¼Œå¦ä¸€ä¸ªæŽ¥æ”¶æ•°æ®ã€‚æŽ¥å£å°±ä¸¤ä¸ªï¼Œsendå’Œrecvï¼Œæ¥ä¸ªä¾‹å­ï¼š123456789101112import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() # point to point communication data_send = [comm_rank]*5 comm.send(data_send,dest=(comm_rank+1)%comm_size) data_recv =comm.recv(source=(comm_rank-1)%comm_size) print("my rank is %d, and Ireceived:" % comm_rank) print data_recvâ€‹ å¯åŠ¨5ä¸ªè¿›ç¨‹è¿è¡Œä»¥ä¸Šä»£ç ï¼Œç»“æžœå¦‚ä¸‹ï¼š12345678910my rank is 0, and I received: [4, 4, 4, 4, 4] my rank is 1, and I received: [0, 0, 0, 0, 0] my rank is 2, and I received: [1, 1, 1, 1, 1] my rank is 3, and I received: [2, 2, 2, 2, 2] my rank is 4, and I received: [3, 3, 3, 3, 3]â€‹ å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½åˆ›å»ºäº†ä¸€ä¸ªæ•°ç»„ï¼Œç„¶åŽæŠŠå®ƒä¼ é€’ç»™ä¸‹ä¸€ä¸ªè¿›ç¨‹ï¼Œæœ€åŽçš„é‚£ä¸ªè¿›ç¨‹ä¼ é€’ç»™ç¬¬ä¸€ä¸ªè¿›ç¨‹ã€‚comm_sizeå°±æ˜¯mpiçš„è¿›ç¨‹ä¸ªæ•°ï¼Œä¹Ÿå°±æ˜¯-npæŒ‡å®šçš„é‚£ä¸ªæ•°ã€‚MPI.COMM_WORLDè¡¨ç¤ºè¿›ç¨‹æ‰€åœ¨çš„é€šä¿¡ç»„ã€‚â€‹ ä½†è¿™é‡Œé¢æœ‰ä¸ªéœ€è¦æ³¨æ„çš„é—®é¢˜ï¼Œå¦‚æžœæˆ‘ä»¬è¦å‘é€çš„æ•°æ®æ¯”è¾ƒå°çš„è¯ï¼Œmpiä¼šç¼“å­˜æˆ‘ä»¬çš„æ•°æ®ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰§è¡Œåˆ°sendè¿™ä¸ªä»£ç çš„æ—¶å€™ï¼Œä¼šç¼“å­˜è¢«sendçš„æ•°æ®ï¼Œç„¶åŽç»§ç»­æ‰§è¡ŒåŽé¢çš„æŒ‡ä»¤ï¼Œè€Œä¸ä¼šç­‰å¾…å¯¹æ–¹è¿›ç¨‹æ‰§è¡ŒrecvæŒ‡ä»¤æŽ¥æ”¶å®Œè¿™ä¸ªæ•°æ®ã€‚ä½†æ˜¯ï¼Œå¦‚æžœè¦å‘é€çš„æ•°æ®å¾ˆå¤§ï¼Œé‚£ä¹ˆè¿›ç¨‹å°±æ˜¯æŒ‚èµ·ç­‰å¾…ï¼Œç›´åˆ°æŽ¥æ”¶è¿›ç¨‹æ‰§è¡Œäº†recvæŒ‡ä»¤æŽ¥æ”¶äº†è¿™ä¸ªæ•°æ®ï¼Œè¿›ç¨‹æ‰ç»§ç»­å¾€ä¸‹æ‰§è¡Œã€‚æ‰€ä»¥ä¸Šè¿°çš„ä»£ç å‘é€[rank]5æ²¡å•¥é—®é¢˜ï¼Œå¦‚æžœå‘é€[rank]500ç¨‹åºå°±ä¼šåŠæ­»ä¸æ´»çš„æ ·å­äº†ã€‚å› ä¸ºæ‰€æœ‰çš„è¿›ç¨‹éƒ½ä¼šå¡åœ¨å‘é€è¿™æ¡æŒ‡ä»¤ï¼Œç­‰å¾…ä¸‹ä¸€ä¸ªè¿›ç¨‹å‘èµ·æŽ¥æ”¶çš„è¿™ä¸ªæŒ‡ä»¤ï¼Œä½†æ˜¯è¿›ç¨‹æ˜¯æ‰§è¡Œå®Œå‘é€çš„æŒ‡ä»¤æ‰èƒ½æ‰§è¡ŒæŽ¥æ”¶çš„æŒ‡ä»¤ï¼Œè¿™å°±å’Œæ­»é”å·®ä¸å¤šäº†ã€‚æ‰€ä»¥ä¸€èˆ¬ï¼Œæˆ‘ä»¬å°†å…¶ä¿®æ”¹æˆä»¥ä¸‹çš„æ–¹å¼ï¼š12345678910111213141516import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() data_send = [comm_rank]*5 if comm_rank == 0: comm.send(data_send, dest=(comm_rank+1)%comm_size) if comm_rank &gt; 0: data_recv = comm.recv(source=(comm_rank-1)%comm_size) comm.send(data_send, dest=(comm_rank+1)%comm_size) if comm_rank == 0: data_recv = comm.recv(source=(comm_rank-1)%comm_size) print("my rank is %d, and Ireceived:" % comm_rank) print data_recvâ€‹ ç¬¬ä¸€ä¸ªè¿›ç¨‹ä¸€å¼€å§‹å°±å‘é€æ•°æ®ï¼Œå…¶ä»–è¿›ç¨‹ä¸€å¼€å§‹éƒ½æ˜¯åœ¨ç­‰å¾…æŽ¥æ”¶æ•°æ®ï¼Œè¿™æ—¶å€™è¿›ç¨‹1æŽ¥æ”¶äº†è¿›ç¨‹0çš„æ•°æ®ï¼Œç„¶åŽå‘é€è¿›ç¨‹1çš„æ•°æ®ï¼Œè¿›ç¨‹2æŽ¥æ”¶äº†ï¼Œå†å‘é€è¿›ç¨‹2çš„æ•°æ®â€¦â€¦çŸ¥é“æœ€åŽè¿›ç¨‹0æŽ¥æ”¶æœ€åŽä¸€ä¸ªè¿›ç¨‹çš„æ•°æ®ï¼Œä»Žè€Œé¿å…äº†ä¸Šè¿°é—®é¢˜ã€‚â€‹ ä¸€ä¸ªæ¯”è¾ƒå¸¸ç”¨çš„æ–¹æ³•æ˜¯å°ä¸€ä¸ªç»„é•¿ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªä¸»è¿›ç¨‹ï¼Œä¸€èˆ¬æ˜¯è¿›ç¨‹0ä½œä¸ºä¸»è¿›ç¨‹leaderã€‚ä¸»è¿›ç¨‹å°†æ•°æ®å‘é€ç»™å…¶ä»–çš„è¿›ç¨‹ï¼Œå…¶ä»–çš„è¿›ç¨‹å¤„ç†æ•°æ®ï¼Œç„¶åŽè¿”å›žç»“æžœç»™è¿›ç¨‹0ã€‚æ¢å¥è¯è¯´ï¼Œå°±æ˜¯è¿›ç¨‹0æ¥æŽ§åˆ¶æ•´ä¸ªæ•°æ®å¤„ç†æµç¨‹ã€‚ç¾¤ä½“é€šä¿¡â€‹ ç‚¹å¯¹ç‚¹é€šä¿¡æ˜¯Aå‘é€ç»™Bï¼Œä¸€ä¸ªäººå°†è‡ªå·±çš„ç§˜å¯†å‘Šè¯‰å¦ä¸€ä¸ªäººï¼Œç¾¤ä½“é€šä¿¡ï¼ˆCollective Communicationsï¼‰åƒæ˜¯æ‹¿ä¸ªå¤§å–‡å­ï¼Œä¸€æ¬¡æ€§å‘Šè¯‰æ‰€æœ‰çš„äººã€‚å‰è€…æ˜¯ä¸€å¯¹ä¸€ï¼ŒåŽè€…æ˜¯ä¸€å¯¹å¤šã€‚ä½†æ˜¯ï¼Œç¾¤ä½“é€šä¿¡æ˜¯ä»¥æ›´æœ‰æ•ˆçš„æ–¹å¼å·¥ä½œçš„ã€‚å®ƒçš„åŽŸåˆ™å°±ä¸€ä¸ªï¼šå°½é‡æŠŠæ‰€æœ‰çš„è¿›ç¨‹åœ¨æ‰€æœ‰çš„æ—¶åˆ»éƒ½ä½¿ç”¨ä¸Šï¼æˆ‘ä»¬åœ¨ä¸‹é¢çš„bcastå°èŠ‚è®²è¿°ã€‚â€‹ ç¾¤ä½“é€šä¿¡è¿˜æ˜¯å‘é€å’ŒæŽ¥æ”¶ä¸¤ç±»ï¼Œä¸€ä¸ªæ˜¯ä¸€æ¬¡æ€§æŠŠæ•°æ®å‘ç»™æ‰€æœ‰äººï¼Œå¦ä¸€ä¸ªæ˜¯ä¸€æ¬¡æ€§ä»Žæ‰€æœ‰äººé‚£é‡Œå›žæ”¶ç»“æžœã€‚å¹¿æ’­bcastâ€‹ å°†ä¸€ä»½æ•°æ®å‘é€ç»™æ‰€æœ‰çš„è¿›ç¨‹ã€‚ä¾‹å¦‚æˆ‘æœ‰200ä»½æ•°æ®ï¼Œæœ‰10ä¸ªè¿›ç¨‹ï¼Œé‚£ä¹ˆæ¯ä¸ªè¿›ç¨‹éƒ½ä¼šå¾—åˆ°è¿™200ä»½æ•°æ®ã€‚1234567891011import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() if comm_rank == 0: data = range(comm_size) data = comm.bcast(data if comm_rank == 0else None, root=0) print 'rank %d, got:' % (comm_rank) print dataâ€‹ ç»“æžœå¦‚ä¸‹ï¼š12345678910rank 0, got: [0, 1, 2, 3, 4] rank 1, got: [0, 1, 2, 3, 4] rank 2, got: [0, 1, 2, 3, 4] rank 3, got: [0, 1, 2, 3, 4] rank 4, got: [0, 1, 2, 3, 4]â€‹ Rootè¿›ç¨‹è‡ªå·±å»ºäº†ä¸€ä¸ªåˆ—è¡¨ï¼Œç„¶åŽå¹¿æ’­ç»™æ‰€æœ‰çš„è¿›ç¨‹ã€‚è¿™æ ·æ‰€æœ‰çš„è¿›ç¨‹éƒ½æ‹¥æœ‰äº†è¿™ä¸ªåˆ—è¡¨ã€‚ç„¶åŽçˆ±å¹²å˜›å°±å¹²å˜›äº†ã€‚â€‹ å¯¹å¹¿æ’­æœ€ç›´è§‚çš„è§‚ç‚¹æ˜¯æŸä¸ªç‰¹å®šè¿›ç¨‹å°†æ•°æ®ä¸€ä¸€å‘é€ç»™æ¯ä¸ªè¿›ç¨‹ã€‚å‡è®¾æœ‰nä¸ªè¿›ç¨‹ï¼Œé‚£ä¹ˆå‡è®¾æˆ‘ä»¬çš„æ•°æ®åœ¨0è¿›ç¨‹ï¼Œé‚£ä¹ˆ0è¿›ç¨‹å°±éœ€è¦å°†æ•°æ®å‘é€ç»™å‰©ä¸‹çš„n-1ä¸ªè¿›ç¨‹ï¼Œè¿™æ˜¯éžå¸¸ä½Žæ•ˆçš„ï¼Œå¤æ‚åº¦æ˜¯O(n)ã€‚é‚£æœ‰æ²¡æœ‰é«˜æ•ˆçš„æ–¹å¼ï¼Ÿä¸€ä¸ªæœ€å¸¸ç”¨ä¹Ÿæ˜¯éžå¸¸é«˜æ•ˆçš„æ‰‹æ®µæ˜¯è§„çº¦æ ‘å¹¿æ’­ï¼šæ”¶åˆ°å¹¿æ’­æ•°æ®çš„æ‰€æœ‰è¿›ç¨‹éƒ½å‚ä¸Žåˆ°æ•°æ®å¹¿æ’­çš„è¿‡ç¨‹ä¸­ã€‚é¦–å…ˆåªæœ‰ä¸€ä¸ªè¿›ç¨‹æœ‰æ•°æ®ï¼Œç„¶åŽå®ƒæŠŠå®ƒå‘é€ç»™ç¬¬ä¸€ä¸ªè¿›ç¨‹ï¼Œæ­¤æ—¶æœ‰ä¸¤ä¸ªè¿›ç¨‹æœ‰æ•°æ®ï¼›ç„¶åŽè¿™ä¸¤ä¸ªè¿›ç¨‹éƒ½å‚ä¸Žåˆ°ä¸‹ä¸€æ¬¡çš„å¹¿æ’­ä¸­ï¼Œè¿™æ—¶å°±ä¼šæœ‰4ä¸ªè¿›ç¨‹æœ‰æ•°æ®ï¼Œâ€¦â€¦ï¼Œä»¥æ­¤ç±»æŽ¨ï¼Œæ¯æ¬¡éƒ½ä¼šæœ‰2çš„æ¬¡æ–¹ä¸ªè¿›ç¨‹æœ‰æ•°æ®ã€‚é€šè¿‡è¿™ç§è§„çº¦æ ‘çš„å¹¿æ’­æ–¹æ³•ï¼Œå¹¿æ’­çš„å¤æ‚åº¦é™ä¸ºO(log n)ã€‚è¿™å°±æ˜¯ä¸Šé¢è¯´çš„ç¾¤ä½“é€šä¿¡çš„é«˜æ•ˆåŽŸåˆ™ï¼šå……åˆ†åˆ©ç”¨æ‰€æœ‰çš„è¿›ç¨‹æ¥å®žçŽ°æ•°æ®çš„å‘é€å’ŒæŽ¥æ”¶ã€‚æ•£æ’­scatterâ€‹ å°†ä¸€ä»½æ•°æ®å¹³åˆ†ç»™æ‰€æœ‰çš„è¿›ç¨‹ã€‚ä¾‹å¦‚æˆ‘æœ‰200ä»½æ•°æ®ï¼Œæœ‰10ä¸ªè¿›ç¨‹ï¼Œé‚£ä¹ˆæ¯ä¸ªè¿›ç¨‹ä¼šåˆ†åˆ«å¾—åˆ°20ä»½æ•°æ®ã€‚1234567891011121314import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() if comm_rank == 0: data = range(comm_size) print data else: data = None local_data = comm.scatter(data, root=0) print 'rank %d, got:' % comm_rank print local_dataâ€‹ ç»“æžœå¦‚ä¸‹ï¼š1234567891011[0, 1, 2, 3, 4] rank 0, got: 0 rank 1, got: 1 rank 2, got: 2 rank 3, got: 3 rank 4, got: 4â€‹ è¿™é‡Œrootè¿›ç¨‹åˆ›å»ºäº†ä¸€ä¸ªlistï¼Œç„¶åŽå°†å®ƒæ•£æ’­ç»™æ‰€æœ‰çš„è¿›ç¨‹ï¼Œç›¸å½“äºŽå¯¹è¿™ä¸ªliståšäº†åˆ’åˆ†ï¼Œæ¯ä¸ªè¿›ç¨‹èŽ·å¾—ç­‰åˆ†çš„æ•°æ®ï¼Œè¿™é‡Œå°±æ˜¯listçš„æ¯ä¸€ä¸ªæ•°ã€‚ï¼ˆä¸»è¦æ ¹æ®listçš„ç´¢å¼•æ¥åˆ’åˆ†ï¼Œlistç´¢å¼•ä¸ºç¬¬iä»½çš„æ•°æ®å°±å‘é€ç»™ç¬¬iä¸ªè¿›ç¨‹ï¼‰ã€‚å¦‚æžœæ˜¯çŸ©é˜µï¼Œé‚£ä¹ˆå°±ç­‰åˆ†çš„åˆ’åˆ†è¡Œï¼Œæ¯ä¸ªè¿›ç¨‹èŽ·å¾—ç›¸åŒçš„è¡Œæ•°è¿›è¡Œå¤„ç†ã€‚â€‹ éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒMPIçš„å·¥ä½œæ–¹å¼æ˜¯æ¯ä¸ªè¿›ç¨‹éƒ½ä¼šæ‰§è¡Œæ‰€æœ‰çš„ä»£ç ï¼Œæ‰€ä»¥æ¯ä¸ªè¿›ç¨‹éƒ½ä¼šæ‰§è¡Œscatterè¿™ä¸ªæŒ‡ä»¤ï¼Œä½†åªæœ‰rootæ‰§è¡Œå®ƒçš„æ—¶å€™ï¼Œå®ƒæ‰å…¼å¤‡å‘é€è€…å’ŒæŽ¥æ”¶è€…çš„èº«ä»½ï¼ˆrootä¹Ÿä¼šå¾—åˆ°å±žäºŽè‡ªå·±çš„æ•°æ®ï¼‰ï¼Œå¯¹äºŽå…¶ä»–è¿›ç¨‹æ¥è¯´ï¼Œä»–ä»¬éƒ½åªæ˜¯æŽ¥æ”¶è€…è€Œå·²ã€‚æ”¶é›†gatherâ€‹ é‚£æœ‰å‘é€ï¼Œå°±æœ‰ä¸€èµ·å›žæ”¶çš„å‡½æ•°ã€‚Gatheræ˜¯å°†æ‰€æœ‰è¿›ç¨‹çš„æ•°æ®æ”¶é›†å›žæ¥ï¼Œåˆå¹¶æˆä¸€ä¸ªåˆ—è¡¨ã€‚ä¸‹é¢è”åˆscatterå’Œgatherç»„æˆä¸€ä¸ªå®Œæˆçš„åˆ†å‘å’Œæ”¶å›žè¿‡ç¨‹ï¼š123456789101112131415161718import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() if comm_rank == 0: data = range(comm_size) print data else: data = None local_data = comm.scatter(data, root=0) local_data = local_data * 2 print 'rank %d, got and do:' % comm_rank print local_data combine_data = comm.gather(local_data,root=0) if comm_rank == 0: printcombine_dataâ€‹ ç»“æžœå¦‚ä¸‹ï¼š123456789101112[0, 1, 2, 3, 4] rank 0, got and do: 0 rank 1, got and do: 2 rank 2, got and do: 4 rank 4, got and do: 8 rank 3, got and do: 6 [0, 2, 4, 6, 8]â€‹ Rootè¿›ç¨‹å°†æ•°æ®é€šè¿‡scatterç­‰åˆ†å‘ç»™æ‰€æœ‰çš„è¿›ç¨‹ï¼Œç­‰å¾…æ‰€æœ‰çš„è¿›ç¨‹éƒ½å¤„ç†å®ŒåŽï¼ˆè¿™é‡Œåªæ˜¯ç®€å•çš„ä¹˜ä»¥2ï¼‰ï¼Œrootè¿›ç¨‹å†é€šè¿‡gatherå›žæ”¶ä»–ä»¬çš„ç»“æžœï¼Œå’Œåˆ†å‘çš„åŽŸåˆ™ä¸€æ ·ï¼Œç»„æˆä¸€ä¸ªlistã€‚Gatherè¿˜æœ‰ä¸€ä¸ªå˜ä½“å°±æ˜¯allgatherï¼Œå¯ä»¥ç†è§£ä¸ºå®ƒåœ¨gatherçš„åŸºç¡€ä¸Šå°†gatherçš„ç»“æžœå†bcastäº†ä¸€æ¬¡ã€‚å•¥æ„æ€ï¼Ÿæ„æ€æ˜¯rootè¿›ç¨‹å°†æ‰€æœ‰è¿›ç¨‹çš„ç»“æžœéƒ½å›žæ”¶ç»Ÿè®¡å®ŒåŽï¼Œå†æŠŠæ•´ä¸ªç»Ÿè®¡ç»“æžœå‘Šè¯‰å¤§å®¶ã€‚è¿™æ ·ï¼Œä¸ä»…rootå¯ä»¥è®¿é—®combine_dataï¼Œæ‰€æœ‰çš„è¿›ç¨‹éƒ½å¯ä»¥è®¿é—®combine_dataäº†ã€‚è§„çº¦reduceâ€‹ è§„çº¦æ˜¯æŒ‡ä¸ä½†å°†æ‰€æœ‰çš„æ•°æ®æ”¶é›†å›žæ¥ï¼Œæ”¶é›†å›žæ¥çš„è¿‡ç¨‹ä¸­è¿˜è¿›è¡Œäº†ç®€å•çš„è®¡ç®—ï¼Œä¾‹å¦‚æ±‚å’Œï¼Œæ±‚æœ€å¤§å€¼ç­‰ç­‰ã€‚ä¸ºä»€ä¹ˆè¦æœ‰è¿™ä¸ªå‘¢ï¼Ÿæˆ‘ä»¬ä¸æ˜¯å¯ä»¥ç›´æŽ¥ç”¨gatherå…¨éƒ¨æ”¶é›†å›žæ¥äº†ï¼Œå†å¯¹åˆ—è¡¨æ±‚ä¸ªsumæˆ–è€…maxå°±å¯ä»¥äº†å—ï¼Ÿè¿™æ ·ä¸æ˜¯ç´¯æ­»ç»„é•¿å—ï¼Ÿä¸ºä»€ä¹ˆä¸å……åˆ†ä½¿ç”¨æ¯ä¸ªå·¥äººå‘¢ï¼Ÿè§„çº¦å®žé™…ä¸Šæ˜¯ä½¿ç”¨è§„çº¦æ ‘æ¥å®žçŽ°çš„ã€‚ä¾‹å¦‚æ±‚maxï¼Œå®Œæˆå¯ä»¥è®©å·¥äººä¸¤ä¸¤pkåŽï¼Œå†è¿”å›žä¸¤ä¸¤pkçš„æœ€å¤§å€¼ï¼Œç„¶åŽå†å¯¹ç¬¬äºŒå±‚çš„æœ€å¤§å€¼ä¸¤ä¸¤pkï¼Œç›´åˆ°è¿”å›žä¸€ä¸ªæœ€ç»ˆçš„maxç»™ç»„é•¿ã€‚ç»„é•¿å°±éžå¸¸èªæ˜Žçš„å°†å·¥ä½œåˆ†é…ä¸‹å·¥äººé«˜æ•ˆçš„å®Œæˆäº†ã€‚è¿™æ˜¯O(n)çš„å¤æ‚åº¦ï¼Œä¸‹é™åˆ°O(log n)ï¼ˆåº•æ•°ä¸º2ï¼‰çš„å¤æ‚åº¦ã€‚1234567891011121314151617import mpi4py.MPI as MPI comm = MPI.COMM_WORLD comm_rank = comm.Get_rank() comm_size = comm.Get_size() if comm_rank == 0: data = range(comm_size) print data else: data = None local_data = comm.scatter(data, root=0) local_data = local_data * 2 print 'rank %d, got and do:' % comm_rank print local_data all_sum = comm.reduce(local_data, root=0,op=MPI.SUM) if comm_rank == 0: print 'sumis:%d' % all_sumâ€‹ ç»“æžœå¦‚ä¸‹ï¼š123456789101112[0, 1, 2, 3, 4] rank 0, got and do: 0 rank 1, got and do: 2 rank 2, got and do: 4 rank 3, got and do: 6 rank 4, got and do: 8 sum is:20â€‹ å¯ä»¥çœ‹åˆ°ï¼Œæœ€åŽå¯ä»¥å¾—åˆ°ä¸€ä¸ªsumå€¼ã€‚å¸¸è§ç”¨æ³•å¯¹ä¸€ä¸ªæ–‡ä»¶çš„å¤šä¸ªè¡Œå¹¶è¡Œå¤„ç†1234567891011121314151617181920212223242526272829303132333435363738#!usr/bin/env python #-*- coding: utf-8 -*- import sys import os import mpi4py.MPI as MPI import numpy as np # # Global variables for MPI # # instance for invoking MPI relatedfunctions comm = MPI.COMM_WORLD # the node rank in the whole community comm_rank = comm.Get_rank() # the size of the whole community, i.e.,the total number of working nodes in the MPI cluster comm_size = comm.Get_size() if __name__ == '__main__': if comm_rank == 0: sys.stderr.write("processor root starts reading data...\n") all_lines = sys.stdin.readlines() all_lines = comm.bcast(all_lines if comm_rank == 0 else None, root = 0) num_lines = len(all_lines) local_lines_offset = np.linspace(0, num_lines, comm_size +1).astype('int') local_lines = all_lines[local_lines_offset[comm_rank] :local_lines_offset[comm_rank + 1]] sys.stderr.write("%d/%d processor gets %d/%d data \n" %(comm_rank, comm_size, len(local_lines), num_lines)) cnt = 0 for line in local_lines: fields = line.strip().split('\t') cnt += 1 if cnt % 100 == 0: sys.stderr.write("processor %d has processed %d/%d lines \n" %(comm_rank, cnt, len(local_lines))) output = line.strip() + ' process every line here' print outputå¯¹å¤šä¸ªæ–‡ä»¶å¹¶è¡Œå¤„ç†â€‹ å¦‚æžœæˆ‘ä»¬çš„æ–‡ä»¶å¤ªå¤§ï¼Œä¾‹å¦‚å‡ åƒä¸‡è¡Œï¼Œé‚£ä¹ˆmpiæ˜¯æ²¡åŠžæ³•å°†è¿™ä¹ˆå¤§çš„æ•°æ®bcastç»™æ‰€æœ‰çš„è¿›ç¨‹çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å…ˆæŠŠå¤§çš„æ–‡ä»¶splitæˆå°çš„æ–‡ä»¶ï¼Œå†è®©æ¯ä¸ªè¿›ç¨‹å¤„ç†å°‘æ•°çš„æ–‡ä»¶ã€‚123456789101112131415161718192021222324252627282930313233343536373839404142#!usr/bin/env python #-*- coding: utf-8 -*- import sys import os import mpi4py.MPI as MPI import numpy as np # # Global variables for MPI # # instance for invoking MPI relatedfunctions comm = MPI.COMM_WORLD # the node rank in the whole community comm_rank = comm.Get_rank() # the size of the whole community, i.e.,the total number of working nodes in the MPI cluster comm_size = comm.Get_size() if __name__ == '__main__': if len(sys.argv) != 2: sys.stderr.write("Usage: python *.py directoty_with_files\n") sys.exit(1) path = sys.argv[1] if comm_rank == 0: file_list = os.listdir(path) sys.stderr.write("%d files\n" % len(file_list)) file_list = comm.bcast(file_list if comm_rank == 0 else None, root = 0) num_files = len(file_list) local_files_offset = np.linspace(0, num_files, comm_size +1).astype('int') local_files = file_list[local_files_offset[comm_rank] :local_files_offset[comm_rank + 1]] sys.stderr.write("%d/%d processor gets %d/%d data \n" %(comm_rank, comm_size, len(local_files), num_files)) cnt = 0 for file_name in local_files: hd = open(os.path.join(path, file_name)) for line in hd: output = line.strip() + ' process every line here' print output cnt += 1 sys.stderr.write("processor %d has processed %d/%d files \n" %(comm_rank, cnt, len(local_files))) hd.close()è”åˆnumpyå¯¹çŸ©é˜µçš„å¤šä¸ªè¡Œæˆ–è€…å¤šåˆ—å¹¶è¡Œå¤„ç†â€‹ Mpi4pyä¸€ä¸ªéžå¸¸ä¼˜ç§€çš„ç‰¹æ€§æ˜¯å®Œç¾Žæ”¯æŒnumpyï¼12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import os, sys, time import numpy as np import mpi4py.MPI as MPI # # Global variables for MPI # # instance for invoking MPI relatedfunctions comm = MPI.COMM_WORLD # the node rank in the whole community comm_rank = comm.Get_rank() # the size of the whole community, i.e.,the total number of working nodes in the MPI cluster comm_size = comm.Get_size() # test MPI if __name__ == "__main__": #create a matrix if comm_rank == 0: all_data = np.arange(20).reshape(4, 5) print "************ data ******************" print all_data #broadcast the data to all processors all_data = comm.bcast(all_data if comm_rank == 0 else None, root = 0) #divide the data to each processor num_samples = all_data.shape[0] local_data_offset = np.linspace(0, num_samples, comm_size + 1).astype('int') #get the local data which will be processed in this processor local_data = all_data[local_data_offset[comm_rank] :local_data_offset[comm_rank + 1]] print "****** %d/%d processor gets local data ****" %(comm_rank, comm_size) print local_data #reduce to get sum of elements local_sum = local_data.sum() all_sum = comm.reduce(local_sum, root = 0, op = MPI.SUM) #process in local local_result = local_data ** 2 #gather the result from all processors and broadcast it result = comm.allgather(local_result) result = np.vstack(result) if comm_rank == 0: print "*** sum: ", all_sum print "************ result ******************" print resultMPIå’Œmpi4pyçš„çŽ¯å¢ƒæ­å»ºâ€‹ è¿™ç« æ”¾åˆ°è¿™é‡Œæ˜¯ä½œä¸ºä¸€ä¸ªé™„å½•ã€‚æˆ‘ä»¬çš„çŽ¯å¢ƒæ˜¯linuxï¼Œéœ€è¦å®‰è£…çš„åŒ…æœ‰pythonã€openmpiã€numpyã€cpythonå’Œmpi4pyï¼Œè¿‡ç¨‹å¦‚ä¸‹ï¼šå®‰è£…Python12345tar xzvf Python-2.7.tgz cd Python-2.7 ./configure--prefix=/home/work/vis/zouxiaoyi/my_tools make make installâ€‹ å…ˆå°†Pythonæ”¾åˆ°çŽ¯å¢ƒå˜é‡é‡Œé¢ï¼Œè¿˜æœ‰Pythonçš„æ’ä»¶åº“12exportPATH=/home/work/vis/zouxiaoyi/my_tools/bin:$PATH exportLD_LIBRARY_PATH=/home/work/vis/zouxiaoyi/my_tools/lib:$LD_LIBRARY_PATHâ€‹ æ‰§è¡Œpythonï¼Œå¦‚æžœçœ‹åˆ°å¯çˆ±çš„&gt;&gt;&gt;å‡ºæ¥ï¼Œå°±è¡¨ç¤ºæˆåŠŸäº†ã€‚æŒ‰crtl+dé€€å‡ºå®‰è£…openmpi123456wget http://www.open-mpi.org/software/ompi/v1.4/downloads/openmpi-1.4.1.tar.gz tar xzvf openmpi-1.4.1.tar.gz cd openmpi-1.4.1 ./configure--prefix=/home/work/vis/zouxiaoyi/my_tools make -j 8 make installâ€‹ ç„¶åŽæŠŠbinè·¯å¾„åŠ åˆ°çŽ¯å¢ƒå˜é‡é‡Œé¢ï¼š12exportPATH=/home/work/vis/zouxiaoyi/my_tools/bin:$PATH exportLD_LIBRARY_PATH=/home/work/vis/zouxiaoyi/my_tools/lib:$LD_LIBRARY_PATHâ€‹ æ‰§è¡Œmpirunï¼Œå¦‚æžœæœ‰å¸®åŠ©ä¿¡æ¯æ‰“å°å‡ºæ¥ï¼Œå°±è¡¨ç¤ºå®‰è£…å¥½äº†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘å®‰è£…äº†å‡ ä¸ªç‰ˆæœ¬éƒ½æ²¡æœ‰æˆåŠŸï¼Œæœ€åŽå®‰è£…äº†1.4.1è¿™ä¸ªç‰ˆæœ¬æ‰èƒ½æˆåŠŸï¼Œå› æ­¤å°±çœ‹ä½ çš„äººå“äº†ã€‚å®‰è£…numpyå’ŒCythonâ€‹ å®‰è£…pythonåº“çš„æ–¹æ³•å¯ä»¥å‚è€ƒä¹‹å‰çš„åšå®¢ã€‚è¿‡ç¨‹ä¸€èˆ¬å¦‚ä¸‹ï¼š123tar â€“xgvf Cython-0.20.2.tar.gz cd Cython-0.20.2 python setup.py installâ€‹ æ‰“å¼€Pythonï¼Œimport Cythonï¼Œå¦‚æžœæ²¡æœ‰æŠ¥é”™ï¼Œå°±è¡¨ç¤ºå®‰è£…æˆåŠŸäº†å®‰è£…mpi4py123tar â€“xgvf mpi4py_1.3.1.tar.gz cd mpi4py vi mpi.cfgâ€‹ åœ¨68è¡Œï¼Œ[openmpi]ä¸‹é¢ï¼Œå°†åˆšæ‰å·²ç»å®‰è£…å¥½çš„openmpiçš„ç›®å½•ç»™æ”¹ä¸Šã€‚12mpi_dir = /home/work/vis/zouxiaoyi/my_tools python setup.py installâ€‹ æ‰“å¼€Pythonï¼Œimport mpi4py as MPIï¼Œå¦‚æžœæ²¡æœ‰æŠ¥é”™ï¼Œå°±è¡¨ç¤ºå®‰è£…æˆåŠŸäº†â€‹ ä¸‹é¢å°±å¯ä»¥å¼€å§‹å±žäºŽä½ çš„å¹¶è¡Œä¹‹æ—…äº†ï¼Œå‹‡æ•¢æŽ¢ç´¢å¤šæ ¸çš„ä¹è¶£å§ã€‚]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[A Visual Guide to Evolution Strategies]]></title>
      <url>%2F2018%2F01%2F29%2FA-Visual-Guide-to-Evolution-Strategies%2F</url>
      <content type="text"><![CDATA[Source post is here.Survival of the fittest.In this post I explain how evolution strategies (ES) work with the aid of a few visual examples. I try to keep the equations light, and I provide links to original articles if the reader wishes to understand more details. This is the first post in a series of articles, where I plan to show how to apply these algorithms to a range of tasks from MNIST, OpenAI Gym, Roboschool to PyBullet environments.IntroductionNeural network models are highly expressive and flexible, and if we are able to find a suitable set of model parameters, we can use neural nets to solve many challenging problems. Deep learningâ€™s success largely comes from the ability to use the backpropagation algorithm to efficiently calculate the gradient of an objective function over each model parameter. With these gradients, we can efficiently search over the parameter space to find a solution that is often good enough for our neural net to accomplish difficult tasks.However, there are many problems where the backpropagation algorithm cannot be used. For example, in reinforcement learning (RL) problems, we can also train a neural network to make decisions to perform a sequence of actions to accomplish some task in an environment. However, it is not trivial to estimate the gradient of reward signals given to the agent in the future to an action performed by the agent right now, especially if the reward is realised many timesteps in the future. Even if we are able to calculate accurate gradients, there is also the issue of being stuck in a local optimum, which exists many for RL tasks.Stuck in a local optimum.A whole area within RL is devoted to studying this credit-assignment problem, and great progress has been made in recent years. However, credit assignment is still difficult when the reward signals are sparse. In the real world, rewards can be sparse and noisy. Sometimes we are given just a single reward, like a bonus check at the end of the year, and depending on our employer, it may be difficult to figure out exactly why it is so low. For these problems, rather than rely on a very noisy and possibly meaningless gradient estimate of the future to our policy, we might as well just ignore any gradient information, and attempt to use black-box optimisation techniques such as genetic algorithms (GA) or ES.OpenAI published a paper called Evolution Strategies as a Scalable Alternative to Reinforcement Learning where they showed that evolution strategies, while being less data efficient than RL, offer many benefits. The ability to abandon gradient calculation allows such algorithms to be evaluated more efficiently. It is also easy to distribute the computation for an ES algorithm to thousands of machines for parallel computation. By running the algorithm from scratch many times, they also showed that policies discovered using ES tend to be more diverse compared to policies discovered by RL algorithms.I would like to point out that even for the problem of identifying a machine learning model, such as designing a neural netâ€™s architecture, is one where we cannot directly compute gradients. While RL, Evolution, GA etc., can be applied to search in the space of model architectures, in this post, I will focus only on applying these algorithms to search for parameters of a pre-defined model.What is an Evolution Strategy?Two-dimensional Rastrigin function has many local optima (Source: Wikipedia).The diagrams below are top-down plots of shifted 2D Schaffer and Rastrigin functions, two of several simple toy problems used for testing continuous black-box optimisation algorithms. Lighter regions of the plots represent higher values of $F(x,y)$. As you can see, there are many local optimums in this function. Our job is to find a set of model parameters $(x, y)$, such that $F(x,y)$ is as close as possible to the global maximum.Schaffer-2D FunctionRastrigin-2D FunctionAlthough there are many definitions of evolution strategies, we can define an evolution strategy as an algorithm that provides the user a set of candidate solutions to evaluate a problem. The evaluation is based on an objective function that takes a given solution and returns a single fitness value. Based on the fitness results of the current solutions, the algorithm will then produce the next generation of candidate solutions that is more likely to produce even better results than the current generation. The iterative process will stop once the best known solution is satisfactory for the user.Given an evolution strategy algorithm called EvolutionStrategy, we can use in the following way:solver = EvolutionStrategy()while True:# ask the ES to give us a set of candidate solutionssolutions = solver.ask()# create an array to hold the fitness results.fitness_list = np.zeros(solver.popsize)# evaluate the fitness for each given solution.for i in range(solver.popsize):fitness_list[i] = evaluate(solutions[i])# give list of fitness results back to ESsolver.tell(fitness_list)# get best parameter, fitness from ESbest_solution, best_fitness = solver.result()if best_fitness &gt; MY_REQUIRED_FITNESS:breakAlthough the size of the population is usually held constant for each generation, they donâ€™t need to be. The ES can generate as many candidate solutions as we want, because the solutions produced by an ES are sampled from a distribution whose parameters are being updated by the ES at each generation. I will explain this sampling process with an example of a simple evolution strategy.Simple Evolution StrategyOne of the simplest evolution strategy we can imagine will just sample a set of solutions from a Normal distribution, with a mean \muÎ¼and a fixed standard deviation \sigmaÏƒ. In our 2D problem, \mu = (\mu_x, \mu_y)Î¼=(Î¼x,Î¼y) and \sigma = (\sigma_x, \sigma_y)Ïƒ=(Ïƒx,Ïƒy). Initially, \muÎ¼ is set at the origin. After the fitness results are evaluated, we set \muÎ¼ to the best solution in the population, and sample the next generation of solutions around this new mean. This is how the algorithm behaves over 20 generations on the two problems mentioned earlier:In the visualisation above, the green dot indicates the mean of the distribution at each generation, the blue dots are the sampled solutions, and the red dot is the best solution found so far by our algorithm.This simple algorithm will generally only work for simple problems. Given its greedy nature, it throws away all but the best solution, and can be prone to be stuck at a local optimum for more complicated problems. It would be beneficial to sample the next generation from a probability distribution that represents a more diverse set of ideas, rather than just from the best solution from the current generation.Simple Genetic AlgorithmOne of the oldest black-box optimisation algorithms is the genetic algorithm. There are many variations with many degrees of sophistication, but I will illustrate the simplest version here.The idea is quite simple: keep only 10% of the best performing solutions in the current generation, and let the rest of the population die. In the next generation, to sample a new solution is to randomly select two solutions from the survivors of the previous generation, and recombine their parameters to form a new solution. This crossover recombination process uses a coin toss to determine which parent to take each parameter from. In the case of our 2D toy function, our new solution might inherit xx or yy from either parents with 50% chance. Gaussian noise with a fixed standard deviation will also be injected into each new solution after this recombination process.The figure above illustrates how the simple genetic algorithm works. The green dots represent members of the elite population from the previous generation, the blue dots are the offsprings to form the set of candidate solutions, and the red dot is the best solution.Genetic algorithms help diversity by keeping track of a diverse set of candidate solutions to reproduce the next generation. However, in practice, most of the solutions in the elite surviving population tend to converge to a local optimum over time. There are more sophisticated variations of GA out there, such as CoSyNe, ESP, and NEAT, where the idea is to cluster similar solutions in the population together into different species, to maintain better diversity over time.Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)A shortcoming of both the Simple ES and Simple GA is that our standard deviation noise parameter is fixed. There are times when we want to explore more and increase the standard deviation of our search space, and there are times when we are confident we are close to a good optima and just want to fine tune the solution. We basically want our search process to behave like this:Amazing isnâ€™it it? The search process shown in the figure above is produced by Covariance-Matrix Adaptation Evolution Strategy (CMA-ES). CMA-ES an algorithm that can take the results of each generation, and adaptively increase or decrease the search space for the next generation. It will not only adapt for the mean $\mu$ and sigma $\sigma$ parameters, but will calculate the entire covariance matrix of the parameter space. At each generation, CMA-ES provides the parameters of a multi-variate normal distribution to sample solutions from. So how does it know how to increase or decrease the search space?Before we discuss its methodology, letâ€™s review how to estimate a covariance matrix. This will be important to understand CMA-ESâ€™s methodology later on. If we want to estimate the covariance matrix of our entire sampled population of size of $N$, we can do so using the set of equations below to calculate the maximum likelihood estimate of a covariance matrix $C$. We first calculate the means of each of the $x_i$ and $y_i$ in our population:$$\mu_x = \frac{1}{N} \sum_{i=1}^{N}x_i,$$$$\mu_y = \frac{1}{N} \sum_{i=1}^{N}y_i.$$The terms of the 2x2 covariance matrix $C$ will be:$$\begin{align}\sigma_x^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)^2, \\\sigma_y^2 &amp;= \frac{1}{N} \sum_{i=1}^{N}(y_i - \mu_y)^2, \\\sigma_{xy} &amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i - \mu_x)(y_i - \mu_y).\end{align}$$Of course, these resulting mean estimates $\mu_x$ and $\mu_y$, and covariance terms $\sigma_x$, $\sigma_y$, $\sigma_{xy}$ will just be an estimate to the actual covariance matrix that we originally sampled from, and not particularly useful to us.CMA-ES modifies the above covariance calculation formula in a clever way to make it adapt well to an optimisation problem. I will go over how it does this step-by-step. Firstly, it focuses on the best $N_{best}$ solutions in the current generation. For simplicity letâ€™s set $N_{best}$ to be the best 25% of solutions. After sorting the solutions based on fitness, we calculate the mean $\mu^{(g+1)}$ of the next generation $(g+1)$ as the average of only the best 25% of the solutions in current population $(g)$, i.e.:$$\begin{align}\mu_x^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}x_i, \\\mu_y^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}y_i.\end{align}$$Next, we use only the best 25% of the solutions to estimate the covariance matrix $C^{(g+1)}$ of the next generation, but the clever hack here is that it uses the current generationâ€™s $\mu^{(g)}$, rather than the updated $\mu^{(g+1)}$ parameters that we had just calculated, in the calculation:$$\begin{align}\sigma_x^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})^2, \\\sigma_y^{2, (g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(y_i - \mu_y^{(g)})^2, \\\sigma_{xy}^{(g+1)} &amp;= \frac{1}{N_{best}} \sum_{i=1}^{N_{best}}(x_i - \mu_x^{(g)})(y_i - \mu_y^{(g)}).\end{align}$$Armed with a set of $\mu_x$, $\mu_y$, $\sigma_x$, $\sigma_y$, and $\sigma_{xy}$ parameters for the next generation $(g+1)$, we can now sample the next generation of candidate solutions.Below is a set of figures to visually illustrate how it uses the results from the current generation $(g)$ to construct the solutions in the next generation $(g+1)$:Step 1Step 2Step 3Step 4Calculate the fitness score of each candidate solution in generation $(g)$.Isolates the best 25% of the population in generation $(g)$, in purple.Using only the best solutions, along with the mean $\mu^{(g)}$ of the current generation (the green dot), calculate the covariance matrix $C^{(g+1)}$ of the next generation.Sample a new set of candidate solutions using the updated mean $\mu^{(g+1)}$ and covariance matrix $C^{(g+1)}$.Letâ€™s visualise the scheme one more time, on the entire search process on both problems:Because CMA-ES can adapt both its mean and covariance matrix using information from the best solutions, it can decide to cast a wider net when the best solutions are far away, or narrow the search space when the best solutions are close by. My description of the CMA-ES algorithm for a 2D toy problem is highly simplified to get the idea across. For more details, I suggest reading the CMA-ES Tutorial prepared by Nikolaus Hansen, the author of CMA-ES.This algorithm is one of the most popular gradient-free optimisation algorithms out there, and has been the algorithm of choice for many researchers and practitioners alike. The only real drawback is the performance if the number of model parameters we need to solve for is large, as the covariance calculation is $O(N^2)$, although recently there has been approximations to make it $O(N)$. CMA-ES is my algorithm of choice when the search space is less than a thousand parameters. I found it still usable up to ~ 10K parameters if Iâ€™m willing to be patient.Natural Evolution StrategiesImagine if you had built an artificial life simulator, and you sample a different neural network to control the behavior of each ant inside an ant colony. Using the Simple Evolution Strategy for this task will optimise for traits and behaviours that benefit individual ants, and with each successive generation, our population will be full of alpha ants who only care about their own well-being.Instead of using a rule that is based on the survival of the fittest ants, what if you take an alternative approach where you take the sum of all fitness values of the entire ant population, and optimise for this sum instead to maximise the well-being of the entire ant population over successive generations? Well, you would end up creating a Marxist utopia.A perceived weakness of the algorithms mentioned so far is that they discard the majority of the solutions and only keep the best solutions. Weak solutions contain information about what not to do, and this is valuable information to calculate a better estimate for the next generation.Many people who studied RL are familiar with the REINFORCE paper. In this 1992 paper, Williams outlined an approach to estimate the gradient of the expected rewards with respect to the model parameters of a policy neural network. This paper also proposed using REINFORCE as an Evolution Strategy, in Section 6 of the paper. This special case of REINFORCE-ES was expanded later on in Parameter-Exploring Policy Gradients (PEPG, 2009) and Natural Evolution Strategies (NES, 2014).In this approach, we want to use all of the information from each member of the population, good or bad, for estimating a gradient signal that can move the entire population to a better direction in the next generation. Since we are estimating a gradient, we can also use this gradient in a standard SGD update rule typically used for deep learning. We can even use this estimated gradient with Momentum SGD, RMSProp, or Adam if we want to.The idea is to maximise the expected value of the fitness score of a sampled solution. If the expected result is good enough, then the best performing member within a sampled population will be even better, so optimising for the expectation might be a sensible approach. Maximising the expected fitness score of a sampled solution is almost the same as maximising the total fitness score of the entire population.If $z$ is a solution vector sampled from a probability distribution function $\pi(z, \theta)$, we can define the expected value of the objective function $F$ as:$$J(\theta) = E_{\theta}[F(z)] = \int F(z) \; \pi(z, \theta) \; dz,$$where $\theta$ are the parameters of the probability distribution function. For example, if $\pi$ is a normal distribution, then $\theta$ would be \muÎ¼and $\sigma$. For our simple 2D toy problems, each ensemble $z$ is a 2D vector $(x, y)$.The NES paper contains a nice derivation of the gradient of $J(\theta)$ with respect to $\theta$. Using the same log-likelihood trick as in the REINFORCE algorithm allows us to calculate the gradient of $J(\theta)$:$$\nabla_{\theta} J(\theta) = E_{\theta}[ \; F(z) \; \nabla_{\theta} \log \pi(z, \theta) \; ].$$In a population size of $N$, where we have solutions $z^1, z^2, â€¦ z^N$, we can estimate this gradient as a summation:$$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).$$With this gradient $\nabla_{\theta} J(\theta)$, we can use a learning rate parameter \alphaÎ± (such as 0.01) and start optimising the $\theta$ parameters of pdf $\pi$ so that our sampled solutions will likely get higher fitness scores on the objective function $F$. Using SGD (or Adam), we can update $\theta$ for the next generation:$$\theta \rightarrow \theta + \alpha \nabla_{\theta} J(\theta),$$and sample a new set of candidate solutions $z$ from this updated pdf, and continue until we arrive at a satisfactory solution.In Section 6 of the REINFORCE paper, Williams derived closed-form formulas of the gradient $\nabla_{\theta} \log \pi(z^i, \theta)$, for the special case where $ \pi(z, \theta)$ is a factored multi-variate normal distribution (i.e., the correlation parameters are zero). In this special case, $\theta$ are the $\mu$ and $\sigma$ vectors. Therefore, each element of a solution can be sampled from a univariate normal distribution $z_j \sim N(\mu_j, \sigma_j)$.The closed-form formulas for $\nabla_{\theta} \log N(z^i, \theta)$, for each individual element of vector $\theta$ on each solution $i$ in the population can be derived as:$$\nabla_{\mu_{j}} \log N(z^i, \mu, \sigma) = \frac{z_j^i - \mu_j}{\sigma_j},$$$$\nabla_{\sigma_{j}} \log N(z^i, \mu, \sigma) = \frac{(z_j^i - \mu_j)^2 - \sigma_j^2}{\sigma_j^3}.$$For clarity, I use the index of jj, to count across parameter space, and this is not to be confused with superscript $i$, used to count across each sampled member of the population. For our 2D problems, $z_1 = x, z_2 = y, \mu_1 = \mu_x, \mu_2 = \mu_y, \sigma_1 = \sigma_x, \sigma_2 = \sigma_y$ in this context.These two formulas can be plugged back into the approximate gradient formula to derive explicit update rules for \muÎ¼ and \sigmaÏƒ. In the papers mentioned above, they derived more explicit update rules, incorporated a baseline, and introduced other tricks such as antithetic sampling in PEPG, which is what my implementation is based on. NES proposed incorporating the inverse of the Fisher Information Matrix into the gradient update rule. But the concept is basically the same as other ES algorithms, where we update the mean and standard deviation of a multi-variate normal distribution at each new generation, and sample a new set of solutions from the updated distribution. Below is a visualization of this algorithm in action, following the formulas described above:We see that this algorithm is able to dynamically change the $\sigma$â€™s to explore or fine tune the solution space as needed. Unlike CMA-ES, there is no correlation structure in our implementation, so we donâ€™t get the diagonal ellipse samples, only the vertical or horizontal ones, although in principle we can derive update rules to incorporate the entire covariance matrix if we needed to, at the expense of computational efficiency.I like this algorithm because like CMA-ES, the $\sigma$â€™s can adapt so our search space can be expanded or narrowed over time. Because the correlation parameter is not used in this implementation, the efficiency of the algorithm is $O(N)$ so I use PEPG if the performance of CMA-ES becomes an issue. I usually use PEPG when the number of model parameters exceed several thousand.OpenAI Evolution StrategyIn OpenAIâ€™s paper, they implement an evolution strategy that is a special case of the REINFORCE-ES algorithm outlined earlier. In particular, \sigmaÏƒ is fixed to a constant number, and only the \muÎ¼ parameter is updated at each generation. Below is how this strategy looks like, with a constant \sigmaÏƒ parameter:In addition to the simplification, this paper also proposed a modification of the update rule that is suitable for parallel computation across different worker machines. In their update rule, a large grid of random numbers have been pre-computed using a fixed seed. By doing this, each worker can reproduce the parameters of every other worker over time, and each worker needs only to communicate a single number, the final fitness result, to all of the other workers. This is important if we want to scale evolution strategies to thousands or even a million workers located on different machines, since while it may not be feasible to transmit an entire solution vector a million times at each generation update, it may be feasible to transmit only the final fitness results. In the paper, they showed that by using 1440 workers on Amazon EC2 they were able to solve the Mujoco Humanoid walking task in ~ 10 minutes.I think in principle, this parallel update rule should work with the original algorithm where they can also adapt $\sigma$, but perhaps in practice, they wanted to keep the number of moving parts to a minimum for large-scale parallel computing experiments. This inspiring paper also discussed many other practical aspects of deploying ES for RL-style tasks, and I highly recommend going through it to learn more.Fitness ShapingMost of the algorithms above are usually combined with a fitness shaping method, such as the rank-based fitness shaping method I will discuss here. Fitness shaping allows us to avoid outliers in the population from dominating the approximate gradient calculation mentioned earlier:$$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \; F(z^i) \; \nabla_{\theta} \log \pi(z^i, \theta).$$If a particular $F(z^m)$ is much larger than other $F(z^i)$ in the population, then the gradient might become dominated by this outliers and increase the chance of the algorithm being stuck in a local optimum. To mitigate this, one can apply a rank transformation of the fitness. Rather than use the actual fitness function, we would rank the results and use an augmented fitness function which is proportional to the solutionâ€™s rank in the population. Below is a comparison of what the original set of fitness may look like, and what the ranked fitness looks like:What this means is supposed we have a population size of 101. We would evaluate each population to the actual fitness function, and then sort the solutions based by their fitness. We will assign an augmented fitness value of -0.50 to the worse performer, -0.49 to the second worse solution, â€¦, 0.49 to the second best solution, and finally a fitness value of 0.50 to the best solution. This augmented set of fitness values will be used to calculate the gradient update, instead of the actual fitness values. In a way, it is a similar to just applying Batch Normalization to the results, but more direct. There are alternative methods for fitness shaping but they all basically give similar results in the end.I find fitness shaping to be very useful for RL tasks if the objective function is non-deterministic for a given policy network, which is often the cases on RL environments where maps are randomly generated and various opponents have random policies. It is less useful for optimising for well-behaved functions that are deterministic, and the use of fitness shaping can sometimes slow down the time it takes to find a good solution.MNISTAlthough ES might be a way to search for more novel solutions that are difficult for gradient-based methods to find, it still vastly underperforms gradient-based methods on many problems where we can calculate high quality gradients. For instance, only an idiot would attempt to use a genetic algorithm for image classification. But sometimes such people do exist in the world, and sometimes these explorations can be fruitful!Since all ML algorithms should be tested on MNIST, I also tried to apply these various ES algorithms to find weights for a small, simple 2-layer convnet used to classify MNIST, just to see where we stand compared to SGD. The convnet only has ~ 11k parameters so we can accommodate the slower CMA-ES algorithm. The code and the experiments are available here.Below are the results for various ES methods, using a population size of 101, over 300 epochs. We keep track of the model parameters that performed best on the entire training set at the end of each epoch, and evaluate this model once on the test set after 300 epochs. It is interesting how sometimes the test setâ€™s accuracy is higher than the training set for the models that have lower scores.MethodTrain SetTest SetAdam (BackProp) Baseline99.898.9Simple GA82.182.4CMA-ES98.498.1OpenAI-ES96.096.2PEPG98.598.0We should take these results with a grain of salt, since they are based on a single run, rather than the average of 5-10 runs. The results based on a single-run seem to indicate that CMA-ES is the best at the MNIST task, but the PEPG algorithm is not that far off. Both of these algorithms achieved ~ 98% test accuracy, 1% lower than the SGD/ADAM baseline. Perhaps the ability to dynamically alter its covariance matrix, and standard deviation parameters over each generation allowed it to fine-tune its weights better than OpenAIâ€™s simpler variation.Try It YourselfThere are probably open source implementations of all of the algorithms described in this article. The author of CMA-ES, Nikolaus Hansen, has been maintaining a numpy-based implementation of CMA-ES with lots of bells and whistles. His python implementation introduced me to the training loop interface described earlier. Since this interface is quite easy to use, I also implemented the other algorithms such as Simple Genetic Algorithm, PEPG, and OpenAIâ€™s ES using the same interface, and put it in a small python file called es.py, and also wrapped the original CMA-ES library in this small library. This way, I can quickly compare different ES algorithms by just changing one line:import es#solver = es.SimpleGA(...)#solver = es.PEPG(...)#solver = es.OpenES(...)solver = es.CMAES(...)while True:solutions = solver.ask()fitness_list = np.zeros(solver.popsize)for i in range(solver.popsize):fitness_list[i] = evaluate(solutions[i])solver.tell(fitness_list)result = solver.result()if result[1] &gt; MY_REQUIRED_FITNESS:breakYou can look at es.py on GitHub and the IPython notebook examples using the various ES algorithms.In this IPython notebook that accompanies es.py, I show how to use the ES solvers in es.py to solve a 100-Dimensional version of the Rastrigin function with even more local optimum points. The 100-D version is somewhat more challenging than the trivial 2D version used to produce the visualizations in this article. Below is a comparison of the performance for various algorithms discussed:On this 100-D Rastrigin problem, none of the optimisers got to the global optimum solution, although CMA-ES comes close. CMA-ES blows everything else away. PEPG is in 2nd place, and OpenAI-ES / Genetic Algorithm falls behind. I had to use an annealing schedule to gradually lower \sigmaÏƒ for OpenAI-ES to make it perform better for this task.Final solution that CMA-ES discovered for 100-D Rastrigin function.Global optimal solution is a 100-dimensional vector of exactly 10.References and Other LinksBelow are a few links to information related to evolutionary computing which I found useful or inspiring.Image Credits of Lemmings Jumping off a Cliff. Your results may vary when investing in ICOs.CMA-ES: Official Reference Implementation on GitHub, Tutorial, Original CMA-ES Paper from 2001, Overview SlidesSimple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning (REINFORCE), 1992.Parameter-Exploring Policy Gradients, 2009.Natural Evolution Strategies, 2014.Evolution Strategies as a Scalable Alternative to Reinforcement Learning, OpenAI, 2017.Risto Miikkulainenâ€™s Slides on Neuroevolution.A Neuroevolution Approach to General Atari Game Playing, 2013.Kenneth Stanleyâ€™s Talk on Why Greatness Cannot Be Planned: The Myth of the Objective, 2015.Neuroevolution: A Different Kind of Deep Learning. The quest to evolve neural networks through evolutionary algorithms.Compressed Network Search Finds Complex Neural Controllers with a Million Weights.Karl Sims Evolved Virtual Creatures, 1994.Evolved Step Climbing Creatures.Super Mario World Agent Mario I/O, Mario Kart 64 Controller using) using NEAT Algorithm.Ingo Rechenberg, the inventor of Evolution Strategies.A Tutorial on Differential Evolution with Python.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Introduction to Genetic Algorithm]]></title>
      <url>%2F2018%2F01%2F23%2FIntroduction-to-Genetic-Algorithm%2F</url>
      <content type="text"><![CDATA[1. Intuition behind Genetic AlgorithmsLetâ€™s start with the famous quote by Charles Darwin:It is not the strongest of the species that survives, nor the most intelligent , but the one most responsive to change.You must be thinking what has this quote got to do with genetic algorithm? Actually, the entire concept of a genetic algorithm is based on the above line.Let us understand with a basic example:Letâ€™s take a hypothetical situation where, you are head of a country, and in order to keep your city safe from bad things, you implement a policy like this.You select all the good people, and ask them to extend their generation by having their children.This repeats for a few generations.You will notice that now you have an entire population of good people.Now, that may not be entirely possible, but this example was just to help you understand the concept. So the basic idea was that we changed the input (i.e. population) such that we get better output (i.e. better country).Now, I suppose you have got some intuition that the concept of a genetic algorithm is somewhat related to biology. So letâ€™s us quickly grasp some little concepts, so that we can draw a parallel line between them.2. Biological InspirationI am sure you would remember:Cells are the basic building block of all living things.Therefore in each cell, there is the same set of chromosomes. Chromosome are basically the strings of DNA.Traditionally, these chromosomes are represented in binary as strings of 0â€™s and 1â€™s.Source : linkA chromosome consists of genes, commonly referred as blocks of DNA, where each gene encodes a specific trait, for example hair color or eye color.I wanted you to recall these basics concept of biology before going further. Letâ€™s get back and understand what actually is a genetic algorithm?3. What is a Genetic Algorithm?Letâ€™s get back to the example we discussed above and summarize what we did.Firstly, we defined our initial population as our countrymen.We defined a function to classify whether is a person is good or bad.Then we selected good people for mating to produce their off-springs.And finally, these off-springs replace the bad people from the population and this process repeats.This is how genetic algorithm actually works, which basically tries to mimic the human evolution to some extent.So to formalize a definition of a genetic algorithm, we can say that it is an optimization technique, which tries to find out such values of input so that we get the best output values or results.The working of a genetic algorithm is also derived from biology, which is as shown in the image below.Source: linkSo, let us try to understand the steps one by one.4. Steps Involved in Genetic AlgorithmHere, to make things easier, let us understand it by the famous Knapsack problem.If you havenâ€™t come across this problem, let me introduce my version of this problem.Letâ€™s say, you are going to spend a month in the wilderness. Only thing you are carrying is the backpack which can hold a maximum weight of 30 kg. Now you have different survival items, each having its own â€œSurvival Pointsâ€ (which are given for each item in the table). So, your objective is maximise the survival points.Here is the table giving details about each item.4.1 InitialisationTo solve this problem using genetic algorithm, our first step would be defining our population. So our population will contain individuals, each having their own set of chromosomes.We know that, chromosomes are binary strings, where for this problem 1 would mean that the following item is taken and 0 meaning that it is dropped.This set of chromosome is considered as our initial population.4.2 Fitness FunctionLet us calculate fitness points for our first two chromosomes.For A1 chromosome [100110],Similarly for A2 chromosome [001110],So, for this problem, our chromosome will be considered as more fit when it contains more survival points.Therefore chromosome 1 is more fit than chromosome 2.4.3 SelectionNow, we can select fit chromosomes from our population which can mate and create their off-springs.General thought is that we should select the fit chromosomes and allow them to produce off-springs. But that would lead to chromosomes that are more close to one another in a few next generation, and therefore less diversity.Therefore, we generally use Roulette Wheel Selection method.Donâ€™t be afraid of name, just take a look at the image below.I suppose we all have seen this, either in real or in movies. So, letâ€™s build our roulette wheel.Consider a wheel, and letâ€™s divide that into m divisions, where m is the number of chromosomes in our populations. The area occupied by each chromosome will be proportional to its fitness value.Based on these values, let us create our roulette wheel.So, now this wheel is rotated and the region of wheel which comes in front of the fixed point is chosen as the parent. For the second parent, the same process is repeated.Sometimes we mark two fixed point as shown in the figure below.So, in this method we can get both our parents in one go. This method is known as Stochastic Universal Selection method.4.4 CrossoverSo in this previous step, we have selected parent chromosomes that will produce off-springs. So in biological terms, crossover is nothing but reproduction.So let us find the crossover of chromosome 1 and 4, which were selected in the previous step. Take a look at the image below.This is the most basic form of crossover, known as one point crossover. Here we select a random crossover point and the tails of both the chromosomes are swapped to produce a new off-springs.If you take two crossover point, then it will called as multi point crossover which is as shown below.4.5 MutationNow if you think in the biological sense, are the children produced have the same traits as their parents? The answer is NO. During their growth, there is some change in the genes of children which makes them different from its parents.This process is known as mutation, which may be defined as a random tweak in the chromosome, which also promotes the idea of diversity in the population.A simple method of mutation is shown in the image below.So the entire process is summarise as shown in the figure.Source : linkThe off-springs thus produced are again validated using our fitness function, and if considered fit then will replace the less fit chromosomes from the population.But the question is how we will get to know that we have reached our best possible solution?So basically there are different termination conditions, which are listed below:There is no improvement in the population for over x iterations.We have already predefined an absolute number of generation for our algorithm.When our fitness function has reached a predefined value.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Evolution Strategies as a Scalable Alternative to Reinforcement Learning [Repost]]]></title>
      <url>%2F2018%2F01%2F22%2FEvolution-Strategies-as-a-Scalable-Alternative-to-Reinforcement-Learning-Repost%2F</url>
      <content type="text"><![CDATA[Source blog is here.Weâ€™ve discovered that evolution strategies (ES), an optimization technique thatâ€™s been known for decades, rivals the performance of standard reinforcement learning (RL)techniques on modern RL benchmarks (e.g. Atari/MuJoCo), while overcoming many of RLâ€™s inconveniences.In particular, ES is simpler to implement (there is no need for backpropagation), it is easier to scale in a distributed setting, it does not suffer in settings with sparse rewards, and has fewer hyperparameters. This outcome is surprising because ES resembles simple hill-climbing in a high-dimensional space based only on finite differences along a few random directions at each step.Our finding continues the modern trend of achieving strong results with decades-old ideas. For example, in 2012, the â€œAlexNetâ€ paper showed how to design, scale and train convolutional neural networks (CNNs) to achieve extremely strong results on image recognition tasks, at a time when most researchers thought that CNNs were not a promising approach to computer vision. Similarly, in 2013, the Deep Q-Learning paper showed how to combine Q-Learning with CNNs to successfully solve Atari games, reinvigorating RL as a research field with exciting experimental (rather than theoretical) results. Likewise, our work demonstrates that ES achieves strong performance on RL benchmarks, dispelling the common belief that ES methods are impossible to apply to high dimensional problems.ES is easy to implement and scale. Running on a computing cluster of 80 machines and 1,440 CPU cores, our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes (A3C on 32 cores takes about 10 hours). Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour.In what follows, weâ€™ll first briefly describe the conventional RL approach, contrast that with our ES approach, discuss the tradeoffs between ES and RL, and finally highlight some of our experiments.Reinforcement LearningLetâ€™s briefly look at how RL works. Suppose we are given some environment (e.g. a game) that weâ€™d like to train an agent on. To describe the behavior of the agent, we define a policy function (the brain of the agent), which computes how the agent should act in any given situation. In practice, the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions. A typical policy function might have about 1,000,000 parameters, so our task comes down to finding the precise setting of these parameters such that the policy plays well (i.e. wins a lot of games).Above: In the game of Pong, the policy could take the pixels of the screen and compute the probability of moving the playerâ€™s paddle (in green, on right) Up, Down, or neither.The training process for the policy works as follows. Starting from a random initialization, we let the agent interact with the environment for a while and collect episodes of interaction (e.g. each episode is one game of Pong). We thus obtain a complete recording of what happened: what sequence of states we encountered, what actions we took in each state, and what the reward was at each step. As an example, below is a diagram of three episodes that each took 10 time steps in a hypothetical environment. Each rectangle is a state, and rectangles are colored green if the reward was positive (e.g. we just got the ball past our opponent) and red if the reward was negative (e.g. we missed the ball):This diagram suggests a recipe for how we can improve the policy; whatever we happened to do leading up to the green states was good, and whatever we happened to do in the states leading up to the red areas was bad. We can then use backpropagation to compute a small update on the networkâ€™s parameters that would make the green actions more likely in those states in the future, and the red actions less likely in those states in the future. We expect that the updated policy works a bit better as a result. We then iterate the process: collect another batch of episodes, do another update, etc.Exploration by injecting noise in the actions. The policies we usually use in RL are stochastic, in that they only compute probabilities of taking any action. This way, during the course of training, the agent may find itself in a particular state many times, and at different times it will take different actions due to the sampling. This provides the signal needed for learning; some of those actions will lead to good outcomes, and get encouraged, and some of them will not work out, and get discouraged. We therefore say that we introduce exploration into the learning process by injecting noise into the agentâ€™s actions, which we do by sampling from the action distribution at each time step. This will be in contrast to ES, which we describe next.Evolution StrategiesOn â€œEvolutionâ€. Before we dive into the ES approach, it is important to note that despite the word â€œevolutionâ€, ES has very little to do with biological evolution. Early versions of these techniques may have been inspired by biological evolution and the approach can, on an abstract level, be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations. However, the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black-box stochastic optimization techniques.Black-box optimization. In ES, we forget entirely that there is an agent, an environment, that there are neural networks involved, or that interactions take place over time, etc. The whole setup is that 1,000,000 numbers (which happen to describe the parameters of the policy network) go in, 1 number comes out (the total reward), and we want to find the best setting of the 1,000,000 numbers. Mathematically, we would say that we are optimizing a function f(w) with respect to the input vector w(the parameters / weights of the network), but we make no assumptions about the structure of f, except that we can evaluate it (hence â€œblack boxâ€).The ES algorithm. Intuitively, the optimization is a â€œguess and checkâ€ process, where we start with some random parameters and then repeatedly 1) tweak the guess a bit randomly, and 2) move our guess slightly towards whatever tweaks worked better. Concretely, at each step we take a parameter vector w and generate a population of, say, 100 slightly different parameter vectors w1 ... w100 by jittering w with gaussian noise. We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while, and add up all the rewards in each case. The updated parameter vector then becomes the weighted sum of the 100 vectors, where each weight is proportional to the total reward (i.e. we want the more successful candidates to have a higher weight). Mathematically, youâ€™ll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences, except we only do it along 100 random directions. Yet another way to see it is that weâ€™re still doing RL (Policy Gradients, or REINFORCE specifically), where the agentâ€™s actions are to emit entire parameter vectors using a gaussian policy.Above: ES optimization process, in a setting with only two parameters and a reward function (red = high, blue = low). At each iteration we show the current parameter value (in white), a population of jittered samples (in black), and the estimated gradient (white arrow). We keep moving the parameters to the top of the arrow until we converge to a local optimum. You can reproduce this figure with this notebook.Code sample. To make the core algorithm concrete and to highlight its simplicity, here is a short example of optimizing a quadratic function using ES (or see this longer version with more comments):1234567891011121314151617# simple example: minimize a quadratic around some solution pointimport numpy as npsolution = np.array([0.5, 0.1, -0.3])def f(w): return -np.sum((w - solution)**2)npop = 50 # population sizesigma = 0.1 # noise standard deviationalpha = 0.001 # learning ratew = np.random.randn(3) # initial guessfor i in range(300): N = np.random.randn(npop, 3) R = np.zeros(npop) for j in range(npop): w_try = w + sigma*N[j] R[j] = f(w_try) A = (R - np.mean(R)) / np.std(R) w = w + alpha/(npop*sigma) * np.dot(N.T, A)Injecting noise in the parameters. Notice that the objective is identical to the one that RL optimizes: the expected reward. However, RL injects noise in the action space and uses backpropagation to compute the parameter updates, while ES injects noise directly in the parameter space. Another way to describe this is that RL is a â€œguess and checkâ€ on actions, while ES is a â€œguess and checkâ€ on parameters. Since weâ€™re injecting noise in the parameters, it is possible to use deterministic policies (and we do, in our experiments). It is also possible to add noise in both actions and parameters to potentially combine the two approaches.Tradeoffs between ES and RLES enjoys multiple advantages over RL algorithms (some of them are a little technical):No need for backpropagation. ES only requires the forward pass of the policy and does not require backpropagation (or value function estimation), which makes the code shorter and between 2-3 times faster in practice. On memory-constrained systems, it is also not necessary to keep a record of the episodes for a later update. There is also no need to worry about exploding gradients in RNNs. Lastly, we can explore a much larger function class of policies, including networks that are not differentiable (such as in binary networks), or ones that include complex modules (e.g. pathfinding, or various optimization layers).Highly parallelizable. ES only requires workers to communicate a few scalars between each other, while in RL it is necessary to synchronize entire parameter vectors (which can be millions of numbers). Intuitively, this is because we control the random seeds on each worker, so each worker can locally reconstruct the perturbations of the other workers. Thus, all that we need to communicate between workers is the reward of each perturbation. As a result, we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization.Higher robustness. Several hyperparameters that are difficult to set in RL implementations are side-stepped in ES. For example, RL is not â€œscale-freeâ€, so one can achieve very different learning outcomes (including a complete failure) with different settings of the frame-skip hyperparameter in Atari. As we show in our work, ES works about equally well with any frame-skip.Structured exploration. Some RL algorithms (especially policy gradients) initialize with random policies, which often manifests as random jitter on spot for a long time. This effect is mitigated in Q-Learning due to epsilon-greedy policies, where the max operation can cause the agents to perform some consistent action for a while (e.g. holding down a left arrow). This is more likely to do something in a game than if the agent jitters on spot, as is the case with policy gradients. Similar to Q-learning, ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration.Credit assignment over long time scales. By studying both ES and RL gradient estimators mathematically we can see that ES is an attractive choice especially when the number of time steps in an episode is long, where actions have longlasting effects, or if no good value function estimates are available.Conversely, we also found some challenges to applying ES in practice. One core problem is that in order for ES to work, adding noise in parameters must lead to different outcomes to obtain some gradient signal. As we elaborate on in our paper, we found that the use of virtual batchnorm can help alleviate this problem, but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary. As an example of a related difficulty, we found that in Montezumaâ€™s Revenge, one is very unlikely to get the key in the first level with a random network, while this is occasionally possible with random actions.ES is competitive with RLWe compared the performance of ES and RL on two standard RL benchmarks: MuJoCo control tasks and Atari game playing. Each MuJoCo task (see examples below) contains a physically-simulated articulated figure, where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward. Below are some example agents trained on three MuJoCo control tasks, where the objective is to move forward:We usually compare the performance of algorithms by looking at their efficiency of learning from data; as a function of how many states weâ€™ve seen, what is our average reward? Here are the example learning curves that we obtain, in comparison to RL (the TRPO algorithm in this case):Data efficiency comparison. The comparisons above show that ES (orange) can reach a comparable performance to TRPO (blue), although it doesnâ€™t quite match or surpass it in all cases. Moreover, by scanning horizontally we can see that ES is less efficient, but no worse than about a factor of 10 (note the x-axis is in log scale).Wall clock comparison. Instead of looking at the raw number of states seen, one can argue that the most important metric to look at is the wall clock time: how long (in number of seconds) does it take to solve a given problem? This quantity ultimately dictates the achievable speed of iteration for a researcher. Since ES requires negligible communication between workers, we were able to solve one of the hardest MuJoCo tasks (a 3D humanoid) using 1,440 CPUs across 80 machines in only 10 minutes. As a comparison, in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours. It is also possible that the performance of RL could also improve with more algorithmic and engineering effort, but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements.Below are a few videos of 3D humanoid walkers trained with ES. As we can see, the results have quite a bit of variety, based on which local minimum the optimization ends up converging into.On Atari, ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day. Below are some result snippets on Pong, Seaquest and Beamrider. These videos show the preprocessed frames, which is exactly what the agent sees when it is playing:In particular, note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels.Related WorkES is an algorithm from the neuroevolution literature, which has a long history in AI and a complete literature review is beyond the scope of this post. However, we encourage an interested reader to look at Wikipedia, Scholarpedia, and JÃ¼rgen Schmidhuberâ€™s review article (Section 6.6). The work that most closely informed our approach is Natural Evolution Strategies by Wierstra et al. 2014. Compared to this work and much of the work it has inspired, our focus is specifically on scaling these algorithms to large-scale, distributed settings, finding components that make the algorithms work better with deep neural networks (e.g. virtual batch norm), and evaluating them on modern RL benchmarks.It is also worth noting that neuroevolution-related approaches have seen some recent resurgence in the machine learning literature, for example with HyperNetworks, â€œLarge-Scale Evolution of Image Classifiersâ€ and â€œConvolution by Evolutionâ€.ConclusionOur work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent-environment benchmarks, while offering significant benefits related to code complexity and ease of scaling to large-scale distributed settings. We also expect that more exciting work can be done by revisiting other ideas from this line of work, such as indirect encoding methods, or evolving the network structure in addition to the parameters.Note on supervised learning. It is also important to note that supervised learning problems (e.g. image classification, speech recognition, or most other tasks in the industry), where one can compute the exact gradient of the loss function with backpropagation, are not directly impacted by these findings. For example, in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1,000 times slower than using backpropagation. It is only in RL settings, where one has to estimate the gradient of the expected reward by sampling, where ES becomes competitive.Code release. Finally, if youâ€™d like to try running ES yourself, we encourage you to dive into the full details by reading our paper or looking at our code on this Github repo.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Seq2Seq with Attention and Beam Search [Repost]]]></title>
      <url>%2F2018%2F01%2F21%2FSeq2Seq-with-Attention-and-Beam-Search-Repost%2F</url>
      <content type="text"><![CDATA[Source Post is hereSequence to Sequence basicsLetâ€™s explain the sequence to sequence framework as weâ€™ll rely on it for our model. Letâ€™s start with the simplest version on the translation task.As an example, letâ€™s translate how are you in French comment vas tu.Vanilla Seq2SeqThe Seq2Seq framework relies on the encoder-decoder paradigm. The encoder encodes the input sequence, while the decoder produces the target sequenceEncoderOur input sequence is how are you. Each word from the input sequence is associated to a vector $w \in \mathbb{R}^d$ (via a lookup table). In our case, we have 3 words, thus our input will be transformed into $[w_0, w_1, w_2] \in \mathbb{R}^{d \times 3}$. Then, we simply run an LSTM over this sequence of vectors and store the last hidden state outputed by the LSTM: this will be our encoder representation $e$. Letâ€™s write the hidden states $[e_0,e_1,e_2]$ (and thus $e=e_2$)Vanilla EncoderDecoderNow that we have a vector ee that captures the meaning of the input sequence, weâ€™ll use it to generate the target sequence word by word. Feed to another LSTM cell: ee as hidden state and a special start of sentence vector $w_{\text{sos}}$ as input. The LSTM computes the next hidden state $h_0 \in \mathbb{R}^{h}$. Then, we apply some function $g : \mathbb{R}^h \mapsto \mathbb{R}^V$ so that $s_0 := g(h_0) \in \mathbb{R}^V$ is a vector of the same size as the vocabulary.$$\begin{align}h_0 &amp;= \operatorname{LSTM}\left(e, w_{sos} \right)\\s_0 &amp;= g(h_0)\\p_0 &amp;= \operatorname{softmax}(s_0)\\i_0 &amp;= \operatorname{argmax}(p_0)\\\end{align}$$Then, apply a softmax to $s_0$ to normalize it into a vector of probabilities $p_0 \in \mathbb{R}^V$ . Now, each entry of $p_0$ will measure how likely is each word in the vocabulary. Letâ€™s say that the word â€œcommentâ€ has the highest probability (and thus $i_0 = \operatorname{argmax}(p_0)$) corresponds to the index of â€œcommentâ€). Get a corresponding vector and $w_{i_0} = w_{comment}$ repeat the procedure: the LSTM will take $h_0$ as hidden state and $w_{comment}$ as input and will output a probability vector $p_1$ over the second word, etc.$$\begin{align}h_1 &amp;= \operatorname{LSTM}\left(h_0, w_{i_0} \right)\\s_1 &amp;= g(h_1)\\p_1 &amp;= \operatorname{softmax}(s_1)\\i_1 &amp;= \operatorname{argmax}(p_1)\end{align}$$The decoding stops when the predicted word is a special end of sentence token.Vanilla DecoderIntuitively, the hidden vector represents the â€œamount of meaningâ€ that has not been decoded yet.The above method aims at modelling the distribution of the next word conditionned on the beginning of the sentence$$\mathbb{P}\left[ y_{t+1} | y_1, \dots, y_{t}, x_0, \dots, x_n \right]$$by writing$$\mathbb{P}\left[ y_{t+1} | y_t, h_{t}, e \right]$$Seq2Seq with AttentionThe previous model has been refined over the past few years and greatly benefited from what is known as attention. Attention is a mechanism that forces the model to learn to focus (=to attend) on specific parts of the input sequence when decoding, instead of relying only on the hidden vector of the decoderâ€™s LSTM. One way of performing attention is explained by Bahdanau et al.. We slightly modify the reccurrence formula that we defined above by adding a new vector $c_t$ to the input of the LSTM$$\begin{align}h_{t} &amp;= \operatorname{LSTM}\left(h_{t-1}, [w_{i_{t-1}}, c_t] \right)\\s_t &amp;= g(h_t)\\p_t &amp;= \operatorname{softmax}(s_t)\\i_t &amp;= \operatorname{argmax}(p_t)\end{align}$$The vector ctct is the attention (or context) vector. We compute a new context vector at each decoding step. First, with a function $f (h_{t-1}, e_{tâ€™}) \mapsto \alpha_{tâ€™} \in \mathbb{R}$, compute a score for each hidden state $e_{t^{\prime}}$ of the encoder. Then, normalize the sequence of using a softmax and compute $c_t$ as the weighted average of the $e_{t^{\prime}}$.$$\begin{align}\alpha_{tâ€™} &amp;= f(h_{t-1}, e_{tâ€™}) \in \mathbb{R} &amp; \text{for all } tâ€™\\\bar{\alpha} &amp;= \operatorname{softmax} (\alpha)\\c_t &amp;= \sum_{tâ€™=0}^n \bar{\alpha}_{tâ€™} e_{tâ€™}\end{align}$$Attention MechanismThe choice of the function ff varies, but is usually one of the following$$f(h_{t-1}, e_{tâ€™}) =\begin{cases}h_{t-1}^T e_{tâ€™} &amp; \text{dot}\\h_{t-1}^T W e_{tâ€™} &amp; \text{general}\\v^T \tanh \left(W [h_{t-1}, e_{tâ€™}]\right) &amp; \text{concat}\\\end{cases}$$It turns out that the attention weighs $\bar{\alpha}$ can be easily interpreted. When generating the word vas(corresponding to are in English), we expect $\bar{\alpha} _ {\text{are}}$ are to be close to $1$ while $\bar{\alpha} _ {\text{how}}$ and $\bar{\alpha} _ {\text{you}}$ to be close to $0$. Intuitively, the context vector $c$ will be roughly equal to the hidden vector of are and it will help to generate the French word vas.By putting the attention weights into a matrix (rows = input sequence, columns = output sequence), we would have access to the alignment between the words from the English and French sentencesâ€¦ (see page 6) There is still a lot of things to say about sequence to sequence models (for instance, it works better if the encoder processes the input sequence backwardsâ€¦).TrainingWhat happens if the first time step is not sure about wether it should generate comment or vas (most likely case at the beginning of the training)? Then it would mess up the entire sequence, and the model will hardly learn anythingâ€¦If we use the predicted token as input to the next step during training (as explained above), errors would accumulate and the model would rarely be exposed to the correct distribution of inputs, making training slow or impossible. To speedup things, one trick is to feed the actual output sequence (&lt;sos&gt; comment vas tu) into the decoderâ€™s LSTM and predict the next token at every position (comment vas tu &lt;eos&gt;).TrainingThe decoder outputs vectors of probability over the vocabulary $p_i \in \mathbb{R}^V$ for each time step. Then, for a given target sequence $y_1, \dots, y_n$, we can compute its probability as the product of the probabilities of each token being produced at each relevant time step:$$\mathbb{P}\left(y_1, \dots, y_m \right) = \prod_{i=1}^m p_i [y_i]$$where $p_i [y_i]$ means that we extract the $y_i$-th entry of the probability vector $p_i$ from the $i$-th decoding step. In particular, we can compute the probability of the actual target sequence. A perfect system would give a probabilty of 1 to this target sequence, so we are going to train our network to maximize the probability of the target sequence, which is the same as minimizing$$\begin{align}-\log \mathbb{P} \left(y_1, \dots, y_m \right) &amp;= - \log \prod_{i=1}^m p_i [y_i]\\&amp;= - \sum_{i=1}^n \log p_i [y_i]\\\end{align}$$in our example, this is equal to$$\log p_1[\text{comment}] - \log p_2[\text{vas}] - \log p_3[\text{tu}] - \log p_4[\text{}]$$and you recognize the standard cross entropy: we actually are minimizing the cross entropy between the target distribution (all one-hot vectors) and the predicted distribution outputed by our model (our vectors $p_i$).DecodingThe main takeaway from the discussion above is that for the same model, we can define different behaviors. In particular, we defined a specific behavior that speeds up training.What about inference/testing time then? Is there an other way to decode a sentence?There indeed are 2 main ways of performing decoding at testing time (translating a sentence for which we donâ€™t have a translation). The first of these methods is the one covered at the beginning of the article: greedy decoding. It is the most natural way and it consists in feeding to the next step the most likely word predicted at the previous step.Greedy Decoder - feeds the best token to the next stepBut didnâ€™t we say that this behavior is likely to accumulate errors?Even after having trained the model, it can happen that the model makes a small error (and gives a small advantage to vas over comment for the first step of the decoding). This would mess up the entire decodingâ€¦There is a better way of performing decoding, called Beam Search. Instead of only predicting the token with the best score, we keep track of $k$ hypotheses (for example $k=5$, we refer to kk as the beam size). At each new time step, for these 5 hypotheses we have $V$ new possible tokens. It makes a total of $5V$ new hypotheses. Then, only keep the $5$ best ones, and so onâ€¦ Formally, define $\mathcal{H}_t$ the set of hypotheses decoded at time step $t$.$$\mathcal{H}_ t := \{ (w^1_1, \dots, w^1_t), \dots, (w^k_1, \dots, w^k_t) \}$$For instance if $k=2$, one possible $\mathcal{H}_2$ would be$$\mathcal{H}_ 2 := \{ (\text{comment vas}), (\text{comment tu}) \}$$Now we consider all the possible candidates $\mathcal{C}_{t+1}$, produced from $\mathcal{H}_t$ by adding all possible new tokens$$\mathcal{C}_ {t+1} := \bigcup_{i=1}^k \{ (w^i_1, \dots, w^i_t, 1), \dots, (w^i_1, \dots, w^i_t, V) \}$$and keep the $k$ highest scores (probability of the sequence). If we keep our example$$\begin{align}\mathcal{C}_ 3 =&amp; \{ (\text{comment vas comment}), (\text{comment vas vas}), (\text{comment vas tu})\} \\\cup &amp; \{ (\text{comment tu comment}), \ \ (\text{comment tu vas}), \ \ (\text{comment tu tu}) \}\end{align}$$and for instance we can imagine that the 2 best ones would be$$\mathcal{H}_ 3 := \{ (\text{comment vas tu}), (\text{comment tu vas}) \}$$Once every hypothesis reached the &lt;eos&gt; token, we return the hypothesis with the highest score.If we use beam search, a small error at the first step might be rectified at the next step, as we keep the gold hypthesis in the beam!ConclusionIn this article we covered the seq2seq concepts. We showed that training is different than decoding. We covered two methods for decoding: greedy and beam search. While beam search generally achieves better results, it is not perfect and still suffers from exposure bias. During training, the model is never exposed to its errors! It also suffers from Loss-Evaluation Mismatch. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentenceâ€¦Now, letâ€™s apply Seq2Seq for LaTeX generation from images!Producing LaTeX code from an imageApproachPrevious part covered the concepts of sequence-to-sequence applied to Machine Translation. The same framework can be applied to our LaTeX generation problem. The input sequence would just be replaced by an image, preprocessed with some convolutional model adapted to OCR (in a sense, if we unfold the pixels of an image into a sequence, this is exactly the same problem). This idea proved to be efficient for image captioning (see the reference paper Show, Attend and Tell). Building on some great work from the Harvard NLP group, my teammate Romain and I chose to follow a similar approach.Keep the seq2seq framework but replace the encoder by a convolutional network over the image!Good Tensorflow implementations of such models were hard to find. Together with this post, I am releasing the code and hope some will find it useful. You can use it to train your own image captioning model or adapt it for a more advanced use. The code does not rely on the Tensorflow Seq2Seq library as it was not entirely ready at the time of the project and I also wanted more flexibility (but adopts a similar interface).DataTo train our model, weâ€™ll need labeled examples: images of formulas along with the LaTeX code used to generate the images. A good source of LaTeX code is arXiv, that has thousands of articles under the .tex format. After applying some heuristics to find equations in the .tex files, keeping only the ones that actually compile, the Harvard NLP group extracted $\sim 100,000$ formulas.Waitâ€¦ Donâ€™t you have a problem as different LaTeX codes can give the same image?Good point: (x^2 + 1) and \left( x^{2} + 1 \right) indeed give the same output. Thatâ€™s why Harvardâ€™s paper found out that normalizing the data using a parser (KaTeX) improved performance. It forces adoption of some conventions, like writing x ^ { 2 } instead of x^2, etc. After normalization, they end up with a .txt file containing one formula per line that looks like1234\alpha + \beta\frac &#123; 1 &#125; &#123; 2 &#125;\frac &#123; \alpha &#125; &#123; \beta &#125;1 + 2From this file, weâ€™ll produce images 0.png, 1.png, etc. and a matching file mapping the image files to the indices (=line numbers) of the formulas12340.png 01.png 12.png 23.png 3The reason why we use this format is that it is flexible and allows you to use the pre-built dataset from Harvard (You may need to use the preprocessing scripts as explained here). Youâ€™ll also need to have pdflatex and ImageMagick installed.We also build a vocabulary, to map LaTeX tokens to indices that will be given as input to our model. If we keep the same data as above, our vocabulary will look like+ 1 2 \alpha \beta \frac { }ModelOur model is going to rely on a variation of the Seq2Seq model, adapted to images. First, letâ€™s define the input of our graph. Not surprisingly we get as input a batch of black-and-white images of shape $[H,W]$ and a batch of formulas (ids of the LaTeX tokens):123456# batch of images, shape = (batch size, height, width, 1)img = tf.placeholder(tf.uint8, shape=(None, None, None, 1), name='img')# batch of formulas, shape = (batch size, length of the formula)formula = tf.placeholder(tf.int32, shape=(None, None), name='formula')# for paddingformula_length = tf.placeholder(tf.int32, shape=(None, ), name='formula_length')A special note on the type of the image input. You may have noticed that we use tf.uint8. This is because our image is encoded in grey-levels (integers from 0 to 255 - and $2^8=256$). Even if we could give a tf.float32 Tensor as input to Tensorflow, this would be 4 times more expensive in terms of memory bandwith. As data starvation is one of the main bottlenecks of GPUs, this simple trick can save us some training time. For further improvement of the data pipeline, have a look at the new Tensorflow data pipeline.EncoderHigh-level idea Apply some convolutional network on top of the image an flatten the output into a sequence of vectors $[e_1, \cdots, e_n]$, each of those corresponding to a region of the input image. These vectors will correspond to the hidden vectors of the LSTM that we used for translation.Once our image is transformed into a sequence, we can use the seq2seq model!Convolutional Encoder - produces a sequence of vectorsWe need to extract features from our image, and for this, nothing has (yet) been proven more effective than convolutions. Here, there is nothing much to say except that we pick some architecture that has been proven to be effective for Optical Character Recognition (OCR), which stacks convolutional layers and max-pooling to produce a Tensor of shape $[H^{\prime},W^{\prime},512]$12345678910111213141516171819# casting the image back to float32 on the GPUimg = tf.cast(img, tf.float32) / 255.out = tf.layers.conv2d(img, 64, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.max_pooling2d(out, 2, 2, "SAME")out = tf.layers.conv2d(out, 128, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.max_pooling2d(out, 2, 2, "SAME")out = tf.layers.conv2d(out, 256, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.conv2d(out, 256, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.max_pooling2d(out, (2, 1), (2, 1), "SAME")out = tf.layers.conv2d(out, 512, 3, 1, "SAME", activation=tf.nn.relu)out = tf.layers.max_pooling2d(out, (1, 2), (1, 2), "SAME")# encoder representation, shape = (batch size, height', width', 512)out = tf.layers.conv2d(out, 512, 3, 1, "VALID", activation=tf.nn.relu)Now that we have extracted some features from the image, letâ€™s unfold the image to get a sequence so that we can use our sequence to sequence framework. We end up with a sequence of length $[H^{\prime},W^{\prime}]$12H, W = tf.shape(out)[1:2]seq = tf.reshape(out, shape=[-1, H*W, 512])Donâ€™t you loose a lot of structural information by reshaping? Iâ€™m afraid that when performing attention over the image, my decoder wonâ€™t be able to understand the location of each feature vector in the original image!It turns out that the model manages to work despite this issue, but thatâ€™s not completely satisfying. In the case of translation, the hidden states of the LSTM contained some positional information that was computed by the LSTM (after all, LSTM are by essence sequential). Can we fix this issue?Positional Embeddings I decided to follow the idea from Attention is All you Need that adds positional embeddings to the image representation (out), and has the huge advantage of not adding any new trainable parameter to our model. The idea is that for each position of the image, we compute a vector of size $512$ such that its components are coscos or sinsin. More formally, the (2i)-th and (2i+1)-th entries of my positional embedding $v$ at position $p$ will be$$\begin{align}v_{2i} &amp;= \sin\left(p / f^{2i}\right)\\v_{2i+1} &amp;= \cos\left(p / f^{2i}\right)\\\end{align}$$where $f$ is some frequency parameter. Intuitively, because $\sin(a+b)$ and $\cosâ¡(a+b)$ can be expressed in terms of $\sinâ¡(b)$ , $\sin(a)$ , $\cosâ¡(b)$ and $\cosâ¡(a)$, there will be linear dependencies between the components of distant embeddings, authorizing the model to extract relative positioning information. Good news: the tensorflow code for this technique is available in the library tensor2tensor, so we just need to reuse the same function and transform our out with the following call1out = add_timing_signal_nd(out)DecoderNow that we have a sequence of vectors $[e_1, \dots, e_n]$ that represents our input image, letâ€™s decode it! First, letâ€™s explain what variant of the Seq2Seq framework we are going to use.First hidden vector of the decoderâ€™s LSTM In the seq2seq framework, this is usually just the last hidden vector of the encoderâ€™s LSTM. Here, we donâ€™t have such a vector, so a good choice would be to learn to compute it with a matrix $W$ and a vector $b$$$h_0 = \tanh\left( W \cdot \left( \frac{1}{n} \sum_{i=1}^n e_i\right) + b \right)$$This can be done in Tensorflow with the following logic1234img_mean = tf.reduce_mean(seq, axis=1)W = tf.get_variable("W", shape=[512, 512])b = tf.get_variable("b", shape=[512])h = tf.tanh(tf.matmul(img_mean, W) + b)Attention Mechanism We first need to compute a score $\alpha_{t^{\prime}}$ for each vector $e_{t^{\prime}}$ of the sequence. We use the following method$$\begin{align}\alpha_{tâ€™} &amp;= \beta^T \tanh\left( W_1 \cdot e_{tâ€™} + W_2 \cdot h_{t} \right)\\\bar{\alpha} &amp;= \operatorname{softmax}\left(\alpha\right)\\c_t &amp;= \sum_{i=1}^n \bar{\alpha}_{tâ€™} e_{tâ€™}\\\end{align}$$This can be done in Tensorflow with the follwing code12345678910111213141516# over the image, shape = (batch size, n, 512)W1_e = tf.layers.dense(inputs=seq, units=512, use_bias=False)# over the hidden vector, shape = (batch size, 512)W2_h = tf.layers.dense(inputs=h, units=512, use_bias=False)# sums the two contributionsa = tf.tanh(W1_e + tf.expand_dims(W2_h, axis=1))beta = tf.get_variable("beta", shape=[512, 1], dtype=tf.float32)a_flat = tf.reshape(a, shape=[-1, 512])a_flat = tf.matmul(a_flat, beta)a = tf.reshape(a, shape=[-1, n])# compute weightsa = tf.nn.softmax(a)a = tf.expand_dims(a, axis=-1)c = tf.reduce_sum(a * seq, axis=1)Note that the line W1_e = tf.layers.dense(inputs=seq, units=512, use_bias=False) is common to every decoder time step, so we can just compute it once and for all. The dense layer with no bias is just a matrix multiplication.Now that we have our attention vector, letâ€™s just add a small modification and compute an other vector $o_{t-1}$ (as in Luong, Pham and Manning) that we will use to make our final prediction and that we will feed as input to the LSTM at the next step. Here $w_{tâˆ’1}$ denotes the embedding of the token generated at the previous step.$o_t$ passes some information about the distribution from the previous time step as well as the confidence it had for the predicted token$$\begin{align}h_t &amp;= \operatorname{LSTM}\left( h_{t-1}, [w_{t-1}, o_{t-1}] \right)\\c_t &amp;= \operatorname{Attention}([e_1, \dots, e_n], h_t)\\o_{t} &amp;= \tanh\left(W_3 \cdot [h_{t}, c_t] \right)\\p_t &amp;= \operatorname{softmax}\left(W_4 \cdot o_{t} \right)\\\end{align}$$and now the code1234567# compute oW3_o = tf.layers.dense(inputs=tf.concat([h, c], axis=-1), units=512, use_bias=False)o = tf.tanh(W3_o)# compute the logits scores (before softmax)logits = tf.layers.dense(inputs=o, units=vocab_size, use_bias=False)# the softmax will be computed in the loss or somewhere elseIf I read carefully, I notice that for the first step of the decoding process, we need to compute an $o_0$ too, right?This is a good point, and we just use the same technique that we used to generate $h_0$ but with different weights.TrainingWeâ€™ll need to create 2 different outputs in the Tensorflow graph: one for training (that uses the formulaand feeds the ground truth at each time step, see part I) and one for test time (that ignores everything about the actual formula and uses the prediction from the previous step).AttentionCellWeâ€™ll need to encapsulate the reccurent logic into a custom cell that inherits RNNCell. Our custom cell will be able to call the LSTM cell (initialized in the __init__). It also has a special recurrent state that combines the LSTM state and the vector oo (as we need to pass it through). An elegant way is to define a namedtuple for this recurrent state:12345678910111213141516171819202122AttentionState = collections.namedtuple("AttentionState", ("lstm_state", "o"))class AttentionCell(RNNCell): def __init__(self): self.lstm_cell = LSTMCell(512) def __call__(self, inputs, cell_state): """ Args: inputs: shape = (batch_size, dim_embeddings) embeddings from previous time step cell_state: (AttentionState) state from previous time step """ lstm_state, o = cell_state # compute h h, new_lstm_state = self.lstm_cell(tf.concat([inputs, o], axis=-1), lstm_state) # apply previous logic c = ... new_o = ... logits = ... new_state = AttentionState(new_lstm_state, new_o) return logits, new_stateThen, to compute our output sequence, we just need to call the previous cell on the sequence of LaTeX tokens. We first produce the sequence of token embeddings to which we concatenate the special &lt;sos&gt; token. Then, we call dynamic_rnn.123456789101112131415# 1. get token embeddingsE = tf.get_variable("E", shape=[vocab_size, 80], dtype=tf.float32)# special &lt;sos&gt; tokenstart_token = tf.get_variable("start_token", dtype=tf.float32, shape=[80])tok_embeddings = tf.nn.embedding_lookup(E, formula)# 2. add the special &lt;sos&gt; token embedding at the beggining of every formulastart_token_ = tf.reshape(start_token, [1, 1, dim])start_tokens = tf.tile(start_token_, multiples=[batch_size, 1, 1])# remove the &lt;eos&gt; that won't be used because we reached the endtok_embeddings = tf.concat([start_tokens, tok_embeddings[:, :-1, :]], axis=1)# 3. decodeattn_cell = AttentionCell()seq_logits, _ = tf.nn.dynamic_rnn(attn_cell, tok_embeddings, initial_state=AttentionState(h_0, o_0))LossCode speaks for itself12345678910# compute - log(p_i[y_i]) for each time step, shape = (batch_size, formula length)losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=seq_logits, labels=formula)# masking the lossesmask = tf.sequence_mask(formula_length)losses = tf.boolean_mask(losses, mask)# averaging the loss over the batchloss = tf.reduce_mean(losses)# building the train opoptimizer = tf.train.AdamOptimizer(learning_rate)train_op = optimizer.minimize(loss)and when iterating over the batches during training, train_op will be given to the tf.Session along with a feed_dict containing the data for the placeholders.Decoding in TensorflowLetâ€™s have a look at the Tensorflow implementation of the Greedy Method before dealing with Beam SearchGreedy SearchWhile greedy decoding is easy to conceptualize, implementing it in Tensorflow is not straightforward, as you need to use the previous prediction and canâ€™t use dynamic_rnn on the formula. There are basically 2 ways of approaching the problemModify our AttentionCell and AttentionState so that AttentionState also contains the embedding of the predicted word at the previous time step,123456789101112131415AttentionState = namedtuple("AttentionState", ("lstm_state", "o", "embedding"))class AttentionCell(RNNCell): def __call__(self, inputs, cell_state): lstm_state, o, embbeding = cell_state # compute h h, new_lstm_state = self.lstm_cell(tf.concat([embedding, o], axis=-1), lstm_state) # usual logic logits = ... # compute new embeddding new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32) new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids) new_state = AttentionState(new_lstm_state, new_o, new_embedding) return logits, new_stateThis technique has a few downsides. It doesnâ€™t use inputs (which used to be the embedding of the gold token from the formula and thus we would have to call dynamic_rnn on a â€œfakeâ€ sequence). Also, how do you know when to stop decoding, once youâ€™ve reached the &lt;eos&gt; token?Implement a variant of dynamic_rnn that would not run on a sequence but feed the prediction from the previous time step to the cell, while having a maximum number of decoding steps. This would involve delving deeper into Tensorflow, using tf.while_loop. Thatâ€™s the method weâ€™re going to use as it fixes all the problems of the first technique. We eventually want something that looks like12345attn_cell = AttentionCell(...)# wrap the attention cell for decodingdecoder_cell = GreedyDecoderCell(attn_cell)# call a special dynamic_decode primitivetest_outputs, _ = dynamic_decode(decoder_cell, max_length_formula+1)Much better isnâ€™t it? Now letâ€™s see what GreedyDecoderCell and dynamic_decode look like.Greedy Decoder CellWe first wrap the attention cell in a GreedyDecoderCell that takes care of the greedy logic for us, without having to modify the AttentionCell12345678910111213141516class DecoderOutput(collections.namedtuple("DecoderOutput", ("logits", "ids"))): passclass GreedyDecoderCell(object): def step(self, time, state, embedding, finished): # next step of attention cell logits, new_state = self._attention_cell.step(embedding, state) # get ids of words predicted and get embedding new_ids = tf.cast(tf.argmax(logits, axis=-1), tf.int32) new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids) # create new state of decoder new_output = DecoderOutput(logits, new_ids) new_finished = tf.logical_or(finished, tf.equal(new_ids, self._end_token)) return (new_output, new_state, new_embedding, new_finished)Dynamic Decode primitiveWe need to implement a function dynamic_decode that will recursively call the above step function. We do this with a tf.while_loop that stops when all the hypotheses reached &lt;eos&gt; or time is greater than the max number of iterations.123456789101112131415161718192021def dynamic_decode(decoder_cell, maximum_iterations): # initialize variables (details on github) def condition(time, unused_outputs_ta, unused_state, unused_inputs, finished): return tf.logical_not(tf.reduce_all(finished)) def body(time, outputs_ta, state, inputs, finished): new_output, new_state, new_inputs, new_finished = decoder_cell.step( time, state, inputs, finished) # store the outputs in TensorArrays (details on github) new_finished = tf.logical_or(tf.greater_equal(time, maximum_iterations), new_finished) return (time + 1, outputs_ta, new_state, new_inputs, new_finished) with tf.variable_scope("rnn"): res = tf.while_loop( condition, body, loop_vars=[initial_time, initial_outputs_ta, initial_state, initial_inputs, initial_finished]) # return the final outputs (details on github)Some details using TensorArrays or nest.map_structure have been omitted for clarity but may be found on githubNotice that we place the tf.while_loop inside a scope named rnn. This is because dynamic_rnndoes the same thing and thus the weights of our LSTM are defined in that scope.Beam Search Decoder CellWe can follow the same approach as in the greedy method and use dynamic_decodeLetâ€™s create a new wrapper for AttentionCell in the same way we did for GreedyDecoderCell. This time, the code is going to be more complicated and the following is just for intuition. Note that when selecting the top kk hypotheses from the set of candidates, we must know which â€œbeginningâ€ they used (=parent hypothesis).1234567891011121314151617181920212223class BeamSearchDecoderCell(object): # notice the same arguments as for GreedyDecoderCell def step(self, time, state, embedding, finished): # compute new logits logits, new_cell_state = self._attention_cell.step(embedding, state.cell_state) # compute log probs of the step (- log p(w) for all words w) # shape = [batch_size, beam_size, vocab_size] step_log_probs = tf.nn.log_softmax(new_logits) # compute scores for the (beam_size * vocabulary_size) new hypotheses log_probs = state.log_probs + step_log_probs # get top k hypotheses new_probs, indices = tf.nn.top_k(log_probs, self._beam_size) # get ids of next token along with the parent hypothesis new_ids = ... new_parents = ... # compute new embeddings, new_finished, new_cell state... new_embedding = tf.nn.embedding_lookup(self._embeddings, new_ids)Look at github for the details. The main idea is that we add a beam dimension to every tensor, but when feeding it into AttentionCell we merge the beam dimension with the batch dimension. There is also some trickery involved to compute the parents and the new ids using modulos.ConclusionI hope that you learned something with this post, either about the technique or Tensorflow. While the model achieves impressive performance (at least on short formulas with roughly 85% of the LaTeX being reconstructed), it still raises some questions that I list here:How do we evaluate the performance of our model?. We can use standard metrics from Machine Translation like BLEU to evaluate how good the decoded LaTeX is compared to the reference. We can also choose to compile the predicted LaTeX sequence to get the image of the formula, and then compare this image to the orignal. As a formula is a sequence, computing the pixel-wise distance wouldnâ€™t really make sense. A good idea is proposed by Harvardâ€™s paper. First, slice the image vertically. Then, compare the edit distance between these slicesâ€¦How to fix exposure bias? While beam search generally achieves better results, it is not perfect and still suffers from exposure bias. During training, the model is never exposed to its errors! It also suffers from Loss-Evaluation Mismatch. The model is optimized w.r.t. token-level cross entropy, while we are interested about the reconstruction of the whole sentenceâ€¦An Example of LaTeX generation - which one is the reference?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Understanding Convolutions]]></title>
      <url>%2F2018%2F01%2F18%2FUnderstanding-Convolutions%2F</url>
      <content type="text"><![CDATA[Lessons from a Dropped BallImagine we drop a ball from some height onto the ground, where it only has one dimension of motion. How likely is it that a ball will go a distance $c$ if you drop it and then drop it again from above the point at which it landed?Letâ€™s break this down. After the first drop, it will land aa units away from the starting point with probability $f(a)$, where $f$ is the probability distribution.Now after this first drop, we pick the ball up and drop it from another height above the point where it first landed. The probability of the ball rolling $b$ units away from the new starting point is $g(b)$, where $g$ may be a different probability distribution if itâ€™s dropped from a different height.If we fix the result of the first drop so we know the ball went distance aa, for the ball to go a total distance cc, the distance traveled in the second drop is also fixed at bb, where $a+b=c$. So the probability of this happening is simply $f(a)â‹…g(b)$.1Letâ€™s think about this with a specific discrete example. We want the total distance $c$ to be 3. If the first time it rolls, $a=2$, the second time it must roll $b=1$ in order to reach our total distance $a+b=3$. The probability of this is $f(2)â‹…g(1)$.However, this isnâ€™t the only way we could get to a total distance of 3. The ball could roll 1 units the first time, and 2 the second. Or 0 units the first time and all 3 the second. It could go any $a$ and $b$, as long as they add to 3.The probabilities are $f(1)â‹…g(2)$ and $f(0)â‹…g(3)$, respectively.In order to find the total likelihood of the ball reaching a total distance of $c$, we canâ€™t consider only one possible way of reaching $c$. Instead, we consider all the possible ways of partitioning $c$ into two drops $a$ and $b$ and sum over the probability of each way.$$â€¦\;\; f(0)\cdot g(3) ~+~ f(1)\cdot g(2) ~+~ f(2)\cdot g(1)\;\;â€¦$$We already know that the probability for each case of $a+b=c$ is simply $f(a)â‹…g(b)$. So, summing over every solution to $a+b=c$, we can denote the total likelihood as:$$\sum_{a+b=c} f(a) \cdot g(b)$$Turns out, weâ€™re doing a convolution! In particular, the convolution of $f$ and $g$, evluated at $c$ is defined:$$(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)~~~~$$If we substitute $b=câˆ’a$, we get:$$(f\ast g)(c) = \sum_a f(a) \cdot g(c-a)$$This is the standard definition2 of convolution.To make this a bit more concrete, we can think about this in terms of positions the ball might land. After the first drop, it will land at an intermediate position aa with probability $f(a)$. If it lands at $a$, it has probability $g(câˆ’a)$ of landing at a position $c$.To get the convolution, we consider all intermediate positions.Visualizing ConvolutionsThereâ€™s a very nice trick that helps one think about convolutions more easily.First, an observation. Suppose the probability that a ball lands a certain distance $x$ from where it started is $f(x)$. Then, afterwards, the probability that it started a distance $x$ from where it landed is $f(âˆ’x)$.If we know the ball lands at a position $c$ after the second drop, what is the probability that the previous position was $a$?So the probability that the previous position was $a$ is $g(âˆ’(aâˆ’c))=g(câˆ’a)$.Now, consider the probability each intermediate position contributes to the ball finally landing at $c$. We know the probability of the first drop putting the ball into the intermediate position a is $f(a)$. We also know that the probability of it having been in $a$, if it lands at $c$ is $g(câˆ’a)$.Summing over the $a$s, we get the convolution.The advantage of this approach is that it allows us to visualize the evaluation of a convolution at a value $c$ in a single picture. By shifting the bottom half around, we can evaluate the convolution at other values of $c$. This allows us to understand the convolution as a whole.For example, we can see that it peaks when the distributions align.And shrinks as the intersection between the distributions gets smaller.By using this trick in an animation, it really becomes possible to visually understand convolutions.Below, weâ€™re able to visualize the convolution of two box functions:From WikipediaArmed with this perspective, a lot of things become more intuitive.Letâ€™s consider a non-probabilistic example. Convolutions are sometimes used in audio manipulation. For example, one might use a function with two spikes in it, but zero everywhere else, to create an echo. As our double-spiked function slides, one spike hits a point in time first, adding that signal to the output sound, and later, another spike follows, adding a second, delayed copy.Higher Dimensional ConvolutionsConvolutions are an extremely general idea. We can also use them in a higher number of dimensions.Letâ€™s consider our example of a falling ball again. Now, as it falls, itâ€™s position shifts not only in one dimension, but in two.Convolution is the same as before:$$(f\ast g)(c) = \sum_{a+b=c} f(a) \cdot g(b)$$Except, now $a$, $b$ and $c$ are vectors. To be more explicit,$$(f\ast g)(c_1, c_2) = \sum_{\begin{array}{c}a_1+b_1=c_1\\a_2+b_2=c_2\end{array}} f(a_1,a_2) \cdot g(b_1,b_2)$$Or in the standard definition:$$(f\ast g)(c_1, c_2) = \sum_{a_1, a_2} f(a_1, a_2) \cdot g(c_1-a_1,~ c_2-a_2)$$Just like one-dimensional convolutions, we can think of a two-dimensional convolution as sliding one function on top of another, multiplying and adding.One common application of this is image processing. We can think of images as two-dimensional functions. Many important image transformations are convolutions where you convolve the image function with a very small, local function called a â€œkernel.â€From the River Trail documentationThe kernel slides to every position of the image and computes a new pixel as a weighted sum of the pixels it floats over.For example, by averaging a 3x3 box of pixels, we can blur an image. To do this, our kernel takes the value $\dfrac{1}{9}$ on each pixel in the box,Derived from the Gimp documentationWe can also detect edges by taking the values âˆ’1âˆ’1 and 11 on two adjacent pixels, and zero everywhere else. That is, we subtract two adjacent pixels. When side by side pixels are similar, this is gives us approximately zero. On edges, however, adjacent pixels are very different in the direction perpendicular to the edge.Derived from the Gimp documentationThe gimp documentation has many other examples.Convolutional Neural NetworksSo, how does convolution relate to convolutional neural networks?Consider a 1-dimensional convolutional layer with inputs $\{x_n\}$ and outputs $\{y_n\}$, like we discussed in the previous post:As we observed, we can describe the outputs in terms of the inputs:$$y_n = A(x_{n}, x_{n+1}, â€¦)$$Generally, $A$ would be multiple neurons. But suppose it is a single neuron for a moment.Recall that a typical neuron in a neural network is described by:$$\sigma(w_0x_0 + w_1x_1 + w_2x_2 ~â€¦~ + b)$$Where $x_0$, $x_1$â€¦ are the inputs. The weights, $w_0$, $w_1$, â€¦ describe how the neuron connects to its inputs. A negative weight means that an input inhibits the neuron from firing, while a positive weight encourages it to. The weights are the heart of the neuron, controlling its behavior.3 Saying that multiple neurons are identical is the same thing as saying that the weights are the same.Itâ€™s this wiring of neurons, describing all the weights and which ones are identical, that convolution will handle for us.Typically, we describe all the neurons in a layer at once, rather than individually. The trick is to have a weight matrix, $W$:$$y = \sigma(Wx + b)$$For example, we get:$$y_0 = \sigma(W_{0,0}x_0 + W_{0,1}x_1 + W_{0,2}x_2 â€¦)$$$$y_1 = \sigma(W_{1,0}x_0 + W_{1,1}x_1 + W_{1,2}x_2 â€¦)$$Each row of the matrix describes the weights connecting a neuron to its inputs.Returning to the convolutional layer, though, because there are multiple copies of the same neuron, many weights appear in multiple positions.Which corresponds to the equations:$$y_0 = \sigma(W_0x_0 + W_1x_1 -b)$$$$y_1 = \sigma(W_0x_1 + W_1x_2 -b)$$So while, normally, a weight matrix connects every input to every neuron with different weights:$$W = \left[\begin{array}{ccccc}W_{0,0} &amp; W_{0,1} &amp; W_{0,2} &amp; W_{0,3} &amp; â€¦\\W_{1,0} &amp; W_{1,1} &amp; W_{1,2} &amp; W_{1,3} &amp; â€¦\\W_{2,0} &amp; W_{2,1} &amp; W_{2,2} &amp; W_{2,3} &amp; â€¦\\W_{3,0} &amp; W_{3,1} &amp; W_{3,2} &amp; W_{3,3} &amp; â€¦\\â€¦ &amp; â€¦ &amp; â€¦ &amp; â€¦ &amp; â€¦\\\end{array}\right]$$The matrix for a convolutional layer like the one above looks quite different. The same weights appear in a bunch of positions. And because neurons donâ€™t connect to many possible inputs, thereâ€™s lots of zeros.$$W = \left[\begin{array}{ccccc}w_0 &amp; w_1 &amp; 0 &amp; 0 &amp; â€¦\\0 &amp; w_0 &amp; w_1 &amp; 0 &amp; â€¦\\0 &amp; 0 &amp; w_0 &amp; w_1 &amp; â€¦\\0 &amp; 0 &amp; 0 &amp; w_0 &amp; â€¦\\â€¦ &amp; â€¦ &amp; â€¦ &amp; â€¦ &amp; â€¦\\\end{array}\right]$$Multiplying by the above matrix is the same thing as convolving with $[â€¦0, w_1, w_0, 0â€¦]$. The function sliding to different positions corresponds to having neurons at those positions.What about two-dimensional convolutional layers?The wiring of a two dimensional convolutional layer corresponds to a two-dimensional convolution.Consider our example of using a convolution to detect edges in an image, above, by sliding a kernel around and applying it to every patch. Just like this, a convolutional layer will apply a neuron to every patch of the image.We want the probability of the ball rolling aa units the first time and also rolling bb units the second time. The distributions $P(A)=f(a)$ and $P(b)=g(b)$ are independent, with both distributions centered at 0. So $P(a,b)=P(a)âˆ—P(b)=f(a)â‹…g(b)$.â†©The non-standard definition, which I havenâ€™t previously seen, seems to have a lot of benefits. In future posts, we will find this definition very helpful because it lends itself to generalization to new algebraic structures. But it also has the advantage that it makes a lot of algebraic properties of convolutions really obvious.For example, convolution is a commutative operation. That is, $fâˆ—g=gâˆ—f$. Why?â€‹$$\sum_{a+b=c} f(a) \cdot g(b) ~~=~ \sum_{b+a=c} g(b) \cdot f(a)$$Convolution is also associative. That is, $(fâˆ—g)âˆ—h=fâˆ—(gâˆ—h)$. Why?â€‹$$\sum_{(a+b)+c=d} (f(a) \cdot g(b)) \cdot h(c) ~~=~ \sum_{a+(b+c)=d} f(a) \cdot (g(b) \cdot h(c))$$â†©â€‹Thereâ€™s also the bias, which is the â€œthresholdâ€ for whether the neuron fires, but itâ€™s much simpler and I donâ€™t want to clutter this section talking about it.â†©]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PCA With Tensorflow]]></title>
      <url>%2F2018%2F01%2F17%2FPCA-With-Tensorflow%2F</url>
      <content type="text"><![CDATA[PCA (Principal Component Analysis) is probably the oldest trick in the book.PCA is well studied and there are numerous ways to get to the same solution, we will talk about two of them here, Eigen decomposition and Singular Value Decomposition (SVD) and then we will implement the SVD way in TensorFlow.From now on, X will be our data matrix, of shape (n, p) where n is the number of examples, and p are the dimensions.So given X, both methods will try to find, in their own way, a way to manipulate and decompose X in a manner that later on we could multiply the decomposed results to represent maximum information in less dimensions. I know I know, sounds horrible but I will spare you most of the math but keep the parts that contribute to the understanding of the method pros and cons.So Eigen decomposition and SVD are both ways to decompose matrices, lets see how they help us in PCA and how they are connected.Take a glance at the flow chart below and I will explain right after.Figure 1 PCA workflowSo why should you care about this? Well there is something very fundamental about the two procedures that tells us a lot about PCA.As you can see both methods are pure linear algebra, that basically tells us that using PCA is looking at the real data, from a different angle â€” this is unique to PCA since the other methods start with random representation of lower dimensional data and try to get it to behave like the high dimensional data.Some other notable things are that all operations are linear and with SVD are super-super fast.Also given the same data PCA will always give the same answer (which is not true about the other two methods).Notice how in SVD we choose the r (r is the number of dimensions we want to reduce to) left most values of Î£ to lower dimensionality?Well there is something special about Î£ .Î£ is a diagonal matrix, there are p (number of dimensions) diagonal values (called singular values) and their magnitude indicates how significant they are to preserving the information.So we can choose to reduce dimensionality, to the number of dimensions that will preserve approx. given amount of percentage of the data and I will demonstrate that in the code (e.g. gives us the ability to reduce dimensionality with a constraint of losing a max of 15% of the data).As you will see, coding this in TensorFlow is pretty simple â€” what we are are going to code is a class that has fit method and a reduce method which we will supply the dimensions to.CODE (PCA)Lets see how the fit method looks like, given self.X contains the data and self.dtype=tf.float321234567891011121314def fit(self): self.graph = tf.Graph() with self.graph.as_default(): self.X = tf.placeholder(self.dtype, shape=self.data.shape) # Perform SVD singular_values, u, _ = tf.svd(self.X) # Create sigma matrix sigma = tf.diag(singular_values) with tf.Session(graph=self.graph) as session: self.u, self.singular_values, self.sigma = session.run([u, singular_values, sigma], feed_dict=&#123;self.X: self.data&#125;)So the goal of fit is to create our Î£ and U for later use.Weâ€™ll start with the line tf.svd which gives us the singular values, which are the diagonal values of what was denoted as Î£ in Figure 1, and the matrices U and V.Then tf.diag is TensorFlowâ€™s way of converting a 1D vector, to a diagonal matrix, which in our case will result in Î£.At the end of the fit call we will have the singular values, Î£ and U.Now lets lets implement reduce.123456789101112131415161718192021def reduce(self, n_dimensions=None, keep_info=None): if keep_info: # Normalize singular values normalized_singular_values = self.singular_values / sum(self.singular_values) # Create the aggregated ladder of kept information per dimension ladder = np.cumsum(normalized_singular_values) # Get the first index which is above the given information threshold index = next(idx for idx, value in enumerate(ladder) if value &gt;= keep_info) + 1 n_dimensions = index with self.graph.as_default(): # Cut out the relevant part from sigma sigma = tf.slice(self.sigma, [0, 0], [self.data.shape[1], n_dimensions]) # PCA pca = tf.matmul(self.u, sigma) with tf.Session(graph=self.graph) as session: return session.run(pca, feed_dict=&#123;self.X: self.data&#125;)So as you can see reduce gets either keep_info or n_dimensions (I didnâ€™t implement the input check where only one must be supplied).If we supply n_dimensions it will simply reduce to that number, but if we supply keep_info which should be a float between 0 and 1, we will preserve that much information from the original data (0.9 â€” preserve 90% of the data).In the first â€˜ifâ€™, we normalize and check how many singular values are needed, basically figuring out n_dimensions out of keep_info.In the graph, we just slice the Î£ (sigma) matrix for as much data as we need and perform the matrix multiplication.So lets try it out on the iris dataset, which is (150, 4) dataset of 3 species of iris flowers.123456789101112from sklearn import datasetsimport matplotlib.pyplot as pltimport seaborn as snstf_pca = TF_PCA(iris_dataset.data, iris_dataset.target)tf_pca.fit()pca = tf_pca.reduce(keep_info=0.9) # Results in 2 dimensionscolor_mapping = &#123;0: sns.xkcd_rgb['bright purple'], 1: sns.xkcd_rgb['lime'], 2: sns.xkcd_rgb['ochre']&#125;colors = list(map(lambda x: color_mapping[x], tf_pca.target))plt.scatter(pca[:, 0], pca[:, 1], c=colors)Figure 2 Iris dataset PCA 2 dimensional plotNot so bad huh?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Word Embedding - Approximating the Softmax [Repost]]]></title>
      <url>%2F2018%2F01%2F16%2FWord-Embedding-Approximating-the-Softmax-Repost%2F</url>
      <content type="text"><![CDATA[This is the second post in a series on word embeddings and representation learning. In the previous post, we gave an overview of word embedding models and introduced the classic neural language model by Bengio et al. (2003), the C&amp;W model by Collobert and Weston (2008), and the word2vec model by Mikolov et al. (2013). We observed that mitigating the complexity of computing the final softmax layer has been one of the main challenges in devising better word embedding models, a commonality with machine translation (MT) (Jean et al. [10]) and language modelling (Jozefowicz et al. [6]).In this post, we will thus focus on giving an overview of various approximations to the softmax layer that have been proposed over the last years, some of which have so far only been employed in the context of language modelling or MT. We will postpone the discussion of additional hyperparameters to the subsequent post.Let us know partially re-introduce the previous postâ€™s notation both for consistency and to facilitate comparison as well as introduce some new notation: We assume a training corpus containing a sequence of $T$ training words $w_1,w_2,w_3, \cdots ,w_T$ that belong to a vocabulary $V$ whose size is $|V|$. Our models generally consider a context $c$ of $n$ words. We associate every word with an input embedding $v_w$ (the eponymous word embedding in the Embedding Layer) with $d$ dimensions and an output embedding $v_{w^{\prime}}$ (the representation of the word in the weight matrix of the softmax layer). We finally optimize an objective function $J_{\theta}$ with regard to our model parameters $\theta$.Recall that the softmax calculates the probability of a word $w$ given its context $c$ and can be computed using the following equation:$$p(w|c) = \frac{\exp(h^{\text{T}} v_{w^{\prime}})}{\sum_{w_i \in V} \exp(h^{\text{T}}v_{w_i}^{\prime})}$$where $h$ is the output vector of the penultimate network layer. Note that we use $c$ for the context as mentioned above and drop the index $t$ of the target word $w_t$ for simplicity. Computing the softmax is expensive as the inner product between $h$ and the output embedding of every word $w_i$ in the vocabulary $V$ needs to be computed as part of the sum in the denominator in order to obtain the normalized probability of the target word $w$ given its context $c$.In the following we will discuss different strategies that have been proposed to approximate the softmax. These approaches can be grouped into softmax-based and sampling-based approaches. Softmax-based approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency. Sampling-based approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax.Softmax-based ApproachesHierarchical SoftmaxHierarchical softmax (H-Softmax) is an approximation inspired by binary trees that was proposed by Morin and Bengio [3]. H-Softmax essentially replaces the flat softmax layer with a hierarchical layer that has the words as leaves, as can be seen in Figure 1.This allows us to decompose calculating the probability of one word into a sequence of probability calculations, which saves us from having to calculate the expensive normalization over all words. Replacing a softmax layer with H-Softmax can yield speedups for word prediction tasks of at least $50 \times $ and is thus critical for low-latency tasks such as real-time communication in Googleâ€™s new messenger app Allo.Figure 1: Hierarchical softmax (Quora)We can think of the regular softmax as a tree of depth 11, with each word in $V$ as a leaf node. Computing the softmax probability of one word then requires normalizing over the probabilities of all $|V|$ leaves. If we instead structure the softmax as a binary tree, with the words as leaf nodes, then we only need to follow the path to the leaf node of that word, without having to consider any of the other nodes.Since a balanced binary tree has a depth of $\log_2(|V|)$ we only need to evaluate at most $\log_2(|V|)$ nodes to obtain the final probability of a word. Note that this probability is already normalized, as the probabilities of all leaves in a binary tree sum to 11 and thus form a probability distribution. To informally verify this, we can reason that at a treeâ€™s root node (Node 0) in Figure 1), the probabilities of branching decisions must sum to 11. At each subsequent node, the probability mass is then split among its children, until it eventually ends up at the leaf nodes, i.e. the words. Since no probability is lost along the way and since all words are leaves, the probabilities of all words must necessarily sum to 11 and hence the hierarchical softmax defines a normalized probability distribution over all words in $V$.To get a bit more concrete, as we go through the tree, we have to be able to calculate the probability of taking the left or right branch at every junction. For this reason, we assign a representation to every node. In contrast to the regular softmax, we thus no longer have output embeddings $v^{\prime}_w$ for every word $w$ â€“ instead, we have embeddings $v^{\prime}_n$ for every node $n$. As we have $|V|âˆ’1$ nodes and each one possesses a unique representation, the number of parameters of H-Softmax is almost the same as for the regular softmax. We can now calculate the probability of going right (or left) at a given node $n$ given the context $c$ the following way:$$p(\text{right}|n,c) = \sigma(h^{\text{T}}v^{\prime}_n).$$This is almost the same as the computations in the regular softmax; now instead of computing the dot product between $h$ and the output word embedding $v^{\prime}_w$, we compute the dot product between $h$ and the embedding $v^{\prime}_w$ of each node in the tree; additionally, instead of computing a probability distribution over the entire vocabulary words, we output just one probability, the probability of going right at node $n$ in this case, with the sigmoid function. Conversely, the probability of turning left is simply $1âˆ’p(\text{right} | n,c)$.Figure 2: Hierarchical softmax computations (Hugo Lachorelleâ€™s Youtube lectures)The probability of a word ww given its context cc is then simply the product of the probabilities of taking right and left turns respectively that lead to its leaf node. To illustrate this, given the context â€œtheâ€, â€œdogâ€, â€œandâ€, â€œtheâ€, the probability of the word â€œcatâ€ in Figure 2 can be computed as the product of the probability of turning left at node 1, turning right at node 2, and turning right at node 5. Hugo Lachorelle gives a more detailed account in his excellent lecture video. Rong [7] also does a good job of explaining these concepts and also derives the derivatives of H-Softmax.Obviously, the structure of the tree is of significance. Intuitively, we should be able to achieve better performance, if we make it easier for the model to learn the binary predictors at every node, e.g. by enabling it to assign similar probabilities to similar paths. Based on this idea, Morin and Bengio use the synsets in WordNet as clusters for the tree. However, they still report inferior performance to the regular softmax. Mnih and Hinton [8] learn the tree structure with a clustering algorithm that recursively partitions the words in two clusters and allows them to achieve the same performance as the regular softmax at a fraction of the computation.Notably, we are only able to obtain this speed-up during training, when we know the word we want to predict (and consequently its path) in advance. During testing, when we need to find the most likely prediction, we still need to calculate the probability of all words, although narrowing down the choices in advance helps here.In practice, instead of using â€œrightâ€ and â€œleftâ€ in order to designate nodes, we can index every node with a bit vector that corresponds to the path it takes to reach that node. In Figure 2, if we assume a 0 bit for turning left and a 1 bit for turning right, we can thus represent the path to â€œcatâ€ as 011.Recall that the path length in a balanced binary tree is $\log_2|V|$. If we set $|V|=10000$, this amounts to an average path length of about $13.3$. Analogously, we can represent every word by the bit vector of its path that is on average $13.3$ bits long. In information theory, this is referred to as an information content of $13.3$ bits per word.A note on the information content of wordsRecall that the information content $I(w)$ of a word $w$ is the negative logarithm of its probability $p(w)$:$$I(w) = âˆ’ \log_2p(w)$$The entropy $H$ of all words in a corpus is then the expectation of the information content of all words in the vocabulary:$$H= \sum_{i \in V} p(w_i) I(w_i)$$We can also conceive of the entropy of a data source as the average number of bits needed to encode it. For a fair coin flip, we need $1$ bit per flip, whereas we need $0$ bits for a data source that always emits the same symbol. For a balanced binary tree, where we treat every word equally, the word entropy $H$ equals the information content $I(w)$ of every word $w$, as each word has the same probability. The average word entropy $H$ in a balanced binary tree with $|V|=10000$ thus coincides with its average path length:$$H = âˆ’ \sum_{i \in V}\frac{1}{10000} \log_2 \frac{â¡1}{10000} = 13.3.$$We saw before that the structure of the tree is important. Notably, we can leverage the tree structure not only to gain better performance, but also to speed up computation: If we manage to encode more information into the tree, we can get away with taking shorter paths for less informative words. Morin and Bengio point out that leveraging word probabilities should work even better; as some words are more likely to occur than others, they can be encoded using less information. They note that the word entropy of their corpus (with $|V|=10,000$) is about $9.16$.Thus, by taking into account frequencies, we can reduce the average number of bits per word in the corpus from $13.3$ to $9.16$ in this case, which amounts to a speed-up of 31%. A Huffman tree, which is used by Mikolov et al. [1] for their hierarchical softmax, generates such a coding by assigning fewer bits to more common symbols. For instance, â€œtheâ€, the most common word in the English language, would be assigned the shortest bit code in the tree, the second most frequent word would be assigned the second-shortest bit code, and so on. While we still need the same number of codes to designate all words, when we predict the words in a corpus, short codes appear now a lot more often, and we consequently need fewer bits to represent each word on average.A coding such as Huffman coding is also known as entropy encoding, as the length of each codeword is approximately proportional to the entropy of each symbol as we have observed. Shannon [5] establishes in his experiments that the lower bound on the information rate in English is between $0.6$ to $1.3$ bits per character; given an average word length of $4.5$, this amounts to $2.7$ - $5.85$ bits per word.To tie this back to language modelling (which we already talked about in the previous post): perplexity, the evaluation measure of language modelling, is $2^H$ where $H$ is the entropy. A unigram entropy of $9.16$ thus entails a still very high perplexity of $2^{9.16}=572.0$. We can render this value more tangible by observing that a model with a perplexity of $572$ is as confused by the data as if it had to choose among $572$ possibilities for each word uniformly and independently.To put this into context: The state-of-the-art language model by Jozefowicz et al. (2016) achieves a perplexity of $24.2$ per word on the 1B Word Benchmark. Such a model would thus require an average of around 4.604.60 bits to encode each word, as $2^{4.60}=24.2$, which is incredibly close to the experimental lower bounds documented by Shannon. If and how we could use such a model to construct a better hierarchical softmax layer is still left to be explored.Differentiated SoftmaxChen et al. [9] introduce a variation on the traditional softmax layer, the Differentiated Softmax (D-Softmax). D-Softmax is based on the intuition that not all words require the same number of parameters: Many occurrences of frequent words allow us to fit many parameters to them, while extremely rare words might only allow to fit a few.In order to do this, instead of the dense matrix of the regular softmax layer of size $dÃ—|V|$ containing the output word embeddings $v^{\prime}_w \in \mathbb{R}^d$, they use a sparse matrix. They then arrange $vâ€²w$ in blocks sorted by frequency, with the embeddings in each block being of a certain dimensionality $d_k$. The number of blocks and their embedding sizes are hyperparameters that can be tuned.Figure 3: Differentiated softmax (Chen et al. (2015))In Figure 3, embeddings in partition $A$ are of dimensionality $d_A$ (these are embeddings of frequent words, as they are allocated more parameters), while embeddings in partitions $B$ and $C$ have $d_B$ and $d_C$ dimensions respectively. Note that all areas not part of any partition, i.e. the non-shaded areas in Figure 1, are set to $0$.The output of the previous hidden layer $h$ is treated as a concatenation of features corresponding to each partition of the dimensionality of that partition, e.g. $h$ in Figure 3 is made up of partitions of size $d_A$, $d_B$, and $d_B$ respectively. Instead of computing the matrix-vector product between the entire output embedding matrix and $h$ as in the regular softmax, D-Softmax then computes the product of each partition and its corresponding section in $h$.As many words will only require comparatively few parameters, the complexity of computing the softmax is reduced, which speeds up training. In contrast to H-Softmax, this speed-up persists during testing. Chen et al. (2015) observe that D-Softmax is the fastest method when testing, while being one of the most accurate. However, as it assigns fewer parameters to rare words, D-Softmax does a worse job at modelling them.CNN-SoftmaxAnother modification to the traditional softmax layer is inspired by recent work by Kim et al. [13] who produce input word embeddings $v_w$ via a character-level CNN. Jozefowicz et al. (2016) in turn suggest to do the same thing for the output word embeddings $v^{\prime}_w$ via a character-level CNN â€“ and refer to this as CNN-Softmax. Note that if we have a CNN at the input and at the output as in Figure 4, the CNN generating the output word embeddings $v^{\prime}_w$ is necessarily different from the CNN generating the input word embeddings $v_w$, just as the input and output word embedding matrices would be different.Figure 4: CNN-Softmax (Jozefowicz et al. (2016))While this still requires computing the regular softmax normalization, this approach drastically reduces the number of parameters of the model: Instead of storing an embedding matrix of $d \times |V|$, we now only need to keep track of the parameters of the CNN. During testing, the output word embeddings $v^{\prime}_w$ can be pre-computed, so that there is no loss in performance.However, as characters are represented in a continuous space and as the resulting model tends to learn a smooth function mapping characters to a word embedding, character-based models often find it difficult to differentiate between similarly spelled words with different meanings. To mitigate this, the authors add a correction factor that is learned per word, which significantly reduces the performance gap between regular and CNN-softmax. By adjusting the dimensionality of the correction term, the authors are able to trade-off model size versus performance.The authors also note that instead of using a CNN-softmax, the output of the previous layer hh can be fed to a character-level LSTM, which predicts the output word one character at a time. Instead of a softmax over words, a softmax outputting a probability distribution over characters would thus be used at every time step. They, however, fail to achieve competitive performance with this layer. Ling et al. [14] use a similar layer for machine translation and achieve competitive results.Sampling-based ApproachesWhile the approaches discussed so far still maintain the overall structure of the softmax, sampling-based approaches on the other hand completely do away with the softmax layer. They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. However, sampling-based approaches are only useful at training time â€“ during inference, the full softmax still needs to be computed to obtain a normalised probability.In order to gain some intuitions about the softmax denominatorâ€™s impact on the loss, we will derive the gradient of our loss function $J_{\theta}$ w.r.t. the parameters of our model $\theta$.During training, we aim to minimize the cross-entropy loss of our model for every word $w$ in the training set. This is simply the negative logarithm of the output of our softmax. If you are unsure of this connection, have a look at Karpathyâ€™s explanation to gain some more intuitions about the connection between softmax and cross-entropy. The loss of our model is then the following:$$J_{\theta} = âˆ’ \log \frac{\exp(h^{\text{T}} v^{\prime}_w)}{\sum_{w_i \in V} \exp(h^{\text{T}} v^{\prime}_{w_i})}.$$Note that in practice $J_{\theta}$ would be the average of all negative log-probabilities over the whole corpus. To facilitate the derivation, we decompose $J_{\theta}$ into a sum as $\log \frac{x}{y} = \log x âˆ’ \log y$:$$J_\theta = - \: h^\top v^{\prime}_{w} + \text{log} \sum_{w_i \in V} \text{exp}(h^\top vâ€™_{w_i})$$For brevity and to conform with the notation of Bengio and SenÃ©cal [4, 15] (note that in the first paper, they compute the gradient of the positive logarithm), we replace the dot product $h^\top vâ€™_{w}$ with $- \mathcal{E}(w)$. Our loss then looks like the following:$$J_\theta = \: \mathcal{E}(w) + \text{log} \sum_{w_i \in V} \text{exp}( - \mathcal{E}(w_i))$$For back-propagation, we can now compute the gradient $\nabla$i of $J_{\theta}$ w.r.t. our modelâ€™s parameters $\theta$:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \nabla_\theta \text{log} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))$$As the gradient of $\log x$ is $\dfrac{1}{x}$, an application of the chain rule yields:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)$$We can now move the gradient inside the sum:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \nabla_\theta \: \text{exp}(- \mathcal{E}(w_i))$$As the gradient of $\exp(x)â€‹$ is just $\exp(x)â€‹$, another application of the chain rule yields:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \dfrac{1}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i)) \nabla_\theta (- \mathcal{E}(w_i))$$We can rewrite this as:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} \dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))} \nabla_\theta (- \mathcal{E}(w_i))$$Note that $\dfrac{\text{exp}(- \mathcal{E}(w_i))}{\sum_{w_i \in V} \text{exp}(- \mathcal{E}(w_i))}$ is just the softmax probability $P(w_i)$ of $w_i$ (we omit the dependence on the context cc here for brevity). Replacing it yields:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) + \sum_{w_i \in V} P(w_i) \nabla_\theta (- \mathcal{E}(w_i))$$Finally, repositioning the negative coefficient in front of the sum yields:$$\nabla_\theta J_\theta = \: \nabla_\theta \mathcal{E}(w) - \sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i)$$Bengio and SenÃ©cal (2003) note that the gradient essentially has two parts: a positive reinforcement for the target word $w$ (the first term in the above equation) and a negative reinforcement for all other words $w_i$, which is weighted by their probability (the second term). As we can see, this negative reinforcement is just the expectation $\mathbb{E}_{w_i \sim P}$ of the gradient of $\mathcal{E}$ for all words $w_i$ in $V$:$$\sum_{w_i \in V} P(w_i) \nabla_\theta \mathcal{E}(w_i) = \mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)]$$The crux of most sampling-based approach now is to approximate this negative reinforcement in some way to make it easier to compute, since we donâ€™t want to sum over the probabilities for all words in $V$.Importance SamplingWe can approximate the expected value $E$ of any probability distribution using the Monte Carlo method, i.e. by taking the mean of random samples of the probability distribution. If we knew the networkâ€™s distribution, i.e. $P(w)$, we could thus directly sample mm words $w_1, \cdots ,w_m$ from it and approximate the above expectation with:$$\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{m} \sum\limits^m_{i=1} \nabla_\theta \mathcal{E}(w_i)$$However, in order to sample from the probability distribution $P$, we need to compute $P$, which is just what we wanted to avoid in the first place. We therefore have find some other distribution $Q$ (we call this the proposal distribution), from which it is cheap to sample and which can be used as the basis of Monte-Carlo sampling. Preferably, $Q$ should also be similar to $P$, since we want our approximated expectation to be as accurate as possible. A straightforward choice in the case of language modelling is to simply use the unigram distribution of the training set for $Q$.This is essentially what classical Importance Sampling (IS) does: It uses Monte-Carlo sampling to approximate a target distribution $P$ via a proposal distribution $Q$. However, this still requires computing $P(w)$ for every word ww that is sampled. To avoid this, Bengio and SenÃ©cal (2003) use a biased estimator that was first proposed by Liu [16]. This estimator can be used when $P(w)$ is computed as a product, which is the case here, since every division can be transformed into a multiplication.Essentially, instead of weighting the gradient $\nabla_\theta \mathcal{E}(w_i)$ with the expensive to compute probability $P_{w_i}$, we weight it with a factor that leverages the proposal distribution $Q$. For biased IS, this factor is $\dfrac{1}{R}r(w_i)$ where $r(w) = \dfrac{\text{exp}(- \mathcal{E}(w))}{Q(w)}$ and $R = \sum^m_{j=1} r(w_j)$.Note that we use $r$ and $R$ instead of $w$ and $W$ as in Bengio and SenÃ©cal (2003, 2008) to avoid name clashes. As we can see, we still compute the numerator of the softmax, but replace the normalisation in the denominator with the proposal distribution $Q$. Our biased estimator that approximates the expectation thus looks like the following:$$\mathbb{E}_{w_i \sim P}[\nabla_\theta \mathcal{E}(w_i)] \approx \dfrac{1}{R} \sum\limits^m_{i=1} r(w_i) \nabla_\theta \: \mathcal{E}(w_i)$$Note that the fewer samples we use, the worse is our approximation. We additionally need to adjust our sample size during training, as the networkâ€™s distribution $P$ might diverge from the unigram distribution $Q$ during training, which leads to divergence of the model, if the sample size that is used is too small. Consequently, Bengio and SenÃ©cal introduce a measure to calculate the effective sample size in order to protect against possible divergence. Finally, the authors report a speed-up factor of $19$ over the regular softmax for this method.Adaptive Importance SamplingBengio and SenÃ©cal (2008) note that for Importance Sampling, substituting more complex distributions, e.g. bigram and trigram distributions, later in training to combat the divergence of the unigram distribution $Q$ from the modelâ€™s true distribution $P$ does not help, as n-gram distributions seem to be quite different from the distribution of trained neural language models. As an alternative, they propose an n-gram distribution that is adapted during training to follow the target distribution $P$ more closely. To this end, they interpolate a bigram distribution and a unigram distribution according to some mixture function, whose parameters they train with SGD for different frequency bins to minimize the Kullback-Leibler divergence between the distribution $Q$ and the target distribution $P$. For experiments, they report a speed-up factor of about $100$.Target SamplingJean et al. (2015) propose to use Adaptive Importance Sampling for machine translation. In order to make the method more suitable for processing on a GPU with limited memory, they limit the number of target words that need to be sampled from. They do this by partitioning the training set and including only a fixed number of sample words in every partition, which form a subset $V^{\prime}$ of the vocabulary.This essentially means that a separate proposal distribution $Q_i$ can be used for every partition ii of the training set, which assigns equal probability to all words included in the vocabulary subset $Vâ€™_i$ and zero probability to all other words.Noise Contrastive EstimationNoise Contrastive Estimation (NCE) (Gutmann and HyvÃ¤rinen) [17] is proposed by Mnih and Teh [18] as a more stable sampling method than Importance Sampling (IS), as we have seen that IS poses the risk of having the proposal distribution $Q$ diverge from the distribution $P$ that should be optimized. In contrast to the former, NCE does not try to estimate the probability of a word directly. Instead, it uses an auxiliary loss that also optimises the goal of maximizing the probability of correct words.Recall the pairwise-ranking criterion of Collobert and Weston (2008) that ranks positive windows higher than â€œcorruptedâ€ windows, which we discussed in the previous post. NCE does a similar thing: We train a model to differentiate the target word from noise. We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive, genuine data from noise samples, as can be seen in Figure 4 below.Figure 4: Noise Contrastive Estimation (Stephan Gouwsâ€™ PhD dissertation [24])For every word $w_i$ given its context $c_i$ of $n$ previous words $w_{t-1} , \cdots , w_{t-n+1}$ in the training set, we thus generate $k$ noise samples $w~ik$ from a noise distribution $Q$. As in IS, we can sample from the unigram distribution of the training set. As we need labels to perform our binary classification task, we designate all correct words $w_i$ given their context $c_i$ as true ($y=1$) and all noise samples $\tilde{w}_{ik}$ as false ($y=0$).We can now use logistic regression to minimize the negative log-likelihood, i.e. cross-entropy of our training examples against the noise (conversely, we could also maximize the positive log-likelihood as some papers do):$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \mathbb{E}_{\tilde{w}_{ik} \sim Q} [ \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]]$$Instead of computing the expectation $\mathbb{E}_{\tilde{w}_{ik} \sim Q}$ of our noise samples, which would still require summing over all words in $V$ to predict the normalised probability of a negative label, we can again take the mean with the Monte Carlo approximation:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + k \: \sum_{j=1}^k \dfrac{1}{k} \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]$$which reduces to:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: P(y=1\:|\:w_i,c_i) + \: \sum_{j=1}^k \:\text{log} \: P(y=0 \:|\:\tilde{w}_{ij},c_i)]$$By generating $k$ noise samples for every genuine word $w_i$ given its context $c$, we are effectively sampling words from two different distributions: Correct words are sampled from the empirical distribution of the training set $P_{\text{train}}$ and depend on their context $c$, whereas noise samples come from the noise distribution $Q$. We can thus represent the probability of sampling either a positive or a noise sample as a mixture of those two distributions, which are weighted based on the number of samples that come from each:$$P(y, w \: | \: c) = \dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)$$Given this mixture, we can now calculate the probability that a sample came from the training $P_{\text{train}}$ distribution as a conditional probability of $y$ given $w$ and $c$:$$P(y=1\:|\:w,c)= \dfrac{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)}{\dfrac{1}{k+1} P_{\text{train}}(w \: | \: c)+ \dfrac{k}{k+1}Q(w)}$$which can be simplified to:$$P(y=1\:|\:w,c)= \dfrac{P_{\text{train}}(w \: | \: c)}{P_{\text{train}}(w \: | \: c) + k \: Q(w)}$$As we donâ€™t know $P_{\text{train}}$ (which is what we would like to calculate), we replace $P_{\text{train}}$ with the probability of our model $P$:$$P(y=1\:|\:w,c)= \dfrac{P(w \: | \: c)}{P(w \: | \: c) + k \: Q(w)}$$The probability of predicting a noise sample ($y=0$) is then simply $P(y=0\:|\:w,c) = 1 - P(y=1\:|\:w,c)$. Note that computing $P(w|c)$, i.e. the probability of a word $w$ given its context $c$ is essentially the definition of our softmax:$$P(w \: | \: c) = \dfrac{\text{exp}({h^\top vâ€™_{w}})}{\sum_{w_i \in V} \text{exp}({h^\top vâ€™_{w_i}})}$$For notational brevity and unambiguity, let us designate the denominator of the softmax with $Z(c)$, since the denominator only depends on $h$, which is generated from $c$ (assuming a fixed $V$). The softmax then looks like this:$$P(w \: | \: c) = \dfrac{\text{exp}({h^\top vâ€™_{w}})}{Z(c)}$$Having to compute $P(w|c)$ means that â€“ again â€“ we need to compute $Z(c)$, which requires us to sum over the probabilities of all words in $V$. In the case of NCE, there exists a neat trick to circumvent this issue: We can treat the normalisation denominator $Z(c)$ as a parameter that the model can learn.Mnih and Teh (2012) and Vaswani et al. [20] actually keep $Z(c)$ fixed at $1$, which they report does not affect the modelâ€™s performance. This assumption has the nice side-effect of reducing the modelâ€™s parameters, while ensuring that the model self-normalises by not depending on the explicit normalisation in $Z(c)$. Indeed, Zoph et al. [19] find that even when learned, $Z(c)$ is close to $1$ and has low variance.If we thus set $Z(c)$ to $1$ in the above softmax equation, we are left with the following probability of word $w$ given a context $c$:$$P(w \: | \: c) = \text{exp}({h^\top vâ€™_{w}})$$We can now insert this term in the above equation to compute $P(y=1|w,c)$:$$P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top vâ€™_{w}})}{\text{exp}({h^\top vâ€™_{w}}) + k \: Q(w)}$$Inserting this term in turn in our logistic regression objective finally yields the full NCE loss:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{\text{exp}({h^\top vâ€™_{w_i}})}{\text{exp}({h^\top vâ€™_{w_i}}) + k \: Q(w_i)} + \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{\text{exp}({h^\top vâ€™_{\tilde{w}_{ij}}})}{\text{exp}({h^\top vâ€™_{\tilde{w}_{ij}}}) + k \: Q(\tilde{w}_{ij})})]$$Note that NCE has nice theoretical guarantees: It can be shown that as we increase the number of noise samples $k$, the NCE derivative tends towards the gradient of the softmax function. Mnih and Teh (2012) report that $25$ noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about $45$. For more information on NCE, Chris Dyer has published some excellent notes [21].One caveat of NCE is that as typically different noise samples are sampled for every training word ww, the noise samples and their gradients cannot be stored in dense matrices, which reduces the benefit of using NCE with GPUs, as it cannot benefit from fast dense matrix multiplications. Jozefowicz et al. (2016) and Zoph et al. (2016) independently propose to share noise samples across all training words in a mini-batch, so that NCE gradients can be computed with dense matrix operations, which are more efficient on GPUs.Similarity between NCE and ISJozefowicz et al. (2016) show that NCE and IS are not only similar as both are sampling-based approaches, but are strongly connected. While NCE uses a binary classification task, they show that IS can be described similarly using a surrogate loss function: Instead of performing binary classification with a logistic loss function like NCE, IS then optimises a multi-class classification problem with a softmax and cross-entropy loss function. They observe that as IS performs multi-class classification, it may be a better choice for language modelling, as the loss leads to tied updates between the data and noise samples rather than independent updates as with NCE. Indeed, Jozefowicz et al. (2016) use IS for language modelling and obtain state-of-the-art performance (as mentioned above) on the 1B Word benchmark.Negative SamplingNegative Sampling (NEG), the objective that has been popularised by Mikolov et al. (2013), can be seen as an approximation to NCE. As we have mentioned above, NCE can be shown to approximate the loss of the softmax as the number of samples kk increases. NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modelling.NEG also uses a logistic loss function to minimise the negative log-likelihood of words in the training set. Recall that NCE calculated the probability that a word $w$ comes from the empirical training distribution $P_{\text{train}}$ given a context $c$ as follows:$$P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top vâ€™_{w}})}{\text{exp}({h^\top vâ€™_{w}}) + k \: Q(w)}$$The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible. For this reason, it sets the most expensive term, $kQ(w)$ to $1$, which leaves us with:$$P(y=1\:|\:w,c)= \dfrac{\text{exp}({h^\top vâ€™_{w}})}{\text{exp}({h^\top vâ€™_{w}}) + 1}$$$kQ(w)=1$ is exactly then true, when $k=|V|$ and $Q$ is a uniform distribution. In this case, NEG is equivalent to NCE. The reason we set $kQ(w)=1$ and not to some other constant can be seen by rewriting the equation, as $P(y=1|w,c)$ can be transformed into the sigmoid function:$$P(y=1\:|\:w,c)= \dfrac{1}{1 + \text{exp}({-h^\top vâ€™_{w}})}$$If we now insert this back into the logistic regression loss from before, we get:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top vâ€™_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (1 - \dfrac{1}{1 + \text{exp}({-h^\top vâ€™_{\tilde{w}_{ij}}})}]$$By simplifying slightly, we obtain:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: \dfrac{1}{1 + \text{exp}({-h^\top vâ€™_{w_i}})} + \: \sum_{j=1}^k \:\text{log} \: (\dfrac{1}{1 + \text{exp}({h^\top vâ€™_{\tilde{w}_{ij}}})}]$$Setting $\sigma(x) = \dfrac{1}{1 + \text{exp}({-x})}$ finally yields the NEG loss:$$J_\theta = - \sum_{w_i \in V} [ \text{log} \: \sigma(h^\top vâ€™_{w_i}) + \: \sum_{j=1}^k \:\text{log} \: \sigma(-h^\top vâ€™_{\tilde{w}_{ij}})]$$To conform with the notation of Mikolov et al. (2013), $h$ must be replaced with $v_{w_{I}}$, vâ€²wivwiâ€² with vâ€²wOvwOâ€² and vw~ijvw~ij with vâ€²wivwiâ€². Also, in contrast to Mikolovâ€™s NEG objective, we a) optimise the objective over the whole corpus, b) minimise negative log-likelihood instead of maximising positive log-likelihood (as mentioned before), and c) have already replaced the expectation Ew~ikâˆ¼QEw~ikâˆ¼Q with its Monte Carlo approximation. For more insights on the derivation of NEG, have a look at Goldberg and Levyâ€™s notes [22].We have seen that NEG is only equivalent to NCE when $k=|V|$ and $Q$ is uniform. In all other cases, NEG only approximates NCE, which means that it will not directly optimise the likelihood of correct words, which is key for language modelling. While NEG may thus be useful for learning word embeddings, its lack of asymptotic consistency guarantees makes it inappropriate for language modelling.Self-NormalisationEven though the self-normalisation technique proposed by Devlin et al. 23 is not a sampling-based approach, it provides further intuitions on self-normalisation of language models, which we briefly touched upon. We previously mentioned in passing that by setting the denominator $Z(c)$ of the NCE loss to $1$, the model essentially self-normalises. This is a useful property as it allows us to skip computing the expensive normalisation in $Z(c)$.Recall that our loss function $J_{\theta}$ minimises the negative log-likelihood of all words $w_i$ in our training data:$$J_\theta = - \sum\limits_i [\text{log} \: \dfrac{\text{exp}({h^\top vâ€™_{w_i}})}{Z(c)}]$$We can decompose the softmax into a sum as we did before:$$J_\theta \: P(w \: | \: c) = - \sum\limits_i [h^\top vâ€™_{w_i} + \text{log} \: Z(c)]$$If we are able to constrain our model so that it sets $Z(c)=1$ or similarly $\log Z(c)=0$, then we can avoid computing the normalisation in $Z(c)$ altogether. Devlin et al. (2014) thus propose to add a squared error penalty term to the loss function that encourages the model to keep $\log Z(c)$ as close as possible to $0$:$$J_\theta = - \sum\limits_i [h^\top vâ€™_{w_i} + \text{log} \: Z(c) - \alpha \:(\text{log}(Z(c)) - 0)^2]$$which can be rewritten as:$$J_\theta = - \sum\limits_i [h^\top vâ€™_{w_i} + \text{log} \: Z(c) - \alpha \: \text{log}^2 Z(c)]$$where Î±Î± allows us to trade-off between model accuracy and mean self-normalisation. By doing this, we can essentially guarantee that $Z(c)$ will be as close to $1$ as we want. At decoding time in their MT system, Devlin et al. (2014) then set the denominator of the softmax to $1$ and only use the numerator for computing P(w|c)P(w|c) together with their penalty term:$$J_\theta = - \sum\limits_i [h^\top vâ€™_{w_i} - \alpha \: \text{log}^2 Z(c)]$$They report that self-normalisation achieves a speed-up factor of about $15$, while only resulting in a small degradation of BLEU scores compared to a regular non-self-normalizing neural language model.Infrequent NormalisationAndreas and Klein [11] suggest that it should even be sufficient to only normalise a fraction of the training examples and still obtain approximate self-normalising behaviour. They thus propose Infrequent Normalisation (IN), which down-samples the penalty term, making this a sampling-based approach.Let us first decompose the sum of the previous loss $J_{\theta}$ into two separate sums:$$J_\theta = - \sum\limits_i h^\top vâ€™_{w_i} + \alpha \sum\limits_i \text{log}^2 Z(c)$$We can now down-sample the second term by only computing the normalisation for a subset $C$ of words $w_j$ and thus of contexts $c_j$ (as $Z(c)$ only depends on the context $c$) in the training data:$$J_\theta = - \sum\limits_i h^\top vâ€™_{w_i} + \dfrac{\alpha}{\gamma} \sum\limits_{c_j \in C} \text{log}^2 Z(c_j)$$where $\gamma$ controls the size of the subset $C$. Andreas and Klein (2015) suggest that IF combines the strengths of NCE and self-normalisation as it does not require computing the normalisation for all training examples (which NCE avoids entirely), but like self-normalisation allows trading-off between the accuracy of the model and how well normalisation is approximated. They observe a speed-up factor of $10$ when normalising only a tenth of the training set, with no noticeable performance penalty.Other ApproachesSo far, we have focused exclusively on approximating or even entirely avoiding the computation of the softmax denominator $Z(c)$, as it is the most expensive term in the computation. We have thus not paid particular attention to $h^\top vâ€™_{w}$, i.e. the dot-product between the penultimate layer representation hh and output word embedding $v^{\prime}_w$. Vijayanarasimhan et al. [12] propose fast locality-sensitive hashing to approximate $h^\top v^{\prime}_{w}$. However, while this technique accelerates the model at test time, during training, these speed-ups virtually vanish as embeddings must be re-indexed and the batch size increases.Which Approach to Choose?Having reviewed the most popular softmax-based and sampling-based approaches, we have shown that there are plenty of alternatives to the good olâ€™ softmax and almost all of them promise a significant speed-up and equivalent or at most marginally deteriorated performance. This naturally poses the question which approach is the best for a particular task.ApproachSpeed-upfactorDuringtraining?Duringtesting?Performance(small vocab)Performance(large vocab)Proportion ofparametersSoftmax1x--very goodvery poor100%Hierarchical Softmax25x (50-100x)X-very poorvery good100%Differentiated Softmax2xXXvery goodvery good&lt; 100%CNN-Softmax-X--bad - good30%Importance Sampling(19x)X---100%AdaptiveImportance Sampling(100x)X---100%Target Sampling2xX-goodbad100%Noise ContrastiveEstimation8x (45x)X-very badvery bad100%Negative Sampling(50-100x)X---100%Self-Normalisation(15x)X---100%InfrequentNormalisation6x (10x)X-very goodgood100%Table 1: Comparison of approaches to approximate the softmax for language modelling.We compare the performance of the approaches we discussed in this post for language modelling in Table 1. Speed-up factors and performance are based on the experiments by Chen et al. (2015), while we show speed-up factors reported by the authors of the original papers in brackets. The third and fourth columns indicate if the speed-up is achieved during training and testing respectively. Note that divergence of speed-up factors might be due to unoptimised implementations or the fact that the original authors might not have had access to GPUs, which benefit the regular softmax more than some of the other approaches. Performance for approaches where no comparison is available should largely be analogous to similar approaches, i.e. Self-Normalisation should achieve similar performance as Infrequent Normalisation and Importance Sampling and Adaptive Importance Sampling should achieve similar performance as Target Sampling. The performance of CNN-Softmax is as reported by Jozefowicz et al. (2016) and ranges from bad to good depending on the size of the correction. Of all approaches, only CNN-Softmax achieves a substantial reduction in parameters as the other approaches still require storing output embeddings. Differentiated Softmax reduces parameters by being able to store a sparse weight matrix.As it always is, there is no clear winner that beats all other approaches on all datasets or tasks. For language modelling, the regular softmax still achieves very good performance on small vocabulary datasets, such as the Penn Treebank, and even performs well on medium datasets, such as Gigaword, but does very poorly on large vocabulary datasets, e.g. the 1B Word Benchmark. Target Sampling, Hierarchical Softmax, and Infrequent Normalisation in turn do better with large vocabularies.Differentiated Softmax generally does well for both small and large vocabularies and is the only approach that ensures a speed-up at test time. Interestingly, Hierarchical Softmax (HS) performs very poorly with small vocabularies. However, of all methods, HS is the fastest and processes most training examples in a given time frame. While NCE performs well with large vocabularies, it is generally worse than the other methods. Negative Sampling does not work well for language modelling, but it is generally superior for learning word representations, as attested by word2vecâ€™s success. Note that all results should be taken with a grain of salt: Chen et al. (2015) report having difficulties using Noise Contrastive Estimation in practice; Kim et al. (2016) use Hierarchical Softmax to achieve state-of-the-art with a small vocabulary, while Importance Sampling is used by the state-of-the-art language model by Jozefowicz et al. (2016) on a dataset with a large vocabulary.Finally, if you are looking to actually use the described methods, TensorFlow has implementations for a few sampling-based approaches and also explains the differences between some of them here.ConclusionThis overview of different methods to approximate the softmax attempted to provide you with intuitions that can not only be applied to improve and speed-up learning word representations, but are also relevant for language modelling and machine translation. As we have seen, most of these approaches are closely related and are driven by one uniting factor: the necessity to approximate the expensive normalisation in the denominator of the softmax. With these approaches in mind, I hope you feel now better equipped to train and understand your models and that you might even feel ready to work on learning better word representations yourself.As we have seen, learning word representations is a vast field and many factors are relevant for success. In the previous blog post, we looked at the architectures of popular models and in this blog post, we investigated more closely a key component, the softmax layer. In the next one, we will introduce GloVe, a method that relies on matrix factorisation rather than language modelling, and turn our attention to other hyperparameters that are essential for successfully learning word embeddings.As always, let me know about any mistakes I made and approaches I missed in the comments below.CitationIf you found this blog post helpful, please consider citing it as:Sebastian Ruder. On word embeddings - Part 2: Approximating the Softmax. http://ruder.io/word-embeddings-softmax, 2016.Other blog posts on word embeddingsIf you want to learn more about word embeddings, these other blog posts on word embeddings are also available:On word embeddings - Part 1On word embeddings - Part 3: The secret ingredients of word2vecUnofficial Part 4: A survey of cross-lingual embedding modelsUnofficial Part 5: Word embeddings in 2017 - Trends and future directionsTranslationsThis blog post has been translated into the following languages:ChineseReferencesMikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. NIPS, 1â€“9. Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of the International Conference on Learning Representations (ICLR 2013), 1â€“12. Morin, F., &amp; Bengio, Y. (2005). Hierarchical Probabilistic Neural Network Language Model. Aistats, 5. Bengio, Y., &amp; SenÃ©cal, J.-S. (2003). Quick Training of Probabilistic Neural Nets by Importance Sampling. AISTATS. http://www.iro.umontreal.ca/~lisa/pointeurs/submit_aistats2003.pdf Shannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50â€“64. http://doi.org/10.1002/j.1538-7305.1951.tb01366.x Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., &amp; Wu, Y. (2016). Exploring the Limits of Language Modeling. Retrieved from http://arxiv.org/abs/1602.02410 Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738, 1â€“19. Retrieved from http://arxiv.org/abs/1411.2738 Mnih, A., &amp; Hinton, G. E. (2008). A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, 1â€“8. Retrieved from http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf Chen, W., Grangier, D., &amp; Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. Retrieved from http://arxiv.org/abs/1512.04906 Jean, S., Cho, K., Memisevic, R., &amp; Bengio, Y. (2015). On Using Very Large Target Vocabulary for Neural Machine Translation. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 1â€“10. Retrieved from http://www.aclweb.org/anthology/P15-1001 Andreas, J., &amp; Klein, D. (2015). When and why are log-linear models self-normalizing? Naacl-2015, 244â€“249. Vijayanarasimhan, S., Shlens, J., Monga, R., &amp; Yagnik, J. (2015). Deep Networks With Large Output Spaces. Iclr, 1â€“9. Retrieved from http://arxiv.org/abs/1412.7479 Kim, Y., Jernite, Y., Sontag, D., &amp; Rush, A. M. (2016). Character-Aware Neural Language Models. AAAI. Retrieved from http://arxiv.org/abs/1508.06615 Ling, W., Trancoso, I., Dyer, C., &amp; Black, A. W. (2016). Character-based Neural Machine Translation. ICLR, 1â€“11. Retrieved from http://arxiv.org/abs/1511.04586 Bengio, Y., &amp; SenÃ©cal, J.-S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713â€“722. http://doi.org/10.1109/TNN.2007.912312 Liu, J. S. (2001). Monte Carlo Strategies in Scientific Computing. Springer. http://doi.org/10.1017/CBO9781107415324.004 Gutmann, M., &amp; HyvÃ¤rinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. International Conference on Artificial Intelligence and Statistics, 1â€“8. Retrieved from http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf Mnih, A., &amp; Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICMLâ€™12), 1751â€“1758. Zoph, B., Vaswani, A., May, J., &amp; Knight, K. (2016). Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies. NAACL. Vaswani, A., Zhao, Y., Fossum, V., &amp; Chiang, D. (2013). Decoding with Large-Scale Neural Language Models Improves Translation. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), (October), 1387â€“1392. Dyer, C. (2014). Notes on Noise Contrastive Estimation and Negative Sampling. Arxiv preprint. Retrieved from http://arxiv.org/abs/1410.8251 Goldberg, Y., &amp; Levy, O. (2014). word2vec Explained: Deriving Mikolov et al.â€™s Negative-Sampling Word-Embedding Method. arXiv Preprint arXiv:1402.3722, (2), 1â€“5. Retrieved from http://arxiv.org/abs/1402.3722 Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., &amp; Makhoul, J. (2014). Fast and robust neural network joint models for statistical machine translation. Proc. ACLâ€™2014, 1370â€“1380. Gouws, S. (2016). Training neural word embeddings for transfer learning and translation (Doctoral dissertation, Stellenbosch: Stellenbosch University). Credit for the cover image goes to Stephan Gouws who included the image in his PhD dissertation and in the Tensorflow word2vec tutorial.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[DropConnect Implementation in Python and TensorFlow [Repost]]]></title>
      <url>%2F2018%2F01%2F15%2FDropConnect-Implementation-in-Python-and-TensorFlow-Repost%2F</url>
      <content type="text"><![CDATA[Source post is here.I wouldnâ€™t expect DropConnect to appear in TensorFlow, Keras, or Theano since, as far as I know, itâ€™s used pretty rarely and doesnâ€™t seem as well-studied or demonstrably more useful than its cousin, Dropout. However, there donâ€™t seem to be any implementations out there, so Iâ€™ll provide a few ways of doing so.For the briefest of refreshers, DropConnect (Wan et al.) regularizes networks like Dropout. Instead of dropping neurons, DropConnect regularizes by randomly dropping a subset of weights. A binary mask drawn from a Bernoulli distribution is applied to the original weight matrix (weâ€™re just setting some connections to 0 with a certain probability):where a is an activation function, v is input matrix, W is weight matrix, is Hadamard (element-wise multiplication), and M is the binary mask drawn from a Bernoulli distribution with probability p.Pure Python:1234567891011121314151617import operatorimport numpy as np def mask_size_helper(args): # multiply n dimensions to get array size return reduce(operator.mul, args) def create_dropconnect_mask(dc_keep_prob, dimensions): # get binary mask of size=*dimensions from binomial dist. with dc_keep_prob = prob of drawing a 1 mask_vector = np.random.binomial(1, dc_keep_prob, mask_size_helper(dimensions)) # reshape mask to correct dimensions (we could just broadcast, but that's messy) mask_array = mask_vector.reshape(dimensions) return mask_array def dropconnect(W, dc_keep_prob): dimensions = W.shape return W * create_dropconnect_mask(dc_keep_prob, dimensions)TensorFlow (unnecessarily hard way):12345def dropconnect(W, p): M_vector = tf.multinomial(tf.log([[1-p, p]]), np.prod(W_shape)) M = tf.reshape(M_vector, W_shape) M = tf.cast(M, tf.float32) return M * WTensorFlow (easy way / recommended):12def dropconnect(W, p): return tf.nn.dropout(W, keep_prob=p) * pYes, sadly after a good amount of time spent searching for existing implementations and then creating my own, I took a look at the dropout source code and found that plain old dropout does the job so long as you remember to scale the weight matrix back down by keep_prob. After realizing that a connection weight matrix used for DropConnect is compatible input for the layer of neurons used in dropout, the only actual implementation difference between Dropout and DropConnect on TensorFlow is whether or not the weights in the masked matrix get scaled up (to preserve the expected sum).I find DropConnect interesting, not so much as a regularization method but for some novel extensions that Iâ€™d like to try. Iâ€™ve played around with using keep_prob in our new DropConnect function as a trainable variable in the graph so that, if you incorporate keep_prob into the loss function in a way that creates interesting gradients, you can punish your network for the amount of connections it makes between neurons.More interesting would be to see if we can induce modularity in the network by persisting dropped connections. That is, instead of randomly dropping an entirely new subset of connections at each training example, connections would drop and stay dropped perhaps as a result of the input data class or the connectionâ€™s contribution to deeper layers. For another postâ€¦]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Short Video Title Classification]]></title>
      <url>%2F2017%2F12%2F06%2FShort-Video-Title-Classification-Problem%2F</url>
      <content type="text"><![CDATA[æœ¬æ–‡æ¡£æ˜¯Githubé¡¹ç›®çš„æµç¨‹è§£é‡Šæ–‡æ¡£ï¼Œå…·ä½“å®žçŽ°è¯·ç§»æ­¥ã€‚æœ¬é¡¹ç›®è§£å†³çš„æ˜¯è§†é¢‘çŸ­æ ‡é¢˜çš„å¤šåˆ†ç±»é—®é¢˜ï¼Œç›®å‰æ¶‰åŠåˆ°33ä¸ªç±»ï¼Œæ‰€é‡‡ç”¨çš„ç®—æ³•åŒ…æ‹¬TextCNNï¼ŒTextRNNï¼ŒTextRCNNä»¥åŠHANã€‚ç›®å‰æ•ˆæžœæœ€å¥½çš„æ˜¯TextCNNç®—æ³•ã€‚é¡¹ç›®æµç¨‹å¤§ä½“æ¡†æž¶å¦‚ä¸‹ï¼šæ•°æ®é¢„å¤„ç†æ•°æ®é¢„å¤„ç†éƒ¨åˆ†ä¸»è¦æ¶‰åŠåˆ°çš„æ–‡ä»¶æœ‰ï¼šordered_set.pypreprocess.pyå¤§è‡´æµç¨‹å¦‚ä¸‹ï¼šæ•°æ®åŠ è½½åˆå§‹çš„æ–‡ä»¶åŒ…æ‹¬ä¸‰ä¸ªï¼šall_video_info.txt è¯¥æ–‡ä»¶æ˜¯åŽä¸¤ä¸ªæ•°æ®çš„åˆå¹¶ï¼Œä½œä¸ºæ•°æ®é¢„å¤„ç†ç®—æ³•è¾“å…¥all_video_info_month_day.txtï¼ˆè¿™é‡Œçš„monthå’Œdayç”±å…·ä½“æ•°å€¼æ›¿æ¢ï¼‰è¿™ç±»æ–‡ä»¶åŒ…å«å¤šä¸ªï¼Œåªä½¿ç”¨æœ€æ–°çš„ï¼Œæ˜¯æ­£å¼çš„æ ‡é¢˜æ•°æ®ï¼Œ åŒ…æ‹¬å·²æ ‡è®°çš„ä»¥åŠæœªæ ‡è®°çš„add_signed_video_info.txt è¯¥æ–‡ä»¶æ˜¯ä»Žå…¶ä»–æ•°æ®åº“ä¸­é€‰å–çš„ç»äººå·¥æ ‡æ³¨çš„æ•°æ®ï¼Œåªå«æœ‰å·²æ ‡è®°çš„æ ‡é¢˜æ‰€æœ‰æ–‡ä»¶çš„æ ¼å¼éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬ï¼Œåˆ†ä¸ºå››åˆ—ï¼Œä¸­é—´ç”¨åˆ¶è¡¨ç¬¦é—´éš”ã€‚å…¶ä¸­ç¬¬ä¸€åˆ—ä»£è¡¨è§†é¢‘URLï¼›ç¬¬äºŒåˆ—ä¸ºè¯¥è§†é¢‘ç±»åˆ«æ˜¯å¦ç»è¿‡ç®—æ³•ä¿®æ”¹ï¼Œæœ€å¼€å§‹å…¨éƒ½ä¸º0ï¼›ç¬¬ä¸‰åˆ—ä¸ºè§†é¢‘æ ‡ç­¾ï¼›ç¬¬å››åˆ—ä¸ºè§†é¢‘æ ‡é¢˜ã€‚è§†é¢‘æ ‡ç­¾çš„æ˜ å°„è¡¨å¦‚ä¸‹ï¼šåœ¨æ•°æ®åŠ è½½éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†ä¸ºæœ‰æ ‡è®°æ•°æ®ä»¥åŠæ— æ ‡è®°æ•°æ®ï¼Œæœ‰æ ‡è®°æ•°æ®å°†ç”¨æ¥è®­ç»ƒä»¥åŠæµ‹è¯•åˆ†ç±»å™¨ï¼Œç„¶åŽç”¨è®­ç»ƒå¥½çš„åˆ†ç±»å™¨é¢„æµ‹æ— æ ‡è®°æ•°æ®çš„æ ‡ç­¾ã€‚åˆ†ç±»çš„ä¾æ®é¦–å…ˆæ˜¯æ ¹æ®è§†é¢‘æ ‡ç­¾æ˜¯å¦ä¸º0ï¼Œå¦‚æžœä¸º0ï¼Œä»£è¡¨è§†é¢‘æ˜¯æœªæ ‡è®°çš„ã€‚å…¶æ¬¡ï¼Œå·²æ ‡è®°çš„æ•°æ®ä¸­æœ‰äº›ç±»åˆ«æ˜¯ä¼šå¯¹ç®—æ³•é€ æˆå¹²æ‰°ï¼Œè¿™é‡Œæˆ‘ä»¬ä¹Ÿå°†å…¶åŽ»æŽ‰ã€‚å…·ä½“ä»£ç å‚ç…§preprocess.pyæ–‡ä»¶ä¸­çš„load_dataæ–¹æ³•ã€‚åŽ»é™¤ç‰¹æ®Šç¬¦å·ç”±äºŽè§†é¢‘æ ‡é¢˜ä¸­å­˜åœ¨ä¸€äº›è¡¨æƒ…ç­‰ç‰¹æ®Šç¬¦å·ï¼Œåœ¨è¿™ä¸ªé˜¶æ®µå°†å…¶åŽ»æŽ‰ã€‚å…·ä½“ä»£ç å‚ç…§preprocess.pyæ–‡ä»¶ä¸­çš„remove_emojiæ–¹æ³•ã€‚åˆ†è¯æœ¬é¡¹ç›®é‡‡ç”¨ç»“å·´åˆ†è¯ä½œä¸ºåˆ†è¯å™¨ã€‚å…·ä½“ä»£ç å‚ç…§preprocess.pyæ–‡ä»¶ä¸­çš„cutæ–¹æ³•ã€‚åŽ»åœæ­¢è¯æœ¬é¡¹ç›®é‡‡ç”¨äº†data/stopword.dicæ–‡ä»¶ä¸­çš„åœæ­¢è¯è¡¨ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¥å­åŽ»åœæ­¢è¯å‰åŽåŽ»åœæ­¢è¯åŽï¼Œå•è¯çš„ç›¸å¯¹é¡ºåºä¿æŒä¸å˜ã€‚è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨äº†æœ‰åºé›†åˆï¼ˆå…·ä½“å®žçŽ°åœ¨ordered_set.pyæ–‡ä»¶ä¸­ï¼‰å®žçŽ°ã€‚ç»è¿‡è¿™ä¸€æ­¥ä¹‹åŽï¼Œå¥å­ä¸­é‡å¤çš„éžåœæ­¢è¯å°†åªä¼šå–ä¸€æ¬¡ã€‚ä½†æ˜¯ç”±äºŽè§†é¢‘æ ‡é¢˜è¾ƒçŸ­ï¼Œå‡ºçŽ°é‡å¤è¯çš„æ¦‚çŽ‡éžå¸¸å°ï¼Œå› æ­¤ä¸ä¼šæœ‰å¤ªå¤§å½±å“ã€‚å…·ä½“ä»£ç å‚ç…§preprocess.pyæ–‡ä»¶ä¸­çš„remove_stop_wordsæ–¹æ³•ã€‚å»ºç«‹è¯å…¸å°†æ‰€æœ‰è§†é¢‘æ ‡é¢˜ç»è¿‡åˆ†è¯åŽçš„å•è¯æ±‡æ€»èµ·æ¥å»ºç«‹ä¸€ä¸ªè¯å…¸ï¼Œä¾›åŽç»­å¥å­å»ºæ¨¡ä½¿ç”¨ã€‚å…·ä½“ä»£ç å‚ç…§preprocess.pyæ–‡ä»¶ä¸­çš„vocab_buildæ–¹æ³•ã€‚å¥å­å»ºæ¨¡å°†åˆ†è¯åŽçš„è§†é¢‘æ ‡é¢˜ä¸­çš„æ¯ä¸ªè¯æ›¿æ¢ä¸ºå…¶åœ¨è¯å…¸ä¸­çš„åºå·ï¼Œè¿™æ ·æ¯ä¸ªæ ‡é¢˜å°†ä¼šè½¬æ¢ä¸ºç”±ä¸€ä¸²æ•°ç»„æž„æˆçš„å‘é‡ã€‚å…·ä½“ä»£ç å‚ç…§preprocess.pyæ–‡ä»¶ä¸­çš„word2indexæ–¹æ³•ã€‚è®­ç»ƒä¹‹å‰æåˆ°è¿‡ï¼Œæœ¬æ–‡ä¸€å…±è¿ç”¨äº†å››ç§æ·±åº¦å­¦ä¹ æ¨¡åž‹ï¼Œé‡‡ç”¨tensorflowæ¡†æž¶ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­æ¶‰åŠåˆ°çš„æ–‡ä»¶åˆ†ä¸ºä¸¤ç±»ï¼šæ¨¡åž‹æ–‡ä»¶ï¼Œ åŒ…æ‹¬textcnn.py, textrnn.py, textrcnn.pyä»¥åŠhan.pyè®­ç»ƒæ–‡ä»¶ï¼ŒåŒ…æ‹¬train_cnn.py, train_rnn.py, train_rcnn.pyä»¥åŠtrain_han.pyæ¨¡åž‹æ–‡ä»¶å®šä¹‰äº†å…·ä½“çš„æ¨¡åž‹ï¼Œæœ¬ç¯‡æ–‡æ¡£å°†ä¸ä¼šå…·ä½“åœ°è®²è§£å®žçŽ°ä»£ç ï¼Œåªä¼šä»Žç†è®ºå±‚é¢ä»‹ç»æ¨¡åž‹ã€‚è®­ç»ƒæ–‡ä»¶åŒ…å«äº†ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ï¼Œç”±äºŽä¸åŒç®—æ³•çš„è®­ç»ƒæµç¨‹ä¸€è‡´ï¼Œè¿™é‡Œå•æŒ‘TextCNNè®²è§£ã€‚ä¸‹é¢å¼€å§‹ä»‹ç»æ¨¡åž‹ï¼Œå¦‚æžœåªå…³æ³¨å®žçŽ°å¯ä»¥è·³è¿‡åˆ°è®­ç»ƒéƒ¨åˆ†ã€‚æ¨¡åž‹è¯å‘é‡åˆ†å¸ƒå¼è¡¨ç¤ºï¼ˆDistributed Representationï¼‰æ˜¯Hinton åœ¨1986å¹´æå‡ºçš„ï¼ŒåŸºæœ¬æ€æƒ³æ˜¯å°†æ¯ä¸ªè¯è¡¨è¾¾æˆ n ç»´ç¨ å¯†ã€è¿žç»­çš„å®žæ•°å‘é‡ï¼Œä¸Žä¹‹ç›¸å¯¹çš„one-hot encodingå‘é‡ç©ºé—´åªæœ‰ä¸€ä¸ªç»´åº¦æ˜¯1ï¼Œå…¶ä½™éƒ½æ˜¯0ã€‚åˆ†å¸ƒå¼è¡¨ç¤ºæœ€å¤§çš„ä¼˜ç‚¹æ˜¯å…·å¤‡éžå¸¸powerfulçš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ï¼Œæ¯”å¦‚ n ç»´å‘é‡æ¯ç»´ k ä¸ªå€¼ï¼Œå¯ä»¥è¡¨å¾ $k^n$ä¸ªæ¦‚å¿µã€‚äº‹å®žä¸Šï¼Œä¸ç®¡æ˜¯ç¥žç»ç½‘ç»œçš„éšå±‚ï¼Œè¿˜æ˜¯å¤šä¸ªæ½œåœ¨å˜é‡çš„æ¦‚çŽ‡ä¸»é¢˜æ¨¡åž‹ï¼Œéƒ½æ˜¯åº”ç”¨åˆ†å¸ƒå¼è¡¨ç¤ºã€‚ä¸‹å›¾æ˜¯03å¹´Bengioåœ¨ A Neural Probabilistic Language Model çš„ç½‘ç»œç»“æž„ï¼šè¿™ç¯‡æ–‡ç« æå‡ºçš„ç¥žç»ç½‘ç»œè¯­è¨€æ¨¡åž‹ï¼ˆNNLMï¼ŒNeural Probabilistic Language Modelï¼‰é‡‡ç”¨çš„æ˜¯æ–‡æœ¬åˆ†å¸ƒå¼è¡¨ç¤ºï¼Œå³æ¯ä¸ªè¯è¡¨ç¤ºä¸ºç¨ å¯†çš„å®žæ•°å‘é‡ã€‚NNLMæ¨¡åž‹çš„ç›®æ ‡æ˜¯æž„å»ºè¯­è¨€æ¨¡åž‹ï¼šè¯çš„åˆ†å¸ƒå¼è¡¨ç¤ºå³è¯å‘é‡ï¼ˆword embeddingï¼‰æ˜¯è®­ç»ƒè¯­è¨€æ¨¡åž‹çš„ä¸€ä¸ªé™„åŠ äº§ç‰©ï¼Œå³å›¾ä¸­çš„Matrix Cã€‚å°½ç®¡Hinton 86å¹´å°±æå‡ºäº†è¯çš„åˆ†å¸ƒå¼è¡¨ç¤ºï¼ŒBengio 03å¹´ä¾¿æå‡ºäº†NNLMï¼Œè¯å‘é‡çœŸæ­£ç«èµ·æ¥æ˜¯google Mikolov 13å¹´å‘è¡¨çš„ä¸¤ç¯‡word2vecçš„æ–‡ç«  Efficient Estimation of Word Representations in Vector Spaceå’ŒDistributed Representations of Words and Phrases and their Compositionalityï¼Œæ›´é‡è¦çš„æ˜¯å‘å¸ƒäº†ç®€å•å¥½ç”¨çš„word2vecå·¥å…·åŒ…ï¼Œåœ¨è¯­ä¹‰ç»´åº¦ä¸Šå¾—åˆ°äº†å¾ˆå¥½çš„éªŒè¯ï¼Œæžå¤§çš„æŽ¨è¿›äº†æ–‡æœ¬åˆ†æžçš„è¿›ç¨‹ã€‚ä¸‹å›¾æ˜¯æ–‡ä¸­æå‡ºçš„CBOW å’Œ Skip-Gramä¸¤ä¸ªæ¨¡åž‹çš„ç»“æž„ï¼ŒåŸºæœ¬ç±»ä¼¼äºŽNNLMï¼Œä¸åŒçš„æ˜¯æ¨¡åž‹åŽ»æŽ‰äº†éžçº¿æ€§éšå±‚ï¼Œé¢„æµ‹ç›®æ ‡ä¸åŒï¼ŒCBOWæ˜¯ä¸Šä¸‹æ–‡è¯é¢„æµ‹å½“å‰è¯ï¼ŒSkip-Gramåˆ™ç›¸åã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæå‡ºäº†Hierarchical Softmax å’Œ Negative Sampleä¸¤ä¸ªæ–¹æ³•ï¼Œå¾ˆå¥½çš„è§£å†³äº†è®¡ç®—æœ‰æ•ˆæ€§ï¼Œäº‹å®žä¸Šè¿™ä¸¤ä¸ªæ–¹æ³•éƒ½æ²¡æœ‰ä¸¥æ ¼çš„ç†è®ºè¯æ˜Žï¼Œæœ‰äº›trickä¹‹å¤„ï¼Œéžå¸¸çš„å®žç”¨ä¸»ä¹‰ã€‚è¯¦ç»†çš„è¿‡ç¨‹ä¸å†é˜è¿°äº†ï¼Œæœ‰å…´è¶£æ·±å…¥ç†è§£word2vecçš„ï¼ŒæŽ¨èè¯»è¯»è¿™ç¯‡å¾ˆä¸é”™çš„paper: word2vec Parameter Learning Explainedã€‚é¢å¤–å¤šæä¸€ç‚¹ï¼Œå®žé™…ä¸Šword2vecå­¦ä¹ çš„å‘é‡å’ŒçœŸæ­£è¯­ä¹‰è¿˜æœ‰å·®è·ï¼Œæ›´å¤šå­¦åˆ°çš„æ˜¯å…·å¤‡ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„è¯ï¼Œæ¯”å¦‚â€œgoodâ€ â€œbadâ€ç›¸ä¼¼åº¦ä¹Ÿå¾ˆé«˜ï¼Œåè€Œæ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡è¾“å…¥æœ‰ç›‘ç£çš„è¯­ä¹‰èƒ½å¤Ÿå­¦åˆ°æ›´å¥½çš„è¯­ä¹‰è¡¨ç¤ºã€‚è‡³æ­¤ï¼Œæ–‡æœ¬çš„è¡¨ç¤ºé€šè¿‡è¯å‘é‡çš„è¡¨ç¤ºæ–¹å¼ï¼ŒæŠŠæ–‡æœ¬æ•°æ®ä»Žé«˜çº¬åº¦é«˜ç¨€ç–çš„ç¥žç»ç½‘ç»œéš¾å¤„ç†çš„æ–¹å¼ï¼Œå˜æˆäº†ç±»ä¼¼å›¾åƒã€è¯­éŸ³çš„çš„è¿žç»­ç¨ å¯†æ•°æ®ã€‚æ·±åº¦å­¦ä¹ ç®—æ³•æœ¬èº«æœ‰å¾ˆå¼ºçš„æ•°æ®è¿ç§»æ€§ï¼Œå¾ˆå¤šä¹‹å‰åœ¨å›¾åƒé¢†åŸŸå¾ˆé€‚ç”¨çš„æ·±åº¦å­¦ä¹ ç®—æ³•æ¯”å¦‚CNNç­‰ä¹Ÿå¯ä»¥å¾ˆå¥½çš„è¿ç§»åˆ°æ–‡æœ¬é¢†åŸŸäº†ï¼Œæ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±»æ¨¡åž‹TextCNNæœ¬ç¯‡æ–‡ç« çš„é¢˜å›¾é€‰ç”¨çš„å°±æ˜¯14å¹´è¿™ç¯‡æ–‡ç« æå‡ºçš„TextCNNçš„ç»“æž„ï¼ˆè§ä¸‹å›¾ï¼‰ã€‚å·ç§¯ç¥žç»ç½‘ç»œCNN Convolutional Neural Networkæœ€åˆåœ¨å›¾åƒé¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼ŒCNNåŽŸç†å°±ä¸è®²äº†ï¼Œæ ¸å¿ƒç‚¹åœ¨äºŽå¯ä»¥æ•æ‰å±€éƒ¨ç›¸å…³æ€§ï¼Œå…·ä½“åˆ°æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­å¯ä»¥åˆ©ç”¨CNNæ¥æå–å¥å­ä¸­ç±»ä¼¼ n-gram çš„å…³é”®ä¿¡æ¯ã€‚TextCNNçš„è¯¦ç»†è¿‡ç¨‹åŽŸç†å›¾è§ä¸‹ï¼šTextCNNè¯¦ç»†è¿‡ç¨‹ï¼šç¬¬ä¸€å±‚æ˜¯å›¾ä¸­æœ€å·¦è¾¹çš„7ä¹˜5çš„å¥å­çŸ©é˜µï¼Œæ¯è¡Œæ˜¯è¯å‘é‡ï¼Œç»´åº¦=5ï¼Œè¿™ä¸ªå¯ä»¥ç±»æ¯”ä¸ºå›¾åƒä¸­çš„åŽŸå§‹åƒç´ ç‚¹äº†ã€‚ç„¶åŽç»è¿‡æœ‰ filter_size=(2,3,4) çš„ä¸€ç»´å·ç§¯å±‚ï¼Œæ¯ä¸ªfilter_size æœ‰ä¸¤ä¸ªè¾“å‡º channelã€‚ç¬¬ä¸‰å±‚æ˜¯ä¸€ä¸ª1-max poolingå±‚ï¼Œè¿™æ ·ä¸åŒé•¿åº¦å¥å­ç»è¿‡poolingå±‚ä¹‹åŽéƒ½èƒ½å˜æˆå®šé•¿çš„è¡¨ç¤ºäº†ï¼Œæœ€åŽæŽ¥ä¸€å±‚å…¨è¿žæŽ¥çš„ softmax å±‚ï¼Œè¾“å‡ºæ¯ä¸ªç±»åˆ«çš„æ¦‚çŽ‡ã€‚ç‰¹å¾ï¼šè¿™é‡Œçš„ç‰¹å¾å°±æ˜¯è¯å‘é‡ï¼Œæœ‰é™æ€ï¼ˆstaticï¼‰å’Œéžé™æ€ï¼ˆnon-staticï¼‰æ–¹å¼ã€‚staticæ–¹å¼é‡‡ç”¨æ¯”å¦‚word2vecé¢„è®­ç»ƒçš„è¯å‘é‡ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸æ›´æ–°è¯å‘é‡ï¼Œå®žè´¨ä¸Šå±žäºŽè¿ç§»å­¦ä¹ äº†ï¼Œç‰¹åˆ«æ˜¯æ•°æ®é‡æ¯”è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œé‡‡ç”¨é™æ€çš„è¯å‘é‡å¾€å¾€æ•ˆæžœä¸é”™ã€‚non-staticåˆ™æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°è¯å‘é‡ã€‚æŽ¨èçš„æ–¹å¼æ˜¯ non-static ä¸­çš„ fine-tunningæ–¹å¼ï¼Œå®ƒæ˜¯ä»¥é¢„è®­ç»ƒï¼ˆpre-trainï¼‰çš„word2vecå‘é‡åˆå§‹åŒ–è¯å‘é‡ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´è¯å‘é‡ï¼Œèƒ½åŠ é€Ÿæ”¶æ•›ï¼Œå½“ç„¶å¦‚æžœæœ‰å……è¶³çš„è®­ç»ƒæ•°æ®å’Œèµ„æºï¼Œç›´æŽ¥éšæœºåˆå§‹åŒ–è¯å‘é‡æ•ˆæžœä¹Ÿæ˜¯å¯ä»¥çš„ã€‚é€šé“ï¼ˆChannelsï¼‰ï¼šå›¾åƒä¸­å¯ä»¥åˆ©ç”¨ (R, G, B) ä½œä¸ºä¸åŒchannelï¼Œè€Œæ–‡æœ¬çš„è¾“å…¥çš„channelé€šå¸¸æ˜¯ä¸åŒæ–¹å¼çš„embeddingæ–¹å¼ï¼ˆæ¯”å¦‚ word2vecæˆ–Gloveï¼‰ï¼Œå®žè·µä¸­ä¹Ÿæœ‰åˆ©ç”¨é™æ€è¯å‘é‡å’Œfine-tunningè¯å‘é‡ä½œä¸ºä¸åŒchannelçš„åšæ³•ã€‚ä¸€ç»´å·ç§¯ï¼ˆconv-1dï¼‰ï¼šå›¾åƒæ˜¯äºŒç»´æ•°æ®ï¼Œç»è¿‡è¯å‘é‡è¡¨è¾¾çš„æ–‡æœ¬ä¸ºä¸€ç»´æ•°æ®ï¼Œå› æ­¤åœ¨TextCNNå·ç§¯ç”¨çš„æ˜¯ä¸€ç»´å·ç§¯ã€‚ä¸€ç»´å·ç§¯å¸¦æ¥çš„é—®é¢˜æ˜¯éœ€è¦è®¾è®¡é€šè¿‡ä¸åŒ filter_size çš„ filter èŽ·å–ä¸åŒå®½åº¦çš„è§†é‡Žã€‚Poolingå±‚ï¼šåˆ©ç”¨CNNè§£å†³æ–‡æœ¬åˆ†ç±»é—®é¢˜çš„æ–‡ç« è¿˜æ˜¯å¾ˆå¤šçš„ï¼Œæ¯”å¦‚è¿™ç¯‡ A Convolutional Neural Network for Modelling Sentences æœ€æœ‰æ„æ€çš„è¾“å…¥æ˜¯åœ¨ pooling æ”¹æˆ (dynamic) k-max pooling ï¼Œpoolingé˜¶æ®µä¿ç•™ k ä¸ªæœ€å¤§çš„ä¿¡æ¯ï¼Œä¿ç•™äº†å…¨å±€çš„åºåˆ—ä¿¡æ¯ã€‚æ¯”å¦‚åœ¨æƒ…æ„Ÿåˆ†æžåœºæ™¯ï¼Œä¸¾ä¸ªä¾‹å­ï¼š1â€œ æˆ‘è§‰å¾—è¿™ä¸ªåœ°æ–¹æ™¯è‰²è¿˜ä¸é”™ï¼Œä½†æ˜¯äººä¹Ÿå®žåœ¨å¤ªå¤šäº† â€è™½ç„¶å‰åŠéƒ¨åˆ†ä½“çŽ°æƒ…æ„Ÿæ˜¯æ­£å‘çš„ï¼Œå…¨å±€æ–‡æœ¬è¡¨è¾¾çš„æ˜¯åè´Ÿé¢çš„æƒ…æ„Ÿï¼Œåˆ©ç”¨ k-max poolingèƒ½å¤Ÿå¾ˆå¥½æ•æ‰è¿™ç±»ä¿¡æ¯ã€‚TextRNNå°½ç®¡TextCNNèƒ½å¤Ÿåœ¨å¾ˆå¤šä»»åŠ¡é‡Œé¢èƒ½æœ‰ä¸é”™çš„è¡¨çŽ°ï¼Œä½†CNNæœ‰ä¸ªæœ€å¤§é—®é¢˜æ˜¯å›ºå®š filter_size çš„è§†é‡Žï¼Œä¸€æ–¹é¢æ— æ³•å»ºæ¨¡æ›´é•¿çš„åºåˆ—ä¿¡æ¯ï¼Œå¦ä¸€æ–¹é¢ filter_size çš„è¶…å‚è°ƒèŠ‚ä¹Ÿå¾ˆç¹çã€‚CNNæœ¬è´¨æ˜¯åšæ–‡æœ¬çš„ç‰¹å¾è¡¨è¾¾å·¥ä½œï¼Œè€Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ›´å¸¸ç”¨çš„æ˜¯é€’å½’ç¥žç»ç½‘ç»œï¼ˆRNN, Recurrent Neural Networkï¼‰ï¼Œèƒ½å¤Ÿæ›´å¥½çš„è¡¨è¾¾ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…·ä½“åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒBi-directional RNNï¼ˆå®žé™…ä½¿ç”¨çš„æ˜¯åŒå‘LSTMï¼‰ä»ŽæŸç§æ„ä¹‰ä¸Šå¯ä»¥ç†è§£ä¸ºå¯ä»¥æ•èŽ·å˜é•¿ä¸”åŒå‘çš„çš„ â€œn-gramâ€ ä¿¡æ¯ã€‚RNNç®—æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸéžå¸¸ä¸€ä¸ªæ ‡é…ç½‘ç»œäº†ï¼Œåœ¨åºåˆ—æ ‡æ³¨/å‘½åä½“è¯†åˆ«/seq2seqæ¨¡åž‹ç­‰å¾ˆå¤šåœºæ™¯éƒ½æœ‰åº”ç”¨ï¼ŒRecurrent Neural Network for Text Classification with Multi-Task Learningæ–‡ä¸­ä»‹ç»äº†RNNç”¨äºŽåˆ†ç±»é—®é¢˜çš„è®¾è®¡ï¼Œä¸‹å›¾LSTMç”¨äºŽç½‘ç»œç»“æž„åŽŸç†ç¤ºæ„å›¾ï¼Œç¤ºä¾‹ä¸­çš„æ˜¯åˆ©ç”¨æœ€åŽä¸€ä¸ªè¯çš„ç»“æžœç›´æŽ¥æŽ¥å…¨è¿žæŽ¥å±‚softmaxè¾“å‡ºäº†ã€‚TextRCNN (TextCNN + TextRNN)æˆ‘ä»¬å‚è€ƒçš„æ˜¯ä¸­ç§‘é™¢15å¹´å‘è¡¨åœ¨AAAIä¸Šçš„è¿™ç¯‡æ–‡ç«  Recurrent Convolutional Neural Networks for Text Classification çš„ç»“æž„ï¼šåˆ©ç”¨å‰å‘å’ŒåŽå‘RNNå¾—åˆ°æ¯ä¸ªè¯çš„å‰å‘å’ŒåŽå‘ä¸Šä¸‹æ–‡çš„è¡¨ç¤ºï¼šè¿™æ ·è¯çš„è¡¨ç¤ºå°±å˜æˆè¯å‘é‡å’Œå‰å‘åŽå‘ä¸Šä¸‹æ–‡å‘é‡concatèµ·æ¥çš„å½¢å¼äº†ï¼Œå³ï¼šæœ€åŽå†æŽ¥è·ŸTextCNNç›¸åŒå·ç§¯å±‚ï¼Œpoolingå±‚å³å¯ï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯å·ç§¯å±‚ filter_size = 1å°±å¯ä»¥äº†ï¼Œä¸å†éœ€è¦æ›´å¤§ filter_size èŽ·å¾—æ›´å¤§è§†é‡Žï¼Œè¿™é‡Œè¯çš„è¡¨ç¤ºä¹Ÿå¯ä»¥åªç”¨åŒå‘RNNè¾“å‡ºã€‚HAN (TextRNN + Attention)CNNå’ŒRNNç”¨åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­å°½ç®¡æ•ˆæžœæ˜¾è‘—ï¼Œä½†éƒ½æœ‰ä¸€ä¸ªä¸è¶³çš„åœ°æ–¹å°±æ˜¯ä¸å¤Ÿç›´è§‚ï¼Œå¯è§£é‡Šæ€§ä¸å¥½ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†æžbadcaseæ—¶å€™æ„Ÿå—å°¤å…¶æ·±åˆ»ã€‚è€Œæ³¨æ„åŠ›ï¼ˆAttentionï¼‰æœºåˆ¶æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸€ä¸ªå¸¸ç”¨çš„å»ºæ¨¡é•¿æ—¶é—´è®°å¿†æœºåˆ¶ï¼Œèƒ½å¤Ÿå¾ˆç›´è§‚çš„ç»™å‡ºæ¯ä¸ªè¯å¯¹ç»“æžœçš„è´¡çŒ®ï¼ŒåŸºæœ¬æˆäº†Seq2Seqæ¨¡åž‹çš„æ ‡é…äº†ã€‚Attentionæœºåˆ¶ä»‹ç»ï¼šè¯¦ç»†ä»‹ç»Attentionææ€•éœ€è¦ä¸€å°ç¯‡æ–‡ç« çš„ç¯‡å¹…ï¼Œæ„Ÿå…´è¶£çš„å¯å‚è€ƒ14å¹´è¿™ç¯‡paper NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATEã€‚ä»¥æœºå™¨ç¿»è¯‘ä¸ºä¾‹ç®€å•ä»‹ç»ä¸‹ï¼Œä¸‹å›¾ä¸­$x_t$æ˜¯æºè¯­è¨€çš„ä¸€ä¸ªè¯ï¼Œ$y_t$æ˜¯ç›®æ ‡è¯­è¨€çš„ä¸€ä¸ªè¯ï¼Œæœºå™¨ç¿»è¯‘çš„ä»»åŠ¡å°±æ˜¯ç»™å®šæºåºåˆ—å¾—åˆ°ç›®æ ‡åºåˆ—ã€‚ç¿»è¯‘$y_t$çš„è¿‡ç¨‹äº§ç”Ÿå–å†³äºŽä¸Šä¸€ä¸ªè¯ $y_{t-1}$ å’Œæºè¯­è¨€çš„è¯çš„è¡¨ç¤º $h_{j}$($x_{j}$) çš„ bi-RNN æ¨¡åž‹çš„è¡¨ç¤ºï¼‰ï¼Œè€Œæ¯ä¸ªè¯æ‰€å çš„æƒé‡æ˜¯ä¸ä¸€æ ·çš„ã€‚æ¯”å¦‚æºè¯­è¨€æ˜¯ä¸­æ–‡ â€œæˆ‘ / æ˜¯ / ä¸­å›½äººâ€ ç›®æ ‡è¯­è¨€ â€œi / am / Chineseâ€ï¼Œç¿»è¯‘å‡ºâ€œChineseâ€æ—¶å€™æ˜¾ç„¶å–å†³äºŽâ€œä¸­å›½äººâ€ï¼Œè€Œä¸Žâ€œæˆ‘ / æ˜¯â€åŸºæœ¬æ— å…³ã€‚ä¸‹å›¾å…¬å¼, $\alpha _{ij}$åˆ™æ˜¯ç¿»è¯‘è‹±æ–‡ç¬¬$i$ä¸ªè¯æ—¶ï¼Œä¸­æ–‡ç¬¬$j$ä¸ªè¯çš„è´¡çŒ®ï¼Œä¹Ÿå°±æ˜¯æ³¨æ„åŠ›ã€‚æ˜¾ç„¶åœ¨ç¿»è¯‘â€œChineseâ€æ—¶ï¼Œâ€œä¸­å›½äººâ€çš„æ³¨æ„åŠ›å€¼éžå¸¸å¤§ã€‚Attentionçš„æ ¸å¿ƒpointæ˜¯åœ¨ç¿»è¯‘æ¯ä¸ªç›®æ ‡è¯ï¼ˆæˆ– é¢„æµ‹å•†å“æ ‡é¢˜æ–‡æœ¬æ‰€å±žç±»åˆ«ï¼‰æ‰€ç”¨çš„ä¸Šä¸‹æ–‡æ˜¯ä¸åŒçš„ï¼Œè¿™æ ·çš„è€ƒè™‘æ˜¾ç„¶æ˜¯æ›´åˆç†çš„ã€‚TextRNN + Attention æ¨¡åž‹ï¼šæˆ‘ä»¬å‚è€ƒäº†è¿™ç¯‡æ–‡ç«  Hierarchical Attention Networks for Document Classificationï¼Œä¸‹å›¾æ˜¯æ¨¡åž‹çš„ç½‘ç»œç»“æž„å›¾ï¼Œå®ƒä¸€æ–¹é¢ç”¨å±‚æ¬¡åŒ–çš„ç»“æž„ä¿ç•™äº†æ–‡æ¡£çš„ç»“æž„ï¼Œå¦ä¸€æ–¹é¢åœ¨word-levelå’Œsentence-levelã€‚æ ‡é¢˜åœºæ™¯åªéœ€è¦ word-level è¿™ä¸€å±‚çš„ Attention å³å¯ã€‚åŠ å…¥Attentionä¹‹åŽæœ€å¤§çš„å¥½å¤„è‡ªç„¶æ˜¯èƒ½å¤Ÿç›´è§‚çš„è§£é‡Šå„ä¸ªå¥å­å’Œè¯å¯¹åˆ†ç±»ç±»åˆ«çš„é‡è¦æ€§ã€‚è®­ç»ƒçŽ°åœ¨æ¥è¯¦ç»†è®²è§£è®­ç»ƒè¿‡ç¨‹ï¼Œæ¶‰åŠåˆ°çš„æ–‡ä»¶train_cnn.py, utils.py, textcnn.pyæ³¨æ„åˆ°train_cnn.pyæ–‡ä»¶æœ€åŽï¼š123if __name__ == '__main__': os.environ["CUDA_VISIBLE_DEVICES"] = '1' tf.app.run()å…¶ä¸­ç¬¬ä¸€è¡Œæ˜¯æŒ‡å®šåªç”¨ä¸€ä¸ªGPUã€‚ç¬¬äºŒè¡Œæ˜¯tensorflowçš„ä¸€ä¸ªè¿è¡Œæ¡†æž¶ï¼Œrunä¼šè¿è¡Œæ–‡ä»¶å†…çš„mainæ–¹æ³•ï¼Œå¹¶ä¸”ä¼ å…¥æ–‡ä»¶æœ€å¼€å§‹è®¾å®šçš„å‚æ•°ï¼š123456789101112131415161718192021222324252627282930# configurationFLAGS = tf.app.flags.FLAGStf.app.flags.DEFINE_integer("num_classes", 33, "number of label")tf.app.flags.DEFINE_float("learning_rate", 0.01, "learning rate")tf.app.flags.DEFINE_integer( "batch_size", 64, "Batch size for training/evaluating.")tf.app.flags.DEFINE_integer( "decay_steps", 1000, "how many steps before decay learning rate.")tf.app.flags.DEFINE_float( "decay_rate", 0.95, "Rate of decay for learning rate.")tf.app.flags.DEFINE_string( "ckpt_dir", "text_cnn_title_desc_checkpoint/", "checkpoint location for the model")tf.app.flags.DEFINE_integer( "sentence_len", 15, "max sentence length")tf.app.flags.DEFINE_integer("embed_size", 64, "embedding size")tf.app.flags.DEFINE_boolean( "is_training", True, "is traning.true:tranining,false:testing/inference")tf.app.flags.DEFINE_integer( "num_epochs", 30, "number of epochs to run.")tf.app.flags.DEFINE_integer( "validate_every", 1, "Validate every validate_every epochs.")tf.app.flags.DEFINE_boolean( "use_embedding", True, "whether to use embedding or not.")tf.app.flags.DEFINE_integer( "num_filters", 256, "number of filters")tf.app.flags.DEFINE_boolean( "multi_label_flag", False, "use multi label or single label.")tf.app.flags.DEFINE_boolean( "just_train", False, "whether use all data to train or not.")ç¬¬ä¸€ä¸ªå‚æ•°ä»£è¡¨å‚æ•°åï¼ˆè°ƒç”¨è¿™ä¸ªå‚æ•°çš„æ–¹æ³•ï¼šFLAGS.nameï¼‰ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯é»˜è®¤å€¼ï¼Œç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯æè¿°ã€‚å€¼å¾—è¯´æ˜Žçš„æ˜¯è¿™é‡Œæœ‰ä¸€ä¸ªjust_trainå‚æ•°ï¼Œå®ƒä»£è¡¨æ˜¯å¦å°†æµ‹è¯•é›†æ”¾å…¥è®­ç»ƒé›†ä¸€èµ·è®­ç»ƒï¼Œä¸€èˆ¬åœ¨ç”¨æ¨¡åž‹æœ€ç»ˆç¡®å®šä¹‹åŽã€‚æ‰€ä»¥è¿è¡Œpython train_cnn.pyå°±æ˜¯å¯åŠ¨è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶å¯ä»¥ä¼ å…¥å‚æ•°ï¼Œæ–¹æ³•ä¸ºpython train_cnn.py --name value, è¿™é‡Œçš„nameå°±æ˜¯æ–‡ä»¶å®šä¹‰çš„å‚æ•°åï¼Œvalueå°±æ˜¯ä½ è¦è®¾å®šçš„å€¼ã€‚å¦‚æžœä¸ä¼ å…¥å‚æ•°ï¼Œåˆ™å‚æ•°ä¸ºé»˜è®¤å€¼ã€‚ä¸‹é¢æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹mainå‡½æ•°ï¼Œæµç¨‹å¦‚ä¸‹ï¼šâ€‹ æ•°æ®åŠ è½½è¿™ä¸ªè¿‡ç¨‹ä¸»è¦æ˜¯è°ƒç”¨train_test_loaderæ–¹æ³•åˆ‡åˆ†è®­ç»ƒé›†ä¸Žæµ‹è¯•é›†ã€‚12X_train, X_val, y_train, y_val, n_classes = train_test_loader(FLAGS.just_train)è¯å…¸åŠ è½½åŠ è½½æ•°æ®é¢„å¤„ç†è¿‡ç¨‹ä¸­å»ºç«‹çš„è¯å…¸ã€‚ç›®çš„æ˜¯ç”¨æ¥ä»Žé¢„è®­ç»ƒçš„è¯å‘é‡è¯å…¸ä¸­æ‹¿å‡ºå¯¹åº”çš„è¯å‘é‡ã€‚1234with open('data/vocab.dic', 'rb') as f: vocab = pickle.load(f)vocab_size = len(vocab) + 1print('size of vocabulary: &#123;&#125;'.format(vocab_size))è¿™é‡Œå°†è¯å…¸çš„é•¿åº¦åŠ ä¸€æ˜¯ä¸ºäº†ç»™ä¸€ä¸ªç‰¹æ®Šè¯â€œç©ºâ€åŠ å…¥ä½ç½®ï¼Œâ€œç©ºâ€çš„ä½œç”¨æ˜¯å¡«å……çŸ­æ ‡é¢˜ï¼Œè®©æ‰€æœ‰æ ‡é¢˜é•¿åº¦ä¸€æ ·ã€‚Paddingè¿™ä¸ªé˜¶æ®µå°±æ˜¯å°†æ‰€æœ‰æ ‡é¢˜é•¿åº¦å˜æˆä¸€è‡´ï¼ŒçŸ­äº†å°±å¡«å……ï¼Œé•¿äº†å°±æˆªæ–­ã€‚æ ‡é¢˜é•¿åº¦æ˜¯ä¸€ä¸ªå‚æ•°ï¼Œå¯ä»¥è®¾ç½®ã€‚123456# padding sentences X_train = pad_sequences(X_train, maxlen=FLAGS.sentence_len, value=float(vocab_size - 1)) if not FLAGS.just_train: X_val = pad_sequences( X_val, maxlen=FLAGS.sentence_len, value=float(vocab_size - 1))æ¨¡åž‹å®žä¾‹åŒ–12345textcnn = TextCNN(filter_sizes, FLAGS.num_filters, FLAGS.num_classes, FLAGS.learning_rate, FLAGS.batch_size, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.sentence_len, vocab_size, FLAGS.embed_size, FLAGS.is_training, multi_label_flag=False)å¦‚æžœæœ‰ä¹‹å‰è®­ç»ƒåˆ°ä¸€åŠçš„æ¨¡åž‹ï¼Œé‚£æˆ‘ä»¬å°±åŠ è½½é‚£ä¸ªæ¨¡åž‹çš„å‚æ•°ï¼Œç»§ç»­è®­ç»ƒï¼Œå¦åˆ™è¿›è¡Œå‚æ•°åˆå§‹åŒ–12345678910111213# Initialize save saver = tf.train.Saver() if os.path.exists(FLAGS.ckpt_dir + 'checkpoint'): print('restoring variables from checkpoint') saver.restore( sess, tf.train.latest_checkpoint(FLAGS.ckpt_dir)) else: print('Initializing Variables') sess.run(tf.global_variables_initializer()) if FLAGS.use_embedding: assign_pretrained_word_embedding( sess, vocab, vocab_size, textcnn)æ¨¡åž‹è®­ç»ƒæ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…æ‹¬ä¸¤ä¸ªå¾ªçŽ¯ï¼Œç¬¬ä¸€ä¸ªæ˜¯å¤§å¾ªçŽ¯ï¼Œè¡¨ç¤ºéåŽ†æ‰€æœ‰è®­ç»ƒæ•°æ®å¤šå°‘éã€‚ç¬¬äºŒä¸ªæ˜¯mini-batchå¾ªçŽ¯ï¼Œå°å¾ªçŽ¯èµ°è¿‡ä¸€éä»£è¡¨éåŽ†äº†æ‰€æœ‰è®­ç»ƒæ•°æ®ä¸€éã€‚123456for epoch in range(curr_epoch, total_epochs): loss, acc, counter = .0, .0, 0 for start, end in zip( range(0, number_of_training_data, batch_size), range(batch_size, number_of_training_data, batch_size)):ä¸‹é¢å°±æ˜¯å°†è®­ç»ƒæ•°æ®å–‚åˆ°æ¨¡åž‹ä¸­:12feed_dict = &#123;textcnn.input_x: X_train[start:end], textcnn.dropout_keep_prob: 0.5&#125;ç¬¬äºŒä¸ªå‚æ•°æ˜¯æ¨¡åž‹ç›¸å…³çš„dropoutå‚æ•°ï¼Œç”¨äºŽå‡å°‘è¿‡æ‹Ÿåˆï¼ŒèŒƒå›´æ˜¯(0, 1]ï¼ŒåŸºæœ¬ä¸ç”¨æ”¹å˜ã€‚123curr_loss, curr_acc, _ = sess.run( [textcnn.loss_val, textcnn.accuracy, textcnn.train_op], feed_dict)è¿™ä¸€æ­¥å°±æ˜¯å¾—åˆ°è¿™ä¸€å°éƒ¨åˆ†è®­ç»ƒæ•°æ®å¯¹åº”çš„å‡†ç¡®çŽ‡ä»¥åŠlossã€‚ç„¶åŽæ¯ç»è¿‡validate_everyä¸ªå¤§å¾ªçŽ¯çš„è®­ç»ƒï¼Œåœ¨æµ‹è¯•é›†ä¸Šçœ‹çœ‹æ¨¡åž‹æ€§èƒ½ã€‚å¦‚æžœæ€§èƒ½æ¯”ä¸Šä¸€æ¬¡æ›´å¥½ï¼Œå°±ä¿å­˜æ¨¡åž‹ï¼Œå¦åˆ™å°±é€€å‡ºï¼Œå› ä¸ºç®—æ³•å¼€å§‹å‘æ•£äº†ã€‚æ¨¡åž‹è®­ç»ƒå®Œæ¯•æ£€æŸ¥æ€§èƒ½ä¹‹åŽï¼Œå¦‚æžœæ¨¡åž‹å¯è¡Œï¼Œä¸‹ä¸€æ­¥å°±å°†æ‰€æœ‰æ•°æ®ç”¨äºŽè®­ç»ƒï¼Œä¹Ÿå³è¿è¡Œä»¥ä¸‹å‘½ä»¤python train_cnn.py --just_train Trueã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šè¿­ä»£å›ºå®šçš„20ä¸ªå¤§å¾ªçŽ¯ã€‚è®­ç»ƒå®Œæ¯•ä¹‹åŽï¼Œä¸‹é¢çš„é¢„æµ‹è¿‡ç¨‹å°†ä½¿ç”¨è¿™ä¸ªæ¨¡åž‹ã€‚é¢„æµ‹é¢„æµ‹æ¶‰åŠåˆ°çš„æ–‡ä»¶predict_cnn.pyä»¥åŠutils.pyé¢„æµ‹çš„æµç¨‹å’Œè®­ç»ƒå·®ä¸å¤šï¼Œåªä¸è¿‡ä¸å†è¿›è¡Œå¤šæ¬¡å¯¹æ•°æ®é›†çš„éåŽ†ï¼Œåªè¿›è¡Œå¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œä¸€æ¬¡éåŽ†ï¼Œæ‹¿åˆ°ç»“æžœä¹‹åŽï¼Œç”±äºŽç®—æ³•è¾“å‡ºçš„ç»“æžœæ˜¯[0, 32]è¿™æ ·ä¸€ä¸ªåºå·ï¼Œæˆ‘ä»¬éœ€è¦è½¬åŒ–ä¸ºä¸­æ–‡æ ‡ç­¾ã€‚å…·ä½“å‚ç…§ä»£ç ï¼Œä¸å†èµ˜è¿°ã€‚å¼•ç”¨ã€1ã€‘https://zhuanlan.zhihu.com/p/25928551ã€2ã€‘https://github.com/brightmart/text_classification]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[BFG Repo-Cleaner]]></title>
      <url>%2F2017%2F11%2F29%2FBFG-Repo-Cleaner%2F</url>
      <content type="text"><![CDATA[An alternative to git-filter-branchThe BFG is a simpler, faster alternative to git-filter-branch for cleansing bad data out of your Git repository history:Removing Crazy Big FilesRemoving Passwords, Credentials &amp; other Private dataThe git-filter-branch command is enormously powerful and can do things that the BFG canâ€™t - but the BFG is much better for the tasks above, because:Faster : 10 - 720x fasterSimpler : The BFG isnâ€™t particularily clever, but is focused on making the above tasks easyBeautiful : If you need to, you can use the beautiful Scala language to customise the BFG. Which has got to be better than Bash scripting at least some of the time.UsageFirst clone a fresh copy of your repo, using the --mirror flag:1$ git clone --mirror git://example.com/some-big-repo.gitThis is a bare repo, which means your normal files wonâ€™t be visible, but it is a full copy of the Git database of your repository, and at this point you should make a backup of it to ensure you donâ€™t lose anything.Now you can run the BFG to clean your repository up:1$ java -jar bfg.jar --strip-blobs-bigger-than 100M some-big-repo.gitThe BFG will update your commits and all branches and tags so they are clean, but it doesnâ€™t physically delete the unwanted stuff. Examine the repo to make sure your history has been updated, and then use the standard git gc command to strip out the unwanted dirty data, which Git will now recognise as surplus to requirements:12$ cd some-big-repo.git$ git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressiveFinally, once youâ€™re happy with the updated state of your repo, push it back up (note that because your clone command used the â€“mirror flag, this push will update *all* refs on your remote server):1$ git pushAt this point, youâ€™re ready for everyone to ditch their old copies of the repo and do fresh clones of the nice, new pristine data. Itâ€™s best to delete all old clones, as theyâ€™ll have dirty history that you donâ€™t want to risk pushing back into your newly cleaned repo.ExamplesIn all these examples bfg is an alias for java -jar bfg.jar.Delete all files named â€˜id_rsaâ€™ or â€˜id_dsaâ€™ :1$ bfg --delete-files id_&#123;dsa,rsa&#125; my-repo.gitRemove all blobs bigger than 50 megabytes :1$ bfg --strip-blobs-bigger-than 50M my-repo.gitReplace all passwords listed in a file (prefix lines â€˜regex:â€™ or â€˜glob:â€™ if required) with ***REMOVED***wherever they occur in your repository :1$ bfg --replace-text passwords.txt my-repo.gitRemove all folders or files named â€˜.gitâ€™ - a reserved filename in Git. These often become a problemwhen migrating to Git from other source-control systems like Mercurial :1$ bfg --delete-folders .git --delete-files .git --no-blob-protection my-repo.gitFor further command-line options, you can run the BFG without any arguments, which will output text like this.Your current files are sacredâ€¦The BFG treats you like a reformed alcoholic: youâ€™ve made some mistakes in the past, but now youâ€™ve cleaned up your act. Thus the BFG assumes that your latest commit is a good one, with none of the dirty files you want removing from your history still in it. This assumption by the BFG protects your work, and gives you peace of mind knowing that the BFG is only changing your repo history, not meddling with the current files of your project.By default the HEAD branch is protected, and while its history will be cleaned, the very latest commit (the â€˜tipâ€™) is a protected commit and its file-hierarchy wonâ€™t be changed at all.If you want to protect the tips of several branches or tags (not just HEAD), just name them for the BFG:1$ bfg --strip-biggest-blobs 100 --protect-blobs-from master,maint,next repo.gitNote:Cleaning Git repos is about completely eradicating bad stuff from history. If something â€˜badâ€™ (like a 10MB file, when youâ€™re specifying --strip-blobs-bigger-than 5M) is in a protected commit, it wonâ€™t be deleted - itâ€™ll persist in your repository, even if the BFG deletes if from earlier commits. If you want the BFG to delete something you need to make sure your current commits are clean.Note that although the files in those protected commits wonâ€™t be changed, when those commits follow on from earlier dirty commits, their commit ids will change, to reflect the changed history - only the SHA-1 id of the filesystem-tree will remain the same.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Understanding LSTM Networks [repost]]]></title>
      <url>%2F2017%2F11%2F28%2FUnderstanding-LSTM-Networks-repost%2F</url>
      <content type="text"><![CDATA[source post is here.Recurrent Neural NetworksHumans donâ€™t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You donâ€™t throw everything away and start thinking from scratch again. Your thoughts have persistence.Traditional neural networks canâ€™t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. Itâ€™s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.Recurrent Neural Networks have loops.In the above diagram, a chunk of neural network, AA, looks at some input xtxt and outputs a value htht. A loop allows information to be passed from one step of the network to the next.These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they arenâ€™t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop:An unrolled recurrent neural network.This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. Theyâ€™re the natural architecture of neural network to use for such data.And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioningâ€¦ The list goes on. Iâ€™ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathyâ€™s excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks. But they really are pretty amazing.Essential to these successes is the use of â€œLSTMs,â€ a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. Itâ€™s these LSTMs that this essay will explore.The Problem of Long-Term DependenciesOne of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, theyâ€™d be extremely useful. But can they? It depends.Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in â€œthe clouds are in the sky,â€ we donâ€™t need any further context â€“ itâ€™s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that itâ€™s needed is small, RNNs can learn to use the past information.But there are also cases where we need more context. Consider trying to predict the last word in the text â€œI grew up in Franceâ€¦ I speak fluent French.â€ Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. Itâ€™s entirely possible for the gap between the relevant information and the point where it is needed to become very large.Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.In theory, RNNs are absolutely capable of handling such â€œlong-term dependencies.â€ A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs donâ€™t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult.Thankfully, LSTMs donâ€™t have this problem!LSTM NetworksLong Short Term Memory networks â€“ usually just called â€œLSTMsâ€ â€“ are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.The repeating module in a standard RNN contains a single layer.LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.The repeating module in an LSTM contains four interacting layers.Donâ€™t worry about the details of whatâ€™s going on. Weâ€™ll walk through the LSTM diagram step by step later. For now, letâ€™s just try to get comfortable with the notation weâ€™ll be using.In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.The Core Idea Behind LSTMsThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram.The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. Itâ€™s very easy for information to just flow along it unchanged.The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means â€œlet nothing through,â€ while a value of one means â€œlet everything through!â€An LSTM has three of these gates, to protect and control the cell state.Step-by-Step LSTM Walk ThroughThe first step in our LSTM is to decide what information weâ€™re going to throw away from the cell state. This decision is made by a sigmoid layer called the â€œforget gate layer.â€ It looks at htâˆ’1htâˆ’1and xtxt, and outputs a number between 00 and 11 for each number in the cell state Ctâˆ’1Ctâˆ’1. A 11represents â€œcompletely keep thisâ€ while a 00 represents â€œcompletely get rid of this.â€Letâ€™s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.The next step is to decide what new information weâ€™re going to store in the cell state. This has two parts. First, a sigmoid layer called the â€œinput gate layerâ€ decides which values weâ€™ll update. Next, a tanh layer creates a vector of new candidate values, C~tC~t, that could be added to the state. In the next step, weâ€™ll combine these two to create an update to the state.In the example of our language model, weâ€™d want to add the gender of the new subject to the cell state, to replace the old one weâ€™re forgetting.Itâ€™s now time to update the old cell state, Ctâˆ’1Ctâˆ’1, into the new cell state CtCt. The previous steps already decided what to do, we just need to actually do it.We multiply the old state by ftft, forgetting the things we decided to forget earlier. Then we add itâˆ—C~titâˆ—C~t. This is the new candidate values, scaled by how much we decided to update each state value.In the case of the language model, this is where weâ€™d actually drop the information about the old subjectâ€™s gender and add the new information, as we decided in the previous steps.Finally, we need to decide what weâ€™re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state weâ€™re going to output. Then, we put the cell state through tanhtanh (to push the values to be between âˆ’1âˆ’1 and 11) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case thatâ€™s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if thatâ€™s what follows next.Variants on Long Short Term MemoryWhat Iâ€™ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but itâ€™s worth mentioning some of them.One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding â€œpeephole connections.â€ This means that we let the gate layers look at the cell state.The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others.Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when weâ€™re going to input something in its place. We only input new values to the state when we forget something older.A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single â€œupdate gate.â€ It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015). Thereâ€™s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014).Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that theyâ€™re all about the same. Jozefowicz, et al. (2015)tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.ConclusionEarlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks!Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable.LSTMs were a big step in what we can accomplish with RNNs. Itâ€™s natural to wonder: is there another big step? A common opinion among researchers is: â€œYes! There is a next step and itâ€™s attention!â€ The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al.(2015) do exactly this â€“ it might be a fun starting point if you want to explore attention! Thereâ€™s been a number of really exciting results using attention, and it seems like a lot more are around the cornerâ€¦Attention isnâ€™t the only exciting thread in RNN research. For example, Grid LSTMs by Kalchbrenner, et al. (2015) seem extremely promising. Work using RNNs in generative models â€“ such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer &amp; Osendorfer (2015) â€“ also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so!]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[A Simple Multi-Class Classification Task: Keras and Scikit-Learn]]></title>
      <url>%2F2017%2F11%2F21%2FA-Simple-Multi-Class-Classification-Task-Keras-and-Scikit-Learn%2F</url>
      <content type="text"><![CDATA[1. Problem DescriptionIn this tutorial, we will use the standard machine learning problem called the iris flowers dataset.This dataset is well studied and is a good problem for practicing on neural networks because all of the 4 input variables are numeric and have the same scale in centimeters. Each instance describes the properties of an observed flower measurements and the output variable is specific iris species.This is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species. This is an important type of problem on which to practice with neural networks because the three class values require specialized handling.The iris flower dataset is a well-studied problem and a such we can expect to achieve a model accuracy in the range of 95% to 97%. This provides a good target to aim for when developing our models.You can download the iris flowers dataset from the UCI Machine Learning repository and place it in your current working directory with the filename â€œiris.csvâ€œ.Need help with Deep Learning in Python?Take my free 2-week email course and discover MLPs, CNNs and LSTMs (with sample code).Click to sign-up now and also get a free PDF Ebook version of the course.Start Your FREE Mini-Course Now!2. Import Classes and FunctionsWe can begin by importing all of the classes and functions we will need in this tutorial.This includes both the functionality we require from Keras, but also data loading from pandasas well as data preparation and model evaluation from scikit-learn.12345678910import numpyimport pandasfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasClassifierfrom keras.utils import np_utilsfrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import KFoldfrom sklearn.preprocessing import LabelEncoderfrom sklearn.pipeline import Pipeline3. Initialize Random Number GeneratorNext, we need to initialize the random number generator to a constant value (7).This is important to ensure that the results we achieve from this model can be achieved again precisely. It ensures that the stochastic process of training a neural network model can be reproduced.123# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)4. Load The DatasetThe dataset can be loaded directly. Because the output variable contains strings, it is easiest to load the data using pandas. We can then split the attributes (columns) into input variables (X) and output variables (Y).12345# load datasetdataframe = pandas.read_csv("iris.csv", header=None)dataset = dataframe.valuesX = dataset[:,0:4].astype(float)Y = dataset[:,4]5. Encode The Output VariableThe output variable contains three different string values.When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not.This is called one hot encoding or creating dummy variables from a categorical variable.For example, in this problem three class values are Iris-setosa, Iris-versicolor and Iris-virginica. If we had the observations:123Iris-setosaIris-versicolorIris-virginicaWe can turn this into a one-hot encoded binary matrix for each data instance that would look as follows:1234Iris-setosa, Iris-versicolor, Iris-virginica1, 0, 00, 1, 00, 0, 1We can do this by first encoding the strings consistently to integers using the scikit-learn class LabelEncoder. Then convert the vector of integers to a one hot encoding using the Keras function to_categorical().123456# encode class values as integersencoder = LabelEncoder()encoder.fit(Y)encoded_Y = encoder.transform(Y)# convert integers to dummy variables (i.e. one hot encoded)dummy_y = np_utils.to_categorical(encoded_Y)6. Define The Neural Network ModelThe Keras library provides wrapper classes to allow you to use neural network models developed with Keras in scikit-learn.There is a KerasClassifier class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.Below is a function that will create a baseline neural network for the iris classification problem. It creates a simple fully connected network with one hidden layer that contains 8 neurons.The hidden layer uses a rectifier activation function which is a good practice. Because we used a one-hot encoding for our iris dataset, the output layer must create 3 output values, one for each class. The output value with the largest value will be taken as the class predicted by the model.The network topology of this simple one-layer neural network can be summarized as:14 inputs -&gt; [8 hidden nodes] -&gt; 3 outputsNote that we use a â€œsoftmaxâ€ activation function in the output layer. This is to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities.Finally, the network uses the efficient Adam gradient descent optimization algorithm with a logarithmic loss function, which is called â€œcategorical_crossentropyâ€ in Keras.123456789# define baseline modeldef baseline_model(): # create model model = Sequential() model.add(Dense(8, input_dim=4, activation='relu')) model.add(Dense(3, activation='softmax')) # Compile model model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return modelWe can now create our KerasClassifier for use in scikit-learn.We can also pass arguments in the construction of the KerasClassifier class that will be passed on to the fit() function internally used to train the neural network. Here, we pass the number of epochs as 200 and batch size as 5 to use when training the model. Debugging is also turned off when training by setting verbose to 0.12estimator = KerasClassifier( build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)7. Evaluate The Model with k-Fold Cross ValidationWe can now evaluate the neural network model on our training data.The scikit-learn has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating machine learning models is k-fold cross validation.First we can define the model evaluation procedure. Here, we set the number of folds to be 10 (an excellent default) and to shuffle the data before partitioning it.1kfold = KFold(n_splits=10, shuffle=True, random_state=seed)Now we can evaluate our model (estimator) on our dataset (X and dummy_y) using a 10-fold cross-validation procedure (kfold).Evaluating the model only takes approximately 10 seconds and returns an object that describes the evaluation of the 10 constructed models for each of the splits of the dataset.12results = cross_val_score(estimator, X, dummy_y, cv=kfold)print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))The results are summarized as both the mean and standard deviation of the model accuracy on the dataset. This is a reasonable estimation of the performance of the model on unseen data. It is also within the realm of known top results for this problem.1Accuracy: 97.33% (4.42%)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The Right Way to Oversample in Predictive Modeling]]></title>
      <url>%2F2017%2F11%2F21%2FThe-Right-Way-to-Oversample-in-Predictive-Modeling%2F</url>
      <content type="text"><![CDATA[The Source Blog: https://beckernick.github.io/oversampling-modeling/Imbalanced datasets spring up everywhere. Amazon wants to classify fake reviews, banks want to predict fraudulent credit card charges, and, as of this November, Facebook researchers are probably wondering if they can predict which news articles are fake.In each of these cases, only a small fraction of observations are actually positives. Iâ€™d guess that only 1 in 10,000 credit card charges are fraudulent, at most. Recently, oversampling the minority class observations has become a common approach to improve the quality of predictive modeling. By oversampling, models are sometimes better able to learn patterns that differentiate classes.However, this post isnâ€™t about how this can improve modeling. Instead, itâ€™s about how the *timing* of oversampling can affect the generalization ability of a model. Since one of the primary goals of model validation is to estimate how it will perform on unseen data, oversampling correctly is critical.Preparing the DataIâ€™m going to try to predict whether someone will default on or a creditor will have to charge off a loan, using data from Lending Club. Iâ€™ll start by importing some modules and loading the data.123456import numpy as npimport pandas as pdfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import recall_scorefrom imblearn.over_sampling import SMOTE12loans = pd.read_csv('../lending-club-data.csv.zip')loans.iloc[0]Thereâ€™s a lot of cool person and loan-specific information in this dataset. The target variable is bad_loans, which is 1 if the loan was charged off or the lessee defaulted, and 0 otherwise. I know this dataset should be imbalanced (most loans are paid off), but how imbalanced is it?1loans.bad_loans.value_counts()1230 994571 23150Name: bad_loans, dtype: int64Charge offs occurred or people defaulted on about 19% of loans, so thereâ€™s some imbalance in the data but itâ€™s not terrible. Iâ€™ll remove a few observations with missing values for a payment-to-income ratio and then pick a handful of features to use in a random forest model.1loans = loans[~loans.payment_inc_ratio.isnull()]123model_variables = ['grade', 'home_ownership','emp_length_num', 'sub_grade','short_emp', 'dti', 'term', 'purpose', 'int_rate', 'last_delinq_none', 'last_major_derog_none', 'revol_util', 'total_rec_late_fee', 'payment_inc_ratio', 'bad_loans']loans_data_relevent = loans[model_variables]Next, I need to one-hot encode the categorical features as binary variables to use them in sklearnâ€™s random forest classifier.1loans_relevant_enconded = pd.get_dummies(loans_data_relevent)Creating the Training and Test SetsWith the data prepared, I can create a training dataset and a test dataset. Iâ€™ll use the training dataset to build and validate the model, and treat the test dataset as the unseen new data Iâ€™d see if the model were in production.1234training_features, test_features, \training_target, test_target, = train_test_split(loans_relevant_enconded.drop(['bad_loans'], axis=1), loans_relevant_enconded['bad_loans'], test_size = .1, random_state=12)The Wrong Way to OversampleWith my training data created, Iâ€™ll upsample the bad loans using the SMOTE algorithm (Synthetic Minority Oversampling Technique). At a high level, SMOTE creates synthetic observations of the minority class (bad loans) by:Finding the k-nearest-neighbors for minority class observations (finding similar observations)Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.After upsampling to a class ratio of 1.0, I should have a balanced dataset. Thereâ€™s no need (and often itâ€™s not smart) to balance the classes, but it magnifies the issue caused by incorrectly timed oversampling.123sm = SMOTE(random_state=12, ratio = 1.0)x_res, y_res = sm.fit_sample(training_features, training_target)print training_target.value_counts(), np.bincount(y_res)1230 894931 20849Name: bad_loans, dtype: int64 [89493 89493]After upsampling, Iâ€™ll split the data into separate training and validation sets and build a random forest model to classify the bad loans.1234x_train_res, x_val_res, y_train_res, y_val_res = train_test_split(x_res, y_res, test_size = .1, random_state=12)123clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)clf_rf.fit(x_train_res, y_train_res)clf_rf.score(x_val_res, y_val_res)10.8846862953237610888% accuracy looks good, but Iâ€™m not just interested in accuracy. I also want to know how well I can specifically classify bad loans, since theyâ€™re more important. In statistics, this is called recall, and itâ€™s the number of correctly predicted â€œpositivesâ€ divided by the total number of â€œpositivesâ€.1recall_score(y_val_res, clf_rf.predict(x_val_res))10.8119209733229154681% recall. That means the model correctly identified 81% of the total bad loans. Thatâ€™s pretty great. But is this actually representative of how the model will perform? To find out, Iâ€™ll calculate the accuracy and recall for the model on the test dataset I created initially.12print clf_rf.score(test_features, test_target)print recall_score(test_target, clf_rf.predict(test_features))120.8019737378680.129943502825Only 80% accuracy and 13% recall on the test data. Thatâ€™s a huge difference!What Happened?By oversampling before splitting into training and validation datasets, I â€œbleedâ€ information from the validation set into the training of the model.To see how this works, think about the case of simple oversampling (where I just duplicate observations). If I upsample a dataset before splitting it into a train and validation set, I could end up with the same observation in both datasets. As a result, a complex enough model will be able to perfectly predict the value for those observations when predicting on the validation set, inflating the accuracy and recall.When upsampling using SMOTE, I donâ€™t create duplicate observations. However, because the SMOTE algorithm uses the nearest neighbors of observations to create synthetic data, it still bleeds information. If the nearest neighbors of minority class observations in the training set end up in the validation set, their information is partially captured by the synthetic data in the training set. Since Iâ€™m splitting the data randomly, weâ€™d expect to have this happen. As a result, the model will be better able to predict validation set values than completely new data.The Right Way to OversampleOkay, so Iâ€™ve gone through the wrong way to oversample. Now Iâ€™ll go through the right way: oversampling on only the training data.1234x_train, x_val, y_train, y_val = \ train_test_split(training_features, training_target, test_size = .1, random_state=12)12sm = SMOTE(random_state=12, ratio = 1.0)x_train_res, y_train_res = sm.fit_sample(x_train, y_train)By oversampling only on the training data, none of the information in the validation data is being used to create synthetic observations. So these results should be generalizable. Letâ€™s see if thatâ€™s true.12clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)clf_rf.fit(x_train_res, y_train_res)123456print 'Validation Results'print clf_rf.score(x_val, y_val)print recall_score(y_val, clf_rf.predict(x_val))print '\nTest Results'print clf_rf.score(test_features, test_target)print recall_score(test_target, clf_rf.predict(test_features))1234567Validation Results0.8003624830090.138195777351Test Results0.8032786885250.142546718818The validation results closely match the unseen test data results, which is exactly what I would want to see after putting a model into production.ConclusionOversampling is a well-known way to potentially improve models trained on imbalanced data. But itâ€™s important to remember that oversampling incorrectly can lead to thinking a model will generalize better than it actually does. Random forests are great because the model architecture reduces overfitting (see Brieman 2001 for a proof), but poor sampling practices can still lead to false conclusions about the quality of a model.When the model is in production, itâ€™s predicting on unseen data. The main point of model validation is to estimate how the model will generalize to new data. If the decision to put a model into production is based on how it performs on a validation set, itâ€™s critical that oversampling is done correctly.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Note of the DenseNet (contains TensorFlow and PyTorch Implementation)]]></title>
      <url>%2F2017%2F11%2F20%2FNote-of-the-DenseNet-contains-TensorFlow-and-PyTorch-Implementation%2F</url>
      <content type="text"><![CDATA[The blog source:https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504.I have added the PyTorch implementation fromhttps://github.com/gpleiss/efficient_densenet_pytorch.DenseNet(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. Itâ€™s quite similar to ResNet but has some fundamental differences.With all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:Error rates on various datasets(from source paper)And for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:Ð¡omparison of the DenseNet and ResNet Top-1 error rates on the ImageNet classification dataset as a function of learned parameters (left) and flops during test-time (right)(from source paper).This post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:Why dense net differs from another convolution networks.What difficulties Iâ€™ve met during the implementation of DenseNet in tensorflow.If you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the second chapter or check the source code on GitHub. If you not familiar with any topics but want to get some knowledge â€” I highly advise you CS231n Stanford classes.Compare DenseNet with other Convolution NetworksUsually, ConvNets work such way:We have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.So the output from the Láµ¢ layer is input to the Láµ¢â‚Šâ‚ layer. It seems like this:source: &lt;http://cs231n.github.io/convolutional-networks/&gt;ResNet architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the Láµ¢ layer was obtained by summation of outputs from previous layers.In contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.So, letâ€™s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.The output from Láµ¢ layer will be (28, 28, 12).But input to the Láµ¢â‚Šâ‚ will be (28, 28, 24+12), for Láµ¢â‚Šâ‚‚ (28, 28, 24 + 12 + 12) and so on.Block of convolution layers with results concatenatedAfter a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).All these N layers are named Block in the paper. Thereâ€™s also batch normalization, nonlinearity and dropout inside the block.To reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).Transition layerNow we can again pass the image through the block with N convolutions.With this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.Full DenseNet example with 3 blocks from source paperNotes about implementationIn the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.First of all, it was not clear how many blocks should be used depends on depth. After Iâ€™ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.Second Iâ€™ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .Despite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:1234for block in range(required_blocks): output = build_block(output) if block != (required_blocks â€” 1): output = transition_layer(output)For weights initialization authors proposed use MRSA initialization(as perthis paper). In tensorflow this initialization can be easy implemented withvariance scaling initializer.In the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:1batch norm -&gt; relu -&gt; conv 1x1 -&gt; dropout -&gt; batch norm -&gt; relu -&gt; conv 3x3 -&gt; dropout -&gt; output.Despite two conv filters, only last output will be concatenated to the main pool of features.Also at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).Where theta â€” some reduction values, in the range (0, 1).In case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.Last, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used â€” just divide every image pixel by 255, so we have pixels values in the range [0, 1].At first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next Iâ€™ve implemented per channel normalizationâ€¦ And networks began works even worse. It was not clear for me why. So Iâ€™ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.And some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.1234567# without this line next slice assignment will silently fail!# at least in numpy 1.12.0images = images.astype(â€˜float64â€™)for i in range(channels): images[:, :, :, i] = ( (images[:, :, :, i] â€” self.images_means[i]) / self.images_stds[i])ConclusionDenseNets are powerful neural nets that achieve state of the art performance on many datasets. And itâ€™s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.Appendix: PyTorch Implementation (naive version ~100 lines)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101# This implementation is based on the DenseNet-BC implementation in torchvision# https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.pyimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom collections import OrderedDictclass _DenseLayer(nn.Sequential): def __init__(self, num_input_features, growth_rate, bn_size, drop_rate): super(_DenseLayer, self).__init__() self.add_module('norm.1', nn.BatchNorm2d(num_input_features)), self.add_module('relu.1', nn.ReLU(inplace=True)), self.add_module('conv.1', nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)), self.add_module('norm.2', nn.BatchNorm2d(bn_size * growth_rate)), self.add_module('relu.2', nn.ReLU(inplace=True)), self.add_module('conv.2', nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)), self.drop_rate = drop_rate def forward(self, x): new_features = super(_DenseLayer, self).forward(x) if self.drop_rate &gt; 0: new_features = F.dropout(new_features, p=self.drop_rate, training=self.training) return torch.cat([x, new_features], 1)class _Transition(nn.Sequential): def __init__(self, num_input_features, num_output_features): super(_Transition, self).__init__() self.add_module('norm', nn.BatchNorm2d(num_input_features)) self.add_module('relu', nn.ReLU(inplace=True)) self.add_module('conv', nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False)) self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))class _DenseBlock(nn.Sequential): def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate): super(_DenseBlock, self).__init__() for i in range(num_layers): layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate) self.add_module('denselayer%d' % (i + 1), layer)class DenseNet(nn.Module): r"""Densenet-BC model class, based on `"Densely Connected Convolutional Networks" &lt;https://arxiv.org/pdf/1608.06993.pdf&gt;` Args: growth_rate (int) - how many filters to add each layer (`k` in paper) block_config (list of 3 or 4 ints) - how many layers in each pooling block num_init_features (int) - the number of filters to learn in the first convolution layer bn_size (int) - multiplicative factor for number of bottle neck layers (i.e. bn_size * k features in the bottleneck layer) drop_rate (float) - dropout rate after each dense layer num_classes (int) - number of classification classes """ def __init__(self, growth_rate=12, block_config=(16, 16, 16), compression=0.5, num_init_features=24, bn_size=4, drop_rate=0, avgpool_size=8, num_classes=10): super(DenseNet, self).__init__() assert 0 &lt; compression &lt;= 1, 'compression of densenet should be between 0 and 1' self.avgpool_size = avgpool_size # First convolution self.features = nn.Sequential(OrderedDict([ ('conv0', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)), ])) # Each denseblock num_features = num_init_features for i, num_layers in enumerate(block_config): block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate) self.features.add_module('denseblock%d' % (i + 1), block) num_features = num_features + num_layers * growth_rate if i != len(block_config) - 1: trans = _Transition(num_input_features=num_features, num_output_features=int(num_features * compression)) self.features.add_module('transition%d' % (i + 1), trans) num_features = int(num_features * compression) # Final batch norm self.features.add_module('norm_final', nn.BatchNorm2d(num_features)) # Linear layer self.classifier = nn.Linear(num_features, num_classes) def forward(self, x): features = self.features(x) out = F.relu(features, inplace=True) out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view( features.size(0), -1) out = self.classifier(out) return out]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Word2Vec: The Skip-Gram Model]]></title>
      <url>%2F2017%2F11%2F14%2FWord2Vec-The-Skip-Gram-Model%2F</url>
      <content type="text"><![CDATA[This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual introductory and abstract insights about Word2Vec, and get into more of the details. Specifically here Iâ€™m diving into the skip gram neural network model.The ModelThe skip-gram neural network model is actually surprisingly simple in its most basic form; I think itâ€™s the all the little tweaks and enhancements that start to clutter the explanation.Letâ€™s start with a high-level insight about where weâ€™re going. Word2Vec uses a trick you may have seen elsewhere in machine learning. Weâ€™re going to train a simple neural network with a single hidden layer to perform a certain task, but then weâ€™re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layerâ€“weâ€™ll see that these weights are actually the â€œword vectorsâ€ that weâ€™re trying to learn.Another place you may have seen this trick is in unsupervised feature learning, where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layerâ€“itâ€™s a trick for learning good image features without having labeled training data.The Fake TaskSo now we need to talk about this â€œfakeâ€ task that weâ€™re going to build the neural network to perform, and then weâ€™ll come back later to how this indirectly gives us those word vectors that we are really after.Weâ€™re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the â€œnearby wordâ€ that we chose.When I say â€œnearbyâ€, there is actually a â€œwindow sizeâ€ parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word â€œSovietâ€, the output probabilities are going to be much higher for words like â€œUnionâ€ and â€œRussiaâ€ than for unrelated words like â€œwatermelonâ€ and â€œkangarooâ€.Weâ€™ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence â€œThe quick brown fox jumps over the lazy dog.â€ Iâ€™ve used a small window size of 2 just for the example. The word highlighted in blue is the input word.The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (â€œSovietâ€, â€œUnionâ€) than it is of (â€œSovietâ€, â€œSasquatchâ€). When the training is finished, if you give it the word â€œSovietâ€ as input, then it will output a much higher probability for â€œUnionâ€ or â€œRussiaâ€ than it will for â€œSasquatchâ€.Model DetailsSo how is this all represented?First of all, you know you canâ€™t feed a word just as a text string to a neural network, so we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documentsâ€“letâ€™s say we have a vocabulary of 10,000 unique words.Weâ€™re going to represent an input word like â€œantsâ€ as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and weâ€™ll place a â€œ1â€ in the position corresponding to the word â€œantsâ€, and 0s in all of the other positions.The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.Hereâ€™s the architecture of our neural network.There is no activation function on the hidden layer neurons, but the output neurons use softmax. Weâ€™ll come back to this later.When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vectorrepresenting the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector).The Hidden LayerFor our example, weâ€™re going to say that weâ€™re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). The number of features is a â€œhyper parameterâ€ that you would just have to tune to your application (that is, try different values and see what yields the best results).If you look at the rows of this weight matrix, these are actually what will be our word vectors!So the end goal of all of this is really just to learn this hidden layer weight matrix â€“ the output layer weâ€™ll just toss when weâ€™re done!Letâ€™s get back, though, to working through the definition of this model that weâ€™re going to train.Now, you might be asking yourselfâ€“â€œThat one-hot vector is almost all zerosâ€¦ whatâ€™s the effect of that?â€ If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just select the matrix row corresponding to the â€œ1â€. Hereâ€™s a small example to give you a visual.This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the â€œword vectorâ€ for the input word.The Output LayerThe 1 x 300 word vector for â€œantsâ€ then gets fed to the output layer. The output layer is a softmax regression classifier. Thereâ€™s an in-depth tutorial on Softmax Regression here, but the gist of it is that each output neuron (one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function exp(x) to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from all 10,000 output nodes.Hereâ€™s an illustration of calculating the output of the output neuron for the word â€œcarâ€.Note that neural network does not know anything about the offset of the output word relative to the input word. It does not learn a different set of probabilities for the word before the input versus the word after. To understand the implication, letâ€™s say that in our training corpus, every single occurrence of the word â€˜Yorkâ€™ is preceded by the word â€˜Newâ€™. That is, at least according to the training data, there is a 100% probability that â€˜Newâ€™ will be in the vicinity of â€˜Yorkâ€™. However, if we take the 10 words in the vicinity of â€˜Yorkâ€™ and randomly pick one of them, the probability of it being â€˜Newâ€™ is not 100%; you may have picked one of the other words in the vicinity.IntuitionOk, are you ready for an exciting bit of insight into this network?If two different words have very similar â€œcontextsâ€ (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da!And what does it mean for two words to have similar contexts? I think you could expect that synonyms like â€œintelligentâ€ and â€œsmartâ€ would have very similar contexts. Or that words that are related, like â€œengineâ€ and â€œtransmissionâ€, would probably have similar contexts as well.This can also handle stemming for you â€“ the network will likely learn similar word vectors for the words â€œantâ€ and â€œantsâ€ because these should have similar contexts.More Math DetailsFor each word $t=1\cdots T$, predict surrounding words in a window of â€œradiusâ€ $m$ of every word.Objective function:Maximize the probability of any context word given the current center word:The Skip-Gram Algorithm:Gradients]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Prioritized Experience Replay]]></title>
      <url>%2F2017%2F10%2F30%2FPrioritized-Experience-Replay%2F</url>
      <content type="text"><![CDATA[Prioritized Experience ReplayOne of the possible improvements already acknowledged in the original research2 lays in the way experience is used. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay3(PER) is one strategy that tries to leverage this fact by changing the sampling distribution.The main idea is that we prefer transitions that does not fit well to our current estimate of the Q function, because these are the transitions that we can learn most from. This reflects a simple intuition from our real world â€“ if we encounter a situation that really differs from our expectation, we think about it over and over and change our model until it fits.We can define an error of a sample $S = (s, a, r, sâ€™)$ as a distance between the $Q(s, a)$ and its target $T(S)$:$$error = |Q(s, a) - T(S)|$$For DDQN described above, $T$ it would be:$$T(S) = r + \gamma \tilde{Q}(sâ€™, argmax_a Q(sâ€™, a))$$We will store this error in the agentâ€™s memory along with every sample and update it with each learning step.One of the possible approaches to PER is proportional prioritization. The error is first converted to priority using this formula:$$p = (error + \epsilon)^\alpha$$Epsilon $\epsilon$ is a small positive constant that ensures that no transition has zero priority.Alpha, $0 \leq \alpha \leq 1$, controls the difference between high and low error. It determines how much prioritization is used. With $\alpha$ we would get the uniform case.Priority is translated to probability of being chosen for replay. A sample $i$ has a probability of being picked during the experience replay determined by a formula:$$P_i = \frac{p_i}{\sum_k p_k}$$The algorithm is simple â€“ during each learning step we will get a batch of samples with this probability distribution and train our network on it. We only need an effective way of storing these priorities and sampling from them.Initialization and new transitionsThe original paper says that new transitions come without a known error3, but I argue that with definition given above, we can compute it with a simple forward pass as it arrives. Itâ€™s also effective, because high value transitions are discovered immediately.Another thing is the initialization. Remember that before the learning itself, we fill the memory using random agent. But this agent does not use any neural network, so how could we estimate any error? We can use a fact that untrained neural network is likely to return a value around zero for every input. In this case the error formula becomes very simple:$$error = |Q(s, a) - T(S)| = |Q(s, a) - r - \gamma \tilde{Q}(sâ€™, argmax_a Q(sâ€™, a))| = | r |$$The error in this case is simply the reward experienced in a given sample. Indeed, the transitions where the agent experienced any reward intuitively seem to be very promising.Efficient implementationSo how do we store the experience and effectively sample from it?A naive implementation would be to have all samples in an array sorted according to their priorities. A random number s, $0 \leq s \leq \sum_k p_k$, would be picked and the array would be walked left to right, summing up a priority of the current element until the sum is exceeded and that element is chosen. This will select a sample with the desired probability distribution.But this would have a terrible efficiency: $O(n log n)$ for insertion and update and O$(n) $for sampling.A first important observation is that we donâ€™t have to actually store the array sorted. Unsorted array would do just as well. Elements with higher priorities are still picked with higher probability.This releases the need for sorting, improving the algorithm to O(1) for insertion and update.But the O(n) for sampling is still too high. We can further improve our algorithm by using a different data structure. We can store our samples in unsorted sum tree â€“ a binary tree data structure where the parentâ€™s value is the sum of its children. The samples themselves are stored in the leaf nodes.Update of a leaf node involves propagating a value difference up the tree, obtaining O(log n). Sampling follows the thought process of the array case, but achieves O(log n). For a value s, $0 \leq s \leq \sum_k p_k$, we use the following algorithm (pseudo code):12345def retrieve(n, s): if n is leaf_node: return n if n.left.val &gt;= s: return retrieve(n.left, s) else: return retrieve(n.right, s - n.left.val)Following picture illustrates sampling from a tree with s = 24:With this effective implementation we can use large memory sizes, with hundreds of thousands or millions of samples.For known capacity, this sum tree data structure can be backed by an array. Its reference implementation containing 50 lines of code is available on GitHub.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu Server 16.04 Install Gnome and remote connect from Windows VNCViewer]]></title>
      <url>%2F2017%2F10%2F26%2FUbuntu-Server-16-04-Install-Gnome-and-remote-connect-from-Windows-VNCViewer%2F</url>
      <content type="text"><![CDATA[ç¬¬ä¸€æ­¥ï¼šè£… Gnome çŽ¯å¢ƒé¦–å…ˆæŒ‰ç…§å¦‚ä¸‹å‘½ä»¤å®‰è£… Gnome çŽ¯å¢ƒã€‚12345sudo apt-get updatesudo apt-get upgradesudo apt-get install gnomesudo apt-get install ubuntu-gnome-desktopsudo apt-get install gnome-shellç¬¬äºŒæ­¥ï¼šå®‰è£… Gnome ç•Œé¢ç®¡ç†å·¥å…·å®‰è£… Gnome æ¡Œé¢çŽ¯å¢ƒçš„é…ç½®å·¥å…·ã€‚å¯ä»¥ä½¿ç”¨è¯¥å·¥ä½œå¯¹ Linux è¿›è¡Œå¾ˆå¤šé…ç½®ï¼ŒåŒ…æ‹¬å¤–è§‚ï¼Œå·¥ä½œå°çš„æ•°é‡ç­‰ã€‚åŽç»­å®‰è£…çš„ä¸»é¢˜å’Œå›¾æ ‡éƒ½å¯ä»¥é€šè¿‡è¿™ä¸ªå·¥å…·çš„ _å¤–è§‚ï¼ˆAppearanceï¼‰_ è¿›è¡Œè°ƒæ•´ã€‚1sudo apt-get gnome-tweak-toolç¬¬ä¸‰æ­¥ï¼šå®‰è£… Dash to Dock å·¥å…·æ¡å®‰è£… Gnome æ¡Œé¢çŽ¯å¢ƒä¸‹çš„ Dock å·¥å…·æ¡ï¼Œå¯æä¾› mac os ä¸‹dockç±»ä¼¼çš„ä½¿ç”¨ä½“éªŒã€‚åœ¨ä»»æ„æµè§ˆå™¨æ‰“å¼€ Gnome extensions.æ‰¾åˆ° _Dash to Dock_ æ‰©å±•æ ï¼Œç‚¹å¼€å³é¢çš„ _[ON OFF]_ é€‰é¡¹ã€‚ç‚¹å‡»æ—è¾¹çš„ _å·¥å…·_ é€‰é¡¹ï¼Œå¯è¿›ä¸€æ­¥é…ç½®æ›´å¤šé€‰é¡¹ã€‚ç¬¬å››æ­¥ï¼šå®‰è£… _ARC_ æ‰å¹³åŒ–ä¸»é¢˜å’Œå›¾æ ‡12345sudo add-apt-repository ppa:noobslab/themessudo add-apt-repository ppa:noobslab/iconssudo apt-get updatesudo apt-get install arc-themesudo apt-get install arc-iconsç¬¬äº”æ­¥ï¼šé€‰è£… _Flat Plat_ æ‰å¹³åŒ–ä¸»é¢˜å¦ä¸€ä¸ªæ‰å¹³åŒ–ä¸»é¢˜ã€‚123curl -sL https://github.com/nana-4/Flat-Plat/archive/v20170323.tar.gz | tar xzcd Flat-Plat-20170323/sudo ./install.shç¬¬å…­æ­¥ï¼šå®‰è£…vncserver12345sudo apt-get install vnc4serversudo apt-get install gnome-panel gnome-settings-daemon metacity nautilus gnome-terminalcd ~/.vncmv xstartup xstartup.bakvim xstartupä½¿ç”¨ä»¥ä¸‹é…ç½®æ–‡ä»¶ï¼š1234567891011121314151617#!/bin/shexport XKL_XMODMAP_DISABLE=1unset SESSION_MANAGERunset DBUS_SESSION_BUS_ADDRESS[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresourcesxsetroot -solid greyvncconfig -iconic &amp;gnome-session &amp;gnome-panel &amp;gnome-settings-daemon &amp;metacity &amp;nautilus &amp;gnome-terminal &amp;ç¬¬ä¸ƒæ­¥ï¼šå¯åŠ¨ vncserver12# ï¼š1å¯ä»¥æ›´æ”¹vncserver -geometry 1920x1080 -alwaysshared :1ç¬¬å…«æ­¥ï¼šåœ¨ Windows ä¸Šå®‰è£… VNCViewerå¯åŠ¨åªè¦ è¾“å…¥ ip:1 å³å¯]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Union Find]]></title>
      <url>%2F2017%2F10%2F23%2FUnion-Find%2F</url>
      <content type="text"><![CDATA[äº”åˆ†é’Ÿæžæ‡‚å¹¶æŸ¥é›†è½¬è‡ªï¼šlasersshttp://blog.csdn.net/dellaserss/article/details/7724401/å¹¶æŸ¥é›†æ˜¯æˆ‘æš‘å‡ä»Žé«˜æ‰‹é‚£é‡Œå­¦åˆ°çš„ä¸€æ‹›ï¼Œè§‰å¾—çœŸæ˜¯å¤ªç²¾å¦™çš„è®¾è®¡äº†ã€‚æ¥çœ‹ä¸€ä¸ªå®žä¾‹ï¼Œæ­ç”µ1232ç•…é€šå·¥ç¨‹ã€‚é¦–å…ˆåœ¨åœ°å›¾ä¸Šç»™ä½ è‹¥å¹²ä¸ªåŸŽé•‡ï¼Œè¿™äº›åŸŽé•‡éƒ½å¯ä»¥çœ‹ä½œç‚¹ï¼Œç„¶åŽå‘Šè¯‰ä½ å“ªäº›å¯¹åŸŽé•‡ä¹‹é—´æ˜¯æœ‰é“è·¯ç›´æŽ¥ç›¸è¿žçš„ã€‚æœ€åŽè¦è§£å†³çš„æ˜¯æ•´å¹…å›¾çš„è¿žé€šæ€§é—®é¢˜ã€‚æ¯”å¦‚éšæ„ç»™ä½ ä¸¤ä¸ªç‚¹ï¼Œè®©ä½ åˆ¤æ–­å®ƒä»¬æ˜¯å¦è¿žé€šï¼Œæˆ–è€…é—®ä½ æ•´å¹…å›¾ä¸€å…±æœ‰å‡ ä¸ªè¿žé€šåˆ†æ”¯ï¼Œä¹Ÿå°±æ˜¯è¢«åˆ†æˆäº†å‡ ä¸ªäº’ç›¸ç‹¬ç«‹çš„å—ã€‚åƒç•…é€šå·¥ç¨‹è¿™é¢˜ï¼Œé—®è¿˜éœ€è¦ä¿®å‡ æ¡è·¯ï¼Œå®žè´¨å°±æ˜¯æ±‚æœ‰å‡ ä¸ªè¿žé€šåˆ†æ”¯ã€‚å¦‚æžœæ˜¯1ä¸ªè¿žé€šåˆ†æ”¯ï¼Œè¯´æ˜Žæ•´å¹…å›¾ä¸Šçš„ç‚¹éƒ½è¿žèµ·æ¥äº†ï¼Œä¸ç”¨å†ä¿®è·¯äº†ï¼›å¦‚æžœæ˜¯2ä¸ªè¿žé€šåˆ†æ”¯ï¼Œåˆ™åªè¦å†ä¿®1æ¡è·¯ï¼Œä»Žä¸¤ä¸ªåˆ†æ”¯ä¸­å„é€‰ä¸€ä¸ªç‚¹ï¼ŒæŠŠå®ƒä»¬è¿žèµ·æ¥ï¼Œé‚£ä¹ˆæ‰€æœ‰çš„ç‚¹éƒ½æ˜¯è¿žèµ·æ¥çš„äº†ï¼›å¦‚æžœæ˜¯3ä¸ªè¿žé€šåˆ†æ”¯ï¼Œåˆ™åªè¦å†ä¿®ä¸¤æ¡è·¯â€¦â€¦ä»¥ä¸‹é¢è¿™ç»„æ•°æ®è¾“å…¥æ•°æ®æ¥è¯´æ˜Ž4 2 1 3 4 3ç¬¬ä¸€è¡Œå‘Šè¯‰ä½ ï¼Œä¸€å…±æœ‰4ä¸ªç‚¹ï¼Œ2æ¡è·¯ã€‚ä¸‹é¢ä¸¤è¡Œå‘Šè¯‰ä½ ï¼Œ1ã€3ä¹‹é—´æœ‰æ¡è·¯ï¼Œ4ã€3ä¹‹é—´æœ‰æ¡è·¯ã€‚é‚£ä¹ˆæ•´å¹…å›¾å°±è¢«åˆ†æˆäº†1-3-4å’Œ2ä¸¤éƒ¨åˆ†ã€‚åªè¦å†åŠ ä¸€æ¡è·¯ï¼ŒæŠŠ2å’Œå…¶ä»–ä»»æ„ä¸€ä¸ªç‚¹è¿žèµ·æ¥ï¼Œç•…é€šå·¥ç¨‹å°±å®žçŽ°äº†ï¼Œé‚£ä¹ˆè¿™ä¸ªè¿™ç»„æ•°æ®çš„è¾“å‡ºç»“æžœå°±æ˜¯1ã€‚å¥½äº†ï¼ŒçŽ°åœ¨ç¼–ç¨‹å®žçŽ°è¿™ä¸ªåŠŸèƒ½å§ï¼ŒåŸŽé•‡æœ‰å‡ ç™¾ä¸ªï¼Œè·¯æœ‰ä¸çŸ¥é“å¤šå°‘æ¡ï¼Œè€Œä¸”å¯èƒ½æœ‰å›žè·¯ã€‚è¿™å¯å¦‚ä½•æ˜¯å¥½ï¼Ÿæˆ‘ä»¥å‰ä¹Ÿä¸ä¼šå‘€ï¼Œè‡ªä»Žç”¨äº†å¹¶æŸ¥é›†ä¹‹åŽï¼Œå—¨ï¼Œæ•ˆæžœè¿˜çœŸå¥½ï¼æˆ‘ä»¬å…¨å®¶éƒ½ç”¨å®ƒï¼å¹¶æŸ¥é›†ç”±ä¸€ä¸ªæ•´æ•°åž‹çš„æ•°ç»„å’Œä¸¤ä¸ªå‡½æ•°æž„æˆã€‚æ•°ç»„pre[]è®°å½•äº†æ¯ä¸ªç‚¹çš„å‰å¯¼ç‚¹æ˜¯ä»€ä¹ˆï¼Œå‡½æ•°findæ˜¯æŸ¥æ‰¾ï¼Œjoinæ˜¯åˆå¹¶ã€‚12345678910111213141516171819202122232425int pre[1000 ];int find(int x) //æŸ¥æ‰¾æ ¹èŠ‚ç‚¹&#123; int r=x; while ( pre[r ] != r ) //è¿”å›žæ ¹èŠ‚ç‚¹ r r=pre[r ]; int i=x , j ; while( i != r ) //è·¯å¾„åŽ‹ç¼© &#123; j = pre[ i ]; // åœ¨æ”¹å˜ä¸Šçº§ä¹‹å‰ç”¨ä¸´æ—¶å˜é‡ j è®°å½•ä¸‹ä»–çš„å€¼ pre[ i ]= r ; //æŠŠä¸Šçº§æ”¹ä¸ºæ ¹èŠ‚ç‚¹ i=j; &#125; return r ; &#125; //åˆ¤æ–­x yæ˜¯å¦è¿žé€šï¼Œå¦‚æžœå·²ç»è¿žé€šï¼Œå°±ä¸ç”¨ç®¡äº† //å¦‚æžœä¸è¿žé€šï¼Œå°±æŠŠå®ƒä»¬æ‰€åœ¨çš„è¿žé€šåˆ†æ”¯åˆå¹¶èµ·, void join(int x,int y) &#123; int fx=find(x),fy=find(y); if(fx!=fy) pre[fx ]=fy; &#125;ä¸ºäº†è§£é‡Šå¹¶æŸ¥é›†çš„åŽŸç†ï¼Œæˆ‘å°†ä¸¾ä¸€ä¸ªæ›´æœ‰çˆ±çš„ä¾‹å­ã€‚ è¯è¯´æ±Ÿæ¹–ä¸Šæ•£è½ç€å„å¼å„æ ·çš„å¤§ä¾ ï¼Œæœ‰ä¸Šåƒä¸ªä¹‹å¤šã€‚ä»–ä»¬æ²¡æœ‰ä»€ä¹ˆæ­£å½“èŒä¸šï¼Œæ•´å¤©èƒŒç€å‰‘åœ¨å¤–é¢èµ°æ¥èµ°åŽ»ï¼Œç¢°åˆ°å’Œè‡ªå·±ä¸æ˜¯ä¸€è·¯äººçš„ï¼Œå°±å…ä¸äº†è¦æ‰“ä¸€æž¶ã€‚ä½†å¤§ä¾ ä»¬æœ‰ä¸€ä¸ªä¼˜ç‚¹å°±æ˜¯è®²ä¹‰æ°”ï¼Œç»å¯¹ä¸æ‰“è‡ªå·±çš„æœ‹å‹ã€‚è€Œä¸”ä»–ä»¬ä¿¡å¥‰â€œæœ‹å‹çš„æœ‹å‹å°±æ˜¯æˆ‘çš„æœ‹å‹â€ï¼Œåªè¦æ˜¯èƒ½é€šè¿‡æœ‹å‹å…³ç³»ä¸²è”èµ·æ¥çš„ï¼Œä¸ç®¡æ‹äº†å¤šå°‘ä¸ªå¼¯ï¼Œéƒ½è®¤ä¸ºæ˜¯è‡ªå·±äººã€‚è¿™æ ·ä¸€æ¥ï¼Œæ±Ÿæ¹–ä¸Šå°±å½¢æˆäº†ä¸€ä¸ªä¸€ä¸ªçš„ç¾¤è½ï¼Œé€šè¿‡ä¸¤ä¸¤ä¹‹é—´çš„æœ‹å‹å…³ç³»ä¸²è”èµ·æ¥ã€‚è€Œä¸åœ¨åŒä¸€ä¸ªç¾¤è½çš„äººï¼Œæ— è®ºå¦‚ä½•éƒ½æ— æ³•é€šè¿‡æœ‹å‹å…³ç³»è¿žèµ·æ¥ï¼ŒäºŽæ˜¯å°±å¯ä»¥æ”¾å¿ƒå¾€æ­»äº†æ‰“ã€‚ä½†æ˜¯ä¸¤ä¸ªåŽŸæœ¬äº’ä¸ç›¸è¯†çš„äººï¼Œå¦‚ä½•åˆ¤æ–­æ˜¯å¦å±žäºŽä¸€ä¸ªæœ‹å‹åœˆå‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸ªæœ‹å‹åœˆå†…æŽ¨ä¸¾å‡ºä¸€ä¸ªæ¯”è¾ƒæœ‰åæœ›çš„äººï¼Œä½œä¸ºè¯¥åœˆå­çš„ä»£è¡¨äººç‰©ï¼Œè¿™æ ·ï¼Œæ¯ä¸ªåœˆå­å°±å¯ä»¥è¿™æ ·å‘½åâ€œé½è¾¾å†…æœ‹å‹ä¹‹é˜Ÿâ€â€œç½—çº³å°”å¤šæœ‹å‹ä¹‹é˜Ÿâ€â€¦â€¦ä¸¤äººåªè¦äº’ç›¸å¯¹ä¸€ä¸‹è‡ªå·±çš„é˜Ÿé•¿æ˜¯ä¸æ˜¯åŒä¸€ä¸ªäººï¼Œå°±å¯ä»¥ç¡®å®šæ•Œå‹å…³ç³»äº†ã€‚ä½†æ˜¯è¿˜æœ‰é—®é¢˜å•Šï¼Œå¤§ä¾ ä»¬åªçŸ¥é“è‡ªå·±ç›´æŽ¥çš„æœ‹å‹æ˜¯è°ï¼Œå¾ˆå¤šäººåŽ‹æ ¹å°±ä¸è®¤è¯†é˜Ÿé•¿ï¼Œè¦åˆ¤æ–­è‡ªå·±çš„é˜Ÿé•¿æ˜¯è°ï¼Œåªèƒ½æ¼«æ— ç›®çš„çš„é€šè¿‡æœ‹å‹çš„æœ‹å‹å…³ç³»é—®ä¸‹åŽ»ï¼šâ€œä½ æ˜¯ä¸æ˜¯é˜Ÿé•¿ï¼Ÿä½ æ˜¯ä¸æ˜¯é˜Ÿé•¿ï¼Ÿâ€è¿™æ ·ä¸€æ¥ï¼Œé˜Ÿé•¿é¢å­ä¸ŠæŒ‚ä¸ä½äº†ï¼Œè€Œä¸”æ•ˆçŽ‡å¤ªä½Žï¼Œè¿˜æœ‰å¯èƒ½é™·å…¥æ— é™å¾ªçŽ¯ä¸­ã€‚äºŽæ˜¯é˜Ÿé•¿ä¸‹ä»¤ï¼Œé‡æ–°ç»„é˜Ÿã€‚é˜Ÿå†…æ‰€æœ‰äººå®žè¡Œåˆ†ç­‰çº§åˆ¶åº¦ï¼Œå½¢æˆæ ‘çŠ¶ç»“æž„ï¼Œæˆ‘é˜Ÿé•¿å°±æ˜¯æ ¹èŠ‚ç‚¹ï¼Œä¸‹é¢åˆ†åˆ«æ˜¯äºŒçº§é˜Ÿå‘˜ã€ä¸‰çº§é˜Ÿå‘˜ã€‚æ¯ä¸ªäººåªè¦è®°ä½è‡ªå·±çš„ä¸Šçº§æ˜¯è°å°±è¡Œäº†ã€‚é‡åˆ°åˆ¤æ–­æ•Œå‹çš„æ—¶å€™ï¼Œåªè¦ä¸€å±‚å±‚å‘ä¸Šé—®ï¼Œç›´åˆ°æœ€é«˜å±‚ï¼Œå°±å¯ä»¥åœ¨çŸ­æ—¶é—´å†…ç¡®å®šé˜Ÿé•¿æ˜¯è°äº†ã€‚ç”±äºŽæˆ‘ä»¬å…³å¿ƒçš„åªæ˜¯ä¸¤ä¸ªäººä¹‹é—´æ˜¯å¦è¿žé€šï¼Œè‡³äºŽä»–ä»¬æ˜¯å¦‚ä½•è¿žé€šçš„ï¼Œä»¥åŠæ¯ä¸ªåœˆå­å†…éƒ¨çš„ç»“æž„æ˜¯æ€Žæ ·çš„ï¼Œç”šè‡³é˜Ÿé•¿æ˜¯è°ï¼Œå¹¶ä¸é‡è¦ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ”¾ä»»é˜Ÿé•¿éšæ„é‡æ–°ç»„é˜Ÿï¼Œåªè¦ä¸æžé”™æ•Œå‹å…³ç³»å°±å¥½äº†ã€‚äºŽæ˜¯ï¼Œé—¨æ´¾äº§ç”Ÿäº†ã€‚ä¸‹é¢æˆ‘ä»¬æ¥çœ‹å¹¶æŸ¥é›†çš„å®žçŽ°ã€‚ int pre[1000]; è¿™ä¸ªæ•°ç»„ï¼Œè®°å½•äº†æ¯ä¸ªå¤§ä¾ çš„ä¸Šçº§æ˜¯è°ã€‚å¤§ä¾ ä»¬ä»Ž1æˆ–è€…0å¼€å§‹ç¼–å·ï¼ˆä¾æ®é¢˜æ„è€Œå®šï¼‰ï¼Œpre[15]=3å°±è¡¨ç¤º15å·å¤§ä¾ çš„ä¸Šçº§æ˜¯3å·å¤§ä¾ ã€‚å¦‚æžœä¸€ä¸ªäººçš„ä¸Šçº§å°±æ˜¯ä»–è‡ªå·±ï¼Œé‚£è¯´æ˜Žä»–å°±æ˜¯æŽŒé—¨äººäº†ï¼ŒæŸ¥æ‰¾åˆ°æ­¤ä¸ºæ­¢ã€‚ä¹Ÿæœ‰å­¤å®¶å¯¡äººè‡ªæˆä¸€æ´¾çš„ï¼Œæ¯”å¦‚æ¬§é˜³é”‹ï¼Œé‚£ä¹ˆä»–çš„ä¸Šçº§å°±æ˜¯ä»–è‡ªå·±ã€‚æ¯ä¸ªäººéƒ½åªè®¤è‡ªå·±çš„ä¸Šçº§ã€‚æ¯”å¦‚èƒ¡é’ç‰›åŒå­¦åªçŸ¥é“è‡ªå·±çš„ä¸Šçº§æ˜¯æ¨å·¦ä½¿ã€‚å¼ æ— å¿Œæ˜¯è°ï¼Ÿä¸è®¤è¯†ï¼è¦æƒ³çŸ¥é“è‡ªå·±çš„æŽŒé—¨æ˜¯è°ï¼Œåªèƒ½ä¸€çº§çº§æŸ¥ä¸ŠåŽ»ã€‚ findè¿™ä¸ªå‡½æ•°å°±æ˜¯æ‰¾æŽŒé—¨ç”¨çš„ï¼Œæ„ä¹‰å†æ¸…æ¥šä¸è¿‡äº†ï¼ˆè·¯å¾„åŽ‹ç¼©ç®—æ³•å…ˆä¸è®ºï¼ŒåŽé¢å†è¯´ï¼‰ã€‚1234567int find(int x) //æŸ¥æ‰¾æˆ‘ï¼ˆxï¼‰çš„æŽŒé—¨&#123; int r=x; //å§”æ‰˜ r åŽ»æ‰¾æŽŒé—¨ while (pre[r ]!=r) //å¦‚æžœrçš„ä¸Šçº§ä¸æ˜¯rè‡ªå·±ï¼ˆä¹Ÿå°±æ˜¯è¯´æ‰¾åˆ°çš„å¤§ä¾ ä»–ä¸æ˜¯æŽŒé—¨ = =ï¼‰ r=pre[r] ; // r å°±æŽ¥ç€æ‰¾ä»–çš„ä¸Šçº§ï¼Œç›´åˆ°æ‰¾åˆ°æŽŒé—¨ä¸ºæ­¢ã€‚ return r ; //æŽŒé—¨é©¾åˆ°~~~&#125;å†æ¥çœ‹çœ‹joinå‡½æ•°ï¼Œå°±æ˜¯åœ¨ä¸¤ä¸ªç‚¹ä¹‹é—´è¿žä¸€æ¡çº¿ï¼Œè¿™æ ·ä¸€æ¥ï¼ŒåŽŸå…ˆå®ƒä»¬æ‰€åœ¨çš„ä¸¤ä¸ªæ¿å—çš„æ‰€æœ‰ç‚¹å°±éƒ½å¯ä»¥äº’é€šäº†ã€‚è¿™åœ¨å›¾ä¸Šå¾ˆå¥½åŠžï¼Œç”»æ¡çº¿å°±è¡Œäº†ã€‚ä½†æˆ‘ä»¬çŽ°åœ¨æ˜¯ç”¨å¹¶æŸ¥é›†æ¥æè¿°æ­¦æž—ä¸­çš„çŠ¶å†µçš„ï¼Œä¸€å…±åªæœ‰ä¸€ä¸ªpre[]æ•°ç»„ï¼Œè¯¥å¦‚ä½•å®žçŽ°å‘¢ï¼Ÿè¿˜æ˜¯ä¸¾æ±Ÿæ¹–çš„ä¾‹å­ï¼Œå‡è®¾çŽ°åœ¨æ­¦æž—ä¸­çš„å½¢åŠ¿å¦‚å›¾æ‰€ç¤ºã€‚è™šç«¹å°å’Œå°šä¸Žå‘¨èŠ·è‹¥MMæ˜¯æˆ‘éžå¸¸å–œæ¬¢çš„ä¸¤ä¸ªäººç‰©ï¼Œä»–ä»¬çš„ç»ˆæžbossåˆ†åˆ«æ˜¯çŽ„æ…ˆæ–¹ä¸ˆå’Œç­ç»å¸ˆå¤ªï¼Œé‚£æ˜Žæ˜¾å°±æ˜¯ä¸¤ä¸ªé˜µè¥äº†ã€‚æˆ‘ä¸å¸Œæœ›ä»–ä»¬äº’ç›¸æ‰“æž¶ï¼Œå°±å¯¹ä»–ä¿©è¯´ï¼šâ€œä½ ä»¬ä¸¤ä½æ‹‰æ‹‰å‹¾ï¼Œåšå¥½æœ‹å‹å§ã€‚â€ä»–ä»¬çœ‹åœ¨æˆ‘çš„é¢å­ä¸Šï¼ŒåŒæ„äº†ã€‚è¿™ä¸€åŒæ„å¯éžåŒå°å¯ï¼Œæ•´ä¸ªå°‘æž—å’Œå³¨çœ‰æ´¾çš„äººå°±ä¸èƒ½æ‰“æž¶äº†ã€‚è¿™ä¹ˆé‡å¤§çš„å˜åŒ–ï¼Œå¯å¦‚ä½•å®žçŽ°å‘€ï¼Œè¦æ”¹åŠ¨å¤šå°‘åœ°æ–¹ï¼Ÿå…¶å®žéžå¸¸ç®€å•ï¼Œæˆ‘å¯¹çŽ„æ…ˆæ–¹ä¸ˆè¯´ï¼šâ€œå¤§å¸ˆï¼Œéº»çƒ¦ä½ æŠŠä½ çš„ä¸Šçº§æ”¹ä¸ºç­ç»å¸ˆå¤ªå§ã€‚è¿™æ ·ä¸€æ¥ï¼Œä¸¤æ´¾åŽŸå…ˆçš„æ‰€æœ‰äººå‘˜çš„ç»ˆæžbosséƒ½æ˜¯å¸ˆå¤ªï¼Œé‚£è¿˜æ‰“ä¸ªçƒå•Šï¼åæ­£æˆ‘ä»¬å…³å¿ƒçš„åªæ˜¯è¿žé€šæ€§ï¼Œé—¨æ´¾å†…éƒ¨çš„ç»“æž„ä¸è¦ç´§çš„ã€‚â€çŽ„æ…ˆä¸€å¬è‚¯å®šç«å¤§äº†ï¼šâ€œæˆ‘é ï¼Œå‡­ä»€ä¹ˆæ˜¯æˆ‘å˜æˆå¥¹æ‰‹ä¸‹å‘€ï¼Œæ€Žä¹ˆä¸åè¿‡æ¥ï¼Ÿæˆ‘æŠ—è®®ï¼â€æŠ—è®®æ— æ•ˆï¼Œä¸Šå¤©å®‰æŽ’çš„ï¼Œæœ€å¤§ã€‚åæ­£è°åŠ å…¥è°æ•ˆæžœæ˜¯ä¸€æ ·çš„ï¼Œæˆ‘å°±éšæ‰‹æŒ‡å®šäº†ä¸€ä¸ªã€‚è¿™æ®µå‡½æ•°çš„æ„æ€å¾ˆæ˜Žç™½äº†å§ï¼Ÿ1234567void join(int x,int y) //æˆ‘æƒ³è®©è™šç«¹å’Œå‘¨èŠ·è‹¥åšæœ‹å‹ &#123; int fx=find(x),fy=find(y); //è™šç«¹çš„è€å¤§æ˜¯çŽ„æ…ˆï¼ŒèŠ·è‹¥MMçš„è€å¤§æ˜¯ç­ç» if(fx!=fy) //çŽ„æ…ˆå’Œç­ç»æ˜¾ç„¶ä¸æ˜¯åŒä¸€ä¸ªäºº pre[fx]=fy; //æ–¹ä¸ˆåªå¥½å§”å§”å±ˆå±ˆåœ°å½“äº†å¸ˆå¤ªçš„æ‰‹ä¸‹å•¦ &#125;å†æ¥çœ‹çœ‹è·¯å¾„åŽ‹ç¼©ç®—æ³•ã€‚å»ºç«‹é—¨æ´¾çš„è¿‡ç¨‹æ˜¯ç”¨joinå‡½æ•°ä¸¤ä¸ªäººä¸¤ä¸ªäººåœ°è¿žæŽ¥èµ·æ¥çš„ï¼Œè°å½“è°çš„æ‰‹ä¸‹å®Œå…¨éšæœºã€‚æœ€åŽçš„æ ‘çŠ¶ç»“æž„ä¼šå˜æˆä»€ä¹ˆèƒŽå”‡æ ·ï¼Œæˆ‘ä¹Ÿå®Œå…¨æ— æ³•é¢„è®¡ï¼Œä¸€å­—æŽ’å¼€ä¹Ÿæœ‰å¯èƒ½ã€‚è¿™æ ·æŸ¥æ‰¾çš„æ•ˆçŽ‡å°±ä¼šæ¯”è¾ƒä½Žä¸‹ã€‚æœ€ç†æƒ³çš„æƒ…å†µå°±æ˜¯æ‰€æœ‰äººçš„ç›´æŽ¥ä¸Šçº§éƒ½æ˜¯æŽŒé—¨ï¼Œä¸€å…±å°±ä¸¤çº§ç»“æž„ï¼Œåªè¦æ‰¾ä¸€æ¬¡å°±æ‰¾åˆ°æŽŒé—¨äº†ã€‚å“ªæ€•ä¸èƒ½å®Œå…¨åšåˆ°ï¼Œä¹Ÿæœ€å¥½å°½é‡æŽ¥è¿‘ã€‚è¿™æ ·å°±äº§ç”Ÿäº†è·¯å¾„åŽ‹ç¼©ç®—æ³•ã€‚è®¾æƒ³è¿™æ ·ä¸€ä¸ªåœºæ™¯ï¼šä¸¤ä¸ªäº’ä¸ç›¸è¯†çš„å¤§ä¾ ç¢°é¢äº†ï¼Œæƒ³çŸ¥é“èƒ½ä¸èƒ½æã€‚ äºŽæ˜¯èµ¶ç´§æ‰“ç”µè¯é—®è‡ªå·±çš„ä¸Šçº§ï¼šâ€œä½ æ˜¯ä¸æ˜¯æŽŒé—¨ï¼Ÿâ€ ä¸Šçº§è¯´ï¼šâ€œæˆ‘ä¸æ˜¯å‘€ï¼Œæˆ‘çš„ä¸Šçº§æ˜¯è°è°è°ï¼Œä½ é—®é—®ä»–çœ‹çœ‹ã€‚â€ ä¸€è·¯é—®ä¸‹åŽ»ï¼ŒåŽŸæ¥ä¸¤äººçš„æœ€ç»ˆbosséƒ½æ˜¯ä¸œåŽ‚æ›¹å…¬å…¬ã€‚ â€œå“Žå‘€å‘€ï¼ŒåŽŸæ¥æ˜¯è®°å·±äººï¼Œè¥¿ç¤¼è¥¿ç¤¼ï¼Œåœ¨ä¸‹ä¸‰è¥å…­ç»„ç™½é¢è‘«èŠ¦å¨ƒ!â€ â€œå¹¸ä¼šå¹¸ä¼šï¼Œåœ¨ä¸‹ä¹è¥åå…«ç»„ä»™å­ç‹—å°¾å·´èŠ±ï¼â€ ä¸¤äººé«˜é«˜å…´å…´åœ°æ‰‹æ‹‰æ‰‹å–é…’åŽ»äº†ã€‚ â€œç­‰ç­‰ç­‰ç­‰ï¼Œä¸¤ä½åŒå­¦è¯·ç•™æ­¥ï¼Œè¿˜æœ‰äº‹æƒ…æ²¡å®Œæˆå‘¢ï¼â€æˆ‘å«ä½ä»–ä¿©ã€‚ â€œå“¦ï¼Œå¯¹äº†ï¼Œè¿˜è¦åšè·¯å¾„åŽ‹ç¼©ã€‚â€ä¸¤äººé†’æ‚Ÿã€‚ç™½é¢è‘«èŠ¦å¨ƒæ‰“ç”µè¯ç»™ä»–çš„ä¸Šçº§å…­ç»„é•¿ï¼šâ€œç»„é•¿å•Šï¼Œæˆ‘æŸ¥è¿‡äº†ï¼Œå…¶ä¹ å¶ä»¬çš„æŽŒé—¨æ˜¯æ›¹å…¬å…¬ã€‚ä¸å¦‚å¶ä»¬ä¸€èµ·åŠæŽ¥æ‹œåœ¨æ›¹å…¬å…¬æ‰‹ä¸‹å§ï¼Œçœå¾—çº§åˆ«å¤ªä½Žï¼Œä»¥åŽæŸ¥æ‰¾æŽŒé—¨éº»çŽ¯ã€‚â€ â€œå””ï¼Œæœ‰é“ç†ã€‚â€ ç™½é¢è‘«èŠ¦å¨ƒæŽ¥ç€æ‰“ç”µè¯ç»™åˆšæ‰æ‹œè®¿è¿‡çš„ä¸‰è¥é•¿â€¦â€¦ä»™å­ç‹—å°¾å·´èŠ±ä¹Ÿåšäº†åŒæ ·çš„äº‹æƒ…ã€‚è¿™æ ·ï¼ŒæŸ¥è¯¢ä¸­æ‰€æœ‰æ¶‰åŠåˆ°çš„äººç‰©éƒ½èšé›†åœ¨æ›¹å…¬å…¬çš„ç›´æŽ¥é¢†å¯¼ä¸‹ã€‚æ¯æ¬¡æŸ¥è¯¢éƒ½åšäº†ä¼˜åŒ–å¤„ç†ï¼Œæ‰€ä»¥æ•´ä¸ªé—¨æ´¾æ ‘çš„å±‚æ•°éƒ½ä¼šç»´æŒåœ¨æ¯”è¾ƒä½Žçš„æ°´å¹³ä¸Šã€‚è·¯å¾„åŽ‹ç¼©çš„ä»£ç ï¼Œçœ‹å¾—æ‡‚å¾ˆå¥½ï¼Œçœ‹ä¸æ‡‚ä¹Ÿæ²¡å…³ç³»ï¼Œç›´æŽ¥æŠ„ä¸Šç”¨å°±è¡Œäº†ã€‚æ€»ä¹‹å®ƒæ‰€å®žçŽ°çš„åŠŸèƒ½å°±æ˜¯è¿™ä¹ˆä¸ªæ„æ€ã€‚ä¸‹é¢ç»™å‡ºæ­ç”µ1232ç•…é€šå·¥ç¨‹çš„è§£é¢˜ä»£ç ï¼Œä»…ä¾›å¤§å®¶å‚è€ƒï¼Œä½¿ç”¨å¹¶æŸ¥é›†æ¥è§£å†³é—®é¢˜ã€‚1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include&lt;iostream using namespace std; int pre[1050]; bool t[1050]; //t ç”¨äºŽæ ‡è®°ç‹¬ç«‹å—çš„æ ¹ç»“ç‚¹ int Find(int x) &#123; int r=x; while(r!=pre[r]) r=pre[r]; int i=x,j; while(pre[i]!=r) &#123; j=pre[i]; pre[i]=r; i=j; &#125; return r; &#125; void mix(int x,int y) &#123; int fx=Find(x),fy=Find(y); if(fx!=fy) &#123; pre[fy]=fx; &#125; &#125; int main() &#123; int N,M,a,b,i,j,ans; while(scanf(&quot;%d%d&quot;,&amp;N,&amp;M)&amp;&amp;N) &#123; for(i=1;i&lt;=N;i++) //åˆå§‹åŒ– pre[i]=i; for(i=1;i&lt;=M;i++) //å¸æ”¶å¹¶æ•´ç†æ•°æ® &#123; scanf(&quot;%d%d&quot;,&amp;a,&amp;b); mix(a,b); &#125; memset(t,0,sizeof(t)); for(i=1;i&lt;=N;i++) //æ ‡è®°æ ¹ç»“ç‚¹ &#123; t[Find(i)]=1; &#125; for(ans=0,i=1;i&lt;=N;i++) if(t[i]) ans++; printf(&quot;%d\n&quot;,ans-1); &#125; return 0; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[37 Reasons why your Neural Network is not working [Repost]]]></title>
      <url>%2F2017%2F07%2F28%2F37-Reasons-why-your-Neural-Network-is-not-working%2F</url>
      <content type="text"><![CDATA[The network had been training for the last 12 hours. It all looked good: the gradients were flowing and the loss was decreasing. But then came the predictions: all zeroes, all background, nothing detected. â€œWhat did I do wrong?â€ â€” I asked my computer, who didnâ€™t answer.Where do you start checking if your model is outputting garbage (for example predicting the mean of all outputs, or it has really poor accuracy)?A network might not be training for a number of reasons. Over the course of many debugging sessions, I would often find myself doing the same checks. Iâ€™ve compiled my experience along with the best ideas around in this handy list. I hope they would be of use to you, too.Table of Contents0. How to use this guide?I. Dataset issuesII. Data Normalization/Augmentation issuesIII. Implementation issuesIV. Training issues0. How to use this guide?A lot of things can go wrong. But some of them are more likely to be broken than others. I usually start with this short list as an emergency first response:Start with a simple model that is known to work for this type of data (for example, VGG for images). Use a standard loss if possible.Turn off all bells and whistles, e.g. regularization and data augmentation.If finetuning a model, double check the preprocessing, for it should be the same as the original modelâ€™s training.Verify that the input data is correct.Start with a really small dataset (2â€“20 samples). Overfit on it and gradually add more data.Start gradually adding back all the pieces that were omitted: augmentation/regularization, custom loss functions, try more complex models.If the steps above donâ€™t do it, start going down the following big list and verify things one by one.I. Dataset issuesSource: http://dilbert.com/strip/2014-05-071. Check your input dataCheck if the input data you are feeding the network makes sense. For example, Iâ€™ve more than once mixed the width and the height of an image. Sometimes, I would feed all zeroes by mistake. Or I would use the same batch over and over. So print/display a couple of batches of input and target output and make sure they are OK.2. Try random inputTry passing random numbers instead of actual data and see if the error behaves the same way. If it does, itâ€™s a sure sign that your net is turning data into garbage at some point. Try debugging layer by layer /op by op/ and see where things go wrong.3. Check the data loaderYour data might be fine but the code that passes the input to the net might be broken. Print the input of the first layer before any operations and check it.4. Make sure input is connected to outputCheck if a few input samples have the correct labels. Also make sure shuffling input samples works the same way for output labels.5. Is the relationship between input and output too random?Maybe the non-random part of the relationship between the input and output is too small compared to the random part (one could argue that stock prices are like this). I.e. the input are not sufficiently related to the output. There isnâ€™t an universal way to detect this as it depends on the nature of the data.6. Is there too much noise in the dataset?This happened to me once when I scraped an image dataset off a food site. There were so many bad labels that the network couldnâ€™t learn. Check a bunch of input samples manually and see if labels seem off.The cutoff point is up for debate, as this paper got above 50% accuracy on MNIST using 50% corrupted labels.7. Shuffle the datasetIf your dataset hasnâ€™t been shuffled and has a particular order to it (ordered by label) this could negatively impact the learning. Shuffle your dataset to avoid this. Make sure you are shuffling input and labels together.8. Reduce class imbalanceAre there a 1000 class A images for every class B image? Then you might need to balance your loss function or try other class imbalance approaches.9. Do you have enough training examples?If you are training a net from scratch (i.e. not finetuning), you probably need lots of data. For image classification, people say you need a 1000 images per class or more.10. Make sure your batches donâ€™t contain a single labelThis can happen in a sorted dataset (i.e. the first 10k samples contain the same class). Easily fixable by shuffling the dataset.11. Reduce batch sizeThis paper points out that having a very large batch can reduce the generalization ability of the model.Addition 1. Use standard dataset (e.g. mnist, cifar10)Thanks to @hengcherkeng for this one:When testing new network architecture or writing a new piece of code, use the standard datasets first, instead of your own data. This is because there are many reference results for these datasets and they are proved to be â€˜solvableâ€™. There will be no issues of label noise, train/test distribution difference , too much difficulty in dataset, etc.II. Data Normalization/Augmentation12. Standardize the featuresDid you standardize your input to have zero mean and unit variance?13. Do you have too much data augmentation?Augmentation has a regularizing effect. Too much of this combined with other forms of regularization (weight L2, dropout, etc.) can cause the net to underfit.14. Check the preprocessing of your pretrained modelIf you are using a pretrained model, make sure you are using the same normalization and preprocessing as the model was when training. For example, should an image pixel be in the range [0, 1], [-1, 1] or [0, 255]?15. Check the preprocessing for train/validation/test setCS231n points out a common pitfall:â€œâ€¦ any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation/test data. E.g. computing the mean and subtracting it from every image across the entire dataset and then splitting the data into train/val/test splits would be a mistake. â€œAlso, check for different preprocessing in each sample or batch.III. Implementation issuesCredit: https://xkcd.com/1838/16. Try solving a simpler version of the problemThis will help with finding where the issue is. For example, if the target output is an object class and coordinates, try limiting the prediction to object class only.17. Look for correct loss â€œat chanceâ€Again from the excellent CS231n: Initialize with small parameters, without regularization. For example, if we have 10 classes, at chance means we will get the correct class 10% of the time, and the Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302.After this, try increasing the regularization strength which should increase the loss.18. Check your loss functionIf you implemented your own loss function, check it for bugs and add unit tests. Often, my loss would be slightly incorrect and hurt the performance of the network in a subtle way.19. Verify loss inputIf you are using a loss function provided by your framework, make sure you are passing to it what it expects. For example, in PyTorch I would mix up the NLLLoss and CrossEntropyLoss as the former requires a softmax input and the latter doesnâ€™t.20. Adjust loss weightsIf your loss is composed of several smaller loss functions, make sure their magnitude relative to each is correct. This might involve testing different combinations of loss weights.21. Monitor other metricsSometimes the loss is not the best predictor of whether your network is training properly. If you can, use other metrics like accuracy.22. Test any custom layersDid you implement any of the layers in the network yourself? Check and double-check to make sure they are working as intended.23. Check for â€œfrozenâ€ layers or variablesCheck if you unintentionally disabled gradient updates for some layers/variables that should be learnable.24. Increase network sizeMaybe the expressive power of your network is not enough to capture the target function. Try adding more layers or more hidden units in fully connected layers.25. Check for hidden dimension errorsIf your input looks like (k, H, W) = (64, 64, 64) itâ€™s easy to miss errors related to wrong dimensions. Use weird numbers for input dimensions (for example, different prime numbers for each dimension) and check how they propagate through the network.26. Explore Gradient checkingIf you implemented Gradient Descent by hand, gradient checking makes sure that your backpropagation works like it should. More info: 1 2 3.IV. Training issuesCredit: http://carlvondrick.com/ihog/27. Solve for a really small datasetOverfit a small subset of the data and make sure it works. For example, train with just 1 or 2 examples and see if your network can learn to differentiate these. Move on to more samples per class.28. Check weights initializationIf unsure, use Xavier or He initialization. Also, your initialization might be leading you to a bad local minimum, so try a different initialization and see if it helps.29. Change your hyperparametersMaybe you using a particularly bad set of hyperparameters. If feasible, try a grid search.30. Reduce regularizationToo much regularization can cause the network to underfit badly. Reduce regularization such as dropout, batch norm, weight/bias L2 regularization, etc. In the excellent â€œPractical Deep Learning for codersâ€ course, Jeremy Howard advises getting rid of underfitting first. This means you overfit the training data sufficiently, and only then addressing overfitting.31. Give it timeMaybe your network needs more time to train before it starts making meaningful predictions. If your loss is steadily decreasing, let it train some more.32. Switch from Train to Test modeSome frameworks have layers like Batch Norm, Dropout, and other layers behave differently during training and testing. Switching to the appropriate mode might help your network to predict properly.33. Visualize the trainingMonitor the activations, weights, and updates of each layer. Make sure their magnitudes match. For example, the magnitude of the updates to the parameters (weights and biases) should be 1-e3.Consider a visualization library like Tensorboard and Crayon. In a pinch, you can also print weights/biases/activations.Be on the lookout for layer activations with a mean much larger than 0. Try Batch Norm or ELUs.Deeplearning4j points out what to expect in histograms of weights and biases:â€œFor weights, these histograms should have an approximately Gaussian (normal) distribution, after some time. For biases, these histograms will generally start at 0, and will usually end up being approximately Gaussian(One exception to this is for LSTM). Keep an eye out for parameters that are diverging to +/- infinity. Keep an eye out for biases that become very large. This can sometimes occur in the output layer for classification if the distribution of classes is very imbalanced.â€Check layer updates, they should have a Gaussian distribution.34. Try a different optimizerYour choice of optimizer shouldnâ€™t prevent your network from training unless you have selected particularly bad hyperparameters. However, the proper optimizer for a task can be helpful in getting the most training in the shortest amount of time. The paper which describes the algorithm you are using should specify the optimizer. If not, I tend to use Adam or plain SGD with momentum.Check this excellent post by Sebastian Ruder to learn more about gradient descent optimizers.35. Exploding / Vanishing gradientsCheck layer updates, as very large values can indicate exploding gradients. Gradient clipping may help.Check layer activations. From Deeplearning4j comes a great guideline: â€œA good standard deviation for the activations is on the order of 0.5 to 2.0. Significantly outside of this range may indicate vanishing or exploding activations.â€36. Increase/Decrease Learning RateA low learning rate will cause your model to converge very slowly.A high learning rate will quickly decrease the loss in the beginning but might have a hard time finding a good solution.Play around with your current learning rate by multiplying it by 0.1 or 10.37. Overcoming NaNsGetting a NaN (Non-a-Number) is a much bigger issue when training RNNs (from what I hear). Some approaches to fix it:Decrease the learning rate, especially if you are getting NaNs in the first 100 iterations.NaNs can arise from division by zero or natural log of zero or negative number.Russell Stewart has great pointers on how to deal with NaNs.Try evaluating your network layer by layer and see where the NaNs appear.Resources:http://cs231n.github.io/neural-networks-3/http://russellsstewart.com/notes/0.htmlhttps://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-classhttps://deeplearning4j.org/visualizationhttps://www.reddit.com/r/MachineLearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phasehttp://book.caltech.edu/bookforum/showthread.php?t=4113https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134https://www.quora.com/How-do-I-debug-an-artificial-neural-network-algorithmOrigin post is here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PyTorch Tutorial (fork from official website)]]></title>
      <url>%2F2017%2F07%2F24%2FPyTorch-Totorial-fork-from-official-website%2F</url>
      <content type="text"><![CDATA[Tensor tutorialAutograd_tutorialNeural_networks_tutorialCIFAR10_tutorial]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning Models Implemented By Tensorflow]]></title>
      <url>%2F2017%2F07%2F24%2FMachine-Learning-Models-Implemented-By-Tensorflow%2F</url>
      <content type="text"><![CDATA[Project: https://github.com/ewanlee/finch/tree/masterThere are these algorithms in the tensorflow-models:Linear regressionLogistic regressionSVMAutoencoder (MLP based and CNN based)NMFGANConditional GANDCGANCNNRNN (for classification and for regression)Highway network (MLP based)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sklearn-based feature engineering]]></title>
      <url>%2F2017%2F07%2F20%2Fsklearn-based-feature-engineering%2F</url>
      <content type="text"><![CDATA[feature engineeringpipeline]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[WGAN-GP [Repost]]]></title>
      <url>%2F2017%2F07%2F18%2FWGAN-GP%2F</url>
      <content type="text"><![CDATA[WGANå­˜åœ¨ç€è®­ç»ƒå›°éš¾ã€æ”¶æ•›é€Ÿåº¦æ…¢ç­‰é—®é¢˜ã€‚WGANçš„ä½œè€…Martin Arjovskyä¸ä¹…åŽå°±åœ¨redditä¸Šè¡¨ç¤ºä»–ä¹Ÿæ„è¯†åˆ°äº†è¿™ä¸ªé—®é¢˜ï¼Œè®¤ä¸ºå…³é”®åœ¨äºŽåŽŸè®¾è®¡ä¸­Lipschitzé™åˆ¶çš„æ–½åŠ æ–¹å¼ä¸å¯¹ï¼šI am now pretty convinced that the problems that happen sometimes in WGANs is due to the specific way of how weight clipping works. Itâ€™s just a terrible way of enforcing a Lipschitz constraint, and better ways are out there. I feel like apologizing for being too lazy and sticking to what could be done in one line of torch code.A simple alternative (less than 5 lines of code) has been found by MontrÃ©al students. It works on quite a few settings (inc 100 layer resnets) with default hyperparameters. Arxiv coming this or next week, stay tuned.å¹¶åœ¨æ–°è®ºæ–‡ä¸­æå‡ºäº†ç›¸åº”çš„æ”¹è¿›æ–¹æ¡ˆï¼šè®ºæ–‡ï¼š[1704.00028] Improved Training of Wasserstein GANsTensorflowå®žçŽ°ï¼šbrianherman/improved_wgan_training (Python 3)igul222/improved_wgan_training (Python 2)é¦–å…ˆå›žé¡¾ä¸€ä¸‹WGANçš„å…³é”®éƒ¨åˆ†â€”â€”Lipschitzé™åˆ¶æ˜¯ä»€ä¹ˆã€‚WGANä¸­ï¼Œåˆ¤åˆ«å™¨Då’Œç”Ÿæˆå™¨Gçš„losså‡½æ•°åˆ†åˆ«æ˜¯$$\begin{align}L(D) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] \\L(G) &amp;= - \mathbb{E}_{x \sim P_r} [D(x)]\end{align}$$å…¬å¼1è¡¨ç¤ºåˆ¤åˆ«å™¨å¸Œæœ›å°½å¯èƒ½æ‹‰é«˜çœŸæ ·æœ¬çš„åˆ†æ•°ï¼Œæ‹‰ä½Žå‡æ ·æœ¬çš„åˆ†æ•°ï¼Œå…¬å¼2è¡¨ç¤ºç”Ÿæˆå™¨å¸Œæœ›å°½å¯èƒ½æ‹‰é«˜å‡æ ·æœ¬çš„åˆ†æ•°ã€‚Lipschitzé™åˆ¶åˆ™ä½“çŽ°ä¸ºï¼Œåœ¨æ•´ä¸ªæ ·æœ¬ç©ºé—´$\mathcal{X}$ä¸Šï¼Œè¦æ±‚åˆ¤åˆ«å™¨å‡½æ•°$D(x)$æ¢¯åº¦çš„$L_p$ normå¤§äºŽä¸€ä¸ªæœ‰é™çš„å¸¸æ•°Kï¼š$$| \nabla_x D(x) |_p \leq K, \forall x \in \mathcal{X}$$ç›´è§‚ä¸Šè§£é‡Šï¼Œå°±æ˜¯å½“è¾“å…¥çš„æ ·æœ¬ç¨å¾®å˜åŒ–åŽï¼Œåˆ¤åˆ«å™¨ç»™å‡ºçš„åˆ†æ•°ä¸èƒ½å‘ç”Ÿå¤ªè¿‡å‰§çƒˆçš„å˜åŒ–ã€‚åœ¨åŽŸæ¥çš„è®ºæ–‡ä¸­ï¼Œè¿™ä¸ªé™åˆ¶å…·ä½“æ˜¯é€šè¿‡weight clippingçš„æ–¹å¼å®žçŽ°çš„ï¼šæ¯å½“æ›´æ–°å®Œä¸€æ¬¡åˆ¤åˆ«å™¨çš„å‚æ•°ä¹‹åŽï¼Œå°±æ£€æŸ¥åˆ¤åˆ«å™¨çš„æ‰€æœ‰å‚æ•°çš„ç»å¯¹å€¼æœ‰æ²¡æœ‰è¶…è¿‡ä¸€ä¸ªé˜ˆå€¼ï¼Œæ¯”å¦‚0.01ï¼Œæœ‰çš„è¯å°±æŠŠè¿™äº›å‚æ•°clipå›ž [-0.01, 0.01] èŒƒå›´å†…ã€‚é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿è¯åˆ¤åˆ«å™¨çš„æ‰€æœ‰å‚æ•°æœ‰ç•Œï¼Œå°±ä¿è¯äº†åˆ¤åˆ«å™¨ä¸èƒ½å¯¹ä¸¤ä¸ªç•¥å¾®ä¸åŒçš„æ ·æœ¬ç»™å‡ºå¤©å·®åœ°åˆ«çš„åˆ†æ•°å€¼ï¼Œä»Žè€Œé—´æŽ¥å®žçŽ°äº†Lipschitzé™åˆ¶ã€‚ç„¶è€Œweight clippingçš„å®žçŽ°æ–¹å¼å­˜åœ¨ä¸¤ä¸ªä¸¥é‡é—®é¢˜ï¼šç¬¬ä¸€ï¼Œå¦‚å…¬å¼1æ‰€è¨€ï¼Œåˆ¤åˆ«å™¨losså¸Œæœ›å°½å¯èƒ½æ‹‰å¤§çœŸå‡æ ·æœ¬çš„åˆ†æ•°å·®ï¼Œç„¶è€Œweight clippingç‹¬ç«‹åœ°é™åˆ¶æ¯ä¸€ä¸ªç½‘ç»œå‚æ•°çš„å–å€¼èŒƒå›´ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬å¯ä»¥æƒ³è±¡ï¼Œæœ€ä¼˜çš„ç­–ç•¥å°±æ˜¯å°½å¯èƒ½è®©æ‰€æœ‰å‚æ•°èµ°æžç«¯ï¼Œè¦ä¹ˆå–æœ€å¤§å€¼ï¼ˆå¦‚0.01ï¼‰è¦ä¹ˆå–æœ€å°å€¼ï¼ˆå¦‚-0.01ï¼‰ï¼ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œä½œè€…ç»Ÿè®¡äº†ç»è¿‡å……åˆ†è®­ç»ƒçš„åˆ¤åˆ«å™¨ä¸­æ‰€æœ‰ç½‘ç»œå‚æ•°çš„æ•°å€¼åˆ†å¸ƒï¼Œå‘çŽ°çœŸçš„é›†ä¸­åœ¨æœ€å¤§å’Œæœ€å°ä¸¤ä¸ªæžç«¯ä¸Šï¼šè¿™æ ·å¸¦æ¥çš„ç»“æžœå°±æ˜¯ï¼Œåˆ¤åˆ«å™¨ä¼šéžå¸¸å€¾å‘äºŽå­¦ä¹ ä¸€ä¸ªç®€å•çš„æ˜ å°„å‡½æ•°ï¼ˆæƒ³æƒ³çœ‹ï¼Œå‡ ä¹Žæ‰€æœ‰å‚æ•°éƒ½æ˜¯æ­£è´Ÿ0.01ï¼Œéƒ½å·²ç»å¯ä»¥ç›´æŽ¥è§†ä¸ºä¸€ä¸ªäºŒå€¼ç¥žç»ç½‘ç»œ**äº†ï¼Œå¤ªç®€å•äº†ï¼‰ã€‚è€Œä½œä¸ºä¸€ä¸ªæ·±å±‚ç¥žç»ç½‘ç»œæ¥è¯´ï¼Œè¿™å®žåœ¨æ˜¯å¯¹è‡ªèº«å¼ºå¤§æ‹Ÿåˆèƒ½åŠ›çš„å·¨å¤§æµªè´¹ï¼åˆ¤åˆ«å™¨æ²¡èƒ½å……åˆ†åˆ©ç”¨è‡ªèº«çš„æ¨¡åž‹èƒ½åŠ›ï¼Œç»è¿‡å®ƒå›žä¼ ç»™ç”Ÿæˆå™¨çš„æ¢¯åº¦ä¹Ÿä¼šè·Ÿç€å˜å·®ã€‚åœ¨æ­£å¼ä»‹ç»gradient penaltyä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆçœ‹çœ‹åœ¨å®ƒçš„æŒ‡å¯¼ä¸‹ï¼ŒåŒæ ·å……åˆ†è®­ç»ƒåˆ¤åˆ«å™¨ä¹‹åŽï¼Œå‚æ•°çš„æ•°å€¼åˆ†å¸ƒå°±åˆç†å¾—å¤šäº†ï¼Œåˆ¤åˆ«å™¨ä¹Ÿèƒ½å¤Ÿå……åˆ†åˆ©ç”¨è‡ªèº«æ¨¡åž‹çš„æ‹Ÿåˆèƒ½åŠ›ï¼šç¬¬äºŒä¸ªé—®é¢˜ï¼Œweight clippingä¼šå¯¼è‡´å¾ˆå®¹æ˜“ä¸€ä¸å°å¿ƒå°±æ¢¯åº¦æ¶ˆå¤±æˆ–è€…æ¢¯åº¦çˆ†ç‚¸ã€‚åŽŸå› æ˜¯åˆ¤åˆ«å™¨æ˜¯ä¸€ä¸ªå¤šå±‚ç½‘ç»œï¼Œå¦‚æžœæˆ‘ä»¬æŠŠclipping thresholdè®¾å¾—ç¨å¾®å°äº†ä¸€ç‚¹ï¼Œæ¯ç»è¿‡ä¸€å±‚ç½‘ç»œï¼Œæ¢¯åº¦å°±å˜å°ä¸€ç‚¹ç‚¹ï¼Œå¤šå±‚ä¹‹åŽå°±ä¼šæŒ‡æ•°è¡°å‡ï¼›åä¹‹ï¼Œå¦‚æžœè®¾å¾—ç¨å¾®å¤§äº†ä¸€ç‚¹ï¼Œæ¯ç»è¿‡ä¸€å±‚ç½‘ç»œï¼Œæ¢¯åº¦å˜å¤§ä¸€ç‚¹ç‚¹ï¼Œå¤šå±‚ä¹‹åŽå°±ä¼šæŒ‡æ•°çˆ†ç‚¸ã€‚åªæœ‰è®¾å¾—ä¸å¤§ä¸å°ï¼Œæ‰èƒ½è®©ç”Ÿæˆå™¨èŽ·å¾—æ°åˆ°å¥½å¤„çš„å›žä¼ æ¢¯åº¦ï¼Œç„¶è€Œåœ¨å®žé™…åº”ç”¨ä¸­è¿™ä¸ªå¹³è¡¡åŒºåŸŸå¯èƒ½å¾ˆç‹­çª„ï¼Œå°±ä¼šç»™è°ƒå‚å·¥ä½œå¸¦æ¥éº»çƒ¦ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œgradient penaltyå°±å¯ä»¥è®©æ¢¯åº¦åœ¨åŽå‘ä¼ æ’­çš„è¿‡ç¨‹ä¸­ä¿æŒå¹³ç¨³ã€‚è®ºæ–‡é€šè¿‡ä¸‹å›¾ä½“çŽ°äº†è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­æ¨ªè½´ä»£è¡¨åˆ¤åˆ«å™¨ä»Žä½Žåˆ°é«˜ç¬¬å‡ å±‚ï¼Œçºµè½´ä»£è¡¨æ¢¯åº¦å›žä¼ åˆ°è¿™ä¸€å±‚ä¹‹åŽçš„å°ºåº¦å¤§å°ï¼ˆæ³¨æ„çºµè½´æ˜¯å¯¹æ•°åˆ»åº¦ï¼‰ï¼Œcæ˜¯clipping thresholdï¼šè¯´äº†è¿™ä¹ˆå¤šï¼Œgradient penaltyåˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿå‰é¢æåˆ°ï¼ŒLipschitzé™åˆ¶æ˜¯è¦æ±‚åˆ¤åˆ«å™¨çš„æ¢¯åº¦ä¸è¶…è¿‡Kï¼Œé‚£æˆ‘ä»¬ä½•ä¸ç›´æŽ¥è®¾ç½®ä¸€ä¸ªé¢å¤–çš„lossé¡¹æ¥ä½“çŽ°è¿™ä¸€ç‚¹å‘¢ï¼Ÿæ¯”å¦‚è¯´ï¼š$$ReLU[| \nabla_x D(x) |_p - K]$$ä¸è¿‡ï¼Œæ—¢ç„¶åˆ¤åˆ«å™¨å¸Œæœ›å°½å¯èƒ½æ‹‰å¤§çœŸå‡æ ·æœ¬çš„åˆ†æ•°å·®è·ï¼Œé‚£è‡ªç„¶æ˜¯å¸Œæœ›æ¢¯åº¦è¶Šå¤§è¶Šå¥½ï¼Œå˜åŒ–å¹…åº¦è¶Šå¤§è¶Šå¥½ï¼Œæ‰€ä»¥åˆ¤åˆ«å™¨åœ¨å……åˆ†è®­ç»ƒä¹‹åŽï¼Œå…¶æ¢¯åº¦normå…¶å®žå°±ä¼šæ˜¯åœ¨Ké™„è¿‘ã€‚çŸ¥é“äº†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠä¸Šé¢çš„lossæ”¹æˆè¦æ±‚æ¢¯åº¦normç¦»Kè¶Šè¿‘è¶Šå¥½ï¼Œæ•ˆæžœæ˜¯ç±»ä¼¼çš„ï¼š$$[| \nabla_x D(x) |_p - K]^2$$ç©¶ç«Ÿæ˜¯å…¬å¼4å¥½è¿˜æ˜¯å…¬å¼5å¥½ï¼Œæˆ‘çœ‹ä¸å‡ºæ¥ï¼Œå¯èƒ½éœ€è¦å®žéªŒéªŒè¯ï¼Œåæ­£è®ºæ–‡ä½œè€…é€‰çš„æ˜¯å…¬å¼5ã€‚æŽ¥ç€æˆ‘ä»¬ç®€å•åœ°æŠŠKå®šä¸º1ï¼Œå†è·ŸWGANåŽŸæ¥çš„åˆ¤åˆ«å™¨lossåŠ æƒåˆå¹¶ï¼Œå°±å¾—åˆ°æ–°çš„åˆ¤åˆ«å™¨lossï¼š$$L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \mathcal{X}} [| \nabla_x D(x) |_p - 1]^2$$è¿™å°±æ˜¯æ‰€è°“çš„gradient penaltyäº†å—ï¼Ÿè¿˜æ²¡å®Œã€‚å…¬å¼6æœ‰ä¸¤ä¸ªé—®é¢˜ï¼Œé¦–å…ˆæ˜¯losså‡½æ•°ä¸­å­˜åœ¨æ¢¯åº¦é¡¹ï¼Œé‚£ä¹ˆä¼˜åŒ–è¿™ä¸ªlosså²‚ä¸æ˜¯è¦ç®—æ¢¯åº¦çš„æ¢¯åº¦ï¼Ÿä¸€äº›è¯»è€…å¯èƒ½å¯¹æ­¤å­˜åœ¨ç–‘æƒ‘ï¼Œä¸è¿‡è¿™å±žäºŽå®žçŽ°ä¸Šçš„é—®é¢˜ï¼Œæ”¾åˆ°åŽé¢è¯´ã€‚å…¶æ¬¡ï¼Œ3ä¸ªlossé¡¹éƒ½æ˜¯æœŸæœ›çš„å½¢å¼ï¼Œè½åˆ°å®žçŽ°ä¸Šè‚¯å®šå¾—å˜æˆé‡‡æ ·çš„å½¢å¼ã€‚å‰é¢ä¸¤ä¸ªæœŸæœ›çš„é‡‡æ ·æˆ‘ä»¬éƒ½ç†Ÿæ‚‰ï¼Œç¬¬ä¸€ä¸ªæœŸæœ›æ˜¯ä»ŽçœŸæ ·æœ¬é›†é‡Œé¢é‡‡ï¼Œç¬¬äºŒä¸ªæœŸæœ›æ˜¯ä»Žç”Ÿæˆå™¨çš„å™ªå£°è¾“å…¥åˆ†å¸ƒé‡‡æ ·åŽï¼Œå†ç”±ç”Ÿæˆå™¨æ˜ å°„åˆ°æ ·æœ¬ç©ºé—´ã€‚å¯æ˜¯ç¬¬ä¸‰ä¸ªåˆ†å¸ƒè¦æ±‚æˆ‘ä»¬åœ¨æ•´ä¸ªæ ·æœ¬ç©ºé—´$\mathcal{X}$ä¸Šé‡‡æ ·ï¼Œè¿™å®Œå…¨ä¸ç§‘å­¦ï¼ç”±äºŽæ‰€è°“çš„ç»´åº¦ç¾éš¾é—®é¢˜ï¼Œå¦‚æžœè¦é€šè¿‡é‡‡æ ·çš„æ–¹å¼åœ¨å›¾ç‰‡æˆ–è‡ªç„¶è¯­è¨€è¿™æ ·çš„é«˜ç»´æ ·æœ¬ç©ºé—´ä¸­ä¼°è®¡æœŸæœ›å€¼ï¼Œæ‰€éœ€æ ·æœ¬é‡æ˜¯æŒ‡æ•°çº§çš„ï¼Œå®žé™…ä¸Šæ²¡æ³•åšåˆ°ã€‚æ‰€ä»¥ï¼Œè®ºæ–‡ä½œè€…å°±éžå¸¸æœºæ™ºåœ°æå‡ºï¼Œæˆ‘ä»¬å…¶å®žæ²¡å¿…è¦åœ¨æ•´ä¸ªæ ·æœ¬ç©ºé—´ä¸Šæ–½åŠ Lipschitzé™åˆ¶ï¼Œåªè¦é‡ç‚¹æŠ“ä½ç”Ÿæˆæ ·æœ¬é›†ä¸­åŒºåŸŸã€çœŸå®žæ ·æœ¬é›†ä¸­åŒºåŸŸä»¥åŠå¤¹åœ¨å®ƒä»¬ä¸­é—´çš„åŒºåŸŸå°±è¡Œäº†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å…ˆéšæœºé‡‡ä¸€å¯¹çœŸå‡æ ·æœ¬ï¼Œè¿˜æœ‰ä¸€ä¸ª0-1çš„éšæœºæ•°ï¼š$$x_r \sim P_r, x_g \sim P_g, \epsilon \sim Uniform[0, 1]$$ç„¶åŽåœ¨$x_r$å’Œ$x_g$çš„è¿žçº¿ä¸Šéšæœºæ’å€¼é‡‡æ ·ï¼š$$\hat{x} = \epsilon x_r + (1 - \epsilon) x_g$$æŠŠæŒ‰ç…§ä¸Šè¿°æµç¨‹é‡‡æ ·å¾—åˆ°çš„$\hat{x}$æ‰€æ»¡è¶³çš„åˆ†å¸ƒè®°ä¸º$P_{\hat{x}}$, å°±å¾—åˆ°æœ€ç»ˆç‰ˆæœ¬çš„åˆ¤åˆ«å™¨lossï¼š$$L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x \sim \hat{x}} [| \nabla_x D(x) |_p - 1]^2$$è¿™å°±æ˜¯æ–°è®ºæ–‡æ‰€é‡‡ç”¨çš„gradient penaltyæ–¹æ³•ï¼Œç›¸åº”çš„æ–°WGANæ¨¡åž‹ç®€ç§°ä¸ºWGAN-GPã€‚æˆ‘ä»¬å¯ä»¥åšä¸€ä¸ªå¯¹æ¯”ï¼šweight clippingæ˜¯å¯¹æ ·æœ¬ç©ºé—´å…¨å±€ç”Ÿæ•ˆï¼Œä½†å› ä¸ºæ˜¯é—´æŽ¥é™åˆ¶åˆ¤åˆ«å™¨çš„æ¢¯åº¦normï¼Œä¼šå¯¼è‡´ä¸€ä¸å°å¿ƒå°±æ¢¯åº¦æ¶ˆå¤±æˆ–è€…æ¢¯åº¦çˆ†ç‚¸ï¼›gradient penaltyåªå¯¹çœŸå‡æ ·æœ¬é›†ä¸­åŒºåŸŸã€åŠå…¶ä¸­é—´çš„è¿‡æ¸¡åœ°å¸¦ç”Ÿæ•ˆï¼Œä½†å› ä¸ºæ˜¯ç›´æŽ¥æŠŠåˆ¤åˆ«å™¨çš„æ¢¯åº¦normé™åˆ¶åœ¨1é™„è¿‘ï¼Œæ‰€ä»¥æ¢¯åº¦å¯æŽ§æ€§éžå¸¸å¼ºï¼Œå®¹æ˜“è°ƒæ•´åˆ°åˆé€‚çš„å°ºåº¦å¤§å°ã€‚è®ºæ–‡è¿˜è®²äº†ä¸€äº›ä½¿ç”¨gradient penaltyæ—¶éœ€è¦æ³¨æ„çš„é…å¥—äº‹é¡¹ï¼Œè¿™é‡Œåªæä¸€ç‚¹ï¼šç”±äºŽæˆ‘ä»¬æ˜¯å¯¹æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹åœ°æ–½åŠ æ¢¯åº¦æƒ©ç½šï¼Œæ‰€ä»¥åˆ¤åˆ«å™¨çš„æ¨¡åž‹æž¶æž„ä¸­ä¸èƒ½ä½¿ç”¨Batch Normalizationï¼Œå› ä¸ºå®ƒä¼šå¼•å…¥åŒä¸ªbatchä¸­ä¸åŒæ ·æœ¬çš„ç›¸äº’ä¾èµ–å…³ç³»ã€‚å¦‚æžœéœ€è¦çš„è¯ï¼Œå¯ä»¥é€‰æ‹©å…¶ä»–normalizationæ–¹æ³•ï¼Œå¦‚Layer Normalizationã€Weight Normalizationå’ŒInstance Normalizationï¼Œè¿™äº›æ–¹æ³•å°±ä¸ä¼šå¼•å…¥æ ·æœ¬ä¹‹é—´çš„ä¾èµ–ã€‚è®ºæ–‡æŽ¨èçš„æ˜¯Layer Normalizationã€‚å®žéªŒè¡¨æ˜Žï¼Œgradient penaltyèƒ½å¤Ÿæ˜¾è‘—æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œè§£å†³äº†åŽŸå§‹WGANæ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ï¼šè™½ç„¶è¿˜æ˜¯æ¯”ä¸è¿‡DCGANï¼Œä½†æ˜¯å› ä¸ºWGANä¸å­˜åœ¨å¹³è¡¡åˆ¤åˆ«å™¨ä¸Žç”Ÿæˆå™¨çš„é—®é¢˜ï¼Œæ‰€ä»¥ä¼šæ¯”DCGANæ›´ç¨³å®šï¼Œè¿˜æ˜¯å¾ˆæœ‰ä¼˜åŠ¿çš„ã€‚ä¸è¿‡ï¼Œä½œè€…å‡­ä»€ä¹ˆèƒ½è¿™ä¹ˆè¯´ï¼Ÿå› ä¸ºä¸‹é¢çš„å®žéªŒä½“çŽ°å‡ºï¼Œåœ¨å„ç§ä¸åŒçš„ç½‘ç»œæž¶æž„ä¸‹ï¼Œå…¶ä»–GANå˜ç§èƒ½ä¸èƒ½è®­ç»ƒå¥½ï¼Œå¯ä»¥è¯´æ˜¯ä¸€ä»¶ç›¸å½“çœ‹äººå“çš„äº‹æƒ…ï¼Œä½†æ˜¯WGAN-GPå…¨éƒ½èƒ½å¤Ÿè®­ç»ƒå¥½ï¼Œå°¤å…¶æ˜¯æœ€ä¸‹é¢ä¸€è¡Œæ‰€å¯¹åº”çš„101å±‚æ®‹å·®ç¥žç»ç½‘ç»œï¼šå‰©ä¸‹çš„å®žéªŒç»“æžœä¸­ï¼Œæ¯”è¾ƒåŽ‰å®³çš„æ˜¯ç¬¬ä¸€æ¬¡æˆåŠŸåšåˆ°äº†â€œçº¯ç²¹çš„â€çš„æ–‡æœ¬GANè®­ç»ƒï¼æˆ‘ä»¬çŸ¥é“åœ¨å›¾åƒä¸Šè®­ç»ƒGANæ˜¯ä¸éœ€è¦é¢å¤–çš„æœ‰ç›‘ç£ä¿¡æ¯çš„ï¼Œä½†æ˜¯ä¹‹å‰å°±æ²¡æœ‰äººèƒ½å¤Ÿåƒè®­ç»ƒå›¾åƒGANä¸€æ ·è®­ç»ƒå¥½ä¸€ä¸ªæ–‡æœ¬GANï¼Œè¦ä¹ˆä¾èµ–äºŽé¢„è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡åž‹ï¼Œè¦ä¹ˆå°±æ˜¯åˆ©ç”¨å·²æœ‰çš„æœ‰ç›‘ç£ground truthæä¾›æŒ‡å¯¼ä¿¡æ¯ã€‚è€ŒçŽ°åœ¨WGAN-GPç»ˆäºŽåœ¨æ— éœ€ä»»ä½•æœ‰ç›‘ç£ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆå‡ºä¸‹å›¾æ‰€ç¤ºçš„è‹±æ–‡å­—ç¬¦åºåˆ—ï¼šå®ƒæ˜¯æ€Žä¹ˆåšåˆ°çš„å‘¢ï¼Ÿæˆ‘è®¤ä¸ºå…³é”®ä¹‹å¤„æ˜¯å¯¹æ ·æœ¬å½¢å¼çš„æ›´æ”¹ã€‚ä»¥å‰æˆ‘ä»¬ä¸€èˆ¬ä¼šæŠŠæ–‡æœ¬è¿™æ ·çš„ç¦»æ•£åºåˆ—æ ·æœ¬è¡¨ç¤ºä¸ºsequence of indexï¼Œä½†æ˜¯å®ƒæŠŠæ–‡æœ¬è¡¨ç¤ºæˆsequence of probability vectorã€‚å¯¹äºŽç”Ÿæˆæ ·æœ¬æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å–ç½‘ç»œsoftmaxå±‚è¾“å‡ºçš„è¯å…¸æ¦‚çŽ‡åˆ†å¸ƒå‘é‡ï¼Œä½œä¸ºåºåˆ—ä¸­æ¯ä¸€ä¸ªä½ç½®çš„å†…å®¹ï¼›è€Œå¯¹äºŽçœŸå®žæ ·æœ¬æ¥è¯´ï¼Œæ¯ä¸ªprobability vectorå®žé™…ä¸Šå°±èœ•åŒ–ä¸ºæˆ‘ä»¬ç†Ÿæ‚‰çš„onehot vectorã€‚ä½†æ˜¯å¦‚æžœæŒ‰ç…§ä¼ ç»ŸGANçš„æ€è·¯æ¥åˆ†æžï¼Œè¿™ä¸æ˜¯ä½œæ­»å—ï¼Ÿä¸€è¾¹æ˜¯hard onehot vectorï¼Œå¦ä¸€è¾¹æ˜¯soft probability vectorï¼Œåˆ¤åˆ«å™¨ä¸€ä¸‹å­å°±èƒ½å¤ŸåŒºåˆ†å®ƒä»¬ï¼Œç”Ÿæˆå™¨è¿˜æ€Žä¹ˆå­¦ä¹ ï¼Ÿæ²¡å…³ç³»ï¼Œå¯¹äºŽWGANæ¥è¯´ï¼ŒçœŸå‡æ ·æœ¬å¥½ä¸å¥½åŒºåˆ†å¹¶ä¸æ˜¯é—®é¢˜ï¼ŒWGANåªæ˜¯æ‹‰è¿‘ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„Wassersteinè·ç¦»ï¼Œå°±ç®—æ˜¯ä¸€è¾¹æ˜¯hard onehotå¦ä¸€è¾¹æ˜¯soft probabilityä¹Ÿå¯ä»¥æ‹‰è¿‘ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¦‚çŽ‡å‘é‡ä¸­çš„æœ‰äº›é¡¹å¯èƒ½ä¼šæ…¢æ…¢å˜æˆ0.8ã€0.9åˆ°æŽ¥è¿‘1ï¼Œæ•´ä¸ªå‘é‡ä¹Ÿä¼šæŽ¥è¿‘onehotï¼Œæœ€åŽæˆ‘ä»¬è¦çœŸæ­£è¾“å‡ºsequence of indexå½¢å¼çš„æ ·æœ¬æ—¶ï¼Œåªéœ€è¦å¯¹è¿™äº›æ¦‚çŽ‡å‘é‡å–argmaxå¾—åˆ°æœ€å¤§æ¦‚çŽ‡çš„indexå°±è¡Œäº†ã€‚æ–°çš„æ ·æœ¬è¡¨ç¤ºå½¢å¼+WGANçš„åˆ†å¸ƒæ‹‰è¿‘èƒ½åŠ›æ˜¯ä¸€ä¸ªâ€œé»„é‡‘ç»„åˆâ€ï¼Œä½†é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰å…¶ä»–å› ç´ å¸®åŠ©è®ºæ–‡ä½œè€…è·‘å‡ºä¸Šå›¾çš„æ•ˆæžœï¼ŒåŒ…æ‹¬ï¼šæ–‡æœ¬ç²’åº¦ä¸ºè‹±æ–‡å­—ç¬¦ï¼Œè€Œéžè‹±æ–‡å•è¯ï¼Œæ‰€ä»¥å­—å…¸å¤§å°æ‰äºŒä¸‰åï¼Œå¤§å¤§å‡å°äº†æœç´¢ç©ºé—´æ–‡æœ¬é•¿åº¦ä¹Ÿæ‰32ç”Ÿæˆå™¨ç”¨çš„ä¸æ˜¯å¸¸è§çš„LSTMæž¶æž„ï¼Œè€Œæ˜¯å¤šå±‚åå·ç§¯ç½‘ç»œï¼Œè¾“å…¥ä¸€ä¸ªé«˜æ–¯å™ªå£°å‘é‡ï¼Œç›´æŽ¥ä¸€æ¬¡æ€§è½¬æ¢å‡ºæ‰€æœ‰32ä¸ªå­—ç¬¦æœ€åŽè¯´å›žgradient penaltyçš„å®žçŽ°é—®é¢˜ã€‚lossä¸­æœ¬èº«åŒ…å«æ¢¯åº¦ï¼Œä¼˜åŒ–losså°±éœ€è¦æ±‚æ¢¯åº¦çš„æ¢¯åº¦ï¼Œè¿™ä¸ªåŠŸèƒ½å¹¶ä¸æ˜¯çŽ°åœ¨æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¡†æž¶çš„æ ‡é…åŠŸèƒ½ï¼Œä¸è¿‡å¥½åœ¨Tensorflowå°±æœ‰æä¾›è¿™ä¸ªæŽ¥å£â€”tf.gradientsã€‚å¼€å¤´é“¾æŽ¥çš„GitHubæºç ä¸­å°±æ˜¯è¿™ä¹ˆå†™çš„ï¼š12# interpolateså°±æ˜¯éšæœºæ’å€¼é‡‡æ ·å¾—åˆ°çš„å›¾åƒï¼Œgradientså°±æ˜¯lossä¸­çš„æ¢¯åº¦æƒ©ç½šé¡¹gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]å®Œæ•´çš„lossæ˜¯è¿™æ ·å®žçŽ°çš„ï¼š12345678910111213141516171819202122232425gen_cost = -tf.reduce_mean(disc_fake)disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)alpha = tf.random_uniform( shape=[BATCH_SIZE,1], minval=0., maxval=1.)differences = fake_data - real_datainterpolates = real_data + (alpha*differences)gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))gradient_penalty = tf.reduce_mean((slopes-1.)**2)disc_cost += LAMBDA*gradient_penaltygen_train_op = tf.train.AdamOptimizer( learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(gen_cost, var_list=gen_params)disc_train_op = tf.train.AdamOptimizer( learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(disc_cost, var_list=disc_params)å¯¹äºŽæˆ‘è¿™æ ·çš„PyTorchå…šå°±éžå¸¸ä¸å¹¸äº†ï¼Œé«˜é˜¶æ¢¯åº¦çš„åŠŸèƒ½è¿˜åœ¨å¼€å‘ï¼Œæ„Ÿå…´è¶£çš„PyTorchå…šå¯ä»¥è®¢é˜…è¿™ä¸ªGitHubçš„pull requestï¼šAutograd refactorï¼Œå¦‚æžœå®ƒè¢«mergedäº†è¯å°±å¯ä»¥åœ¨æœ€æ–°ç‰ˆä¸­ä½¿ç”¨é«˜é˜¶æ¢¯åº¦çš„åŠŸèƒ½å®žçŽ°gradient penaltyäº†ã€‚ä½†æ˜¯é™¤äº†ç­‰å¾…æˆ‘ä»¬å°±æ²¡æœ‰åˆ«çš„åŠžæ³•äº†å—ï¼Ÿå…¶å®žå¯èƒ½æ˜¯æœ‰çš„ï¼Œæˆ‘æƒ³åˆ°äº†ä¸€ç§è¿‘ä¼¼æ–¹æ³•æ¥å®žçŽ°gradient penaltyï¼Œåªéœ€è¦æŠŠå¾®åˆ†æ¢æˆå·®åˆ†ï¼š$$L(D) = - \mathbb{E}_{x \sim P_r} [D(x)] + \mathbb{E}_{x \sim P_g} [D(x)] + \lambda \mathbb{E}_{x_1 \sim \hat{x}, x_2 \sim \hat{x}} [ \frac{|D(x_1) - D(x_2)|}{| x_1 - x_2 |_p} - 1]^2$$ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä»ç„¶æ˜¯åœ¨åˆ†å¸ƒ $P_{\hat{x}}$ ä¸Šéšæœºé‡‡æ ·ï¼Œä½†æ˜¯ä¸€æ¬¡é‡‡ä¸¤ä¸ªï¼Œç„¶åŽè¦æ±‚å®ƒä»¬çš„è¿žçº¿æ–œçŽ‡è¦æŽ¥è¿‘1ï¼Œè¿™æ ·ç†è®ºä¸Šä¹Ÿå¯ä»¥èµ·åˆ°è·Ÿå…¬å¼9ä¸€æ ·çš„æ•ˆæžœï¼Œæˆ‘è‡ªå·±åœ¨MNIST+MLPä¸Šç®€å•éªŒè¯è¿‡æœ‰ä½œç”¨ï¼ŒPyTorchå…šç”šè‡³Tensorflowå…šéƒ½å¯ä»¥å°è¯•ç”¨ä¸€ä¸‹ã€‚ä½œè€…ï¼šéƒ‘åŽæ»¨é“¾æŽ¥ï¼šhttps://www.zhihu.com/question/52602529/answer/158727900æ¥æºï¼šçŸ¥ä¹Žè‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…èŽ·å¾—æŽˆæƒï¼Œéžå•†ä¸šè½¬è½½è¯·æ³¨æ˜Žå‡ºå¤„ã€‚]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Policy Gradient Methods]]></title>
      <url>%2F2017%2F07%2F10%2FPolicy-Gradient-Methods%2F</url>
      <content type="text"><![CDATA[Until now almost all the methods have learned the values of actions and then selected actions based on their estimated action values; their policies would not even exist without the action-value estimates. In this post we consider methods that instead learn a parameterized policy that can select actions without consulting a value function. A value function may still be used to learn the policy parameter, but is not required for action selection. We use the notation $\boldsymbol{\theta} \in \mathbb{R}^d$ for the policyâ€™s parameter vector. Thus we write $\pi(a|s, \boldsymbol{\theta}) = \text{Pr}(A_t=a | S_t=s, \boldsymbol{\theta}_t=\boldsymbol{\theta})$ for the probability that action $a$ is taken at time $t$ given that the agent is in state $s$ at time $t$ with parameter $\boldsymbol{\theta}$. If a method uses a learned value function as well, then the value functionâ€™s weight vector is denoted $\mathbf{w} \in \mathbb{R}^m$, as in $\hat{v}(s, \mathbf{w})$.In this chapter we consider methods for learning the policy parameter based on the gradient of some performance measure $J(\boldsymbol{\theta})$ with respect to the policy parameter. These methods seek to maximize performance, so their updates approximate gradient ascent in $J$ :$$\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t + \alpha \widehat{\nabla J(\boldsymbol{\theta}_t)}.$$All methods that follow this general schema we call policy gradient methods, whether or not they also learn an approximate value function. Methods that learn approximations to both policy and value functions are often called actorâ€“critic methods, where â€˜actorâ€™ is a reference to the learned policy, and â€˜criticâ€™ refers to the learned value function, usually a state-value function.Policy ApproximationThe most preferred actions in each state are given the highest probability of being selected, for example, according to an exponential softmax distribution:$$\pi(a|s, \boldsymbol{\theta}) = \frac{\exp(h(s, a, \boldsymbol{\theta}))}{\sum_b \exp(h(s, b, \boldsymbol{\theta}))}.$$For example, they might be computed by a deep neural network, where $\boldsymbol{\theta}$ is the vector of all the connection weights of the network. Or the preferences could simply be linear in features,$$h(s, a, \boldsymbol{\theta}) = \boldsymbol{\theta}^{\top} \mathbf{x}(s, a).$$The Policy Gradient TheoremWe deï¬ne the performance measure as the value of the start state of the episode. We can simplify the notation without losing any meaningful generality by assuming that every episode starts in some particular (non-random) state s0. Then, in the episodic case we deï¬ne performance as$$J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}}(s_0),$$where $ v_{\pi_{\boldsymbol{\theta}}}$ is the true value function for $\pi_{\boldsymbol{\theta}}$, the policy determined by $\boldsymbol{\theta}$.The policy gradient theorem is that$$\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}),$$where $\mu_{\pi}(s)$ we mentioned in earlier.REINFORCE: Monte Carlo Policy Gradient$$\begin{align}\nabla J(\boldsymbol{\theta}) &amp;= \sum_s \mu_{\pi}(s) \sum_a q_{\pi}(s, a) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}) \\&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a q_{\pi}(S_t, a) \nabla_{\boldsymbol{\theta}} \pi(a | S_t, \boldsymbol{\theta}) \Bigg] \\&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t \sum_a \pi(a|S_t, \boldsymbol{\theta}) q_{\pi}(S_t, a) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})} \Bigg] \\&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t q_{\pi}(S_t, A_t) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \text{(replacing a by the sample } A_t \sim \pi \;) \\&amp;= \mathbb{E}_{\pi} \Bigg[ \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \Bigg] \;\;\; \;\;\; \;\;\; \;\;\; \;\; (\text{because } \mathbb{E}_{\pi}[G_t|S_t,A_t]=q_{\pi}(S_t,A_t)).\end{align}$$So we get$$\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t G_t \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.$$This is shown explicitly in the boxed pseudocode below.Notice that $\nabla \log x = \frac{\nabla x}{x}$.REINFORCE with BaselineThe policy gradient theorem can be generalized to include a comparison of the action value to an arbitrary baseline $b(s)$:$$\nabla J(\boldsymbol{\theta}) = \sum_s \mu_{\pi}(s) \sum_a \big(q_{\pi}(s, a) - b(s)\big) \nabla_{\boldsymbol{\theta}} \pi(a | s, \boldsymbol{\theta}).$$The baseline can be any function, even a random variable, as long as it does not vary with $a$; the equation remains true, because the subtracted quantity is zero:$$\sum_a b(s) \nabla_{\boldsymbol{\theta}} \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} \sum_a \pi(a|s, \boldsymbol{\theta}) = b(s) \nabla_{\boldsymbol{\theta}} 1 = 0 \;\;\;\; \forall s \in \mathcal{S}.$$The update rule that we end up with is a new version of REINFORCE that includes a general baseline:$$\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \big(G_t-b(S_t) \big) \frac{\nabla_{\boldsymbol{\theta}} \pi(a|S_t, \boldsymbol{\theta})}{\pi(a|S_t, \boldsymbol{\theta})}.$$One natural choice for the baseline is an estimate of the state value, $\hat{v}(S_t, \mathbf{w})$, where $\mathbf{w} \in \mathbb{R}^m$ is a weight vector learned by one of the methods presented in previous posts. A complete pseudocode algorithm for REINFROCE with baseline is given in the box (use Monte Carlo method for learning the policy parameter and state-value weights).Actor-Critic MethodsAlthough the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor-critic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating a state from the estimated values of subsequent states), but only as a baseline for the state being updated. Inorder to gain these advantages in the case of policy gradient methods we use actor-critic methods with a true bootstrapping critic.One-step actor-critic methods replace the full return of REINFORCE with the one-step return (and use a learned state-value function as the baseline) as follow:$$\begin{align}\boldsymbol{\theta}_{t+1} &amp;\doteq \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( G_{t:t+1} - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \Big( R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}) - \hat{v}(S_t, \mathbf{w})\Big) \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})} \\&amp;= \boldsymbol{\theta}_{t} + \alpha \gamma^t \delta_t \frac{\nabla_{\boldsymbol{\theta}} \pi(A_t|S_t, \boldsymbol{\theta})}{\pi(A_t|S_t, \boldsymbol{\theta})}.\end{align}$$The natural state-value-function learning method to pair with this is semi-gradient TD(0). Pseudocode for the complete algorithm is given in the box below. Note that it is now a fully online, incremental algorithm, with states, actions, and rewards processed as they occur and then never revisited.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Using Tensorflow and Deep Q-Network/Double DQN to Play Breakout]]></title>
      <url>%2F2017%2F07%2F09%2FUsing-Tensorflow-and-Deep-Q-Network-Double-DQN-to-Play-Breakout%2F</url>
      <content type="text"><![CDATA[In previous blog, we use the Keras to play the FlappyBird. Similarity, we will use another deep learning toolkit Tensorflow to develop the DQN and Double DQN and to play the another game Breakout (Atari 3600).Here, we will use the OpenAI gym toolkit to construct out environment. Detail implementation is as follows:1env = gym.envs.make("Breakout-v0")And then we look some demos:1234567891011121314print("Action space size: &#123;&#125;".format(env.action_space.n))# print(env.get_action_meanings())observation = env.reset()print("Observation space shape: &#123;&#125;".format(observation.shape))plt.figure()plt.imshow(env.render(mode='rgb_array'))[env.step(2) for x in range(1)]plt.figure()plt.imshow(env.render(mode='rgb_array'))env.render(close=True)For deep learning purpose, we need to crop the image to a square image:12# Check out what a cropped image looks likeplt.imshow(observation[34:-16,:,:])Not bad !Ok, now let us to use the Tensorflow to develop the DQN algorithm first.First of all, we need to reference some packages and initialize the environment.1234567891011121314151617181920%matplotlib inlineimport gymfrom gym.wrappers import Monitorimport itertoolsimport numpy as npimport osimport randomimport sysimport tensorflow as tfif "../" not in sys.path: sys.path.append("../")from lib import plottingfrom collections import deque, namedtupleenv = gym.envs.make("Breakout-v0")# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actionsVALID_ACTIONS = [0, 1, 2, 3]As mentioned above, we need to crop the image and preprocess the input before feed the raw image into the algorithm. So we define a StateProcessor class to do this.123456789101112131415161718192021222324class StateProcessor(): """ Processes a raw Atari images. Resizes it and converts it to grayscale. """ def __init__(self): # Build the Tensorflow graph with tf.variable_scope("state_processor"): self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8) self.output = tf.image.rgb_to_grayscale(self.input_state) self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160) self.output = tf.image.resize_images( self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) self.output = tf.squeeze(self.output) def process(self, sess, state): """ Args: sess: A Tensorflow session object state: A [210, 160, 3] Atari RGB State Returns: A processed [84, 84, 1] state representing grayscale values. """ return sess.run(self.output, &#123; self.input_state: state &#125;)We first convert the image to gray image and then resize it to 84 by 84 (the size DQN paper used). Then, we construct the neural network to estimate the value function. The structure of the network as the same as the DQN paper.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102class Estimator(): """Q-Value Estimator neural network. This network is used for both the Q-Network and the Target Network. """ def __init__(self, scope="estimator", summaries_dir=None): self.scope = scope # Writes Tensorboard summaries to disk self.summary_writer = None with tf.variable_scope(scope): # Build the graph self._build_model() if summaries_dir: summary_dir = os.path.join(summaries_dir, "summaries_&#123;&#125;".format(scope)) if not os.path.exists(summary_dir): os.makedirs(summary_dir) self.summary_writer = tf.summary.FileWriter(summary_dir) def _build_model(self): """ Builds the Tensorflow graph. """ # Placeholders for our input # Our input are 4 RGB frames of shape 160, 160 each self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name="X") # The TD target value self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name="y") # Integer id of which action was selected self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name="actions") X = tf.to_float(self.X_pl) / 255.0 batch_size = tf.shape(self.X_pl)[0] # Three convolutional layers conv1 = tf.contrib.layers.conv2d( X, 32, 8, 4, activation_fn=tf.nn.relu) conv2 = tf.contrib.layers.conv2d( conv1, 64, 4, 2, activation_fn=tf.nn.relu) conv3 = tf.contrib.layers.conv2d( conv2, 64, 3, 1, activation_fn=tf.nn.relu) # Fully connected layers flattened = tf.contrib.layers.flatten(conv3) fc1 = tf.contrib.layers.fully_connected(flattened, 512) self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS)) # Get the predictions for the chosen actions only gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices) # Calcualte the loss self.losses = tf.squared_difference(self.y_pl, self.action_predictions) self.loss = tf.reduce_mean(self.losses) # Optimizer Parameters from original paper self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6) self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step()) # Summaries for Tensorboard self.summaries = tf.summary.merge([ tf.summary.scalar("loss", self.loss), tf.summary.histogram("loss_hist", self.losses), tf.summary.histogram("q_values_hist", self.predictions), tf.summary.scalar("max_q_value", tf.reduce_max(self.predictions)) ]) def predict(self, sess, s): """ Predicts action values. Args: sess: Tensorflow session s: State input of shape [batch_size, 4, 160, 160, 3] Returns: Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated action values. """ return sess.run(self.predictions, &#123; self.X_pl: s &#125;) def update(self, sess, s, a, y): """ Updates the estimator towards the given targets. Args: sess: Tensorflow session object s: State input of shape [batch_size, 4, 160, 160, 3] a: Chosen actions of shape [batch_size] y: Targets of shape [batch_size] Returns: The calculated loss on the batch. """ feed_dict = &#123; self.X_pl: s, self.y_pl: y, self.actions_pl: a &#125; summaries, global_step, _, loss = sess.run( [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss], feed_dict) if self.summary_writer: self.summary_writer.add_summary(summaries, global_step) return lossAs mentioned in DQN paper, there are two network that share the same parameters in DQN algorithm. We need to copy the parameters to the target network on each $t$ steps.1234567891011121314151617181920def copy_model_parameters(sess, estimator1, estimator2): """ Copies the model parameters of one estimator to another. Args: sess: Tensorflow session instance estimator1: Estimator to copy the paramters from estimator2: Estimator to copy the parameters to """ e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)] e1_params = sorted(e1_params, key=lambda v: v.name) e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)] e2_params = sorted(e2_params, key=lambda v: v.name) update_ops = [] for e1_v, e2_v in zip(e1_params, e2_params): op = e2_v.assign(e1_v) update_ops.append(op) sess.run(update_ops)We also need a policy to take an action.1234567891011121314151617181920def make_epsilon_greedy_policy(estimator, nA): """ Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon. Args: estimator: An estimator that returns q values for a given state nA: Number of actions in the environment. Returns: A function that takes the (sess, observation, epsilon) as an argument and returns the probabilities for each action in the form of a numpy array of length nA. """ def policy_fn(sess, observation, epsilon): A = np.ones(nA, dtype=float) * epsilon / nA q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0] best_action = np.argmax(q_values) A[best_action] += (1.0 - epsilon) return A return policy_fnNow let us to develop the DQN algorithm (we skip the details here because we explained it earlier).123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187def deep_q_learning(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size=500000, replay_memory_init_size=50000, update_target_estimator_every=10000, discount_factor=0.99, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay_steps=500000, batch_size=32, record_video_every=50): """ Q-Learning algorithm for fff-policy TD control using Function Approximation. Finds the optimal greedy policy while following an epsilon-greedy policy. Args: sess: Tensorflow Session object env: OpenAI environment q_estimator: Estimator object used for the q values target_estimator: Estimator object used for the targets state_processor: A StateProcessor object num_episodes: Number of episodes to run for experiment_dir: Directory to save Tensorflow summaries in replay_memory_size: Size of the replay memory replay_memory_init_size: Number of random experiences to sampel when initializing the reply memory. update_target_estimator_every: Copy parameters from the Q estimator to the target estimator every N steps discount_factor: Lambda time discount factor epsilon_start: Chance to sample a random action when taking an action. Epsilon is decayed over time and this is the start value epsilon_end: The final minimum value of epsilon after decaying is done epsilon_decay_steps: Number of steps to decay epsilon over batch_size: Size of batches to sample from the replay memory record_video_every: Record a video every N episodes Returns: An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards. """ Transition = namedtuple("Transition", ["state", "action", "reward", "next_state", "done"]) # The replay memory replay_memory = [] # Keeps track of useful statistics stats = plotting.EpisodeStats( episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes)) # Create directories for checkpoints and summaries checkpoint_dir = os.path.join(experiment_dir, "checkpoints") checkpoint_path = os.path.join(checkpoint_dir, "model") monitor_path = os.path.join(experiment_dir, "monitor") if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir) if not os.path.exists(monitor_path): os.makedirs(monitor_path) saver = tf.train.Saver() # Load a previous checkpoint if we find one latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir) if latest_checkpoint: print("Loading model checkpoint &#123;&#125;...\n".format(latest_checkpoint)) saver.restore(sess, latest_checkpoint) # Get the current time step total_t = sess.run(tf.contrib.framework.get_global_step()) # The epsilon decay schedule epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps) # The policy we're following policy = make_epsilon_greedy_policy( q_estimator, len(VALID_ACTIONS)) # Populate the replay memory with initial experience print("Populating replay memory...") state = env.reset() state = state_processor.process(sess, state) state = np.stack([state] * 4, axis=2) for i in range(replay_memory_init_size): action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)]) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) next_state, reward, done, _ = env.step(VALID_ACTIONS[action]) next_state = state_processor.process(sess, next_state) next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2) replay_memory.append(Transition(state, action, reward, next_state, done)) if done: state = env.reset() state = state_processor.process(sess, state) state = np.stack([state] * 4, axis=2) else: state = next_state # Record videos # Add env Monitor wrapper env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True) for i_episode in range(num_episodes): # Save the current checkpoint saver.save(tf.get_default_session(), checkpoint_path) # Reset the environment state = env.reset() state = state_processor.process(sess, state) state = np.stack([state] * 4, axis=2) loss = None # One step in the environment for t in itertools.count(): # Epsilon for this time step epsilon = epsilons[min(total_t, epsilon_decay_steps-1)] # Add epsilon to Tensorboard episode_summary = tf.Summary() episode_summary.value.add(simple_value=epsilon, tag="epsilon") q_estimator.summary_writer.add_summary(episode_summary, total_t) # Maybe update the target estimator if total_t % update_target_estimator_every == 0: copy_model_parameters(sess, q_estimator, target_estimator) print("\nCopied model parameters to target network.") # Print out which step we're on, useful for debugging. print("\rStep &#123;&#125; (&#123;&#125;) @ Episode &#123;&#125;/&#123;&#125;, loss: &#123;&#125;".format( t, total_t, i_episode + 1, num_episodes, loss), end="") sys.stdout.flush() # Take a step action_probs = policy(sess, state, epsilon) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) next_state, reward, done, _ = env.step(VALID_ACTIONS[action]) next_state = state_processor.process(sess, next_state) next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2) # If our replay memory is full, pop the first element if len(replay_memory) == replay_memory_size: replay_memory.pop(0) # Save transition to replay memory replay_memory.append(Transition(state, action, reward, next_state, done)) # Update statistics stats.episode_rewards[i_episode] += reward stats.episode_lengths[i_episode] = t # Sample a minibatch from the replay memory samples = random.sample(replay_memory, batch_size) states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples)) # Calculate q values and targets q_values_next = target_estimator.predict(sess, next_states_batch) targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1) # Perform gradient descent update states_batch = np.array(states_batch) loss = q_estimator.update(sess, states_batch, action_batch, targets_batch) if done: break state = next_state total_t += 1 # Add summaries to tensorboard episode_summary = tf.Summary() episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name="episode_reward", tag="episode_reward") episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name="episode_length", tag="episode_length") q_estimator.summary_writer.add_summary(episode_summary, total_t) q_estimator.summary_writer.flush() yield total_t, plotting.EpisodeStats( episode_lengths=stats.episode_lengths[:i_episode+1], episode_rewards=stats.episode_rewards[:i_episode+1]) return statsFinally, run it.1234567891011121314151617181920212223242526272829303132333435tf.reset_default_graph()# Where we save our checkpoints and graphsexperiment_dir = os.path.abspath("./experiments/&#123;&#125;".format(env.spec.id))# Create a glboal step variableglobal_step = tf.Variable(0, name='global_step', trainable=False) # Create estimatorsq_estimator = Estimator(scope="q", summaries_dir=experiment_dir)target_estimator = Estimator(scope="target_q")# State processorstate_processor = StateProcessor()# Run it!with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for t, stats in deep_q_learning(sess, env, q_estimator=q_estimator, target_estimator=target_estimator, state_processor=state_processor, experiment_dir=experiment_dir, num_episodes=10000, replay_memory_size=500000, replay_memory_init_size=50000, update_target_estimator_every=10000, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay_steps=500000, discount_factor=0.99, batch_size=32): print("\nEpisode Reward: &#123;&#125;".format(stats.episode_rewards[-1]))Next, we will develop the Double-DQN algorithm. This algorithm only has a little changes.In DQN q_learning method,123# Calculate q values and targetsq_values_next = target_estimator.predict(sess, next_states_batch)targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)we just change these codes to,123456# Calculate q values and targets# This is where Double Q-Learning comes in!q_values_next = q_estimator.predict(sess, next_states_batch)best_actions = np.argmax(q_values_next, axis=1)q_values_next_target = target_estimator.predict(sess, next_states_batch)targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * q_values_next_target[np.arange(batch_size), best_actions]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Summary of Papers]]></title>
      <url>%2F2017%2F07%2F08%2FSummary-of-the-papers%2F</url>
      <content type="text"><![CDATA[CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep LearningDataSetChestX-ray14 dataset Wang et al. 2017ArchDenseNet (121 layers)Batch NormalizationThe weights of the network are randomly initializedTrained end-to-end using Adam with standard pa- rameters (Î²1 = 0.9 and Î²2 = 0.999)Batch size = 16Oversample the minority (positive) class Buda et al., 2017Use an initial learning rate of 0.01 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss.Model InterpretationTo interpret the network predictions, we also produce heatmaps to visualize the areas of the image most in- dicative of the disease using class activation mappings (CAMs) Zhou et al., 2016ResultsRelated WordDiabetic retinopathy detection Gulshan et al., 2016Skin cancer classification Esteva et al., 2017Arrhythmia detection Rajpurkar et al., 2017Hemorrhage identificatio Grewal et al., 2017Pulmonary tuberculosis classification Lakhani &amp; Sun- daram, 2017Lung nodule detection Huang et al., 2017, Islam et al. 2017studied the performance of various convolutional architectures on different ab- normalities using the publicly available OpenI dataset Demner-Fushman et al., 2015Yao et al. 2017 exploited statistical dependencies between la- bels in order make more accurate predictions, outper- forming Wang et al. 2017 on 13 of 14 classes.###Maximum Entropy Deep Inverse Reinforcement Learning [2016]Maximum Entropy Inverse Reinforcement Learning Brian [2008][Repost]this is a summary of Ziebart et alâ€™s 2008 paper: Maximum Entropy Inverse Reinforcement Learning **. I found this is a good way for me to distill the essence of the paper. Since the Maxent algorithm is mostly cited by the later papers in IRL/imitation learning, I would like to look into details of this algorithm. Code is available at github. Hope this post can also help others who are interested in IRL.MotivationsThe paper points out the limitations of the previous algorithms including LPIRL (Ng &amp; Russell 2000), structured maximum margin prediction (MMP, Ratliff, Bagnell &amp; Zinkevich 2006, Apprenticeship Learning vis IRL (Abbeel &amp; Ng 2004) about feature counts and IRLBoth IRL and the matching of feature counts are ambiguous.Each policy can be optimal for many reward functions.many policies lead to the same feature counts.The ambiguity of suboptimality is unresolved.NotationsAn MDP is a tuple $(S,A,P_{sa},\gamma,R)$$S$ is a finite set of $N$ states.$A={a_1,..,a_k}$ is a set of $k$ actions.$P_{sa}(s^{\prime})$ is the state transition probability of landing at state $s^{\prime}$: $P(s,a,s^{\prime})$ upon taking the action aa at state $s$.$\gamma \in [0,1)$ is the discount factor.$R: S \rightarrow \mathbf{R}$ is the reward function.Maxent$\zeta:\{(s, a)\}$ is a trajectory.$\mathbf{f_s} \in \mathbf{R}^k$ is the feature vector of the state $s$.$\theta \in \mathbf{R}^k$ reward function parameters.$P(\zeta)$ probability of the trajectory $\zeta$ to occur$P(s)$ the probatility of visiting state $s$ (state visitation frequency), $P(\zeta) = \prod_{s\in\zeta} P(s)$.AssumptionsThe reward of a trajectory is expressed as a linearly combination with feature counts$$R(\zeta) = \theta ^T \mathbf{f}_{\zeta} = \sum_{s\in \zeta} \theta ^T \mathbf{f}_s$$Principle of maximum entropy (Jaynes 1957): probability of a trajectory demonstrated by the expert is exponentially higher for higher rewards than lower rewards,$$P(\zeta) \propto e^{R(\zeta)}$$AlgorithmThe Maxent algorithm learns from demonstrated expert trajectories with the objective being maximizing the likelihood of the demonstrated trajectories,$$\begin{align}\theta^{\star} &amp;= \text{argmax}_{\theta} L(\theta) \\&amp;= \text{argmax}_{\theta} \frac{1}{M}\text{log} P(\{\zeta\} | \theta)\\&amp;= \text{argmax}_{\theta} \frac{1}{M}\text{log} \prod_{\zeta} P(\zeta|\theta)\\&amp;= \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} \text{log} P(\zeta|\theta)\\&amp;= \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} \text{log} \frac{e^{R(\zeta)}}{Z}\\&amp;= \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} R(\zeta) - \text{log} Z\\\end{align}$$Where $M$ is the number of trajectories, $Z$ is the normalization term,$$Z = \sum_{\zeta} e^{R(\zeta)}$$Then,$$\theta^{\star} = \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} R(\zeta) - \text{log} \sum_{\zeta} e^{R(\zeta)}\\\theta^{\star} = \text{argmax}_{\theta} \frac{1}{M}\sum_{\zeta} \theta ^T \mathbf{f}_{\zeta} - \text{log} \sum_{\zeta} e^{\theta ^T \mathbf{f}_{\zeta}}\\$$And the objective is convex! (with the second term being log-sum-exp). We go ahead to differentiate the objective to find the gradients:$$\begin{align}\nabla_{\theta} L &amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \frac{1}{\sum_\zeta e^{R(\zeta)}} \sum_{\zeta} (e^{R(\zeta)\frac{d R(\zeta)}{d\theta}})\\&amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \frac{1}{\sum_\zeta e^{R(\zeta)}} \sum_{\zeta} (e^{R(\zeta)}\frac{d R(\zeta)}{d\theta})\\&amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \sum_{\zeta}\frac{e^{R(\zeta)}}{\sum_\zeta e^{R(\zeta)}} \mathbf{f}_{\zeta}\\&amp;= \frac{1}{M}\sum_\zeta \mathbf{f}_{\zeta} - \sum_{\zeta} P(\zeta | \theta) \mathbf{f}_{\zeta}\\\end{align}$$Since the trajectories {Î¶}{Î¶} are consist of states,$$\nabla_{\theta} L = \frac{1}{M}\sum_s \mathbf{f}_{s} - \sum_{s} P(s | \theta) \mathbf{f}_{s}$$Where $\mathbf{f}_s$ is the feature vector for the state $s$. And$$P(s|\theta)$$is the state visitation frequency for state $s$.So far the main body of the algorithm is described. The only thing left is to compute the state visitation frequency (SVF) vector. To do so, we can use the following dynamic programming algorithm (for convienience we use $P(s)$ to denote SVF on state $s$).We use Î¼t(s)Î¼t(s) to denote the prob of visiting $s$ at $t$ (obviously,$$P(s) = \sum_t \mu_t(s)$$solve the MDP using value iteration with the intermediate recovered rewards to get current optimal policy $\{\pi(a,s)\}$.compute $\mu_1(s)$ using sampled trajectoriesusing DP to solve for the rest given optimal policy $\{\pi(a,s)\}$ and the transition dynamics $\{P_{sa}(sâ€™)\}$For $t = 1,..,T$$$\mu_{t+1} (s) = \sum_{a}\sum_{sâ€™} \mu_{t}(sâ€™)\pi(a,sâ€™)P_{sa}(sâ€™)$$And finally.$$P(s) = \sum_t \mu_t(s)$$One key things to note is that, the algorithm solves MDP in each iteration of training.If the transition dynamics $\{P_{sa}(sâ€™)\}$ is unknown, we can actually using Monte Carlo to estimate the SVF with the trajectories. This is much more easier, so the details are omitted. Plugging the SVF back to the gradients, we can use iterative gradient descent to solve for the parameters $\theta$.SummaryAs a final summary of the algorithm, here is the slide from UC Berkeleyâ€™s CS 294, Deep Reinforcement Learning course,Strengths and LimitationsStrengthsscales to neural network costs (overcome the drawbacks of linear costs)efficient enough for real robotsLimitationsrequires repeatedly solving the MDPassumes known dynamicsReferencesZiebart et alâ€™s 2008 paper: Maximum Entropy Inverse Reinforcement Learning **UCBâ€™s CS 294 DRL course, lecture on IRLCode Details123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384def compute_state_visition_freq(P_a, gamma, trajs, policy, deterministic=True): """compute the expected states visition frequency p(s| theta, T) using dynamic programming inputs: P_a NxNxN_ACTIONS matrix - transition dynamics gamma float - discount factor trajs list of list of Steps - collected from expert policy Nx1 vector (or NxN_ACTIONS if deterministic=False) - policy returns: p Nx1 vector - state visitation frequencies """ N_STATES, _, N_ACTIONS = np.shape(P_a) T = len(trajs[0]) # mu[s, t] is the prob of visiting state s at time t mu = np.zeros([N_STATES, T]) for traj in trajs: mu[traj[0].cur_state, 0] += 1 mu[:,0] = mu[:,0]/len(trajs) for s in range(N_STATES): for t in range(T-1): if deterministic: mu[s, t+1] = sum([mu[pre_s, t]*P_a[pre_s, s, int(policy[pre_s])] for pre_s in range(N_STATES)]) else: mu[s, t+1] = sum([sum([mu[pre_s, t]*P_a[pre_s, s, a1]*policy[pre_s, a1] for a1 in range(N_ACTIONS)]) for pre_s in range(N_STATES)]) p = np.sum(mu, 1) return pdef maxent_irl(feat_map, P_a, gamma, trajs, lr, n_iters): """ Maximum Entropy Inverse Reinforcement Learning (Maxent IRL) inputs: feat_map NxD matrix - the features for each state P_a NxNxN_ACTIONS matrix - P_a[s0, s1, a] is the transition prob of landing at state s1 when taking action a at state s0 gamma float - RL discount factor trajs a list of demonstrations lr float - learning rate n_iters int - number of optimization steps returns rewards Nx1 vector - recoverred state rewards """ N_STATES, _, N_ACTIONS = np.shape(P_a) # init parameters theta = np.random.uniform(size=(feat_map.shape[1],)) # calc feature expectations feat_exp = np.zeros([feat_map.shape[1]]) for episode in trajs: for step in episode: feat_exp += feat_map[step.cur_state,:] feat_exp = feat_exp/len(trajs) # training for iteration in range(n_iters): if iteration % (n_iters/20) == 0: print 'iteration: &#123;&#125;/&#123;&#125;'.format(iteration, n_iters) # compute reward function rewards = np.dot(feat_map, theta) # compute policy _, policy = value_iteration.value_iteration(P_a, rewards, gamma, error=0.01, deterministic=False) # compute state visition frequences svf = compute_state_visition_freq(P_a, gamma, trajs, policy, deterministic=False) # compute gradients grad = feat_exp - feat_map.T.dot(svf) # update params theta += lr * grad rewards = np.dot(feat_map, theta) # return sigmoid(normalize(rewards)) return normalize(rewards)Apprenticeship Learning via Inverse Reinforcement Learning [ICML 2004]PreliminariesThe reward function can be expressed as a linear combination of known features.The value of a policy $\pi$:$$\begin{align}E_{s_0 \sim D}[V^{\pi}(s_0)] &amp;= E[\sum_{t=0}^{\infty}\gamma^t R(s_t) | \pi] \\&amp;= E[\sum_{t=0}^{\infty}\gamma^t w \cdot \phi(s_t)| \pi] \\&amp;= w \cdot E[\sum_{t=0}^{\infty}\gamma^t \phi(s_t)| \pi],\end{align}$$where vector of features $\phi: S \rightarrow [0, 1]^k$ or $\phi: S \times A \ \rightarrow [0, 1]^k$, $w \in \mathbb{R}^k$. In order to ensure that the rewards are bounded by 1, we also assume $| w |_1 \leq 1$.The feature exceptions:$$\mu(\pi) = E[\sum_{t=0}^{\infty}\gamma^t \phi(s_t)| \pi] \in \mathbb{R}^k.$$So the value of a policy may be rewritten to$$E_{s_0 \sim D}[V^{\pi}(s_0)] = w \cdot \mu(\pi)$$We assume the expert policy is $\pi_E$ and we need to estimate the expertâ€™s feature expectations $\mu_E = \mu(\pi_E)$. Specifically, given a set of $m$ trajectories $\{s_0^{(i)}, s_1^{(i)}, \cdots, \}_{i=1}^{m}$ generated by the expert, we denote the empirical estimate for $\mu_E$ by$$\hat{\mu}_E = \frac{1}{m}\sum_{i=1}^m\sum_{t=0}^m \gamma^t \phi(s_t^{(i)}).$$Furthermore, we could construct a new policy by linear combination some other policies. More specifically, we have$$\mu(\pi_3) = \lambda \mu(\pi_1) + (1 - \lambda) \mu(\pi_2) .$$Note that the randomization step selecting between $\pi_1$ and $\pi_2$ occurs only once at the start of a trajectory, and not on every step taken in the MDP.AlgorithmWe want to find a policy whose performance is close to that of the expert, that is, for any $w \in \mathbb{R}^k (|w|_1 \leq 1)$,$$|w^T\mu(\tilde{\pi}) - w^T\mu_E| \leq |w|_2 |w^T\mu(\tilde{\pi}) - w^T\mu_E|_2 \leq 1 \cdot \epsilon \leq \epsilon.$$So the problem is reduced to finding a policy $\tilde{\pi}$ that induces feature expectations $\mu(\tilde{\pi})$ close to $\mu_E$. We find such a policy as follows:Randomly pick some policy $\pi^{(0)}$. compute (or approximate via Monte Carlo) $\mu^{(0)} = \mu(\pi^{(0)})$, and set $i=1$.Solve optimization problem (Solver as same as SVM) (*):If $t^{(i)} \leq \epsilon$, then terminal.Using the RL algorithm, compute the optimal policy $\pi^{(i)}$ for the MDP using reward $R = (w^{(i)})^T\phi$.Compute (or estimate) $\mu^{(i)} = \mu(\pi^{(i)})$.Set $i = i + 1$, and go back to step $2$.Finally, we can find the point closest to $\mu_E$ in the convex closure of $\mu^{(0)}, \cdots, \mu^{(n)}$ by solving the following QP:$$\min| \mu_E - \mu |_2, \; \text{s.t.} \; \mu = \sum_i \lambda_i \mu^{(i)}, \lambda_i \geq 0, \sum_i \lambda_i = 1.$$(*)$$\begin{align}\max_{t, w} &amp;\;t \\s.t. &amp;\; w^T\mu_E \geq w^T\mu^{(j)} + t, \; j = 0, \cdots, i-1 \\&amp;\; |w |_2 \leq 1.\end{align}$$Note that, step 2 could replace by a simpler way that no QP solver is needed:Set $\bar{\mu}^{(i-1)} = \bar{\mu}^{(i-2)} + \frac{(\mu^{(i-1)} - \bar{\mu}^{(i-2)})^T(\mu_E - \bar{\mu}^{(i-2)})}{(\mu^{(i-1)} - \bar{\mu}^{(i-2)})^T(\mu^{(i-1)} - \bar{\mu}^{(i-2)})} (\mu^{(i-1)} - \bar{\mu}^{(i-2)})^T$This computes the orthogonal projection of $\mu_E$ onto the line through $\bar{\mu}^{(i-2)}$ and $\mu^{(i-1)}$.Set $w^{(i)} = \mu_E - \bar{\mu}^{(i-1)}$Set $t^{(i)} = | \mu_E - \bar{\mu}^{(i-1)} |_2$In the first iteration, we also set $w^{(1)} = \mu_E - \mu^{(0)}$ and $\bar{\mu}^{(0)} = \mu^{(0)}$.Deep Reinforcement Learning with Double Q-learningThe idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having introduce additional networks. We therefore propose to evaluate the greedy policy according to the online network, but using the target network to estimate its value. In reference to both Double Q-learning and DQN, we refer to the resulting algorithm as Double DQN. Its update is the same as for DQN, but replacing the target $Y_t^{DQN}$ with$$Y_t^{\text{DoubleDQN}} \equiv R_{t+1} + \gamma Q(S_{t+1}, {\arg \max}_a Q(S_{t+1},a; \boldsymbol{\theta}_t),\boldsymbol{\theta}_t^{\mathbf{-}}).$$In comparison to Double Q-learning$$Y_t^{\text{DoubleDQN}} \equiv R_{t+1} + \gamma Q(S_{t+1}, {\arg \max}_a Q(S_{t+1},a; \boldsymbol{\theta}_t),\boldsymbol{\theta}_t^{\boldsymbol{\prime}}),$$the weights of the second network $\boldsymbol{\theta_t^{\prime}}$ are replaced with the weights of the target network $\boldsymbol{\theta_t^{-}}$ for the evaluation of the current greedy policy. The update to the target stays unchanged from DQN, and remains a periodic copy of the online network.Prioritized Experience Replaywhere $p_i &gt; 0$ is the priority of transition $i$. The exponent $\alpha$ determines how much prioritization is used, with $\alpha=0$ corresponding to the uniform case.The first we consider is the direct, proportional prioritization where $p_i = |\delta_i| + \epsilon$, where $\delta_i$ is the TD-error of transition $i$ and $\epsilon$ is a small positive constant that prevent the edge-case of transitions not being revisited once their error is zero. The second variant is an indirect, rand-based prioritization where $p_i = \frac{1}{\text{rank}(i)}$, where $\text{rank}_i$ is the rank of transition $i$ when the replay memory is sorted according to $|\delta_i|$Dueling Network Architectures for Deep Reinforcement LearningLet us consider the dueling network shown in above, where we make one stream of fully-connected layers output a scalar $V(s;\theta,\beta)$, and the other stream output an $\mathcal{A}$-dimensional vector $A(s, a; \theta, \alpha)$. Here, $\theta$ denotes the parameters of the convolutional layers, while $\alpha$ and $\beta$ are the parameters of the two streams of fully-connected layers. Using the definition of advantage, we might be tempted to construct the aggregating module as follows:$$Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + A(s, a; \theta, \alpha),$$Note that this expression applies to all $(s, a)$ instances; that is, to express equation above in matrix form we need to replicate the scalar, $V(s;\theta,\beta)$, $|\mathcal{A}|$ times.Equation above is unidentifiable in the sense that given $Q$ we cannot recover V and A uniquely. To see this, add a constant to $V(s;\theta,\beta)$ and subtract the same constant from $A(s, a; \theta, \alpha)$. This constant cancels out resulting in the same $Q$ value. This lack of identifiability is mirrored by poor practical performance when this equation is used directly.To address this issue of identifiability, we can replace the equation above to this one:$$Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + \Big( A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a^{\prime}} A(s, a^{\prime}; \theta, \alpha) \Big).$$Learning Tetris Using the Noisy Cross-Entropy Methodthe paper works on solving Tetris with modified cross entropy method. original CE method in reinforcement learning usually results in early convergence.Cross entropy method in reinforcement learningfirst we start with a random uniform distribution F_0drawn from F_0 and get N samples Î¸_0, Î¸_1, â€¦choose the top K samples that get the highest scores and use these selected sample(Î¸_0, Î¸_1, â€¦) update distribution and get F_1keypointsadd noise to the cross-entropy method to prevent early convergeif we decrease the noise, which is only depend on time steps, the performance can even be better.noise ðŸ‘‰ prevent early convergenotecan view the noise apply to std as ensure enough explorationDoubly Robust Off-policy Value Evaluation for Reinforcement LearningWe study the off-policy value evaluation problem, where one aims to estimate the value of a policy with data collected by another policy.There are roughly two classes of approaches to off-policy value evaluation. The first is to fit an MDP model from data via regression, and evaluate the policy against the model; The second class of approaches are based on the idea of importance sampling (IS), which corrects the mismatch between the distributions induces by the target policy ang by the behavior policy.Importance Sampling EstimatorsThe basic IS EstimatorThe IS estimator provides an unbiased estimate of $\pi_1$â€™s value by averaging the following function of each trajectory $(s_1, a_1, r_1, \cdots, s_{H+1})$ in the data: define the per-step importance ratio as $\rho_t := \pi_1(a_t|s_t)/\pi_0(a_t|s_t)$, and the cumulative importance ratio $\rho_{1:t}:=\prod_{t^{\prime}=1}^{t}\rho_{t^{\prime}}$; the basic (trajectory-wise) IS estimator, and an improved step-wise version are given as follows:$$V_{\text{IS}} := \rho_{1:H} \cdot (\sum_{t=1}^{H} \gamma^{t-1} r_t),$$$$V_{\text{step-IS}} := \sum_{t=1}^{H} \gamma^{t-1}\rho_{1:t}r_t.$$Given a dataset $D$, the IS estimator is simply the average estimate over the trajectories, namely $\frac{1}{|D|}\sum_{i=1}V_{\text{IS}}^{(i)}$, where $|D|$ is the number of trajectories in $D$ and $V_{\text{IS}}^{(i)}$ is IS applied to the $i$-th trajectory.The weighted importance sampling (WIS)Define $w_t = \sum_{i=1}^{|D|}\rho_{1:t}^{(i)}/|D|$ as the average cumulative importance ratio at horizon $t$ in a dataset $D$, then for each trajectory in $D$, the estimates given by trajectory-wise and step-wise WIS are respectively$$V_{\text{WIS}} = \frac{\rho_{1:H}}{w_{H}}(\sum_{t=1}^H\gamma^{t-1}r_t),$$$$V_{\text{step-WIS}} = \sum_{t=1}^H\gamma^{t-1}\frac{\rho_{1:t}}{w_t}r_t.$$The doubly robust estimator for contextual bandits$$V_{\text{DR}} := \widehat{V}(s) + \rho(r - \widehat{R}(s, a)),$$where $\rho := \frac{\pi_1(a|s)}{\pi_0(a|s)}$ and $\widehat{V}(s) := \sum_a \pi_1(a|s)\widehat{R}(s, a)$. If $\widehat{R}(s, a)$ is a good estimator of $r$, the magnitude of $r - \widehat{R}(s, a)$ can be much smaller than of $r$. Consequently, the variance of $\rho(r - \widehat{R}(s, a))$ tends to be smaller than that of $\rho r$, implying that DR often has a lower variance that IS (DudÂ´Ä±k et al., 2011).DR Estimator for the Sequential SettingA key observation is that Eqn.(6) can be written in a recursive form. Define $V_{\text{step-IS}}^0 := 0$, and for $t=1, \cdots, H$,$$V_{\text{step-IS}}^{H+1-t} := \rho_t (r_t + \gamma V_{\text{step-IS}}^{H-t}).$$We can apply the bandit DR estimator at each horizon, and obtain the following unbiased estimator: define $V_{\text{DR}}^0 := 0$, and$$V_{\text{DR}}^{H+1-t} := \widehat{V}(s_t) + \rho_t (r_t + \gamma V_{\text{DR}}^{H-t} - \widehat{Q}(s_t, a_t)).$$The DR estimator of the policy value is then $V_{\text{DR}} := V_{\text{DR}}^H$.One modification of DR that further reduces the variance in state transitions is:$$V_{\text{DR-v2}}^{H+1-t} := \widehat{V}(s_t) + \rho_t (r_t + \gamma V_{\text{DR-v2}}^{H-t} - \widehat{Q}(s_t, a_t) - \gamma \widehat{V}(s_{t+1})).$$Continuous State-Space Models for Optimal Sepsis Treatment: a Deep Reinforcement Learning ApproachModelState Autoencoder + Deep Q-Network + Dueling Network + Double Q Learning + Prioritized Experience ReplayNote: loss function have changed:$$\mathcal{L}(\theta) = \mathbb{E}[(Q_{\text{double-target}} - Q(s, a;\theta))^2] - \lambda \cdot \max(|Q(s, a;\theta) - R_{\max}|, 0),$$where $R_{\max} = \pm15, Q_{\text{double-target}}=r+\gamma Q(s^{\prime}, \arg\max_{a^{\prime}}Q(s^{\prime}, a^{\prime};\theta);\theta)$. $\theta$ are the weights used to parameterize the main network, and $\theta^{\prime}$ are the weights used to parameterize the target network.FeaturePreprocessingData were aggregated into windows of 4 hours, with the mean or sum being recorded (as appropriate) when several data points were present in one window. Variables with excessive missingness were removed, and any remaining missing values were imputed with k-nearest neighbors, yielding a 47 Ã— 1 feature vector for each patient at each timestep. Values exceeding clinical limits were capped, and capped data were normalized per-feature to zero mean and unit variance.Feature listThe physiological features used in our model are presented below.Demographics/StaticShock Index, Elixhauser, SIRS, Gender, Re-admission, GCS - Glasgow Coma Scale, SOFA - Sequential Organ Failure Assessment, AgeLab ValuesAlbumin, Arterial pH, Calcium, Glucose, Hemoglobin, Magnesium, PTT - Partial Thromboplastin Time, Potassium, SGPT - Serum Glutamic-Pyruvic Transaminase, Arterial Blood Gas, BUN - Blood Urea Nitrogen, Chloride, Bicarbonate, INR -International Normalized Ratio, Sodium, Arterial Lactate, CO2, Creatinine, Ionised Calcium, PT - Prothrombin Time, Platelets Count, SGOT - Serum Glutamic-Oxaloacetic Transaminase, Total bilirubin, White Blood Cell CountVital SignsDiastolic Blood Pressure, Systolic Blood Pressure, Mean Blood Pressure, PaCO2, PaO2, FiO2, PaO/FiO2 ratio, Respiratory Rate, Temperature (Celsius), Weight (kg), Heart Rate, SpO2Intake and Output EventsFluid Output - 4 hourly period, Total Fluid Output, Mechanical VentilationEvaluationDiscounted Returns vs. MortalityWe bin Q-values obtained via SARSA (baseline) on the test set into discrete buckets, and for each, if it is part of a trajectory where a patient died, we assign it a label of 1; if the patient survived, we assign a label of 0.State RepresentationDoubly Robust off-policy value evaluationAction differencesAction vs. MortalityA Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care UnitsModelPreprocessing using Gaussian ProcessesDenoting the observations of the vital signs by $v$ and the measurement time $t$, we model$$v = f(t) + \varepsilon,$$where $\varepsilon$ vector represents i.i.d Gaussian noise, and $f(t)$ is the latent noise-free function we would like to estimate. We put a GP prior on the latent function $f(t)$:$$f(t) \sim \mathcal{GP}(m(t), \mathcal{K}(t, t^{\prime})),$$where $m(t)$ is the mean function and $\mathcal{K}(t, t^{\prime})$ is the covariance function or kernel. In this work, we use a multi-output GP to account for temporal correlations between physiological signals during interpolation. We adapt the framework in Cheng et al. [2017] to impute the physiological signals jointly by estimating covariance structures between them, excluding the sparse prior settings (please read the paper for more details).Note: Just for continuous features.Reward FunctionFitted Q-iteration with sampling (Omitted)EvaluationsFeatures importanceThe outcome of differenceWe divide the 664 test admissions into six groups according to the fraction ofFQI policy actions that differ from the hospitalâ€™s policy: $\Delta_0$ comprises admissions in which the true and recommended policies agree perfectly, while those in $\Delta_5$ show the greatest deviation. Plotting the distribution of the number of reintubations and the mean accumulated reward over patient admissions respectively, for all patients in each set.Deep Reinforcement Learning, Decision Making and Control (ICML 2017 Tutorial)Model-Free RLpolicy gradientsREINFORCE algorithm:sample ${\tau^i}$ from $\pi_\theta(s_t|a_t)$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i(\sum_t \nabla_\theta \log \pi_\theta(a_t^i|s_t^i))(\sum_t r(s_t^i, a_t^i))$$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$Reduce variance$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i(\sum_t \nabla_\theta \log \pi_\theta(a_t^i|s_t^i))(\sum_{t^{\prime}=1}^T r(s_{t^{\prime}}^i, a_{t^{\prime}}^i))$â€œreward to goâ€Baselinesone baseline: average reward.$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i\nabla_\theta \log \pi_\theta(\tau) [r(\tau) - b]$$b = \frac{1}{N} \sum_{i=1}^N r(\tau)$Control variates (see also: Gu et al. 2016 (Q-Prop))$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_i(\sum_t \nabla_\theta \log \pi_\theta(a_t^i|s_t^i)) \Big (\hat{Q}_t^i - b(s_{t^{\prime}}^i, a_{t^{\prime}}^i) \Big ) + \frac{1}{N} \sum_i \sum_t \nabla_\theta E_{a \sim \pi_\theta(a_t | s_t^i)}[b(s_t^i, a_t)]$covatriant/natural policy gradientnatural gradient: pick $\alpha$$\theta \leftarrow \theta + \alpha \mathbf{F}^{-1}\nabla_\theta J(\theta)$$\mathbf{F} = E_{\pi_{\theta}}[\log\pi_{\theta}(a|s)\log\pi_{\theta}(a|s)^T]$trust region policy optimization: pick $\epsilon$$\theta^{\prime} \leftarrow \arg\max_{\theta^{\prime}} (\theta^{\prime} - \theta)^T \nabla_\theta J(\theta) \; \text{s.t.} (\theta^{\prime} - \theta)^T\mathbf{F}(\theta^{\prime} - \theta) \leq \epsilon$Policy gradients suggested readingsâ€¢ Classic papersâ€‹ â€¢ Williams (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning: introduces REINFORCE algorithmâ€‹ â€¢ Baxter &amp; Bartlett (2001). Infinite-horizon policy-gradient estimation: temporally decomposed policy gradient (not the first paper on this! see actor-critic section later)â€‹ â€¢ Peters &amp; Schaal (2008). Reinforcement learning of motor skills with policy gradients: very accessible overview of optimal baselines and natural gradientâ€¢ Deep reinforcement learning policy gradient papersâ€‹ â€¢ L. &amp; Koltun (2013). Guided policy search: deep RL with importance sampled policy gradient (unrelated to later discussion of guided policy search)â€‹ â€¢ Schulman, L., Moritz, Jordan, Abbeel (2015). Trust region policy optimization: deep RL with natural policy gradient and adaptive step sizeâ€‹ â€¢ Schulman, Wolski, Dhariwal, Radford, Klimov (2017). Proximal policy optimization algorithms: deep RL with importance sampled policy gradientActor-Critic algorithmsValue function fittingbatch actor-critic algorithm:sample $\{s_i, a_i\}$ from $\pi_\theta(a|s)$fit $\hat{V_\phi}(s)$ to sampled reward sums (*)evaluate $\hat{A^{\theta}}(s_i, a_i) = r(s_i, a_i) + \hat{V_\phi}(s_i^{\prime}) - \hat{V_\phi}(s_i)$$\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log \pi_\theta(a^i|s^i) \hat{A}(s_i, a_i) $$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$(*) $y_{i,t} \approx \sum_{t=1}^T r(s_t^i, a_{t}^i)$$\mathcal{L}(\phi) = \frac{1}{2}\sum_i | \hat{V_\phi}(s_i) - y_i|^2$Discount factors$y_{i,t} \approx r(s_t^i, a_{t}^i) + \gamma \hat{V_\phi}(s_{t+1}^i)$ (0.99 works well)online actor-critic algorithm:take action $\mathbf{a} \sim \pi_\theta(a|s)$, get $(s, a, s^{\prime}, r)$update $\hat{V_\phi^{\pi}}(s)$ using target $r + \gamma \hat{V_\phi^{\pi}}(s^{\prime})$evaluate $\hat{A^{\theta}}(s_i, a_i) = r(s_i, a_i) + \hat{V_\phi}(s_i^{\prime}) - \hat{V_\phi}(s_i)$$\nabla_\theta J(\theta) \approx \sum_i \nabla_\theta \log \pi_\theta(a^i|s^i) \hat{A}(s_i, a_i) $$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$Step 2 and 4 works best with a batch.We can design better estimators (for both batch and online). See Schulman, Moritz, L. Jordan, Abbeel â€˜16: Generalized advantage estimation.We can use single network for actor and critic.Actor-critic suggested readingsâ€¢ Classic papersâ€‹ â€¢ Sutton, McAllester, Singh, Mansour (1999). Policy gradient methods forreinforcement learning with function approximation: actor-critic algorithms withvalue function approximationâ€¢ Deep reinforcement learning actor-critic papersâ€‹ â€¢ Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu (2016).Asynchronous methods for deep reinforcement learning: A3C â€“ parallel onlineactor-criticâ€‹ â€¢ Schulman, Moritz, L., Jordan, Abbeel (2016). High-dimensional continuouscontrol using generalized advantage estimation: batch-mode actor-critic withblended Monte Carlo and function approximator returnsâ€‹ â€¢ Gu, Lillicrap, Ghahramani, Turner, L. (2017). Q-Prop: sample-efficient policy-gradient with an off-policy critic: policy gradient with Q-function control variateValue FunctionsQ-Learningtake some action $a_i$ and observe $(s_i, a_i, s^{\prime}_i, r_i)$, add it to $\mathcal{R}$sample mini-batch $(s_j, a_j, s^{\prime}_j, r_j)$ from $\mathcal{R}$ uniformlycompute $y_j = r_j + \gamma \max_{a^{\prime}_j} Q_{\phi^{\prime}} (s_j^{\prime},a_j^{\prime})$ using target network $Q_{\phi^{\prime}} $$\phi \leftarrow \phi - \alpha \sum_j \frac{dQ_{\phi}}{d\phi} (Q_\phi(s_j, a_j) - y_j)$update $\phi^{\prime}$: copy $\phi$ every $N$ steps, or Polyak average $\phi^{\prime} \leftarrow \tau\phi^{\prime} + (1-\tau)\phi$Q-Learning with continuous actionsOption 1: use function class that is easy to optimize (Gu, Lillicrap, Sutskever, L., ICML 2016)$Q(s, a | \theta^Q) = A(s, a|\theta^A) + V(x|\theta^V)$$A(s, a|\theta^A) = -\frac{1}{2}(s-\mu(x|\theta^{\mu}))^TP(s|\theta^P)(s-\mu(x|\theta^{\mu}))$Option 2: learn an approximate maximizer DDPG (Lillicrap et al., ICLR 2016)$\mu_\theta(s) = a, \theta \leftarrow \arg\max_\theta Q_{\phi}(s, \mu_{\theta}(s))$$y_j = r_j + \gamma \max_{a^{\prime}_j} Q_{\phi^{\prime}}(s_j^{\prime}, \mu_{\theta(s_j^{\prime})})$Q-learning suggested readingsâ€¢ Classic papersâ€‹ â€¢ Watkins. (1989). Learning from delayed rewards: introduces Q-learningâ€‹ â€¢ Riedmiller. (2005). Neural fitted Q-iteration: batch-mode Q-learning with neuralnetworksâ€¢ Deep reinforcement learning Q-learning papersâ€‹ â€¢ Lange, Riedmiller. (2010). Deep auto-encoder neural networks in reinforcementlearning: early image-based Q-learning method using autoencoders to constructembeddingsâ€‹ â€¢ Mnih et al. (2013). Human-level control through deep reinforcement learning: Q-learning with convolutional networks for playing Atari.â€‹ â€¢ Van Hasselt, Guez, Silver. (2015). Deep reinforcement learning with double Q-learning: a very effective trick to improve performance of deep Q-learning.â€‹ â€¢ Lillicrap et al. (2016). Continuous control with deep reinforcement learning: continuous Q-learning with actor network for approximate maximization.â€‹ â€¢ Gu, Lillicrap, Stuskever, L. (2016). Continuous deep Q-learning with model-basedacceleration: continuous Q-learning with action-quadratic value functions.â€‹ â€¢ Wang, Schaul, Hessel, van Hasselt, Lanctot, de Freitas (2016). Dueling networkarchitectures for deep reinforcement learning: separates value and advantage estimation in Q-function.Soft optimality (WIG)RL inference in a graphical modelsoft Q-learning$\phi \leftarrow \phi + \alpha \nabla_\phi Q_\phi (s, a) (r(s, a) + \gamma V(s^{\prime}) - Q_\phi(s, a))$target value: $V(s^{\prime}) = \text{soft}\max_{a^{\prime}} Q_\phi(s^{\prime}, a^{\prime}) = \log \int \exp(Q_\phi(s^{\prime}, a^{\prime})da^{\prime}$$\pi(a|s) = \exp(Q_\phi(s,a)-V(s)) = \exp(A(s, a))$policy gradient with soft optimality (WIG)Soft optimality suggested readingsâ€¢ Todorov. (2006). Linearly solvable Markov decision problems: one framework forreasoning about soft optimality.â€¢ Todorov. (2008). General duality between optimal control and estimation: primer on the equivalence between inference and control.â€¢ Kappen. (2009). Optimal control as a graphical model inference problem: frames control as an inference problem in a graphical model.â€¢ Ziebart. (2010). Modeling interaction via the principle of maximal causal entropy:connection between soft optimality and maximum entropy modeling.â€¢ Rawlik, Toussaint, Vijaykumar. (2013). On stochastic optimal control and reinforcement learning by approximate inference: temporal difference style algorithm with soft optimality.â€¢ Haarnoja, Tang, Abbeel, L. (2017). Reinforcement learning with deep energy basedmodels: soft Q-learning algorithm, deep RL with continuous actions and soft optimalityâ€¢ Nachum, Norouzi, Xu, Schuurmans. (2017). Bridging the gap between value and policy based reinforcement learning.â€¢ Schulman, Abbeel, Chen. (2017). Equivalence between policy gradients and soft Q-learning.Inverse RLMaximum Entropy Inverse RL (Ziebart et al. â€™08)Initialize $\psi$, gather demonstrations $\mathcal{D}$Solve for optimal policy $\pi(a|s)$ w.r.t. reward $r_\psi$Solve for state visitation frequencies $p(s|\psi)$Compute gradient $\nabla_\psi \mathcal{L} = -\frac{1}{|\mathcal{D}|}\sum_{\tau_d \in \mathcal{D}} \frac{dr_\psi}{d\psi}(\tau_d)\sum_{s} \frac{dr_\psi}{d\psi}(s)$Update $\psi$ with one gradient step using $\nabla_\psi\mathcal{L}$guided cost learning &amp; generative adversarial imitation algorithm (Finn et al. ICML â€™16, Ho &amp; Ermon NIPS â€™16)Suggested Reading on Inverse RL Classic PapersAbbeel &amp; Ng ICML â€™04. Apprenticeship Learning via Inverse Reinforcement Learning. Good introduction to inverse reinforcement learningZiebart et al. AAAI â€™08. Maximum Entropy Inverse Reinforcement Learning.Introduction of probabilistic method for inverse reinforcement learningModern PapersWulfmeier et al. arXiv â€™16. Deep Maximum Entropy Inverse Reinforcement Learning. MaxEnt IRL using deep reward functionsFinn et al. ICML â€™16. Guided Cost Learning. Sampling-based method for MaxEnt IRLthat handles unknown dynamics and deep reward functionsHo &amp; Ermon NIPS â€™16. Generative Adversarial Imitation Learning. IRL methodbuilding on Abbeel &amp; Ng â€™04 using generative adversarial networksFurther Reading on Inverse RLMaxEnt-based IRL: Ziebart et al. AAAI â€™08, Wulfmeier et al. arXiv â€™16, Finn et al. ICML â€˜16Adversarial IRL: Ho &amp; Ermon NIPS â€™16, Finn, Christiano et al. arXiv â€™16, Baram et al. ICML â€™17Handling multimodality: Li et al. arXiv â€™17, Hausman et al. arXiv â€™17, Wang, Merel et al. â€˜17Handling domain shift: Stadie et al. ICLR â€˜17Model-bases RL (WIS)Guided Policy Search (GPS)Suggested Reading on Model-based RLTassa et al. IROS â€™12. Synthesis and Stabilization of Complex Behaviors. Goodintroduction to MPC with a known modelLevine, Finn et al. JMLR â€™16. End-to-End Learning of Deep Visuomotor Policies.Thorough paper on guided policy search for learning real robotic vision-based skillsHeess et al. NIPS â€™15. Stochastic Value Gradients. Backdrop through dynamics to assist model-free learnerWatter et al. NIPS â€™15. Embed-to-Control, Learn latent space and use model-baed RL in learned latent space to reach image of goalFinn &amp; Levine ICRA â€™17. Deep Visual Foresight for Planning Robot Motion. Plan using learned action-conditioned video prediction modelFurther Reading on Model-based RLUse known model: Tassa et al. IROS â€™12, Tan et al. TOG â€™14, Mordatch et al. TOG â€˜14Guided policy search: Levine, Finn et al. JMLR â€™16, Mordatch et al. RSS â€™14, NIPS â€˜15Backprop through model: Deisenroth et al. ICML â€™11, Heess et al. NIPS â€™15, Mishra et al. ICML â€™17, Degrave et al. â€™17, Henaï¬€ et al. â€˜17MBRL in latent space: Watter et al. NIPS â€™15, Finn et al. ICRA â€˜16MPC with deep models: Lenz et al. RSS â€™15, Finn &amp; Levine ICRA â€˜17Combining Model-Based &amp; Model-Free:use roll-outs from model as experience: Sutton â€™90, Gu et al. ICML â€˜16use model as baseline: Chebotar et al. ICML â€˜17use model for exploration: Stadie et al. arXiv â€™15, Oh et al. NIPS â€™16model-free policy with planning capabilities: Tamar et al. NIPS â€™16, Pascanu et al. â€˜17model-based look-ahead: Guo et al. NIPS â€™14, Silver et al. Nature â€˜16]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Using Keras and Deep Q-Network to Play FlappyBird (Repost)]]></title>
      <url>%2F2017%2F07%2F07%2FUsing-Keras-and-Deep-Q-Network-to-Play-FlappyBird-Repost%2F</url>
      <content type="text"><![CDATA[200 lines of python code to demonstrate DQN with KerasOverviewThis project demonstrates how to use the Deep-Q Learning algorithm with Keras together to play FlappyBird.This article is intended to target newcomers who are interested in Reinforcement Learning.Installation Dependencies:(Update : 13 March 2017, code and weight file has been updated to support latest version of tensorflow and keras)Python 2.7Keras 1.0pygamescikit-imageHow to Run?CPU only/TensorFlow123git clone https://github.com/yanpanlau/Keras-FlappyBird.gitcd Keras-FlappyBirdpython qlearn.py -m &quot;Run&quot;GPU version (Theano)123git clone https://github.com/yanpanlau/Keras-FlappyBird.gitcd Keras-FlappyBirdTHEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=0.2 python qlearn.py -m &quot;Run&quot;lib.cnmem=0.2 means you assign only 20% of the GPUâ€™s memory to the program.If you want to train the network from beginning, delete â€œmodel.h5â€ and run qlearn.py -m â€œTrainâ€What is Deep Q-Network?Deep Q-Network is a learning algorithm developed by Google DeepMind to play Atari games. They demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable because it demonstrates the algorithm is generic enough to play various games.The following post is a must-read for those who are interested in deep reinforcement learning.Demystifying Deep Reinforcement LearningCode Explanation (in details)Letâ€™s go though the example in qlearn.py, line by line. If you familiar with Keras and DQN, you can skip this sessionThe code simply does the following:The code receives the Game Screen Input in the form of a pixel arrayThe code does some image pre-processingThe processed image will be fed into a neural network (Convolution Neural Network), and the network will then decide the best action to execute (Flap or Not Flap)The network will be trained millions of times, via an algorithm called Q-learning, to maximize the future expected reward.Game Screen InputFirst of all, the FlappyBird is already written in Python via pygame, so here is the code snippet to access the FlappyBird API12import wrapped_flappy_bird as gamex_t1_colored, r_t, terminal = game_state.frame_step(a_t)The idea is quite simple, the input is a_t (0 represent donâ€™t flap, 1 represent flap), the API will give you the next frame x_t1_colored, the reward (0.1 if alive, +1 if pass the pipe, -1 if die) and terminal is a boolean flag indicates whether the game is FINISHED or NOT. We also followed DeepMind suggestion to clip the reward between [-1,+1] to improve the stability. I have not yet get a chance to test out different reward functions but it would be an interesting exercise to see how the performance is changed with different reward functions.Interesting readers can modify the reward function in game/wrapped_flappy_bird.pyâ€, under the function **def frame_step(self, input_actions)Image pre-processingIn order to make the code train faster, it is vital to do some image processing. Here are the key elements:I first convert the color image into grayscaleI crop down the image size into 80x80 pixelI stack 4 frames together before I feed into neural network.Why do I need to stack 4 frames together? This is one way for the model to be able to infer the velocity information of the bird.123456x_t1 = skimage.color.rgb2gray(x_t1_colored)x_t1 = skimage.transform.resize(x_t1,(80,80))x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))x_t1 = x_t1.reshape(1, 1, x_t1.shape[0], x_t1.shape[1])s_t1 = np.append(x_t1, s_t[:, :3, :, :], axis=1)x_t1 is a single frame with shape (1x1x80x80) and s_t1 is the stacked frame with shape (1x4x80x80). You might ask, why the input dimension is (1x4x80x80) but not (4x80x80)? Well, it is a requirement in Keras so letâ€™s stick with it.Note: Some readers may ask what is axis=1? It means that when I stack the frames, I want to stack on the â€œ2ndâ€ dimension. i.e. I am stacking under (1x4x80x80), the 2nd index.Convolution Neural NetworkNow, we can input the pre-processed screen into the neural network, which is a convolution neural network:123456789101112131415161718def buildmodel(): print("Now we build the model") model = Sequential() model.add(Convolution2D(32, 8, 8, subsample=(4,4),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode='same',input_shape=(img_channels,img_rows,img_cols))) model.add(Activation('relu')) model.add(Convolution2D(64, 4, 4, subsample=(2,2),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode='same')) model.add(Activation('relu')) model.add(Convolution2D(64, 3, 3, subsample=(1,1),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode='same')) model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(512, init=lambda shape, name: normal(shape, scale=0.01, name=name))) model.add(Activation('relu')) model.add(Dense(2,init=lambda shape, name: normal(shape, scale=0.01, name=name))) adam = Adam(lr=1e-6) model.compile(loss='mse',optimizer=adam) print("We finish building the model") return modelThe exact architecture is following : The input to the neural network consists of an 4x80x80 images. The first hidden layer convolves 32 filters of 8 x 8 with stride 4 and applies ReLU activation function. The 2nd layer convolves a 64 filters of 4 x 4 with stride 2 and applies ReLU activation function. The 3rd layer convolves a 64 filters of 3 x 3 with stride 1 and applies ReLU activation function. The final hidden layer is fully-connected consisted of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action.So wait, what is convolution? The easiest way to understand a convolution is by thinking of it as a sliding window function apply to a matrix. The following gif file should help to understand.You might ask whatâ€™s the purpose the convolution? It actually help computer to learn higher features like edges and shapes. The example below shows how the edges are stand out after a convolution filter is appliedFor more details about Convolution in Neural Network, please read Understanding Convolution Neural Networks for NLPNoteKeras makes it very easy to build convolution neural network. However, there are few things I would like to highlightA) It is important to choose a right initialization method. I choose normal distribution with $\sigma=0.01$1init=lambda shape, name: normal(shape, scale=0.01, name=name)B) The ordering of the dimension is important, the default setting is 4x80x80 (Theano setting), so if your input is 80x80x4 (Tensorflow setting) then you are in trouble because the dimension is wrong. Alert: If your input dimension is 80x80x4 (Tensorflow setting) you need to set dim_ordering = tf (tf means tensorflow, th means theano)C) In Keras, subsample=(2,2) means you down sample the image size from (80x80) to (40x40). In ML literature it is often called â€œstrideâ€D) We have used an adaptive learning algorithm called ADAM to do the optimization. The learning rate is 1-e6.Interested readers who want to learn more various learning algoithms please read belowAn overview of gradient descent optimization algorithmsDQNFinally, we can using the Q-learning algorithm to train the neural network.So, what is Q-learning? In Q-learning the most important thing is the Q-function : Q(s, a) representing the maximum discounted future reward when we perform action a in state s. Q(s, a) gives you an estimation of how good to choose an action a in state s.REPEAT : Q(s, a) representing the maximum discounted future reward when we perform action a in state sYou might ask 1) Why Q-function is useful? 2) How can I get the Q-function?Let me give you an analogy of the Q-function: Suppose you are playing a difficult RPG game and you donâ€™t know how to play it well. If you have bought a strategy guide, which is an instruction book that contain hints or complete solutions to a specific video game, then playing that video game is easy. You just follow the guidiance from the strategy book. Here, Q-function is similar to a strategy guide. Suppose you are in state s and you need to decide whether you take action a or b. If you have this magical Q-function, the answers become really simple â€“ pick the action with highest Q-value!$${\pi(s) = {argmax}_{a} Q(s,a)}$$Here, $\pi$ represents the policy, which you will often see in the ML literature.How do we get the Q-function? Thatâ€™s where Q-learning is coming from. Let me quickly derive here:Define total future reward from time t onward$$R_t = r_t + r_{t+1} + r_{t+2} â€¦ + r_n$$But because our environment is stochastic, we can never be sure, the more future we go, the more uncertain. Therefore, it is common to use discount future reward instead$$R_t = r_t + \gamma r_{t+1} + \gamma^{2} r_{t+2} â€¦ + \gamma^{n-t} r_n$$which, can be written as$$R_t = r_t + \gamma \ast R_{t+1}$$Recall the definition of Q-function (maximum discounted future reward if we choose action a in state s)$$Q(s_t, a_t) = max R_{t+1}$$therefore, we can rewrite the Q-function as below$$Q(s, a) = r + \gamma \ast max_{a^{â€˜}} Q(s^{\prime}, a^{\prime})$$In plain English, it means maximum future reward for this state and action (s,a) is the immediate reward r plus maximum future reward for the next state $s^{\prime}$ , action $a^{\prime}$We could now use an iterative method to solve for the Q-function. Given a transition $(s, a, r, s^{\prime})$ , we are going to convert this episode into training set for the network. i.e. We want $r + \gamma max_a Q (s,a)$ to be equal to $Q(s,a)$. You can think of finding a Q-value is a regession task now, I have a estimator $r + \gamma max_a Q (s,a)$ and a predictor $Q(s,a)$, I can define the mean squared error (MSE), or the loss function, as below:Define a loss function$$L = [r + \gamma max_{a^{\prime}} Q (s^{\prime},a^{\prime}) - Q(s,a)]^{2}$$Given a transition $(s, a, r, s^{\prime})$, how can I optimize my Q-function such that it returns the smallest mean squared error loss? If L getting smaller, we know the Q-function is getting converged into the optimal value, which is our â€œstrategy bookâ€.Now, you might ask, where is the role of the neural network? This is where the DEEP Q-Learning comes in. You recall that $Q(s,a)$, is a stategy book, which contains millions or trillions of states and actions if you display as a table. The idea of the DQN is that I use the neural network to COMPRESS this Q-table, using some parameters \thetaÎ¸ (We called it weight in Neural Network). So instead of handling a large table, I just need to worry the weights of the neural network. By smartly tuning the weight parameters, I can find the optimal Q-function via the various Neural Network training algorithm.$$Q(s,a) = f_{\theta}(s)$$where $f$ is our neural network with input $s$ and weight parameters $\theta$Here is the code below to demonstrate how it works123456789101112131415161718192021222324252627282930if t &gt; OBSERVE: #sample a minibatch to train on minibatch = random.sample(D, BATCH) inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3])) #32, 80, 80, 4 targets = np.zeros((inputs.shape[0], ACTIONS)) #32, 2 #Now we do the experience replay for i in range(0, len(minibatch)): state_t = minibatch[i][0] action_t = minibatch[i][1] #This is action index reward_t = minibatch[i][2] state_t1 = minibatch[i][3] terminal = minibatch[i][4] # if terminated, only equals reward inputs[i:i + 1] = state_t #I saved down s_t targets[i] = model.predict(state_t) # Hitting each buttom probability Q_sa = model.predict(state_t1) if terminal: targets[i, action_t] = reward_t else: targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa) loss += model.train_on_batch(inputs, targets) s_t = s_t1 t = t + 1Experience ReplayIf you examine the code above, there is a comment called â€œExperience Replayâ€. Let me explain what it does: It was found that approximation of Q-value using non-linear functions like neural network is not very stable. The most important trick to solve this problem is called experience replay. During the gameplay all the episode $(s, a, r, s^{â€˜})$ are stored in replay memory D. (I use Python function deque() to store it). When training the network, random mini-batches from the replay memory are used instead of most the recent transition, which will greatly improve the stability.Exploration vs. ExploitationThere is another issue in the reinforcement learning algorithm which called Exploration vs. Exploitation. How much of an agentâ€™s time should be spent exploiting its existing known-good policy, and how much time should be focused on exploring new, possibility better, actions? We often encounter similar situations in real life too. For example, we face on which restaurant to go to on Saturday night. We all have a set of restaurants that we prefer, based on our policy/strategy book $Q(s,a)$. If we stick to our normal preference, there is a strong probability that weâ€™ll pick a good restaurant. However, sometimes, we occasionally like to try new restaurants to see if they are better. The RL agents face the same problem. In order to maximize future reward, they need to balance the amount of time that they follow their current policy (this is called being â€œgreedyâ€), and the time they spend exploring new possibilities that might be better. A popular approach is called $\epsilon$ greedy approach. Under this approach, the policy tells the agent to try a random action some percentage of the time, as defined by the variable $\epsilon$ (epsilon), which is a number between 0 and 1. The strategy will help the RL agent to occasionally try something new and see if we can achieve ultimate strategy.123456789if random.random() &lt;= epsilon: print("----------Random Action----------") action_index = random.randrange(ACTIONS) a_t[action_index] = 1 else: q = model.predict(s_t) #input a stack of 4 images, get the prediction max_Q = np.argmax(q) action_index = max_Q a_t[max_Q] = 1I think thatâ€™s it. I hope this blog will help you to understand how DQN works.FAQMy training is very slowYou might need a GPU to accelerate the calculation. I used a TITAN X and train for at least 1 million frames to make it workFuture works and thoughtsCurrent DQN depends on large experience replay. Is it possible to replace it or even remove it?How can one decide on the optimal Convolution Neural Network?Training is very slow, how to speed it up/to make the model converge faster?What does the Neural Network actually learn? Is the knowledge transferable?I believe the questions are still not resolved and itâ€™s an active research area in Machine Learning.Reference[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level Control through Deep Reinforcement Learning. Nature, 529-33, 2015.DisclaimerThis work is highly based on the following repos:https://github.com/yenchenlin/DeepLearningFlappyBirdhttp://edersantana.github.io/articles/keras_rl/AcknowledgementI must thank to @hardmaru to encourage me to write this blog. I also thank to @fchollet to help me on the weight initialization in Keras and @edersantana his post on Keras and reinforcement learning which really help me to understand it.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Demystifying Deep Reinforcement Learning (Repost)]]></title>
      <url>%2F2017%2F07%2F07%2FDemystifying-Deep-Reinforcement-Learning%2F</url>
      <content type="text"><![CDATA[Two years ago, a small company in London called DeepMind uploaded their pioneering paper â€œPlaying Atari with Deep Reinforcement Learningâ€ to Arxiv. In this paper they demonstrated how a computer learned to play Atari 2600 video games by observing just the screen pixels and receiving a reward when the game score increased. The result was remarkable, because the games and the goals in every game were very different and designed to be challenging for humans. The same model architecture, without any change, was used to learn seven different games, and in three of them the algorithm performed even better than a human!It has been hailed since then as the first step towards general artificial intelligence â€“ an AI that can survive in a variety of environments, instead of being confined to strict realms such as playing chess. No wonder DeepMind was immediately bought by Google and has been on the forefront of deep learning research ever since. In February 2015 their paper â€œHuman-level control through deep reinforcement learningâ€ was featured on the cover of Nature, one of the most prestigious journals in science. In this paper they applied the same model to 49 different games and achieved superhuman performance in half of them.Still, while deep models for supervised and unsupervised learning have seen widespread adoption in the community, deep reinforcement learning has remained a bit of a mystery. In this blog post I will be trying to demystify this technique and understand the rationale behind it. The intended audience is someone who already has background in machine learning and possibly in neural networks, but hasnâ€™t had time to delve into reinforcement learning yet.The roadmap ahead:What are the main challenges in reinforcement learning? We will cover the credit assignment problem and the exploration-exploitation dilemma here.How to formalize reinforcement learning in mathematical terms? We will define Markov Decision Process and use it for reasoning about reinforcement learning.How do we form long-term strategies? We define â€œdiscounted future rewardâ€, that forms the main basis for the algorithms in the next sections.How can we estimate or approximate the future reward? Simple table-based Q-learning algorithm is defined and explained here.What if our state space is too big? Here we see how Q-table can be replaced with a (deep) neural network.What do we need to make it actually work? Experience replay technique will be discussed here, that stabilizes the learning with neural networks.Are we done yet? Finally we will consider some simple solutions to the exploration-exploitation problem.Reinforcement LearningConsider the game Breakout. In this game you control a paddle at the bottom of the screen and have to bounce the ball back to clear all the bricks in the upper half of the screen. Each time you hit a brick, it disappears and your score increases â€“ you get a reward.Figure 1: Atari Breakout game. Image credit: DeepMind.Suppose you want to teach a neural network to play this game. Input to your network would be screen images, and output would be three actions: left, right or fire (to launch the ball). It would make sense to treat it as a classification problem â€“ for each game screen you have to decide, whether you should move left, right or press fire. Sounds straightforward? Sure, but then you need training examples, and a lots of them. Of course you could go and record game sessions using expert players, but thatâ€™s not really how we learn. We donâ€™t need somebody to tell us a million times which move to choose at each screen. We just need occasional feedback that we did the right thing and can then figure out everything else ourselves.This is the task reinforcement learning tries to solve. Reinforcement learning lies somewhere in between supervised and unsupervised learning. Whereas in supervised learning one has a target label for each training example and in unsupervised learning one has no labels at all, in reinforcement learning one has sparse and time-delayed labels â€“ the rewards. Based only on those rewards the agent has to learn to behave in the environment.While the idea is quite intuitive, in practice there are numerous challenges. For example when you hit a brick and score a reward in the Breakout game, it often has nothing to do with the actions (paddle movements) you did just before getting the reward. All the hard work was already done, when you positioned the paddle correctly and bounced the ball back. This is called the credit assignment problem â€“ i.e., which of the preceding actions was responsible for getting the reward and to what extent.Once you have figured out a strategy to collect a certain number of rewards, should you stick with it or experiment with something that could result in even bigger rewards? In the above Breakout game a simple strategy is to move to the left edge and wait there. When launched, the ball tends to fly left more often than right and you will easily score about 10 points before you die. Will you be satisfied with this or do you want more? This is called the explore-exploit dilemma â€“ should you exploit the known working strategy or explore other, possibly better strategies.Reinforcement learning is an important model of how we (and all animals in general) learn. Praise from our parents, grades in school, salary at work â€“ these are all examples of rewards. Credit assignment problems and exploration-exploitation dilemmas come up every day both in business and in relationships. Thatâ€™s why it is important to study this problem, and games form a wonderful sandbox for trying out new approaches.Markov Decision ProcessNow the question is, how do you formalize a reinforcement learning problem, so that you can reason about it? The most common method is to represent it as a Markov decision process.Suppose you are an agent, situated in an environment (e.g. Breakout game). The environment is in a certain state(e.g. location of the paddle, location and direction of the ball, existence of every brick and so on). The agent can perform certain actions in the environment (e.g. move the paddle to the left or to the right). These actions sometimes result in a reward (e.g. increase in score). Actions transform the environment and lead to a new state, where the agent can perform another action, and so on. The rules for how you choose those actions are called policy. The environment in general is stochastic, which means the next state may be somewhat random (e.g. when you lose a ball and launch a new one, it goes towards a random direction).Figure 2: Left: reinforcement learning problem. Right: Markov decision process.The set of states and actions, together with rules for transitioning from one state to another, make up a Markov decision process. One episode of this process (e.g. one game) forms a finite sequence of states, actions and rewards:Here si represents the state, ai is the action and ri+1 is the reward after performing the action. The episode ends with terminal state sn (e.g. â€œgame overâ€ screen). A Markov decision process relies on the Markov assumption, that the probability of the next state si+1 depends only on current state si and action ai, but not on preceding states or actions.Discounted Future RewardTo perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. How should we go about that?Given one run of the Markov decision process, we can easily calculate the total reward for one episode:Given that, the total future reward from time point t onward can be expressed as:But because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use discounted future reward instead:Here Î³ is the discount factor between 0 and 1 â€“ the more into the future the reward is, the less we take it into consideration. It is easy to see, that discounted future reward at time step t can be expressed in terms of the same thing at time step t+1:If we set the discount factor Î³=0, then our strategy will be short-sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like Î³=0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor Î³=1.A good strategy for an agent would be to always choose an action that maximizes the (discounted) future reward.Q-learningIn Q-learning we define a function Q(s, a) representing the maximum discounted future reward when we perform action a in state s, and continue optimally from that point on.The way to think about Q(s, a) is that it is â€œthe best possible score at the end of the game after performing action ain state sâ€œ. It is called Q-function, because it represents the â€œqualityâ€ of a certain action in a given state.This may sound like quite a puzzling definition. How can we estimate the score at the end of game, if we know just the current state and action, and not the actions and rewards coming after that? We really canâ€™t. But as a theoretical construct we can assume existence of such a function. Just close your eyes and repeat to yourself five times: â€œQ(s, a) exists, Q(s, a) exists, â€¦â€. Feel it?If youâ€™re still not convinced, then consider what the implications of having such a function would be. Suppose you are in state and pondering whether you should take action a or b. You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer becomes really simple â€“ pick the action with the highest Q-value!Here Ï€ represents the policy, the rule how we choose an action in each state.OK, how do we get that Q-function then? Letâ€™s focus on just one transition &lt;s, a, r, sâ€™&gt;. Just like with discounted future rewards in the previous section, we can express the Q-value of state s and action a in terms of the Q-value of the next state sâ€™.This is called the Bellman equation. If you think about it, it is quite logical â€“ maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state.The main idea in Q-learning is that we can iteratively approximate the Q-function using the Bellman equation. In the simplest case the Q-function is implemented as a table, with states as rows and actions as columns. The gist of the Q-learning algorithm is as simple as the following[1]:Î± in the algorithm is a learning rate that controls how much of the difference between previous Q-value and newly proposed Q-value is taken into account. In particular, when Î±=1, then two Q[s,a] cancel and the update is exactly the same as the Bellman equation.The maxaâ€™ Q[sâ€™,aâ€™] that we use to update Q[s,a] is only an approximation and in early stages of learning it may be completely wrong. However the approximation get more and more accurate with every iteration and it has been shown, that if we perform this update enough times, then the Q-function will converge and represent the true Q-value.Deep Q NetworkThe state of the environment in the Breakout game can be defined by the location of the paddle, location and direction of the ball and the presence or absence of each individual brick. This intuitive representation however is game specific. Could we come up with something more universal, that would be suitable for all the games? The obvious choice is screen pixels â€“ they implicitly contain all of the relevant information about the game situation, except for the speed and direction of the ball. Two consecutive screens would have these covered as well.If we apply the same preprocessing to game screens as in the DeepMind paper â€“ take the four last screen images, resize them to 84Ã—84 and convert to grayscale with 256 gray levels â€“ we would have 25684x84x4 â‰ˆ 1067970 possible game states. This means 1067970 rows in our imaginary Q-table â€“ more than the number of atoms in the known universe! One could argue that many pixel combinations (and therefore states) never occur â€“ we could possibly represent it as a sparse table containing only visited states. Even so, most of the states are very rarely visited and it would take a lifetime of the universe for the Q-table to converge. Ideally, we would also like to have a good guess for Q-values for states we have never seen before.This is the point where deep learning steps in. Neural networks are exceptionally good at coming up with good features for highly structured data. We could represent our Q-function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q-value. Alternatively we could take only game screens as input and output the Q-value for each possible action. This approach has the advantage, that if we want to perform a Q-value update or pick the action with the highest Q-value, we only have to do one forward pass through the network and have all Q-values for all actions available immediately.Figure 3: Left: Naive formulation of deep Q-network. Right: More optimized architecture of deep Q-network, used in DeepMind paper.The network architecture that DeepMind used is as follows:This is a classical convolutional neural network with three convolutional layers, followed by two fully connected layers. People familiar with object recognition networks may notice that there are no pooling layers. But if you really think about it, pooling layers buy you translation invariance â€“ the network becomes insensitive to the location of an object in the image. That makes perfectly sense for a classification task like ImageNet, but for games the location of the ball is crucial in determining the potential reward and we wouldnâ€™t want to discard this information!Input to the network are four 84Ã—84 grayscale game screens. Outputs of the network are Q-values for each possible action (18 in Atari). Q-values can be any real values, which makes it a regression task, that can be optimized with simple squared error loss.Given a transition &lt; s, a, r, sâ€™ &gt;, the Q-table update rule in the previous algorithm must be replaced with the following:Do a feedforward pass for the current state s to get predicted Q-values for all actions.Do a feedforward pass for the next state sâ€™ and calculate maximum overall network outputs max aâ€™ Q(sâ€™, aâ€™).Set Q-value target for action to r + Î³max aâ€™ Q(sâ€™, aâ€™) (use the max calculated in step 2). For all other actions, set the Q-value target to the same as originally returned from step 1, making the error 0 for those outputs.Update the weights using backpropagation.Experience ReplayBy now we have an idea how to estimate the future reward in each state using Q-learning and approximate the Q-function using a convolutional neural network. But it turns out that approximation of Q-values using non-linear functions is not very stable. There is a whole bag of tricks that you have to use to actually make it converge. And it takes a long time, almost a week on a single GPU.The most important trick is experience replay. During gameplay all the experiences &lt; s, a, r, sâ€™ &gt; are stored in a replay memory. When training the network, random minibatches from the replay memory are used instead of the most recent transition. This breaks the similarity of subsequent training samples, which otherwise might drive the network into a local minimum. Also experience replay makes the training task more similar to usual supervised learning, which simplifies debugging and testing the algorithm. One could actually collect all those experiences from human gameplay and then train network on these.Exploration-ExploitationQ-learning attempts to solve the credit assignment problem â€“ it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward. But we havenâ€™t touched the exploration-exploitation dilemma yetâ€¦Firstly observe, that when a Q-table or Q-network is initialized randomly, then its predictions are initially random as well. If we pick an action with the highest Q-value, the action will be random and the agent performs crude â€œexplorationâ€. As a Q-function converges, it returns more consistent Q-values and the amount of exploration decreases. So one could say, that Q-learning incorporates the exploration as part of the algorithm. But this exploration is â€œgreedyâ€, it settles with the first effective strategy it finds.A simple and effective fix for the above problem is Îµ-greedy exploration â€“ with probability Îµ choose a random action, otherwise go with the â€œgreedyâ€ action with the highest Q-value. In their system DeepMind actually decreases Îµ over time from 1 to 0.1 â€“ in the beginning the system makes completely random moves to explore the state space maximally, and then it settles down to a fixed exploration rate.Deep Q-learning AlgorithmThis gives us the final deep Q-learning algorithm with experience replay:There are many more tricks that DeepMind used to actually make it work â€“ like target network, error clipping, reward clipping etc, but these are out of scope for this introduction.The most amazing part of this algorithm is that it learns anything at all. Just think about it â€“ because our Q-function is initialized randomly, it initially outputs complete garbage. And we are using this garbage (the maximum Q-value of the next state) as targets for the network, only occasionally folding in a tiny reward. That sounds insane, how could it learn anything meaningful at all? The fact is, that it does.Final notesMany improvements to deep Q-learning have been proposed since its first introduction â€“ Double Q-learning, Prioritized Experience Replay, Dueling Network Architecture and extension to continuous action space to name a few. For latest advancements check out the NIPS 2015 deep reinforcement learning workshop and ICLR 2016(search for â€œreinforcementâ€ in title). But beware, that deep Q-learning has been patented by Google.It is often said, that artificial intelligence is something we havenâ€™t figured out yet. Once we know how it works, it doesnâ€™t seem intelligent any more. But deep Q-networks still continue to amaze me. Watching them figure out a new game is like observing an animal in the wild â€“ a rewarding experience by itself.CreditsThanks to Ardi Tampuu, Tanel PÃ¤rnamaa, Jaan Aru, Ilya Kuzovkin, Arjun Bansal and Urs KÃ¶ster for comments and suggestions on the drafts of this post.LinksDavid Silverâ€™s lecture about deep reinforcement learningSlightly awkward but accessible illustration of Q-learningUC Berkleyâ€™s course on deep reinforcement learningDavid Silverâ€™s reinforcement learning courseNando de Freitasâ€™ course on machine learning (two lectures about reinforcement learning in the end)Andrej Karpathyâ€™s course on convolutional neural networks[1] Algorithm adapted from http://artint.info/html/ArtInt_265.htmlThis blog was first published at: http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/This is the part 1 of my series on deep reinforcement learning. Tune in next week for â€œDeep Reinforcement Learning with Neonâ€ for an actual implementation with Neon deep learning toolkit.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[On-policy Control with Approximation]]></title>
      <url>%2F2017%2F07%2F06%2FOn-policy-Control-with-Approximation%2F</url>
      <content type="text"><![CDATA[In this post we turn to the control problem with parametric approximation of the action-value function $\hat{q}(s, a, \mathbf{w}) \approx q_{\ast}(s, a)$, where $\mathbf{w} \in \mathbb{R}^d$ is a finite-dimensional weight vector.Episodic Semi-gradient ControlThe general gradient-descent update for action-value prediction is$$\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).$$For example, the update for the one-step Sarsa method is$$\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ R_{t+1} + \gamma \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t)- \hat{q}(S_t, A_t, \mathbf{w}_t) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).$$We call this method episode semi-gradient one-step sarsa.To form control methods, we need to couple such action-value prediction methods with techniques for policy improvement and action selection. Suitable techniques applicable to continuous actions, or to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution. On the other hand, if the action set is discrete and not too large, then we can use the techniques already developed in previous chapters.Example: Mountain-Car TaskConsider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram in the below.The diï¬ƒculty is that gravity is stronger than the carâ€™s engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to ï¬rst move away from the goal and up the opposite slope on the left. Then, by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way. This is a simple example of a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. Many control methodologies have great diï¬ƒculties with tasks of this kind unless explicitly aided by a human designer.The reward in this problem is âˆ’1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (âˆ’1), and zero throttle (0). The car moves according to a simpliï¬ed physics. Its position, $x_t$, and velocity, $\dot{x}_t$, are updated by$$\begin{align}x_{t+1} &amp;\doteq bound \big[x_t + \dot{x}_{t+1} \big] \\\dot{x}_{t+1} &amp;\doteq bound \big[\dot{x}_t + 0.001 A_t - 0.0025 \cos(3x_t) \big],\end{align}$$where the bound operation enforces $-1.2 \leq x_{t+1} \leq 0.5 \;\; \text{and} \; -0.07 \leq \dot{x}_{t+1} \leq 0.07$. In addition, when $x_{t+1}$ reached the left bound, $\dot{x}_{t+1}$ was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position $x_t \in [âˆ’0.6, âˆ’0.4)$ and zero velocity. To convert the two continuous state variables to binary features, we used grid-tilings.First of all, we define the environment of this problem:123456789101112131415# all possible actionsACTION_REVERSE = -1ACTION_ZERO = 0ACTION_FORWARD = 1# order is importantACTIONS = [ACTION_REVERSE, ACTION_ZERO, ACTION_FORWARD]# bound for position and velocityPOSITION_MIN = -1.2POSITION_MAX = 0.5VELOCITY_MIN = -0.07VELOCITY_MAX = 0.07# use optimistic initial value, so it's ok to set epsilon to 0EPSILON = 0After take an action, we transition to a new state and get a reward:1234567891011# take an @action at @position and @velocity# @return: new position, new velocity, reward (always -1)def takeAction(position, velocity, action): newVelocity = velocity + 0.001 * action - 0.0025 * np.cos(3 * position) newVelocity = min(max(VELOCITY_MIN, newVelocity), VELOCITY_MAX) newPosition = position + newVelocity newPosition = min(max(POSITION_MIN, newPosition), POSITION_MAX) reward = -1.0 if newPosition == POSITION_MIN: newVelocity = 0.0 return newPosition, newVelocity, rewardThe $\varepsilon$-greedy policy:12345678# get action at @position and @velocity based on epsilon greedy policy and @valueFunctiondef getAction(position, velocity, valueFunction): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(ACTIONS) values = [] for action in ACTIONS: values.append(valueFunction.value(position, velocity, action)) return np.argmax(values) - 1We need map out continuous state to discrete state:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# wrapper class for state action value functionclass ValueFunction: # In this example I use the tiling software instead of implementing standard tiling by myself # One important thing is that tiling is only a map from (state, action) to a series of indices # It doesn't matter whether the indices have meaning, only if this map satisfy some property # View the following webpage for more information # http://incompleteideas.net/sutton/tiles/tiles3.html # @maxSize: the maximum # of indices def __init__(self, stepSize, numOfTilings=8, maxSize=2048): self.maxSize = maxSize self.numOfTilings = numOfTilings # divide step size equally to each tiling self.stepSize = stepSize / numOfTilings self.hashTable = IHT(maxSize) # weight for each tile self.weights = np.zeros(maxSize) # position and velocity needs scaling to satisfy the tile software self.positionScale = self.numOfTilings / (POSITION_MAX - POSITION_MIN) self.velocityScale = self.numOfTilings / (VELOCITY_MAX - VELOCITY_MIN) # get indices of active tiles for given state and action def getActiveTiles(self, position, velocity, action): # I think positionScale * (position - position_min) would be a good normalization. # However positionScale * position_min is a constant, so it's ok to ignore it. activeTiles = tiles(self.hashTable, self.numOfTilings, [self.positionScale * position, self.velocityScale * velocity], [action]) return activeTiles # estimate the value of given state and action def value(self, position, velocity, action): if position == POSITION_MAX: return 0.0 activeTiles = self.getActiveTiles(position, velocity, action) return np.sum(self.weights[activeTiles]) # learn with given state, action and target def learn(self, position, velocity, action, target): activeTiles = self.getActiveTiles(position, velocity, action) estimation = np.sum(self.weights[activeTiles]) delta = self.stepSize * (target - estimation) for activeTile in activeTiles: self.weights[activeTile] += delta # get # of steps to reach the goal under current state value function def costToGo(self, position, velocity): costs = [] for action in ACTIONS: costs.append(self.value(position, velocity, action)) return -np.max(costs)Because the one-step semi-gradient sarsa is a special case of n-step, so we develop the n-step algorithm$$G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}), \; n \geq 1,0 \leq t \leq T-n.$$The n-step equation is$$\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ G_{t:t+n}- \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}) \Big] \nabla \hat{q}(S_t, A_t, \mathbf{w}_{t+n-1}), \;\;\; 0 \leq t \leq T.$$Complete pseudocode is given in the box below.So the code is as follows:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# semi-gradient n-step Sarsa# @valueFunction: state value function to learn# @n: # of stepsdef semiGradientNStepSarsa(valueFunction, n=1): # start at a random position around the bottom of the valley currentPosition = np.random.uniform(-0.6, -0.4) # initial velocity is 0 currentVelocity = 0.0 # get initial action currentAction = getAction(currentPosition, currentVelocity, valueFunction) # track previous position, velocity, action and reward positions = [currentPosition] velocities = [currentVelocity] actions = [currentAction] rewards = [0.0] # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step time += 1 if time &lt; T: # take current action and go to the new state newPostion, newVelocity, reward = takeAction(currentPosition, currentVelocity, currentAction) # choose new action newAction = getAction(newPostion, newVelocity, valueFunction) # track new state and action positions.append(newPostion) velocities.append(newVelocity) actions.append(newAction) rewards.append(reward) if newPostion == POSITION_MAX: T = time # get the time of the state to update updateTime = time - n if updateTime &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(updateTime + 1, min(T, updateTime + n) + 1): returns += rewards[t] # add estimated state action value to the return if updateTime + n &lt;= T: returns += valueFunction.value(positions[updateTime + n], velocities[updateTime + n], actions[updateTime + n]) # update the state value function if positions[updateTime] != POSITION_MAX: valueFunction.learn(positions[updateTime], velocities[updateTime], actions[updateTime], returns) if updateTime == T - 1: break currentPosition = newPostion currentVelocity = newVelocity currentAction = newAction return timeNext, we use the method mentioned earlier to solve this problem:12345678910episodes = 9000targetEpisodes = [1-1, 12-1, 104-1, 1000-1, episodes - 1]numOfTilings = 8alpha = 0.3valueFunction = ValueFunction(alpha, numOfTilings)for episode in range(0, episodes): print('episode:', episode) semiGradientNStepSarsa(valueFunction) if episode in targetEpisodes: prettyPrint(valueFunction, 'Episode: ' + str(episode + 1))Result is as follows:The result shows what typically happens while learning to solve this task with this form of function approximation. Shown is the negative of the value function (the cost-to-go function) learned on a single run. The initial action values were all zero, which was optimistic (all true values are negative in this task), causing extensive exploration to occur even though the exploration parameter, $\varepsilon$, was 0. All the states visited frequently are valued worse than unexplored states, because the actual rewards have been worse than what was (unrealistically) expected. This continually drives the agent away from wherever it has been, to explore new states, until a solution is found.Next, let us test the performance of various step size (learning rate).12345678910111213141516171819202122232425runs = 10episodes = 500numOfTilings = 8alphas = [0.1, 0.2, 0.5]steps = np.zeros((len(alphas), episodes))for run in range(0, runs): valueFunctions = [ValueFunction(alpha, numOfTilings) for alpha in alphas] for index in range(0, len(valueFunctions)): for episode in range(0, episodes): print('run:', run, 'alpha:', alphas[index], 'episode:', episode) step = semiGradientNStepSarsa(valueFunctions[index]) steps[index, episode] += stepsteps /= runsglobal figureIndexplt.figure(figureIndex)figureIndex += 1for i in range(0, len(alphas)): plt.plot(steps[i], label='alpha = '+str(alphas[i])+'/'+str(numOfTilings))plt.xlabel('Episode')plt.ylabel('Steps per episode')plt.yscale('log')plt.legend()The result is as follows:And then, let us test the performance of one-step sarsa and multi-step sarsa on the Mountain Car task:12345678910111213141516171819202122232425runs = 10episodes = 500numOfTilings = 8alphas = [0.5, 0.3]nSteps = [1, 8]steps = np.zeros((len(alphas), episodes))for run in range(0, runs): valueFunctions = [ValueFunction(alpha, numOfTilings) for alpha in alphas] for index in range(0, len(valueFunctions)): for episode in range(0, episodes): print('run:', run, 'steps:', nSteps[index], 'episode:', episode) step = semiGradientNStepSarsa(valueFunctions[index], nSteps[index]) steps[index, episode] += stepsteps /= runsglobal figureIndexplt.figure(figureIndex)figureIndex += 1for i in range(0, len(alphas)): plt.plot(steps[i], label='n = '+str(nSteps[i]))plt.xlabel('Episode')plt.ylabel('Steps per episode')plt.yscale('log')plt.legend()The result is as follows:Finally, let me shows the results of a more detailed study of the eï¬€ect of the parameters $\alpha$ and $n$ on the rate of learning on this task.123456789101112131415161718192021222324252627282930313233alphas = np.arange(0.25, 1.75, 0.25)nSteps = np.power(2, np.arange(0, 5))episodes = 50runs = 5truncateStep = 300steps = np.zeros((len(nSteps), len(alphas)))for run in range(0, runs): for nStepIndex, nStep in zip(range(0, len(nSteps)), nSteps): for alphaIndex, alpha in zip(range(0, len(alphas)), alphas): if (nStep == 8 and alpha &gt; 1) or \ (nStep == 16 and alpha &gt; 0.75): # In these cases it won't converge, so ignore them steps[nStepIndex, alphaIndex] += truncateStep * episodes continue valueFunction = ValueFunction(alpha) for episode in range(0, episodes): print('run:', run, 'steps:', nStep, 'alpha:', alpha, 'episode:', episode) step = semiGradientNStepSarsa(valueFunction, nStep) steps[nStepIndex, alphaIndex] += step# average over independent runs and episodessteps /= runs * episodes# truncate high values for better displaysteps[steps &gt; truncateStep] = truncateStepglobal figureIndexplt.figure(figureIndex)figureIndex += 1for i in range(0, len(nSteps)): plt.plot(alphas, steps[i, :], label='n = '+str(nSteps[i]))plt.xlabel('alpha * number of tilings(8)')plt.ylabel('Steps per episode')plt.legend()The result is as follows:UpdateUse OpenAI gymNow, let us use the OpenAI gym toolkit to simply our Mountain Car task. As we know, we need to define the environment of task before develop the detail algorithm. Itâ€™s easy to make mistakes with some detail. So it is fantastic if we do not need to care about that. The gym toolkit help us to define the environment. Such as the Mountain Car task, there is a build-in model in the gym toolkit. We just need to write one row code:1env = gym.envs.make("MountainCar-v0")That is amazing!We also can test the environment very convenience and get a pretty good user graphic:12345678910111213env.reset()plt.figure()plt.imshow(env.render(mode='rgb_array'))# for x in range(10000):# env.step(0)# plt.figure()# plt.imshow(env.render(mode='rgb_array')) [env.step(0) for x in range(10000)]plt.figure()plt.imshow(env.render(mode='rgb_array'))env.render(close=True)These codes will return the result as follows:Bravo~Now, let us solve the task by to use the Q-Learning algorithm. Meanwhile, we will use the RBF kernel to construct the features and use the SGDRegressor gradient-descent method of scikit-learn package to update $\mathbf{w}$.First of all, we need to normalized our features to accelerate the convergence speed and use the RBF kernel to reconstruct our feature space. The both methods (Normalization and Reconstruction) need to use some training set to train a model. Then we use this model to transform our raw data.123456789101112131415# Feature Preprocessing: Normalize to zero mean and unit variance# We use a few samples from the observation space to do thisobservation_examples = np.array([env.observation_space.sample() for x in range(10000)])scaler = sklearn.preprocessing.StandardScaler()scaler.fit(observation_examples)# Used to converte a state to a featurizes represenation.# We use RBF kernels with different variances to cover different parts of the spacefeaturizer = sklearn.pipeline.FeatureUnion([ ("rbf1", RBFSampler(gamma=5.0, n_components=100)), ("rbf2", RBFSampler(gamma=2.0, n_components=100)), ("rbf3", RBFSampler(gamma=1.0, n_components=100)), ("rbf4", RBFSampler(gamma=0.5, n_components=100)) ])featurizer.fit(scaler.transform(observation_examples))Next, we define a class named Estimator to simply the gradient descent process:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Estimator(): """ Value Function approximator. """ def __init__(self): # We create a separate model for each action in the environment's # action space. Alternatively we could somehow encode the action # into the features, but this way it's easier to code up. self.models = [] for _ in range(env.action_space.n): model = SGDRegressor(learning_rate="constant") # We need to call partial_fit once to initialize the model # or we get a NotFittedError when trying to make a prediction # This is quite hacky. model.partial_fit([self.featurize_state(env.reset())], [0]) self.models.append(model) def featurize_state(self, state): """ Returns the featurized representation for a state. """ scaled = scaler.transform([state]) featurized = featurizer.transform(scaled) return featurized[0] def predict(self, s, a=None): """ Makes value function predictions. Args: s: state to make a prediction for a: (Optional) action to make a prediction for Returns If an action a is given this returns a single number as the prediction. If no action is given this returns a vector or predictions for all actions in the environment where pred[i] is the prediction for action i. """ features = self.featurize_state(s) if not a: return np.array([m.predict([features])[0] for m in self.models]) else: return self.models[a].predict([features])[0] def update(self, s, a, y): """ Updates the estimator parameters for a given state and action towards the target y. """ features = self.featurize_state(s) self.models[a].partial_fit([features], [y])We also need a $\varepsilon$-greedy policy to select action:123456789101112131415161718192021def make_epsilon_greedy_policy(estimator, epsilon, nA): """ Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon. Args: estimator: An estimator that returns q values for a given state epsilon: The probability to select a random action . float between 0 and 1. nA: Number of actions in the environment. Returns: A function that takes the observation as an argument and returns the probabilities for each action in the form of a numpy array of length nA. """ def policy_fn(observation): A = np.ones(nA, dtype=float) * epsilon / nA q_values = estimator.predict(observation) best_action = np.argmax(q_values) A[best_action] += (1.0 - epsilon) return A return policy_fnThen we develop the Q-Learning method:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980def q_learning(env, estimator, num_episodes, discount_factor=1.0, epsilon=0.1, epsilon_decay=1.0): """ Q-Learning algorithm for fff-policy TD control using Function Approximation. Finds the optimal greedy policy while following an epsilon-greedy policy. Args: env: OpenAI environment. estimator: Action-Value function estimator num_episodes: Number of episodes to run for. discount_factor: Lambda time discount factor. epsilon: Chance the sample a random action. Float betwen 0 and 1. epsilon_decay: Each episode, epsilon is decayed by this factor Returns: An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards. """ # Keeps track of useful statistics stats = plotting.EpisodeStats( episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes)) for i_episode in range(num_episodes): # The policy we're following policy = make_epsilon_greedy_policy( estimator, epsilon * epsilon_decay**i_episode, env.action_space.n) # Print out which episode we're on, useful for debugging. # Also print reward for last episode last_reward = stats.episode_rewards[i_episode - 1] sys.stdout.flush() # Reset the environment and pick the first action state = env.reset() # Only used for SARSA, not Q-Learning next_action = None # One step in the environment for t in itertools.count(): # Choose an action to take # If we're using SARSA we already decided in the previous step if next_action is None: action_probs = policy(state) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) else: action = next_action # Take a step next_state, reward, done, _ = env.step(action) # Update statistics stats.episode_rewards[i_episode] += reward stats.episode_lengths[i_episode] = t # TD Update q_values_next = estimator.predict(next_state) # Use this code for Q-Learning # Q-Value TD Target td_target = reward + discount_factor * np.max(q_values_next) # Use this code for SARSA TD Target for on policy-training: # next_action_probs = policy(next_state) # next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs) # td_target = reward + discount_factor * q_values_next[next_action] # Update the function approximator using our target estimator.update(state, action, td_target) print("\rStep &#123;&#125; @ Episode &#123;&#125;/&#123;&#125; (&#123;&#125;)".format(t, i_episode + 1, num_episodes, last_reward), end="") if done: break state = next_state return statsRun this method:12345estimator = Estimator()# Note: For the Mountain Car we don't actually need an epsilon &gt; 0.0# because our initial estimate for all states is too "optimistic" which leads# to the exploration of all states.stats = q_learning(env, estimator, 100, epsilon=0.0)The result is as follows:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[On-policy Prediction with Approximation]]></title>
      <url>%2F2017%2F07%2F05%2FOn-policy-Prediction-with-Approximation%2F</url>
      <content type="text"><![CDATA[The novelty in this post is that the approximate value function is represented not as a table but as a parameterized functional form with weight vector $\mathbf{w} \in \mathbb{R}^d$. We will write $\hat{v}(s, \mathbf{w}) \approx v_{\pi}(s)$ for the approximated value of state $s$ given weight vector $\mathbf{w}$. For example, $\hat{v}$ might be a linear function in features of the state, with $\mathbf{w}$ the vector of feature weights. Consequently, when a single state is updated, the change generalizes from that state to affect the values of many other states.The prediction Objective (MSVE)In the tabular case a continuous measure of prediction quality was not necessary because the learned value function could come to equal the true value function exactly. But with genuine approximation, an update at one state aï¬€ects many others, and it is not possible to get all states exactly correct. By assumption we have far more states than weights, so making one stateâ€™s estimate more accurate invariably means making othersâ€™ less accurate.By the error in a state $s$ we mean the square of the diï¬€erence between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_{\pi}(s)$. Weighting this over the state space by the distribution $\mu$, we obtain a natural objective function, the Mean Squared Value Error, or MSVE:$$\text{MSVE}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s) \Big[v_{\pi}(s) - \hat{v}(s, \mathbf{w}) \Big]^2.$$The square root of this measure, the root MSVE or RMSVE, gives a rough measure of how much the approximate values diï¬€er from the true values and is often used in plots. Typically one chooses $\mu(s)$ to be the fraction of time spent in $s$ under the target policy $\pi$. This is called the on-policy distribution.Stochastic-gradient MethodsWe assume that states appear in examples with the same distribution, Âµ, over which we are trying to minimize the MSVE. A good strategy in this case is to try to minimize error on the observed examples. Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example by a small amount in the direction that would most reduce the error on that example:$$\begin{align}\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t - \frac{1}{2} \alpha \nabla \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big]^2 \\&amp;= \mathbf{w}_t + \alpha \Big[ v_{\pi}(s) - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).\end{align}$$And$$\nabla f(\mathbf{w}) \doteq \Big( \frac{\partial f(\mathbf{w})}{\partial w_1}, \frac{\partial f(\mathbf{w})}{\partial w_2}, \cdots, \frac{\partial f(\mathbf{w})}{\partial w_d}\Big)^{\top}.$$Although $v_{\pi}(S_t)$ is unknown, but we can approximate it by substituting $U_t$ (the $t$th training example) in place of $v_{\pi}(S_t)$. This yields the following general SGD method for state-value prediction:$$\mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat{v}(S_t, \mathbf{w}_t)\Big] \nabla \hat{v}(S_t, \mathbf{w}_t).$$If $U_t$ is an unbiased estimate, that is, if $\mathbb{E}[U_t]=v_{\pi}(S_t)$, for each $t$, then $\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic approximation conditions for decreasing $\alpha$.For example, suppose the states in the examples are the states generated by interaction (or simulated interaction) with the environment using policy $\pi$. Because the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t \doteq G_t$ is by deï¬nition an unbiased estimate of $v_{\pi}(S_t)$. Thus, the gradient-descent version of Monte Carlo state-value prediction is guaranteed to ï¬nd a locally optimal solution. Pseudocode for a complete algorithmis shown in the box.Example: State Aggregation on the 1000-state Random WalkState aggregation is a simple form of generalizing function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group. The value of a state is estimated as its groupâ€™s component, and when the state is updated, that component alone is updated. State aggregation is a special case of SGD in which the gradient, $\nabla \hat{v}(S_t, \mathbf{w}_t)$, is 1 for $S_t$â€™s groupâ€™s component and 0 for the other components.Consider a 1000-state version of the random walk task. The states are numbered from 1 to 1000, left to right, and all episodes begin near the center, in state 500. State transitions are from the current state to one of the 100 neighboring states to its left, or to one of the 100 neighboring states to its right, all with equal probability. Of course, if the current state is near an edge, then there may be fewer than 100 neighbors on that side of it. In this case, all the probability that would have gone into those missing neighbors goes into the probability of terminating on that side (thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right). As usual, termination on the left produces a reward of âˆ’1, and termination on the right produces a reward of +1. All other transitions have a reward of zero.Now, let us solve this problem by gradient Monte Carlo algorithm. First of all, we define the environment of this problem.12345678910111213141516171819202122# # of states except for terminal statesN_STATES = 1000# true state values, just a promising guesstrueStateValues = np.arange(-1001, 1003, 2) / 1001.0# all statesstates = np.arange(1, N_STATES + 1)# start from a central stateSTART_STATE = 500# terminal statesEND_STATES = [0, N_STATES + 1]# possible actionsACTION_LEFT = -1ACTION_RIGHT = 1ACTIONS = [ACTION_LEFT, ACTION_RIGHT]# maximum stride for an actionSTEP_RANGE = 100We need a true value of each state, thus use the dynamic programming to get these value:12345678910111213141516171819# Dynamic programming to find the true state values, based on the promising guess above# Assume all rewards are 0, given that we have already given value -1 and 1 to terminal stateswhile True: oldTrueStateValues = np.copy(trueStateValues) for state in states: trueStateValues[state] = 0 for action in ACTIONS: for step in range(1, STEP_RANGE + 1): step *= action newState = state + step newState = max(min(newState, N_STATES + 1), 0) # asynchronous update for faster convergence trueStateValues[state] += 1.0 / (2 * STEP_RANGE) * trueStateValues[newState] error = np.sum(np.abs(oldTrueStateValues - trueStateValues)) print(error) if error &lt; 1e-2: break# correct the state value for terminal states to 0trueStateValues[0] = trueStateValues[-1] = 0The policy of episodes generation:12345678910111213# take an @action at @state, return new state and reward for this transitiondef takeAction(state, action): step = np.random.randint(1, STEP_RANGE + 1) step *= action state += step state = max(min(state, N_STATES + 1), 0) if state == 0: reward = -1 elif state == N_STATES + 1: reward = 1 else: reward = 0 return state, rewardThe reward after take an action:12345# get an action, following random policydef getAction(): if np.random.binomial(1, 0.5) == 1: return 1 return -1And we have a special value function:1234567891011121314151617181920212223# a wrapper class for aggregation value functionclass ValueFunction: # @numOfGroups: # of aggregations def __init__(self, numOfGroups): self.numOfGroups = numOfGroups self.groupSize = N_STATES // numOfGroups # thetas self.params = np.zeros(numOfGroups) # get the value of @state def value(self, state): if state in END_STATES: return 0 groupIndex = (state - 1) // self.groupSize return self.params[groupIndex] # update parameters # @delta: step size * (target - old estimation) # @state: state of current sample def update(self, delta, state): groupIndex = (state - 1) // self.groupSize self.params[groupIndex] += deltaAnd the gradient MC algorithm:12345678910111213141516171819202122# gradient Monte Carlo algorithm# @valueFunction: an instance of class ValueFunction# @alpha: step size# @distribution: array to store the distribution statisticsdef gradientMonteCarlo(valueFunction, alpha, distribution=None): currentState = START_STATE trajectory = [currentState] # We assume gamma = 1, so return is just the same as the latest reward reward = 0.0 while currentState not in END_STATES: action = getAction() newState, reward = takeAction(currentState, action) trajectory.append(newState) currentState = newState # Gradient update for each state in this trajectory for state in trajectory[:-1]: delta = alpha * (reward - valueFunction.value(state)) valueFunction.update(delta, state) if distribution is not None: distribution[state] += 1Finally. let us solve this problem:12345678910111213141516171819202122232425nEpisodes = int(1e5)alpha = 2e-5# we have 10 aggregations in this example, each has 100 statesvalueFunction = ValueFunction(10)distribution = np.zeros(N_STATES + 2)for episode in range(0, nEpisodes): print('episode:', episode) gradientMonteCarlo(valueFunction, alpha, distribution)distribution /= np.sum(distribution)stateValues = [valueFunction.value(i) for i in states]plt.figure(0)plt.plot(states, stateValues, label='Approximate MC value')plt.plot(states, trueStateValues[1: -1], label='True value')plt.xlabel('State')plt.ylabel('Value')plt.legend()plt.figure(1)plt.plot(states, distribution[1: -1], label='State distribution')plt.xlabel('State')plt.ylabel('Distribution')plt.legend()Results are as follows:Semi-gradient MethodsBootstrapping target such as n-step returns $G_{t:t+n}$ or the DP target $\sum_{a, s^{\prime}, r} \pi(a|S_t)p(s^{\prime}, r|S_t, a)[r + \gamma \hat{v}(s^{\prime}, \mathbf{w}_t)]$ all depend on the current value of the weight vector $\mathbf{w}_t$, which implies that they will be biased and that they will not produce a true gradient=descent method. We call them semi-gradient methods.Although semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear case. Moreover, they oï¬€er important advantages which makes them often clearly preferred. One reason for this is that they are typically signiï¬cantly faster to learn. Another is that they enable learning to be continual and online, without waiting for the end of an episode. This enables them to be used on continuing problems and provides computational advantages. A prototypical semi-gradient method is semi-gradient TD(0), which uses $U_t \doteq R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w})$ as its target. Complete pseudocode for this method is given in the box below.Linear MethodsOne of the most important special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot, \mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$. Corresponding to every state $s$, there is a real-valued vector of features $\mathbf{x}(s) \doteq (x_1(s),x_2(s),\cdots,x_d(s))^{\top}$, with the same number of components as $\mathbf{w}$. However the features are constructed, the approximate state-value function is given by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:$$\hat{v}(s, \mathbf{w}) \doteq \mathbf{w^{\top}x}(s) \doteq \sum_{i=1}^d w_i x_i(s).$$The individual functions $x_i:\mathcal{S} \rightarrow \mathbb{E}$ are called basis functions. The gradient of the approximate value function with respect to $\mathbf{w}$ in this case is$$\nabla \hat{v} (s, \mathbf{w}) = \mathbf{x}(s).$$The update at each time $t$ is$$\begin{align}\mathbf{w}_{t+1} &amp;\doteq \mathbf{w}_t + \alpha \Big( R_{t+1} + \gamma \mathbf{w}_t^{\top}\mathbf{x}_{t+1} - \mathbf{w}_t^{\top}\mathbf{x}_{t}\Big)\mathbf{x}_t \\&amp;= \mathbf{w}_t + \alpha \Big( R_{t+1}\mathbf{x}_t - \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \mathbf{w}_t\Big),\end{align}$$where here we have used the notational shorthand $\mathbf{x}_t = \mathbf{x}(S_t)$. If the system converges, it must converge to the weight vector $\mathbf{w}_{TD}$ at which$$\mathbf{w}_{TD} \doteq \mathbf{A^{-1}b},$$where$$\mathbf{b} \doteq \mathbb{E}[R_{t+1}\mathbf{x}_t] \in \mathbb{R}^d \;\; \text{and} \;\; \mathbf{A} \doteq \mathbb{E}\big[ \mathbf{x}_t(\mathbf{x}_t - \gamma \mathbf{x}_{t+1})^{\top} \big] \in \mathbb{R}^d \times \mathbb{R}^d.$$This quantity is called the TD fixedpoint. At this point we have:$$\text{MSVE}(\mathbf{w}_{TD}) \leq \frac{1}{1 - \gamma} \min_{\mathbf{w}} \text{MSVE}(\mathbf{w}).$$Now we use the state aggregation example again, but use the semi-gradient TD method.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# semi-gradient n-step TD algorithm# @valueFunction: an instance of class ValueFunction# @n: # of steps# @alpha: step sizedef semiGradientTemporalDifference(valueFunction, n, alpha): # initial starting state currentState = START_STATE # arrays to store states and rewards for an episode # space isn't a major consideration, so I didn't use the mod trick states = [currentState] rewards = [0] # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step time += 1 if time &lt; T: # choose an action randomly action = getAction() newState, reward = takeAction(currentState, action) # store new state and new reward states.append(newState) rewards.append(reward) if newState in END_STATES: T = time # get the time of the state to update updateTime = time - n if updateTime &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(updateTime + 1, min(T, updateTime + n) + 1): returns += rewards[t] # add state value to the return if updateTime + n &lt;= T: returns += valueFunction.value(states[updateTime + n]) stateToUpdate = states[updateTime] # update the value function if not stateToUpdate in END_STATES: delta = alpha * (returns - valueFunction.value(stateToUpdate)) valueFunction.update(delta, stateToUpdate) if updateTime == T - 1: break currentState = newState1234567891011121314nEpisodes = int(1e5) alpha = 2e-4 valueFunction = ValueFunction(10) for episode in range(0, nEpisodes): print('episode:', episode) semiGradientTemporalDifference(valueFunction, 1, alpha) stateValues = [valueFunction.value(i) for i in states] plt.figure(2) plt.plot(states, stateValues, label='Approximate TD value') plt.plot(states, trueStateValues[1: -1], label='True value') plt.xlabel('State') plt.ylabel('Value') plt.legend()Results are as follows:We also could use the n-step semi-gradient TD method. To obtain such quantitatively similar results we switched the state aggregation to 20 groups of 50 states each. The 20 groups are then quantitatively close to the 19 states of the tabular problem.The semi-gradient n-step TD algorithm we used in this example is the natural extension of the tabular n-step TD algorithm. The key equation is$$\mathbf{w}_{t+n} \doteq \mathbf{w}_{t+n-1} + \alpha \Big[ G_{t:t+n} - \hat{v}(S_t, \mathbf{w}_{t+n-1})\Big] \nabla \hat{v}(S_t, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T,$$where$$G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{v}(S_{t+n}, \mathbf{w}_{t+n-1}), \;\; 0 \leq t \leq T-n.$$Pseudocode for the complete algorithm is given in the box below.Now let us show the performance of different value of n:1234567891011121314151617181920212223242526272829303132333435363738# truncate value for better displaytruncateValue = 0.55# all possible stepssteps = np.power(2, np.arange(0, 10))# all possible alphasalphas = np.arange(0, 1.1, 0.1)# each run has 10 episodesepisodes = 10# perform 100 independent runsruns = 100# track the errors for each (step, alpha) combinationerrors = np.zeros((len(steps), len(alphas)))for run in range(0, runs): for stepInd, step in zip(range(len(steps)), steps): for alphaInd, alpha in zip(range(len(alphas)), alphas): print('run:', run, 'step:', step, 'alpha:', alpha) # we have 20 aggregations in this example valueFunction = ValueFunction(20) for ep in range(0, episodes): semiGradientTemporalDifference(valueFunction, step, alpha) # calculate the RMS error currentStateValues = np.asarray([valueFunction.value(i) for i in states]) errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - trueStateValues[1: -1], 2)) / N_STATES)# take averageerrors /= episodes * runs# truncate the errorerrors[errors &gt; truncateValue] = truncateValueplt.figure(3)for i in range(0, len(steps)): plt.plot(alphas, errors[i, :], label='n = ' + str(steps[i]))plt.xlabel('alpha')plt.ylabel('RMS error')plt.legend()The results are as follows:Feature Construction for Linear MethodsChoosing features appropriate to the task is an important way of adding prior domain knowledge to reinforcement learning systems. Intuitively, the features should correspond to the natural features of the task, those along which generalization is most appropriate. If we are valuing geometric objects, for example, we might want to have features for each possible shape, color, size, or function. If we are valuing states of a mobile robot, then we might want to have features for locations, degrees of remaining battery power, recent sonar readings, and so on.PolynomialsFourier BasisKonidaris et al. (2011) found that when using Fourier cosine basis functions with a learning algorithm such as semi-gradient TD(0), or semi-gradient Sarsa, it is helpful to use a diï¬€erent step-size parameter for each basis function. If $\alpha$ is the basic step-size parameter, they suggest setting the step-size parameter for basis function $x_i$ to $a_i=\alpha/\sqrt{(c_1^i)^2 + \cdots + (c_d^i)^2}$ (except when each $c_j^i=0$, in which case $\alpha_i=\alpha$).Now, let us we compare the Fourier and polynomial bases on the 1000-state random walk example. In general, we do not recommend using the polynomial basis for online learning.123456789101112131415161718192021222324252627282930313233# a wrapper class for polynomial / Fourier -based value functionPOLYNOMIAL_BASES = 0FOURIER_BASES = 1class BasesValueFunction: # @order: # of bases, each function also has one more constant parameter (called bias in machine learning) # @type: polynomial bases or Fourier bases def __init__(self, order, type): self.order = order self.weights = np.zeros(order + 1) # set up bases function self.bases = [] if type == POLYNOMIAL_BASES: for i in range(0, order + 1): self.bases.append(lambda s, i=i: pow(s, i)) elif type == FOURIER_BASES: for i in range(0, order + 1): self.bases.append(lambda s, i=i: np.cos(i * np.pi * s)) # get the value of @state def value(self, state): # map the state space into [0, 1] state /= float(N_STATES) # get the feature vector feature = np.asarray([func(state) for func in self.bases]) return np.dot(self.weights, feature) def update(self, delta, state): # map the state space into [0, 1] state /= float(N_STATES) # get derivative value derivativeValue = np.asarray([func(state) for func in self.bases]) self.weights += delta * derivativeValueThe function upper is used to construction the features of states (map states to features).Next, we will compare different super-parametersâ€™ (order) performance:1234567891011121314151617181920212223242526272829303132333435363738runs = 1episodes = 5000# # of basesorders = [5, 10, 20]alphas = [1e-4, 5e-5]labels = [['polynomial basis'] * 3, ['fourier basis'] * 3]# track errors for each episodeerrors = np.zeros((len(alphas), len(orders), episodes))for run in range(0, runs): for i in range(0, len(orders)): valueFunctions = [BasesValueFunction(orders[i], POLYNOMIAL_BASES), BasesValueFunction(orders[i], FOURIER_BASES)] for j in range(0, len(valueFunctions)): for episode in range(0, episodes): print('run:', run, 'order:', orders[i], labels[j][i], 'episode:', episode) # gradient Monte Carlo algorithm gradientMonteCarlo(valueFunctions[j], alphas[j]) # get state values under current value function stateValues = [valueFunctions[j].value(state) for state in states] # get the root-mean-squared error errors[j, i, episode] += np.sqrt(np.mean(np.power(trueStateValues[1: -1] - stateValues, 2)))# average over independent runserrors /= runsplt.figure(5)for i in range(0, len(alphas)): for j in range(0, len(orders)): plt.plot(errors[i, j, :], label=labels[i][j]+' order = ' + str(orders[j]))plt.xlabel('Episodes')plt.ylabel('RMSVE')plt.legend()Results:TODO: TILE CODING]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[n-step TD]]></title>
      <url>%2F2017%2F07%2F04%2Fn-step-TD%2F</url>
      <content type="text"><![CDATA[In this post, we unify the Monte Carlo methods and the one-step TD methods. We first consider the prediction problem.n-step TD PredictionMonte Carlo methods preform a backup for each state based on the entire sequence of observed rewards from that state until the end of the episode. One-step TD methods is based on just on next reward. So n-step TD methods perform a backup based on an intermediate number of rewards: more than one, but less than all of them until termination.More formally, consider the backup applied to state $S_t$ as a result of the state-reward sequence, $S_t, R_{t+1},S_{t+1}, R_{t+2}, \cdots, R_T, S_T$ (omitting the actions for simplicity). We know that in Monte Carlo backups the estimate of $v_{\pi}(S_t)$ updated in the direction of the complete return:$$G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T,$$where $T$ is the last time step of the episode. Let us call this quantity the target of the backup. Whereas in Monte Carlo backups the target is the return, in one-step backups the target is the first reward plus the discounted estimated value of the next state, which we call the one-step return:$$G_{t:t+1} \doteq R_{t+1} + \gamma V_t (S_{t+1}),$$where $V_t : \mathcal{S} \rightarrow \mathbb{R}$ here is an estimate at time $t$ of $v_{\pi}$. The subscripts on $G_{t:t+1}$ indicate that it is truncated return for time t using rewards up until time $t+1$. In the one-step return, $\gamma V_t (S_{t+1})$ takes the place of the other terms $ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$ of the full return. Our point now is that this idea makes just as much sense after two steps as it does after one. The target for a two-step backup is the two-step return:$$G_{t:t+2} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 V_{t+1} (S_{t+2}),$$where now $\gamma^2 V_{t+1}(S_{t+2})$ corrects for the absence of the terms $\gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1} R_T$. Similarly, the target for an arbitrary n-step backup is the n-step return:$$G_{t:t+n} \doteq R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n V_{t+n-1} (S_{t+n}),$$for all $n,t$ such that $n \ge 1$ and $0 \leq t \leq T-n$. If $t+n \ge T$, then all the missing terms are taken as zero, and the n-step return defined to be equal to the ordinary full return.No real algorithm can use the n-step return until after it has seen $R_{t+n}$ and computed $V_{t+n-1}$. The first time these are available is $t+n$. The natural algorithm state-value learning algorithm for using n-step returns is thus$$V_{t+n}(S_t) \doteq V_{t+n-1}(S_t) + \alpha [G_{t:t+n} - V_{t+n-1}(S_t)], \;\;\;\;\;\; 0 \leq t \leq T$$while the values of all other states remain unchanged. Note that no changes at all are made during the first $n-1$ steps of each episode. Complete pseudocode is given in the box below.The worst error of the expected n-step return is guaranteed to be less than or equal to $\gamma^n$ times the worst error under $V_{t+n-1}$:$$\max_s \left |\mathbb{E}[G_{t:t+1}|S_t=s] - v_{\pi}(s) \right | \leq \gamma^n \max_s |V_{t+n-1}(s) - v_{\pi}(s)|,$$for all $n \geq 1$. This is called the error reduction property of n-step returns. The n-step TD methods thus form a family of sound methods, with one-step TD methods and Monte Carlo methods as extreme members.Example: n-step TD Methods on the Random WalkNow we have a larger MDP (19 non-terminal states). First of all we need to define the new environment:1234567891011121314151617181920212223# all statesN_STATES = 19# discountGAMMA = 1# initial state valuesstateValues = np.zeros(N_STATES + 2)# all states but terminal statesstates = np.arange(1, N_STATES + 1)# start from the middle stateSTART_STATE = 10# two terminal states# an action leading to the left terminal state has reward -1# an action leading to the right terminal state has reward 1END_STATES = [0, N_STATES + 1]# true state value from bellman equationrealStateValues = np.arange(-20, 22, 2) / 20.0realStateValues[0] = realStateValues[-1] = 0And then develop the n-step TD algorithm:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# n-steps TD method# @stateValues: values for each state, will be updated# @n: # of steps# @alpha: # step sizedef temporalDifference(stateValues, n, alpha): # initial starting state currentState = START_STATE # arrays to store states and rewards for an episode # space isn't a major consideration, so I didn't use the mod trick states = [currentState] rewards = [0] # track the time time = 0 # the length of this episode T = float('inf') while True: # go to next time step time += 1 if time &lt; T: # choose an action randomly if np.random.binomial(1, 0.5) == 1: newState = currentState + 1 else: newState = currentState - 1 if newState == 0: reward = -1 elif newState == 20: reward = 1 else: reward = 0 # store new state and new reward states.append(newState) rewards.append(reward) if newState in END_STATES: T = time # get the time of the state to update updateTime = time - n if updateTime &gt;= 0: returns = 0.0 # calculate corresponding rewards for t in range(updateTime + 1, min(T, updateTime + n) + 1): returns += pow(GAMMA, t - updateTime - 1) * rewards[t] # add state value to the return if updateTime + n &lt;= T: returns += pow(GAMMA, n) * stateValues[states[(updateTime + n)]] stateToUpdate = states[updateTime] # update the state value if not stateToUpdate in END_STATES: stateValues[stateToUpdate] += alpha * (returns - stateValues[stateToUpdate]) if updateTime == T - 1: break currentState = newStateNow, let us test the performance under different $n$ values and $\alpha$ values:123456789101112131415161718192021222324252627282930313233343536# truncate value for better displaytruncateValue = 0.55# all possible stepssteps = np.power(2, np.arange(0, 10))# all possible alphasalphas = np.arange(0, 1.1, 0.1)# each run has 10 episodesepisodes = 10# perform 100 independent runsruns = 100# track the errors for each (step, alpha) combinationerrors = np.zeros((len(steps), len(alphas)))for run in range(0, runs): for stepInd, step in zip(range(len(steps)), steps): for alphaInd, alpha in zip(range(len(alphas)), alphas): print('run:', run, 'step:', step, 'alpha:', alpha) currentStateValues = np.copy(stateValues) for ep in range(0, episodes): temporalDifference(currentStateValues, step, alpha) # calculate the RMS error errors[stepInd, alphaInd] += np.sqrt(np.sum(np.power(currentStateValues - realStateValues, 2)) / N_STATES)# take averageerrors /= episodes * runs# truncate the errorerrors[errors &gt; truncateValue] = truncateValueplt.figure()for i in range(0, len(steps)): plt.plot(alphas, errors[i, :], label='n = ' + str(steps[i]))plt.xlabel('alpha')plt.ylabel('RMS error')plt.legend()Results are as follows:TODO: N-STEP SARSATODO: N-STEP OFF-POLICY ALGORITHM]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Temporal-Difference Learning]]></title>
      <url>%2F2017%2F07%2F02%2FTemporal-Difference-Learning%2F</url>
      <content type="text"><![CDATA[If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-difference (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environmentâ€™s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome.TD(0)Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environment is$$V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)],$$where $G_t$ is the actual return following time $t$. Let us call this method $constant\text{-}\alpha \ MC$. Notice that, if we are in a stationary environment (like earlier. For some reason, donâ€™t use incremental implementation), the $\alpha$ is equals to $\frac{1}{N(S_t)}$. whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(S_t)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t+1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update$$V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]$$immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$. In effect, the target for the Monte Carlo update is $G_t$, whereas the target for the TD update is $R_{t+1} + \gamma V(S_{t+1})$. This TD method is called $TD(0)$, or one-step TD. The box below specifies TD(0) completely in procedural form.TD(0)â€™s backup diagram is as follows:Because the TD(0) bases its update in part on an existing estimate, we say that it is a bootstrapping method, like DP. We know that$$\begin{align}v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [G_t \ | \ S_t=s] \\&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} \ | \ S_t=s] \\&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_t=s].\end{align}$$Roughly speaking, Monte Carlo methods use an estimate of (3) as a target, whereas DP methods use an estimate of (5) as a target, The Monte Carlo target is an estimate because the expected value in (3) is not known; a sample return is used in place of the real expected return. The DP target is an estimate not because of the excepted value, which are assumed to be completely provided by a model of the environment (the environment is known for the DP methods), but because $v_{\pi}(S_{t+1})$ is not known and the current estimate, $V(S_{t+1})$, is used instead. The TD target is an estimate for both reasons.Note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_t$ and the better estimate $R_{t+1} + \gamma V(S_{t+1})$. This quantity, called the TD error, arises in various forms throughout reinforcement learning:$$\delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t).$$Notice that the TD error at each time is the error in the estimate made at that time. Because the TD error depends on the next state and the next reward, it is not actually available until one time step later. Also note that if the array $V$ does not change during the episode (as it does not in Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:$$\begin{align}G_t - V(S_t) &amp;= R_{t+1} + \gamma G(S_{t+1}) - V(S_t) + \gamma V(S_{t+1} ) - \gamma V(S_{t+1}) \\&amp;= \delta_t + \gamma (G_{t+1} - V(S_{t+1})) \\&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 (G_{t+1} - V(S_{t+1})) \\&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(G_t-V(S_T)) \\&amp;= \delta_t + \gamma \delta_{t+1} + \gamma^2 \delta_{t+2} + \cdots + \gamma^{T-t-1} \delta_{T-1} + \gamma^{T-t}(0 -0) \\&amp;= \sum_{k=t}^{T-1} \gamma^{k-t} \delta_k.\end{align}$$This identity is not exact if $V$ is updated during the episode (as it is in TD(0)), but if the step size is small then it may still hold approximately. Generalizations of this identity play an important role in the theory and algorithms of temporal-difference learning.Example: Random walkIn this example we empirically compare the prediction abilities of TD(0) and constant-$\alpha$ MC applied to the small Markov reward process shown in the upper part of the figure below. All episodes start in the center state, C, and the proceed either left or right by one state on each step, with equal probability. This behavior can be thought of as due to the combined effect of a fixed policy and an environmentâ€™s state-transition probabilities, but we do not care which; we are concerned only with predicting returns however they are generated. Episodes terminates on the right, a reward of +1 occurs; all other reward are zero. For example, a typical episode might consist of the following state-and-reward sequence: C, 0, B, 0, C, 0, D, 0, E, 1. Because this task is undiscounted, the true value of each state is the probability of terminating on the right if starting from that state. Thus, the true value of the center state is $v_{\pi}(\text{C}) = 0.5$. The true values of all the states, A through E, are $\frac{1}{6}, \frac{2}{6}, \frac{3}{6}, \frac{4}{6}$, and $\frac{5}{6}$. In all cases the approximate value function was initialized to the intermediate value $V(s)=0.5$, for all $s$.Now, let us develop the codes to solve problem.The first, we initialize some truth.1234567891011121314151617# 0 is the left terminal state# 6 is the right terminal state# 1 ... 5 represents A ... Estates = np.zeros(7)states[1:6] = 0.5# For convenience, we assume all rewards are 0# and the left terminal state has value 0, the right terminal state has value 1# This trick has been used in Gambler's Problemstates[6] = 1# set up true state valuestrueValue = np.zeros(7)trueValue[1:6] = np.arange(1, 6) / 6.0trueValue[6] = 1ACTION_LEFT = 0ACTION_RIGHT = 1The below box is the TD(0) algorithm:1234567891011121314151617181920def temporalDifference(states, alpha=0.1, batch=False): state = 3 trajectory = [state] rewards = [0] while True: oldState = state if np.random.binomial(1, 0.5) == ACTION_LEFT: state -= 1 else: state += 1 # Assume all rewards are 0 reward = 0 trajectory.append(state) # TD update if not batch: states[oldState] += alpha * (reward + states[state] - states[oldState]) if state == 6 or state == 0: break rewards.append(reward) return trajectory, rewardsAnd below box is the constant-$\alpha$ Monte Carlo algorithm:1234567891011121314151617181920212223def monteCarlo(states, alpha=0.1, batch=False): state = 3 trajectory = [3] # if end up with left terminal state, all returns are 0 # if end up with right terminal state, all returns are 1 returns = 0 while True: if np.random.binomial(1, 0.5) == ACTION_LEFT: state -= 1 else: state += 1 trajectory.append(state) if state == 6: returns = 1.0 break elif state == 0: returns = 0.0 break if not batch: for state_ in trajectory[:-1]: # MC update states[state_] += alpha * (returns - states[state_]) return trajectory, [returns] * (len(trajectory) - 1)First of all, let us test the performance of the TD(0) algorithm:1234567891011121314def stateValue(): episodes = [0, 1, 10, 100] currentStates = np.copy(states) plt.figure(1) axisX = np.arange(0, 7) for i in range(0, episodes[-1] + 1): if i in episodes: plt.plot(axisX, currentStates, label=str(i) + ' episodes') temporalDifference(currentStates) plt.plot(axisX, trueValue, label='true values') plt.xlabel('state') plt.legend() stateValue()Results are as follows:And then let us show the RMS error of the TD(0) algorithm and constant-$\alpha$ Monte Carlo algorithm, for various $\alpha$ values:12345678910111213141516171819202122232425262728293031def RMSError(): # I'm lazy here, so do not let same alpha value appear in both arrays # For example, if in TD you want to use alpha = 0.2, then in MC you can use alpha = 0.201 TDAlpha = [0.15, 0.1, 0.05] MCAlpha = [0.01, 0.02, 0.03, 0.04] episodes = 100 + 1 runs = 100 plt.figure(2) axisX = np.arange(0, episodes) for alpha in TDAlpha + MCAlpha: totalErrors = np.zeros(episodes) if alpha in TDAlpha: method = 'TD' else: method = 'MC' for run in range(0, runs): errors = [] currentStates = np.copy(states) for i in range(0, episodes): errors.append(np.sqrt(np.sum(np.power(trueValue - currentStates, 2)) / 5.0)) if method == 'TD': temporalDifference(currentStates, alpha=alpha) else: monteCarlo(currentStates, alpha=alpha) totalErrors += np.asarray(errors) totalErrors /= runs plt.plot(axisX, totalErrors, label=method + ', alpha=' + str(alpha)) plt.xlabel('episodes') plt.legend() RMSError()Results are as follows:We can see, the TD method was consistently better than the MC method on this task.Now, suppose that there is available only a finite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning method is to present the experience repeatedly until the method converges upon an answer. We call this batch updating.Example: Random walk under batch updatingAfter each new episodes, all episodes seen so far were treated as a batch. They were repeatedly presented to the algorithm.12345678910111213141516171819202122232425262728293031323334353637def batchUpdating(method, episodes, alpha=0.001): # perform 100 independent runs runs = 100 totalErrors = np.zeros(episodes - 1) for run in range(0, runs): currentStates = np.copy(states) errors = [] # track shown trajectories and reward/return sequences trajectories = [] rewards = [] for ep in range(1, episodes): print('Run:', run, 'Episode:', ep) if method == 'TD': trajectory_, rewards_ = temporalDifference(currentStates, batch=True) else: trajectory_, rewards_ = monteCarlo(currentStates, batch=True) trajectories.append(trajectory_) rewards.append(rewards_) while True: # keep feeding our algorithm with trajectories seen so far until state value function converges updates = np.zeros(7) for trajectory_, rewards_ in zip(trajectories, rewards): for i in range(0, len(trajectory_) - 1): if method == 'TD': updates[trajectory_[i]] += rewards_[i] + currentStates[trajectory_[i + 1]] - currentStates[trajectory_[i]] else: updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]] updates *= alpha if np.sum(np.abs(updates)) &lt; 1e-3: break # perform batch updating currentStates += updates # calculate rms error errors.append(np.sqrt(np.sum(np.power(currentStates - trueValue, 2)) / 5.0)) totalErrors += np.asarray(errors) totalErrors /= runs return totalErrorsNotice that the core codes:12345678910111213141516while True: # keep feeding our algorithm with trajectories seen so far until state # value function converges updates = np.zeros(7) for trajectory_, rewards_ in zip(trajectories, rewards): for i in range(0, len(trajectory_) - 1): if method == 'TD': updates[trajectory_[i]] += rewards_[i] + \ currentStates[trajectory_[i + 1]] - currentStates[trajectory_[i]] else: updates[trajectory_[i]] += rewards_[i] - currentStates[trajectory_[i]] updates *= alpha if np.sum(np.abs(updates)) &lt; 1e-3: break # perform batch updating currentStates += updatesEither TD methods or MC methods, the target is to minimize the TD error (or MC error, I say).The result is as follows:Under batch training, constant-$\alpha$ MC converges to value, $V(s)$, that are sample averages of the actual returns experienced after visiting each state $s$. These are optimal estimate in the sense that they minimize the mean-squared error from the actual returns in the training set. In this sense it is surprising that the batch TD method was able to perform better according to the root mean-squared error measure shown in the top figure. How is it that batch TD was able to perform better than this optimal methods? Consider the example in below box:Example illustrates a general difference between the estimates founds by batch TD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always find the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always finds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the certainty-equivalence estimate.Sarsa$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\right]$$This update is done after every transition from a nonterminal state $S_t$. If $S_{t+1}$ is terminal, then $Q(S_{t+1}, A_{t+1})$ is defined as zero. This rule uses every element of the quintuple of events, $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, that make up a transition from one state-action pair to the next. This quintuple gives rise to the name Sarsa for the algorithm. The backup diagram for Sarsa is as shown to the bottom.The general form of the Sarsa control algorithm is given in the box below.Example: Windy GridworldThe figure below is a standard grid-world, with start and goal states, but with one diï¬€erence: there is a crosswind upward through the middle of the grid. The actions are the standard fourâ€”up, down,right, and leftâ€”but in the middle region the resultant next states are shifted upward by a â€œwind,â€ the strength of which varies from column to column. The strength of the wind is given below each column, in number of cells shifted upward. For example, if you are one cell to the right of the goal, then the action left takes you to the cell just above the goal. Let us treat this as an undiscounted episodic task, with constant rewards of âˆ’1 until the goal state is reached.To demonstrate the problem clearly, we use the OpenAI gym toolkit to develop the algorithm.First of all, we need to define a environment (the windy grid world):12345# represents every action as a integerUP = 0RIGHT = 1DOWN = 2LEFT = 3The environment is a class that inherit the gym default class discrete.DiscreteEnv (shows that the states are discrete):1class WindyGridworldEnv(discrete.DiscreteEnv)First we need to construct our world:1234567891011121314151617181920212223242526272829def __init__(self): self.shape = (7, 10) # the number of all states nS = np.prod(self.shape) # the number of all actions nA = 4 # Wind strength winds = np.zeros(self.shape) winds[:,[3,4,5,8]] = 1 winds[:,[6,7]] = 2 # Calculate transition probabilities # P is the transition matrix P = &#123;&#125; for s in range(nS): position = np.unravel_index(s, self.shape) P[s] = &#123; a : [] for a in range(nA) &#125; P[s][UP] = self._calculate_transition_prob(position, [-1, 0], winds) P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1], winds) P[s][DOWN] = self._calculate_transition_prob(position, [1, 0], winds) P[s][LEFT] = self._calculate_transition_prob(position, [0, -1], winds) # We always start in state (3, 0) isd = np.zeros(nS) isd[np.ravel_multi_index((3,0), self.shape)] = 1.0 super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)This is natural, uh? Notice that there is a method called _calculate_transition_prob:123456def _calculate_transition_prob(self, current, delta, winds): new_position = np.array(current) + np.array(delta) + np.array([-1, 0]) * winds[tuple(current)] new_position = self._limit_coordinates(new_position).astype(int) new_state = np.ravel_multi_index(tuple(new_position), self.shape) is_done = tuple(new_position) == (3, 7) return [(1.0, new_state, -1.0, is_done)]and _limit_corrdinates method:123456def _limit_coordinates(self, coord): coord[0] = min(coord[0], self.shape[0] - 1) coord[0] = max(coord[0], 0) coord[1] = min(coord[1], self.shape[1] - 1) coord[1] = max(coord[1], 0) return coordIt is worth to mention that the default gym environment class has some useful parameters: nS, nA, P and is_done. nS is the total number of states and nA is the total number of actions (here assume all states only could take the same fixed actions). P is the state transition matrix, the default environment class has a step method (accept a parameter action) that could generates episode automatically according the P and is_done that represents whether a state is terminal state or not.Finally, we define a output method for pretty show the result:123456789101112131415161718192021222324def _render(self, mode='human', close=False): if close: return outfile = StringIO() if mode == 'ansi' else sys.stdout for s in range(self.nS): position = np.unravel_index(s, self.shape) # print(self.s) if self.s == s: output = " x " elif position == (3,7): output = " T " else: output = " o " if position[1] == 0: output = output.lstrip() if position[1] == self.shape[1] - 1: output = output.rstrip() output += "\n" outfile.write(output) outfile.write("\n")Then, let us test our modelï¼š12345678910111213141516171819202122env = WindyGridworldEnv()print(env.reset())env.render()print(env.step(1))env.render()print(env.step(1))env.render()print(env.step(1))env.render()print(env.step(2))env.render()print(env.step(1))env.render()print(env.step(1))env.render()The results are as follows:Each state transition, the step method return a tuple (next_state, reward, is_done, some_extra_info).Next, we define the episodes generation policy:def make_epsilon_greedy_policy(Q, epsilon, nA):1234567891011121314151617181920"""Creates an epsilon-greedy policy based on a given Q-function and epsilon.Args: Q: A dictionary that maps from state -&gt; action-values. Each value is a numpy array of length nA (see below) epsilon: The probability to select a random action . float between 0 and 1. nA: Number of actions in the environment.Returns: A function that takes the observation as an argument and returns the probabilities for each action in the form of a numpy array of length nA."""def policy_fn(observation): A = np.ones(nA, dtype=float) * epsilon / nA best_action = np.argmax(Q[observation]) A[best_action] += (1.0 - epsilon) return Areturn policy_fnNow, let us implement the sarsa algorithm:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1): """ SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy. Args: env: OpenAI environment. num_episodes: Number of episodes to run for. discount_factor: Lambda time discount factor. alpha: TD learning rate. epsilon: Chance the sample a random action. Float betwen 0 and 1. Returns: A tuple (Q, stats). Q is the optimal action-value function, a dictionary mapping state -&gt; action values. stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards. """ # The final action-value function. # A nested dictionary that maps state -&gt; (action -&gt; action-value). Q = defaultdict(lambda: np.zeros(env.action_space.n)) # Keeps track of useful statistics stats = plotting.EpisodeStats( episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes)) # The policy we're following policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n) for i_episode in range(num_episodes): # Print out which episode we're on, useful for debugging. if (i_episode + 1) % 100 == 0: print("\rEpisode &#123;&#125;/&#123;&#125;.".format(i_episode + 1, num_episodes), end="") sys.stdout.flush() # Implement this! state = env.reset() action_probs = policy(state) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) for t in itertools.count(): next_state, reward, is_done, _ = env.step(action) next_action_probs = policy(next_state) stats.episode_rewards[i_episode] += reward stats.episode_lengths[i_episode] = t next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs) Q[state][action] += alpha * (reward + discount_factor * Q[next_state][next_action] - Q[state][action]) if is_done: break state = next_state action = next_action return Q, statsFor understand easily, we put the pesudo-code here again:The results (with $\varepsilon=0.1,\ \alpha=0.5$) are as follows:The increasing slope (bottom figure) of the graph shows that the goal is reached more and more quickly over time. Note that Monte Carlo methods cannot easily be used on this task because termination is not guaranteed for all policies. If a policy was ever found that caused the agent to stay in the same state, then the next episode would never end. Step-by-step learning methods such as Sarsa do not have this problem because they quickly learn during the episode that suchpolicies are poor, and switch to something else.Q-learningOne of the early breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning, defined by$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t)\right]$$The algorithm is shown in procedural form in the box below:And below is the backup diagram:Example: Cliff WalkingThis grid world example compares Sarsa and Q-learning, highlighting the difference between on-policy (Sarsa) and off-policy (Q-learning) methods. Consider the grid world shown in the figure below:The same as earlier, we define the environment first. But the new environment just changes a little, so we just paste the code here.Let us test the environment first:12345678910111213141516env = CliffWalkingEnv()print(env.reset())env.render()print(env.step(0))env.render()print(env.step(1))env.render()print(env.step(1))env.render()print(env.step(2))env.render()Not bad.Then, let us develop the Q-learning algorithm (the episodes generation policy is not change):12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1): """ Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy while following an epsilon-greedy policy Args: env: OpenAI environment. num_episodes: Number of episodes to run for. discount_factor: Lambda time discount factor. alpha: TD learning rate. epsilon: Chance the sample a random action. Float betwen 0 and 1. Returns: A tuple (Q, episode_lengths). Q is the optimal action-value function, a dictionary mapping state -&gt; action values. stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards. """ # The final action-value function. # A nested dictionary that maps state -&gt; (action -&gt; action-value). Q = defaultdict(lambda: np.zeros(env.action_space.n)) # Keeps track of useful statistics stats = plotting.EpisodeStats( episode_lengths=np.zeros(num_episodes), episode_rewards=np.zeros(num_episodes)) # The policy we're following policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n) for i_episode in range(num_episodes): # Print out which episode we're on, useful for debugging. if (i_episode + 1) % 100 == 0: print("\rEpisode &#123;&#125;/&#123;&#125;.".format(i_episode + 1, num_episodes), end="") sys.stdout.flush() # Reset the environment and pick the first action state = env.reset() # One step in the environment # total_reward = 0.0 for t in itertools.count(): # Take a step action_probs = policy(state) action = np.random.choice(np.arange(len(action_probs)), p=action_probs) next_state, reward, done, _ = env.step(action) # Update statistics stats.episode_rewards[i_episode] += reward stats.episode_lengths[i_episode] = t # TD Update best_next_action = np.argmax(Q[next_state]) td_target = reward + discount_factor * Q[next_state][best_next_action] td_delta = td_target - Q[state][action] Q[state][action] += alpha * td_delta if done: break state = next_state return Q, statsResults ($\varepsilon=0.1$) are as follows:For compare convenience, we put the result of Sarsa here again:We can see, for average, After an initial transient, Q-learning learns values for the optimal policy, that which travels right along the edge of the cliï¬€. Unfortunately, this results in its occasionally falling oï¬€ the cliï¬€ because of the Îµ-greedy action selection. Sarsa, on the other hand, takes the action selection into account and learns the longer but safer path through the upper part of thegrid. Although Q-learning actually learns the values of the optimal policy, its online performance is worse than that of Sarsa, which learns the roundabout policy. Of course, if Îµ were gradually reduced, then both methods would asymptotically converge to the optimal policy.Expected SarsaConsider the learning algorithm that is just like Q-learning except that instead of the maximum over next state-action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule$$\begin{align}Q(S_t, A_t) &amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \mathbb{E}[Q(S_{t+1}, A_{t+1} \ | \ S_{t+1})] - Q(S_t, A_t) \right ] \\&amp;\leftarrow Q(S_t, A_t) + \alpha \left [ R_{t+1} + \gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1}, a) - Q(S_t, A_t) \right ],\end{align}$$but that otherwise follows the schema of Q-learning. Its backup diagram is shown below:For compare the results on the cliff-walking task with Excepted Sarsa with Sarsa and Q-learning, we develop another codes (here we are not use the OpenAI gym toolkit).The first we define some truth of the environment:1234567891011121314151617181920212223242526272829303132# world heightWORLD_HEIGHT = 4# world widthWORLD_WIDTH = 12# probability for explorationEPSILON = 0.1# step sizeALPHA = 0.5# gamma for Q-Learning and Expected SarsaGAMMA = 1# all possible actionsACTION_UP = 0ACTION_DOWN = 1ACTION_LEFT = 2ACTION_RIGHT = 3actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]# initial state action pair valuesstateActionValues = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))startState = [3, 0]goalState = [3, 11]# reward for each action in each stateactionRewards = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))actionRewards[:, :, :] = -1.0actionRewards[2, 1:11, ACTION_DOWN] = -100.0actionRewards[3, 0, ACTION_RIGHT] = -100.0And then we define the state transitions:123456789101112131415# set up destinations for each action in each stateactionDestination = []for i in range(0, WORLD_HEIGHT): actionDestination.append([]) for j in range(0, WORLD_WIDTH): destinaion = dict() destinaion[ACTION_UP] = [max(i - 1, 0), j] destinaion[ACTION_LEFT] = [i, max(j - 1, 0)] destinaion[ACTION_RIGHT] = [i, min(j + 1, WORLD_WIDTH - 1)] if i == 2 and 1 &lt;= j &lt;= 10: destinaion[ACTION_DOWN] = startState else: destinaion[ACTION_DOWN] = [min(i + 1, WORLD_HEIGHT - 1), j] actionDestination[-1].append(destinaion)actionDestination[3][0][ACTION_RIGHT] = startStateWe also need a policy to generate the next action according to the current state:123456# choose an action based on epsilon greedy algorithmdef chooseAction(state, stateActionValues): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(actions) else: return np.argmax(stateActionValues[state[0], state[1], :])The stateActionValues just is the Q.Then, let us develop the Sarsa (and Excepted Sarsa) algorithm:123456789101112131415161718192021222324252627282930313233# an episode with Sarsa# @stateActionValues: values for state action pair, will be updated# @expected: if True, will use expected Sarsa algorithm# @stepSize: step size for updating# @return: total rewards within this episodedef sarsa(stateActionValues, expected=False, stepSize=ALPHA): currentState = startState currentAction = chooseAction(currentState, stateActionValues) rewards = 0.0 while currentState != goalState: newState = actionDestination[currentState[0]][currentState[1]][currentAction] newAction = chooseAction(newState, stateActionValues) reward = actionRewards[currentState[0], currentState[1], currentAction] rewards += reward if not expected: valueTarget = stateActionValues[newState[0], newState[1], newAction] else: # calculate the expected value of new state valueTarget = 0.0 actions_list = stateActionValues[newState[0], newState[1], :] bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist() for action in actions: if action in bestActions: valueTarget += ((1.0 - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[0], newState[1], action] else: valueTarget += EPSILON / len(actions) * stateActionValues[newState[0], newState[1], action] valueTarget *= GAMMA # Sarsa update stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * (reward + valueTarget - stateActionValues[currentState[0], currentState[1], currentAction]) currentState = newState currentAction = newAction return rewardsBecause we develop the Sarsa algorithm earlier, so we just concentrate on the Excepted Sarsa algorithm here:123456789# calculate the expected value of new statevalueTarget = 0.0actions_list = stateActionValues[newState[0], newState[1], :]bestActions = np.argwhere(actions_list == np.amax(actions_list)).flatten().tolist()for action in actions: if action in bestActions: valueTarget += ((1.0 - EPSILON) / len(bestActions) + EPSILON / len(actions)) * stateActionValues[newState[0], newState[1], action] else: valueTarget += EPSILON / len(actions) * stateActionValues[newState[0], newState[1], action]By the way, let us develop the Q-learning algorithm again:12345678910111213141516171819# an episode with Q-Learning# @stateActionValues: values for state action pair, will be updated# @expected: if True, will use expected Sarsa algorithm# @stepSize: step size for updating# @return: total rewards within this episodedef qLearning(stateActionValues, stepSize=ALPHA): currentState = startState rewards = 0.0 while currentState != goalState: currentAction = chooseAction(currentState, stateActionValues) reward = actionRewards[currentState[0], currentState[1], currentAction] rewards += reward newState = actionDestination[currentState[0]][currentState[1]][currentAction] # Q-Learning update stateActionValues[currentState[0], currentState[1], currentAction] += stepSize * ( reward + GAMMA * np.max(stateActionValues[newState[0], newState[1], :]) - stateActionValues[currentState[0], currentState[1], currentAction]) currentState = newState return rewardsNow we can see the optimal policy in each state of both algorithm (we are not mentioned earlier):1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# print optimal policydef printOptimalPolicy(stateActionValues): optimalPolicy = [] for i in range(0, WORLD_HEIGHT): optimalPolicy.append([]) for j in range(0, WORLD_WIDTH): if [i, j] == goalState: optimalPolicy[-1].append('G') continue bestAction = np.argmax(stateActionValues[i, j, :]) if bestAction == ACTION_UP: optimalPolicy[-1].append('U') elif bestAction == ACTION_DOWN: optimalPolicy[-1].append('D') elif bestAction == ACTION_LEFT: optimalPolicy[-1].append('L') elif bestAction == ACTION_RIGHT: optimalPolicy[-1].append('R') for row in optimalPolicy: print(row)# averaging the reward sums from 10 successive episodesaverageRange = 10# episodes of each runnEpisodes = 500# perform 20 independent runsruns = 20rewardsSarsa = np.zeros(nEpisodes)rewardsQLearning = np.zeros(nEpisodes)for run in range(0, runs): stateActionValuesSarsa = np.copy(stateActionValues) stateActionValuesQLearning = np.copy(stateActionValues) for i in range(0, nEpisodes): # cut off the value by -100 to draw the figure more elegantly rewardsSarsa[i] += max(sarsa(stateActionValuesSarsa), -100) rewardsQLearning[i] += max(qLearning(stateActionValuesQLearning), -100)# averaging over independt runsrewardsSarsa /= runsrewardsQLearning /= runs# averaging over successive episodessmoothedRewardsSarsa = np.copy(rewardsSarsa)smoothedRewardsQLearning = np.copy(rewardsQLearning)for i in range(averageRange, nEpisodes): smoothedRewardsSarsa[i] = np.mean(rewardsSarsa[i - averageRange: i + 1]) smoothedRewardsQLearning[i] = np.mean(rewardsQLearning[i - averageRange: i + 1])# display optimal policyprint('Sarsa Optimal Policy:')printOptimalPolicy(stateActionValuesSarsa)print('Q-Learning Optimal Policy:')printOptimalPolicy(stateActionValuesQLearning)The results are as follows (emits the results of the changes of reward):Now let us compare the three algorithms:123456789101112131415161718192021222324252627282930313233343536373839404142stepSizes = np.arange(0.1, 1.1, 0.1) nEpisodes = 1000 runs = 10 ASY_SARSA = 0 ASY_EXPECTED_SARSA = 1 ASY_QLEARNING = 2 INT_SARSA = 3 INT_EXPECTED_SARSA = 4 INT_QLEARNING = 5 methods = range(0, 6) performace = np.zeros((6, len(stepSizes))) for run in range(0, runs): for ind, stepSize in zip(range(0, len(stepSizes)), stepSizes): stateActionValuesSarsa = np.copy(stateActionValues) stateActionValuesExpectedSarsa = np.copy(stateActionValues) stateActionValuesQLearning = np.copy(stateActionValues) for ep in range(0, nEpisodes): print('run:', run, 'step size:', stepSize, 'episode:', ep) sarsaReward = sarsa(stateActionValuesSarsa, expected=False, stepSize=stepSize) expectedSarsaReward = sarsa(stateActionValuesExpectedSarsa, expected=True, stepSize=stepSize) qLearningReward = qLearning(stateActionValuesQLearning, stepSize=stepSize) performace[ASY_SARSA, ind] += sarsaReward performace[ASY_EXPECTED_SARSA, ind] += expectedSarsaReward performace[ASY_QLEARNING, ind] += qLearningReward if ep &lt; 100: performace[INT_SARSA, ind] += sarsaReward performace[INT_EXPECTED_SARSA, ind] += expectedSarsaReward performace[INT_QLEARNING, ind] += qLearningReward performace[:3, :] /= nEpisodes * runs performace[3:, :] /= runs * 100 labels = ['Asymptotic Sarsa', 'Asymptotic Expected Sarsa', 'Asymptotic Q-Learning', 'Interim Sarsa', 'Interim Expected Sarsa', 'Interim Q-Learning'] plt.figure(2) for method, label in zip(methods, labels): plt.plot(stepSizes, performace[method, :], label=label) plt.xlabel('alpha') plt.ylabel('reward per episode') plt.legend()The results are as follows:As an on-policy method, Expected Sarsa retains the signiï¬cant advantage of Sarsa over Q-learning on this problem. In addition, Expected Sarsa shows a signiï¬cant improvement over Sarsa over a wide range of values for the step-size parameter Î±. In cliï¬€ walking the state transitions are all deterministic and all randomness comes from the policy. In such cases, Expected Sarsa can safely set Î± = 1 without suï¬€ering any degradation of asymptotic performance, whereas Sarsa can only perform well in the long run at a small value of Î±, at which short-term performance is poor. In this and other examples there is a consistent empirical advantage of Expected Sarsa over Sarsa. Except for the small additional computational cost, Expected Sarsa may completely dominate both of the other more-well-known TD control algorithms.Double Q-learningAll the control algorithms that we have discussed so far involve maximization in the construction of their target policies. For example, in Q-learning the target policy is the greedy policy given the current action values, which is deï¬ned with a max, and in Sarsa the policy is often Îµ-greedy, which also involves a maximization operation. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a signiï¬cant positive bias. To see why, consider a single state s where there are many actions a whose true values, $q(s, a)$ are all zero but whose estimated values, $Q(s, a)$, are uncertain and thus distributed some above and some below zero. The maximum of the true values is zero, but the maximum of the estimates is positive, a positive bias. We call this maximizationbias.Example: Maximization BiasWe have a small MDP:the expected return for any trajectory starting with left (from B) is âˆ’0.1, and thus taking left in state A is always a mistake. Nevertheless, our control methods may favor left because of maximization bias making B appear to have a positive value. The results (paste later) shows that Q-learning with Îµ-greedy action selection initially learns to strongly favor the left action on this example. Even at asymptote, Q-learning takes the left action about 5% more often than is optimal at our parameter settings (Îµ = 0.1, Î± = 0.1, and Î³ = 1).We could use the Double Q-learning algorithm to avoid this problem. One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. Suppose we divided the plays in two sets and used them to learn two independent estimates.Of course there are also doubled versions of Sarsa and Expected Sarsa.Now let us develop the both algorithms and compare their performance on the earlier example. First we define the problem environment:123456789101112131415161718192021222324252627282930313233343536# state ASTATE_A = 0# state BSTATE_B = 1# use one terminal stateSTATE_TERMINAL = 2# starts from state ASTATE_START = STATE_A# possible actions in AACTION_A_RIGHT = 0ACTION_A_LEFT = 1# possible actions in B, maybe 10 actionsactionsOfB = range(0, 10)# all possible actionsstateActions = [[ACTION_A_RIGHT, ACTION_A_LEFT], actionsOfB]# state action pair values, if a state is a terminal state, then the value is always 0stateActionValues = [np.zeros(2), np.zeros(len(actionsOfB)), np.zeros(1)]# set up destination for each state and each actionactionDestination = [[STATE_TERMINAL, STATE_B], [STATE_TERMINAL] * len(actionsOfB)]# probability for explorationEPSILON = 0.1# step sizeALPHA = 0.1# discount for max valueGAMMA = 1.0And we need a policy to take an action:123456# choose an action based on epsilon greedy algorithmdef chooseAction(state, stateActionValues): if np.random.binomial(1, EPSILON) == 1: return np.random.choice(stateActions[state]) else: return argmax(stateActionValues[state])After take an action, we get the reward:12345# take @action in @state, return the rewarddef takeAction(state, action): if state == STATE_A: return 0 return np.random.normal(-0.1, 1)Next, we develop the Double Q-learning algorithm:12345678910111213141516171819202122232425262728293031323334# if there are two state action pair value array, use double Q-Learning# otherwise use normal Q-Learningdef qLearning(stateActionValues, stateActionValues2=None): currentState = STATE_START # track the # of action left in state A leftCount = 0 while currentState != STATE_TERMINAL: if stateActionValues2 is None: currentAction = chooseAction(currentState, stateActionValues) else: # derive a action form Q1 and Q2 currentAction = chooseAction(currentState, [item1 + item2 for item1, item2 in zip(stateActionValues, stateActionValues2)]) if currentState == STATE_A and currentAction == ACTION_A_LEFT: leftCount += 1 reward = takeAction(currentState, currentAction) newState = actionDestination[currentState][currentAction] if stateActionValues2 is None: currentStateActionValues = stateActionValues targetValue = np.max(currentStateActionValues[newState]) else: if np.random.binomial(1, 0.5) == 1: currentStateActionValues = stateActionValues anotherStateActionValues = stateActionValues2 else: currentStateActionValues = stateActionValues2 anotherStateActionValues = stateActionValues bestAction = argmax(currentStateActionValues[newState]) targetValue = anotherStateActionValues[newState][bestAction] # Q-Learning update currentStateActionValues[currentState][currentAction] += ALPHA * ( reward + GAMMA * targetValue - currentStateActionValues[currentState][currentAction]) currentState = newState return leftCountAnd now, let us solve the example problem:12345678910111213141516171819202122232425262728# each independent run has 300 episodes episodes = 300 leftCountsQ = np.zeros(episodes) leftCountsDoubleQ = np.zeros(episodes) runs = 1000 for run in range(0, runs): print('run:', run) stateActionValuesQ = [np.copy(item) for item in stateActionValues] stateActionValuesDoubleQ1 = [np.copy(item) for item in stateActionValues] stateActionValuesDoubleQ2 = [np.copy(item) for item in stateActionValues] leftCountsQ_ = [0] leftCountsDoubleQ_ = [0] for ep in range(0, episodes): leftCountsQ_.append(leftCountsQ_[-1] + qLearning(stateActionValuesQ)) leftCountsDoubleQ_.append(leftCountsDoubleQ_[-1] + qLearning(stateActionValuesDoubleQ1, stateActionValuesDoubleQ2)) del leftCountsQ_[0] del leftCountsDoubleQ_[0] leftCountsQ += np.asarray(leftCountsQ_, dtype='float') / np.arange(1, episodes + 1) leftCountsDoubleQ += np.asarray(leftCountsDoubleQ_, dtype='float') / np.arange(1, episodes + 1) leftCountsQ /= runs leftCountsDoubleQ /= runs plt.figure() plt.plot(leftCountsQ, label='Q-Learning') plt.plot(leftCountsDoubleQ, label='Double Q-Learning') plt.plot(np.ones(episodes) * 0.05, label='Optimal') plt.xlabel('episodes') plt.ylabel('% left actions from A') plt.legend()Ok, results are as follows:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning Resources]]></title>
      <url>%2F2017%2F06%2F30%2FReinforcement-Learning-Resources%2F</url>
      <content type="text"><![CDATA[BooksSuttonâ€™s book has new update (draft, version 2017) !Algorithms for Reinforcement Learning (Morgan)PapersReinforcement LearningDeep Reinforcement Learning with Double Q-learningSummaryProjectPrioritized Experience ReplaySummaryDueling Network Architectures for Deep Reinforcement LearningSummaryProjectLearning Tetris Using the Noisy Cross-Entropy MethodSummaryProjectDoubly Robust Off-policy Value Evaluation for Reinforcement LearningSummaryContinuous State-Space Models for Optimal Sepsis Treatment: a Deep Reinforcement Learning ApproachSummaryA Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care UnitsSummaryDeep Reinforcement Learning, Decision Making and Control (ICML 2017 Tutorial)SummaryMaximum Entropy Deep Inverse Reinforcement LearningSummaryMaximum Entropy Inverse Reinforcement LearningSummaryApprenticeship Learning via Inverse Reinforcement LearningSummaryDeep LearningCheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep LearningSummaryâ€‹ProjectsUsing Keras and Deep Q-Network to Play FlappyBirdBlogsDemystifying Deep Reinforcement Learning]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[A simple AI car]]></title>
      <url>%2F2017%2F06%2F27%2FA-simple-AI-car%2F</url>
      <content type="text"><![CDATA[I. å®šä¹‰é¡¹ç›®æ¦‚è¿°é¡¹ç›®åœ°å€ï¼šhttps://github.com/ewanlee/rl_caræœ€è¿‘ï¼Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ååˆ†ç«çƒ­ã€‚ä½†æ˜¯ï¼Œè‡ªåŠ¨é©¾é©¶é—®é¢˜æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ é›†å¤§æˆçš„é—®é¢˜ï¼Œååˆ†çš„å¤æ‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å¯ä»¥è®¾è®¡å‡ºä¸€ä¸ªç®€å•çš„å­¦ä¹ çŽ¯å¢ƒèƒ½å¤Ÿå¯¹è‡ªåŠ¨é©¾é©¶é—®é¢˜è¿›è¡Œæ¨¡æ‹Ÿï¼Œå¹¶ä¸”ä¸éœ€è¦GPU ï¼ˆä¸»è¦æ˜¯å¤ªè´µï¼‰ã€‚æˆ‘ä»¬çš„å­¦ä¹ çŽ¯å¢ƒå€Ÿé‰´äº†Matt Harveyâ€™s virtual car[1] çš„çŽ¯å¢ƒè®¾ç½®ã€‚è¿ç”¨äº† TensorFlowï¼Œ Python 2.7 ä»¥åŠ PyGame 5.0. æœ¬é¡¹ç›®ä¸­è¿ç”¨äº†æ·±åº¦Qå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä½†æ˜¯ä¸ºäº†ç¬¦åˆæˆ‘ä»¬ä¸Šé¢æåˆ°çš„è¦æ±‚ï¼Œæˆ‘ä»¬åŽ»æŽ‰äº†è¯¥ç®—æ³•ä¸­ â€œæ·±åº¦â€ çš„éƒ¨åˆ†ã€‚ä»£ç è®¾è®¡çš„ä¸€äº›æ€æƒ³å€Ÿé‰´äº† songotrekâ€™s Qå­¦ä¹ ç®—æ³•çš„TensorFlowå®žçŽ° [2].é—®é¢˜æè¿°å›¾ç‰‡æ¥æºäºŽ[1]æˆ‘ä»¬æ‰€è¦è§£å†³çš„é—®é¢˜å°±æ˜¯è®¾è®¡ä¸€ä¸ªç®—æ³•ä½¿å¾—æ¨¡æ‹Ÿå°è½¦èƒ½å¤Ÿè‡ªåŠ¨è¡Œé©¶ã€‚ä¸Šå›¾å°±æ˜¯æˆ‘ä»¬å®žéªŒç”¨çš„çŽ¯å¢ƒã€‚å¯ä»¥çœ‹å‡ºï¼Œå®ƒè¶³å¤Ÿç®€å•ï¼Œä½†æ˜¯è¶³å¤Ÿè¿›è¡Œä¸€äº›å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„éªŒè¯ã€‚æœ€å°çš„åœ†åœˆæ˜¯æˆ‘ä»¬æ¨¡æ‹Ÿçš„å°è½¦ï¼Œå®ƒæ‹¥æœ‰ä¸‰ä¸ªå£°çº³æ„Ÿåº”å™¨ ï¼ˆä¸‰æ¡ç™½è‰²çš„è™šçº¿ï¼‰ã€‚ä¸‰ä¸ªè¾ƒå¤§çš„åœ†åœˆä»£è¡¨éšœç¢ç‰©ï¼Œå®ƒä¼šéšç€æ—¶é—´çš„å˜åŒ–ç¼“æ…¢ç§»åŠ¨ã€‚å·¦ä¸Šè§’çš„åœ†åœˆä»£è¡¨ä¸€åªåœ¨çŽ¯å¢ƒä¸­æ¸¸èµ°ï¼ˆé€Ÿåº¦ç›¸æ¯”äºŽéšœç¢ç‰©è¦å¿«å¾ˆå¤šï¼‰çš„çŒ«ã€‚åœ†åœˆä¸Šçš„ç¼ºå£è¡¨ç¤ºæœå‘ã€‚æˆ‘ä»¬æ‰€è¦è§£å†³çš„é—®é¢˜å°±æ˜¯å¸Œæœ›å°è½¦å¯ä»¥å°½å¯èƒ½é•¿æ—¶é—´çš„è¿åŠ¨ï¼Œä½†ä¸ä¼šæ’žåˆ°éšœç¢ç‰©æˆ–è€…çŒ«ã€‚çŽ¯å¢ƒéœ€æ±‚Anaconda Python Distribution 2.7 [3]TensorFlow for Anaconda [4]PyGame [5]ï¼Œç”¨äºŽå±•ç¤ºå›¾å½¢ç•Œé¢PyMunk [6]ï¼Œä¸ºäº†æ¨¡æ‹Ÿæ¸¸æˆä¸­çš„ç‰©ç†çŽ¯å¢ƒNumpy [7]Scipy [8]å®žéªŒè¿è¡Œçš„çŽ¯å¢ƒä¸º Ubuntu 16.04 LTS è™šæ‹Ÿæœºï¼Œ è™šæ‹Ÿæœºä¸ºVMware Workstation 12.5.2 build-4638234ã€‚è™šæ‹Ÿæœºè¿è¡Œåœ¨Windows 10 Proä¸Šã€‚æ€§èƒ½åº¦é‡æˆ‘ä»¬çš„baselineæ˜¯ä¸€ä¸ªéšæœºï¼ˆè¡Œä¸ºéšæœºé€‰æ‹©ï¼‰å°è½¦ï¼Œæœ€åŽçš„è¯„ä»·æŒ‡æ ‡æ˜¯æˆ‘ä»¬å®šä¹‰çš„æŒ‡æ ‡scoreï¼Œä»£è¡¨å°è½¦å­˜æ´»çš„æ—¶é—´ï¼ˆåœ¨æ¸¸æˆä¸­ä»£è¡¨å°è½¦å­˜æ´»çš„frameï¼‰ã€‚å¹¶ä¸”ï¼Œscoreæ˜¯è¿›è¡Œ1000æ¬¡å®žéªŒçš„å¹³å‡å€¼ã€‚ä¼˜åŒ–ç›®æ ‡æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ Deep Q Learning [9] è®ºæ–‡ä¸­å®šä¹‰çš„ QMax å€¼ã€‚QMax å€¼æŒ‡çš„æ˜¯åœ¨ä¸€å®šæ—¶é—´èŒƒå›´å†…ï¼Œå¯¹äºŽæ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬ï¼ŒQ å‡½æ•°ï¼ˆä½¿ç”¨ç¥žç»ç½‘ç»œè¿›è¡Œæ‹Ÿåˆï¼‰è¾“å‡ºçš„æœ€å¤§çš„ Q-valueã€‚éšç€agentï¼ˆæ¨¡æ‹Ÿå°è½¦ï¼‰ä¸æ–­è¿›è¡Œå­¦ä¹ ï¼Œå®ƒå°†é‡‡å–æ›´åŠ ä¼˜ç§€çš„ç­–ç•¥ï¼Œå› æ­¤å­˜æ´»æ—¶é—´ä¼šæ›´é•¿ï¼Œé‚£ä¹ˆ Q-value (åœ¨æˆ‘ä»¬çš„å®žéªŒä¸­ä¾¿æ˜¯score) ä¼šè¶Šå¤§ã€‚å¦‚æžœæˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡æ˜¯å¢žå¤§ Q-value çš„ä¸Šç•Œï¼Œä¹Ÿä¾¿ç›¸åº”çš„å¢žå¤§äº† Q-value å€¼ã€‚å­¦ä¹ è¿‡ç¨‹ç›‘æµ‹æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯Tensorflowè‡ªå¸¦çš„TensorBoardæ¥ç›‘æµ‹QMaxä»¥åŠæœ€å¤§scoreçš„å˜åŒ–æƒ…å†µï¼ˆå¸Œæœ›æ•´ä½“è¶‹åŠ¿æ˜¯é€æ¸å¢žå¤§çš„ï¼‰ä¸‹é¢æ˜¯è¿è¡Œè¿‡ç¨‹ä¸­çš„æˆªå›¾ï¼šä¸‹é¢æ˜¯å„ç½‘ç»œå‚æ•°çš„åˆ†å¸ƒå˜åŒ–æƒ…å†µï¼šåˆ†æžæ•°æ®ç”±äºŽå¼ºåŒ–å­¦ä¹ ä»»åŠ¡çš„æ•°æ®é›†ä¸€èˆ¬éƒ½æ˜¯å®žéªŒä¸­äº§ç”Ÿçš„ï¼Œå› æ­¤ä¸éœ€è¦æ”¶é›†æ•°æ®ã€‚åœ¨æ¯ä¸€æ¬¡è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œæ¨¡æ‹ŸçŽ¯å¢ƒæä¾›ä»¥ä¸‹æ•°æ®ï¼ˆè‡ªå·±è®¾è®¡çš„ï¼‰ï¼šs1, s2, s3 ä¸‰ä¸ªå£°çº³ä¼ æ„Ÿå™¨çš„æ•°å€¼ï¼ŒèŒƒå›´æ˜¯[0, 40]ï¼Œæ•´æ•°å€¼ï¼Œä»£è¡¨ä¸‰ä¸ªæ–¹å‘ä¸Šéšœç¢ç‰©çš„è·ç¦»ã€‚èŒƒå›´ç¡®å®šä¸ºè¿™æ ·çš„åŽŸå› æ˜¯ï¼Œä¸ºäº†æ£€æµ‹éšœç¢ç‰©ï¼Œå£°çº³ä¼ æ„Ÿå™¨ä»Žæºå¤´å¼€å§‹ï¼Œé€æ¸å¾€å¤–æŽ¢æµ‹ï¼Œæ¯å‘å¤–æŽ¢æµ‹ä¸€æ¬¡ï¼Œè·ç¦»å°±åŠ 1ï¼ˆå¯ä»¥çœ‹æˆè™šçº¿çš„ç‚¹æ•°ï¼Œå³è™šçº¿æ˜¯ç”±å¤šå°‘ä¸ªç‚¹ç»„æˆçš„ï¼‰ã€‚x ä»£è¡¨xè½´çš„ä½ç½®ï¼ŒèŒƒå›´æ˜¯[0, 1]y ä»£è¡¨yè½´çš„ä½ç½®ï¼ŒèŒƒå›´æ˜¯[0, 1]theta ä»£è¡¨å°è½¦çš„æ–¹å‘ï¼Œå¼§åº¦è¡¨ç¤ºï¼ŒèŒƒå›´æ˜¯[0, 2$\pi$]å°è½¦èƒ½å¤Ÿé‡‡å–çš„åŠ¨ä½œå¦‚ä¸‹ï¼š0ï¼Œä»£è¡¨ç›´èµ°1ï¼Œ å¾€å·¦è½¬0.2å¼§åº¦2ï¼Œ å¾€å³è½¬0.2å¼§åº¦å°è½¦æ¯è¿›è¡Œä¸€æ¬¡åŠ¨ä½œä¼šä½¿å¾—çŠ¶æ€å‘ç”Ÿå˜åŒ–ï¼Œå¹¶ä¸”æœ‰ä»¥ä¸‹è¿”å›žå€¼ï¼šRewardï¼Œä¸€ä¸ª[-100, 10]ä¹‹é—´çš„æ•´æ•°ï¼Œè´Ÿæ•°ä»£è¡¨åŠ¨ä½œäº§ç”Ÿçš„ç»“æžœä¸å¥½ï¼Œæ­£æ•°åˆ™ç›¸åTermianlï¼Œå¸ƒå°”åž‹æ•°æ®ï¼Œä»£è¡¨å°è½¦æ˜¯å¦å­˜æ´»ï¼ˆæ˜¯å¦æ’žåˆ°éšœç¢ç‰©ï¼‰æˆ‘ä»¬å’ŒåŽŸå§‹æ¨¡åž‹[1]ä¸åŒçš„æ˜¯ï¼Œè¾“äº†$s_1, s_2, s_3$ä¸‰ä¸ªç‰¹å¾ä¹‹å¤–ï¼Œé¢å¤–å¢žåŠ äº†$x, y, theta$ä¸‰ä¸ªç‰¹å¾ã€‚å› ä¸ºæˆ‘ä»¬å¸Œæœ›å°è½¦èƒ½å¤Ÿå°½å¯èƒ½å¾€åœ°å›¾ä¸­é—´è¿è¡Œï¼Œè¿œç¦»å¢™å£ã€‚å¹¶ä¸”å½“å®ƒä»¬é è¿‘éšœç¢ç‰©æ—¶ï¼Œèƒ½å¤Ÿé€‰æ‹©æ›´åŠ åˆç†çš„æ–¹å‘èº²é¿ã€‚å€¼å¾—è¯´æ˜Žçš„ä¸€ç‚¹æ˜¯ï¼Œå°è½¦å¦‚ä½•æ£€æµ‹æ˜¯å¦æ’žåˆ°éšœç¢ç‰©çš„é—®é¢˜ã€‚å®žéªŒä¸­ä½¿ç”¨çš„æ–¹æ³•æ˜¯æ£€æµ‹å£°çº³ä¼ æ„Ÿå™¨çš„æ•°å€¼ï¼Œå¦‚æžœæ•°å€¼æ˜¯1ï¼ˆè€Œä¸æ˜¯0ï¼‰å°±è®¤ä¸ºå°è½¦æ’žä¸Šäº†éšœç¢ç‰©ï¼Œå¹¶ç»™å‡ºä¸€ä¸ª-100çš„rewardã€‚æ­¤æ—¶å®žéªŒå°†é‡æ–°å¼€å§‹ï¼Œå°è½¦ä½ç½®çš„é€‰æ‹©æ˜¯æ ¹æ®ç‰©ç†å®šå¾‹æ¨¡æ‹Ÿçš„ï¼Œå³æ ¹æ®ç¢°æ’žçš„è§’åº¦ç»™å°è½¦ä¸€ä¸ªåå‘çš„é€Ÿåº¦ï¼Œå¹¶ä¸”å°è½¦çš„æœå‘éšæœºå˜åŒ–ã€‚è¿™æ ·æ¨¡æ‹Ÿå‡ºä¸€ç§ç¢°æ’žåŽçš„æ··ä¹±çŠ¶æ€ã€‚ç®—æ³•ä¸‹é¢ä»‹ç»Deep Q-learningç®—æ³•ã€‚ä»¥ä¸Šçš„å®žéªŒçŽ¯å¢ƒå¯ä»¥å½¢å¼åŒ–çš„è¡¨è¿°ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚æˆ‘ä»¬æ‹¥æœ‰ä¸€ä¸ªagentï¼ˆå°è½¦ï¼‰ï¼Œåœ¨æ—¶é—´$t$æ—¶å¿…é¡»è¦é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ$a_t$ã€‚Agenté‡‡å–åŠ¨ä½œä¸ŽçŽ¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œä½¿å¾—åœ¨æ—¶é—´$t+1$æ—¶çŠ¶æ€å˜ä¸º$s_{t+1}$ã€‚åŒæ—¶agentæŽ¥æ”¶åˆ°çŽ¯å¢ƒç»™å®ƒçš„ä¸€ä¸ªåé¦ˆ$r_t$ã€‚è¿™æ ·agentå°±æ ¹æ®$(s_t, a_t, s_{t+1}, r_t)$æ¥å†³å®šé‡‡å–çš„åŠ¨ä½œ$a_{t+1}$æ˜¯ä»€ä¹ˆã€‚æ•´ä¸ªé—®é¢˜å°±æ˜¯ä¸æ–­é‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´åˆ°åˆ°è¾¾æŸä¸ªç»“æŸæ¡ä»¶ã€‚æœºå™¨å­¦ä¹ é¢†åŸŸå°†è¿™ä¸ªé—®é¢˜ç§°ä¸ºå¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚æ¯ä¸€ä¸ªåŠ¨ä½œé€šè¿‡rewardè¢« â€œå¼ºåŒ–â€ï¼Œä½¿å¾—agentä¸æ–­æŽ¥è¿‘æˆ‘ä»¬æœŸæœ›å®ƒåˆ°è¾¾çš„çŠ¶æ€ã€‚ä½†æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å­˜åœ¨ä¸€ä¸ªrewardå»¶è¿Ÿçš„é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒæŸä¸€ä¸ªactionçš„å›žæŠ¥å¯èƒ½ä¸æ˜¯å³æ—¶çš„ï¼Œéœ€è¦å¾ˆå¤šæ—¶é—´æ­¥ä¹‹åŽæ‰èƒ½ç¡®å®šã€‚ä¸¾ä¸ªä¾‹å­ï¼Œä¸‹æ£‹çš„è¿‡ç¨‹ä¸­éœ€è¦å¸ƒå±€ï¼Œä½†æ˜¯è¿™ä¸ªå¸ƒå±€å¹¶ä¸ä¼šé©¬ä¸Šç»™ä½ å¸¦æ¥å¥½å¤„ï¼Œéœ€è¦åœ¨ä»¥åŽçš„æŸä¸ªç‰¹å®šæ—¶é—´ï¼Œä½ çš„å¯¹ä¸ŠæŽ‰å…¥äº†ä½ å¾ˆä¹…å‰è®¾ç½®çš„é™·é˜±é‡Œï¼Œè¿™æ—¶å€™æ‰ç»™ä½ å¸¦æ¥å¥½å¤„ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬éœ€è¦é‡‡ç”¨ä¸€ç§æ–¹å¼æ¥å¯¹è¿™ä¸ªé—®é¢˜è¿›è¡Œå»ºæ¨¡ã€‚æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªä»·å€¼å‡½æ•°$Q(s_t, a_t)$ï¼Œå®ƒè¡¨ç¤ºåœ¨çŠ¶æ€$s_t$æ˜¯é‡‡å–$a_t$è¿™ä¸ªåŠ¨ä½œå¸¦æ¥çš„ â€œä»·å€¼â€ï¼Œè€Œä¸æ˜¯rewardï¼Œrewardæ˜¯å³æ—¶çš„ï¼Œä½†æ˜¯ä»·å€¼æ˜¯è‹¥å¹²æ—¶é—´æ­¥å¸¦æ¥çš„rewardçš„æŸç§ç»¼åˆè€ƒé‡ï¼Œæ›´å…·å®žé™…æ„ä¹‰ã€‚é‚£ä¹ˆæŽ¥ä¸‹æ¥çš„é—®é¢˜å°±æ˜¯ä»·å€¼å‡½æ•°åº”å½“å¦‚ä½•å®šä¹‰ã€‚æœ€ç›´è§‚çš„æƒ³æ³•å°±æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå¼ºåŒ–å­¦ä¹ é—®é¢˜å®šä¹‰ä¸ºä¸€ä¸ªåŠ¨æ€è§„åˆ’çš„é—®é¢˜ã€‚è¿™é‡Œæˆ‘ç›´æŽ¥åˆ—å‡ºå…¬å¼ï¼Œä¹Ÿå°±æ˜¯éžå¸¸è‘—åçš„è´å°”æ›¼æ–¹ç¨‹ï¼ˆBellman equationï¼‰ï¼šå¯ä»¥çœ‹åˆ°ï¼Œè§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜æ˜¯ä¸€ä¸ªä¸æ–­è¿­ä»£çš„è¿‡ç¨‹ï¼Œé‚£ä¹ˆå¦‚ä½•åˆå§‹åŒ–Qéžå¸¸é‡è¦ã€‚ä½†å®žé™…ä¸Šï¼Œå¦‚æžœè¿­ä»£æ¬¡æ•°è¶‹ç´§æ— ç©·å¤§æ—¶ï¼ŒQçš„åˆå§‹å€¼å¯¹äºŽæœ€ç»ˆçš„ç»“æžœå¹¶æ²¡æœ‰å½±å“ï¼Œå› æ­¤ä¸€èˆ¬æ¥è¯´åªè¦åˆå§‹åŒ–ä¸ºå‡å€¼ä¸º0çš„é«˜æ–¯ç™½å™ªéŸ³ã€‚å¯¹äºŽå°è§„æ¨¡çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œç”±äºŽçŠ¶æ€çš„Qå€¼éšç€è¿­ä»£æ¬¡æ•°çš„å¢žåŠ ä¼šä¸æ–­æ›´æ–°ï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåœ°æ–¹æ¥å­˜å‚¨è¿™äº›å€¼ã€‚ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸€èˆ¬é‡‡ç”¨ä¸€å¼ è¡¨æ ¼ï¼ˆæ•°ç»„æˆ–å­—å…¸ï¼‰æ¥å­˜å‚¨è¿™äº›å€¼ã€‚ä½†æ˜¯éšç€é—®é¢˜è§„æ¨¡çš„å¢žå¤§ï¼ŒçŠ¶æ€ä¼šæ˜¾è‘—å¢žåŠ ã€‚å¯¹äºŽæˆ‘ä»¬çš„é—®é¢˜ï¼ŒçŠ¶æ€ç©ºé—´æ›´æ˜¯æ— é™çš„ï¼Œå› ä¸ºçŠ¶æ€æ˜¯ç”±æµ®ç‚¹æ•°ç»„æˆçš„ã€‚è¿™æ ·æˆ‘ä»¬å°±ä¸å¯èƒ½æŠŠè¿™äº›çŠ¶æ€å¯¹åº”çš„Qå€¼éƒ½å­˜å‚¨ä¸‹æ¥ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªå¦‚ä¸‹æ‰€ç¤ºçš„ç¥žç»ç½‘ç»œæ¥ä»£æ›¿è¿™äº›è¡¨æ ¼ï¼Œå³æ‰¾å‡ºçŠ¶æ€å’ŒQå€¼ä¹‹é—´çš„ä¸€ä¸ªæ˜ å°„ã€‚è¿™é‡Œå€¼å¾—è¯´æ˜Žçš„æ˜¯ï¼Œç½‘ç»œè¾“å‡ºçš„æ˜¯æ‰€æœ‰åŠ¨ä½œå¯¹åº”çš„Qå€¼ï¼Œè¿™æ˜¯Deep Q-learningç®—æ³•çš„ä¸€ä¸ªåˆ›æ–°ç‚¹ã€‚åœ¨æˆ‘ä»¬çš„å®žéªŒä¸­ï¼Œè¾“å…¥ç»´åº¦æ˜¯6ç»´ï¼ˆ$s_1, s_2, s_3, x, y, theta$ï¼‰ï¼Œè¾“å‡ºæ˜¯3ç»´ï¼ˆå¯¹åº”ä¸‰ä¸ªåŠ¨ä½œ0ï¼Œ 1ï¼Œ 2ï¼‰ã€‚æˆ‘ä»¬é‡‡ç”¨ç™½å™ªéŸ³æ¥åˆå§‹åŒ–ç½‘ç»œã€‚å…·ä½“æ¥è¯´ï¼Œæƒé‡é‡‡ç”¨æ ‡å‡†é«˜æ–¯å™ªéŸ³ï¼Œåå·®åˆå§‹åŒ–ä¸º0.01ã€‚è‡³äºŽè®­ç»ƒè¿‡ç¨‹ï¼ŒDeep Q-learningç®—æ³•é‡‡ç”¨äº†ä¸€ä¸ªtrickã€‚è¯¥ç®—æ³•é‡‡ç”¨äº†ä¸¤ä¸ªå®Œå…¨ç›¸åŒçš„ç½‘ç»œï¼Œå…¶ä¸­ä¸€ä¸ªç”¨æ¥è®­ç»ƒï¼Œå¦ä¸€ä¸ªåˆ™ç”¨æ¥é¢„æµ‹ã€‚è¿™æ ·è¿˜å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ç”¨äºŽè®­ç»ƒç½‘ç»œçš„è®­ç»ƒé›†å¹¶ä¸æ˜¯agentå½“å‰çš„å››å…ƒç»„$(s_t, a_t, s_{t+1}, r_t)$ï¼Œ è€Œæ˜¯ä»Žæœ€è¿‘å››å…ƒç»„åŽ†å²ï¼ˆä¹‹å‰æŸä¸€ä¸ªæ—¶é—´çª—å£ä¸­çš„æ‰€æœ‰å››å…ƒç»„ï¼‰ä¸­éšæœºé‡‡æ ·å‡ºçš„ä¸€ä¸ªminibatchã€‚æˆ‘ä»¬é€šè¿‡è¿™äº›è®­ç»ƒæ ·æœ¬æ¥æ›´æ–°è®­ç»ƒç½‘ç»œçš„å‚æ•°ï¼Œç»è¿‡ä¸€å®šæ—¶é—´çš„è®­ç»ƒä¹‹åŽï¼ŒæŠŠè®­ç»ƒç½‘ç»œçš„å‚æ•°å¤åˆ¶ç»™é¢„æµ‹ç½‘ç»œï¼Œç”¨é¢„æµ‹ç½‘ç»œæ¥ç»§ç»­äº§ç”Ÿè®­ç»ƒæ ·æœ¬ï¼Œä¾›è®­ç»ƒç½‘ç»œä½¿ç”¨ã€‚æ•´ä¸ªç®—æ³•å°±æ˜¯ä¸æ–­é‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´è‡³æ”¶æ•›ã€‚å…·ä½“ç®—æ³•çš„ä¼ªä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š12345678910111213141516Initialize replay memory D to size NInitialize action-value function Q with random weightsfor episode = 1, M do Initialize state s_1 for t = 1, T do With probability Ïµ select random action a_t otherwise select a_t=argmax_a Q(s_t,a; Î¸_i) Execute action a_t in emulator and observe r_t and s_(t+1) Store transition (s_t,a_t,r_t,s_(t+1)) in D Sample a minibatch of transitions (s_j,a_j,r_j,s_(j+1)) from D Set y_j:= r_j for terminal s_(j+1) r_j+Î³*max_(a^') Q(s_(j+1),a'; Î¸_i) for non-terminal s_(j+1) Perform a gradient step on (y_j-Q(s_j,a_j; Î¸_i))^2 with respect to Î¸ end forend forBenchmarkæˆ‘ä»¬å¸Œæœ›ç®—æ³•èƒ½å¤Ÿæ¯”éšæœºé€‰æ‹©æ›´å¥½ã€‚ä¸‹é¢æ˜¯è¿›è¡Œ1000æ¬¡å®žéªŒéšæœºç®—æ³•çš„ç»“æžœï¼šæ–¹æ³•æ•°æ®é¢„å¤„ç†æˆ‘ä»¬åœ¨å®žéªŒä¹‹å‰è¿›è¡Œäº†æ•°æ®çš„æ ‡å‡†åŒ–ï¼Œä½¿å¾—æ‰€æœ‰æ•°æ®éƒ½å¤„äºŽ0åˆ°1ä¹‹é—´ï¼Œè¿™æ ·å¯ä»¥é¿å…æ¢¯åº¦çˆ†ç‚¸ç­‰çŽ°è±¡çš„å‘ç”Ÿã€‚$xï¼Œ y$ è¿™ä¸¤ä¸ªç‰¹å¾æ²¡æœ‰è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå› ä¸ºå·²ç»ç¬¦åˆè¦æ±‚ã€‚$theta$é€šè¿‡é™¤ä»¥$2\pi$è¿›è¡Œæ ‡å‡†åŒ–ã€‚åœ¨æ²¡æœ‰è¿›è¡Œæ ‡å‡†åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬åœ¨å®žéªŒä¸­å‘çŽ°ï¼Œ$theta$çš„å€¼ä¼šè¾¾åˆ°$10^3$è¿™ä¸ªæ•°é‡çº§ï¼Œä½¿å¾—ç½‘ç»œå‘ç”Ÿäº†bias shiftçŽ°è±¡ã€‚$s_1, s_2, s_3$é€šè¿‡é™¤ä»¥40æ¥è¿›è¡Œæ ‡å‡†åŒ–ã€‚æˆ‘ä»¬åŒæ ·è¯•ç€èƒ½å¤Ÿå°†rewardä¹Ÿè¿›è¡Œæ ‡å‡†åŒ–ï¼Œå°†å…¶èŒƒå›´ç¼©å°åˆ°[-1, 1]ã€‚å› ä¸ºDQNè®ºæ–‡ä¸­åŒæ ·ä½¿ç”¨äº†è¿™ç§æ–¹æ³•ï¼Œä½¿å¾—è¯¥ç®—æ³•åº”ç”¨åœ¨ä¸åŒçš„Atariæ¸¸æˆä¸Šæ—¶ä¸ç”¨å¯¹ç®—æ³•è¿›è¡Œå‚æ•°çš„è°ƒæ•´ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨ç½‘ç»œè®­ç»ƒçš„å‰ä¸€ç™¾ä¸‡æ­¥å¹¶æ²¡æœ‰å‘çŽ°æ€§èƒ½æœ‰æ˜Žæ˜¾çš„æå‡ã€‚å› ä¸ºrewardçš„å€¼æ›´å¤§çš„è¯ï¼Œå­¦ä¹ å°†ä¼šæ›´å®¹æ˜“ï¼Œè¿™æ ·rewardä¿¡å·ä¼šæ›´åŠ æ˜Žæ˜¾ï¼Œä¸ä¼šè¢«æ·¹æ²¡åœ¨ç½‘ç»œçš„é«˜æ–¯å™ªå£°ä¸­ã€‚æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›rewardèƒ½å¤Ÿå¤§ä¸€ç‚¹ï¼Œä½†æ˜¯å¤šå¤§æ¯”è¾ƒåˆé€‚åˆæ˜¯ä¸€ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬æ‰€å€Ÿé‰´çš„ç®—æ³•[1]ï¼Œå°†è¿™ä¸ªrewardçš„æœ€å°å€¼è®¾ç½®æˆäº†-500ï¼ˆå°è½¦æ’žä¸Šäº†éšœç¢ç‰©ï¼‰ï¼Œä½†æˆ‘ä»¬å®žéªŒå‘çŽ°è¿™ä¸ªå€¼è®¾ç½®çš„è¿‡å°ï¼ˆä¸‹é¢å°†ä¼šè§£é‡Šï¼‰ï¼Œæ‰€ä»¥æœ€åŽçš„èŒƒå›´è°ƒæ•´ä¸º[-100, 10] ï¼ˆé€šè¿‡è£å‰ªï¼‰ã€‚æˆ‘ä»¬æŠŠè¿™ä¸ªè¿‡ç¨‹ç§°ä¹‹ä¸ºrewardæ­£åˆ™åŒ–ã€‚Reward æ­£åˆ™åŒ–åœ¨ç½‘ç»œè®­ç»ƒï¼ˆåå‘ä¼ æ’­ï¼‰çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–ä»£ä»·å‡½æ•°ã€‚æˆ‘ä»¬çš„ä»£ä»·å‡½æ•°é€‰ä¸ºè®­ç»ƒç½‘ç»œè¾“å‡ºçš„Qå€¼ä¸Žè®­ç»ƒæ ·æœ¬çš„Qå€¼ä¹‹é—´çš„MSEã€‚åœ¨è¯•éªŒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘çŽ°ï¼Œå¯¹äºŽ$s_1, s_2, s_3$å€¼éƒ½æ¯”è¾ƒå¤§çš„çŠ¶æ€ï¼Œå…¶rewardéƒ½ä¼šè½åœ¨[0, 40]çš„èŒƒå›´å†…ï¼Œå¹¶ä¸”å‡å€¼ä¸º20ã€‚ä½†æ˜¯ç½‘ç»œåˆšå¼€å§‹è®­ç»ƒæ—¶ï¼Œè¾“å‡ºå€¼ä¸ºå‡å€¼ä¸º0çš„é«˜æ–¯å™ªå£°ã€‚ä¹Ÿå°±æ˜¯è¯´åˆå§‹çš„losså¤„äºŽ[400-1600]çš„èŒƒå›´å†…ï¼ˆç”±äºŽæœ€åŽçš„losséœ€è¦é™¤ä»¥æ ·æœ¬çš„æ•°é‡ï¼Œæ‰€ä»¥lossç­‰äºŽä¸€ä¸ªæ ·æœ¬çš„lossï¼‰ã€‚çŽ°åœ¨æˆ‘ä»¬å‡å®šç½‘ç»œå¤„äºŽä¸€ä¸ªæœ€ä¼˜ç‚¹é™„è¿‘ï¼Œè¿™æ—¶å€™å°è½¦çªç„¶æ’žä¸Šäº†æŸä¸ªéšœç¢ç‰©ï¼Œé‚£ä¹ˆå”¯ä¸€çš„å¯èƒ½å°±æ˜¯çŒ«å‡ºçŽ°åœ¨äº†å°è½¦åŽé¢ã€‚è¿™æ—¶å€™å°±ä¼šå¼•å…¥ä¸€ä¸ª250000çš„lossï¼ˆå¦‚æžœå°†rewardçš„æœ€å°å€¼è®¾ç½®ä¸º-500ï¼‰ã€‚ä½†æ˜¯ç½‘ç»œåˆå§‹æ—¶çš„losséƒ½åªå¤„äºŽ[400, 1600]çš„èŒƒå›´å†…ï¼Œè¿™ä¸ªlossæ˜¯åˆå§‹lossçš„100å€ã€‚è¿™ä¹ˆå¤§çš„lossæ‰€å¼•å…¥çš„æ¢¯åº¦å°†ä¼šä½¿å¾—ç½‘ç»œèµ°ä¸€æ®µéžå¸¸å¤§çš„è·ç¦»ï¼Œè¿™å°±å¾ˆå¯èƒ½ç›´æŽ¥è·³è¿‡äº†å±€éƒ¨æœ€ä¼˜ç‚¹ã€‚ä¸æ–­å¦‚æ­¤çš„è¯ï¼Œç½‘ç»œå°±ä¼šéœ‡è¡çš„éžå¸¸åŽ‰å®³ã€‚è®©æˆ‘ä»¬ç”¨æ•°å­¦çš„è§‚ç‚¹æ¥è§£é‡Šè¿™ä¸ªé—®é¢˜ã€‚å½“rewardçš„è´Ÿå€¼è®¾ç½®çš„è¿‡å¤§ï¼Œå°†ä¼šä½¿å¾—åŽŸå§‹é—®é¢˜ç©ºé—´è·ç¦»æœ€ä¼˜ç©ºé—´æœ‰ä¸€ä¸ªéžå¸¸å¤§çš„åå·®ï¼Œå¾ˆéš¾é€šè¿‡æ¢¯åº¦ä¸‹é™é è¿‘ã€‚è¿™ä¸ªå¤§çš„åå·®åœ¨é—®é¢˜ç©ºé—´åˆ›é€ äº†ä¸€äº›éžå¸¸é™¡å³­çš„cliffã€‚å°±åƒæˆ‘ä»¬çˆ¬å±±ä¸€æ ·ï¼Œå¥½ä¸å®¹æ˜“çˆ¬åˆ°äº†å±±é¡¶é™„è¿‘ï¼Œä¸€ä¸å°å¿ƒå°±æŽ‰ä¸‹äº†æ‚¬å´–ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªèƒ½ä¸€æ­¥ä¸€æ­¥éžå¸¸æ…¢çš„çˆ¬ä¸Šæ¥ï¼ŒèŠ±å¾ˆä¹…çš„æ—¶é—´æ‰èƒ½åˆ°è¾¾åˆšæ‰çš„ä½ç½®ã€‚å¦‚æžœä¸€ä¸å°å¿ƒåˆæŽ‰ä¸‹åŽ»äº†ï¼Œé‚£ä¹ˆåˆè¦é‡æ–°çˆ¬ã€‚å› æ­¤ï¼Œå‡å°rewardçš„èŒƒå›´ååˆ†é‡è¦ï¼Œè¿™æ ·å¯ä»¥å‡å°cliffçš„å¡åº¦ï¼Œä½¿å¾—ç½‘ç»œè®­ç»ƒæ›´å¿«æ›´å®¹æ˜“ã€‚ä½†æ˜¯åˆä¸èƒ½å¤ªå°ï¼Œä»¥å…è¢«å™ªå£°æ·¹æ²¡ã€‚æœ€åŽæˆ‘ä»¬é€‰å®šäº†[-100, 10]è¿™ä¸ªèŒƒå›´ã€‚æ¨¡åž‹è¿­ä»£è¿‡ç¨‹æˆ‘ä»¬æœ€å¼€å§‹ç›´æŽ¥é‡‡ç”¨çŽ°æˆçš„æ¨¡åž‹ï¼Œæ˜¯ä¸€ä¸ªä¸¤å±‚çš„ç¥žç»ç½‘ç»œï¼ˆä¸åŒ…æ‹¬è¾“å…¥å±‚ï¼‰ï¼Œæ•ˆæžœå·²ç»ä¸é”™äº†ï¼Œä½†æ˜¯å°è½¦æ€»æ˜¯æ’žä¸Šéšœç¢ç‰©ã€‚å› æ­¤æˆ‘ä»¬åšäº†ä¸€äº›æ”¹å˜ï¼šç±»ä¼¼DQNï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æœ€è¿‘å››æ¬¡çš„stateï¼Œå°†å…¶æ˜ å°„ä¸ºä¸€ä¸ªinputï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„QMaxå€¼æé«˜åˆ°äº†120æˆ‘ä»¬ç»§ç»­è¿›è¡Œæ”¹å˜ï¼Œä»Žä½¿ç”¨æœ€è¿‘å››æ¬¡æ”¹ä¸ºæœ€è¿‘16æ¬¡ï¼Œä½¿å¾—æˆ‘ä»¬çš„QMaxå€¼æé«˜åˆ°äº†140æˆ‘ä»¬å°è¯•äº†ä½¿ç”¨ä¸€ä¸ªæ›´å°çš„ç½‘ç»œè¿›è¡Œå­¦ä¹ ï¼ˆ2å±‚ï¼Œæ¯å±‚32ç»´ï¼‰ï¼Œå¹¶ä¸”åªä½¿ç”¨ä¸€ä¸ªstateè¿›è¡Œè¾“å…¥ï¼Œä½†æ˜¯ç»“æžœæ¯”éšæœºç®—æ³•æ›´å·®ã€‚ç»§ç»­å°è¯•ä½¿ç”¨grid searché€‰æ‹©æ¨¡åž‹ï¼Œè¿˜æ˜¯ä¸¤å±‚ç½‘ç»œï¼Œæ¯ä¸€å±‚çš„ç»´æ•°ä»Ž32åˆ°512ï¼Œè®­ç»ƒè¿­ä»£æ¬¡æ•°ä¸º200, 000ï¼Œä½†æ˜¯æœ€åŽçš„QMaxå€¼è¿˜æ˜¯ä¸èƒ½è¶…è¿‡140ã€‚æˆ‘ä»¬å°è¯•äº†æ›´å°çš„æ—¶é—´çª—å£ï¼Œæ›´å¤§çš„minibatchï¼Œç½‘ç»œè®­ç»ƒæ—¶éœ‡è¡çš„ååˆ†åŽ‰å®³æˆ‘ä»¬å°è¯•åœ¨å°è½¦çš„èƒŒé¢å¢žåŠ ä¸€ä¸ªå£°çº³ä¼ æ„Ÿå™¨ï¼Œå‘iæŒ‰ç½‘ç»œè®­ç»ƒé€Ÿåº¦å˜å¿«äº†ï¼Œä½†æ˜¯æœ€åŽçš„QMaxå€¼è¿˜æ˜¯ä¸èƒ½è¾¾åˆ°æ›´é«˜ã€‚è¿™äº›å°è¯•è¯´æ˜Žåº”å½“æ˜¯ä¸¤å±‚ç½‘ç»œçš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ä¸å¤Ÿï¼Œæˆ‘ä»¬å°è¯•ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œã€‚æœ€åŽä½¿ç”¨çš„ç½‘ç»œæœ‰8å±‚ï¼ˆä¸ç®—è¾“å…¥è¾“å‡ºå±‚ï¼‰ï¼Œè¾“å…¥å±‚å’Œè¾“å‡ºå±‚å„æœ‰32ç»´ï¼Œä¸­é—´6å±‚ä¸º64ç»´ã€‚æœ€åŽå–å¾—äº†å¾ˆå¥½çš„æ•ˆæžœï¼ŒQMaxè¾¾åˆ°äº†ä¹‹å‰çš„10å€ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸€å±‚ç½‘ç»œåŽéƒ½åŠ å…¥äº†ä¸€ä¸ª20%çš„dropoutå±‚ï¼ˆé™¤äº†è¾“å…¥å±‚ä»¥åŠè¾“å‡ºå±‚ä¹‹å‰ï¼‰ï¼Œæ¿€æ´»å‡½æ•°é€‰ç”¨çš„ReLUå‡½æ•°ã€‚å®žéªŒç»“æžœç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š1234567891011121314151617181920212223242526272829303132In(4): ai.cycle()t= 11000[654.53412, 322.84866, 86.578796, 1414.0239]Games played 539Epoch Max score 144Epoch Mean score 30.3580705009t= 21000[474.16202, 251.2959, 79.489487, 1243.3118]Games played 774Epoch Max score 223Epoch Mean score 42.6255319149t= 31000[388.32297, 202.05305, 79.290771, 1086.0581]Games played 1020Epoch Max score 153Epoch Mean score 40.5081300813t= 41000[470.96552, 234.70471, 129.87579, 1320.3688]Games played 1281Epoch Max score 251Epoch Mean score 38.3908045977t= 51000[549.32666, 203.20442, 176.22263, 1079.8307]Games played 1546Epoch Max score 226Epoch Mean score 37.7773584906t= 61000[610.16583, 232.79211, 224.97626, 1264.9712]Games played 1759Epoch Max score 484Epoch Mean score 46.5774647887...å®žéªŒç»“æžœï¼šå¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬çš„ç®—æ³•æ€§èƒ½å®Œå…¨è¶…è¶Šäº†éšæœºç®—æ³•ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬è®­ç»ƒå¤§æ¦‚250,000æ¬¡åŽçš„ç»“æžœï¼šå…³äºŽéšæœºç®—æ³•ä»¥åŠQ-learningç®—æ³•çš„åŠ¨ç”»å±•ç¤ºå¯ä»¥å‚ç…§é¡¹ç›®åœ°å€ã€‚ä½†æ˜¯æˆ‘ä»¬å‘çŽ°å°è½¦è¿˜æ˜¯ä¼šæ’žåˆ°éšœç¢ç‰©ï¼Œè¿™ç»å¸¸å‘ç”Ÿåœ¨å°è½¦ç¢°æ’žä¹‹åŽçš„æ¢å¤è¿‡ç¨‹ä¸­ã€‚è¿™æ—¶å€™å°è½¦å¯èƒ½åˆ°è¾¾åœ°å›¾çš„è§’è½ï¼Œå……æ»¡éšœç¢ç‰©ã€‚ä½†æ˜¯å› ä¸ºå°è½¦åªæœ‰ä¸‰ä¸ªä¼ æ„Ÿå™¨ï¼Œå³ä½¿åœ¨èƒŒé¢åŠ ä¸Šè¿˜æ˜¯å¤ªå°‘äº†ï¼Œæ‰€ä»¥ä¿¡æ¯æ•æ‰ä¸å¤Ÿã€‚è¿™æ˜¯æ¨¡åž‹éœ€è¦æ”¹è¿›çš„åœ°æ–¹ã€‚æˆ‘ä»¬å¯ä»¥äº‹å…ˆåœ¨å°è½¦ä¸­å­˜å‚¨ä¸€ä¸ªç±»ä¼¼äºŽåœ°å›¾çš„æ•°æ®ã€‚å¦å¤–ï¼Œç”±äºŽå°è½¦ä¸€ç›´æ˜¯åŒ€é€Ÿè¡Œé©¶ï¼Œå¦‚æžœåŠ å…¥åŠ é€Ÿï¼Œå‡é€Ÿç­‰è¿‡ç¨‹ï¼Œåº”å½“ä¼šä½¿å¾—æ€§èƒ½æ›´å¥½ã€‚ä½†æ˜¯ç”±äºŽæ—¶é—´åŽŸå› ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è¿›ä¸€æ­¥æ”¹è¿›ã€‚è¿›ä¸€æ­¥å·¥ä½œæœ¬æ¬¡å®žéªŒä»…ä»…æ˜¯åœ¨äºŒç»´çŽ¯å¢ƒä¸­è¿›è¡Œçš„ã€‚ä½†æ˜¯ä¸¥æ ¼æ¥è¯´å¹¶ä¸æ˜¯å¤æ‚çŽ¯å¢ƒçš„æœ€ä½³ç®€åŒ–ã€‚ä¸‰ç»´çŽ¯å¢ƒæ›´åŠ è´´è¿‘çŽ°å®žæƒ…å†µï¼Œä¾‹å¦‚æˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€ä¸ªé£žè¡Œçš„çŽ¯å¢ƒæ¨¡æ‹Ÿã€‚ç›¸å…³é“¾æŽ¥[1]. https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.58wi2s7ct[2]. https://github.com/songrotek/DQN-Atari-Tensorflow/blob/master/BrainDQN_Nature.py[3]. https://www.continuum.io/why-anaconda[4]. https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#anaconda-installation[5]. http://www.pygame.org/wiki/GettingStarted[6]. http://www.pymunk.org/en/latest/[7]. http://www.numpy.org/[8]. http://www.scipy.org/[9]. https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Store Management System]]></title>
      <url>%2F2017%2F06%2F27%2FStore-Management-System%2F</url>
      <content type="text"><![CDATA[SMSSMS (Store Management System), ä¸€ä¸ªç®€å•çš„ç½‘åº—ç®¡ç†ç³»ç»Ÿã€‚æºç ï¼šhttps://github.com/ewanlee/smsè¿™æ˜¯ä¸€ä¸ªç”¨äºŽå±•ç¤ºå¾®æœåŠ¡çš„ proof-of-concept åº”ç”¨ï¼Œè¿ç”¨äº†Spring Boot, Spring Cloud ä»¥åŠ Dockeréƒ¨ç½²ã€‚æ ¸å¿ƒæœåŠ¡SHOP åˆ†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒå¾®æœåŠ¡ï¼Œå®ƒä»¬éƒ½æ˜¯ç‹¬ç«‹å¼€å‘çš„ï¼Œé‡‡ç”¨äº†Spring MVCæž¶æž„ï¼šOrder serviceè¿›è¡Œè®¢å•çš„æ·»åŠ ï¼Œåˆ é™¤ï¼Œä»¥åŠæ˜¾ç¤ºMethodPathDescriptionUser authenticatedAvailable from UIGET/è¿”å›žè®¢å•åˆ—è¡¨æ— æœ‰GET/formå¢žåŠ è®¢å•ï¼Œå¹¶è¿›è¡Œç”¨æˆ·é€‰æ‹©æ— æœ‰POST/lineå¢žåŠ ä¸€æ¡è®¢å•åˆ°æ•°æ®åº“æ— æ— GET/{id}æ˜¾ç¤ºæŸä¸€æ¡è®¢å•çš„è¯¦æƒ…æ— æœ‰POST/å¢žåŠ è®¢å•è¡Œä¸ºæ— æœ‰DELETE/{id}åˆ é™¤è®¢å•æ— æœ‰Customer serviceè¿›è¡Œç”¨æˆ·çš„æ·»åŠ ï¼Œåˆ é™¤ï¼Œä»¥åŠæ˜¾ç¤ºMethodPathDescriptionUser authenticatedAvailable from UIGET/listè¿”å›žç”¨æˆ·åˆ—è¡¨æ— æœ‰GET/{id}è¿”å›žæŒ‡å®šidçš„ç”¨æˆ·è¯¦æƒ…æ— æœ‰GET/formè¿”å›žå¢žåŠ ç”¨æˆ·ç•Œé¢æ— æœ‰POST/formå¢žåŠ ç”¨æˆ·æ— æ— PUT/{id}å¢žåŠ ç”¨æˆ·è¡Œä¸ºæ— æœ‰DELETE/{id}åˆ é™¤ç”¨æˆ·æ— æœ‰Catalog serviceè¿›è¡Œå•†å“çš„æ·»åŠ ï¼Œåˆ é™¤ï¼Œä»¥åŠæ˜¾ç¤ºMethodPathDescriptionUser authenticatedAvailable from UIGET/listè¿”å›žå•†å“åˆ—è¡¨æ— æœ‰GET/{id}è¿”å›žæŒ‡å®šidçš„å•†å“è¯¦æƒ…æ— æœ‰GET/formè¿”å›žå¢žåŠ å•†å“ç•Œé¢æ— æœ‰POST/formå¢žåŠ å•†å“æ— æ— PUT/{id}å¢žåŠ å•†å“è¡Œä¸ºæ— æœ‰DELETE/{id}åˆ é™¤å•†å“æ— æœ‰TEXT_HTML_VALUE/searchFormè¿”å›žæœç´¢ç•Œé¢æ— æœ‰TEXT_HTML_VALUE/searchByNameè¿”å›žæœç´¢ç»“æžœæ— æœ‰æ³¨æ„æ¯ä¸ªå¾®æœåŠ¡éƒ½æœ‰è‡ªå·±çš„æ•°æ®åº“ï¼Œå› æ­¤äº’ç›¸ä¹‹é—´æ²¡æœ‰ç›´æŽ¥è®¿é—®æ•°æ®åº“çš„æŽ¥å£è¿™é‡Œçš„æ•°æ®åº“ä½¿ç”¨çš„æ˜¯springæ¡†æž¶è‡ªå¸¦çš„æ•°æ®åº“æœåŠ¡åˆ°æœåŠ¡çš„é€šä¿¡éžå¸¸ç®€å•ï¼Œé€šè¿‡æš´éœ²çš„æŽ¥å£å³å¯æž¶æž„æœåŠ¡åˆ†å¸ƒå¼ç³»ç»Ÿä¸­æœ‰ä¸€äº›é€šç”¨çš„æ¨¡å¼ï¼ŒSpring Cloudæ¡†æž¶éƒ½æœ‰æä¾›ï¼Œåœ¨æœ¬é¡¹ç›®ä¸­ä»…ä»…è¿ç”¨äº†ä¸€å°éƒ¨åˆ†ï¼šAPI ç½‘å…³å¯ä»¥çœ‹åˆ°ï¼Œæœ‰ä¸‰ä¸ªæ ¸å¿ƒæœåŠ¡ï¼Œå®ƒå°†å¤–éƒ¨APIæš´éœ²ç»™å®¢æˆ·ç«¯ã€‚åœ¨ä¸€ä¸ªçŽ°å®žä¸–ç•Œçš„ç³»ç»Ÿä¸­ï¼Œæ ¸å¿ƒæœåŠ¡çš„æ•°é‡å¯ä»¥éžå¸¸å¿«é€Ÿåœ°å¢žé•¿ï¼Œå¹¶ä¸”æ•´ä¸ªç³»ç»Ÿçš„å¤æ‚æ€§æ›´æ˜¯æ€¥å‰§å¢žåŠ ã€‚å®žé™…ä¸Šï¼Œä¸€ä¸ªå¤æ‚çš„ç½‘é¡µå¯èƒ½éœ€è¦æ¸²æŸ“æ•°ç™¾ä¸ªæœåŠ¡ã€‚ç†è®ºä¸Šï¼Œå®¢æˆ·ç«¯å¯ä»¥ç›´æŽ¥å‘æ¯ä¸ªå¾®æœåŠ¡å™¨å‘å‡ºè¯·æ±‚ã€‚ä½†æ˜¯æ˜¾ç„¶ï¼Œè¿™å°†é¢ä¸´å¾ˆå¤§çš„æŒ‘æˆ˜ä»¥åŠé™åˆ¶ã€‚æ¯”å¦‚å¿…é¡»è¦çŸ¥é“æ‰€æœ‰ç«¯ç‚¹çš„åœ°å€ã€‚é€šå¸¸ä¸€ä¸ªæ›´å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨APIç½‘å…³ã€‚å®ƒæ˜¯ç³»ç»Ÿä¸­çš„å•ä¸ªå…¥å£ç‚¹ï¼Œç”¨äºŽé€šè¿‡å°†è¯·æ±‚è·¯ç”±åˆ°é€‚å½“çš„åŽç«¯æœåŠ¡æˆ–é€šè¿‡è°ƒç”¨å¤šä¸ªåŽç«¯æœåŠ¡å¹¶èšåˆç»“æžœæ¥å¤„ç†è¯·æ±‚ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥ç”¨äºŽè®¤è¯ï¼ŒåŽ‹åŠ›æµ‹è¯•ï¼ŒæœåŠ¡è¿ç§»ï¼Œé™æ€å“åº”å¤„ç†ï¼Œä¸»åŠ¨æµé‡ç®¡ç†ç­‰Netflixå¼€è¾Ÿäº†è¿™æ ·ä¸€ä¸ªä¼˜åŠ¿æœåŠ¡ï¼ŒçŽ°åœ¨ä½¿ç”¨Spring Cloudï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ª@EnableZuulProxyæ³¨é‡Šæ¥å®žçŽ°ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨äº†Zuulå­˜å‚¨é™æ€å†…å®¹ï¼ˆuiåº”ç”¨ç¨‹åºï¼‰ï¼Œå¹¶å°†è¯·æ±‚è·¯ç”±åˆ°é€‚å½“çš„å¾®æœåŠ¡å™¨ã€‚Zuulä½¿ç”¨æœåŠ¡å‘çŽ°æœºåˆ¶æ¥å®šä½æœåŠ¡å®žä¾‹ä»¥åŠæ–­è·¯å™¨å’Œè´Ÿè½½å¹³è¡¡å™¨ï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚æœåŠ¡å‘çŽ°å¦å¤–ä¸€ä¸ªä¼—æ‰€å‘¨çŸ¥çš„æž¶æž„æ¨¡å¼ä¾¿æ˜¯æœåŠ¡å‘çŽ°æœºåˆ¶ã€‚å®ƒå¯ä»¥è¿›è¡ŒæœåŠ¡å®žä¾‹ç½‘ç»œä½ç½®çš„åŠ¨æ€æ£€æµ‹ã€‚å½“åº”ç”¨éœ€è¦æ‰©å±•ã€å®¹é”™æˆ–è€…å‡çº§çš„æ—¶å€™å°±å¯ä»¥è‡ªåŠ¨ä¸ºæœåŠ¡å®žä¾‹åˆ†é…åœ°å€ã€‚æœåŠ¡å‘çŽ°æœºåˆ¶çš„æ ¸å¿ƒæ˜¯æ³¨å†Œé˜¶æ®µã€‚æœ¬é¡¹ç›®ä½¿ç”¨äº† Netflix Eurekaã€‚ Eurekaæ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯çš„å‘çŽ°æ¨¡å¼ï¼Œå› ä¸ºå¾ˆå¤šç½‘ç»œåº”ç”¨éƒ½éœ€è¦å®¢æˆ·ç«¯è‡ªå·±åŽ»ç¡®å®šç‰¹å®šæœåŠ¡çš„åœ°å€ï¼ˆä½¿ç”¨æ³¨å†ŒæœåŠ¡å™¨ï¼‰å¹¶ä¸”è¿›è¡Œè¯·æ±‚çš„è´Ÿè½½å‡è¡¡ã€‚ä½¿ç”¨Spring Bootæ—¶ï¼Œåªè¦åœ¨pomæ–‡ä»¶ä¸­åŠ å…¥spring-cloud-starter-eureka-serverä¾èµ–å¹¶ä¸”ä½¿ç”¨@EnableEurekaServeræ³¨è§£å³å¯ä½¿ç”¨è¯¥æœåŠ¡ã€‚è´Ÿè½½å‡è¡¡ã€æ–­è·¯å™¨ä»¥åŠHttpå®¢æˆ·ç«¯Netflixè¿˜æä¾›äº†å¦å¤–ä¸€äº›ååˆ†å¥½ç”¨çš„å·¥å…·ã€‚RibbonRibbon æ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯çš„è´Ÿè½½å‡è¡¡å™¨ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„å‡è¡¡å™¨ï¼Œä½ å¯ä»¥ä¹‹é—´é“¾æŽ¥åˆ°ç›¸å…³æœåŠ¡ã€‚Ribbonå·²ç»å’ŒSpring Cloudä»¥åŠæœåŠ¡å‘çŽ°æœºåˆ¶é›†æˆåœ¨äº†ä¸€èµ·ã€‚ Eureka Client æä¾›äº†ä¸€ä¸ªå¯ç”¨æœåŠ¡å™¨çš„åŠ¨æ€åˆ—è¡¨ä¾› Ribbon è¿›è¡ŒæœåŠ¡å™¨ä¹‹é—´çš„å‡è¡¡ã€‚HystrixHystrix æ˜¯æ–­è·¯å™¨æ¨¡å¼çš„å…·ä½“å®žçŽ°ï¼Œå…¶å¯ä»¥è°ƒèŠ‚ç½‘ç»œè®¿é—®ä¾èµ–ä¸­ç»å¸¸å‡ºçŽ°çš„å»¶è¿Ÿä»¥åŠé”™è¯¯ã€‚å…¶ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†é˜»æ–­åœ¨åˆ†å¸ƒå¼çŽ¯å¢ƒä¸­å¤§é‡å¾®æœåŠ¡æžæ˜“å‡ºçŽ°çš„çº§è”é”™è¯¯ï¼Œä½¿å¾—ç³»ç»Ÿå°½å¿«é‡æ–°ä¸Šçº¿ã€‚Hystrixè¿˜æä¾›äº†ä¸€ä¸ªç›‘æŽ§é¡µé¢ ï¼ˆä¸‹é¢å°†ä¼šçœ‹åˆ°ï¼‰ã€‚è¿è¡Œå‰æœŸå‡†å¤‡ï¼šç½‘ç»œå®‰è£… Docker ä»¥åŠ Docker composeè¿è¡Œå‘½ä»¤ï¼šcd microservice-demo/æ‰§è¡Œmvn clean packagecd ../docker/æ‰§è¡Œdocker-compose buildä»¥åŠdocker-compose upé‡è¦ç«¯å£ï¼šhttp://127.0.0.1:8080 - ç½‘å…³http://127.0.0.1:8761 - Eureka Dashboardæ³¨æ„ï¼šåº”ç”¨å¯åŠ¨ä¹‹åŽå¦‚æžœé‡åˆ° Whitelabel Error Page é”™è¯¯è¯·åˆ·æ–°é¡µé¢UIIndexCustomer ServiceCatalog ServiceOrder ServiceEukera ServiceHystrix Dashboard]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Learning to act by predicting the future]]></title>
      <url>%2F2017%2F06%2F14%2FLearning-to-act-by-predicting-the-future%2F</url>
      <content type="text"><![CDATA[è®ºæ–‡ Learning to act by predicting the futureè¿™ç¯‡è®ºæ–‡æå‡ºçš„ DFP (Direct Future Prediction) èµ¢å¾—äº†2016å¹´ Virtual Doom AI Competition çš„ â€œFull Deathmatchâ€ çŽ¯èŠ‚çš„æ¯”èµ›ã€‚Virtual Doom æ˜¯ä¸€ä¸ªå¯¹æˆ˜æ€§çš„ç¬¬ä¸€äººç§°å°„å‡»åž‹æ¸¸æˆï¼Œæ ¹æ®çŽ©å®¶å‡»æ€æ•°åˆ¤å®šèƒœè´Ÿã€‚ä¸ºäº†ä½“çŽ°å‡ºæ¨¡åž‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œ è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„åœ°å›¾ä¸åœ¨æ¯”èµ›è¿‡ç¨‹ä¸­å‡ºçŽ°ã€‚DFPçš„æ€§èƒ½è¶…å‡ºäº†ç¬¬äºŒåï¼ˆDeep LSTM Q-Networkï¼‰50%ï¼Œå¹¶ä¸”å…¶æ¨¡åž‹ä»¥åŠè®­ç»ƒæ•°æ®æ›´åŠ ç®€æ´ï¼Œè¡¨çŽ°å‡ºäº†DFPæ¨¡åž‹çš„ä¼˜è¶Šæ€§ã€‚æœºå™¨å­¦ä¹ é—®é¢˜å¯ä»¥åˆ†ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œæ— ç›‘ç£å­¦ä¹ é—®é¢˜ä»¥åŠå¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚ç›‘ç£å­¦ä¹ ä¸»è¦æ˜¯å­¦ä¹ ä¸€ä¸ªè¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å‡½æ•°ï¼Œæ— ç›‘ç£å­¦ä¹ æ›´åŠ å…³æ³¨å¦‚ä½•æŒ–æŽ˜æ•°æ®æœ¬èº«çš„éšå«ç»“æž„ï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ä¸ªé¢å‘ç›®æ ‡çš„ç­–ç•¥å­¦ä¹ é—®é¢˜ã€‚å› æ­¤é‡‡ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•ä½¿å¾—æœºå™¨äººåœ¨Deathmatchæ¸¸æˆä¸­è¡¨çŽ°è‰¯å¥½ååˆ†åˆé€‚ã€‚å› ä¸ºè¿™æ˜¯ä¸€ä¸ªç›´æŽ¥é¢å‘ç›®æ ‡çš„é—®é¢˜ ï¼ˆåœ¨æ¸¸æˆä¸­å–å¾—æœ€å¤§çš„å‡»æ€æ•°ï¼‰ã€‚æ‰€ä»¥ DQN ä»¥åŠ A3C è¿™æ ·çš„ç®—æ³•åº”è¿è€Œç”Ÿï¼Œå¹¶ä¸”å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚ä½†æ˜¯è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªä¸åŒçš„è§‚ç‚¹ã€‚å®ƒå¼•ç”¨äº†Jordan &amp; Rumelhart (1992) è¿™ç¯‡è®ºæ–‡ä¸­æå‡ºçš„ä¸€ä¸ªè§‚ç‚¹ï¼šå¯¹äºŽä¸€ä¸ªå¯ä»¥ä¸ŽçŽ¯å¢ƒè¿›è¡Œäº¤äº’çš„å­¦ä¹ é—®é¢˜ï¼Œå¦‚æžœçŽ¯å¢ƒæä¾›çš„åé¦ˆæ˜¯ç¨€ç–çš„æ ‡é‡ ï¼ˆä¾‹å¦‚ï¼Œå¯¹äºŽä¸€ä¸ªäº”å­æ£‹é—®é¢˜ï¼Œåé¦ˆåªåœ¨æœ€åŽèƒœè´Ÿå·²åˆ†æ—¶ç»™å‡ºï¼Œå¹¶ä¸”åªæ˜¯ä¸€ä¸ªç±»ä¼¼+1ï¼Œ-1çš„æ ‡é‡åé¦ˆï¼‰ï¼Œé‡‡ç”¨ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¼šååˆ†æœ‰æ•ˆï¼›ä½†æ˜¯å¦‚æžœçŽ¯å¢ƒç»™å‡ºçš„åé¦ˆæ˜¯ä¸€ä¸ªå³æ—¶å¯†é›†çš„å¤šç»´åº¦åé¦ˆ ï¼ˆåœ¨çŸ­æ—¶é—´å†…å…·æœ‰å¾ˆå¤§çš„ä¿¡æ¯æ¯”ç‰¹çŽ‡ï¼‰ï¼Œç›‘ç£å­¦ä¹ ç®—æ³•æ›´å…·ä¼˜åŠ¿ã€‚ç”±äºŽç›‘ç£å­¦ä¹ æ–¹é¢çš„ç ”ç©¶å·²ç»éžå¸¸æˆç†Ÿï¼Œæœ€è¿‘ååˆ†ç«çƒ­çš„æ·±åº¦å­¦ä¹ æ›´æ˜¯åœ¨å¾ˆå¤šæ–¹é¢éƒ½å–å¾—äº†å¾ˆå¥½çš„ç»“æžœï¼Œå› æ­¤ï¼Œå¦‚æžœæˆ‘ä»¬èƒ½å¤ŸæŠŠå¼ºåŒ–å­¦ä¹ é—®é¢˜åœ¨æŸç§ç¨‹åº¦ä¸Šè½¬åŒ–ä¸ºä¸€ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œå¯ä»¥ä½¿å¾—é—®é¢˜çš„æ±‚è§£å¤§å¤§ç®€åŒ–ã€‚é‚£ä¹ˆçŽ°åœ¨çš„é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬è¦å¦‚ä½•è®¾è®¡æ¨¡åž‹ï¼Œä»Žè€Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªç›‘ç£ä¿¡å·å‘¢ï¼Ÿå¯ä»¥æƒ³åˆ°ï¼Œæˆ‘ä»¬å”¯ä¸€æ‹¥æœ‰çš„æ•°æ®å°±æ˜¯æœºå™¨äººé€šè¿‡ä¸ŽçŽ¯å¢ƒçš„äº¤äº’å¾—åˆ°çš„çŠ¶æ€è½¬ç§» ï¼ˆå¯¹äºŽæ¸¸æˆæ¥è¯´å°±æ˜¯çŽ©å®¶åœ¨æ¸¸æˆä¸­é‡‡å–ä¸åŒçš„è¡Œä¸ºå¾—åˆ°çš„çŽ¯å¢ƒçš„åé¦ˆï¼Œä¾‹å¦‚ï¼ŒçŽ©å®¶ä½¿ç”¨ä¸€ä¸ªè¡€åŒ…å¯ä»¥æ˜¯çš„ç”Ÿå‘½å€¼å›žå¤ï¼›å‘å·¦è½¬å¯ä»¥ä½¿å¾—ç”»é¢å‘ç”Ÿå˜åŒ–ç­‰ç­‰ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥å¯¹è¿™äº›æ•°æ®è¿›è¡Œç‰¹æ®Šçš„è®¾è®¡ï¼Œä»Žè€Œèƒ½å¤Ÿæ»¡è¶³æˆ‘ä»¬çš„è¦æ±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸å†ç®€å•ä½¿ç”¨ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­å•ä¸€çš„çŠ¶æ€ ï¼ˆä¾‹å¦‚æ¸¸æˆä¸­çš„ä¸€å¸§ç”»é¢ï¼‰ä¸Žå¯¹åº”çš„å›žæŠ¥ã€‚æˆ‘ä»¬æŠŠå•ä¸€çš„çŠ¶æ€æ‹†åˆ†å¼€æ¥ï¼Œå¯¹äºŽåŽŸå§‹çš„å›¾åƒï¼Œå£°éŸ³ç­‰ä¿¡æ¯åŽŸæ ·ä¿ç•™ï¼Œå½¢æˆä¸€ä¸ª â€æ„Ÿè§‰è¾“å…¥æµ (sensory input stream)â€œ ï¼Œå¾ˆæ˜Žæ˜¾å®ƒæ˜¯ä¸€ä¸ªé«˜ç»´çš„å˜é‡ï¼›å¦å¤–ï¼Œæˆ‘ä»¬ä»Žè¿™äº›åŽŸå§‹çš„ä¿¡æ¯ä¸­æå–å‡ºèƒ½å¤Ÿä»£è¡¨æˆ‘ä»¬å­¦ä¹ ç›®æ ‡çš„æµ‹åº¦ ï¼ˆä¾‹å¦‚å¥åº·åº¦ï¼Œå‰©ä½™å¼¹è¯æ•°ä»¥åŠå‡»æ€æ•°ç­‰ï¼‰ï¼Œå½¢æˆä¸€ä¸ª â€æµ‹åº¦æµ (measurement stream)â€œ ï¼Œå®ƒæ˜¯ä¸€ä¸ªä½Žç»´çš„å˜é‡ ï¼ˆå› ä¸ºåªåŒ…å«å‡ ä¸ªé‡è¦çš„å˜é‡ï¼‰ã€‚æ³¨æ„ï¼Œè¿™é‡Œçš„streamä¸æ˜¯ä»£è¡¨äº†å¥½å‡ ä¸ªæ—¶é—´æ­¥ï¼Œè€Œæ˜¯ä»£è¡¨å®ƒæ˜¯å¤šä¸ªæµ‹åº¦çš„ä¸€ä¸ªé›†åˆã€‚è¿™æ ·åšæœ‰ä»€ä¹ˆå¥½å¤„å‘¢ï¼Ÿä¸€ä¸ªä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå…¶è®­ç»ƒå¯¹è±¡å°±æ˜¯æœ€å¤§åŒ–ä¸€ä¸ªå…³äºŽrewardçš„å‡½æ•°ã€‚ä¸€èˆ¬rewardéƒ½æ˜¯äººä¸ºç»™å®šçš„ ï¼ˆè¿˜æ˜¯æ‹¿äº”å­æ£‹ä¸¾ä¾‹ï¼Œæœ€åŽçŽ©å®¶èµ¢äº†ï¼Œå›žæŠ¥å°±æ˜¯ä¸€ä¸ªæ­£æ•°ï¼Œ åä¹‹å°±æ˜¯è´Ÿæ•°ï¼‰ï¼Œä½†æ˜¯è¿™å°±ä½¿å¾—å­¦ä¹ é—®é¢˜çš„æ–¹å·®å˜å¾—å¾ˆå¤§ï¼Œè®­ç»ƒè¿‡ç¨‹ååˆ†ä¸ç¨³å®šï¼Œæ”¶æ•›é€Ÿåº¦æ…¢ï¼Œç”šè‡³å¯èƒ½ä¸æ”¶æ•›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›rewardçš„å€¼ä¸è¦è¿‡äºŽéšæœºåŒ–ï¼Œèƒ½å¤Ÿé€šè¿‡æŸäº›ç›‘ç£ä¿¡å·æ¥å‡å°‘å…¶æ–¹å·®ã€‚è¿™é‡Œå°±å¯ä»¥ä½“çŽ°å‡ºæˆ‘ä»¬ä¹‹å‰è¿›è¡ŒçŠ¶æ€åˆ†è§£çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬å¯ä»¥å°†rewardè¡¨ç¤ºæˆ measurement stream çš„å‡½æ•°ï¼Œç”±äºŽmeasurementæ˜¯agentä¸ŽçœŸå®žçŽ¯å¢ƒè¿›è¡Œäº¤äº’æ—¶å¾—åˆ°çš„ï¼Œå±žäºŽä¸€ç§ç›‘ç£ä¿¡å·ï¼Œè¿™å¾ˆå¥½çš„æ»¡è¶³äº†æˆ‘ä»¬çš„éœ€æ±‚ã€‚æ‰€ä»¥æœ€åŽæˆ‘ä»¬çš„è®­ç»ƒå¯¹è±¡ç”±æœ€å¤§åŒ–ä¸€ä¸ªå…³äºŽrewardçš„å‡½æ•°å˜æˆäº†æœ€å¤§åŒ–ä¸€ä¸ªå…³äºŽmeasurement streamçš„å‡½æ•°ã€‚è€Œè¿™ä¸ªmeasurement streamå¯ä»¥è®¤ä¸ºæ˜¯ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­çš„rewardã€‚æ¨¡åž‹çŽ°åœ¨æˆ‘ä»¬æ­£å¼åœ°å®šä¹‰DFPæ¨¡åž‹ã€‚åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥$t$ï¼ŒagentæŽ¥æ”¶ä¸€ä¸ªè§‚å¯Ÿ ï¼ˆè½¬ç§»åˆ°ä¸€ä¸ªçŠ¶æ€ï¼‰$O_t$, æ ¹æ®è¿™ä¸ªè§‚å¯Ÿ ï¼ˆçŠ¶æ€ï¼‰çš„æŸäº›å›ºæœ‰å±žæ€§ä»Žå¯è¡Œçš„åŠ¨ä½œé›†åˆä¸­é€‰å–ä¸€ä¸ªåŠ¨ä½œæ‰§è¡Œã€‚$O_t$è¯¦ç»†å®šä¹‰å¦‚ä¸‹ï¼š$$\mathbf{o}_t = (\mathbf{s}_t, \mathbf{m}_t)$$æ•´ä¸ªçŠ¶æ€è½¬ç§»è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–ç›®æ ‡ï¼Œå‰é¢æåˆ°äº†ï¼Œå®ƒæ˜¯å…³äºŽmeasurement streamçš„å‡½æ•°ï¼š$$\mathbf{f} = (\mathbf{m}_{t+\tau_1}-\mathbf{m}_t, \cdots, \mathbf{m}_{t+\tau_n}-\mathbf{m}_t)$$$\tau_1, \cdots, \tau_n$ ä»£è¡¨ä¸Žå½“å‰æ—¶é—´æ­¥$t$çš„ä¸€ä¸ªåå·®ã€‚è‡³äºŽä¸ºä»€ä¹ˆä¸ç›´æŽ¥æœ€å¤§åŒ–measurement streamè€Œæ˜¯æœ€å¤§åŒ–ä¸€ä¸ªå·®å€¼ï¼Œæˆ‘è®¤ä¸ºä½œè€…å¯èƒ½æ˜¯æœ‰å¦‚ä¸‹è€ƒè™‘ï¼šå€Ÿé‰´äº†n-step Q-learning çš„åšæ³•ã€‚ç”±äºŽæ¨¡åž‹æ˜¯ä¸ºäº†é¢„æµ‹å½“å‰æ—¶é—´æ­¥$t$çš„measurement streamï¼Œå› æ­¤ä¼˜åŒ–å¯¹è±¡ä¸­åº”è¯¥åŒ…å«å½“å‰çš„measurement streamã€‚æœ€åŽï¼Œ$$\mathbf{Goal} \; = \; u(\mathbf{f};\mathbf{g})$$ä¸€èˆ¬çº¿æ€§å‡½æ•°å³å¯æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ï¼Œå³$$u(\mathbf{f};\mathbf{g}) = \mathbf{g}^{\text{T}}\mathbf{f}$$æ³¨æ„åˆ°çŽ°åœ¨æˆ‘ä»¬çš„é—®é¢˜å˜æˆäº†ä¸€ä¸ªç›‘ç£å­¦ä¹ çš„é—®é¢˜ã€‚ä¸ºäº†è®­ç»ƒæ¨¡åž‹ï¼Œæˆ‘ä»¬éœ€è¦é¢„æµ‹ç›®æ ‡ï¼Œç„¶åŽå†ä¸ŽçœŸå®žçš„ç›®æ ‡æ¯”è¾ƒï¼Œé€šè¿‡æœ€å°åŒ–è¯¯å·®æ¥è¿›è¡Œå­¦ä¹ ã€‚é‚£ä¹ˆæˆ‘ä»¬çŽ°åœ¨å®šä¹‰è¿™ä¸ªé¢„æµ‹è¿‡ç¨‹ã€‚æ³¨æ„åˆ°ï¼Œç”±äºŽç›®æ ‡åªæ˜¯measurement streamçš„å‡½æ•°ï¼Œè€Œä¸”å‚æ•°ä¸€èˆ¬éƒ½æ˜¯ç¡®å®šçš„ï¼Œä¸éœ€è¦è¿›è¡Œå­¦ä¹ ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„é¢„æµ‹å¯¹è±¡æ˜¯measurement streamè€Œä¸æ˜¯ç›®æ ‡ã€‚ä¸‹é¢æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªé¢„æµ‹å™¨F:$$\mathbf{p}_t^a = F(\mathbf{o}_t, a, \mathbf{g};\theta)$$æ³¨æ„ï¼Œè¿™é‡Œçš„$\text{g}$å’Œ(4)ä¸­æ˜¯ä¸ä¸€æ ·çš„ï¼Œå®ƒä»£è¡¨ç›®æ ‡ã€‚$p_t^a$ä»£è¡¨åœ¨$t$æ—¶é—´æ­¥ä¸‹ï¼Œæ‰§è¡Œè¡Œä¸º$a$æ‰€å¾—åˆ°çš„rewardï¼Œä¹Ÿå³measurement streamã€‚å½“è®­ç»ƒå®Œæˆçš„æ—¶å€™ï¼Œæˆ‘ä»¬å°±è¦ç”¨è¿™ä¸ªé¢„æµ‹å™¨Fè¿›è¡Œå†³ç­–ï¼Œç­–ç•¥å®šä¹‰å¦‚ä¸‹ï¼š$$a_t = {\arg\max}_{a \in \mathcal{A}} \mathbf{g}^{\text{T}}F(\mathbf{o}_t, a, \mathbf{g};\theta)$$æ³¨æ„åˆ°ï¼Œæ¨¡åž‹å®žé™…è®­ç»ƒçš„è¿‡ç¨‹ä¸­é‡‡ç”¨çš„æ˜¯$\varepsilon\text{-greedy}$ç­–ç•¥ã€‚è¿™é‡Œå¯ä»¥çœ‹å‡ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ–è€…æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è¦æ‰‹åŠ¨çš„è®¡ç®—å‡º$u(\text{f};\text{g})$çš„å€¼ã€‚ä¸‹é¢æˆ‘ä»¬è¯¦ç»†çš„å‰–æžæ¨¡åž‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚æ¨¡åž‹è®­ç»ƒå¯¹äºŽä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚Q-learningï¼Œå…¶è®­ç»ƒè¿‡ç¨‹æ˜¯ä¸€ä¸ªåœ¨çº¿å­¦ä¹ çš„è¿‡ç¨‹ï¼Œä¹Ÿå³å…¶è®­ç»ƒé›†æ˜¯ä¸€ä¸ªä¸€ä¸ªè¿›è¡Œè¾“å…¥çš„ï¼Œæ¯è¾“å…¥ä¸€æ¬¡éƒ½è¿›è¡Œä¸€æ¬¡å‚æ•°çš„æ›´æ–°ã€‚ç”±äºŽQ-learningä»¥åŠDFPéƒ½æ˜¯é‡‡ç”¨äº†MC (Monte Carlo) ç­–ç•¥ï¼Œè¿™ç§è®­ç»ƒè¿‡ç¨‹å¯èƒ½ååˆ†ä¸ç¨³å®š ï¼ˆç”±äºŽè®­ç»ƒæœ€å¼€å§‹æ—¶æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®æ˜¯é€šè¿‡ä¸€ä¸ªéšæœºç­–ç•¥ä¸ŽçŽ¯å¢ƒäº¤äº’äº§ç”Ÿçš„ï¼‰ï¼Œè‡´ä½¿æ”¶æ•›é€Ÿåº¦å¾ˆæ…¢ï¼Œéœ€è¦å¾ˆå¤šçš„episodesè¿›è¡Œè®­ç»ƒã€‚è¿™é‡Œé‡‡ç”¨äº†å’ŒDQN (Deep Q-Network) ç›¸åŒçš„ experience replayæŠ€æœ¯ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯ä¿å­˜æ¯æ¬¡agentä¸ŽçŽ¯å¢ƒäº¤äº’åŽäº§ç”Ÿçš„æ•°æ®å¯¹$\langle \mathbf{o}_i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle$ åˆ°æ•°æ®é›†$\mathcal{D}$ä¸­ï¼Œå³$\mathcal{D} = \{\langle \mathbf{o}_ i, a_i, \mathbf{g}_i, \mathbf{f}_i \rangle \}_{i=1}^N$. æ³¨æ„è¿™é‡Œçš„$N$ä¸ªæ•°æ®å¯¹å¹¶ä¸æ˜¯ç›´æŽ¥é¡ºåºäº§ç”Ÿçš„ï¼Œè€Œæ˜¯ä»Žå½“å‰episodeä¸­åˆ°å½“å‰æ—¶é—´æ­¥æ—¶ï¼Œæ‰€æœ‰çš„æ•°æ®å¯¹ä¸­é€‰å–æœ€è¿‘çš„$M$ä¸ªï¼Œå†ä»Žå…¶ä¸­éšæœºæŠ½æ ·$N$ä¸ªã€‚å¦å¤–ï¼Œæ¯éš”kæ­¥æ‰è¿›è¡Œä¸€æ¬¡å‚æ•°çš„æ›´æ–°ï¼Œå› ä¸º$\mathbf{f}$çš„è®¡ç®—éœ€è¦è€ƒè™‘åˆ°32ä¸ªæ—¶é—´æ­¥ä¹‹åŽçš„æ•°æ®ï¼Œå› æ­¤$k \ge 32$ï¼ˆå®žéªŒéƒ¨åˆ†å°†è¯¦ç»†ä»‹ç»ï¼‰ã€‚DQN ç»™å‡ºäº†å…·ä½“çš„å®žçŽ°ï¼šå¦å¤–éœ€è¦æ³¨æ„çš„æ˜¯æœ‰äº†è®­ç»ƒé›†ï¼Œæˆ‘ä»¬çŽ°åœ¨å®šä¹‰wä»£ä»·å‡½æ•°ï¼š$$\mathcal{L}(\theta) = \sum_{i=1}^{N} |F(\mathbf{o}_i, a_i, \mathbf{g}_i;\theta) - \mathbf{f}_i|^2$$æˆ‘ä»¬æ¥å¯¹æ¯”ä¸€ä¸‹ DQN çš„ä»£ä»·å‡½æ•°ï¼š$$L_i(\theta_i)= \mathbb{E}_{s, a \sim \rho(\cdot)} \left[ y_i - Q(s, a;\theta_i) \right],$$å…¶ä¸­$y_i = \mathbb{E}_{s^{\prime} \sim \varepsilon}[ r + \gamma \max_{a^{\prime}} Q(s^{\prime}, a^{\prime};\theta_{i-1}) ]$ ã€‚è¿™é‡Œçš„$y_i$æ˜¯ä¸Šä¸€æ¬¡æ¨¡åž‹çš„è¾“å‡ºï¼Œå…¶å€¼éšç€æ›´æ–°æ¬¡æ•°çš„å¢žåŠ ä¹Ÿåœ¨ä¸æ–­å˜åŒ–ã€‚å› æ­¤ä»Žè¿™é‡Œä¹Ÿèƒ½çœ‹å‡ºDFPæ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ ç®—æ³•ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬ä¸ºäº†è§£å†³æŠ¥å‘Šæœ€å¼€å§‹æå‡ºçš„ç›®æ ‡éšç€æ—¶é—´å‘ç”Ÿæ”¹å˜çš„é—®é¢˜ï¼Œé‡‡ç”¨äº†ä¸¤ç§ç›®æ ‡è¿›è¡Œæµ‹è¯•ï¼šç›®æ ‡å‘é‡$\mathbf{g}$ ï¼ˆä¸æ˜¯ç›®æ ‡ï¼‰åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¸å˜ç›®æ ‡å‘é‡åœ¨æ¯ä¸ªepisodeç»“æŸæ—¶éšæœºå˜åŒ–ç½‘ç»œç»“æž„ä¸‹å›¾æ˜¯DFPæ¨¡åž‹çš„ç½‘ç»œç»“æž„ï¼šä»Žå›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œè¯¥ç½‘ç»œæœ‰ä¸‰ä¸ªè¾“å…¥æ¨¡å—ã€‚ä¸€ä¸ªæ„ŸçŸ¥æ¨¡å—$S(s)$ï¼Œä¸€ä¸ªæµ‹åº¦æ¨¡åž‹$M(m)$ä»¥åŠä¸€ä¸ªç›®æ ‡æ¨¡å—$G(g)$ã€‚åœ¨å®žéªŒä¸­ï¼Œ$s$ä»£è¡¨ä¸€å¼ å›¾ç‰‡ï¼Œ$S$ä»£è¡¨ä¸€ä¸ªå·ç§¯ç¥žç»ç½‘ç»œã€‚æµ‹åº¦æ¨¡å—ä»¥åŠç›®æ ‡æ¨¡å—éƒ½æ˜¯ç”±ä¸€ä¸ªå…¨è¿žæŽ¥ç¥žç»ç½‘ç»œæž„æˆã€‚ä¸‰è€…çš„è¾“å‡ºè¿žæŽ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªè”åˆçš„è¾“å…¥è¡¨ç¤ºï¼Œä¾›åŽç»­ç®—æ³•ä½¿ç”¨ï¼š$$\mathbf{j} = J(\mathbf{s, m, g}) = \langle S(\mathbf{s}), M(\mathbf{m}), G(\mathbf{g}) \rangle$$DFPç½‘ç»œé‡‡ç”¨äº†DQNçš„åšæ³•ï¼Œä¸€æ¬¡æ€§è¾“å‡ºæ‰€æœ‰actionå¯¹åº”çš„measurement streamã€‚ä½†æ˜¯æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿç€é‡å…³æ³¨å¯¹actionä¹‹é—´å·®å¼‚çš„å­¦ä¹ ã€‚å› æ­¤é‡‡ç”¨äº†Wang et al. (ICML 2016) è¿™ç¯‡æ–‡ç« ä¸­æ‰åŽ»çš„åšæ³•ï¼Œå°†é¢„æµ‹æ¨¡å—åˆ†ä¸ºä¸¤ä¸ªstreamï¼Œä¸€ä¸ªæœŸæœ›stream $E(\text{j})$ ä»¥åŠä¸€ä¸ªaction stream $A(\text{j})$ã€‚æ³¨æ„è¿™ä¸¤ä¸ªstreaméƒ½æ˜¯ä¸€ä¸ªå…¨è¿žæŽ¥çš„ç¥žç»ç½‘ç»œã€‚æœŸæœ›streamçš„ç›®æ ‡æ˜¯é¢„æµ‹æ‰€æœ‰actionèƒ½å¤ŸèŽ·å¾—çš„measurement streamçš„æœŸæœ›ã€‚Action streamå…³æ³¨ä¸åŒactionä¹‹é—´çš„å·®å¼‚ã€‚å…¶ä¸­ï¼Œ$A(\text{j}) = \langle A^1(\text{j}), \cdots, A^{w}(\text{j}) \rangle$ï¼Œ$w = |\mathcal{A}|$ä»£è¡¨æ‰€æœ‰å¯èƒ½actionçš„ä¸ªæ•°ã€‚åŒæ—¶æˆ‘ä»¬è¿˜åœ¨åŠ å…¥äº†ä¸€ä¸ªæ­£åˆ™åŒ–å±‚ï¼š$$\overline{A^{i}}(\mathbf{j}) = A^{i}(\mathbf{j}) - \frac{1}{w}\sum_{k=1}^{w} A^{k}(\mathbf{j})$$æ­£åˆ™åŒ–å±‚å¯¹æ¯ä¸€ä¸ªactionçš„é¢„æµ‹å€¼å‡åŽ»äº†æ‰€æœ‰actioné¢„æµ‹å€¼çš„æœŸæœ›ï¼Œè¿™æ ·å°±å¼ºåˆ¶æœŸæœ›streamåŽ»å­¦ä¹ è¿™ä¸ªæœŸæœ›ï¼Œè¿™æ ·action streamå°±å¯ä»¥ç€é‡å…³æ³¨ä¸åŒactionä¹‹é—´çš„å·®å¼‚ã€‚æœ€åŽï¼Œç½‘ç»œçš„è¾“å‡ºå¦‚ä¸‹ï¼š$$\mathbf{p} = \langle \mathbf{p}^{a_1}, \cdots, \mathbf{p}^{a_w} \rangle = \langle \overline{A^1}(\mathbf{j})+E(\mathbf{j}), \cdots, \overline{A^w}(\mathbf{j})+E(\mathbf{j}) \rangle$$ä¸ºäº†éªŒè¯ç½‘ç»œä¸­ä½¿ç”¨çš„ä¸‰ä¸ªè¾…åŠ©ç»“æž„ï¼ˆmeasurement streamè¾“å…¥ï¼Œexpectation-actionåˆ†è§£ä»¥åŠactionæ­£åˆ™åŒ–å±‚ï¼‰çš„ä½œç”¨ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æµ‹è¯•ã€‚æˆ‘ä»¬åŸºäºŽD3åœºæ™¯ï¼ˆä¸‹é¢å®žéªŒéƒ¨åˆ†æåŠï¼‰éšæœºäº§ç”Ÿäº†100ä¸ªåœ°å›¾åœºæ™¯ç”¨ä»¥è®­ç»ƒã€‚åŒæ—¶é‡‡ç”¨basicç½‘ç»œ ï¼ˆä¸‹é¢å®žéªŒéƒ¨åˆ†æåŠï¼‰ï¼Œæœ€åŽçš„å®žéªŒç»“æžœå¦‚ä¸‹ï¼šå¯ä»¥çœ‹å‡ºï¼Œexpectation-actionåˆ†è§£çš„ä½œç”¨æœ€å¤§ï¼ŒåŒæ—¶æˆ‘ä»¬è®¾è®¡çš„measurement streamä¹Ÿæ˜¯ååˆ†é‡è¦çš„ã€‚å®žéªŒåŠç»“æžœå…·ä½“çš„å®žéªŒåœºæ™¯è§ä¸‹å›¾ï¼šåœ¨å‰ä¸¤ä¸ªåœºæ™¯ä¸­ï¼Œagentå¯ä»¥é‡‡å–ä¸‰ä¸ªåŠ¨ä½œï¼Œå‘å‰ç§»åŠ¨ã€å‘å·¦è½¬ã€å‘å³è½¬ã€‚è¿™æ ·ä¸€å…±å°±æœ‰8ç§åŠ¨ä½œç»„åˆã€‚é‡‡ç”¨çš„æµ‹åº¦åªæœ‰ä¸€ç§ï¼Œå°±æ˜¯è¡€é‡ã€‚åœ¨åŽä¸¤ä¸ªåœºæ™¯ä¸­ï¼Œagentå¯ä»¥é‡‡å–å…«ä¸ªåŠ¨ä½œç»„åˆï¼Œåˆ†åˆ«æ˜¯å‘å‰ç§»åŠ¨ã€å‘åŽç§»åŠ¨ã€å‘å·¦è½¬ã€å‘å³è½¬ã€å‘å·¦æ‰«å°„ï¼Œå‘å³æ‰«å°„ã€å¥”è·‘ä»¥åŠå°„å‡»ã€‚è¿™æ ·ä¸€å…±å°±æœ‰256ä¸ªåŠ¨ä½œç»„åˆã€‚é‡‡ç”¨çš„æµ‹åº¦ä¸€å…±æœ‰ä¸‰ç§ï¼Œè¡€é‡ï¼Œå¼¹è¯æ•°ä»¥åŠå‡»æ€æ•°ã€‚è¿™é‡Œæˆ‘è®¤ä¸ºå­˜åœ¨ä¸€ä¸ªå¯ä»¥æ”¹è¿›çš„åœ°æ–¹ï¼Œåº”è¯¥æŽ’é™¤æŽ‰ä¸åˆç†çš„åŠ¨ä½œç»„åˆï¼Œä¾‹å¦‚åŒæ—¶å‘å·¦è½¬ä»¥åŠå‘å³è½¬ã€‚è¿™æ ·å¯ä»¥å‡å°‘æœç´¢ç©ºé—´ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼ŒåŒæ—¶å¯ä»¥æé«˜ç­–ç•¥çš„è´¨é‡ã€‚å®žéªŒä¸­ç½‘ç»œçš„ç»“æž„ä¸ŽDQNçš„ç»“æž„ååˆ†ç±»ä¼¼ï¼Œå‚æ•°ä¹Ÿå°½å¯èƒ½ç›¸è¿‘ï¼Œå°±æ˜¯ä¸ºäº†æ¯”è¾ƒèµ·æ¥æ¯”è¾ƒå…¬å¹³ã€‚å…·ä½“æ¥è¯´ï¼Œå®žéªŒä¸­é‡‡ç”¨äº†ä¸¤ç§ç½‘ç»œï¼Œbasicä»¥åŠlargeï¼Œç»“æž„ç›¸åŒï¼Œä½†æ˜¯å‚æ•°æ•°é‡ä¸åŒï¼šBasicç½‘ç»œçš„å‚æ•°ä¸ŽDQNæ¯”è¾ƒæŽ¥è¿‘ï¼Œä»¥ä¾¿æ¯”è¾ƒã€‚ä¸¤ä¸ªç½‘ç»œåœ¨æ‰€æœ‰çš„éžç»ˆæ­¢å±‚åŽéƒ½åŠ å…¥äº†ä¸€ä¸ªéžçº¿æ€§å±‚ï¼Œé‡‡ç”¨çš„æ¿€æ´»å‡½æ•°ä¸ºLeaky ReLUï¼Œå…·ä½“å‡½æ•°ä¸ºï¼š$$\mathbf{LReLU}(x) = \max(x, 0.2x)$$å‚æ•°åˆå§‹åŒ–æ–¹æ³•é‡‡ç”¨äº†He Initializationï¼Œä»£ç å®žçŽ°å¦‚ä¸‹ï¼š12import numpy as npW = np.random.randn(node_in, node_out) / np.sqrt(node_in / 2)Agentä»¥episodeä¸ºå•ä½è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚æ¯ä¸€ä¸ªepisodeæ‹¥æœ‰525ä¸ªæ—¶é—´æ­¥ï¼ˆå¤§çº¦ä¸€åˆ†é’Ÿï¼‰ï¼Œå¦‚æžœagentæ­»äº¡é‚£ä¹ˆepisodeä¹Ÿä¼šç»ˆæ­¢ã€‚åŒæ—¶å°†æ—¶é—´åç½®$\tau_1, \cdots, \tau_n$è®¾ç½®ä¸º1, 2, 4, 8, 16, 32ã€‚æœ€åŽç»“æžœè¡¨æ˜Žåªæœ‰æœ€æ–°çš„ä¸‰ä¸ªæ—¶é—´æ­¥ï¼ˆ8, 16, 32ï¼‰å¯¹ç»“æžœæœ‰è´¡çŒ®ï¼Œè´¡çŒ®æ¯”ä¾‹ä¸º 1:1:2ã€‚å¦å¤–ï¼Œè¾“å…¥å›¾åƒè¢«è½¬æ¢æˆç°åº¦å›¾åƒï¼Œmeasurement streamå¹¶ä¸æ˜¯ç›´æŽ¥è¾“å…¥ï¼Œè€Œæ˜¯è¿›è¡Œäº†æ­£åˆ™åŒ– ï¼ˆé™¤ä»¥æ ‡å‡†å·®ï¼‰ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜åœ¨è®­ç»ƒä»¥åŠæµ‹è¯•è¿‡ç¨‹ä¸­ä½¿ç”¨frame skippingæŠ€æœ¯ã€‚Agentæ¯éš”4å¸§é‡‡å–ä¸€æ¬¡actionã€‚è¿™äº›è¢«å¿½ç•¥çš„å¸§æ‰€é‡‡å–çš„actionä¸Žå…¶ä¹‹åŽçš„å¸§çš„actionä¸€è‡´ï¼Œç›¸å½“äºŽè¿›è¡Œäº†ä¸€æ¬¡ç®€å•çš„å¤åˆ¶ã€‚å¦å¤–ï¼Œç”±äºŽäººç±»çš„ååº”é€Ÿåº¦è‚¯å®šæ˜¯æ¯”ä¸ä¸Šè®¡ç®—æœºçš„ï¼Œå› æ­¤fram skippingä½¿å¾—agentçš„è¡Œä¸ºæ›´åŠ æŽ¥è¿‘äººç±»ã€‚å¯¹äºŽä¹‹å‰æåˆ°çš„experience replayæŠ€æœ¯ï¼Œå®žéªŒä¸­å°†Må€¼è®¾ä¸º20000ï¼Œ Nè®¾ä¸º64ï¼Œkä¹Ÿè®¾ä¸º64ï¼ˆ$\ge32$ï¼‰ã€‚åŒæ—¶ä¸ºäº†èƒ½å¤Ÿæ›´é«˜æ•ˆçš„èŽ·å¾—è®­ç»ƒé›†$\mathcal{D}$ï¼Œæˆ‘ä»¬åŒæ—¶é‡‡ç”¨8ä¸ªagentå¹¶è¡Œè¿è¡Œã€‚è®­ç»ƒæ—¶é‡‡ç”¨çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸ºAdamç®—æ³•ï¼Œå‚æ•°è®¾ç½®å¦‚ä¸‹ï¼š$\beta_1=0.95, \;\beta_2=0.999,\;\varepsilon=10^{-4}$ã€‚Basicç½‘ç»œè®­ç»ƒäº†800,000æ¬¡mini-batchè¿­ä»£ï¼Œlargeç½‘ç»œè®­ç»ƒäº†2,000,000æ¬¡ã€‚ç®—æ³•å®žçŽ°https://github.com/IntelVCL/DirectFuturePredictionã€‚ä¸‹é¢ä»‹ç»æˆ‘ä»¬çš„baselinesã€‚æˆ‘ä»¬åŒä¸‰ä¸ªç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒï¼šDQN (Mnih et al., 2015), A3C (Mnih et al., 2016), ä»¥åŠ DSR (Kulkarni et al., 2016b)ã€‚DQNç”±äºŽå…¶åœ¨Atariæ¸¸æˆä¸Šçš„ä¼˜å¼‚æ•ˆæžœæˆä¸ºäº†è§†è§‰æŽ§åˆ¶çš„æ ‡å‡†baselineã€‚A3Cæ›´æ˜¯è¿™ä¸ªé¢†åŸŸä¸­çš„æœ€å¥½çš„ç®—æ³•ã€‚DSRä¹Ÿåœ¨Virtual Doomå¹³å°ä¸Šè¿›è¡Œäº†å®žéªŒã€‚æ‰€ä»¥æˆ‘ä»¬æŒ‘é€‰äº†è¿™ä¸‰ä¸ªå…·æœ‰ä»£è¡¨æ„ä¹‰çš„ç®—æ³•ã€‚å¯¹äºŽè¿™ä¸‰ä¸ªç®—æ³•æˆ‘ä»¬éƒ½ä½¿ç”¨äº†Githubä¸Šçš„å¼€æºå®žçŽ°ï¼šDQN (https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner) ã€DSR (https://github.com/Ardavans/DSR), ä»¥åŠ A3C (https://github.com/muupan/async-rl)ã€‚å‰ä¸¤ä¸ªéƒ½æ˜¯ä½œè€…æä¾›çš„æºç ï¼Œæœ€åŽçš„A3Cæ˜¯ä¸€ä¸ªç‹¬ç«‹å¼€å‘è€…çš„ä¸ªäººå®žçŽ°ã€‚å¯¹äºŽDQNä»¥åŠDSRæˆ‘ä»¬æµ‹è¯•äº†ä¸‰ä¸ªå­¦ä¹ é€ŸçŽ‡ï¼šé»˜è®¤å€¼ï¼ˆ0.00025ï¼‰ï¼Œ0.00005ä»¥åŠ0.00002ã€‚å…¶ä»–å‚æ•°ç›´æŽ¥é‡‡ç”¨é»˜è®¤å€¼ã€‚å¯¹äºŽA3Cç®—æ³•ï¼Œä¸ºäº†è®­ç»ƒæ›´å¿«ï¼Œå‰ä¸¤ä¸ªä»»åŠ¡æˆ‘ä»¬é‡‡ç”¨äº†5ä¸ªå­¦ä¹ é€ŸçŽ‡ ({2, 4, 8, 16, 32} Â· $10^{-4}$)ã€‚åŽä¸¤ä¸ªä»»åŠ¡æˆ‘ä»¬è®­ç»ƒäº†20ä¸ªæ¨¡åž‹ï¼Œæ¯ä¸ªæ¨¡åž‹çš„å­¦ä¹ é€ŸçŽ‡ä»Žä¸€ä¸ªèŒƒå›´ä»Ž$10^{-4}$åˆ°$10^{-2}$çš„log-uniformåˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ï¼Œ$\beta$å€¼ï¼ˆç†µæ­£åˆ™é¡¹ï¼‰ä»Žä¸€ä¸ªèŒƒå›´ä»Ž$10^{-4}$åˆ°$10^{-}$çš„lo1g-uniformåˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚ç»“æžœé€‰å–æœ€å¥½çš„ã€‚æœ€ç»ˆç»“æžœå¦‚ä¸‹æ‰€ç¤ºï¼šåœ¨å‰ä¸¤ä¸ªæ¸¸æˆåœºæ™¯ä¸­ï¼Œæ¨¡åž‹å°è¯•æœ€å¤§åŒ–è¡€é‡ï¼›åœ¨åŽä¸¤ä¸ªåœºæ™¯ä¸­å°è¯•æœ€å¤§åŒ–è¡€é‡ã€å¼¹è¯æ•°ä»¥åŠå‡»æ€æ•°çš„ä¸€ä¸ªçº¿æ€§ç»„åˆï¼Œå‚æ•°ä¸º0.5, 0.5, 1ã€‚å› ä¸ºæ¸¸æˆæ›´åŠ ä¾§é‡äºŽé€šè¿‡å‡»æ€æ•°åˆ¤æ–­èƒœè´Ÿã€‚æ‰€æœ‰çš„æ•°æ®éƒ½æ˜¯å¯¹ä¸‰æ¬¡è®­ç»ƒç»“æžœè¿›è¡Œå¹³å‡ï¼Œæ›²çº¿å›¾é‡‡æ ·ç‚¹çš„ä¸ªæ•°ä¸º$3 \times 50,000$ã€‚å¯ä»¥çœ‹å‡ºï¼ŒDFPæ¨¡åž‹å–å¾—äº†æœ€å¥½çš„ç»“æžœã€‚å…¶ä¸­DSRç®—æ³•ç”±äºŽè®­ç»ƒé€Ÿåº¦è¿‡æ…¢ï¼Œæ‰€ä»¥æˆ‘ä»¬åªåœ¨D1åœºæ™¯ï¼ˆä¹Ÿè¿›è¡Œäº†å°†è¿‘10å¤©çš„è®­ç»ƒï¼‰è¿›è¡Œäº†æµ‹è¯•ã€‚ä¸‹é¢è¿›è¡Œæ¨¡åž‹æ³›åŒ–èƒ½åŠ›çš„æµ‹è¯•ï¼Œæˆ‘ä»¬åŸºäºŽD3ä»¥åŠD4ä¸¤ä¸ªåœºæ™¯åˆ†åˆ«éšæœºäº§ç”Ÿ100ä¸ªéšæœºåœºæ™¯ã€‚å…¶ä¸­90ä¸ªç”¨äºŽè®­ç»ƒï¼Œå‰©ä¸‹10ä¸ªç”¨äºŽæµ‹è¯•ã€‚æœ€åŽç»“æžœå¦‚ä¸‹ï¼šå…¶ä¸­æœ€åŽä¸€åˆ—é‡‡ç”¨äº†largeç½‘ç»œã€‚å¯ä»¥çœ‹å‡ºï¼Œä»Žå¤æ‚åœºæ™¯è®­ç»ƒä¹‹åŽï¼Œåœ¨ç®€å•åœºæ™¯ä¸Šçš„æ³›åŒ–èƒ½åŠ›å¾€å¾€ä¸é”™ï¼Œè™½ç„¶ä¸¤è€…è§„åˆ™ä¸åŒã€‚ä½†æ˜¯åä¹‹åˆ™ä¸å¯ä»¥ã€‚æŽ¥ä¸‹æ¥è¿›è¡Œå­¦ä¹ å˜åŒ–ç›®æ ‡èƒ½åŠ›çš„æµ‹è¯•ã€‚ç»“æžœè§ä¸‹å›¾ï¼šå…¶ä¸­é‡‡ç”¨ç¬¬äºŒåˆ—çš„ç­–ç•¥æ—¶ï¼Œagentå¹¶ä¸çŸ¥é“æ¯ä¸€ä¸ªmeasurementçš„ç›¸å¯¹é‡è¦æ€§ï¼›æœ€åŽä¸€åˆ—ï¼Œagentäº‹å…ˆå¹¶ä¸çŸ¥é“å“ªä¸€ä¸ªmeasurementæ˜¯ä¸éœ€è¦è€ƒè™‘çš„ã€‚ä½†æ˜¯æœ€åŽæµ‹è¯•æ—¶ï¼Œæ•ˆæžœéƒ½å¾ˆå¥½ï¼Œè€Œä¸”åœ¨å›ºå®šç›®æ ‡ç­–ç•¥æ²¡æœ‰è§è¿‡çš„ç›®æ ‡ä¸Šçš„æ•ˆæžœè¦æ›´å¥½ã€‚è¯´æ˜ŽDFPæ¨¡åž‹å¯¹äºŽå˜åŒ–ç›®æ ‡çš„å­¦ä¹ èƒ½åŠ›ä¼˜å¼‚ã€‚æœ€åŽæˆ‘ä»¬å•ç‹¬å¯¹measurement streamæ—¶é—´åç½®çš„é‡è¦æ€§è¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬é‡‡ç”¨çš„æ˜¯D3-txè®­ç»ƒé›†ï¼Œæœ€åŽç»“æžœå¦‚å›¾ï¼šç›¸æ¯”è¾ƒè€Œè¨€ï¼Œé‡‡ç”¨æ›´å¤šçš„æ—¶é—´åç½®å¯ä»¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæžœã€‚æ€»ç»“çŽ°åœ¨å¼ºåŒ–å­¦ä¹ é—®é¢˜å…³æ³¨çš„é‡ç‚¹è¿˜æ˜¯åœ¨value functionçš„ä¼°è®¡ä¸Šï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ æ¨¡åž‹ä¸€èˆ¬é‡‡ç”¨ä¸€ä¸ªæ·±åº¦ç½‘ç»œç›´æŽ¥å¯¹value functionè¿›è¡Œä¼°è®¡ã€‚è¿™ç¯‡è®ºæ–‡çš„åˆ›æ–°ç‚¹åœ¨äºŽï¼Œåœ¨ä½¿ç”¨æ·±åº¦ç½‘ç»œä¹‹å‰ï¼Œå¯¹value functionè¿›è¡Œäº†ä¸¤æ¬¡é¢å¤–çš„æ˜ å°„ã€‚ç¬¬ä¸€æ¬¡æ˜¯ç”¨measurement streamæ¥ä»£æ›¿rewardï¼Œä½¿å¾—rewardå…·æœ‰æ›´å¼ºçš„çŠ¶æ€è¡¨ç¤ºèƒ½åŠ›ï¼›å…¶æ¬¡ï¼Œå¯¹measurement streamå†æ¬¡è¿›è¡Œäº†ä¸€ä¸ªå‡½æ•°æ˜ å°„ï¼Œé‡‡ç”¨äº†æ—¶é—´åç½®ï¼Œå€Ÿé‰´äº†n-step Q-learningçš„æ€æƒ³ã€‚æœ€åŽï¼Œå†å°†è¾“å‡ºä½œä¸ºæ·±åº¦ç½‘ç»œçš„è¾“å…¥ï¼Œè¿›è¡Œvalue functionçš„ä¼°è®¡ã€‚æœ€åŽçš„å®žéªŒç»“æžœè¯æ˜Žè¿™ç§æƒ³æ³•æ˜¯æœ‰å…¶æ­£ç¡®æ€§çš„ã€‚]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Monte Carlo Methods (Reinforcement Learning)]]></title>
      <url>%2F2017%2F06%2F02%2FMonte-Carlo-Methods-Reinforcement-Learning%2F</url>
      <content type="text"><![CDATA[Here we consider our first learning methods for estimating value functions and discovering optimal policy. Unlike the previous algorithms, we do not assume complete knowledge of the environment. Monte Carlo methods require only experience â€“ sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Although a model is required, the model need only generate sample transition, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP).Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample return. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. Only on the completion of an episode are value estimates and policies changed.To handle the nonstationarity, we adapt the idea of general policy iteration (GPI) developed in earlier for DP.Suppose we wish to estimate $v_{\pi}(s)$, the value of a state $s$ under policy $\pi$, given a set of episodes obtained by following $\pi$ and passing though $s$. Each occurrence of state $s$ in an episode is called a visit to $s$. Let us call the first time it is visited in an episode the first visit to $s$. The first-visit MC method estimates $v_{\pi}(s)$ as the average of the returns following first visits to $s$, whereas the every-visit MC method averages the returns following all visits to $s$.First-visit MC policy evaluation (returns $V \approx v_{\pi}$)Initialize:â€‹ $\pi \leftarrow$ policy to be evaluatedâ€‹ $V \leftarrow $ an arbitrary state-value functionâ€‹ $Return(s) \leftarrow$ an empty list, for all $s \in \mathcal{S}$Repeat forever:â€‹ Generate an episode using $\pi$â€‹ For each state $s$ appearing in the episode:â€‹ $G \leftarrow$ return following the first occurrence of $s$â€‹ Append $G$ to $Return(s)$â€‹ $V(s) \leftarrow$ $\text{average}(Return(s))$Next, weâ€™ll use this algorithm to solve a naive problem that defined as follows:The object of the popular casino card game of blackjack is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealerâ€™s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a natural. He then wins unless the dealer also has a natural, in which case the game is draw. If the player does not have a natural, then he can request additional cards, one by one (hits), until he either stop (sticks) or excepted 21 (goes bust). If he goes bust, he loses; if he sticks, then it becomes the dealerâ€™s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcomeâ€“win, lose, drawâ€“is determined by whose final sum is closer to 21.Playing blackjack is naturally formulated as an episode finite MDP. Each game of blackjack is an episode. Rewards of +1, -1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount ($\gamma=1$); therefore these terminal rewards are also returns. The playerâ€™s actions are to hit or to stick. We assume that cards are dealt from an infinite deck (i.e., with replacement). If the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable. Consider the policy that sticks if the playerâ€™s sum is 20 or 21, and otherwise hits. Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealerâ€™s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.Note that, although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next eventsâ€“in particular, they require the quantities $p(s^{\prime}, r|s, a)$â€“and it is not easy to determine these for blackjack. For example, suppose the playâ€™s sum is 14 and he chooses to sticks. What is his excepted reward as a function of the dealerâ€™s showing card? All of these rewards and transition probabilities must be computed before DP can be applied, and such computations are often complex and error-prone.The conceptual diagram of the experimental results is as follows:Figure 1The first we define some auxiliary variables and methods:12345678910111213141516171819202122232425262728293031323334# actions: hit or stand (stick)ACTION_HIT = 0ACTION_STAND = 1actions = [ACTION_HIT, ACTION_STAND]# policy for playerpolicyPlayer = np.zeros(22)for i in range(12, 20): policyPlayer[i] = ACTION_HITpolicyPlayer[20] = ACTION_STANDpolicyPlayer[21] = ACTION_STAND# function form of target policy of playerdef targetPolicyPlayer(usableAcePlayer, playerSum, dealerCard): return policyPlayer[playerSum]# function form of behavior policy of playerdef behaviorPolicyPlayer(usableAcePlayer, playerSum, dealerCard): if np.random.binomial(1, 0.5) == 1: return ACTION_STAND return ACTION_HIT# policy for dealerpolicyDealer = np.zeros(22)for i in range(12, 17): policyDealer[i] = ACTION_HITfor i in range(17, 22): policyDealer[i] = ACTION_STAND# get a new carddef getCard(): card = np.random.randint(1, 14) card = min(card, 10) return cardFurthermore, we also have a print method:1234567891011121314151617181920# print the state valuefigureIndex = 0def prettyPrint(data, tile, zlabel='reward'): global figureIndex fig = plt.figure(figureIndex) figureIndex += 1 fig.suptitle(tile) ax = fig.add_subplot(111, projection='3d') axisX = [] axisY = [] axisZ = [] for i in range(12, 22): for j in range(1, 11): axisX.append(i) axisY.append(j) axisZ.append(data[i - 12, j - 1]) ax.scatter(axisX, axisY, axisZ) ax.set_xlabel('player sum') ax.set_ylabel('dealer showing') ax.set_zlabel(zlabel)In order to get the figure above, we wrote the following code:12345678def onPolicy(): statesUsableAce1, statesNoUsableAce1 = monteCarloOnPolicy(10000) statesUsableAce2, statesNoUsableAce2 = monteCarloOnPolicy(500000) prettyPrint(statesUsableAce1, 'Usable Ace, 10000 Episodes') prettyPrint(statesNoUsableAce1, 'No Usable Ace, 10000 Episodes') prettyPrint(statesUsableAce2, 'Usable Ace, 500000 Episodes') prettyPrint(statesNoUsableAce2, 'No Usable Ace, 500000 Episodes') plt.show()There is a term named on policy, weâ€™ll explain this term later. Now let us jump into the monteCarloOnPolicy method:12345678910111213141516171819# Monte Carlo Sample with On-Policydef monteCarloOnPolicy(nEpisodes): statesUsableAce = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided statesUsableAceCount = np.ones((10, 10)) statesNoUsableAce = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided statesNoUsableAceCount = np.ones((10, 10)) for i in range(0, nEpisodes): state, reward, _ = play(targetPolicyPlayer) state[1] -= 12 state[2] -= 1 if state[0]: statesUsableAceCount[state[1], state[2]] += 1 statesUsableAce[state[1], state[2]] += reward else: statesNoUsableAceCount[state[1], state[2]] += 1 statesNoUsableAce[state[1], state[2]] += reward return statesUsableAce / statesUsableAceCount, statesNoUsableAce / statesNoUsableAceCountWe ignore he first four variables now and explain them later. nEpisodes represents the number of the episodes and the play method simulates the process of the blackjack cards game. So what is it like? This method is too long (more than 100 rows) so we partially explain it. In the first place, this method define some auxiliary variables:12345678910111213141516# play a game# @policyPlayerFn: specify policy for player# @initialState: [whether player has a usable Ace, sum of player's cards, one card of dealer]# @initialAction: the initial actiondef play(policyPlayerFn, initialState=None, initialAction=None): # player status # sum of player playerSum = 0 # trajectory of player playerTrajectory = [] # whether player uses Ace as 11 usableAcePlayer = False # dealer status dealerCard1 = 0 dealerCard2 = 0 usableAceDealer = FalseThen, we generate a random initial state if the initial state is null. If not, we load the initial state from initialState variable:12345678910111213141516171819202122232425262728293031323334353637383940414243444546if initialState is None: # generate a random initial state numOfAce = 0 # initialize cards of player while playerSum &lt; 12: # if sum of player is less than 12, always hit card = getCard() # if get an Ace, use it as 11 if card == 1: numOfAce += 1 card = 11 usableAcePlayer = True playerSum += card # if player's sum is larger than 21, he must hold at least one Ace, two Aces are possible if playerSum &gt; 21: # use the Ace as 1 rather than 11 playerSum -= 10 # if the player only has one Ace, then he doesn't have usable Ace any more if numOfAce == 1: usableAcePlayer = False # initialize cards of dealer, suppose dealer will show the first card he gets dealerCard1 = getCard() dealerCard2 = getCard() else: # use specified initial state usableAcePlayer = initialState[0] playerSum = initialState[1] dealerCard1 = initialState[2] dealerCard2 = getCard() # initial state of the game state = [usableAcePlayer, playerSum, dealerCard1] # initialize dealer's sum dealerSum = 0 if dealerCard1 == 1 and dealerCard2 != 1: dealerSum += 11 + dealerCard2 usableAceDealer = True elif dealerCard1 != 1 and dealerCard2 == 1: dealerSum += dealerCard1 + 11 usableAceDealer = True elif dealerCard1 == 1 and dealerCard2 == 1: dealerSum += 1 + 11 usableAceDealer = True else: dealerSum += dealerCard1 + dealerCard2Game start! Above all is playerâ€™s turn:1234567891011121314151617181920212223242526# player's turnwhile True: if initialAction is not None: action = initialAction initialAction = None else: # get action based on current sum action = policyPlayerFn(usableAcePlayer, playerSum, dealerCard1) # track player's trajectory for importance sampling playerTrajectory.append([action, (usableAcePlayer, playerSum, dealerCard1)]) if action == ACTION_STAND: break # if hit, get new card playerSum += getCard() # player busts if playerSum &gt; 21: # if player has a usable Ace, use it as 1 to avoid busting and continue if usableAcePlayer == True: playerSum -= 10 usableAcePlayer = False else: # otherwise player loses return state, -1, playerTrajectoryThen is the dealerâ€™s turn if the playerâ€™s turn is end:12345678910111213141516while True: # get action based on current sum action = policyDealer[dealerSum] if action == ACTION_STAND: break # if hit, get a new card dealerSum += getCard() # dealer busts if dealerSum &gt; 21: if usableAceDealer == True: # if dealer has a usable Ace, use it as 1 to avoid busting and continue dealerSum -= 10 usableAceDealer = False else: # otherwise dealer loses return state, 1, playerTrajectoryIf the both sides have finished the game:1234567# compare the sum between player and dealerif playerSum &gt; dealerSum: return state, 1, playerTrajectoryelif playerSum == dealerSum: return state, 0, playerTrajectoryelse: return state, -1, playerTrajectoryNow, let us come back the mentoCarloOnPolicy method:123456789101112131415161718def monteCarloOnPolicy(nEpisodes): statesUsableAce = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided statesUsableAceCount = np.ones((10, 10)) statesNoUsableAce = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided statesNoUsableAceCount = np.ones((10, 10)) for i in range(0, nEpisodes): state, reward, _ = play(targetPolicyPlayer) state[1] -= 12 state[2] -= 1 if state[0]: statesUsableAceCount[state[1], state[2]] += 1 statesUsableAce[state[1], state[2]] += reward else: statesNoUsableAceCount[state[1], state[2]] += 1 statesNoUsableAce[state[1], state[2]] += reward return statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCountIn this method we ignore the playerâ€™s trajectory (represent by the playerTrajectory variable). If you remember a sentence in the game definition (as follows) it will easy to understand.Thus, the player makes decisions on the basis of three variables: his current sum (12-21), the dealerâ€™s one showing card (ace-10), and whether or not he holds a usable ace. This makes for a total of 200 states.This row (as follows) is to calculate the average returns of each state:1return statesUsableAce / statesUsableeCount, statesNoUsableAce / statesNoUsableAceCountRecall the beginning of the code and letâ€™s see what results are like:Figure 2 (from up to down). (1) Usable Ace, 10000 Episodes. (2) No Usable Ace, 10000 Episodes. (3) Usable Ace, 500000 Episodes. (4) No Usable Ace, 500000 Episodes.If a model is not available, then it is particularly useful to estimate action values (the value of state-value pairs) rather than state values. With a model, state values alone are sufficient to determine a policy; one simply look ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the earlier on DP. Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of out primary goals for Monte Carlo methods is to estimate $q_{\star}$. To achieve this, we first consider the policy evaluation problem for action values.The policy evaluation problem for action values is estimate $q_{\pi}(s, a)$. The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a state-action pair rather than to a state. The only complication is that many state-value pairs may never be visited. This is the general problem of maintaining exploration, as discussed in the context of the k-armed bandit problem in here. One way to do this is by specifying that the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start. We call this the assumption of exploring starts.We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal policies. To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with a arbitrary policy $\pi_{0}$ and ending with the optimal policy and optimal action-value function:$$\pi_{0} \stackrel{E}\longrightarrow q_{\pi{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow q_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow q_{\star},$$We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for Monte Carlo method. One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes. We postpone consideration of the first assumption until later.The primary approach to avoiding the infinite number of episodes nominally required for policy evaluation is to forgo to complete policy evaluation before returning to policy improvement. For Monte Carlo policy evaluation it it natural to alternate between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation,and then the policy is improved at all the states visited in the episode.Monte Carlo ES (Exploring Starts)Initialize, for all $s \in \mathcal{S},\; a \in \mathcal{A(s)}$:â€‹ $Q(s,a) \leftarrow \text{arbitrary}$â€‹ $\pi(s) \leftarrow \text{arbitrary}$â€‹ $Returns(s,a) \leftarrow \text{empty list}$Repeat forever:â€‹ Choose $S_{0} \in \mathcal{S}$ and $A_{0} \in \mathcal{A(S_{0})}$ s.t. all pairs have probability &gt; 0â€‹ Generate an episode starting from $S_0, A_0$, following $\pi$â€‹ For each pair $s, a$ appearing in the episode:â€‹ $G \leftarrow \text{return following the first occurrence of} \; s, a$â€‹ Append $G$ to $Returns(s,a)$â€‹ $Q(s,a) \leftarrow \text{average}(Returns(s,a))$â€‹ For each $s$ in the episode:â€‹ $\pi(s) \leftarrow \arg\max_{a} Q(s,a)$Recall the blackjack card game. It is straightforward to apply Monte Carlo ES to blackjack.123456789101112131415161718def figure5_3(): stateActionValues = monteCarloES(500000) stateValueUsableAce = np.zeros((10, 10)) stateValueNoUsableAce = np.zeros((10, 10)) # get the optimal policy actionUsableAce = np.zeros((10, 10), dtype='int') actionNoUsableAce = np.zeros((10, 10), dtype='int') for i in range(10): for j in range(10): stateValueNoUsableAce[i, j] = np.max(stateActionValues[i, j, 0, :]) stateValueUsableAce[i, j] = np.max(stateActionValues[i, j, 1, :]) actionNoUsableAce[i, j] = argmax(stateActionValues[i, j, 0, :]) actionUsableAce[i, j] = argmax(stateActionValues[i, j, 1, :]) prettyPrint(stateValueUsableAce, 'Optimal state value with usable Ace') prettyPrint(stateValueNoUsableAce, 'Optimal state value with no usable Ace') prettyPrint(actionUsableAce, 'Optimal policy with usable Ace', 'Action (0 Hit, 1 Stick)') prettyPrint(actionNoUsableAce, 'Optimal policy with no usable Ace', 'Action (0 Hit, 1 Stick)') plt.show()Run the code weâ€™ll get the conceptual diagram like follows:Let us to see the implementation (monteCarloES method) of this algorithm. Note that, some auxiliary variables are defined earlier.12345678910111213141516171819202122232425262728293031# Monte Carlo with Exploring Startsdef monteCarloES(nEpisodes): # (playerSum, dealerCard, usableAce, action) stateActionValues = np.zeros((10, 10, 2, 2)) # set default to 1 to avoid being divided by 0 stateActionPairCount = np.ones((10, 10, 2, 2)) # behavior policy is greedy def behaviorPolicy(usableAce, playerSum, dealerCard): usableAce = int(usableAce) playerSum -= 12 dealerCard -= 1 return argmax(stateActionValues[playerSum, dealerCard, usableAce, :]) # play for several episodes for episode in range(nEpisodes): print('episode:', episode) # for each episode, use a randomly initialized state and action initialState = [bool(np.random.choice([0, 1])), np.random.choice(range(12, 22)), np.random.choice(range(1, 11))] initialAction = np.random.choice(actions) _, reward, trajectory = play(behaviorPolicy, initialState, initialAction) for action, (usableAce, playerSum, dealerCard) in trajectory: usableAce = int(usableAce) playerSum -= 12 dealerCard -= 1 # update values of state-action pairs stateActionValues[playerSum, dealerCard, usableAce, action] += reward stateActionPairCount[playerSum, dealerCard, usableAce, action] += 1 return stateActionValues / stateActionPairCountYou can see we use the trajectory variable now. The difference between Monte Carlo On Policy and Monte Carlo ES is prior calculates the average return of each state and later calculates the average return of each state-action pair.The results are as follows:How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call on-policy (Do you remember this term?) methods and off-policy methods. On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a policy different from that used to generate the data.. The First-visit Monte Carlo method and the Monte Carlo ES method are an example of an on-policy method. Here we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. Off-policy methods are considered later.The on-policy method we presented in here uses $\epsilon \text{-} greedy$ policies. The complete algorithm is given below.On-policy first-visit MC control (for $\epsilon \text{-soft}$ policies)Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:â€‹ $Q(s,a ) \leftarrow \text{arbitrary}$â€‹ $Returns(s,a) \leftarrow \text{empty list}$â€‹ $\pi(a|s) \leftarrow \text{an arbitrary} \; \epsilon \text{-soft policy}$Repeat forever:â€‹ (a) Generate an episode using $\pi$â€‹ (b) For each pair $s, a$ appearing in the episode:â€‹ $G \leftarrow $ return following the first occurrence of $s, a$â€‹ Append $G$ to $Returns(s,a)$â€‹ $Q(s, a) \leftarrow \text{average}(Returns(s,a))$â€‹ (c) For each s in the episode:â€‹ $A^{\star} \leftarrow \arg\max_{a} Q(s,a)$â€‹ For all $a \in \mathcal{A(s)}$:â€‹ $\pi(a|s) \leftarrow \left\{ \begin{array}{c} 1 - \epsilon + \epsilon/|\mathcal{A(s)}|\;\;\;\text{if} \;a=A^{\star} \\ \epsilon/|\mathcal{A(s)}|\;\;\;\;\;\;\;\;\;\;\;\;\;\;\text{if} \;a\neq A^{\star} \end{array} \right.$All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to find the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromiseâ€“it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to user two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy. In this case we say that learning is from data â€œoffâ€ the target policy, and the overall process is termed off-policy learning.We begin the study of off-policy methods by considering the prediction problem, in which both target and behavior policies are fixed. That is, suppose we wish to estimate $v_{\pi}$ or $q_{\pi}$, but all we have are episodes following another policy $\mu$, where $\mu \neq \pi$. In this case, $\pi$ is the target policy, $\mu$ is the behavior policy, and both policies are considered fixed and given.In order to use episodes from $\mu$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $\mu$. That is, we require that $\pi(a|s) &gt; 0$ implies $\mu(a|s) &gt; 0$. This is called the assumption of converge. It follows from converge that $\mu$ must be stochastic in states where it is not identical to $\pi$. The target policy $\pi$, on the other hand, may be deterministic. In here, we consider the prediction problem, in which $\pi$ is unchanging and given.Almost all off-policy methods utilize importance samplingddd, a general technique for estimating excepted values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectory occurring under the target and behavior policies, called the importance-sampling ratio. Given a starting state $S_{t}$, the probability of the subsequent state-action trajectory , $A_{t}, S_{t+1}, A_{t+1, \cdots, S_{T}}$, occurring under any policy $\pi$ is$$\prod_{k=t}^{T-1} \pi(A_k | S_k)p(S_{k+1} | S_k, A_k),$$where $p$ here is the state-transition probability function. Thus, the relative probability of the trajectory under the target and behavior policies (the important-sampling ratio) is$$\rho_{t}^{T} \doteq \frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k, A_k)}{\prod_{k=t}^{T-1} \mu(A_k|S_k)p(S_{k+1}|S_k, A_k)} = \prod_{k=t}^{T-1} \frac{\pi (A_k | S_k)}{\mu (A_k | S_k)}$$Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy $\mu$ to estimate $v_{\pi}(s)$. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time $t=101$. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state $s$ is visited, denote $\tau(s)$. This is for an every-visit method; for a first-visit method, $\tau(s)$ would only include time steps that were first visits to $s$ within their episodes. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up though $T(t)$. Then $\{G_t\}_{t \in \tau(s)}$ are returns that pertain to state $s$, and $\{\rho_{t}^{T(t)}\}_{t \in \tau(s)}$ are the corresponding importance-sampling ratios. To estimate $v_{\pi}(s)$, we simply scale the returns by the ratios and average the results:$$V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{|\tau(s)|}.$$When importance sampling is done as a simple average in this way it is called ordinary importance sampling.An important alternative is weighted importance sampling, which uses a weighted average, defined as$$V(s) \doteq \frac{\sum_{t \in \tau(s)} \rho_{t}^{T(t)} G_t}{\sum_{t \in \tau(s)} \rho_{t}^{T(t)}},$$or zero if the denominator is zero.We applied both ordinary and weighted importance-sampling methods to estimate the value of as single blackjack state from off-policy data. Recall that one of the advantages of Monte Carlo methods is that they can be used to evaluate a single state without forming estimated for any other states. In this example, we evaluated the state in which the dealer is showing a deuce, the sum of the playerâ€™s cards is 13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three aces). The data was generate by starting in this state then choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21. The value of this state under the target policy is approximately -0.27726 (this was determined by separately generating one-hundred million episodes using the target policy and averaging their returns). Both off-policy methods closely approximated this value after 1000 off-policy episodes using the random policy. To make sure they did this reliably, we performed 100 independent runs, each starting from estimates of zero and learning for 10,000 episodes. The implementation details are as follows:12345678910111213141516171819202122232425262728293031# Monte Carlo Sample with Off-Policydef monteCarloOffPolicy(nEpisodes): initialState = [True, 13, 2] sumOfImportanceRatio = [0] sumOfRewards = [0] for i in range(0, nEpisodes): _, reward, playerTrajectory = play(behaviorPolicyPlayer, initialState=initialState) # get the importance ratio importanceRatioAbove = 1.0 importanceRatioBelow = 1.0 for action, (usableAce, playerSum, dealerCard) in playerTrajectory: if action == targetPolicyPlayer(usableAce, playerSum, dealerCard): importanceRatioBelow *= 0.5 else: importanceRatioAbove = 0.0 break importanceRatio = importanceRatioAbove / importanceRatioBelow sumOfImportanceRatio.append(sumOfImportanceRatio[-1] + importanceRatio) sumOfRewards.append(sumOfRewards[-1] + reward * importanceRatio) del sumOfImportanceRatio[0] del sumOfRewards[0] sumOfRewards= np.asarray(sumOfRewards) sumOfImportanceRatio= np.asarray(sumOfImportanceRatio) ordinarySampling = sumOfRewards / np.arange(1, nEpisodes + 1) with np.errstate(divide='ignore',invalid='ignore'): weightedSampling = np.where(sumOfImportanceRatio != 0, sumOfRewards / sumOfImportanceRatio, 0) return ordinarySampling, weightedSamplingNote that the behaviorPolicyPlayer that is a function that define the behavior policy:12345# function form of behavior policy of playerdef behaviorPolicyPlayer(usableAcePlayer, playerSum, dealerCard): if np.random.binomial(1, 0.5) == 1: return ACTION_STAND return ACTION_HITAnd the calculation of the numerator and denominator of the importance-sampling are the summation operation. As the number of iterations increases, we need to accumulate the result from each episode. So we need to store the previous results. The sumOfRewards and sumOfImportanceRatio are used for this purpose.Then we need to show the result (mean square error):123456789101112131415161718192021# Figure 5.4def offPolicy(): trueValue = -0.27726 nEpisodes = 10000 nRuns = 100 ordinarySampling = np.zeros(nEpisodes) weightedSampling = np.zeros(nEpisodes) for i in range(0, nRuns): ordinarySampling_, weightedSampling_ = monteCarloOffPolicy(nEpisodes) # get the squared error ordinarySampling += np.power(ordinarySampling_ - trueValue, 2) weightedSampling += np.power(weightedSampling_ - trueValue, 2) ordinarySampling /= nRuns weightedSampling /= nRuns axisX = np.log10(np.arange(1, nEpisodes + 1)) plt.plot(axisX, ordinarySampling, label='Ordinary Importance Sampling') plt.plot(axisX, weightedSampling, label='Weighted Importance Sampling') plt.xlabel('Episodes (10^x)') plt.ylabel('Mean square error') plt.legend() plt.show()Result is as follows:Formally, the difference between the two kinds of importance sampling is expressed in their biases and variances. The ordinary importance-sampling estimator is unbiased whereas the weighted importance-sampling estimator is biased (the bias converges asymptotically to zero). On the other hand, the variance of the ordinary importance-sampling estimator is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximate that we explore later.The estimates of ordinary importance sampling will typical have infinite variance, and this can easily happen in off-policy learning when trajectories contains loops. A simple example is shown below:There is only one nonterminal state $s$ and two action, end and back. The end action causes a deterministic transition to termination, whereas the back action transitions, with probability 0.9, back to $s$ or, with probability 0.1, on to termination. The rewards are +1 on latter transition and otherwise zero. Consider the target policy that always selects back. All episodes under this policy consist of some number (possibly zero) of transitions back to $s$ followed by termination with a reward and return of +1. Thus the value of $s$ under the target policy is 1. Suppose we are estimating this value from off-policy data using the behavior policy that selects end and back with equal probability.The implementation details are as follows. We first define the two policies:12345678910ACTION_BACK = 0ACTION_END = 1# behavior policydef behaviorPolicy(): return np.random.binomial(1, 0.5)# target policydef targetPolicy(): return ACTION_BACKThen we define how an episode runs:1234567891011# one turndef play(): # track the action for importance ratio trajectory = [] while True: action = behaviorPolicy() trajectory.append(action) if action == ACTION_END: return 0, trajectory if np.random.binomial(1, 0.9) == 0: return 1, trajectoryNow we start our off-policy (first-visit MC) learning process:123456789101112131415161718192021# Figure 5.5def monteCarloSample(): runs = 10 episodes = 100000 axisX = np.log10(np.arange(1, episodes + 1)) for run in range(0, runs): sumOfRewards = [0] for episode in range(0, episodes): reward, trajectory = play() if trajectory[-1] == ACTION_END: importanceRatio = 0 # Because it is impossible on the target policy else: importanceRatio = 1.0 / pow(0.5, len(trajectory)) sumOfRewards.append(sumOfRewards[-1] + importanceRatio * reward) del sumOfRewards[0] estimations = np.asarray(sumOfRewards) / np.arange(1, episodes + 1) plt.plot(axisX, estimations) plt.xlabel('Episodes (10^x)') plt.ylabel('Ordinary Importance Sampling') plt.show() returnResult is as follows:The correct estimate here is 1, and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite, and the estimates do not convergence to this value.At last, we proposed two fancy algorithms, that is, the Incremental off-policy every-visit MC policy evaluation and the Off-policy every-visit MC control.Incremental off-policy every-visit MC policy evaluationInitialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:â€‹ $Q(s,a) \leftarrow$ arbitraryâ€‹ $C(s,a) \leftarrow$ 0â€‹ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policyâ€‹ $\pi(a|s) \leftarrow$ an arbitrary target policyRepeat forever:â€‹ Generate an episode using $\mu$:â€‹ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$â€‹ $G \leftarrow 0$â€‹ $W \leftarrow 1$â€‹ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:â€‹ $G \leftarrow \gamma G + R_{t+1}$â€‹ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$â€‹ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$â€‹ $W \leftarrow W\frac{\pi(A_t | S_t)}{\mu(A_t | S_t)}$â€‹ If $W = 0$ then ExitForLoopOff-policy every-visit MC control (returns $\pi \approx \pi_{\star}$)Initialize, for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}$:â€‹ $Q(s,a) \leftarrow$ arbitraryâ€‹ $C(s,a) \leftarrow$ 0â€‹ $\mu(a|s) \leftarrow$ an arbitrary soft behavior policyâ€‹ $\pi(s) \leftarrow$ an deterministic policy that is greedy with respect $Q$Repeat forever:â€‹ Generate an episode using $\mu$:â€‹ $S_0, A_0, R_1, \cdots, S_{T-1}, A_{T-1}, R_{T}, S_{T}$â€‹ $G \leftarrow 0$â€‹ $W \leftarrow 1$â€‹ For $t=T-1, T-2, \cdots \;\text{downto} \; 0$:â€‹ $G \leftarrow \gamma G + R_{t+1}$â€‹ $C(S_t, A_t) \leftarrow C(S_t, A_t) + W$â€‹ $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{W}{C(S_t, A_t)}[G - Q(S_t, A_t)]$â€‹ $\pi(S_t) \leftarrow \arg \max_{a}Q(S_t, a)$ (with ties broken consistently)â€‹ If $A_t \neq \pi(S_t)$ then ExitForLoopâ€‹ $W \leftarrow W\frac{1}{\mu(A_t | S_t)}$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dynamic Programming]]></title>
      <url>%2F2017%2F05%2F31%2FDynamic-Programming%2F</url>
      <content type="text"><![CDATA[The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are provides an essential foundation for the understanding of the methods presented later. In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies. In here we show how DP can be used to compute the value functions defined in earlier. As discussed there, we can easily obtain optimal policies once we have found the optimal value functions $v_{\star}$ or $q_{\star}$ which satisfy the Bellman optimality equations:$$\begin{align}v_{\star}(s) &amp;= \max_{a} \mathbb{E} [R_{t+1} + \gamma v_{\star}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\&amp;= \max_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) \left[r + \gamma v_{\star}(s^{\prime})\right]\end{align}$$or$$\begin{align}q_{\star}(s, a) &amp;= \mathbb{E} [R_{t+1} + \gamma \max_{a^{\prime}} q_{\star}(S_{t+1}, a^{\prime}) \ | \ S_{t}=s, A_{t}=a] \\&amp;= \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma \max_{a^{\prime}} q_{\star}(s^{\prime}, a^{\prime})]\end{align}$$for all $s \in \mathcal{S}, \; a \in \mathcal{A(s)}, \; \text{and} \; s^{\prime} \in \mathcal{S^{+}}$. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.First we consider how to compute the state-value function $v_{\pi}$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem. Recall that for all $s \in \mathcal{S}$,$$\begin{align}v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots \ | \ S_{t}=s] \\&amp;= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) \ | \ S_{t}=s] \\&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{\pi}(s^{\prime})]\end{align}$$If the environmentâ€™s dynamics are complete known, then (7) is a system of $|\mathcal{S}|$ simultaneous linear equations in $|\mathcal{S}|$ unknowns (the $v_{\pi}(s), s \in \mathcal{S}$). In principle, its solution is a straightforward, if tedious, computation. For our purpose, iterative solution methods are most suitable. The initial approximation, $v_0$, is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation for $v_{\pi}$ as an update rule:$$\begin{align}v_{k+1}(s) &amp;\doteq \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s] \\&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r|s, a) [r + \gamma v_{k} (s^{\prime})]\end{align}$$This algorithm is called iterative policy evaluation.Iterative policy evaluationInput $\pi$, the policy to be evaluatedInitialize an array $V(s) = 0$, for all $s \in \mathcal{S^{+}}$Repeatâ€‹ $\Delta \leftarrow 0$â€‹ for each $s \in \mathcal{S}$:â€‹ $v \leftarrow V(s)$â€‹ $V(s) \leftarrow \sum_{a} \pi(a|s) \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$â€‹ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$until $\Delta &lt; \theta$ (a small positive number)Output $V \approx v_{\pi}$We can see the algorithm used in the grid world problem just is the iterative policy evaluation.Our reason for computing the value function for a policy is to help find better policies. Once a policy $\pi$ has been improved using $v_{\pi}$ to yield a better policy $\pi^{\prime}$, we can then compute $v_{\pi^{\prime}}$and improve it again to yield an even better $\pi^{\prime\prime}$. We can thus obtain a sequence of monotonically improving policies and value functions:$$\pi_{0} \stackrel{E}\longrightarrow v_{\pi_{0}} \stackrel{I}\longrightarrow \pi_{1} \stackrel{E}\longrightarrow v_{\pi_{1}} \stackrel{I}\longrightarrow \pi_{2} \stackrel{E}\longrightarrow \cdots \stackrel{I}\longrightarrow \pi_{\star} \stackrel{E}\longrightarrow v_{\star},$$where $\stackrel{E}\longrightarrow$ denotes a policy evaluation and $\stackrel{I}\longrightarrow$ denotes a policy improvement. This way of finding an optimal policy is called policy iteration.Policy iteration (using iterative policy evaluation)Initialization$V(s) \in \mathbb{R}$ and $\pi(s) \in \mathcal{A(s)}$ arbitrarily for all $s \in \mathcal{S}$Policy EvaluationRepeatâ€‹ $\Delta \leftarrow 0$â€‹ For each $s \in \mathcal{S}$:â€‹ $v \leftarrow V(s)$â€‹ $V(s) \leftarrow \sum_{s^{\prime}, r} p(s^{\prime}, r | s, \pi(s)) [r + \gamma v(s^{\prime})]$â€‹ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$until $\Delta &lt; \theta$ (a small positive number)Policy Improvementpolicy-stable $\leftarrow$ trueFor each $s \in \mathcal{S}$:â€‹ old-action $\leftarrow$ $\pi_(s)$â€‹ $\pi (s) \leftarrow argmax_{a} \sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v(s^{\prime})]$â€‹ If old-action $\neq \pi(s)$, then policy-stable $\leftarrow$ falseIf policy-stable, then stop and return $V \approx v_{\star} \; \text{and} \; \pi \approx \pi_{\star}$; else go to 2.Let us solve a problem used by policy iteration. The problem defined as follows:Jack manages two locations for a nationwide car rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and it credited \$10 by the national company. If he out of cats at that location, then the business is lost. Cars become available for renting the day after they are returned. To ensure that cars are available where they are needed, Jack ca move them between the two locations overnight, at a cost of \$2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is $n$ is $\frac{\lambda^{n}}{n!}e^{-\lambda}$, where $\lambda$ is the excepted number. Suppose $\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\lambda=0.9$ and formulate this as a continuing finite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight.The excepted result is as follows:Figure 1The first, we define some facts of this problem:1234567891011121314151617# maximum # of cars in each locationMAX_CARS = 20# maximum # of cars to move during nightMAX_MOVE_OF_CARS = 5# expectation for rental requests in first locationRENTAL_REQUEST_FIRST_LOC = 3# expectation for rental requests in second locationRENTAL_REQUEST_SECOND_LOC = 4# expectation for # of cars returned in first locationRETURNS_FIRST_LOC = 3# expectation for # of cars returned in second locationRETURNS_SECOND_LOC = 2DISCOUNT = 0.9# credit earned by a carRENTAL_CREDIT = 10# cost of moving a carMOVE_CAR_COST = 2From the problem definition, we know that in this MDP the states is the number of cars at each location at the end of the day, and the actions are the net numbers of cats moved between the two locations overnight. Each action is a integer that positive number represents the number of cars moving from the first location to second location and vice verse.12345678# current policypolicy = np.zeros((MAX_CARS + 1, MAX_CARS + 1))# current state valuestateValue = np.zeros((MAX_CARS + 1, MAX_CARS + 1))# all possible statesstates = []# all possible actionsactions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)For visualization (Figure 1) convenient, we define a method:123456789101112131415161718192021222324# axes for printing useAxisXPrint = []AxisYPrint = []for i in range(0, MAX_CARS + 1): for j in range(0, MAX_CARS + 1): AxisXPrint.append(i) AxisYPrint.append(j) states.append([i, j])# plot a policy/state value matrixfigureIndex = 0def prettyPrint(data, labels): global figureIndex fig = plt.figure(figureIndex) figureIndex += 1 ax = fig.add_subplot(111, projection='3d') AxisZ = [] for i, j in states: AxisZ.append(data[i, j]) ax.scatter(AxisXPrint, AxisYPrint, AxisZ) ax.set_xlabel(labels[0]) ax.set_ylabel(labels[1]) ax.set_zlabel(labels[2])Next, we define a Poisson function that return the probability:12345678910111213# An up bound for poisson distribution# If n is greater than this value, then the probability of getting n is truncated to 0POISSON_UP_BOUND = 11# Probability for poisson distribution# @lam: lambda should be less than 10 for this functionpoissonBackup = dict()def poisson(n, lam): global poissonBackup key = n * 10 + lam if key not in poissonBackup.keys(): poissonBackup[key] = exp(-lam) * pow(lam, n) / factorial(n) return poissonBackup[key]Now, the preparation is done. Weâ€™ll implement the policy iteration algorithm as follows:12345678910111213141516171819202122232425262728293031323334353637newStateValue = np.zeros((MAX_CARS + 1, MAX_CARS + 1))improvePolicy = FalsepolicyImprovementInd = 0while True: if improvePolicy == True: # start policy improvement print('Policy improvement', policyImprovementInd) policyImprovementInd += 1 newPolicy = np.zeros((MAX_CARS + 1, MAX_CARS + 1)) for i, j in states: actionReturns = [] # go through all actions and select the best one for action in actions: if (action &gt;= 0 and i &gt;= action) or (action &lt; 0 and j &gt;= abs(action)): actionReturns.append(expectedReturn([i, j], action, stateValue)) else: actionReturns.append(-float('inf')) bestAction = argmax(actionReturns) newPolicy[i, j] = actions[bestAction] # if policy is stable policyChanges = np.sum(newPolicy != policy) print('Policy for', policyChanges, 'states changed') if policyChanges == 0: policy = newPolicy break policy = newPolicy improvePolicy = False # start policy evaluation for i, j in states: newStateValue[i, j] = expectedReturn([i, j], policy[i, j], stateValue) if np.sum(np.abs(newStateValue - stateValue)) &lt; 1e-4: stateValue[:] = newStateValue improvePolicy = True continue stateValue[:] = newStateValueWe can see the logistic is the same as the pseudocode of the policy iteration algorithm. There is a core method in the code, that is, exceptedReturn() is used to calculate the reward of cars rental.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# @state: [# of cars in first location, # of cars in second location]# @action: positive if moving cars from first location to second location,# negative if moving cars from second location to first location# @stateValue: state value matrixdef expectedReturn(state, action, stateValue): # initailize total return returns = 0.0 # cost for moving cars returns -= MOVE_CAR_COST * abs(action) # go through all possible rental requests for rentalRequestFirstLoc in range(0, POISSON_UP_BOUND): for rentalRequestSecondLoc in range(0, POISSON_UP_BOUND): # moving cars numOfCarsFirstLoc = int(min(state[0] - action, MAX_CARS)) numOfCarsSecondLoc = int(min(state[1] + action, MAX_CARS)) # valid rental requests should be less than actual # of cars realRentalFirstLoc = min(numOfCarsFirstLoc, rentalRequestFirstLoc) realRentalSecondLoc = min(numOfCarsSecondLoc, rentalRequestSecondLoc) # get credits for renting reward = (realRentalFirstLoc + realRentalSecondLoc) * RENTAL_CREDIT numOfCarsFirstLoc -= realRentalFirstLoc numOfCarsSecondLoc -= realRentalSecondLoc # probability for current combination of rental requests prob = poisson(rentalRequestFirstLoc, RENTAL_REQUEST_FIRST_LOC) * \ poisson(rentalRequestSecondLoc, RENTAL_REQUEST_SECOND_LOC) # if set True, model is simplified such that the # of cars returned in daytime becomes constant # rather than a random value from poisson distribution, which will reduce calculation time # and leave the optimal policy/value state matrix almost the same constantReturnedCars = True if constantReturnedCars: # get returned cars, those cars can be used for renting tomorrow returnedCarsFirstLoc = RETURNS_FIRST_LOC returnedCarsSecondLoc = RETURNS_SECOND_LOC numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS) numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS) returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc]) else: numOfCarsFirstLoc_ = numOfCarsFirstLoc numOfCarsSecondLoc_ = numOfCarsSecondLoc prob_ = prob for returnedCarsFirstLoc in range(0, POISSON_UP_BOUND): for returnedCarsSecondLoc in range(0, POISSON_UP_BOUND): numOfCarsFirstLoc = numOfCarsFirstLoc_ numOfCarsSecondLoc = numOfCarsSecondLoc_ prob = prob_ numOfCarsFirstLoc = min(numOfCarsFirstLoc + returnedCarsFirstLoc, MAX_CARS) numOfCarsSecondLoc = min(numOfCarsSecondLoc + returnedCarsSecondLoc, MAX_CARS) prob = poisson(returnedCarsFirstLoc, RETURNS_FIRST_LOC) * \ poisson(returnedCarsSecondLoc, RETURNS_SECOND_LOC) * prob returns += prob * (reward + DISCOUNT * stateValue[numOfCarsFirstLoc, numOfCarsSecondLoc]) return returnsThe comments are very clear, and weâ€™re going to do a lot of this. Finally, let us print the result:123prettyPrint(policy, ['# of cars in first location', '# of cars in second location', '# of cars to move during night'])prettyPrint(stateValue, ['# of cars in first location', '# of cars in second location', 'expected returns'])plt.show()The results are as follows:12345678910Policy improvement 0Policy for 332 states changedPolicy improvement 1Policy for 286 states changedPolicy improvement 2Policy for 83 states changedPolicy improvement 3Policy for 19 states changedPolicy improvement 4Policy for 0 states changedOne drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to $v_{\pi}$ occurs only in the limit. In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This algorithm is called value iteration. It can be written as a particular simple backup operation that combines the policy improvement and truncated policy evaluation steps:$$\begin{align}v_{k+1} &amp;\doteq \max_{a} \mathbb{E}[R_{t+1} + \gamma v_{k}(S_{t+1}) \ | \ S_{t}=s, A_{t}=a] \\&amp;= \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma v_{k}(s^{\prime})],\end{align}$$for all $s \in \mathcal{S}$.Value iterationInitialize array $V$ arbitrarily (e.g. $V(s) = 0$ for all $s \in \mathcal{S^{+}}$)Repeatâ€‹ $\Delta \leftarrow 0$â€‹ For each $s \in \mathcal{S}$:â€‹ $v \leftarrow V(s)$â€‹ $V(s) \leftarrow \max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$â€‹ $\Delta \leftarrow \max(\Delta, |v - V(s)|)$until $\Delta &lt; \theta$ (a small positive number)Output a deterministic policy, $\pi \approx \pi_{\star}$, such thatâ€‹ $\pi(s) = \arg\max_{a}\sum_{s^{\prime}, r} p(s^{\prime}, r | s, a) [r + \gamma V(s^{\prime})]$Let us use the value iteration algorithm to solve a Gamblerâ€™s Problem. The problem defined as follows:A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he staked on the flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of \$100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer number of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gamblerâ€™s capital, $s \in \{1, 2, \cdots, 99\}$ and the actions are stakes, $a \in \{0, 1, \cdots, \min(s, 100-s)\}$. The reward is zero on all transitions excepted those on which the gambler reaches his goal, when it is +1. The state-value function then gives the probability of winning from each state. A policy is mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal. Let $p_h$ denote the probability of the coin coming up heads. If $p_h$ is known, then the entire problem is known and it can be solved, for instance, by value iteration.OK, now let us to solve this problem by use the value iteration algorithm.The first we defined some facts and some auxiliary data structure:1234567891011# goalGOAL = 100# all states, including state 0 and state 100states = np.arange(GOAL + 1)# probability of headheadProb = 0.4# optimal policypolicy = np.zeros(GOAL + 1)# state valuestateValue = np.zeros(GOAL + 1)stateValue[GOAL] = 1.0The step of value iteration:123456789101112131415# value iterationwhile True: delta = 0.0 for state in states[1:GOAL]: # get possilbe actions for current state actions = np.arange(min(state, GOAL - state) + 1) actionReturns = [] for action in actions: actionReturns.append(headProb * stateValue[state + action] + (1 - headProb) * stateValue[state - action]) newValue = np.max(actionReturns) delta += np.abs(stateValue[state] - newValue) # update state value stateValue[state] = newValue if delta &lt; 1e-9: breakCalculate the optimal policy:12345678# calculate the optimal policyfor state in states[1:GOAL]: actions = np.arange(min(state, GOAL - state) + 1) actionReturns = [] for action in actions: actionReturns.append(headProb * stateValue[state + action] + (1 - headProb) * stateValue[state - action]) # due to tie and precision, can't reproduce the optimal policy in book policy[state] = actions[argmax(actionReturns)]Print the results:123456789plt.figure(1)plt.xlabel('Capital')plt.ylabel('Value estimates')plt.plot(stateValue)plt.figure(2)plt.scatter(states, policy)plt.xlabel('Capital')plt.ylabel('Final policy (stake)')plt.show()The results are as follows:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The GridWorld problem]]></title>
      <url>%2F2017%2F05%2F29%2FThe-GridWorld-problem%2F</url>
      <content type="text"><![CDATA[A reinforcement learning task that satisfied the Markov property is called Markov decision process, or MDP. If the state and action spaces are finite, then it is called a finite Markov decision process (finite MDP).A particular finite MDP is defined by its state and action sets and by the one-step dynamics of the environment. Given any state and action s and a, the probability of each possible pair of next state and reward, sâ€™, r, is denoted$$p(s^{\prime}, r | s, a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime}, R_{t+1}=r \ | \ S_{t}=s, A_{t}=a \}$$Given that, one can compute anything else one might want to know about the environment, such as the excepted rewards of state-action pairs,$$r(s,a) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a] = \sum_{r \in \mathcal{R}}r\sum_{s^{\prime} \in \mathcal{S}}p(s^{\prime},r|s, a)$$the state-transition probabilities,$$p(s^{\prime}|s,a) \doteq \mathrm{Pr}\{S_{t+1}=s^{\prime} \ | \ S_{t}=s, A_{t}=a\} = \sum_{r \in \mathcal{R}} p(s^{\prime},r|s, a)$$and the excepted rewards for state-action-next-state triples,$$r(s, a, s^{\prime}) \doteq \mathbb{E}[R_{t+1} \ | \ S_{t}=s, A_{t}=a, S_{t+1}=s^{\prime}] = \frac{\sum_{r \in \mathcal{R}}rp(s^{\prime},r|s,a)}{p(s^{\prime}|s,a)}$$Almost all reinforcement learning algorithms involve estimating value functionsâ€“functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).Recall that a policy, $\pi$, is a mapping from a each state, $s \in \mathcal{S}$, and action, $a \in \mathcal{A}(s)$, to the probability $\pi(a|s)$ of taking action a when in state s. Informally, the value of a state s under a policy $\pi$, denoted $v_{\pi}(s)$, is the excepted return when starting in s and following $\pi$ thereafter. For MDPs, we can define $v_{\pi}(s)$ formally as$$v_{\pi}(s) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right]$$Note that the value of the terminal state, if any, is always zero. We call the function $v_{\pi}$ the state-value function for policy $\pi$.Similarly, we define the value of taking action a in state s under a policy $\pi$, denoted $q_{\pi}(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:$$q_{\pi}(s,a) \doteq \mathbb{E_{\pi}}[G_{t} \ | \ S_{t}=s, A_{t}=a] = \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s, A_{t}=a\right]$$We call $q_{\pi}$ the action-value function for policy $\pi$.A fundamental property of the value functions used in reinforcement learning and dynamic programming is that they satisfy particular recursive relationships. For any policy $\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:$$\begin{align}v_{\pi}(s) &amp;\doteq \mathbb{E_{\pi}[G_{t} \ | \ S_{t}=s]} \\&amp;= \mathbb{E_{\pi}}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} \ | \ S_{t}=s \right] \\&amp;= \mathbb{E_{\pi}}\left[R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} \ | \ S_{t}=s \right] \\&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}}\sum_{r}p(s^{\prime},r|s,a) \left[ r + \gamma \mathbb{E_{\pi}} \left[ \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+2} | S_{t+1}=s^{\prime} \right] \right] \\&amp;= \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},\end{align}$$Equation (11) is the Bellman equation for $v_{\pi}$.Figure 1 (left) shows a rectangular grid world representation of a simple finite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Action would take the agent off the grid leave its location unchanged, but also result in a reward of -1. Other actions result in a reward of 0, excepted those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to $\mathrm{A^{\prime}}$. From state B, all actions yield a reward +5 and take the agent to $\mathrm{B^{\prime}}$.Figure 1Suppose the agent selects all four actions with equal probability in all states. Figure 1 (right) shows the value function, $v_{\pi}$, for this policy, for the discounted reward case with $\gamma = 0.9$. This value function was computed by solving the system of linear equations (11).OK, now let us solve this problem. The first, we need to define the grid world by code.12345678WORLD_SIZE = 5A_POS = [0, 1]A_PRIME_POS = [4, 1]B_POS = [0, 3]B_PRIME_POS = [2, 3]discount = 0.9world = np.zeros((WORLD_SIZE, WORLD_SIZE))This world has 5 by 5 cells, and there are four special cells: A, Aâ€™, B, Bâ€™. Discount represents the $\gamma $ in equation (11). We know that the agent in the world selects all four actions with equal probability in all states (cells). So we have:12345678# left, up, right, downactions = ['L', 'U', 'R', 'D']actionProb = []for i in range(0, WORLD_SIZE): actionProb.append([]) for j in range(0, WORLD_SIZE): actionProb[i].append(dict(&#123;'L':0.25, 'U':0.25, 'R':0.25, 'D':0.25&#125;))The actionProb is a list that has five items. Each item represents a row in the grid and it also is a list that has five items that represents a column in corresponding row, that is, each item in a row represents a cell in the grid. In all cells (states), there are four direction could be selected with equal probability 0.25. Then, weâ€™ll define a undirected graph with weights. The node represented the cell in grid. If between two node has a edge then the agent could move between this two nodes (cells). The weight on the edges represents the reward do this move.12345678910111213141516171819202122232425262728293031323334353637383940414243444546nextState = []actionReward = []for i in range(0, WORLD_SIZE): nextState.append([]) actionReward.append([]) for j in range(0, WORLD_SIZE): next = dict() reward = dict() if i == 0: next['U'] = [i, j] reward['U'] = -1.0 else: next['U'] = [i - 1, j] reward['U'] = 0.0 if i == WORLD_SIZE - 1: next['D'] = [i, j] reward['D'] = -1.0 else: next['D'] = [i + 1, j] reward['D'] = 0.0 if j == 0: next['L'] = [i, j] reward['L'] = -1.0 else: next['L'] = [i, j - 1] reward['L'] = 0.0 if j == WORLD_SIZE - 1: next['R'] = [i, j] reward['R'] = -1.0 else: next['R'] = [i, j + 1] reward['R'] = 0.0 if [i, j] == A_POS: next['L'] = next['R'] = next['D'] = next['U'] = A_PRIME_POS reward['L'] = reward['R'] = reward['D'] = reward['U'] = 10.0 if [i, j] == B_POS: next['L'] = next['R'] = next['D'] = next['U'] = B_PRIME_POS reward['L'] = reward['R'] = reward['D'] = reward['U'] = 5.0 nextState[i].append(next) actionReward[i].append(reward)The nextState and actionReward are the same as actionProb that we explained earlier.Now, we could solve this problem by use the equation (11):$$\begin{align}v_{\pi}(s) &amp;\doteq \sum_{a} \pi(a|s) \sum_{s^{\prime}, r}p(s^{\prime},r|s,a) \left[ r + \gamma v_{\pi}(s^{\prime}) \right], \;\;\; \forall s \in \mathcal{S},\end{align}$$Let us jump into the implementation detail.1234567891011121314while True: # keep iteration until convergence newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE)) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): for action in actions: newPosition = nextState[i][j][action] # bellman equation newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]]) if np.sum(np.abs(world - newWorld)) &lt; 1e-4: print('Random Policy') print(newWorld) break world = newWorldThe core code is:1newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]])The += represents the first sum notation in the equation (11). If we ensure the current state (cell) and action will take in this world, then the next state and reward also will be ensured. So $\sum_{s^{\prime},r} p(s^{\prime}, r | s, a)$ is equal to 1.The result as follows:123456Random Policy[[ 3.30902999 8.78932551 4.42765281 5.3224012 1.49221235] [ 1.52162172 2.9923515 2.25017358 1.90760531 0.5474363 ] [ 0.05085614 0.73820423 0.67314689 0.35821982 -0.40310755] [-0.97355865 -0.43546179 -0.35484864 -0.58557148 -1.18304148] [-1.8576669 -1.34519762 -1.22923364 -1.42288454 -1.97514545]]We can see the value of all states is the same as the Figure 1.Solving a reinforcement learning task means, roughly, finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi^{\prime}$ if its excepted return is greater than or equal to that of $\pi^{\prime}$ for all states. In other words, $\pi \ge \pi^{\prime}$ if and only if $v_{\pi}(s) \ge v_{\pi^{\prime}}(s)$ for all $s \in \mathcal{S}$. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by $\pi_{\star}$. They share the same state-value function, called the optimal state-value function, denote $v_{\star}$, and defined as$$v_{\star}(s) \doteq \max_{\pi} v_{\pi}(s),$$for all $s \in \mathcal{S}$.Optimal policies also share the same optimal action-value function, denoted $q_{\star}$, and defined as$$q_{\star}(s, a) \doteq \max_{\pi} q_{\pi}(s, a)$$for all $s \in \mathcal{S}$ and $a \in \mathcal{A(s)}$. For the state-action pair (s, a), this function gives the excepted return for taking action a in state s and thereafter following an optimal policy. Thus, we can write $q_{\star}$ in terms of $v_{\star}$ as follows:$$q_{\star}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\star} \ | \ S_{t}=s, A_{t}=a]$$Suppose we solve the Bellman equation for $v_{\star}$ for the simple grid task introduced in earlier and shown again in Figure 2 (left). Recall that state A is followed by a reward of +10 and transition to state Aâ€™. while state B is followed by a reward of +5 and transition to state Bâ€™. Figure 2 (middle) shows the optimal value function, and Figure 2 (right) shows the corresponding optimal policies. Where there are multiple arrows in a cell, any of the corresponding actions are optimal.Figure 2Now, let us solve this problem:1234567891011121314151617world = np.zeros((WORLD_SIZE, WORLD_SIZE))while True: # keep iteration until convergence newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE)) for i in range(0, WORLD_SIZE): for j in range(0, WORLD_SIZE): values = [] for action in actions: newPosition = nextState[i][j][action] # value iteration values.append(actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]]) newWorld[i][j] = np.max(values) if np.sum(np.abs(world - newWorld)) &lt; 1e-4: print('Optimal Policy') print(newWorld) break world = newWorldWe can see the core code is as follows:1newWorld[i][j] = np.max(values)The only difference between this code and the earlier code is the prior only uses the maximum value and the latter uses the weighted average.The result is below:123456Optimal Policy[[ 21.97744338 24.41938153 21.97744338 19.41938153 17.47744338] [ 19.77969904 21.97744338 19.77969904 17.80172914 16.02153504] [ 17.80172914 19.77969904 17.80172914 16.02153504 14.41938153] [ 16.02153504 17.80172914 16.02153504 14.41938153 12.97744338] [ 14.41938153 16.02153504 14.41938153 12.97744338 11.67969904]]It is not doubt that the result is the same as the Figure 2 (middle).]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[k-Armed Bandit Problem]]></title>
      <url>%2F2017%2F05%2F27%2Fk-Armed-Bandit-Problem%2F</url>
      <content type="text"><![CDATA[Consider the following learning problem. You are faced repeatedly with a choice among $k$ different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.This is the original form of the k-armed bandit problem. Each of the k actions has an excepted or mean reward given that action is selected; let us call this value of that action. We denote the action selected on time step t as $A_{t}$, and the corresponding reward as $R_{t}$. The value then of an arbitrary action a, denoted $q_{\star}(a)$, is the excepted reward given that a is selected:$$q_{\star}(a) = \mathbb{E}[R_t|A_t=a]$$If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time t as $Q_{t}(a) \approx q_{\star}(a)$.We begin by looking more closely at some simple methods for estimating the values of actions and for using the estimates to make action selection decisions. Recall that the true value of an action is the mean reward when action is selected. One natural way to estimate this is by averaging the rewards actually received:$$Q_t(a) \doteq \frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}} = \frac{\sum_{i=1}^{t-1}R_i \cdot \mathbf{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbf{1}_{A_i=a}}$$where $\mathbf{1}_{\text{predicate}}$ denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as $Q_1(a) = 0$. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_{\star} (a)$. We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards.The simplest action selection rule is to select the action (or one of the actions) with highest estimated action value, that is, to select at step t one of the greedy actions, $A_t^\star$ for which $Q_t(A_t^\star) = \max_a Q_t(a)$. This greedy action selection method can be written as$$A_t \doteq argmax_a Q_t(a)$$Naturally, we could use the $\epsilon$-greedy method rather the greedy method. Weâ€™ll show their difference on the performance. Now, letâ€™s jump into the implementation details. In order to be able to see the results quickly, we set to k to be 10. The first, we generate 10 stationary probability distributions that weâ€™ll sample from to generate action values. The generate method is below:1data=np.random.randn(200,10) + np.random.randn(10)We first generate randomly 10 true excepted values by np.random.randn(10), then Iâ€™m going to change the variance to 1. So we get 10 stationary probability distributions. The visualizations are as follows:Weâ€™re going to compare how different $\epsilon$ values affect the end result.123456789101112131415161718192021def epsilonGreedy(nBandits, time): epsilons = [0, 0.1, 0.01] bandits = [] for epsInd, eps in enumerate(epsilons): bandits.append([Bandit(epsilon=eps, sampleAverages=True) for _ in range(0, nBandits)]) bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits) global figureIndex plt.figure(figureIndex) figureIndex += 1 for eps, counts in zip(epsilons, bestActionCounts): plt.plot(counts, label='epsilon = '+str(eps)) plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.figure(figureIndex) figureIndex += 1 for eps, rewards in zip(epsilons, averageRewards): plt.plot(rewards, label='epsilon = '+str(eps)) plt.xlabel('Steps') plt.ylabel('average reward') plt.legend()Before we go into the details, we introduce the Bandit object first.123456789101112class Bandit: # @kArm: # of arms # @epsilon: probability for exploration in epsilon-greedy algorithm # @initial: initial estimation for each action # @stepSize: constant step size for updating estimations # @sampleAverages: if True, use sample averages to update estimations instead of constant step size # @UCB: if not None, use UCB algorithm to select action # @gradient: if True, use gradient based bandit algorithm # @gradientBaseline: if True, use average reward as baseline for gradient based bandit algorithm def __init__(self, kArm=10, epsilon=0., initial=0., stepSize=0.1, sampleAverages=False, UCBParam=None, gradient=False, gradientBaseline=False, trueReward=0.): def getAction(self): def takeAction(self, action):For now we just introduce sample-average method, so skip other methods parameters. Let us see the initialization method.12345678910111213141516171819202122232425262728293031def __init__(self, kArm=10, epsilon=0., initial=0., stepSize=0.1, sampleAverages=False, UCBParam=None, gradient=False, gradientBaseline=False, trueReward=0.): self.k = kArm self.stepSize = stepSize self.sampleAverages = sampleAverages self.indices = np.arange(self.k) self.time = 0 self.UCBParam = UCBParam self.gradient = gradient self.gradientBaseline = gradientBaseline self.averageReward = 0 self.trueReward = trueReward # real reward for each action self.qTrue = [] # estimation for each action self.qEst = np.zeros(self.k) # # of chosen times for each action self.actionCount = [] self.epsilon = epsilon # initialize real rewards with N(0,1) distribution and estimations with desired initial value for i in range(0, self.k): self.qTrue.append(np.random.randn() + trueReward) self.qEst[i] = initial self.actionCount.append(0) self.bestAction = np.argmax(self.qTrue)There are some important attributes. time is a number that represents the time steps now. actionCount is the times that correspond actions have been taken prior to current time steps. qTrue is a list. And each item is the true excepted value corresponding to each action. qEst is the estimate value of each action. Itâ€™s initialized to zero. epsilon is the $\epsilon$. Next, in the for loop, we initialize real rewards with N(0, 1) distribution and estimations with desired initial value. At last, the bestAction store the current best action will be take.The next method tell us how to get the next action should be take:1234567891011121314151617def getAction(self): # explore if self.epsilon &gt; 0: if np.random.binomial(1, self.epsilon) == 1: np.random.shuffle(self.indices) return self.indices[0] # exploit if self.UCBParam is not None: UCBEst = self.qEst + \ self.UCBParam * np.sqrt(np.log(self.time + 1) / (np.asarray(self.actionCount) + 1)) return np.argmax(UCBEst) if self.gradient: expEst = np.exp(self.qEst) self.actionProb = expEst / np.sum(expEst) return np.random.choice(self.indices, p=self.actionProb) return np.argmax(self.qEst)We can skip the second and the third if statements (weâ€™ll introduce this two methods later). If we use greedy method, we just return the action that has highest value. Otherwise, weâ€™re choosing randomly at $\epsilon$ probability.12345678910111213141516171819202122def takeAction(self, action): # generate the reward under N(real reward, 1) reward = np.random.randn() + self.qTrue[action] self.time += 1 self.averageReward = (self.time - 1.0) / self.time * self.averageReward + reward / self.time self.actionCount[action] += 1 if self.sampleAverages: # update estimation using sample averages self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action]) elif self.gradient: oneHot = np.zeros(self.k) oneHot[action] = 1 if self.gradientBaseline: baseline = self.averageReward else: baseline = 0 self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb) else: # update estimation with constant step size self.qEst[action] += self.stepSize * (reward - self.qEst[action]) return rewardSimilarly, we just skip other if statements and focus on this row:1self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action])This formula seems different with the earlier one. The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. We now turn to the question of how these averages can be computed in a computationally efficient manner, in particular, with constant memory and per-time-step computation.To simplify notation we concentrate on a single action. Let $R_i$ now denote the reward received after ith selection of this action, and let $Q_n$ denote the estimate of its action value after it has been select $n-1$ times, which we can now write simply as$$Q_n \doteq \frac{R_1 + R_2 + \cdots + R_{n-1}}{n-1}$$The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, in this case the memory and computational requirements would grow over time as more rewards are seen.It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given $Q_n$ and the nth reward, $R_n$, the new average of all n rewards can be computed by$$\begin{align}Q_{n+1} &amp;\doteq \frac{1}{n}\sum_{i=1}^{n} R_i \\&amp;= \frac{1}{n}(R_n + \sum_{i=1}^{n-1} R_i) \\&amp;= \frac{1}{n}(R_n + (n-1) \frac{1}{n-1}\sum_{i=1}^{n-1} R_i) \\&amp;= \frac{1}{n}(R_n + (n-1) Q_n) \\&amp;= \frac{1}{n}(R_n + n Q_n - Q_n) \\&amp;= Q_n + \frac{1}{n}[R_n - Q_n]\end{align}$$So this is why the code is look like this:1self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action])Back to epsilonGreedy() method:123456789101112131415161718192021def epsilonGreedy(nBandits, time): epsilons = [0, 0.1, 0.01] bandits = [] for epsInd, eps in enumerate(epsilons): bandits.append([Bandit(epsilon=eps, sampleAverages=True) for _ in range(0, nBandits)]) bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits) global figureIndex plt.figure(figureIndex) figureIndex += 1 for eps, counts in zip(epsilons, bestActionCounts): plt.plot(counts, label='epsilon = '+str(eps)) plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.figure(figureIndex) figureIndex += 1 for eps, rewards in zip(epsilons, averageRewards): plt.plot(rewards, label='epsilon = '+str(eps)) plt.xlabel('Steps') plt.ylabel('average reward') plt.legend()Now, we get nBandits bandits and each bandit has 10 arm. Then we use a bandit simulation to simulate this process.1234567891011121314def banditSimulation(nBandits, time, bandits): bestActionCounts = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))] averageRewards = [np.zeros(time, dtype='float') for _ in range(0, len(bandits))] for banditInd, bandit in enumerate(bandits): for i in range(0, nBandits): for t in range(0, time): action = bandit[i].getAction() reward = bandit[i].takeAction(action) averageRewards[banditInd][t] += reward if action == bandit[i].bestAction: bestActionCounts[banditInd][t] += 1 bestActionCounts[banditInd] /= nBandits averageRewards[banditInd] /= nBandits return bestActionCounts, averageRewardsThe bandits is a list that has three item. Each item is a list that contains nBandits bandits that has a corresponding epsilon value. We calculate the average total reward of 2000 bandits is to eliminate the effect of noise. Ok, let us see the final result.We can see the algorithm reaches the best performance when epsilon is set to 0.1.The averaging methods discussed so far are appropriate in a stationary environment, but not if the bandit is changing over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to weight recent rewards more heavily than long-past ones. One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule for updating an average $Q_n$ of the $n-1$ past rewards is modified to be$$Q_{n+1} \doteq Q_n + \alpha [R_n - Q_n]$$where the step-size parameter $\alpha \in (0, 1]$ is constant. This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_1$:$$\begin{align}Q_{n+1} &amp;\doteq Q_n + \alpha [R_n - Q_n] \\&amp;= \alpha R_n + (1 - \alpha) Q_n \\&amp;= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] \\&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 Q_{n-1} \\&amp;= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha) ^2 R_{n-1} + \cdots + (1 - \alpha) ^ {n-1} \alpha R_1 + (1 - \alpha) ^ n Q_1 \\&amp;= (1 - \alpha) ^ n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha) ^ {n-i} R_i\end{align}$$We call this a weighted average because the sum of the weights is 1. In fact, the weight decays exponentially according to the exponent on $1-\alpha$. According, this is sometimes called an exponential, recency-weighted average.Sometimes it is convenient to vary the step-size parameter from step to step. Let $\alpha_n(a)$ denote the step-size parameter used to process the reward received after nth selection of action a. As we noted, the choice $\alpha_n(a)=\frac{1}{n}$ results in the sample-average method, which is guaranteed to converge to the true action values by the law of the large numbers. A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:$$\sum_{n=1}^{\infty}\alpha_{n}(a) = \infty \; \text{and} \; \sum_{n=1}^{\infty}\alpha_{n}^{2}(a) &lt; \infty$$All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, $Q_1(a)$. In the language of statistics, these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant $\alpha$ (weighted avetrage), the bias is permanent, though decreasing over time. In practice, this kind of bias is usually not a problem and can sometimes be very helpful. The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be excepted. So, let us do a experiment. The first we set them all to zero and the we set them all to a constant, 5.1234567891011121314151617181920def optimisticInitialValues(nBandits, time): bandits = [[], []] bandits[0] = [Bandit(epsilon=0, initial=5, stepSize=0.1) for _ in range(0, nBandits)] bandits[1] = [Bandit(epsilon=0.1, initial=0, stepSize=0.1) for _ in range(0, nBandits)] bestActionCounts, averageRewards = banditSimulation(nBandits, time, bandits) global figureIndex plt.figure(figureIndex) figureIndex += 1 plt.plot(bestActionCounts[0], label='epsilon = 0, q = 5') plt.plot(bestActionCounts[1], label='epsilon = 0.1, q = 0') plt.xlabel('Steps') plt.ylabel('% optimal action') plt.legend() plt.figure(figureIndex) figureIndex += 1 plt.plot(averageRewards[0], label='epsilon=0, initial=5, stepSize=0.1') plt.plot(averageRewards[1], label='epsilon=0.1, initial=0, stepSize=0.1') plt.xlabel('Steps') plt.ylabel('average reward') plt.legend()The Bandit objectâ€™s takeAction() has a little difference:1self.qEst[action] += self.stepSize * (reward - self.qEst[action])The result is as follows:We can see it reaches the better performance than the $\epsilon$-greedy method, when they are all used the weighted average method. Notice that the $\epsilon$-greedy with weighted-average method is worsen than the $\epsilon$-greedy with sample-average method.We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial state in any special way is unlikely to help with the general nonstationary case.Exploration is needed because the estimates of the action values are uncertain. The greedy actions are those that look best at present, but some of the other actions may actually be better. $\epsilon$-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions as$$A_t \doteq argmax_a \left [Q_t(a) + c\sqrt{\frac{\ln{t}}{N_t(a)}}\ \right]$$where $N_t(a)$ denotes the number of times that action a has been selected prior to time t, and the number $c&gt;0$ controls the degree of exploration. If $N_t(a)=0$, then a is considered to be a maximizing action. The idea of this is called upper confidence bound (UCB). Let us implement it.12345678910111213def ucb(nBandits, time): bandits = [[], []] bandits[0] = [Bandit(epsilon=0, stepSize=0.1, UCBParam=2) for _ in range(0, nBandits)] bandits[1] = [Bandit(epsilon=0.1, stepSize=0.1) for _ in range(0, nBandits)] _, averageRewards = banditSimulation(nBandits, time, bandits) global figureIndex plt.figure(figureIndex) figureIndex += 1 plt.plot(averageRewards[0], label='UCB c = 2') plt.plot(averageRewards[1], label='epsilon greedy epsilon = 0.1') plt.xlabel('Steps') plt.ylabel('Average reward') plt.legend()We note that the UCBParam=2. The Bandit object explains this. The getAction() method and takeAction() method are as follows:1234567891011121314151617181920212223242526272829303132333435363738394041def getAction(self): # explore if self.epsilon &gt; 0: if np.random.binomial(1, self.epsilon) == 1: np.random.shuffle(self.indices) return self.indices[0] # exploit if self.UCBParam is not None: UCBEst = self.qEst + \ self.UCBParam * np.sqrt(np.log(self.time + 1) / (np.asarray(self.actionCount) + 1)) return np.argmax(UCBEst) if self.gradient: expEst = np.exp(self.qEst) self.actionProb = expEst / np.sum(expEst) return np.random.choice(self.indices, p=self.actionProb) return np.argmax(self.qEst)# take an action, update estimation for this actiondef takeAction(self, action): # generate the reward under N(real reward, 1) reward = np.random.randn() + self.qTrue[action] self.time += 1 self.averageReward = (self.time - 1.0) / self.time * self.averageReward + reward / self.time self.actionCount[action] += 1 if self.sampleAverages: # update estimation using sample averages self.qEst[action] += 1.0 / self.actionCount[action] * (reward - self.qEst[action]) elif self.gradient: oneHot = np.zeros(self.k) oneHot[action] = 1 if self.gradientBaseline: baseline = self.averageReward else: baseline = 0 self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot - self.actionProb) else: # update estimation with constant step size self.qEst[action] += self.stepSize * (reward - self.qEst[action]) return rewardWe can see the policy get next action has changed but the update policy has not changed. The result is here:We omit the figure about the optimal action because the trend of this two figures are the same. UCB will often perform well, but is more difficult than $\epsilon$-greedy to extend beyond bandits to the more general reinforcement learning setting considered in the more advanced problems. In these more advanced settings there is currently no known practical way of utilizing the idea of UCB action selection.So far we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. Now, we consider learning a numerical preference $H_t(a)$ for each action a. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the preferences there is no effect on the action probabilities, which are determined according to a softmax distribution as follows:$$Pr\{A_t=a\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} \doteq \pi_t(a)$$where here we have also introduced a useful new notation $\pi_t(a)$ for the probability of taking action a at time t. Initially all preferences are the same (e.g., $H_1(a)=0, \forall a$) so that all actions have an equal probability of being selected.There is a natural learning algorithm for this setting based on the idea of stochastic gradient ascent. On each step, after selection the action $A_t$ and receiving the reward $R_t$, the preferences are updated by:$$\begin{align}H_{t+1}(A_t) \doteq H_t(A_t) + \alpha (R_t - \overline{R_t}) (1 - \pi_t(A_t)), \; \text{and} \\H_{t+1}(a) \doteq H_t(a) - \alpha (R_t - \overline{R_t}) \pi_t(a), \;\;\;\;\;\; \forall{a \neq A_t}\end{align}$$where $\alpha&gt;0$ is a ste-size parameter, and $\overline{R_t} \in \mathbb{R}$ is the average of all the rewards up through and including time t, which can be computed incrementally. The $\overline{R_t}$ term serves as a baseline with which the reward is compared. Let us see the implement details (takeAction() method and getAction() method).1234if self.gradient: expEst = np.exp(self.qEst) self.actionProb = expEst / np.sum(expEst) return np.random.choice(self.indices, p=self.actionProb)12345678elif self.gradient: oneHot = np.zeros(self.k) oneHot[action] = 1 if self.gradientBaseline: baseline = self.averageReward else: baseline = 0 self.qEst = self.qEst + self.stepSize * (reward - baseline) * (oneHot self.actionProb)The below figure shows results with the gradient bandit algorithm on a variant of the 10-armed testbed in which the true excepted rewards were selected according to a normal distribution with a mean of +4 instead of zero (and with unit variance as before).Despite their simplicity, in our opinion the methods presented in here can fairly be considered the state of the art. Finally, we do a parameter study of the various bandit algorithms presented in here.123456789101112131415161718192021222324252627def figure2_6(nBandits, time): labels = ['epsilon-greedy', 'gradient bandit', 'UCB', 'optimistic initialization'] generators = [lambda epsilon: Bandit(epsilon=epsilon, sampleAverages=True), lambda alpha: Bandit(gradient=True, stepSize=alpha, gradientBaseline=True), lambda coef: Bandit(epsilon=0, stepSize=0.1, UCBParam=coef), lambda initial: Bandit(epsilon=0, initial=initial, stepSize=0.1)] parameters = [np.arange(-7, -1), np.arange(-5, 2), np.arange(-4, 3), np.arange(-2, 3)] bandits = [[generator(math.pow(2, param)) for _ in range(0, nBandits)] for generator, parameter in zip(generators, parameters) for param in parameter] _, averageRewards = banditSimulation(nBandits, time, bandits) rewards = np.sum(averageRewards, axis=1)/time global figureIndex plt.figure(figureIndex) figureIndex += 1 i = 0 for label, parameter in zip(labels, parameters): l = len(parameter) plt.plot(parameter, rewards[i:i+l], label=label) i += l plt.xlabel('Parameter(2^x)') plt.ylabel('Average reward') plt.legend()The results as follows:]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Tic-Tac-Toe Game]]></title>
      <url>%2F2017%2F05%2F26%2FTic-tac-toe%2F</url>
      <content type="text"><![CDATA[What is the Tic-Tac-Toe game? Two players take turns playing on a three-by-three board. One player plays Xs and the other Os until one player wins by placing three marks in a row, horizontally, vertically, or diagonally, as the X player has in the game shown to the blew. If the board fills up with neither player getting three in a row, the game is a draw.There have three steps. Train, compete and play.The first, letâ€™s to see the train period. Follow is the train() method.123456789101112131415161718def train(epochs=20000): player1 = Player() player2 = Player() judger = Judger(player1, player2) player1Win = 0.0 player2Win = 0.0 for i in range(0, epochs): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / epochs) print(player2Win / epochs) player1.savePolicy() player2.savePolicy()Train() method create two Player objects first, and then let them to play the tic-tac-toe through a Judger object. Itâ€™s a very simple process.Next. let to get into the Player object.Follow is the code of the Player object. For understand easily, I omitted the implementation details of each method.1234567891011121314class Player: # @stepSize: step size to update estimations # @exploreRate: possibility to explore def __init__(self, stepSize=0.1, exploreRate=0.1): def reset(self): def setSymbol(self, symbol): # accept a state def feedState(self, state): # update estimation according to reward def feedReward(self, reward): # determine next action def takeAction(self): def savePolicy(self): def loadPolicy(self):As a Player, the important thing during the train process is to learn a policy. The policy is a selection when the player faces a state. So there are two method savePolicy() and loadPolicy(). When the train process end, the player save its learned policy and load the same policy when the player compete with someone else later.Follow is the implementation details:123456789def savePolicy(self): fw = open('optimal_policy_' + str(self.symbol), 'wb') pickle.dump(self.estimations, fw) fw.close()def loadPolicy(self): fr = open('optimal_policy_' + str(self.symbol), 'rb') self.estimations = pickle.load(fr) fr.close()And, letâ€™s to jump into the initialization method, the below is its source code:123456def __init__(self, stepSize=0.1, exploreRate=0.1): self.allStates = allStates self.estimations = dict() self.stepSize = stepSize self.exploreRate = exploreRate self.states = []Every player hold a dictionary. For each item in the dictionary, the key is the state, and the value is the estimation of the probability to win from this state. We use the TD(0) method to solve the problem. That is, we need to update .the state-value function step by step. The update rule is below:$$ V(s) = V(s) + \alpha [V(sâ€™) - V(s)] â€‹$$The $\alpha$ is the step size, and the $sâ€™$ is the next state, $s$ is the current state. $V(\star)$ is the estimation of the probability to win from $*$ state.So, what is the explore rate. We need to know how to choose the next action at current state if we want to understand the explore rate. For every state, first we find the every state it can transfer to. Then we look up the dictionary to find the state that has the highest estimation value. This state is our action will take. The method called greedy policy. But, the value of each state is our estimation, so we canâ€™t say itâ€™s the true probability. So the greedy policy has some error. There is a method to solve this problem. At every state, we not only select the next state that has the highest probability but also choose the next state randomly by explore rate probability. Formerly, if we use the symbol $\epsilon$ represents the explore rate, then the method is called $\epsilon$-greedy method.Next, letâ€™s see what is the allStates variable.1self.allStates = allStateswe can see12# all possible board configurationsallStates = getAllStates()So what is the getAllStates() look like?1234567def getAllStates(): currentSymbol = 1 currentState = State() allStates = dict() allStates[currentState.getHash()] = (currentState, currentState.isEnd()) getAllStatesImpl(currentState, currentSymbol, allStates) return allStatesUntil now you may ask what is the STATE? Below is the definition:123456class State: def __init__(self): def getHash(self): def isEnd(self): def nextState(self, i, j, symbol): def show(self):One state is one arrangement of pieces on the board. So one state has some extra attributions. Such as who is the winner at current state, if the state is the terminal state or not and so on. Specially, each state has a hash value for representation convenient. The board is represented by a n by n array, that is, one state is a n by n array.123456789def __init__(self): # the board is represented by a n * n array, # 1 represents chessman of the player who moves first, # -1 represents chessman of another player # 0 represents empty position self.data = np.zeros((BOARD_ROWS, BOARD_COLS)) self.winner = None self.hashVal = None self.end = NoneBelow is how to calculate the hash value of a state:123456789# calculate the hash value for one state, it's uniquedef getHash(self): if self.hashVal is None: self.hashVal = 0 for i in self.data.reshape(BOARD_ROWS * BOARD_COLS): if i == -1: i = 2 self.hashVal = self.hashVal * 3 + i return int(self.hashVal)Below is how to judge if a state is end or not:12345678910111213141516171819202122232425262728293031323334353637383940# determine whether a player has won the game, or it's a tiedef isEnd(self): if self.end is not None: return self.end results = [] # check row for i in range(0, BOARD_ROWS): results.append(np.sum(self.data[i, :])) # check columns for i in range(0, BOARD_COLS): results.append(np.sum(self.data[:, i])) # check diagonals results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, i] results.append(0) for i in range(0, BOARD_ROWS): results[-1] += self.data[i, BOARD_ROWS - 1 - i] for result in results: if result == 3: self.winner = 1 self.end = True return self.end if result == -3: self.winner = -1 self.end = True return self.end # whether it's a tie sum = np.sum(np.abs(self.data)) if sum == BOARD_ROWS * BOARD_COLS: self.winner = 0 self.end = True return self.end # game is still going on self.end = False return self.endThere are two scenarios for the end of the game: Someone wins or ties. Because player Aâ€™s chessman is represents by 1 and play B is -1. So if A wins, then one row â€˜s sum is n or one columnâ€™s sum is n or one diagnoseâ€™s sum is n. Otherwise is -n. And the stateâ€™s winner attribute is 1 or -1, that is, player A or player B. if the sum of the absolute value of the all chessman in the board is n by n, then the game is tie, winner is 0 (that is no one wins).When someone put a chessman in the board, then the state is change and transfer to another state. How to get the state?12345def nextState(self, i, j, symbol): newState = State() newState.data = np.copy(self.data) newState.data[i, j] = symbol return newStateAnd the last, we are play a game so we need a GUI. This is what the show() to do.123456789101112131415# print the boarddef show(self): for i in range(0, BOARD_ROWS): print('-------------') out = '| ' for j in range(0, BOARD_COLS): if self.data[i, j] == 1: token = '*' if self.data[i, j] == 0: token = '0' if self.data[i, j] == -1: token = 'x' out += token + ' | ' print(out) print('-------------')Letâ€™s come back to the getAllStates() method.1234567def getAllStates(): currentSymbol = 1 currentState = State() allStates = dict() allStates[currentState.getHash()] = (currentState, currentState.isEnd()) getAllStatesImpl(currentState, currentSymbol, allStates) return allStatesNow we know what is a state and the next we need to generate all possible state. The first, we need build a data structure to store the all states. So we define a dictionary allStates. Itâ€™s key is the hash value of the state, and itâ€™s value is a tuple. The first item of the tuple is the state (a n by n array) and the second item is a flag that represent the state whether is a terminal state or not. For generate the all states, we jump into the getAllStatesImpl() method.1234567891011def getAllStatesImpl(currentState, currentSymbol, allStates): for i in range(0, BOARD_ROWS): for j in range(0, BOARD_COLS): if currentState.data[i][j] == 0: newState = currentState.nextState(i, j, currentSymbol) newHash = newState.getHash() if newHash not in allStates.keys(): isEnd = newState.isEnd() allStates[newHash] = (newState, isEnd) if not isEnd: getAllStatesImpl(newState, -currentSymbol, allStates)The getAllStatesImpl() method start with a empty board (currentState), and generate the states step by step (because it recursive calls itself). Because the game is very simple, so we could generate all possible states. But for the larger game, this is impossible.Tada~Letâ€™s come back to the Player object. We put the code here again for convenience.1234567891011121314class Player: # @stepSize: step size to update estimations # @exploreRate: possibility to explore def __init__(self, stepSize=0.1, exploreRate=0.1): def reset(self): def setSymbol(self, symbol): # accept a state def feedState(self, state): # update estimation according to reward def feedReward(self, reward): # determine next action def takeAction(self): def savePolicy(self): def loadPolicy(self):We has explained the initialization method. Itâ€™s worth to notice that the Player object has a attribute states. Weâ€™ll explain it later.Below is the reset() method:12def reset(self): self.states = []and below is the setSymbol() method:1234567891011def setSymbol(self, symbol): self.symbol = symbol for hash in self.allStates.keys(): (state, isEnd) = self.allStates[hash] if isEnd: if state.winner == self.symbol: self.estimations[hash] = 1.0 else: self.estimations[hash] = 0 else: self.estimations[hash] = 0.5We know that every playerâ€™s chessman in the board has a symbol (1 or -1). This method is set a symbol to the player. Furthermore, this method initialize the estimate state-value dictionary (we mentioned it earlier).And the feedState() method:12def feedState(self, state): self.states.append(state)The same as the states variable, weâ€™ll explain it later.Go on, below is the feedForward() method.This method not only the core of the Player object, but also itâ€™s the core of the method that solve this game. That is, itâ€™s the core of the TD(0) method.1234567891011def feedReward(self, reward): if len(self.states) == 0: return self.states = [state.getHash() for state in self.states] target = reward for latestState in reversed(self.states): value = self.estimations[ latestState] + self.stepSize * (target - self.estimations[latestState]) self.estimations[latestState] = value target = value self.states = []We mentioned the update rule earlier. Below is itâ€™s implementation:12value = self.estimations[ latestState] + self.stepSize * (target - self.estimations[latestState])Notice that we can see there are two row in the code:12self.estimations[latestState] = valuetarget = valueSo the update rule is a chain-like update rule. Specially, the states variable is set to empty (In the same way, weâ€™ll explain it later).The next method (implement the $\epsilon$-greedy policy) also is very important, because it tells the player how to take the next action:12345678910111213141516171819202122232425262728def takeAction(self): state = self.states[-1] nextStates = [] nextPositions = [] for i in range(BOARD_ROWS): for j in range(BOARD_COLS): if state.data[i, j] == 0: nextPositions.append([i, j]) nextStates.append(state.nextState( i, j, self.symbol).getHash()) if np.random.binomial(1, self.exploreRate): np.random.shuffle(nextPositions) # Not sure if truncating is the best way to deal with exploratory step # Maybe it's better to only skip this step rather than forget all # the history self.states = [] action = nextPositions[0] action.append(self.symbol) return action values = [] for hash, pos in zip(nextStates, nextPositions): values.append((self.estimations[hash], pos)) np.random.shuffle(values) values.sort(key=lambda x: x[0], reverse=True) action = values[0][1] action.append(self.symbol) return actionWeâ€™ll see that the return action is a list that the first item is a list contains the next position and the second item is the symbol that represents the player.Ok, the travel about the Player object is over. Then, weâ€™ll look into the Judger object. Before that, letâ€™s recall the train() process.123456789101112131415161718def train(epochs=20000): player1 = Player() player2 = Player() judger = Judger(player1, player2) player1Win = 0.0 player2Win = 0.0 for i in range(0, epochs): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / epochs) print(player2Win / epochs) player1.savePolicy() player2.savePolicy()We can see that the Judger object accept two parameters, that is, two player object. The definition of Judger is below:123456789101112class Judger: # @player1: player who will move first, its chessman will be 1 # @player2: another player with chessman -1 # @feedback: if True, both players will receive rewards when game is end def __init__(self, player1, player2, feedback=True): # give reward to two players def giveReward(self): def feedCurrentState(self): def reset(self): # @show: if True, print each board during the game def play(self, show=False):Notice that the rewards only receive at the end of the game. The first, letâ€™s see the initialization method.1234567891011def __init__(self, player1, player2, feedback=True): self.p1 = player1 self.p2 = player2 self.feedback = feedback self.currentPlayer = None self.p1Symbol = 1 self.p2Symbol = -1 self.p1.setSymbol(self.p1Symbol) self.p2.setSymbol(self.p2Symbol) self.currentState = State() self.allStates = allStatesp1 and p2 is the two player that play the game. The feedback represents if the reward propagation back or not. On the train process the feedback is true and on the compete process and play process the feedback is false. currentPlayer represents who should move next. and next the judger set symbol for each player. The currentState is the start state (the board is empty).Go on. Below is the giveReward() method:12345678910def giveReward(self): if self.currentState.winner == self.p1Symbol: self.p1.feedReward(1) self.p2.feedReward(0) elif self.currentState.winner == self.p2Symbol: self.p1.feedReward(0) self.p2.feedReward(1) else: self.p1.feedReward(0) self.p2.feedReward(0)Just like we say earlier, the rewards only receive at the end of the game. So if player A wins, then we give him a reward 1 and otherwise we give him a reward 0. If ties, then all reward is 0. We explain the feedCurrentState() later. Now we explain reset() method first.12345def reset(self): self.p1.reset() self.p2.reset() self.currentState = State() self.currentPlayer = NoneItâ€™s simple right? Letâ€™s skip it and go to the core method:1234567891011121314151617181920def play(self, show=False): self.reset() self.feedCurrentState() while True: # set current player if self.currentPlayer == self.p1: self.currentPlayer = self.p2 else: self.currentPlayer = self.p1 if show: self.currentState.show() [i, j, symbol] = self.currentPlayer.takeAction() self.currentState = self.currentState.nextState(i, j, symbol) hashValue = self.currentState.getHash() self.currentState, isEnd = self.allStates[hashValue] self.feedCurrentState() if isEnd: if self.feedback: self.giveReward() return self.currentState.winnerWe can see the two player alternate to play chess. Each reached state on the game will feed to the playersâ€™ states attribute.1self.feedCurrentState()So below is the method like:123def feedCurrentState(self): self.p1.feedState(self.currentState) self.p2.feedState(self.currentState)12def feedState(self, state): self.states.append(state)Letâ€™s explain the states now. Each player only update the states that the game reached in one game. Each reached state on the game will feed to the playersâ€™ states attribute. Note that, the player just update part of the states of the all states. Only after a lot of games, the all states could be updated. So all TD methods need a lot of epochs.Ouch! Finally three core objects are explained. Now weâ€™ll clear about the three process: train, compete and play.123456789101112131415161718def train(epochs=20000): player1 = Player() player2 = Player() judger = Judger(player1, player2) player1Win = 0.0 player2Win = 0.0 for i in range(0, epochs): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / epochs) print(player2Win / epochs) player1.savePolicy() player2.savePolicy()123456789101112131415161718def compete(turns=500): player1 = Player(exploreRate=0) player2 = Player(exploreRate=0) judger = Judger(player1, player2, False) player1.loadPolicy() player2.loadPolicy() player1Win = 0.0 player2Win = 0.0 for i in range(0, turns): print("Epoch", i) winner = judger.play() if winner == 1: player1Win += 1 if winner == -1: player2Win += 1 judger.reset() print(player1Win / turns) print(player2Win / turns)12345678910111213def play(): while True: player1 = Player(exploreRate=0) player2 = HumanPlayer() judger = Judger(player1, player2, False) player1.loadPolicy() winner = judger.play(True) if winner == player2.symbol: print("Win!") elif winner == player1.symbol: print("Lose!") else: print("Tie!")Itâ€™s worth noting that there is a HumanPlayer object.1234567891011121314151617181920212223242526272829class HumanPlayer: def __init__(self, stepSize=0.1, exploreRate=0.1): self.symbol = None self.currentState = None return def reset(self): return def setSymbol(self, symbol): self.symbol = symbol return def feedState(self, state): self.currentState = state return def feedReward(self, reward): return def takeAction(self): data = int(input("Input your position:")) data -= 1 i = data // int(BOARD_COLS) j = data % BOARD_COLS if self.currentState.data[i, j] != 0: return self.takeAction() return (i, j, self.symbol)Weâ€™ll see that this object do nothing. It just put a chess to on the board.OK, youâ€™re done! Finally, we put the complete code here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[How to generate a unique ID in a distribute system]]></title>
      <url>%2F2017%2F05%2F25%2FHow-to-generate-a-unique-ID-in-a-distribute-system%2F</url>
      <content type="text"><![CDATA[çŽ°ä»Šæ‰€æœ‰çš„ä¼ä¸šçº§åº”ç”¨éƒ½éœ€è¦å¤„ç†æµ·é‡çš„æ•°æ®å¯¹è±¡ï¼Œè¿™äº›å¯¹è±¡éƒ½éœ€è¦ä¸€ä¸ªå”¯ä¸€çš„IDä¸Žå…¶ä»–çš„å¯¹è±¡åŒºåˆ†å¼€æ¥ã€‚åœ¨å…³ç³»åž‹æ•°æ®åº“ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬æ˜¯åˆ›å»ºä¸»é”®æ¥è¾¾åˆ°è¿™ä¸ªç›®çš„ã€‚ä¸€äº›æ•°æ®åº“æ”¯æŒå†…å»ºçš„åˆ—ç±»åž‹ï¼ˆAUTO_INCREMENT/AUTO_NUMBERï¼‰æ¥äº§ç”Ÿä¸€ä¸ªå•è°ƒé€’å¢žçš„64ä½é•¿çš„æ•°ã€‚æœ‰äº›äººå–œæ¬¢åœ¨ä»–ä»¬çš„åº”ç”¨å±‚ä¸­ç”Ÿæˆidï¼Œä»¥ä¾¿èŽ·å¾—å¯¹è¿™ä»£äººçš„æ›´å¤šæŽ§åˆ¶ï¼Œç„¶åŽä½¿ç”¨æ•°æ®å±‚ä¿å­˜è®°å½•ã€‚ä½†æ˜¯ï¼Œç¬¬äºŒç§æ–¹æ³•é€šè¿‡ç¼“å­˜æœ€æ–°ç”Ÿæˆçš„æ•°å­—ï¼Œå¹¶ä¸”é€šè¿‡æŸç§æŒä¹…æ€§æŠ€æœ¯ä¿å­˜å·²ç»ç”Ÿæˆçš„idçš„è½¨è¿¹æ¥é¿å…ä¸»é”®å†²çªã€‚ä¸Šè¿°ä¸¤ç§æ–¹æ³•æœ¬èº«éƒ½æœ‰å„è‡ªçš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œä½†å®ƒä»¬éƒ½æœ‰ä¸€ä¸ªå…±åŒçš„ç¼ºç‚¹ï¼Œå³åœ¨åˆ†å¸ƒå¼æž¶æž„çš„æƒ…å†µä¸‹ï¼Œè¿™äº›éƒ½ä¸å…·æœ‰å¼¹æ€§ã€‚é‚£ä¹ˆéœ€è¦è€ƒè™‘æ•°æ®åˆ†ç‰‡åœ¨å¤šä¸ªæ•°æ®åº“èŠ‚ç‚¹ä¹‹é—´æ—¶ï¼Œç¬¬ä¸€ä¸ªæŠ€æœ¯å¦‚ä½•ç¡®ä¿ä¸åŒèŠ‚ç‚¹ä¸­çš„è¡¨ä¸ä¼šäº§ç”Ÿç›¸åŒçš„auto_incrementæ•°æˆ–æƒ³è±¡ä¸€ä¸ªæ‹“æ‰‘ï¼Œåœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œåº”ç”¨ç¨‹åºï¼Œé‚£ä¹ˆç¬¬äºŒç§æŠ€æœ¯å¦‚ä½•æ»¡è¶³æ‰€æœ‰èŠ‚ç‚¹çš„éœ€æ±‚ã€‚æ²¡æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥æ»¡è¶³æ‰€æœ‰çš„éœ€æ±‚ï¼Œä¸‹é¢æ˜¯åœ¨è®¸å¤šå¤§åž‹åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨çš„æœ€æµè¡Œçš„æ–¹æ³•ã€‚1. æ•°æ®åº“è‡ªå¢žé•¿åºåˆ—æˆ–å­—æ®µæœ€å¸¸è§çš„æ–¹å¼ã€‚åˆ©ç”¨æ•°æ®åº“ï¼Œå…¨æ•°æ®åº“å”¯ä¸€ã€‚ä¼˜ç‚¹ï¼š1ï¼‰ç®€å•ï¼Œä»£ç æ–¹ä¾¿ï¼Œæ€§èƒ½å¯ä»¥æŽ¥å—ã€‚2ï¼‰æ•°å­—IDå¤©ç„¶æŽ’åºï¼Œå¯¹åˆ†é¡µæˆ–è€…éœ€è¦æŽ’åºçš„ç»“æžœå¾ˆæœ‰å¸®åŠ©ã€‚ç¼ºç‚¹ï¼š1ï¼‰ä¸åŒæ•°æ®åº“è¯­æ³•å’Œå®žçŽ°ä¸åŒï¼Œæ•°æ®åº“è¿ç§»çš„æ—¶å€™æˆ–å¤šæ•°æ®åº“ç‰ˆæœ¬æ”¯æŒçš„æ—¶å€™éœ€è¦å¤„ç†ã€‚2ï¼‰åœ¨å•ä¸ªæ•°æ®åº“æˆ–è¯»å†™åˆ†ç¦»æˆ–ä¸€ä¸»å¤šä»Žçš„æƒ…å†µä¸‹ï¼Œåªæœ‰ä¸€ä¸ªä¸»åº“å¯ä»¥ç”Ÿæˆã€‚æœ‰å•ç‚¹æ•…éšœçš„é£Žé™©ã€‚3ï¼‰åœ¨æ€§èƒ½è¾¾ä¸åˆ°è¦æ±‚çš„æƒ…å†µä¸‹ï¼Œæ¯”è¾ƒéš¾äºŽæ‰©å±•ã€‚4ï¼‰å¦‚æžœé‡è§å¤šä¸ªç³»ç»Ÿéœ€è¦åˆå¹¶æˆ–è€…æ¶‰åŠåˆ°æ•°æ®è¿ç§»ä¼šç›¸å½“ç—›è‹¦ã€‚5ï¼‰åˆ†è¡¨åˆ†åº“çš„æ—¶å€™ä¼šæœ‰éº»çƒ¦ã€‚ä¼˜åŒ–æ–¹æ¡ˆï¼š1ï¼‰é’ˆå¯¹ä¸»åº“å•ç‚¹ï¼Œå¦‚æžœæœ‰å¤šä¸ªMasteråº“ï¼Œåˆ™æ¯ä¸ªMasteråº“è®¾ç½®çš„èµ·å§‹æ•°å­—ä¸ä¸€æ ·ï¼Œæ­¥é•¿ä¸€æ ·ï¼Œå¯ä»¥æ˜¯Masterçš„ä¸ªæ•°ã€‚æ¯”å¦‚ï¼šMaster1 ç”Ÿæˆçš„æ˜¯ 1ï¼Œ4ï¼Œ7ï¼Œ10ï¼ŒMaster2ç”Ÿæˆçš„æ˜¯2,5,8,11 Master3ç”Ÿæˆçš„æ˜¯ 3,6,9,12ã€‚è¿™æ ·å°±å¯ä»¥æœ‰æ•ˆç”Ÿæˆé›†ç¾¤ä¸­çš„å”¯ä¸€IDï¼Œä¹Ÿå¯ä»¥å¤§å¤§é™ä½ŽIDç”Ÿæˆæ•°æ®åº“æ“ä½œçš„è´Ÿè½½ã€‚2. UUIDå¸¸è§çš„æ–¹å¼ã€‚å¯ä»¥åˆ©ç”¨æ•°æ®åº“ä¹Ÿå¯ä»¥åˆ©ç”¨ç¨‹åºç”Ÿæˆï¼Œä¸€èˆ¬æ¥è¯´å…¨çƒå”¯ä¸€ã€‚ä¼˜ç‚¹ï¼š1ï¼‰ç®€å•ï¼Œä»£ç æ–¹ä¾¿ã€‚2ï¼‰ç”ŸæˆIDæ€§èƒ½éžå¸¸å¥½ï¼ŒåŸºæœ¬ä¸ä¼šæœ‰æ€§èƒ½é—®é¢˜ã€‚3ï¼‰å…¨çƒå”¯ä¸€ï¼Œåœ¨é‡è§æ•°æ®è¿ç§»ï¼Œç³»ç»Ÿæ•°æ®åˆå¹¶ï¼Œæˆ–è€…æ•°æ®åº“å˜æ›´ç­‰æƒ…å†µä¸‹ï¼Œå¯ä»¥ä»Žå®¹åº”å¯¹ã€‚ç¼ºç‚¹ï¼š1ï¼‰æ²¡æœ‰æŽ’åºï¼Œæ— æ³•ä¿è¯è¶‹åŠ¿é€’å¢žã€‚2ï¼‰UUIDå¾€å¾€æ˜¯ä½¿ç”¨å­—ç¬¦ä¸²å­˜å‚¨ï¼ŒæŸ¥è¯¢çš„æ•ˆçŽ‡æ¯”è¾ƒä½Žã€‚3ï¼‰å­˜å‚¨ç©ºé—´æ¯”è¾ƒå¤§ï¼Œå¦‚æžœæ˜¯æµ·é‡æ•°æ®åº“ï¼Œå°±éœ€è¦è€ƒè™‘å­˜å‚¨é‡çš„é—®é¢˜ã€‚4ï¼‰ä¼ è¾“æ•°æ®é‡å¤§5ï¼‰ä¸å¯è¯»ã€‚3. UUIDçš„å˜ç§1ï¼‰ä¸ºäº†è§£å†³UUIDä¸å¯è¯»ï¼Œå¯ä»¥ä½¿ç”¨UUID to Int64çš„æ–¹æ³•ã€‚åŠ12345678/// &lt;summary&gt;/// æ ¹æ®GUIDèŽ·å–å”¯ä¸€æ•°å­—åºåˆ—/// &lt;/summary&gt;public static long GuidToInt64()&#123; byte[] bytes = Guid.NewGuid().ToByteArray(); return BitConverter.ToInt64(bytes, 0);&#125;2ï¼‰ä¸ºäº†è§£å†³UUIDæ— åºçš„é—®é¢˜ï¼ŒNHibernateåœ¨å…¶ä¸»é”®ç”Ÿæˆæ–¹å¼ä¸­æä¾›äº†Combç®—æ³•ï¼ˆcombined guid/timestampï¼‰ã€‚ä¿ç•™GUIDçš„10ä¸ªå­—èŠ‚ï¼Œç”¨å¦6ä¸ªå­—èŠ‚è¡¨ç¤ºGUIDç”Ÿæˆçš„æ—¶é—´ï¼ˆDateTimeï¼‰ã€‚12345678910111213141516171819202122232425262728293031323334/// &lt;summary&gt; /// Generate a new &lt;see cref="Guid"/&gt; using the comb algorithm. /// &lt;/summary&gt; private Guid GenerateComb()&#123; byte[] guidArray = Guid.NewGuid().ToByteArray(); DateTime baseDate = new DateTime(1900, 1, 1); DateTime now = DateTime.Now; // Get the days and milliseconds which will be used to build //the byte string TimeSpan days = new TimeSpan(now.Ticks - baseDate.Ticks); TimeSpan msecs = now.TimeOfDay; // Convert to a byte array // Note that SQL Server is accurate to 1/300th of a // millisecond so we divide by 3.333333 byte[] daysArray = BitConverter.GetBytes(days.Days); byte[] msecsArray = BitConverter.GetBytes((long) (msecs.TotalMilliseconds / 3.333333)); // Reverse the bytes to match SQL Servers ordering Array.Reverse(daysArray); Array.Reverse(msecsArray); // Copy the bytes into the guid Array.Copy(daysArray, daysArray.Length - 2, guidArray, guidArray.Length - 6, 2); Array.Copy(msecsArray, msecsArray.Length - 4, guidArray, guidArray.Length - 4, 4); return new Guid(guidArray);&#125;ç”¨ä¸Šé¢çš„ç®—æ³•æµ‹è¯•ä¸€ä¸‹ï¼Œå¾—åˆ°å¦‚ä¸‹çš„ç»“æžœï¼šä½œä¸ºæ¯”è¾ƒï¼Œå‰é¢3ä¸ªæ˜¯ä½¿ç”¨COMBç®—æ³•å¾—å‡ºçš„ç»“æžœï¼Œæœ€åŽ12ä¸ªå­—ç¬¦ä¸²æ˜¯æ—¶é—´åºï¼ˆç»Ÿä¸€æ¯«ç§’ç”Ÿæˆçš„3ä¸ªUUIDï¼‰ï¼Œè¿‡æ®µæ—¶é—´å¦‚æžœå†æ¬¡ç”Ÿæˆï¼Œåˆ™12ä¸ªå­—ç¬¦ä¸²ä¼šæ¯”å›¾ç¤ºçš„è¦å¤§ã€‚åŽé¢3ä¸ªæ˜¯ç›´æŽ¥ç”Ÿæˆçš„GUIDã€‚å¦‚æžœæƒ³æŠŠæ—¶é—´åºæ”¾åœ¨å‰é¢ï¼Œå¯ä»¥ç”ŸæˆåŽæ”¹å˜12ä¸ªå­—ç¬¦ä¸²çš„ä½ç½®ï¼Œä¹Ÿå¯ä»¥ä¿®æ”¹ç®—æ³•ç±»çš„æœ€åŽä¸¤ä¸ªArray.Copyã€‚4. Redisç”ŸæˆIDå½“ä½¿ç”¨æ•°æ®åº“æ¥ç”ŸæˆIDæ€§èƒ½ä¸å¤Ÿè¦æ±‚çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨Redisæ¥ç”ŸæˆIDã€‚è¿™ä¸»è¦ä¾èµ–äºŽRedisæ˜¯å•çº¿ç¨‹çš„ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥ç”¨ç”Ÿæˆå…¨å±€å”¯ä¸€çš„IDã€‚å¯ä»¥ç”¨Redisçš„åŽŸå­æ“ä½œ INCRå’ŒINCRBYæ¥å®žçŽ°ã€‚å¯ä»¥ä½¿ç”¨Redisé›†ç¾¤æ¥èŽ·å–æ›´é«˜çš„åžåé‡ã€‚å‡å¦‚ä¸€ä¸ªé›†ç¾¤ä¸­æœ‰5å°Redisã€‚å¯ä»¥åˆå§‹åŒ–æ¯å°Redisçš„å€¼åˆ†åˆ«æ˜¯1,2,3,4,5ï¼Œç„¶åŽæ­¥é•¿éƒ½æ˜¯5ã€‚å„ä¸ªRedisç”Ÿæˆçš„IDä¸ºï¼šAï¼š1,6,11,16,21Bï¼š2,7,12,17,22Cï¼š3,8,13,18,23Dï¼š4,9,14,19,24Eï¼š5,10,15,20,25è¿™ä¸ªï¼Œéšä¾¿è´Ÿè½½åˆ°å“ªä¸ªæœºç¡®å®šå¥½ï¼Œæœªæ¥å¾ˆéš¾åšä¿®æ”¹ã€‚ä½†æ˜¯3-5å°æœåŠ¡å™¨åŸºæœ¬èƒ½å¤Ÿæ»¡è¶³å™¨ä¸Šï¼Œéƒ½å¯ä»¥èŽ·å¾—ä¸åŒçš„IDã€‚ä½†æ˜¯æ­¥é•¿å’Œåˆå§‹å€¼ä¸€å®šéœ€è¦äº‹å…ˆéœ€è¦äº†ã€‚ä½¿ç”¨Redisé›†ç¾¤ä¹Ÿå¯ä»¥æ–¹å¼å•ç‚¹æ•…éšœçš„é—®é¢˜ã€‚å¦å¤–ï¼Œæ¯”è¾ƒé€‚åˆä½¿ç”¨Redisæ¥ç”Ÿæˆæ¯å¤©ä»Ž0å¼€å§‹çš„æµæ°´å·ã€‚æ¯”å¦‚è®¢å•å·=æ—¥æœŸ+å½“æ—¥è‡ªå¢žé•¿å·ã€‚å¯ä»¥æ¯å¤©åœ¨Redisä¸­ç”Ÿæˆä¸€ä¸ªKeyï¼Œä½¿ç”¨INCRè¿›è¡Œç´¯åŠ ã€‚ä¼˜ç‚¹ï¼š1ï¼‰ä¸ä¾èµ–äºŽæ•°æ®åº“ï¼Œçµæ´»æ–¹ä¾¿ï¼Œä¸”æ€§èƒ½ä¼˜äºŽæ•°æ®åº“ã€‚2ï¼‰æ•°å­—IDå¤©ç„¶æŽ’åºï¼Œå¯¹åˆ†é¡µæˆ–è€…éœ€è¦æŽ’åºçš„ç»“æžœå¾ˆæœ‰å¸®åŠ©ã€‚ç¼ºç‚¹ï¼š1ï¼‰å¦‚æžœç³»ç»Ÿä¸­æ²¡æœ‰Redisï¼Œè¿˜éœ€è¦å¼•å…¥æ–°çš„ç»„ä»¶ï¼Œå¢žåŠ ç³»ç»Ÿå¤æ‚åº¦ã€‚2ï¼‰éœ€è¦ç¼–ç å’Œé…ç½®çš„å·¥ä½œé‡æ¯”è¾ƒå¤§ã€‚5. Twitterçš„snowflakeç®—æ³•snowflakeæ˜¯Twitterå¼€æºçš„åˆ†å¸ƒå¼IDç”Ÿæˆç®—æ³•ï¼Œç»“æžœæ˜¯ä¸€ä¸ªlongåž‹çš„IDã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šä½¿ç”¨41bitä½œä¸ºæ¯«ç§’æ•°ï¼Œ10bitä½œä¸ºæœºå™¨çš„IDï¼ˆ5ä¸ªbitæ˜¯æ•°æ®ä¸­å¿ƒï¼Œ5ä¸ªbitçš„æœºå™¨IDï¼‰ï¼Œ12bitä½œä¸ºæ¯«ç§’å†…çš„æµæ°´å·ï¼ˆæ„å‘³ç€æ¯ä¸ªèŠ‚ç‚¹åœ¨æ¯æ¯«ç§’å¯ä»¥äº§ç”Ÿ 4096 ä¸ª IDï¼‰ï¼Œæœ€åŽè¿˜æœ‰ä¸€ä¸ªç¬¦å·ä½ï¼Œæ°¸è¿œæ˜¯0ã€‚C#ä»£ç å¦‚ä¸‹ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/// &lt;summary&gt; /// From: https://github.com/twitter/snowflake /// An object that generates IDs. /// This is broken into a separate class in case /// we ever want to support multiple worker threads /// per process /// &lt;/summary&gt; public class IdWorker &#123; private long workerId; private long datacenterId; private long sequence = 0L; private static long twepoch = 1288834974657L; private static long workerIdBits = 5L; private static long datacenterIdBits = 5L; private static long maxWorkerId = -1L ^ (-1L &lt;&lt; (int)workerIdBits); private static long maxDatacenterId = -1L ^ (-1L &lt;&lt; (int)datacenterIdBits); private static long sequenceBits = 12L; private long workerIdShift = sequenceBits; private long datacenterIdShift = sequenceBits + workerIdBits; private long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; private long sequenceMask = -1L ^ (-1L &lt;&lt; (int)sequenceBits); private long lastTimestamp = -1L; private static object syncRoot = new object(); public IdWorker(long workerId, long datacenterId) &#123; // sanity check for workerId if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new ArgumentException(string.Format("worker Id can't be greater than %d or less than 0", maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new ArgumentException(string.Format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId)); &#125; this.workerId = workerId; this.datacenterId = datacenterId; &#125; public long nextId() &#123; lock (syncRoot) &#123; long timestamp = timeGen(); if (timestamp &lt; lastTimestamp) &#123; throw new ApplicationException(string.Format("Clock moved backwards. Refusing to generate id for %d milliseconds", lastTimestamp - timestamp)); &#125; if (lastTimestamp == timestamp) &#123; sequence = (sequence + 1) &amp; sequenceMask; if (sequence == 0) &#123; timestamp = tilNextMillis(lastTimestamp); &#125; &#125; else &#123; sequence = 0L; &#125; lastTimestamp = timestamp; return ((timestamp - twepoch) &lt;&lt; (int)timestampLeftShift) | (datacenterId &lt;&lt; (int)datacenterIdShift) | (workerId &lt;&lt; (int)workerIdShift) | sequence; &#125; &#125; protected long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; protected long timeGen() &#123; return (long)(DateTime.UtcNow - new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc)).TotalMilliseconds; &#125; &#125;æµ‹è¯•ä»£ç å¦‚ä¸‹ï¼š1234567891011121314151617181920212223242526272829303132333435363738private static void TestIdWorker() &#123; HashSet&lt;long&gt; set = new HashSet&lt;long&gt;(); IdWorker idWorker1 = new IdWorker(0, 0); IdWorker idWorker2 = new IdWorker(1, 0); Thread t1 = new Thread(() =&gt; DoTestIdWoker(idWorker1, set)); Thread t2 = new Thread(() =&gt; DoTestIdWoker(idWorker2, set)); t1.IsBackground = true; t2.IsBackground = true; t1.Start(); t2.Start(); try &#123; Thread.Sleep(30000); t1.Abort(); t2.Abort(); &#125; catch (Exception e) &#123; &#125; Console.WriteLine("done"); &#125; private static void DoTestIdWoker(IdWorker idWorker, HashSet&lt;long&gt; set) &#123; while (true) &#123; long id = idWorker.nextId(); if (!set.Add(id)) &#123; Console.WriteLine("duplicate:" + id); &#125; Thread.Sleep(1); &#125; &#125;snowflakeç®—æ³•å¯ä»¥æ ¹æ®è‡ªèº«é¡¹ç›®çš„éœ€è¦è¿›è¡Œä¸€å®šçš„ä¿®æ”¹ã€‚æ¯”å¦‚ä¼°ç®—æœªæ¥çš„æ•°æ®ä¸­å¿ƒä¸ªæ•°ï¼Œæ¯ä¸ªæ•°æ®ä¸­å¿ƒçš„æœºå™¨æ•°ä»¥åŠç»Ÿä¸€æ¯«ç§’å¯ä»¥èƒ½çš„å¹¶å‘æ•°æ¥è°ƒæ•´åœ¨ç®—æ³•ä¸­æ‰€éœ€è¦çš„bitæ•°ã€‚ä¼˜ç‚¹ï¼š1ï¼‰ä¸ä¾èµ–äºŽæ•°æ®åº“ï¼Œçµæ´»æ–¹ä¾¿ï¼Œä¸”æ€§èƒ½ä¼˜äºŽæ•°æ®åº“ã€‚2ï¼‰IDæŒ‰ç…§æ—¶é—´åœ¨å•æœºä¸Šæ˜¯é€’å¢žçš„ã€‚ç¼ºç‚¹ï¼š1ï¼‰åœ¨å•æœºä¸Šæ˜¯é€’å¢žçš„ï¼Œä½†æ˜¯ç”±äºŽæ¶‰åŠåˆ°åˆ†å¸ƒå¼çŽ¯å¢ƒï¼Œæ¯å°æœºå™¨ä¸Šçš„æ—¶é’Ÿä¸å¯èƒ½å®Œå…¨åŒæ­¥ï¼Œä¹Ÿè®¸æœ‰æ—¶å€™ä¹Ÿä¼šå‡ºçŽ°ä¸æ˜¯å…¨å±€é€’å¢žçš„æƒ…å†µã€‚Snowflake çš„å…¶ä»–å˜ç§Snowflake æœ‰ä¸€äº›å˜ç§, å„ä¸ªåº”ç”¨ç»“åˆè‡ªå·±çš„å®žé™…åœºæ™¯å¯¹ Snowflake åšäº†ä¸€äº›æ”¹åŠ¨. è¿™é‡Œä¸»è¦ä»‹ç» 3 ç§.5.1 Boundary flakeå˜åŒ–:ID é•¿åº¦æ‰©å±•åˆ° 128 bits:æœ€é«˜ 64 bits æ—¶é—´æˆ³;ç„¶åŽæ˜¯ 48 bits çš„ Worker å· (å’Œ Mac åœ°å€ä¸€æ ·é•¿);æœ€åŽæ˜¯ 16 bits çš„ Seq Numberç”±äºŽå®ƒç”¨ 48 bits ä½œä¸º Worker ID, å’Œ Mac åœ°å€çš„é•¿åº¦ä¸€æ ·, è¿™æ ·å¯åŠ¨æ—¶ä¸éœ€è¦å’Œ Zookeeper é€šè®¯èŽ·å– Worker ID. åšåˆ°äº†å®Œå…¨çš„åŽ»ä¸­å¿ƒåŒ–åŸºäºŽ Erlangå®ƒè¿™æ ·åšçš„ç›®çš„æ˜¯ç”¨æ›´å¤šçš„ bits å®žçŽ°æ›´å°çš„å†²çªæ¦‚çŽ‡, è¿™æ ·å°±æ”¯æŒæ›´å¤šçš„ Worker åŒæ—¶å·¥ä½œ. åŒæ—¶, æ¯æ¯«ç§’èƒ½åˆ†é…å‡ºæ›´å¤šçš„ ID5.2 SimpleflakeSimpleflake çš„æ€è·¯æ˜¯å–æ¶ˆ Worker å·, ä¿ç•™ 41 bits çš„ Timestamp, åŒæ—¶æŠŠ sequence number æ‰©å±•åˆ° 22 bits;Simpleflake çš„ç‰¹ç‚¹:sequence number å®Œå…¨é éšæœºäº§ç”Ÿ (è¿™æ ·ä¹Ÿå¯¼è‡´äº†ç”Ÿæˆçš„ ID å¯èƒ½å‡ºçŽ°é‡å¤)æ²¡æœ‰ Worker å·, ä¹Ÿå°±ä¸éœ€è¦å’Œ Zookeeper é€šè®¯, å®žçŽ°äº†å®Œå…¨åŽ»ä¸­å¿ƒåŒ–Timestamp ä¿æŒå’Œ Snowflake ä¸€è‡´, ä»ŠåŽå¯ä»¥æ— ç¼å‡çº§åˆ° SnowflakeSimpleflake çš„é—®é¢˜å°±æ˜¯ sequence number å®Œå…¨éšæœºç”Ÿæˆ, ä¼šå¯¼è‡´ç”Ÿæˆçš„ ID é‡å¤çš„å¯èƒ½. è¿™ä¸ªç”Ÿæˆ ID é‡å¤çš„æ¦‚çŽ‡éšç€æ¯ç§’ç”Ÿæˆçš„ ID æ•°çš„å¢žé•¿è€Œå¢žé•¿.æ‰€ä»¥, Simpleflake çš„é™åˆ¶å°±æ˜¯æ¯ç§’ç”Ÿæˆçš„ ID ä¸èƒ½å¤ªå¤š (æœ€å¥½å°äºŽ 100æ¬¡/ç§’, å¦‚æžœå¤§äºŽ 100æ¬¡/ç§’çš„åœºæ™¯, Simpleflake å°±ä¸é€‚ç”¨äº†, å»ºè®®åˆ‡æ¢å›ž Snowflake).5.3 instagram çš„åšæ³•å…ˆç®€å•ä»‹ç»ä¸€ä¸‹ instagram çš„åˆ†å¸ƒå¼å­˜å‚¨æ–¹æ¡ˆ:å…ˆæŠŠæ¯ä¸ª Table åˆ’åˆ†ä¸ºå¤šä¸ªé€»è¾‘åˆ†ç‰‡ (logic Shard), é€»è¾‘åˆ†ç‰‡çš„æ•°é‡å¯ä»¥å¾ˆå¤§, ä¾‹å¦‚ 2000 ä¸ªé€»è¾‘åˆ†ç‰‡ç„¶åŽåˆ¶å®šä¸€ä¸ªè§„åˆ™, è§„å®šæ¯ä¸ªé€»è¾‘åˆ†ç‰‡è¢«å­˜å‚¨åˆ°å“ªä¸ªæ•°æ®åº“å®žä¾‹ä¸Šé¢; æ•°æ®åº“å®žä¾‹ä¸éœ€è¦å¾ˆå¤š. ä¾‹å¦‚, å¯¹æœ‰ 2 ä¸ª PostgreSQL å®žä¾‹çš„ç³»ç»Ÿ (instagram ä½¿ç”¨ PostgreSQL); å¯ä»¥ä½¿ç”¨å¥‡æ•°é€»è¾‘åˆ†ç‰‡å­˜æ”¾åˆ°ç¬¬ä¸€ä¸ªæ•°æ®åº“å®žä¾‹, å¶æ•°é€»è¾‘åˆ†ç‰‡å­˜æ”¾åˆ°ç¬¬äºŒä¸ªæ•°æ®åº“å®žä¾‹çš„è§„åˆ™æ¯ä¸ª Table æŒ‡å®šä¸€ä¸ªå­—æ®µä½œä¸ºåˆ†ç‰‡å­—æ®µ (ä¾‹å¦‚, å¯¹ç”¨æˆ·è¡¨, å¯ä»¥æŒ‡å®š uid ä½œä¸ºåˆ†ç‰‡å­—æ®µ)æ’å…¥ä¸€ä¸ªæ–°çš„æ•°æ®æ—¶, å…ˆæ ¹æ®åˆ†ç‰‡å­—æ®µçš„å€¼, å†³å®šæ•°æ®è¢«åˆ†é…åˆ°å“ªä¸ªé€»è¾‘åˆ†ç‰‡ (logic Shard)ç„¶åŽå†æ ¹æ® logic Shard å’Œ PostgreSQL å®žä¾‹çš„å¯¹åº”å…³ç³», ç¡®å®šè¿™æ¡æ•°æ®åº”è¯¥è¢«å­˜æ”¾åˆ°å“ªå° PostgreSQL å®žä¾‹ä¸Šinstagram unique ID çš„ç»„æˆ:41 bits: Timestamp (æ¯«ç§’)13 bits: æ¯ä¸ª logic Shard çš„ä»£å· (æœ€å¤§æ”¯æŒ 8 x 1024 ä¸ª logic Shards)10 bits: sequence number; æ¯ä¸ª Shard æ¯æ¯«ç§’æœ€å¤šå¯ä»¥ç”Ÿæˆ 1024 ä¸ª IDç”Ÿæˆ unique ID æ—¶, 41 bits çš„ Timestamp å’Œ Snowflake ç±»ä¼¼, è¿™é‡Œå°±ä¸ç»†è¯´äº†.ä¸»è¦ä»‹ç»ä¸€ä¸‹ 13 bits çš„ logic Shard ä»£å· å’Œ 10 bits çš„ sequence number æ€Žä¹ˆç”Ÿæˆ.logic Shard ä»£å·:å‡è®¾æ’å…¥ä¸€æ¡æ–°çš„ç”¨æˆ·è®°å½•, æ’å…¥æ—¶, æ ¹æ® uid æ¥åˆ¤æ–­è¿™æ¡è®°å½•åº”è¯¥è¢«æ’å…¥åˆ°å“ªä¸ª logic Shard ä¸­.å‡è®¾å½“å‰è¦æ’å…¥çš„è®°å½•ä¼šè¢«æ’å…¥åˆ°ç¬¬ 1341 å· logic Shard ä¸­ (å‡è®¾å½“å‰çš„è¿™ä¸ª Table ä¸€å…±æœ‰ 2000 ä¸ª logic Shard)æ–°ç”Ÿæˆ ID çš„ 13 bits æ®µè¦å¡«çš„å°±æ˜¯ 1341 è¿™ä¸ªæ•°å­—sequence number åˆ©ç”¨ PostgreSQL æ¯ä¸ª Table ä¸Šçš„ auto-increment sequence æ¥ç”Ÿæˆ:å¦‚æžœå½“å‰è¡¨ä¸Šå·²ç»æœ‰ 5000 æ¡è®°å½•, é‚£ä¹ˆè¿™ä¸ªè¡¨çš„ä¸‹ä¸€ä¸ª auto-increment sequence å°±æ˜¯ 5001 (ç›´æŽ¥è°ƒç”¨ PL/PGSQL æä¾›çš„æ–¹æ³•å¯ä»¥èŽ·å–åˆ°)ç„¶åŽæŠŠ è¿™ä¸ª 5001 å¯¹ 1024 å–æ¨¡å°±å¾—åˆ°äº† 10 bits çš„ sequence numberinstagram è¿™ä¸ªæ–¹æ¡ˆçš„ä¼˜åŠ¿åœ¨äºŽ:åˆ©ç”¨ logic Shard å·æ¥æ›¿æ¢ Snowflake ä½¿ç”¨çš„ Worker å·, å°±ä¸éœ€è¦åˆ°ä¸­å¿ƒèŠ‚ç‚¹èŽ·å– Worker å·äº†. åšåˆ°äº†å®Œå…¨åŽ»ä¸­å¿ƒåŒ–å¦å¤–ä¸€ä¸ªé™„å¸¦çš„å¥½å¤„å°±æ˜¯, å¯ä»¥é€šè¿‡ ID ç›´æŽ¥çŸ¥é“è¿™æ¡è®°å½•è¢«å­˜æ”¾åœ¨å“ªä¸ª logic Shard ä¸ŠåŒæ—¶, ä»ŠåŽåšæ•°æ®è¿ç§»çš„æ—¶å€™, ä¹Ÿæ˜¯æŒ‰ logic Shard ä¸ºå•ä½åšæ•°æ®è¿ç§»çš„, æ‰€ä»¥è¿™ç§åšæ³•ä¹Ÿä¸ä¼šå½±å“åˆ°ä»ŠåŽçš„æ•°æ®è¿ç§»6. åˆ©ç”¨zookeeperç”Ÿæˆå”¯ä¸€IDzookeeperä¸»è¦é€šè¿‡å…¶znodeæ•°æ®ç‰ˆæœ¬æ¥ç”Ÿæˆåºåˆ—å·ï¼Œå¯ä»¥ç”Ÿæˆ32ä½å’Œ64ä½çš„æ•°æ®ç‰ˆæœ¬å·ï¼Œå®¢æˆ·ç«¯å¯ä»¥ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬å·æ¥ä½œä¸ºå”¯ä¸€çš„åºåˆ—å·ã€‚å¾ˆå°‘ä¼šä½¿ç”¨zookeeperæ¥ç”Ÿæˆå”¯ä¸€IDã€‚ä¸»è¦æ˜¯ç”±äºŽéœ€è¦ä¾èµ–zookeeperï¼Œå¹¶ä¸”æ˜¯å¤šæ­¥è°ƒç”¨APIï¼Œå¦‚æžœåœ¨ç«žäº‰è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œéœ€è¦è€ƒè™‘ä½¿ç”¨åˆ†å¸ƒå¼é”ã€‚å› æ­¤ï¼Œæ€§èƒ½åœ¨é«˜å¹¶å‘çš„åˆ†å¸ƒå¼çŽ¯å¢ƒä¸‹ï¼Œä¹Ÿä¸ç”šç†æƒ³ã€‚7. MongoDBçš„ObjectIdMongoDBçš„ObjectIdå’Œsnowflakeç®—æ³•ç±»ä¼¼ã€‚å®ƒè®¾è®¡æˆè½»é‡åž‹çš„ï¼Œä¸åŒçš„æœºå™¨éƒ½èƒ½ç”¨å…¨å±€å”¯ä¸€çš„åŒç§æ–¹æ³•æ–¹ä¾¿åœ°ç”Ÿæˆå®ƒã€‚MongoDB ä»Žä¸€å¼€å§‹å°±è®¾è®¡ç”¨æ¥ä½œä¸ºåˆ†å¸ƒå¼æ•°æ®åº“ï¼Œå¤„ç†å¤šä¸ªèŠ‚ç‚¹æ˜¯ä¸€ä¸ªæ ¸å¿ƒè¦æ±‚ã€‚ä½¿å…¶åœ¨åˆ†ç‰‡çŽ¯å¢ƒä¸­è¦å®¹æ˜“ç”Ÿæˆå¾—å¤šã€‚å…¶æ ¼å¼å¦‚ä¸‹ï¼šå‰4 ä¸ªå­—èŠ‚æ˜¯ä»Žæ ‡å‡†çºªå…ƒå¼€å§‹çš„æ—¶é—´æˆ³ï¼Œå•ä½ä¸ºç§’ã€‚æ—¶é—´æˆ³ï¼Œä¸ŽéšåŽçš„5 ä¸ªå­—èŠ‚ç»„åˆèµ·æ¥ï¼Œæä¾›äº†ç§’çº§åˆ«çš„å”¯ä¸€æ€§ã€‚ç”±äºŽæ—¶é—´æˆ³åœ¨å‰ï¼Œè¿™æ„å‘³ç€ObjectId å¤§è‡´ä¼šæŒ‰ç…§æ’å…¥çš„é¡ºåºæŽ’åˆ—ã€‚è¿™å¯¹äºŽæŸäº›æ–¹é¢å¾ˆæœ‰ç”¨ï¼Œå¦‚å°†å…¶ä½œä¸ºç´¢å¼•æé«˜æ•ˆçŽ‡ã€‚è¿™4 ä¸ªå­—èŠ‚ä¹Ÿéšå«äº†æ–‡æ¡£åˆ›å»ºçš„æ—¶é—´ã€‚ç»å¤§å¤šæ•°å®¢æˆ·ç«¯ç±»åº“éƒ½ä¼šå…¬å¼€ä¸€ä¸ªæ–¹æ³•ä»ŽObjectId èŽ·å–è¿™ä¸ªä¿¡æ¯ã€‚æŽ¥ä¸‹æ¥çš„3 å­—èŠ‚æ˜¯æ‰€åœ¨ä¸»æœºçš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚é€šå¸¸æ˜¯æœºå™¨ä¸»æœºåçš„æ•£åˆ—å€¼ã€‚è¿™æ ·å°±å¯ä»¥ç¡®ä¿ä¸åŒä¸»æœºç”Ÿæˆä¸åŒçš„ObjectIdï¼Œä¸äº§ç”Ÿå†²çªã€‚ä¸ºäº†ç¡®ä¿åœ¨åŒä¸€å°æœºå™¨ä¸Šå¹¶å‘çš„å¤šä¸ªè¿›ç¨‹äº§ç”Ÿçš„ObjectId æ˜¯å”¯ä¸€çš„ï¼ŒæŽ¥ä¸‹æ¥çš„ä¸¤å­—èŠ‚æ¥è‡ªäº§ç”ŸObjectId çš„è¿›ç¨‹æ ‡è¯†ç¬¦ï¼ˆPIDï¼‰ã€‚å‰9 å­—èŠ‚ä¿è¯äº†åŒä¸€ç§’é’Ÿä¸åŒæœºå™¨ä¸åŒè¿›ç¨‹äº§ç”Ÿçš„ObjectId æ˜¯å”¯ä¸€çš„ã€‚åŽ3 å­—èŠ‚å°±æ˜¯ä¸€ä¸ªè‡ªåŠ¨å¢žåŠ çš„è®¡æ•°å™¨ï¼Œç¡®ä¿ç›¸åŒè¿›ç¨‹åŒä¸€ç§’äº§ç”Ÿçš„ObjectId ä¹Ÿæ˜¯ä¸ä¸€æ ·çš„ã€‚åŒä¸€ç§’é’Ÿæœ€å¤šå…è®¸æ¯ä¸ªè¿›ç¨‹æ‹¥æœ‰2563ï¼ˆ16 777 216ï¼‰ä¸ªä¸åŒçš„ObjectIdã€‚8. Flickr çš„å…¨å±€ä¸»é”®ç”Ÿæˆæ–¹æ¡ˆflickrå·§å¦™åœ°ä½¿ç”¨äº†MySQLçš„è‡ªå¢žIDï¼ŒåŠreplace intoè¯­æ³•ï¼Œååˆ†ç®€æ´åœ°å®žçŽ°äº†åˆ†ç‰‡IDç”ŸæˆåŠŸèƒ½ã€‚æ¯”å¦‚åˆ›å»º64ä½çš„è‡ªå¢židï¼šé¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ªè¡¨ï¼š123456CREATE TABLE `uid_sequence` ( `id` bigint(20) unsigned NOT NULL auto_increment, `stub` char(1) NOT NULL default '', PRIMARY KEY (`id`), UNIQUE KEY `stub` (`stub`)) ENGINE=MyISAM;123456123456SELECT * from uid_sequence è¾“å‡ºï¼š+â€”â€”â€”â€”â€”â€”-+â€”â€”+| id | stub |+â€”â€”â€”â€”â€”â€”-+â€”â€”+| 72157623227190423 | a |å¦‚æžœæˆ‘éœ€è¦ä¸€ä¸ªå…¨å±€çš„å”¯ä¸€çš„64ä½uidï¼Œåˆ™æ‰§è¡Œï¼š12REPLACE INTO uid_sequence (stub) VALUES ('a');SELECT LAST_INSERT_ID();1212è¯´æ˜Žï¼šç”¨ REPLACE INTO ä»£æ›¿ INSERT INTO çš„å¥½å¤„æ˜¯é¿å…è¡¨è¡Œæ•°å¤ªå¤§ï¼Œè¿˜è¦å¦å¤–å®šæœŸæ¸…ç†ã€‚stub å­—æ®µè¦è®¾ä¸ºå”¯ä¸€ç´¢å¼•ï¼Œè¿™ä¸ª sequence è¡¨åªæœ‰ä¸€æ¡çºªå½•ï¼Œä½†ä¹Ÿå¯ä»¥åŒæ—¶ä¸ºå¤šå¼ è¡¨ç”Ÿæˆå…¨å±€ä¸»é”®ï¼Œä¾‹å¦‚user_order_idã€‚é™¤éžä½ éœ€è¦è¡¨çš„ä¸»é”®æ˜¯è¿žç»­çš„ï¼Œé‚£ä¹ˆå°±å¦å»ºä¸€ä¸ª user_order_id_sequence è¡¨ã€‚ç»è¿‡å®žé™…å¯¹æ¯”æµ‹è¯•ï¼Œä½¿ç”¨ MyISAM æ¯” Innodb æœ‰æ›´é«˜çš„æ€§èƒ½ã€‚è¿™é‡Œflickrä½¿ç”¨ä¸¤å°æ•°æ®åº“ï¼ˆä¹Ÿå¯ä»¥æ›´å¤šï¼‰ä½œä¸ºè‡ªå¢žåºåˆ—ç”Ÿæˆï¼Œé€šè¿‡è¿™ä¸¤å°æœºå™¨åšä¸»å¤‡å’Œè´Ÿè½½å‡è¡¡ã€‚1234567TicketServer1:auto-increment-increment = 2auto-increment-offset = 1TicketServer2:auto-increment-increment = 2auto-increment-offset = 212345671234567ä¼˜ç‚¹ï¼šç®€å•å¯é ã€‚ç¼ºç‚¹ï¼šidåªæ˜¯ä¸€ä¸ªIDï¼Œæ²¡æœ‰å¸¦å…¥æ—¶é—´ï¼ŒshardingIdç­‰ä¿¡æ¯ã€‚]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Reproduce DQN result]]></title>
      <url>%2F2017%2F05%2F24%2FReproduce-DQN-result%2F</url>
      <content type="text"><![CDATA[è®ºæ–‡é“¾æŽ¥ï¼šhttps://www.nature.com/nature/journal/v518/n7540/full/nature14236.htmlæºä»£ç åœ°å€ï¼šhttps://sites.google.com/a/deepmind.com/dqn/ç”±äºŽæºä»£ç ä¸­åªæœ‰è®­ç»ƒé˜¶æ®µï¼Œæ²¡æœ‰æµ‹è¯•é˜¶æ®µï¼Œå› æ­¤æˆ‘æ‰ç”¨äº†è¿™ä¸ªé¡¹ç›®çš„æµ‹è¯•è„šæœ¬ï¼Œå¹¶ä¸”ç”Ÿæˆæ¸¸æˆåŠ¨å›¾ã€‚å®žéªŒå¤çŽ°çš„æ­¥éª¤å¦‚ä¸‹ï¼ˆè¿™é‡Œå¼•ç”¨ä½œè€…åŽŸæ–‡ï¼‰ï¼šDQN 3.0This project contains the source code of DQN 3.0, a Lua-based deep reinforcement learning architecture, necessary to reproduce the experiments described in the paper â€œHuman-level control through deep reinforcement learningâ€, Nature 518, 529â€“533 (26 February 2015) doi:10.1038/nature14236.To replicate the experiment results, a number of dependencies need to be installed, namely:LuaJIT and Torch 7.0nngraphXitari (fork of the Arcade Learning Environment (Bellemare et al., 2013))AleWrap (a lua interface to Xitari) An install script for these dependencies is provided.Two run scripts are provided: run_cpu and run_gpu. As the names imply, the former trains the DQN network using regular CPUs, while the latter uses GPUs (CUDA), which typically results in a significant speed-up.Installation instructionsThe installation requires Linux with apt-get.Note: In order to run the GPU version of DQN, you should additionally have the NVIDIAÂ® CUDAÂ® (version 5.5 or later) toolkit installed prior to the Torch installation below. This can be downloaded from https://developer.nvidia.com/cuda-toolkit and installation instructions can be found in http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linuxTo train DQN on Atari games, the following components must be installed:LuaJIT and Torch 7.0nngraphXitariAleWrapTo install all of the above in a subdirectory called â€˜torchâ€™, it should be enough to run1./install_dependencies.shfrom the base directory of the package.Note: The above install script will install the following packages via apt-get: build-essential, gcc, g++, cmake, curl, libreadline-dev, git-core, libjpeg-dev, libpng-dev, ncurses-dev, imagemagick, unzipTraining DQN on Atari gamesPrior to running DQN on a game, you should copy its ROM in the â€˜romsâ€™ subdirectory. It should then be sufficient to run the script1./run_cpu &lt;game name&gt;Or, if GPU support is enabled,1./run_gpu &lt;game name&gt;Note: On a system with more than one GPU, DQN training can be launched on a specified GPU by setting the environment variable GPU_ID, e.g. by1GPU_ID=2 ./run_gpu &lt;game name&gt;If GPU_ID is not specified, the first available GPU (ID 0) will be used by default.è¿™ä¹‹åŽæ˜¯æˆ‘é‡‡ç”¨å¦ä¸€ä¸ªé¡¹ç›®çš„æµ‹è¯•æ­¥éª¤ï¼šStoring a .gif for a trained networkOnce you have a snapshot of a network you can run1./test_gpu &lt;game name&gt; &lt;snapshopt filename&gt;to make it play one game and store the .gif under gifs. For example1./test_gpu breakout DQN3_0_1_breakout_FULL_Y.t7OptionsOptions to DQN are set within run_cpu (respectively, run_gpu). You may, for example, want to change the frequency at which information is output to stdout by setting â€˜prog_freqâ€™ to a different value.æˆ‘åœ¨å®žéªŒè¿‡ç¨‹ä¸­ç¢°åˆ°äº†ä¸€ç³»åˆ—çš„é—®é¢˜ï¼Œå®žéªŒçŽ¯å¢ƒä¸ºWindows 10 Vmware Workstationä¸­è¿è¡Œçš„Ubuntu 16.04 LTSè™šæ‹Ÿæœºã€‚éœ€è¦å£°æ˜Žçš„æ˜¯ï¼Œæˆ‘æ‰ç”¨åŽŸå§‹ä»£ç å¹¶æ²¡æœ‰è¿è¡ŒæˆåŠŸï¼Œç»åˆ†æžåº”è¯¥æ˜¯è™šæ‹Ÿæœºçš„é—®é¢˜ï¼Œä½†ä»¥ä¸‹ç¢°åˆ°çš„é—®é¢˜åº”å½“æ˜¯å…·æœ‰ä¸€èˆ¬æ€§åœ°ï¼Œä¸‹ä¸€æ­¥å‡†å¤‡é‡‡ç”¨æµ‹è¯•é¡¹ç›®ä»£ç è¿è¡Œã€‚Some Problem./run_cpuä¹‹åŽå‡ºçŽ°Segmentation faulté”™è¯¯å¯èƒ½æ˜¯å› ä¸ºå…¶åŽå‚æ•°åä¸­å‡ºçŽ°å¤§å†™å­—æ¯å¯èƒ½æ˜¯å› ä¸ºå†…å­˜ä¸è¶³ï¼Œå¯ä»¥å°è¯•æ¢ä¸€ä¸ªæ¸¸æˆè¿è¡Œ./test_cpuä¹‹åŽæç¤ºæ‰¾ä¸åˆ°gdè¿™ä¸ªæ—¶å€™éœ€è¦æ‰‹åŠ¨å®‰è£…gdï¼Œå…·ä½“å®‰è£…æ–¹æ³•å¦‚ä¸‹ï¼šä¸‹è½½åœ°å€ï¼šhttps://ittner.github.io/lua-gd/manual.html#downloadæˆ‘ä¸‹è½½çš„æ˜¯è¿™ä¸ªç‰ˆæœ¬http://files.luaforge.net/releases/lua-gd/lua-gd/lua-gd-2.0.33r2forLua5.1/lua-gd-2.0.33r2.tar.gzä¸‹è½½è§£åŽ‹åŽï¼Œè¿›åˆ°å¯¹åº”çš„ç›®å½•ï¼Œæ‰§è¡Œå‘½ä»¤ï¼šmakemakeæˆåŠŸåŽï¼Œæ‰§è¡Œï¼šsudo make installå¦‚æžœä¸­é—´å‡ºçŽ°é”™è¯¯çš„è¯ï¼Œè¯·æŠŠä¸‹é¢çš„å‡ ä¸ªåŒ…éƒ½å®‰è£…ä¸Šï¼šsudo apt-get install lua5.1sudo apt-get install lua5.1-0-devsudo apt-get install liblua5.1-0-devsudo apt-get install libgd2-devå®‰è£…æˆåŠŸä¹‹åŽä¼šæœ‰å¦‚ä¸‹æç¤ºï¼š12345678910111213gcc -o gd.so `gdlib-config --features |sed -e &quot;s/GD_/-DGD_/g&quot;``gdlib-config --cflags` `pkg-config lua5.1 --cflags` -O3 -Wall -shared`gdlib-config --ldflags` `gdlib-config --libs` `pkg-config lua5.1 --libs`-lgd luagd.clua test_features.luaLua-GD version: lua-gd 2.0.33r2Lua-GD features: PNG support ..................... Enabled GIF support ..................... Enabled JPEG support .................... Enabled XPM/XBM support ................. Enabled FreeType support ................ Enabled Fontconfig support .............. Enabledå®‰è£…gdæ—¶è¿›è¡Œmakeçš„æ—¶å€™å‡ºçŽ°gd.h: No such file or directoryTry to install this package if you are in debian : libgd2-noxpm-devå®‰è£…gdæ—¶è¿›è¡Œmakeçš„æ—¶å€™å‡ºçŽ°srlua makefile error lua.h No such file or directorysudo apt-get install liblua5.1-0-devåœ¨è§£å†³ä»¥ä¸Šé—®é¢˜åŽä¾ç„¶é€šä¸è¿‡ç¼–è¯‘è¿™é‡Œå¼•ç”¨äº†https://groups.google.com/forum/#!topic/bamboo-cn/myYzVk5XLgc]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Question Answering: A Very Brief Introduction]]></title>
      <url>%2F2017%2F05%2F13%2FQuestion-Answering-A-Very-Brief-Introduction%2F</url>
      <content type="text"><![CDATA[Find answers to (natual language) questions by machineTypes of questionsFactoidDefinitionYes-NoOpinionComparisonMultiple Intelligences in Modern in QA SystemsKnowledge-QAç»“æž„åŒ–çš„ï¼ŒåŸºäºŽçŸ¥è¯†åº“ï¼Œå…¶å®žå°±æ˜¯ä¸€ä¸ªå›¾ï¼Œç»“ç‚¹æ˜¯å®žä½“ï¼Œè¾¹æ˜¯è¯­ä¹‰å…³ç³»ã€‚å…³é”®æ˜¯èƒ½å¤Ÿæå–ä¸­é—®é¢˜ä¸­çš„å®žä½“ä»¥åŠå®žä½“ä¹‹é—´çš„è¯­ä¹‰å…³ç³»Document-QAéžç»“æž„åŒ–çš„Social-QAç±»ä¼¼Quora, Zhihu, Stackoverflowç­‰]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Bot Sample: MultiDialog]]></title>
      <url>%2F2017%2F05%2F12%2FBot-Sample-MultiDialog%2F</url>
      <content type="text"><![CDATA[MessageController.cs1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162namespace MultiDialogsBot&#123; using System.Net; using System.Net.Http; using System.Threading.Tasks; using System.Web.Http; using Dialogs; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Connector; [BotAuthentication] public class MessagesController : ApiController &#123; /// &lt;summary&gt; /// POST: api/Messages /// Receive a message from a user and reply to it /// &lt;/summary&gt; public async Task&lt;HttpResponseMessage&gt; Post([FromBody]Activity activity) &#123; if (activity.Type == ActivityTypes.Message) &#123; await Conversation.SendAsync(activity, () =&gt; new RootDialog()); &#125; else &#123; this.HandleSystemMessage(activity); &#125; var response = Request.CreateResponse(HttpStatusCode.OK); return response; &#125; private Activity HandleSystemMessage(Activity message) &#123; if (message.Type == ActivityTypes.DeleteUserData) &#123; // Implement user deletion here // If we handle user deletion, return a real message &#125; else if (message.Type == ActivityTypes.ConversationUpdate) &#123; // Handle conversation state changes, like members being added and removed // Use Activity.MembersAdded and Activity.MembersRemoved and Activity.Action for info // Not available in all channels &#125; else if (message.Type == ActivityTypes.ContactRelationUpdate) &#123; // Handle add/remove from contact lists // Activity.From + Activity.Action represent what happened &#125; else if (message.Type == ActivityTypes.Typing) &#123; // Handle knowing tha the user is typing &#125; else if (message.Type == ActivityTypes.Ping) &#123; &#125; return null; &#125; &#125;&#125;RootDialog.cs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990namespace MultiDialogsBot.Dialogs&#123; using System; using System.Collections.Generic; using System.Threading; using System.Threading.Tasks; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Connector; [Serializable] public class RootDialog : IDialog&lt;object&gt; &#123; private const string FlightsOption = "Flights"; private const string HotelsOption = "Hotels"; public async Task StartAsync(IDialogContext context) &#123; context.Wait(this.MessageReceivedAsync); &#125; public virtual async Task MessageReceivedAsync(IDialogContext context, IAwaitable&lt;IMessageActivity&gt; result) &#123; var message = await result; if (message.Text.ToLower().Contains("help") || message.Text.ToLower().Contains("support") || message.Text.ToLower().Contains("problem")) &#123; await context.Forward(new SupportDialog(), this.ResumeAfterSupportDialog, message, CancellationToken.None); &#125; else &#123; this.ShowOptions(context); &#125; &#125; private void ShowOptions(IDialogContext context) &#123; PromptDialog.Choice(context, this.OnOptionSelected, new List&lt;string&gt;() &#123; FlightsOption, HotelsOption &#125;, "Are you looking for a flight or a hotel?", "Not a valid option", 3); &#125; private async Task OnOptionSelected(IDialogContext context, IAwaitable&lt;string&gt; result) &#123; try &#123; string optionSelected = await result; switch (optionSelected) &#123; case FlightsOption: context.Call(new FlightsDialog(), this.ResumeAfterOptionDialog); break; case HotelsOption: context.Call(new HotelsDialog(), this.ResumeAfterOptionDialog); break; &#125; &#125; catch (TooManyAttemptsException ex) &#123; await context.PostAsync($"Ooops! Too many attemps :(. But don't worry, I'm handling that exception and you can try again!"); context.Wait(this.MessageReceivedAsync); &#125; &#125; private async Task ResumeAfterSupportDialog(IDialogContext context, IAwaitable&lt;int&gt; result) &#123; var ticketNumber = await result; await context.PostAsync($"Thanks for contacting our support team. Your ticket number is &#123;ticketNumber&#125;."); context.Wait(this.MessageReceivedAsync); &#125; private async Task ResumeAfterOptionDialog(IDialogContext context, IAwaitable&lt;object&gt; result) &#123; try &#123; var message = await result; &#125; catch (Exception ex) &#123; await context.PostAsync($"Failed with message: &#123;ex.Message&#125;"); &#125; finally &#123; context.Wait(this.MessageReceivedAsync); &#125; &#125; &#125;&#125;SupportDialog.cs123456789101112131415161718192021222324252627namespace MultiDialogsBot.Dialogs&#123; using System; using System.Threading.Tasks; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Connector; [Serializable] public class SupportDialog : IDialog&lt;int&gt; &#123; public async Task StartAsync(IDialogContext context) &#123; context.Wait(this.MessageReceivedAsync); &#125; public virtual async Task MessageReceivedAsync(IDialogContext context, IAwaitable&lt;IMessageActivity&gt; result) &#123; var message = await result; var ticketNumber = new Random().Next(0, 20000); await context.PostAsync($"Your message '&#123;message.Text&#125;' was registered. Once we resolve it; we will get back to you."); context.Done(ticketNumber); &#125; &#125;&#125;FlightsDialog.cs12345678910111213141516namespace MultiDialogsBot.Dialogs&#123; using System; using System.Threading.Tasks; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Connector; [Serializable] public class FlightsDialog : IDialog&lt;object&gt; &#123; public async Task StartAsync(IDialogContext context) &#123; context.Fail(new NotImplementedException("Flights Dialog is not implemented and is instead being used to show context.Fail")); &#125; &#125;&#125;HotelsDialog.cs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126namespace MultiDialogsBot.Dialogs&#123; using System; using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using System.Web; using Microsoft.Bot.Builder.Dialogs; using Microsoft.Bot.Builder.FormFlow; using Microsoft.Bot.Connector; [Serializable] public class HotelsDialog : IDialog&lt;object&gt; &#123; public async Task StartAsync(IDialogContext context) &#123; await context.PostAsync("Welcome to the Hotels finder!"); var hotelsFormDialog = FormDialog.FromForm(this.BuildHotelsForm, FormOptions.PromptInStart); context.Call(hotelsFormDialog, this.ResumeAfterHotelsFormDialog); &#125; private IForm&lt;HotelsQuery&gt; BuildHotelsForm() &#123; OnCompletionAsyncDelegate&lt;HotelsQuery&gt; processHotelsSearch = async (context, state) =&gt; &#123; await context.PostAsync($"Ok. Searching for Hotels in &#123;state.Destination&#125; from &#123;state.CheckIn.ToString("MM/dd")&#125; to &#123;state.CheckIn.AddDays(state.Nights).ToString("MM/dd")&#125;..."); &#125;; return new FormBuilder&lt;HotelsQuery&gt;() .Field(nameof(HotelsQuery.Destination)) .Message("Looking for hotels in &#123;Destination&#125;...") .AddRemainingFields() .OnCompletion(processHotelsSearch) .Build(); &#125; private async Task ResumeAfterHotelsFormDialog(IDialogContext context, IAwaitable&lt;HotelsQuery&gt; result) &#123; try &#123; var searchQuery = await result; var hotels = await this.GetHotelsAsync(searchQuery); await context.PostAsync($"I found in total &#123;hotels.Count()&#125; hotels for your dates:"); var resultMessage = context.MakeMessage(); resultMessage.AttachmentLayout = AttachmentLayoutTypes.Carousel; resultMessage.Attachments = new List&lt;Attachment&gt;(); foreach (var hotel in hotels) &#123; HeroCard heroCard = new HeroCard() &#123; Title = hotel.Name, Subtitle = $"&#123;hotel.Rating&#125; starts. &#123;hotel.NumberOfReviews&#125; reviews. From $&#123;hotel.PriceStarting&#125; per night.", Images = new List&lt;CardImage&gt;() &#123; new CardImage() &#123; Url = hotel.Image &#125; &#125;, Buttons = new List&lt;CardAction&gt;() &#123; new CardAction() &#123; Title = "More details", Type = ActionTypes.OpenUrl, Value = $"https://www.bing.com/search?q=hotels+in+" + HttpUtility.UrlEncode(hotel.Location) &#125; &#125; &#125;; resultMessage.Attachments.Add(heroCard.ToAttachment()); &#125; await context.PostAsync(resultMessage); &#125; catch (FormCanceledException ex) &#123; string reply; if (ex.InnerException == null) &#123; reply = "You have canceled the operation. Quitting from the HotelsDialog"; &#125; else &#123; reply = $"Oops! Something went wrong :( Technical Details: &#123;ex.InnerException.Message&#125;"; &#125; await context.PostAsync(reply); &#125; finally &#123; context.Done&lt;object&gt;(null); &#125; &#125; private async Task&lt;IEnumerable&lt;Hotel&gt;&gt; GetHotelsAsync(HotelsQuery searchQuery) &#123; var hotels = new List&lt;Hotel&gt;(); // Filling the hotels results manually just for demo purposes for (int i = 1; i &lt;= 5; i++) &#123; var random = new Random(i); Hotel hotel = new Hotel() &#123; Name = $"&#123;searchQuery.Destination&#125; Hotel &#123;i&#125;", Location = searchQuery.Destination, Rating = random.Next(1, 5), NumberOfReviews = random.Next(0, 5000), PriceStarting = random.Next(80, 450), Image = $"https://placeholdit.imgix.net/~text?txtsize=35&amp;txt=Hotel+&#123;i&#125;&amp;w=500&amp;h=260" &#125;; hotels.Add(hotel); &#125; hotels.Sort((h1, h2) =&gt; h1.PriceStarting.CompareTo(h2.PriceStarting)); return hotels; &#125; &#125;&#125;Hotel.cs1234567891011121314151617181920namespace MultiDialogsBot&#123; using System; [Serializable] public class Hotel &#123; public string Name &#123; get; set; &#125; public int Rating &#123; get; set; &#125; public int NumberOfReviews &#123; get; set; &#125; public int PriceStarting &#123; get; set; &#125; public string Image &#123; get; set; &#125; public string Location &#123; get; set; &#125; &#125;&#125;HotelsQuery.cs12345678910111213141516171819namespace MultiDialogsBot&#123; using System; using Microsoft.Bot.Builder.FormFlow; [Serializable] public class HotelsQuery &#123; [Prompt("Please enter your &#123;&amp;&#125;")] public string Destination &#123; get; set; &#125; [Prompt("When do you want to &#123;&amp;&#125;?")] public DateTime CheckIn &#123; get; set; &#125; [Numeric(1, int.MaxValue)] [Prompt("How many &#123;&amp;&#125; do you want to stay?")] public int Nights &#123; get; set; &#125; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The First Course of C#]]></title>
      <url>%2F2017%2F05%2F10%2FThe-First-Course-of-C%2F</url>
      <content type="text"><![CDATA[åŸºæœ¬è¯­æ³•ä¸€ä¸ªä¾‹å­:123456789101112131415using System;namespace HelloWorldApplication&#123; /* ç±»åä¸º HelloWorld */ class HelloWorld &#123; /* mainå‡½æ•° */ static void Main(string[] args) &#123; /* æˆ‘çš„ç¬¬ä¸€ä¸ª C# ç¨‹åº */ Console.WriteLine(&quot;Hello World!&quot;); Console.ReadKey(); &#125; &#125;&#125;å¯¹è±¡ç±»åž‹æ˜¯ C# é€šç”¨ç±»åž‹ç³»ç»Ÿï¼ˆCommon Type System - CTSï¼‰ä¸­æ‰€æœ‰æ•°æ®ç±»åž‹çš„ç»ˆæžåŸºç±»ã€‚Object æ˜¯ System.Object ç±»çš„åˆ«åã€‚æ‰€ä»¥å¯¹è±¡ï¼ˆObjectï¼‰ç±»åž‹å¯ä»¥è¢«åˆ†é…ä»»ä½•å…¶ä»–ç±»åž‹ï¼ˆå€¼ç±»åž‹ã€å¼•ç”¨ç±»åž‹ã€é¢„å®šä¹‰ç±»åž‹æˆ–ç”¨æˆ·è‡ªå®šä¹‰ç±»åž‹ï¼‰çš„å€¼ã€‚ä½†æ˜¯ï¼Œåœ¨åˆ†é…å€¼ä¹‹å‰ï¼Œéœ€è¦å…ˆè¿›è¡Œç±»åž‹è½¬æ¢ã€‚å½“ä¸€ä¸ªå€¼ç±»åž‹è½¬æ¢ä¸ºå¯¹è±¡ç±»åž‹æ—¶ï¼Œåˆ™è¢«ç§°ä¸º è£…ç®±ï¼›å¦ä¸€æ–¹é¢ï¼Œå½“ä¸€ä¸ªå¯¹è±¡ç±»åž‹è½¬æ¢ä¸ºå€¼ç±»åž‹æ—¶ï¼Œåˆ™è¢«ç§°ä¸º æ‹†ç®±ã€‚12object obj;obj = 100; // è¿™æ˜¯è£…ç®±åŠ¨æ€ç±»åž‹æ‚¨å¯ä»¥å­˜å‚¨ä»»ä½•ç±»åž‹çš„å€¼åœ¨åŠ¨æ€æ•°æ®ç±»åž‹å˜é‡ä¸­ã€‚è¿™äº›å˜é‡çš„ç±»åž‹æ£€æŸ¥æ˜¯åœ¨è¿è¡Œæ—¶å‘ç”Ÿçš„ã€‚å£°æ˜ŽåŠ¨æ€ç±»åž‹çš„è¯­æ³•ï¼š1dynamic &lt;variable_name&gt; = value;ä¾‹å¦‚ï¼š1dynamic d = 20;åŠ¨æ€ç±»åž‹ä¸Žå¯¹è±¡ç±»åž‹ç›¸ä¼¼ï¼Œä½†æ˜¯å¯¹è±¡ç±»åž‹å˜é‡çš„ç±»åž‹æ£€æŸ¥æ˜¯åœ¨ç¼–è¯‘æ—¶å‘ç”Ÿçš„ï¼Œè€ŒåŠ¨æ€ç±»åž‹å˜é‡çš„ç±»åž‹æ£€æŸ¥æ˜¯åœ¨è¿è¡Œæ—¶å‘ç”Ÿçš„ã€‚å­—ç¬¦ä¸²çš„ç‰¹æ®Šå®šä¹‰æ–¹å¼å­—ç¬¦ä¸²ï¼ˆStringï¼‰ç±»åž‹å…è®¸æ‚¨ç»™å˜é‡åˆ†é…ä»»ä½•å­—ç¬¦ä¸²å€¼ã€‚å­—ç¬¦ä¸²ï¼ˆStringï¼‰ç±»åž‹æ˜¯ System.String ç±»çš„åˆ«åã€‚å®ƒæ˜¯ä»Žå¯¹è±¡ï¼ˆObjectï¼‰ç±»åž‹æ´¾ç”Ÿçš„ã€‚å­—ç¬¦ä¸²ï¼ˆStringï¼‰ç±»åž‹çš„å€¼å¯ä»¥é€šè¿‡ä¸¤ç§å½¢å¼è¿›è¡Œåˆ†é…ï¼šå¼•å·å’Œ @å¼•å·ã€‚ä¾‹å¦‚ï¼š1String str = &quot;runoob.com&quot;;ä¸€ä¸ª @å¼•å·å­—ç¬¦ä¸²ï¼š1@&quot;runoob.com&quot;;C# string å­—ç¬¦ä¸²çš„å‰é¢å¯ä»¥åŠ  @ï¼ˆç§°ä½œâ€é€å­—å­—ç¬¦ä¸²â€ï¼‰å°†è½¬ä¹‰å­—ç¬¦ï¼ˆ\ï¼‰å½“ä½œæ™®é€šå­—ç¬¦å¯¹å¾…ï¼Œæ¯”å¦‚ï¼š1string str = @&quot;C:\Windows&quot;;ç­‰ä»·äºŽï¼š1string str = &quot;C:\\Windows&quot;;@ å­—ç¬¦ä¸²ä¸­å¯ä»¥ä»»æ„æ¢è¡Œï¼Œæ¢è¡Œç¬¦åŠç¼©è¿›ç©ºæ ¼éƒ½è®¡ç®—åœ¨å­—ç¬¦ä¸²é•¿åº¦ä¹‹å†…ã€‚1234string str = @&quot;&lt;script type=&quot;&quot;text/javascript&quot;&quot;&gt; &lt;!-- --&gt;&lt;/script&gt;&quot;;æ˜¾å¼ç±»åž‹è½¬æ¢æ–¹å¼1234567891011121314151617181920namespace TypeConversionApplication&#123; class StringConversion &#123; static void Main(string[] args) &#123; int i = 75; float f = 53.005f; double d = 2345.7652; bool b = true; Console.WriteLine(i.ToString()); Console.WriteLine(f.ToString()); Console.WriteLine(d.ToString()); Console.WriteLine(b.ToString()); Console.ReadKey(); &#125; &#125;&#125;å‘½ä»¤è¡Œè¾“å…¥Systemå‘½åç©ºé—´ä¸­çš„Consoleç±»æä¾›äº†ä¸€ä¸ªå‡½æ•° ReadLine()ï¼Œç”¨äºŽæŽ¥æ”¶æ¥è‡ªç”¨æˆ·çš„è¾“å…¥ï¼Œå¹¶æŠŠå®ƒå­˜å‚¨åˆ°ä¸€ä¸ªå˜é‡ä¸­ã€‚ä¾‹å¦‚ï¼š12int num;num = Convert.ToInt32(Console.ReadLine());å‡½æ•° Convert.ToInt32()æŠŠç”¨æˆ·è¾“å…¥çš„æ•°æ®è½¬æ¢ä¸ºint æ•°æ®ç±»åž‹ï¼Œå› ä¸º Console.ReadLine()åªæŽ¥å—å­—ç¬¦ä¸²æ ¼å¼çš„æ•°æ®ã€‚ç‰¹æ®Šè¿ç®—ç¬¦è¿ç®—ç¬¦æè¿°å®žä¾‹sizeof()è¿”å›žæ•°æ®ç±»åž‹çš„å¤§å°ã€‚sizeof(int)ï¼Œå°†è¿”å›ž 4.typeof()è¿”å›ž class çš„ç±»åž‹ã€‚typeof(StreamReader);&amp;è¿”å›žå˜é‡çš„åœ°å€ã€‚&a; å°†å¾—åˆ°å˜é‡çš„å®žé™…åœ°å€ã€‚*å˜é‡çš„æŒ‡é’ˆã€‚*a; å°†æŒ‡å‘ä¸€ä¸ªå˜é‡ã€‚? :æ¡ä»¶è¡¨è¾¾å¼å¦‚æžœæ¡ä»¶ä¸ºçœŸ ? åˆ™ä¸º X : å¦åˆ™ä¸º Yisåˆ¤æ–­å¯¹è±¡æ˜¯å¦ä¸ºæŸä¸€ç±»åž‹ã€‚If( Ford is Car) // æ£€æŸ¥ Ford æ˜¯å¦æ˜¯ Car ç±»çš„ä¸€ä¸ªå¯¹è±¡ã€‚aså¼ºåˆ¶è½¬æ¢ï¼Œå³ä½¿è½¬æ¢å¤±è´¥ä¹Ÿä¸ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚Object obj = new StringReader(â€œHelloâ€);StringReader r = obj as StringReader;ç‰¹æ®Šè®¿é—®ä¿®é¥°ç¬¦Internal è®¿é—®ä¿®é¥°ç¬¦Internal è®¿é—®è¯´æ˜Žç¬¦å…è®¸ä¸€ä¸ªç±»å°†å…¶æˆå‘˜å˜é‡å’Œæˆå‘˜å‡½æ•°æš´éœ²ç»™å½“å‰ç¨‹åºä¸­çš„å…¶ä»–å‡½æ•°å’Œå¯¹è±¡ã€‚æ¢å¥è¯è¯´ï¼Œå¸¦æœ‰ internal è®¿é—®ä¿®é¥°ç¬¦çš„ä»»ä½•æˆå‘˜å¯ä»¥è¢«å®šä¹‰åœ¨è¯¥æˆå‘˜æ‰€å®šä¹‰çš„åº”ç”¨ç¨‹åºå†…çš„ä»»ä½•ç±»æˆ–æ–¹æ³•è®¿é—®ã€‚ç±»çš„é»˜è®¤è®¿é—®æ ‡è¯†ç¬¦æ˜¯ internalï¼Œæˆå‘˜çš„é»˜è®¤è®¿é—®æ ‡è¯†ç¬¦æ˜¯ privateã€‚ä¸‹é¢çš„å®žä¾‹è¯´æ˜Žäº†è¿™ç‚¹ï¼š123456789101112131415161718192021222324252627282930313233using System;namespace RectangleApplication&#123; class Rectangle &#123; //æˆå‘˜å˜é‡ internal double length; internal double width; double GetArea() &#123; return length * width; &#125; public void Display() &#123; Console.WriteLine(&quot;é•¿åº¦ï¼š &#123;0&#125;&quot;, length); Console.WriteLine(&quot;å®½åº¦ï¼š &#123;0&#125;&quot;, width); Console.WriteLine(&quot;é¢ç§¯ï¼š &#123;0&#125;&quot;, GetArea()); &#125; &#125;//end class Rectangle class ExecuteRectangle &#123; static void Main(string[] args) &#123; Rectangle r = new Rectangle(); r.length = 4.5; r.width = 3.5; r.Display(); Console.ReadLine(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š123é•¿åº¦ï¼š 4.5å®½åº¦ï¼š 3.5é¢ç§¯ï¼š 15.75åœ¨ä¸Šé¢çš„å®žä¾‹ä¸­ï¼Œè¯·æ³¨æ„æˆå‘˜å‡½æ•° GetArea() å£°æ˜Žçš„æ—¶å€™ä¸å¸¦æœ‰ä»»ä½•è®¿é—®ä¿®é¥°ç¬¦ã€‚å¦‚æžœæ²¡æœ‰æŒ‡å®šè®¿é—®ä¿®é¥°ç¬¦ï¼Œåˆ™ä½¿ç”¨ç±»æˆå‘˜çš„é»˜è®¤è®¿é—®ä¿®é¥°ç¬¦ï¼Œå³ä¸º privateã€‚Protected Internal è®¿é—®ä¿®é¥°ç¬¦Protected Internal è®¿é—®ä¿®é¥°ç¬¦å…è®¸åœ¨æœ¬ç±»,æ´¾ç”Ÿç±»æˆ–è€…åŒ…å«è¯¥ç±»çš„ç¨‹åºé›†ä¸­è®¿é—®ã€‚è¿™ä¹Ÿè¢«ç”¨äºŽå®žçŽ°ç»§æ‰¿ã€‚æŒ‰å¼•ç”¨ä¼ é€’å‚æ•°å¼•ç”¨å‚æ•°æ˜¯ä¸€ä¸ªå¯¹å˜é‡çš„å†…å­˜ä½ç½®çš„å¼•ç”¨ã€‚å½“æŒ‰å¼•ç”¨ä¼ é€’å‚æ•°æ—¶ï¼Œä¸Žå€¼å‚æ•°ä¸åŒçš„æ˜¯ï¼Œå®ƒä¸ä¼šä¸ºè¿™äº›å‚æ•°åˆ›å»ºä¸€ä¸ªæ–°çš„å­˜å‚¨ä½ç½®ã€‚å¼•ç”¨å‚æ•°è¡¨ç¤ºä¸Žæä¾›ç»™æ–¹æ³•çš„å®žé™…å‚æ•°å…·æœ‰ç›¸åŒçš„å†…å­˜ä½ç½®ã€‚åœ¨ C# ä¸­ï¼Œä½¿ç”¨ ref å…³é”®å­—å£°æ˜Žå¼•ç”¨å‚æ•°ã€‚ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†è¿™ç‚¹ï¼š1234567891011121314151617181920212223242526272829303132333435using System;namespace CalculatorApplication&#123; class NumberManipulator &#123; public void swap(ref int x, ref int y) &#123; int temp; temp = x; /* ä¿å­˜ x çš„å€¼ */ x = y; /* æŠŠ y èµ‹å€¼ç»™ x */ y = temp; /* æŠŠ temp èµ‹å€¼ç»™ y */ &#125; static void Main(string[] args) &#123; NumberManipulator n = new NumberManipulator(); /* å±€éƒ¨å˜é‡å®šä¹‰ */ int a = 100; int b = 200; Console.WriteLine(&quot;åœ¨äº¤æ¢ä¹‹å‰ï¼Œa çš„å€¼ï¼š &#123;0&#125;&quot;, a); Console.WriteLine(&quot;åœ¨äº¤æ¢ä¹‹å‰ï¼Œb çš„å€¼ï¼š &#123;0&#125;&quot;, b); /* è°ƒç”¨å‡½æ•°æ¥äº¤æ¢å€¼ */ n.swap(ref a, ref b); Console.WriteLine(&quot;åœ¨äº¤æ¢ä¹‹åŽï¼Œa çš„å€¼ï¼š &#123;0&#125;&quot;, a); Console.WriteLine(&quot;åœ¨äº¤æ¢ä¹‹åŽï¼Œb çš„å€¼ï¼š &#123;0&#125;&quot;, b); Console.ReadLine(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š1234åœ¨äº¤æ¢ä¹‹å‰ï¼Œa çš„å€¼ï¼š100åœ¨äº¤æ¢ä¹‹å‰ï¼Œb çš„å€¼ï¼š200åœ¨äº¤æ¢ä¹‹åŽï¼Œa çš„å€¼ï¼š200åœ¨äº¤æ¢ä¹‹åŽï¼Œb çš„å€¼ï¼š100ç»“æžœè¡¨æ˜Žï¼Œswap å‡½æ•°å†…çš„å€¼æ”¹å˜äº†ï¼Œä¸”è¿™ä¸ªæ”¹å˜å¯ä»¥åœ¨ Main å‡½æ•°ä¸­åæ˜ å‡ºæ¥ã€‚æŒ‰è¾“å‡ºä¼ é€’å‚æ•°return è¯­å¥å¯ç”¨äºŽåªä»Žå‡½æ•°ä¸­è¿”å›žä¸€ä¸ªå€¼ã€‚ä½†æ˜¯ï¼Œå¯ä»¥ä½¿ç”¨ è¾“å‡ºå‚æ•° æ¥ä»Žå‡½æ•°ä¸­è¿”å›žä¸¤ä¸ªå€¼ã€‚è¾“å‡ºå‚æ•°ä¼šæŠŠæ–¹æ³•è¾“å‡ºçš„æ•°æ®èµ‹ç»™è‡ªå·±ï¼Œå…¶ä»–æ–¹é¢ä¸Žå¼•ç”¨å‚æ•°ç›¸ä¼¼ã€‚ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†è¿™ç‚¹ï¼š1234567891011121314151617181920212223242526272829using System;namespace CalculatorApplication&#123; class NumberManipulator &#123; public void getValue(out int x ) &#123; int temp = 5; x = temp; &#125; static void Main(string[] args) &#123; NumberManipulator n = new NumberManipulator(); /* å±€éƒ¨å˜é‡å®šä¹‰ */ int a = 100; Console.WriteLine(&quot;åœ¨æ–¹æ³•è°ƒç”¨ä¹‹å‰ï¼Œa çš„å€¼ï¼š &#123;0&#125;&quot;, a); /* è°ƒç”¨å‡½æ•°æ¥èŽ·å–å€¼ */ n.getValue(out a); Console.WriteLine(&quot;åœ¨æ–¹æ³•è°ƒç”¨ä¹‹åŽï¼Œa çš„å€¼ï¼š &#123;0&#125;&quot;, a); Console.ReadLine(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12åœ¨æ–¹æ³•è°ƒç”¨ä¹‹å‰ï¼Œa çš„å€¼ï¼š 100åœ¨æ–¹æ³•è°ƒç”¨ä¹‹åŽï¼Œa çš„å€¼ï¼š 5æä¾›ç»™è¾“å‡ºå‚æ•°çš„å˜é‡ä¸éœ€è¦èµ‹å€¼ã€‚å½“éœ€è¦ä»Žä¸€ä¸ªå‚æ•°æ²¡æœ‰æŒ‡å®šåˆå§‹å€¼çš„æ–¹æ³•ä¸­è¿”å›žå€¼æ—¶ï¼Œè¾“å‡ºå‚æ•°ç‰¹åˆ«æœ‰ç”¨ã€‚è¯·çœ‹ä¸‹é¢çš„å®žä¾‹ï¼Œæ¥ç†è§£è¿™ä¸€ç‚¹ï¼š1234567891011121314151617181920212223242526272829using System;namespace CalculatorApplication&#123; class NumberManipulator &#123; public void getValues(out int x, out int y ) &#123; Console.WriteLine(&quot;è¯·è¾“å…¥ç¬¬ä¸€ä¸ªå€¼ï¼š &quot;); x = Convert.ToInt32(Console.ReadLine()); Console.WriteLine(&quot;è¯·è¾“å…¥ç¬¬äºŒä¸ªå€¼ï¼š &quot;); y = Convert.ToInt32(Console.ReadLine()); &#125; static void Main(string[] args) &#123; NumberManipulator n = new NumberManipulator(); /* å±€éƒ¨å˜é‡å®šä¹‰ */ int a , b; /* è°ƒç”¨å‡½æ•°æ¥èŽ·å–å€¼ */ n.getValues(out a, out b); Console.WriteLine(&quot;åœ¨æ–¹æ³•è°ƒç”¨ä¹‹åŽï¼Œa çš„å€¼ï¼š &#123;0&#125;&quot;, a); Console.WriteLine(&quot;åœ¨æ–¹æ³•è°ƒç”¨ä¹‹åŽï¼Œb çš„å€¼ï¼š &#123;0&#125;&quot;, b); Console.ReadLine(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼ˆå–å†³äºŽç”¨æˆ·è¾“å…¥ï¼‰ï¼š123456è¯·è¾“å…¥ç¬¬ä¸€ä¸ªå€¼ï¼š7è¯·è¾“å…¥ç¬¬äºŒä¸ªå€¼ï¼š8åœ¨æ–¹æ³•è°ƒç”¨ä¹‹åŽï¼Œa çš„å€¼ï¼š 7åœ¨æ–¹æ³•è°ƒç”¨ä¹‹åŽï¼Œb çš„å€¼ï¼š 8å¯ç©ºç±»åž‹C# æä¾›äº†ä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®ç±»åž‹ï¼Œnullable ç±»åž‹ï¼ˆå¯ç©ºç±»åž‹ï¼‰ï¼Œå¯ç©ºç±»åž‹å¯ä»¥è¡¨ç¤ºå…¶åŸºç¡€å€¼ç±»åž‹æ­£å¸¸èŒƒå›´å†…çš„å€¼ï¼Œå†åŠ ä¸Šä¸€ä¸ª null å€¼ã€‚ä¾‹å¦‚ï¼ŒNullable&lt; Int32 &gt;ï¼Œè¯»ä½œâ€å¯ç©ºçš„ Int32â€œï¼Œå¯ä»¥è¢«èµ‹å€¼ä¸º -2,147,483,648 åˆ° 2,147,483,647 ä¹‹é—´çš„ä»»æ„å€¼ï¼Œä¹Ÿå¯ä»¥è¢«èµ‹å€¼ä¸º null å€¼ã€‚ç±»ä¼¼çš„ï¼ŒNullable&lt; bool &gt; å˜é‡å¯ä»¥è¢«èµ‹å€¼ä¸º true æˆ– false æˆ– nullã€‚åœ¨å¤„ç†æ•°æ®åº“å’Œå…¶ä»–åŒ…å«å¯èƒ½æœªèµ‹å€¼çš„å…ƒç´ çš„æ•°æ®ç±»åž‹æ—¶ï¼Œå°† null èµ‹å€¼ç»™æ•°å€¼ç±»åž‹æˆ–å¸ƒå°”åž‹çš„åŠŸèƒ½ç‰¹åˆ«æœ‰ç”¨ã€‚ä¾‹å¦‚ï¼Œæ•°æ®åº“ä¸­çš„å¸ƒå°”åž‹å­—æ®µå¯ä»¥å­˜å‚¨å€¼ true æˆ– falseï¼Œæˆ–è€…ï¼Œè¯¥å­—æ®µä¹Ÿå¯ä»¥æœªå®šä¹‰ã€‚å£°æ˜Žä¸€ä¸ª nullableç±»åž‹ï¼ˆå¯ç©ºç±»åž‹ï¼‰çš„è¯­æ³•å¦‚ä¸‹ï¼š1&lt; data_type&gt; ? &lt;variable_name&gt; = null;ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†å¯ç©ºæ•°æ®ç±»åž‹çš„ç”¨æ³•ï¼š123456789101112131415161718192021222324using System;namespace CalculatorApplication&#123; class NullablesAtShow &#123; static void Main(string[] args) &#123; int? num1 = null; int? num2 = 45; double? num3 = new double?(); double? num4 = 3.14157; bool? boolval = new bool?(); // æ˜¾ç¤ºå€¼ Console.WriteLine(&quot;æ˜¾ç¤ºå¯ç©ºç±»åž‹çš„å€¼ï¼š &#123;0&#125;, &#123;1&#125;, &#123;2&#125;, &#123;3&#125;&quot;, num1, num2, num3, num4); Console.WriteLine(&quot;ä¸€ä¸ªå¯ç©ºçš„å¸ƒå°”å€¼ï¼š &#123;0&#125;&quot;, boolval); Console.ReadLine(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12æ˜¾ç¤ºå¯ç©ºç±»åž‹çš„å€¼ï¼š , 45, , 3.14157ä¸€ä¸ªå¯ç©ºçš„å¸ƒå°”å€¼ï¼šNullåˆå¹¶è¿ç®—ç¬¦Null åˆå¹¶è¿ç®—ç¬¦ç”¨äºŽå®šä¹‰å¯ç©ºç±»åž‹å’Œå¼•ç”¨ç±»åž‹çš„é»˜è®¤å€¼ã€‚Null åˆå¹¶è¿ç®—ç¬¦ä¸ºç±»åž‹è½¬æ¢å®šä¹‰äº†ä¸€ä¸ªé¢„è®¾å€¼ï¼Œä»¥é˜²å¯ç©ºç±»åž‹çš„å€¼ä¸º Nullã€‚Null åˆå¹¶è¿ç®—ç¬¦æŠŠæ“ä½œæ•°ç±»åž‹éšå¼è½¬æ¢ä¸ºå¦ä¸€ä¸ªå¯ç©ºï¼ˆæˆ–ä¸å¯ç©ºï¼‰çš„å€¼ç±»åž‹çš„æ“ä½œæ•°çš„ç±»åž‹ã€‚å¦‚æžœç¬¬ä¸€ä¸ªæ“ä½œæ•°çš„å€¼ä¸º nullï¼Œåˆ™è¿ç®—ç¬¦è¿”å›žç¬¬äºŒä¸ªæ“ä½œæ•°çš„å€¼ï¼Œå¦åˆ™è¿”å›žç¬¬ä¸€ä¸ªæ“ä½œæ•°çš„å€¼ã€‚ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†è¿™ç‚¹ï¼š123456789101112131415161718192021using System;namespace CalculatorApplication&#123; class NullablesAtShow &#123; static void Main(string[] args) &#123; double? num1 = null; double? num2 = 3.14157; double num3; num3 = num1 ?? 5.34; Console.WriteLine(&quot;num3 çš„å€¼ï¼š &#123;0&#125;&quot;, num3); num3 = num2 ?? 5.34; Console.WriteLine(&quot;num3 çš„å€¼ï¼š &#123;0&#125;&quot;, num3); Console.ReadLine(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12num3 çš„å€¼ï¼š 5.34num3 çš„å€¼ï¼š 3.14157foreachåœ¨å‰é¢çš„å®žä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª for å¾ªçŽ¯æ¥è®¿é—®æ¯ä¸ªæ•°ç»„å…ƒç´ ã€‚æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€ä¸ª foreach è¯­å¥æ¥éåŽ†æ•°ç»„ã€‚123456789101112131415161718192021222324252627using System;namespace ArrayApplication&#123; class MyArray &#123; static void Main(string[] args) &#123; int [] n = new int[10]; /* n æ˜¯ä¸€ä¸ªå¸¦æœ‰ 10 ä¸ªæ•´æ•°çš„æ•°ç»„, èµ‹å€¼æ—¶åˆå§‹åŒ–éœ€è¦ç”¨å¤§æ‹¬å·*/ /* åˆå§‹åŒ–æ•°ç»„ n ä¸­çš„å…ƒç´  */ for ( int i = 0; i &lt; 10; i++ ) &#123; n[i] = i + 100; &#125; /* è¾“å‡ºæ¯ä¸ªæ•°ç»„å…ƒç´ çš„å€¼ */ foreach (int j in n ) &#123; int i = j-100; Console.WriteLine(&quot;Element[&#123;0&#125;] = &#123;1&#125;&quot;, i, j); &#125; Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12345678910Element[0] = 100Element[1] = 101Element[2] = 102Element[3] = 103Element[4] = 104Element[5] = 105Element[6] = 106Element[7] = 107Element[8] = 108Element[9] = 109å¤šç»´æ•°ç»„æ‚¨å¯ä»¥å£°æ˜Žä¸€ä¸ª string å˜é‡çš„äºŒç»´æ•°ç»„ï¼Œå¦‚ä¸‹ï¼š1string [,] names;æˆ–è€…ï¼Œæ‚¨å¯ä»¥å£°æ˜Žä¸€ä¸ª int å˜é‡çš„ä¸‰ç»´æ•°ç»„ï¼Œå¦‚ä¸‹ï¼š1int [ , , ] m;å¤šç»´æ•°ç»„å¯ä»¥é€šè¿‡åœ¨æ‹¬å·å†…ä¸ºæ¯è¡ŒæŒ‡å®šå€¼æ¥è¿›è¡Œåˆå§‹åŒ–ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¸¦æœ‰ 3 è¡Œ 4 åˆ—çš„æ•°ç»„ã€‚12345int [,] a = new int [3,4] &#123; &#123;0, 1, 2, 3&#125; , /* åˆå§‹åŒ–ç´¢å¼•å·ä¸º 0 çš„è¡Œ */ &#123;4, 5, 6, 7&#125; , /* åˆå§‹åŒ–ç´¢å¼•å·ä¸º 1 çš„è¡Œ */ &#123;8, 9, 10, 11&#125; /* åˆå§‹åŒ–ç´¢å¼•å·ä¸º 2 çš„è¡Œ */&#125;;äºŒç»´æ•°ç»„ä¸­çš„å…ƒç´ æ˜¯é€šè¿‡ä½¿ç”¨ä¸‹æ ‡ï¼ˆå³æ•°ç»„çš„è¡Œç´¢å¼•å’Œåˆ—ç´¢å¼•ï¼‰æ¥è®¿é—®çš„ã€‚ä¾‹å¦‚ï¼š1int val = a[2,3];äº¤é”™æ•°ç»„äº¤é”™æ•°ç»„æ˜¯æ•°ç»„çš„æ•°ç»„ã€‚æ‚¨å¯ä»¥å£°æ˜Žä¸€ä¸ªå¸¦æœ‰ int å€¼çš„äº¤é”™æ•°ç»„ scoresï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š1int [][] scores;å£°æ˜Žä¸€ä¸ªæ•°ç»„ä¸ä¼šåœ¨å†…å­˜ä¸­åˆ›å»ºæ•°ç»„ã€‚åˆ›å»ºä¸Šé¢çš„æ•°ç»„ï¼š12345int[][] scores = new int[5][];for (int i = 0; i &lt; scores.Length; i++) &#123; scores[i] = new int[4];&#125;æ‚¨å¯ä»¥åˆå§‹åŒ–ä¸€ä¸ªäº¤é”™æ•°ç»„ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š1int[][] scores = new int[2][]&#123;new int[]&#123;92,93,94&#125;,new int[]&#123;85,66,87,88&#125;&#125;;å…¶ä¸­ï¼Œscores æ˜¯ä¸€ä¸ªç”±ä¸¤ä¸ªæ•´åž‹æ•°ç»„ç»„æˆçš„æ•°ç»„ â€“ scores[0] æ˜¯ä¸€ä¸ªå¸¦æœ‰ 3 ä¸ªæ•´æ•°çš„æ•°ç»„ï¼Œscores[1] æ˜¯ä¸€ä¸ªå¸¦æœ‰ 4 ä¸ªæ•´æ•°çš„æ•°ç»„ã€‚123456789101112131415161718192021222324252627ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨äº¤é”™æ•°ç»„ï¼šusing System;namespace ArrayApplication&#123; class MyArray &#123; static void Main(string[] args) &#123; /* ä¸€ä¸ªç”± 5 ä¸ªæ•´åž‹æ•°ç»„ç»„æˆçš„äº¤é”™æ•°ç»„ */ int[][] a = new int[][]&#123;new int[]&#123;0,0&#125;,new int[]&#123;1,2&#125;, new int[]&#123;2,4&#125;,new int[]&#123; 3, 6 &#125;, new int[]&#123; 4, 8 &#125; &#125;; int i, j; /* è¾“å‡ºæ•°ç»„ä¸­æ¯ä¸ªå…ƒç´ çš„å€¼ */ for (i = 0; i &lt; 5; i++) &#123; for (j = 0; j &lt; 2; j++) &#123; Console.WriteLine(&quot;a[&#123;0&#125;][&#123;1&#125;] = &#123;2&#125;&quot;, i, j, a[i][j]); &#125; &#125; Console.ReadKey(); &#125; &#125;&#125;å‚æ•°æ•°ç»„æœ‰æ—¶ï¼Œå½“å£°æ˜Žä¸€ä¸ªæ–¹æ³•æ—¶ï¼Œæ‚¨ä¸èƒ½ç¡®å®šè¦ä¼ é€’ç»™å‡½æ•°ä½œä¸ºå‚æ•°çš„å‚æ•°æ•°ç›®ã€‚C# å‚æ•°æ•°ç»„è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œå‚æ•°æ•°ç»„é€šå¸¸ç”¨äºŽä¼ é€’æœªçŸ¥æ•°é‡çš„å‚æ•°ç»™å‡½æ•°ã€‚params å…³é”®å­—åœ¨ä½¿ç”¨æ•°ç»„ä½œä¸ºå½¢å‚æ—¶ï¼ŒC# æä¾›äº† params å…³é”®å­—ï¼Œä½¿è°ƒç”¨æ•°ç»„ä¸ºå½¢å‚çš„æ–¹æ³•æ—¶ï¼Œæ—¢å¯ä»¥ä¼ é€’æ•°ç»„å®žå‚ï¼Œä¹Ÿå¯ä»¥åªä¼ é€’ä¸€ç»„æ•°ç»„ã€‚params çš„ä½¿ç”¨æ ¼å¼ä¸ºï¼š1public è¿”å›žç±»åž‹ æ–¹æ³•åç§°( params ç±»åž‹åç§°[] æ•°ç»„åç§° )å®žä¾‹ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨å‚æ•°æ•°ç»„ï¼š12345678910111213141516171819202122232425262728using System;namespace ArrayApplication&#123; class ParamArray &#123; public int AddElements(params int[] arr) &#123; int sum = 0; foreach (int i in arr) &#123; sum += i; &#125; return sum; &#125; &#125; class TestClass &#123; static void Main(string[] args) &#123; ParamArray app = new ParamArray(); int sum = app.AddElements(512, 720, 250, 567, 889); Console.WriteLine(&quot;æ€»å’Œæ˜¯ï¼š &#123;0&#125;&quot;, sum); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š1æ€»å’Œæ˜¯ï¼š 2938Arrayç±»Array ç±»æ˜¯ C# ä¸­æ‰€æœ‰æ•°ç»„çš„åŸºç±»ï¼Œå®ƒæ˜¯åœ¨ System å‘½åç©ºé—´ä¸­å®šä¹‰ã€‚Array ç±»æä¾›äº†å„ç§ç”¨äºŽæ•°ç»„çš„å±žæ€§å’Œæ–¹æ³•ã€‚ä¸‹é¢çš„ç¨‹åºæ¼”ç¤ºäº† Array ç±»çš„ä¸€äº›æ–¹æ³•çš„ç”¨æ³•ï¼š12345678910111213141516171819202122232425262728293031323334353637383940using System;namespace ArrayApplication&#123; class MyArray &#123; static void Main(string[] args) &#123; int[] list = &#123; 34, 72, 13, 44, 25, 30, 10 &#125;; int[] temp = list; Console.Write(&quot;åŽŸå§‹æ•°ç»„ï¼š &quot;); foreach (int i in list) &#123; Console.Write(i + &quot; &quot;); &#125; Console.WriteLine(); // é€†è½¬æ•°ç»„ Array.Reverse(temp); Console.Write(&quot;é€†è½¬æ•°ç»„ï¼š &quot;); foreach (int i in temp) &#123; Console.Write(i + &quot; &quot;); &#125; Console.WriteLine(); // æŽ’åºæ•°ç»„ Array.Sort(list); Console.Write(&quot;æŽ’åºæ•°ç»„ï¼š &quot;); foreach (int i in list) &#123; Console.Write(i + &quot; &quot;); &#125; Console.WriteLine(); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š123åŽŸå§‹æ•°ç»„ï¼š 34 72 13 44 25 30 10é€†è½¬æ•°ç»„ï¼š 10 30 25 44 13 72 34æŽ’åºæ•°ç»„ï¼š 10 13 25 30 34 44 72ç»“æž„ä½“åœ¨ C# ä¸­ï¼Œç»“æž„æ˜¯å€¼ç±»åž‹æ•°æ®ç»“æž„ã€‚å®ƒä½¿å¾—ä¸€ä¸ªå•ä¸€å˜é‡å¯ä»¥å­˜å‚¨å„ç§æ•°æ®ç±»åž‹çš„ç›¸å…³æ•°æ®ã€‚struct å…³é”®å­—ç”¨äºŽåˆ›å»ºç»“æž„ã€‚ç»“æž„æ˜¯ç”¨æ¥ä»£è¡¨ä¸€ä¸ªè®°å½•ã€‚å‡è®¾æ‚¨æƒ³è·Ÿè¸ªå›¾ä¹¦é¦†ä¸­ä¹¦çš„åŠ¨æ€ã€‚æ‚¨å¯èƒ½æƒ³è·Ÿè¸ªæ¯æœ¬ä¹¦çš„ä»¥ä¸‹å±žæ€§ï¼šTitleAuthorSubjectBook IDå®šä¹‰ç»“æž„ä¸ºäº†å®šä¹‰ä¸€ä¸ªç»“æž„ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ struct è¯­å¥ã€‚struct è¯­å¥ä¸ºç¨‹åºå®šä¹‰äº†ä¸€ä¸ªå¸¦æœ‰å¤šä¸ªæˆå‘˜çš„æ–°çš„æ•°æ®ç±»åž‹ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§å¦‚ä¸‹çš„æ–¹å¼å£°æ˜Ž Book ç»“æž„ï¼š1234567struct Books&#123; public string title; public string author; public string subject; public int book_id;&#125;;ä¸‹é¢çš„ç¨‹åºæ¼”ç¤ºäº†ç»“æž„çš„ç”¨æ³•ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546using System; struct Books&#123; public string title; public string author; public string subject; public int book_id;&#125;; public class testStructure&#123; public static void Main(string[] args) &#123; Books Book1; /* å£°æ˜Ž Book1ï¼Œç±»åž‹ä¸º Book */ Books Book2; /* å£°æ˜Ž Book2ï¼Œç±»åž‹ä¸º Book */ /* book 1 è¯¦è¿° */ Book1.title = &quot;C Programming&quot;; Book1.author = &quot;Nuha Ali&quot;; Book1.subject = &quot;C Programming Tutorial&quot;; Book1.book_id = 6495407; /* book 2 è¯¦è¿° */ Book2.title = &quot;Telecom Billing&quot;; Book2.author = &quot;Zara Ali&quot;; Book2.subject = &quot;Telecom Billing Tutorial&quot;; Book2.book_id = 6495700; /* æ‰“å° Book1 ä¿¡æ¯ */ Console.WriteLine( &quot;Book 1 title : &#123;0&#125;&quot;, Book1.title); Console.WriteLine(&quot;Book 1 author : &#123;0&#125;&quot;, Book1.author); Console.WriteLine(&quot;Book 1 subject : &#123;0&#125;&quot;, Book1.subject); Console.WriteLine(&quot;Book 1 book_id :&#123;0&#125;&quot;, Book1.book_id); /* æ‰“å° Book2 ä¿¡æ¯ */ Console.WriteLine(&quot;Book 2 title : &#123;0&#125;&quot;, Book2.title); Console.WriteLine(&quot;Book 2 author : &#123;0&#125;&quot;, Book2.author); Console.WriteLine(&quot;Book 2 subject : &#123;0&#125;&quot;, Book2.subject); Console.WriteLine(&quot;Book 2 book_id : &#123;0&#125;&quot;, Book2.book_id); Console.ReadKey(); &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12345678Book 1 title : C ProgrammingBook 1 author : Nuha AliBook 1 subject : C Programming TutorialBook 1 book_id : 6495407Book 2 title : Telecom BillingBook 2 author : Zara AliBook 2 subject : Telecom Billing TutorialBook 2 book_id : 6495700C# ç»“æž„çš„ç‰¹ç‚¹æ‚¨å·²ç»ç”¨äº†ä¸€ä¸ªç®€å•çš„åä¸º Books çš„ç»“æž„ã€‚åœ¨ C# ä¸­çš„ç»“æž„ä¸Žä¼ ç»Ÿçš„ C æˆ– C++ ä¸­çš„ç»“æž„ä¸åŒã€‚C# ä¸­çš„ç»“æž„æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼šç»“æž„å¯å¸¦æœ‰æ–¹æ³•ã€å­—æ®µã€ç´¢å¼•ã€å±žæ€§ã€è¿ç®—ç¬¦æ–¹æ³•å’Œäº‹ä»¶ã€‚ç»“æž„å¯å®šä¹‰æž„é€ å‡½æ•°ï¼Œä½†ä¸èƒ½å®šä¹‰æžæž„å‡½æ•°ã€‚ä½†æ˜¯ï¼Œæ‚¨ä¸èƒ½ä¸ºç»“æž„å®šä¹‰é»˜è®¤çš„æž„é€ å‡½æ•°ã€‚é»˜è®¤çš„æž„é€ å‡½æ•°æ˜¯è‡ªåŠ¨å®šä¹‰çš„ï¼Œä¸”ä¸èƒ½è¢«æ”¹å˜ã€‚ä¸Žç±»ä¸åŒï¼Œç»“æž„ä¸èƒ½ç»§æ‰¿å…¶ä»–çš„ç»“æž„æˆ–ç±»ã€‚ç»“æž„ä¸èƒ½ä½œä¸ºå…¶ä»–ç»“æž„æˆ–ç±»çš„åŸºç¡€ç»“æž„ã€‚ç»“æž„å¯å®žçŽ°ä¸€ä¸ªæˆ–å¤šä¸ªæŽ¥å£ã€‚ç»“æž„æˆå‘˜ä¸èƒ½æŒ‡å®šä¸º abstractã€virtual æˆ– protectedã€‚å½“æ‚¨ä½¿ç”¨ New æ“ä½œç¬¦åˆ›å»ºä¸€ä¸ªç»“æž„å¯¹è±¡æ—¶ï¼Œä¼šè°ƒç”¨é€‚å½“çš„æž„é€ å‡½æ•°æ¥åˆ›å»ºç»“æž„ã€‚ä¸Žç±»ä¸åŒï¼Œç»“æž„å¯ä»¥ä¸ä½¿ç”¨ New æ“ä½œç¬¦å³å¯è¢«å®žä¾‹åŒ–ã€‚å¦‚æžœä¸ä½¿ç”¨ New æ“ä½œç¬¦ï¼Œåªæœ‰åœ¨æ‰€æœ‰çš„å­—æ®µéƒ½è¢«åˆå§‹åŒ–ä¹‹åŽï¼Œå­—æ®µæ‰è¢«èµ‹å€¼ï¼Œå¯¹è±¡æ‰è¢«ä½¿ç”¨ã€‚ç±» vs ç»“æž„ç±»å’Œç»“æž„æœ‰ä»¥ä¸‹å‡ ä¸ªåŸºæœ¬çš„ä¸åŒç‚¹ï¼šç±»æ˜¯å¼•ç”¨ç±»åž‹ï¼Œç»“æž„æ˜¯å€¼ç±»åž‹ã€‚ç»“æž„ä¸æ”¯æŒç»§æ‰¿ã€‚ç»“æž„ä¸èƒ½å£°æ˜Žé»˜è®¤çš„æž„é€ å‡½æ•°ã€‚é’ˆå¯¹ä¸Šè¿°è®¨è®ºï¼Œè®©æˆ‘ä»¬é‡å†™å‰é¢çš„å®žä¾‹ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051using System; struct Books&#123; private string title; private string author; private string subject; private int book_id; public void getValues(string t, string a, string s, int id) &#123; title = t; author = a; subject = s; book_id = id; &#125; public void display() &#123; Console.WriteLine(&quot;Title : &#123;0&#125;&quot;, title); Console.WriteLine(&quot;Author : &#123;0&#125;&quot;, author); Console.WriteLine(&quot;Subject : &#123;0&#125;&quot;, subject); Console.WriteLine(&quot;Book_id :&#123;0&#125;&quot;, book_id); &#125;&#125;; public class testStructure&#123; public static void Main(string[] args) &#123; Books Book1 = new Books(); /* å£°æ˜Ž Book1ï¼Œç±»åž‹ä¸º Book */ Books Book2 = new Books(); /* å£°æ˜Ž Book2ï¼Œç±»åž‹ä¸º Book */ /* book 1 è¯¦è¿° */ Book1.getValues(&quot;C Programming&quot;, &quot;Nuha Ali&quot;, &quot;C Programming Tutorial&quot;,6495407); /* book 2 è¯¦è¿° */ Book2.getValues(&quot;Telecom Billing&quot;, &quot;Zara Ali&quot;, &quot;Telecom Billing Tutorial&quot;, 6495700); /* æ‰“å° Book1 ä¿¡æ¯ */ Book1.display(); /* æ‰“å° Book2 ä¿¡æ¯ */ Book2.display(); Console.ReadKey(); &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12345678Title : C ProgrammingAuthor : Nuha AliSubject : C Programming TutorialBook_id : 6495407Title : Telecom BillingAuthor : Zara AliSubject : Telecom Billing TutorialBook_id : 6495700å¤šæ€æ€§å¤šæ€ï¼šä¸€ä¸ªæŽ¥å£å¤šä¸ªåŠŸèƒ½ã€‚é™æ€å¤šæ€æ€§ï¼šç¼–è¯‘æ—¶å‘ç”Ÿå‡½æ•°å“åº”ï¼ˆè°ƒç”¨ï¼‰ï¼›åŠ¨æ€å¤šæ€æ€§ï¼šè¿è¡Œæ—¶å‘ç”Ÿå‡½æ•°å“åº”ã€‚é™æ€ç»‘å®šï¼ˆæ—©æœŸç»‘å®šï¼‰ï¼šç¼–è¯‘æ—¶å‡½æ•°å’Œå¯¹è±¡çš„è¿žæŽ¥æœºåˆ¶ã€‚ä¸¤ç§æŠ€æœ¯å®žçŽ°é™æ€å¤šæ€æ€§ï¼šå‡½æ•°é‡è½½/è¿ç®—ç¬¦é‡è½½ã€‚å‡½æ•°é‡è½½ï¼šåœ¨åŒä¸€èŒƒå›´å†…å¯¹ç›¸åŒå‡½æ•°åæœ‰å¤šä¸ªå®šä¹‰ï¼Œå¯ä»¥æ˜¯å‚æ•°ç±»åž‹æˆ–å‚æ•°ä¸ªæ•°çš„ä¸åŒï¼Œä½†ä¸è®¸åªæœ‰è¿”å›žå€¼ç±»åž‹ä¸åŒã€‚è¿ç®—ç¬¦é‡è½½ï¼šå…³é”®å­— abstract å£°æ˜ŽæŠ½è±¡ç±»ï¼šç”¨äºŽæŽ¥å£éƒ¨åˆ†ç±»çš„å®žçŽ°ï¼ˆæ´¾ç”Ÿç±»ç»§æ‰¿æŠ½è±¡ç±»æ—¶ï¼Œå®žçŽ°å®Œæˆï¼‰ã€‚æŠ½è±¡ç±»åŒ…å«æŠ½è±¡æ–¹æ³•ï¼ŒæŠ½è±¡æ–¹æ³•å¯è¢«æ´¾ç”Ÿç±»å®žçŽ°ã€‚æŠ½è±¡ç±»è§„åˆ™ï¼š1.ä¸èƒ½åˆ›å»ºæŠ½è±¡ç±»çš„å®žä¾‹2.ä¸èƒ½åœ¨æŠ½è±¡ç±»å¤–å®šä¹‰æŠ½è±¡æ–¹æ³•3.ä¸èƒ½æŠŠæŠ½è±¡ç±»å£°æ˜Žä¸ºsealedï¼ˆç±»å‰å¸¦å…³é”®å­—sealedä»£è¡¨è¯¥ç±»æ˜¯å¯†å°ç±»ï¼Œä¸èƒ½è¢«ç»§æ‰¿ï¼‰å…³é”®å­—virtualå£°æ˜Žè™šæ–¹æ³•:ç”¨äºŽæ–¹æ³•åœ¨ç»§æ‰¿ç±»ä¸­çš„å®žçŽ°ï¼ˆåœ¨ä¸åŒçš„ç»§æ‰¿ç±»ä¸­æœ‰ä¸åŒçš„å®žçŽ°ï¼‰ã€‚æŠ½è±¡ç±»å’Œè™šæ–¹æ³•å…±åŒå®žçŽ°åŠ¨æ€å¤šæ€æ€§ã€‚æ³¨ï¼šç»§æ‰¿ç±»ä¸­çš„é‡å†™è™šå‡½æ•°éœ€è¦å£°æ˜Žå…³é”®å­— overrideï¼Œåœ¨æ–¹æ³•å‚æ•°ä¼ å…¥ä¸­å†™ï¼ˆç±»å å½¢å‚åï¼‰ä¾‹å¦‚ public void CallArea(Shape sh)ï¼Œæ„æ€æ˜¯ä¼ å…¥ä¸€ä¸ª shape ç±»åž‹çš„ç±»ã€‚è¿ç®—ç¬¦é‡è½½æ‚¨å¯ä»¥é‡å®šä¹‰æˆ–é‡è½½ C# ä¸­å†…ç½®çš„è¿ç®—ç¬¦ã€‚å› æ­¤ï¼Œç¨‹åºå‘˜ä¹Ÿå¯ä»¥ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰ç±»åž‹çš„è¿ç®—ç¬¦ã€‚é‡è½½è¿ç®—ç¬¦æ˜¯å…·æœ‰ç‰¹æ®Šåç§°çš„å‡½æ•°ï¼Œæ˜¯é€šè¿‡å…³é”®å­— operator åŽè·Ÿè¿ç®—ç¬¦çš„ç¬¦å·æ¥å®šä¹‰çš„ã€‚ä¸Žå…¶ä»–å‡½æ•°ä¸€æ ·ï¼Œé‡è½½è¿ç®—ç¬¦æœ‰è¿”å›žç±»åž‹å’Œå‚æ•°åˆ—è¡¨ã€‚ä¾‹å¦‚ï¼Œè¯·çœ‹ä¸‹é¢çš„å‡½æ•°ï¼š12345678public static Box operator+ (Box b, Box c)&#123; Box box = new Box(); box.length = b.length + c.length; box.breadth = b.breadth + c.breadth; box.height = b.height + c.height; return box;&#125;ä¸Šé¢çš„å‡½æ•°ä¸ºç”¨æˆ·è‡ªå®šä¹‰çš„ç±» Box å®žçŽ°äº†åŠ æ³•è¿ç®—ç¬¦ï¼ˆ+ï¼‰ã€‚å®ƒæŠŠä¸¤ä¸ª Box å¯¹è±¡çš„å±žæ€§ç›¸åŠ ï¼Œå¹¶è¿”å›žç›¸åŠ åŽçš„ Box å¯¹è±¡ã€‚è¿ç®—ç¬¦é‡è½½çš„å®žçŽ°ä¸‹é¢çš„ç¨‹åºæ¼”ç¤ºäº†å®Œæ•´çš„å®žçŽ°ï¼š1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677using System;namespace OperatorOvlApplication&#123; class Box &#123; private double length; // é•¿åº¦ private double breadth; // å®½åº¦ private double height; // é«˜åº¦ public double getVolume() &#123; return length * breadth * height; &#125; public void setLength( double len ) &#123; length = len; &#125; public void setBreadth( double bre ) &#123; breadth = bre; &#125; public void setHeight( double hei ) &#123; height = hei; &#125; // é‡è½½ + è¿ç®—ç¬¦æ¥æŠŠä¸¤ä¸ª Box å¯¹è±¡ç›¸åŠ  public static Box operator+ (Box b, Box c) &#123; Box box = new Box(); box.length = b.length + c.length; box.breadth = b.breadth + c.breadth; box.height = b.height + c.height; return box; &#125; &#125; class Tester &#123; static void Main(string[] args) &#123; Box Box1 = new Box(); // å£°æ˜Ž Box1ï¼Œç±»åž‹ä¸º Box Box Box2 = new Box(); // å£°æ˜Ž Box2ï¼Œç±»åž‹ä¸º Box Box Box3 = new Box(); // å£°æ˜Ž Box3ï¼Œç±»åž‹ä¸º Box double volume = 0.0; // ä½“ç§¯ // Box1 è¯¦è¿° Box1.setLength(6.0); Box1.setBreadth(7.0); Box1.setHeight(5.0); // Box2 è¯¦è¿° Box2.setLength(12.0); Box2.setBreadth(13.0); Box2.setHeight(10.0); // Box1 çš„ä½“ç§¯ volume = Box1.getVolume(); Console.WriteLine(&quot;Box1 çš„ä½“ç§¯ï¼š &#123;0&#125;&quot;, volume); // Box2 çš„ä½“ç§¯ volume = Box2.getVolume(); Console.WriteLine(&quot;Box2 çš„ä½“ç§¯ï¼š &#123;0&#125;&quot;, volume); // æŠŠä¸¤ä¸ªå¯¹è±¡ç›¸åŠ  Box3 = Box1 + Box2; // Box3 çš„ä½“ç§¯ volume = Box3.getVolume(); Console.WriteLine(&quot;Box3 çš„ä½“ç§¯ï¼š &#123;0&#125;&quot;, volume); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š123Box1 çš„ä½“ç§¯ï¼š 210Box2 çš„ä½“ç§¯ï¼š 1560Box3 çš„ä½“ç§¯ï¼š 5400å¯é‡è½½å’Œä¸å¯é‡è½½è¿ç®—ç¬¦ä¸‹è¡¨æè¿°äº† C# ä¸­è¿ç®—ç¬¦é‡è½½çš„èƒ½åŠ›ï¼šè¿ç®—ç¬¦æè¿°+, -, !, ~, ++, â€“è¿™äº›ä¸€å…ƒè¿ç®—ç¬¦åªæœ‰ä¸€ä¸ªæ“ä½œæ•°ï¼Œä¸”å¯ä»¥è¢«é‡è½½ã€‚+, -, *, /, %è¿™äº›äºŒå…ƒè¿ç®—ç¬¦å¸¦æœ‰ä¸¤ä¸ªæ“ä½œæ•°ï¼Œä¸”å¯ä»¥è¢«é‡è½½ã€‚==, !=, &lt;, &gt;, &lt;=, &gt;=è¿™äº›æ¯”è¾ƒè¿ç®—ç¬¦å¯ä»¥è¢«é‡è½½ã€‚&amp;&amp;, \\è¿™äº›æ¡ä»¶é€»è¾‘è¿ç®—ç¬¦ä¸èƒ½è¢«ç›´æŽ¥é‡è½½ã€‚+=, -=, *=, /=, %=è¿™äº›èµ‹å€¼è¿ç®—ç¬¦ä¸èƒ½è¢«é‡è½½ã€‚=, ., ?:, -&gt;, new, is, sizeof, typeofè¿™äº›è¿ç®—ç¬¦ä¸èƒ½è¢«é‡è½½ã€‚å®žä¾‹é’ˆå¯¹ä¸Šè¿°è®¨è®ºï¼Œè®©æˆ‘ä»¬æ‰©å±•ä¸Šé¢çš„å®žä¾‹ï¼Œé‡è½½æ›´å¤šçš„è¿ç®—ç¬¦ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178using System;namespace OperatorOvlApplication&#123; class Box &#123; private double length; // é•¿åº¦ private double breadth; // å®½åº¦ private double height; // é«˜åº¦ public double getVolume() &#123; return length * breadth * height; &#125; public void setLength( double len ) &#123; length = len; &#125; public void setBreadth( double bre ) &#123; breadth = bre; &#125; public void setHeight( double hei ) &#123; height = hei; &#125; // é‡è½½ + è¿ç®—ç¬¦æ¥æŠŠä¸¤ä¸ª Box å¯¹è±¡ç›¸åŠ  public static Box operator+ (Box b, Box c) &#123; Box box = new Box(); box.length = b.length + c.length; box.breadth = b.breadth + c.breadth; box.height = b.height + c.height; return box; &#125; public static bool operator == (Box lhs, Box rhs) &#123; bool status = false; if (lhs.length == rhs.length &amp;&amp; lhs.height == rhs.height &amp;&amp; lhs.breadth == rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator !=(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length != rhs.length || lhs.height != rhs.height || lhs.breadth != rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator &lt;(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length &lt; rhs.length &amp;&amp; lhs.height &lt; rhs.height &amp;&amp; lhs.breadth &lt; rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator &gt;(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length &gt; rhs.length &amp;&amp; lhs.height &gt; rhs.height &amp;&amp; lhs.breadth &gt; rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator &lt;=(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length &lt;= rhs.length &amp;&amp; lhs.height &lt;= rhs.height &amp;&amp; lhs.breadth &lt;= rhs.breadth) &#123; status = true; &#125; return status; &#125; public static bool operator &gt;=(Box lhs, Box rhs) &#123; bool status = false; if (lhs.length &gt;= rhs.length &amp;&amp; lhs.height &gt;= rhs.height &amp;&amp; lhs.breadth &gt;= rhs.breadth) &#123; status = true; &#125; return status; &#125; public override string ToString() &#123; return String.Format(&quot;(&#123;0&#125;, &#123;1&#125;, &#123;2&#125;)&quot;, length, breadth, height); &#125; &#125; class Tester &#123; static void Main(string[] args) &#123; Box Box1 = new Box(); // å£°æ˜Ž Box1ï¼Œç±»åž‹ä¸º Box Box Box2 = new Box(); // å£°æ˜Ž Box2ï¼Œç±»åž‹ä¸º Box Box Box3 = new Box(); // å£°æ˜Ž Box3ï¼Œç±»åž‹ä¸º Box Box Box4 = new Box(); double volume = 0.0; // ä½“ç§¯ // Box1 è¯¦è¿° Box1.setLength(6.0); Box1.setBreadth(7.0); Box1.setHeight(5.0); // Box2 è¯¦è¿° Box2.setLength(12.0); Box2.setBreadth(13.0); Box2.setHeight(10.0); // ä½¿ç”¨é‡è½½çš„ ToString() æ˜¾ç¤ºä¸¤ä¸ªç›’å­ Console.WriteLine(&quot;Box1ï¼š &#123;0&#125;&quot;, Box1.ToString()); Console.WriteLine(&quot;Box2ï¼š &#123;0&#125;&quot;, Box2.ToString()); // Box1 çš„ä½“ç§¯ volume = Box1.getVolume(); Console.WriteLine(&quot;Box1 çš„ä½“ç§¯ï¼š &#123;0&#125;&quot;, volume); // Box2 çš„ä½“ç§¯ volume = Box2.getVolume(); Console.WriteLine(&quot;Box2 çš„ä½“ç§¯ï¼š &#123;0&#125;&quot;, volume); // æŠŠä¸¤ä¸ªå¯¹è±¡ç›¸åŠ  Box3 = Box1 + Box2; Console.WriteLine(&quot;Box3ï¼š &#123;0&#125;&quot;, Box3.ToString()); // Box3 çš„ä½“ç§¯ volume = Box3.getVolume(); Console.WriteLine(&quot;Box3 çš„ä½“ç§¯ï¼š &#123;0&#125;&quot;, volume); //comparing the boxes if (Box1 &gt; Box2) Console.WriteLine(&quot;Box1 å¤§äºŽ Box2&quot;); else Console.WriteLine(&quot;Box1 ä¸å¤§äºŽ Box2&quot;); if (Box1 &lt; Box2) Console.WriteLine(&quot;Box1 å°äºŽ Box2&quot;); else Console.WriteLine(&quot;Box1 ä¸å°äºŽ Box2&quot;); if (Box1 &gt;= Box2) Console.WriteLine(&quot;Box1 å¤§äºŽç­‰äºŽ Box2&quot;); else Console.WriteLine(&quot;Box1 ä¸å¤§äºŽç­‰äºŽ Box2&quot;); if (Box1 &lt;= Box2) Console.WriteLine(&quot;Box1 å°äºŽç­‰äºŽ Box2&quot;); else Console.WriteLine(&quot;Box1 ä¸å°äºŽç­‰äºŽ Box2&quot;); if (Box1 != Box2) Console.WriteLine(&quot;Box1 ä¸ç­‰äºŽ Box2&quot;); else Console.WriteLine(&quot;Box1 ç­‰äºŽ Box2&quot;); Box4 = Box3; if (Box3 == Box4) Console.WriteLine(&quot;Box3 ç­‰äºŽ Box4&quot;); else Console.WriteLine(&quot;Box3 ä¸ç­‰äºŽ Box4&quot;); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š123456789101112Box1ï¼š (6, 7, 5)Box2ï¼š (12, 13, 10)Box1 çš„ä½“ç§¯ï¼š 210Box2 çš„ä½“ç§¯ï¼š 1560Box3ï¼š (18, 20, 15)Box3 çš„ä½“ç§¯ï¼š 5400Box1 ä¸å¤§äºŽ Box2Box1 å°äºŽ Box2Box1 ä¸å¤§äºŽç­‰äºŽ Box2Box1 å°äºŽç­‰äºŽ Box2Box1 ä¸ç­‰äºŽ Box2Box3 ç­‰äºŽ Box4FileStream12345678910111213141516171819202122232425262728using System;using System.IO;namespace FileIOApplication&#123; class Program &#123; static void Main(string[] args) &#123; FileStream F = new FileStream(&quot;test.dat&quot;, FileMode.OpenOrCreate, FileAccess.ReadWrite); for (int i = 1; i &lt;= 20; i++) &#123; F.WriteByte((byte)i); &#125; F.Position = 0; for (int i = 0; i &lt;= 20; i++) &#123; Console.Write(F.ReadByte() + &quot; &quot;); &#125; F.Close(); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š11 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 -1StreamReader å’Œ StreamWriterStreamReader ç±»ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†è¯»å–åä¸º Jamaica.txt çš„æ–‡ä»¶ã€‚æ–‡ä»¶å¦‚ä¸‹ï¼š123456789101112131415161718192021222324252627282930313233343536373839Down the way where the nights are gayAnd the sun shines daily on the mountain topI took a trip on a sailing shipAnd when I reached JamaicaI made a stopusing System;using System.IO;namespace FileApplication&#123; class Program &#123; static void Main(string[] args) &#123; try &#123; // åˆ›å»ºä¸€ä¸ª StreamReader çš„å®žä¾‹æ¥è¯»å–æ–‡ä»¶ // using è¯­å¥ä¹Ÿèƒ½å…³é—­ StreamReader using (StreamReader sr = new StreamReader(&quot;c:/jamaica.txt&quot;)) &#123; string line; // ä»Žæ–‡ä»¶è¯»å–å¹¶æ˜¾ç¤ºè¡Œï¼Œç›´åˆ°æ–‡ä»¶çš„æœ«å°¾ while ((line = sr.ReadLine()) != null) &#123; Console.WriteLine(line); &#125; &#125; &#125; catch (Exception e) &#123; // å‘ç”¨æˆ·æ˜¾ç¤ºå‡ºé”™æ¶ˆæ¯ Console.WriteLine(&quot;The file could not be read:&quot;); Console.WriteLine(e.Message); &#125; Console.ReadKey(); &#125; &#125;&#125;å½“æ‚¨ç¼–è¯‘å’Œæ‰§è¡Œä¸Šé¢çš„ç¨‹åºæ—¶ï¼Œå®ƒä¼šæ˜¾ç¤ºæ–‡ä»¶çš„å†…å®¹ã€‚StreamWriter ç±»ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†ä½¿ç”¨ StreamWriter ç±»å‘æ–‡ä»¶å†™å…¥æ–‡æœ¬æ•°æ®ï¼š123456789101112131415161718192021222324252627282930313233using System;using System.IO;namespace FileApplication&#123; class Program &#123; static void Main(string[] args) &#123; string[] names = new string[] &#123;&quot;Zara Ali&quot;, &quot;Nuha Ali&quot;&#125;; using (StreamWriter sw = new StreamWriter(&quot;names.txt&quot;)) &#123; foreach (string s in names) &#123; sw.WriteLine(s); &#125; &#125; // ä»Žæ–‡ä»¶ä¸­è¯»å–å¹¶æ˜¾ç¤ºæ¯è¡Œ string line = &quot;&quot;; using (StreamReader sr = new StreamReader(&quot;names.txt&quot;)) &#123; while ((line = sr.ReadLine()) != null) &#123; Console.WriteLine(line); &#125; &#125; Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12Zara AliNuha AliBinaryReader å’Œ BinaryWriterä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†è¯»å–å’Œå†™å…¥äºŒè¿›åˆ¶æ•°æ®ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273using System;using System.IO;namespace BinaryFileApplication&#123; class Program &#123; static void Main(string[] args) &#123; BinaryWriter bw; BinaryReader br; int i = 25; double d = 3.14157; bool b = true; string s = &quot;I am happy&quot;; // åˆ›å»ºæ–‡ä»¶ try &#123; bw = new BinaryWriter(new FileStream(&quot;mydata&quot;, FileMode.Create)); &#125; catch (IOException e) &#123; Console.WriteLine(e.Message + &quot;\n Cannot create file.&quot;); return; &#125; // å†™å…¥æ–‡ä»¶ try &#123; bw.Write(i); bw.Write(d); bw.Write(b); bw.Write(s); &#125; catch (IOException e) &#123; Console.WriteLine(e.Message + &quot;\n Cannot write to file.&quot;); return; &#125; bw.Close(); // è¯»å–æ–‡ä»¶ try &#123; br = new BinaryReader(new FileStream(&quot;mydata&quot;, FileMode.Open)); &#125; catch (IOException e) &#123; Console.WriteLine(e.Message + &quot;\n Cannot open file.&quot;); return; &#125; try &#123; i = br.ReadInt32(); Console.WriteLine(&quot;Integer data: &#123;0&#125;&quot;, i); d = br.ReadDouble(); Console.WriteLine(&quot;Double data: &#123;0&#125;&quot;, d); b = br.ReadBoolean(); Console.WriteLine(&quot;Boolean data: &#123;0&#125;&quot;, b); s = br.ReadString(); Console.WriteLine(&quot;String data: &#123;0&#125;&quot;, s); &#125; catch (IOException e) &#123; Console.WriteLine(e.Message + &quot;\n Cannot read from file.&quot;); return; &#125; br.Close(); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š1234Integer data: 25Double data: 3.14157Boolean data: TrueString data: I am happyå±žæ€§å±žæ€§ï¼ˆPropertyï¼‰ æ˜¯ç±»ï¼ˆclassï¼‰ã€ç»“æž„ï¼ˆstructureï¼‰å’ŒæŽ¥å£ï¼ˆinterfaceï¼‰çš„å‘½åï¼ˆnamedï¼‰æˆå‘˜ã€‚ç±»æˆ–ç»“æž„ä¸­çš„æˆå‘˜å˜é‡æˆ–æ–¹æ³•ç§°ä¸º åŸŸï¼ˆFieldï¼‰ã€‚å±žæ€§ï¼ˆPropertyï¼‰æ˜¯åŸŸï¼ˆFieldï¼‰çš„æ‰©å±•ï¼Œä¸”å¯ä½¿ç”¨ç›¸åŒçš„è¯­æ³•æ¥è®¿é—®ã€‚å®ƒä»¬ä½¿ç”¨ è®¿é—®å™¨ï¼ˆaccessorsï¼‰ è®©ç§æœ‰åŸŸçš„å€¼å¯è¢«è¯»å†™æˆ–æ“ä½œã€‚å±žæ€§ï¼ˆPropertyï¼‰ä¸ä¼šç¡®å®šå­˜å‚¨ä½ç½®ã€‚ç›¸åï¼Œå®ƒä»¬å…·æœ‰å¯è¯»å†™æˆ–è®¡ç®—å®ƒä»¬å€¼çš„ è®¿é—®å™¨ï¼ˆaccessorsï¼‰ã€‚ä¾‹å¦‚ï¼Œæœ‰ä¸€ä¸ªåä¸º Student çš„ç±»ï¼Œå¸¦æœ‰ ageã€name å’Œ code çš„ç§æœ‰åŸŸã€‚æˆ‘ä»¬ä¸èƒ½åœ¨ç±»çš„èŒƒå›´ä»¥å¤–ç›´æŽ¥è®¿é—®è¿™äº›åŸŸï¼Œä½†æ˜¯æˆ‘ä»¬å¯ä»¥æ‹¥æœ‰è®¿é—®è¿™äº›ç§æœ‰åŸŸçš„å±žæ€§ã€‚è®¿é—®å™¨ï¼ˆAccessorsï¼‰å±žæ€§ï¼ˆPropertyï¼‰çš„è®¿é—®å™¨ï¼ˆaccessorï¼‰åŒ…å«æœ‰åŠ©äºŽèŽ·å–ï¼ˆè¯»å–æˆ–è®¡ç®—ï¼‰æˆ–è®¾ç½®ï¼ˆå†™å…¥ï¼‰å±žæ€§çš„å¯æ‰§è¡Œè¯­å¥ã€‚è®¿é—®å™¨ï¼ˆaccessorï¼‰å£°æ˜Žå¯åŒ…å«ä¸€ä¸ª get è®¿é—®å™¨ã€ä¸€ä¸ª set è®¿é—®å™¨ï¼Œæˆ–è€…åŒæ—¶åŒ…å«äºŒè€…ã€‚ä¾‹å¦‚ï¼š1234567891011121314151617181920212223242526272829303132333435363738// å£°æ˜Žç±»åž‹ä¸º string çš„ Code å±žæ€§public string Code&#123; get &#123; return code; &#125; set &#123; code = value; &#125;&#125;// å£°æ˜Žç±»åž‹ä¸º string çš„ Name å±žæ€§public string Name&#123; get &#123; return name; &#125; set &#123; name = value; &#125;&#125;// å£°æ˜Žç±»åž‹ä¸º int çš„ Age å±žæ€§public int Age&#123; get &#123; return age; &#125; set &#123; age = value; &#125;&#125;å®žä¾‹ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†å±žæ€§ï¼ˆPropertyï¼‰çš„ç”¨æ³•ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172using System;namespace tutorialspoint&#123; class Student &#123; private string code = &quot;N.A&quot;; private string name = &quot;not known&quot;; private int age = 0; // å£°æ˜Žç±»åž‹ä¸º string çš„ Code å±žæ€§ public string Code &#123; get &#123; return code; &#125; set &#123; code = value; &#125; &#125; // å£°æ˜Žç±»åž‹ä¸º string çš„ Name å±žæ€§ public string Name &#123; get &#123; return name; &#125; set &#123; name = value; &#125; &#125; // å£°æ˜Žç±»åž‹ä¸º int çš„ Age å±žæ€§ public int Age &#123; get &#123; return age; &#125; set &#123; age = value; &#125; &#125; public override string ToString() &#123; return &quot;Code = &quot; + Code +&quot;, Name = &quot; + Name + &quot;, Age = &quot; + Age; &#125; &#125; class ExampleDemo &#123; public static void Main() &#123; // åˆ›å»ºä¸€ä¸ªæ–°çš„ Student å¯¹è±¡ Student s = new Student(); // è®¾ç½® student çš„ codeã€name å’Œ age s.Code = &quot;001&quot;; s.Name = &quot;Zara&quot;; s.Age = 9; Console.WriteLine(&quot;Student Info: &#123;0&#125;&quot;, s); // å¢žåŠ å¹´é¾„ s.Age += 1; Console.WriteLine(&quot;Student Info: &#123;0&#125;&quot;, s); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12Student Info: Code = 001, Name = Zara, Age = 9Student Info: Code = 001, Name = Zara, Age = 10æŠ½è±¡å±žæ€§ï¼ˆAbstract Propertiesï¼‰æŠ½è±¡ç±»å¯æ‹¥æœ‰æŠ½è±¡å±žæ€§ï¼Œè¿™äº›å±žæ€§åº”åœ¨æ´¾ç”Ÿç±»ä¸­è¢«å®žçŽ°ã€‚ä¸‹é¢çš„ç¨‹åºè¯´æ˜Žäº†è¿™ç‚¹ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485using System;namespace tutorialspoint&#123; public abstract class Person &#123; public abstract string Name &#123; get; set; &#125; public abstract int Age &#123; get; set; &#125; &#125; class Student : Person &#123; private string code = &quot;N.A&quot;; private string name = &quot;N.A&quot;; private int age = 0; // å£°æ˜Žç±»åž‹ä¸º string çš„ Code å±žæ€§ public string Code &#123; get &#123; return code; &#125; set &#123; code = value; &#125; &#125; // å£°æ˜Žç±»åž‹ä¸º string çš„ Name å±žæ€§ public override string Name &#123; get &#123; return name; &#125; set &#123; name = value; &#125; &#125; // å£°æ˜Žç±»åž‹ä¸º int çš„ Age å±žæ€§ public override int Age &#123; get &#123; return age; &#125; set &#123; age = value; &#125; &#125; public override string ToString() &#123; return &quot;Code = &quot; + Code +&quot;, Name = &quot; + Name + &quot;, Age = &quot; + Age; &#125; &#125; class ExampleDemo &#123; public static void Main() &#123; // åˆ›å»ºä¸€ä¸ªæ–°çš„ Student å¯¹è±¡ Student s = new Student(); // è®¾ç½® student çš„ codeã€name å’Œ age s.Code = &quot;001&quot;; s.Name = &quot;Zara&quot;; s.Age = 9; Console.WriteLine(&quot;Student Info:- &#123;0&#125;&quot;, s); // å¢žåŠ å¹´é¾„ s.Age += 1; Console.WriteLine(&quot;Student Info:- &#123;0&#125;&quot;, s); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12Student Info: Code = 001, Name = Zara, Age = 9Student Info: Code = 001, Name = Zara, Age = 10ç´¢å¼•å™¨ç´¢å¼•å™¨ï¼ˆIndexerï¼‰ å…è®¸ä¸€ä¸ªå¯¹è±¡å¯ä»¥åƒæ•°ç»„ä¸€æ ·è¢«ç´¢å¼•ã€‚å½“æ‚¨ä¸ºç±»å®šä¹‰ä¸€ä¸ªç´¢å¼•å™¨æ—¶ï¼Œè¯¥ç±»çš„è¡Œä¸ºå°±ä¼šåƒä¸€ä¸ª è™šæ‹Ÿæ•°ç»„ï¼ˆvirtual arrayï¼‰ ä¸€æ ·ã€‚æ‚¨å¯ä»¥ä½¿ç”¨æ•°ç»„è®¿é—®è¿ç®—ç¬¦ï¼ˆ[ ]ï¼‰æ¥è®¿é—®è¯¥ç±»çš„å®žä¾‹ã€‚è¯­æ³•ä¸€ç»´ç´¢å¼•å™¨çš„è¯­æ³•å¦‚ä¸‹ï¼š1234567891011121314element-type this[int index] &#123; // get è®¿é—®å™¨ get &#123; // è¿”å›ž index æŒ‡å®šçš„å€¼ &#125; // set è®¿é—®å™¨ set &#123; // è®¾ç½® index æŒ‡å®šçš„å€¼ &#125;&#125;ç´¢å¼•å™¨ï¼ˆIndexerï¼‰çš„ç”¨é€”ç´¢å¼•å™¨çš„è¡Œä¸ºçš„å£°æ˜Žåœ¨æŸç§ç¨‹åº¦ä¸Šç±»ä¼¼äºŽå±žæ€§ï¼ˆpropertyï¼‰ã€‚å°±åƒå±žæ€§ï¼ˆpropertyï¼‰ï¼Œæ‚¨å¯ä½¿ç”¨ get å’Œ set è®¿é—®å™¨æ¥å®šä¹‰ç´¢å¼•å™¨ã€‚ä½†æ˜¯ï¼Œå±žæ€§è¿”å›žæˆ–è®¾ç½®ä¸€ä¸ªç‰¹å®šçš„æ•°æ®æˆå‘˜ï¼Œè€Œç´¢å¼•å™¨è¿”å›žæˆ–è®¾ç½®å¯¹è±¡å®žä¾‹çš„ä¸€ä¸ªç‰¹å®šå€¼ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒæŠŠå®žä¾‹æ•°æ®åˆ†ä¸ºæ›´å°çš„éƒ¨åˆ†ï¼Œå¹¶ç´¢å¼•æ¯ä¸ªéƒ¨åˆ†ï¼ŒèŽ·å–æˆ–è®¾ç½®æ¯ä¸ªéƒ¨åˆ†ã€‚å®šä¹‰ä¸€ä¸ªå±žæ€§ï¼ˆpropertyï¼‰åŒ…æ‹¬æä¾›å±žæ€§åç§°ã€‚ç´¢å¼•å™¨å®šä¹‰çš„æ—¶å€™ä¸å¸¦æœ‰åç§°ï¼Œä½†å¸¦æœ‰ this å…³é”®å­—ï¼Œå®ƒæŒ‡å‘å¯¹è±¡å®žä¾‹ã€‚ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†è¿™ä¸ªæ¦‚å¿µï¼š1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556using System;namespace IndexerApplication&#123; class IndexedNames &#123; private string[] namelist = new string[size]; static public int size = 10; public IndexedNames() &#123; for (int i = 0; i &lt; size; i++) namelist[i] = &quot;N. A.&quot;; &#125; public string this[int index] &#123; get &#123; string tmp; if( index &gt;= 0 &amp;&amp; index &lt;= size-1 ) &#123; tmp = namelist[index]; &#125; else &#123; tmp = &quot;&quot;; &#125; return ( tmp ); &#125; set &#123; if( index &gt;= 0 &amp;&amp; index &lt;= size-1 ) &#123; namelist[index] = value; &#125; &#125; &#125; static void Main(string[] args) &#123; IndexedNames names = new IndexedNames(); names[0] = &quot;Zara&quot;; names[1] = &quot;Riz&quot;; names[2] = &quot;Nuha&quot;; names[3] = &quot;Asif&quot;; names[4] = &quot;Davinder&quot;; names[5] = &quot;Sunil&quot;; names[6] = &quot;Rubic&quot;; for ( int i = 0; i &lt; IndexedNames.size; i++ ) &#123; Console.WriteLine(names[i]); &#125; Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12345678910ZaraRizNuhaAsifDavinderSunilRubicN. A.N. A.N. A.é‡è½½ç´¢å¼•å™¨ï¼ˆIndexerï¼‰ç´¢å¼•å™¨ï¼ˆIndexerï¼‰å¯è¢«é‡è½½ã€‚ç´¢å¼•å™¨å£°æ˜Žçš„æ—¶å€™ä¹Ÿå¯å¸¦æœ‰å¤šä¸ªå‚æ•°ï¼Œä¸”æ¯ä¸ªå‚æ•°å¯ä»¥æ˜¯ä¸åŒçš„ç±»åž‹ã€‚æ²¡æœ‰å¿…è¦è®©ç´¢å¼•å™¨å¿…é¡»æ˜¯æ•´åž‹çš„ã€‚C# å…è®¸ç´¢å¼•å™¨å¯ä»¥æ˜¯å…¶ä»–ç±»åž‹ï¼Œä¾‹å¦‚ï¼Œå­—ç¬¦ä¸²ç±»åž‹ã€‚ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†é‡è½½ç´¢å¼•å™¨ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778using System;namespace IndexerApplication&#123; class IndexedNames &#123; private string[] namelist = new string[size]; static public int size = 10; public IndexedNames() &#123; for (int i = 0; i &lt; size; i++) &#123; namelist[i] = &quot;N. A.&quot;; &#125; &#125; public string this[int index] &#123; get &#123; string tmp; if( index &gt;= 0 &amp;&amp; index &lt;= size-1 ) &#123; tmp = namelist[index]; &#125; else &#123; tmp = &quot;&quot;; &#125; return ( tmp ); &#125; set &#123; if( index &gt;= 0 &amp;&amp; index &lt;= size-1 ) &#123; namelist[index] = value; &#125; &#125; &#125; public int this[string name] &#123; get &#123; int index = 0; while(index &lt; size) &#123; if (namelist[index] == name) &#123; return index; &#125; index++; &#125; return index; &#125; &#125; static void Main(string[] args) &#123; IndexedNames names = new IndexedNames(); names[0] = &quot;Zara&quot;; names[1] = &quot;Riz&quot;; names[2] = &quot;Nuha&quot;; names[3] = &quot;Asif&quot;; names[4] = &quot;Davinder&quot;; names[5] = &quot;Sunil&quot;; names[6] = &quot;Rubic&quot;; // ä½¿ç”¨å¸¦æœ‰ int å‚æ•°çš„ç¬¬ä¸€ä¸ªç´¢å¼•å™¨ for (int i = 0; i &lt; IndexedNames.size; i++) &#123; Console.WriteLine(names[i]); &#125; // ä½¿ç”¨å¸¦æœ‰ string å‚æ•°çš„ç¬¬äºŒä¸ªç´¢å¼•å™¨ Console.WriteLine(names[&quot;Nuha&quot;]); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š1234567891011ZaraRizNuhaAsifDavinderSunilRubicN. A.N. A.N. A.2å§”æ‰˜C# ä¸­çš„å§”æ‰˜ï¼ˆDelegateï¼‰ç±»ä¼¼äºŽ C æˆ– C++ ä¸­å‡½æ•°çš„æŒ‡é’ˆã€‚å§”æ‰˜ï¼ˆDelegateï¼‰ æ˜¯å­˜æœ‰å¯¹æŸä¸ªæ–¹æ³•çš„å¼•ç”¨çš„ä¸€ç§å¼•ç”¨ç±»åž‹å˜é‡ã€‚å¼•ç”¨å¯åœ¨è¿è¡Œæ—¶è¢«æ”¹å˜ã€‚å§”æ‰˜ï¼ˆDelegateï¼‰ç‰¹åˆ«ç”¨äºŽå®žçŽ°äº‹ä»¶å’Œå›žè°ƒæ–¹æ³•ã€‚æ‰€æœ‰çš„å§”æ‰˜ï¼ˆDelegateï¼‰éƒ½æ´¾ç”Ÿè‡ª System.Delegate ç±»ã€‚å£°æ˜Žå§”æ‰˜ï¼ˆDelegateï¼‰å§”æ‰˜å£°æ˜Žå†³å®šäº†å¯ç”±è¯¥å§”æ‰˜å¼•ç”¨çš„æ–¹æ³•ã€‚å§”æ‰˜å¯æŒ‡å‘ä¸€ä¸ªä¸Žå…¶å…·æœ‰ç›¸åŒæ ‡ç­¾çš„æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æœ‰ä¸€ä¸ªå§”æ‰˜ï¼š1public delegate int MyDelegate (string s);ä¸Šé¢çš„å§”æ‰˜å¯è¢«ç”¨äºŽå¼•ç”¨ä»»ä½•ä¸€ä¸ªå¸¦æœ‰ä¸€ä¸ªå•ä¸€çš„ string å‚æ•°çš„æ–¹æ³•ï¼Œå¹¶è¿”å›žä¸€ä¸ª int ç±»åž‹å˜é‡ã€‚å£°æ˜Žå§”æ‰˜çš„è¯­æ³•å¦‚ä¸‹ï¼š1delegate &lt;return type&gt; &lt;delegate-name&gt; &lt;parameter list&gt;å®žä¾‹åŒ–å§”æ‰˜ï¼ˆDelegateï¼‰ä¸€æ—¦å£°æ˜Žäº†å§”æ‰˜ç±»åž‹ï¼Œå§”æ‰˜å¯¹è±¡å¿…é¡»ä½¿ç”¨ new å…³é”®å­—æ¥åˆ›å»ºï¼Œä¸”ä¸Žä¸€ä¸ªç‰¹å®šçš„æ–¹æ³•æœ‰å…³ã€‚å½“åˆ›å»ºå§”æ‰˜æ—¶ï¼Œä¼ é€’åˆ° new è¯­å¥çš„å‚æ•°å°±åƒæ–¹æ³•è°ƒç”¨ä¸€æ ·ä¹¦å†™ï¼Œä½†æ˜¯ä¸å¸¦æœ‰å‚æ•°ã€‚ä¾‹å¦‚ï¼š1234public delegate void printString(string s);...printString ps1 = new printString(WriteToScreen);printString ps2 = new printString(WriteToFile);ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†å§”æ‰˜çš„å£°æ˜Žã€å®žä¾‹åŒ–å’Œä½¿ç”¨ï¼Œè¯¥å§”æ‰˜å¯ç”¨äºŽå¼•ç”¨å¸¦æœ‰ä¸€ä¸ªæ•´åž‹å‚æ•°çš„æ–¹æ³•ï¼Œå¹¶è¿”å›žä¸€ä¸ªæ•´åž‹å€¼ã€‚1234567891011121314151617181920212223242526272829303132333435363738using System;delegate int NumberChanger(int n);namespace DelegateAppl&#123; class TestDelegate &#123; static int num = 10; public static int AddNum(int p) &#123; num += p; return num; &#125; public static int MultNum(int q) &#123; num *= q; return num; &#125; public static int getNum() &#123; return num; &#125; static void Main(string[] args) &#123; // åˆ›å»ºå§”æ‰˜å®žä¾‹ NumberChanger nc1 = new NumberChanger(AddNum); NumberChanger nc2 = new NumberChanger(MultNum); // ä½¿ç”¨å§”æ‰˜å¯¹è±¡è°ƒç”¨æ–¹æ³• nc1(25); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); nc2(5); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12Value of Num: 35Value of Num: 175å§”æ‰˜çš„å¤šæ’­ï¼ˆMulticasting of a Delegateï¼‰å§”æ‰˜å¯¹è±¡å¯ä½¿ç”¨ â€œ+â€ è¿ç®—ç¬¦è¿›è¡Œåˆå¹¶ã€‚ä¸€ä¸ªåˆå¹¶å§”æ‰˜è°ƒç”¨å®ƒæ‰€åˆå¹¶çš„ä¸¤ä¸ªå§”æ‰˜ã€‚åªæœ‰ç›¸åŒç±»åž‹çš„å§”æ‰˜å¯è¢«åˆå¹¶ã€‚â€-â€œ è¿ç®—ç¬¦å¯ç”¨äºŽä»Žåˆå¹¶çš„å§”æ‰˜ä¸­ç§»é™¤ç»„ä»¶å§”æ‰˜ã€‚ä½¿ç”¨å§”æ‰˜çš„è¿™ä¸ªæœ‰ç”¨çš„ç‰¹ç‚¹ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªå§”æ‰˜è¢«è°ƒç”¨æ—¶è¦è°ƒç”¨çš„æ–¹æ³•çš„è°ƒç”¨åˆ—è¡¨ã€‚è¿™è¢«ç§°ä¸ºå§”æ‰˜çš„ å¤šæ’­ï¼ˆmulticastingï¼‰ï¼Œä¹Ÿå«ç»„æ’­ã€‚ä¸‹é¢çš„ç¨‹åºæ¼”ç¤ºäº†å§”æ‰˜çš„å¤šæ’­ï¼š123456789101112131415161718192021222324252627282930313233343536373839using System;delegate int NumberChanger(int n);namespace DelegateAppl&#123; class TestDelegate &#123; static int num = 10; public static int AddNum(int p) &#123; num += p; return num; &#125; public static int MultNum(int q) &#123; num *= q; return num; &#125; public static int getNum() &#123; return num; &#125; static void Main(string[] args) &#123; // åˆ›å»ºå§”æ‰˜å®žä¾‹ NumberChanger nc; NumberChanger nc1 = new NumberChanger(AddNum); NumberChanger nc2 = new NumberChanger(MultNum); nc = nc1; nc += nc2; // è°ƒç”¨å¤šæ’­ nc(5); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š1Value of Num: 75å§”æ‰˜ï¼ˆDelegateï¼‰çš„ç”¨é€”å§”æ‰˜å¤šæ’­å®žä¾‹ï¼šä¾‹å¦‚å°æ˜Žå«å°å¼ ä¹°å®Œè½¦ç¥¨ï¼Œä¹‹åŽæŽ¥ç€åˆè®©ä»–å¸¦å¼ ç”µå½±ç¥¨ï¼š1234567891011121314151617181920212223242526272829303132// å°å¼ ç±»public class MrZhang &#123; // å…¶å®žä¹°è½¦ç¥¨çš„æ‚²æƒ…äººç‰©æ˜¯å°å¼  public static void BuyTicket() &#123; Console.WriteLine(&quot;NND,æ¯æ¬¡éƒ½è®©æˆ‘åŽ»ä¹°ç¥¨ï¼Œé¸¡äººå‘€ï¼&quot;); &#125; public static void BuyMovieTicket() &#123; Console.WriteLine(&quot;æˆ‘åŽ»ï¼Œè‡ªå·±æ³¡å¦žï¼Œè¿˜è¦è®©æˆ‘å¸¦ç”µå½±ç¥¨ï¼&quot;); &#125;&#125;//å°æ˜Žç±»class MrMing&#123; // å£°æ˜Žä¸€ä¸ªå§”æ‰˜ï¼Œå…¶å®žå°±æ˜¯ä¸ªâ€œå‘½ä»¤â€ public delegate void BugTicketEventHandler(); public static void Main(string[] args) &#123; // è¿™é‡Œå°±æ˜¯å…·ä½“é˜è¿°è¿™ä¸ªå‘½ä»¤æ˜¯å¹²ä»€ä¹ˆçš„ï¼Œæœ¬ä¾‹æ˜¯MrZhang.BuyTicketâ€œå°å¼ ä¹°è½¦ç¥¨â€ BugTicketEventHandler myDelegate = new BugTicketEventHandler(MrZhang.BuyTicket); myDelegate += MrZhang.BuyMovieTicket; // è¿™æ—¶å€™å§”æ‰˜è¢«é™„ä¸Šäº†å…·ä½“çš„æ–¹æ³• myDelegate(); Console.ReadKey(); &#125;&#125;äº‹ä»¶äº‹ä»¶ï¼ˆEventï¼‰ åŸºæœ¬ä¸Šè¯´æ˜¯ä¸€ä¸ªç”¨æˆ·æ“ä½œï¼Œå¦‚æŒ‰é”®ã€ç‚¹å‡»ã€é¼ æ ‡ç§»åŠ¨ç­‰ç­‰ï¼Œæˆ–è€…æ˜¯ä¸€äº›å‡ºçŽ°ï¼Œå¦‚ç³»ç»Ÿç”Ÿæˆçš„é€šçŸ¥ã€‚åº”ç”¨ç¨‹åºéœ€è¦åœ¨äº‹ä»¶å‘ç”Ÿæ—¶å“åº”äº‹ä»¶ã€‚ä¾‹å¦‚ï¼Œä¸­æ–­ã€‚äº‹ä»¶æ˜¯ç”¨äºŽè¿›ç¨‹é—´é€šä¿¡ã€‚é€šè¿‡äº‹ä»¶ä½¿ç”¨å§”æ‰˜äº‹ä»¶åœ¨ç±»ä¸­å£°æ˜Žä¸”ç”Ÿæˆï¼Œä¸”é€šè¿‡ä½¿ç”¨åŒä¸€ä¸ªç±»æˆ–å…¶ä»–ç±»ä¸­çš„å§”æ‰˜ä¸Žäº‹ä»¶å¤„ç†ç¨‹åºå…³è”ã€‚åŒ…å«äº‹ä»¶çš„ç±»ç”¨äºŽå‘å¸ƒäº‹ä»¶ã€‚è¿™è¢«ç§°ä¸º å‘å¸ƒå™¨ï¼ˆpublisherï¼‰ ç±»ã€‚å…¶ä»–æŽ¥å—è¯¥äº‹ä»¶çš„ç±»è¢«ç§°ä¸º è®¢é˜…å™¨ï¼ˆsubscriberï¼‰ ç±»ã€‚äº‹ä»¶ä½¿ç”¨ å‘å¸ƒ-è®¢é˜…ï¼ˆpublisher-subscriberï¼‰ æ¨¡åž‹ã€‚å‘å¸ƒå™¨ï¼ˆpublisherï¼‰ æ˜¯ä¸€ä¸ªåŒ…å«äº‹ä»¶å’Œå§”æ‰˜å®šä¹‰çš„å¯¹è±¡ã€‚äº‹ä»¶å’Œå§”æ‰˜ä¹‹é—´çš„è”ç³»ä¹Ÿå®šä¹‰åœ¨è¿™ä¸ªå¯¹è±¡ä¸­ã€‚å‘å¸ƒå™¨ï¼ˆpublisherï¼‰ç±»çš„å¯¹è±¡è°ƒç”¨è¿™ä¸ªäº‹ä»¶ï¼Œå¹¶é€šçŸ¥å…¶ä»–çš„å¯¹è±¡ã€‚è®¢é˜…å™¨ï¼ˆsubscriberï¼‰ æ˜¯ä¸€ä¸ªæŽ¥å—äº‹ä»¶å¹¶æä¾›äº‹ä»¶å¤„ç†ç¨‹åºçš„å¯¹è±¡ã€‚åœ¨å‘å¸ƒå™¨ï¼ˆpublisherï¼‰ç±»ä¸­çš„å§”æ‰˜è°ƒç”¨è®¢é˜…å™¨ï¼ˆsubscriberï¼‰ç±»ä¸­çš„æ–¹æ³•ï¼ˆäº‹ä»¶å¤„ç†ç¨‹åºï¼‰ã€‚å£°æ˜Žäº‹ä»¶ï¼ˆEventï¼‰åœ¨ç±»çš„å†…éƒ¨å£°æ˜Žäº‹ä»¶ï¼Œé¦–å…ˆå¿…é¡»å£°æ˜Žè¯¥äº‹ä»¶çš„å§”æ‰˜ç±»åž‹ã€‚ä¾‹å¦‚ï¼š1public delegate void BoilerLogHandler(string status);ç„¶åŽï¼Œå£°æ˜Žäº‹ä»¶æœ¬èº«ï¼Œä½¿ç”¨ event å…³é”®å­—ï¼š12// åŸºäºŽä¸Šé¢çš„å§”æ‰˜å®šä¹‰äº‹ä»¶public event BoilerLogHandler BoilerEventLog;ä¸Šé¢çš„ä»£ç å®šä¹‰äº†ä¸€ä¸ªåä¸º BoilerLogHandler çš„å§”æ‰˜å’Œä¸€ä¸ªåä¸º BoilerEventLog çš„äº‹ä»¶ï¼Œè¯¥äº‹ä»¶åœ¨ç”Ÿæˆçš„æ—¶å€™ä¼šè°ƒç”¨å§”æ‰˜ã€‚å®žä¾‹ 112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849using System;namespace SimpleEvent&#123; using System; public class EventTest &#123; private int value; public delegate void NumManipulationHandler(); public event NumManipulationHandler ChangeNum; protected virtual void OnNumChanged() &#123; if (ChangeNum != null) &#123; ChangeNum(); &#125; else &#123; Console.WriteLine(&quot;Event fired!&quot;); &#125; &#125; public EventTest(int n ) &#123; SetValue(n); &#125; public void SetValue(int n) &#123; if (value != n) &#123; value = n; OnNumChanged(); &#125; &#125; &#125; public class MainClass &#123; public static void Main() &#123; EventTest e = new EventTest(5); e.SetValue(7); e.SetValue(11); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š123Event Fired!Event Fired!Event Fired!å®žä¾‹ 2æœ¬å®žä¾‹æä¾›ä¸€ä¸ªç®€å•çš„ç”¨äºŽçƒ­æ°´é”…ç‚‰ç³»ç»Ÿæ•…éšœæŽ’é™¤çš„åº”ç”¨ç¨‹åºã€‚å½“ç»´ä¿®å·¥ç¨‹å¸ˆæ£€æŸ¥é”…ç‚‰æ—¶ï¼Œé”…ç‚‰çš„æ¸©åº¦å’ŒåŽ‹åŠ›ä¼šéšç€ç»´ä¿®å·¥ç¨‹å¸ˆçš„å¤‡æ³¨è‡ªåŠ¨è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶ä¸­ã€‚123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100using System;using System.IO;namespace BoilerEventAppl&#123; // boiler ç±» class Boiler &#123; private int temp; private int pressure; public Boiler(int t, int p) &#123; temp = t; pressure = p; &#125; public int getTemp() &#123; return temp; &#125; public int getPressure() &#123; return pressure; &#125; &#125; // äº‹ä»¶å‘å¸ƒå™¨ class DelegateBoilerEvent &#123; public delegate void BoilerLogHandler(string status); // åŸºäºŽä¸Šé¢çš„å§”æ‰˜å®šä¹‰äº‹ä»¶ public event BoilerLogHandler BoilerEventLog; public void LogProcess() &#123; string remarks = &quot;O. K&quot;; Boiler b = new Boiler(100, 12); int t = b.getTemp(); int p = b.getPressure(); if(t &gt; 150 || t &lt; 80 || p &lt; 12 || p &gt; 15) &#123; remarks = &quot;Need Maintenance&quot;; &#125; OnBoilerEventLog(&quot;Logging Info:\n&quot;); OnBoilerEventLog(&quot;Temparature &quot; + t + &quot;\nPressure: &quot; + p); OnBoilerEventLog(&quot;\nMessage: &quot; + remarks); &#125; protected void OnBoilerEventLog(string message) &#123; if (BoilerEventLog != null) &#123; BoilerEventLog(message); &#125; &#125; &#125; // è¯¥ç±»ä¿ç•™å†™å…¥æ—¥å¿—æ–‡ä»¶çš„æ¡æ¬¾ class BoilerInfoLogger &#123; FileStream fs; StreamWriter sw; public BoilerInfoLogger(string filename) &#123; fs = new FileStream(filename, FileMode.Append, FileAccess.Write); sw = new StreamWriter(fs); &#125; public void Logger(string info) &#123; sw.WriteLine(info); &#125; public void Close() &#123; sw.Close(); fs.Close(); &#125; &#125; // äº‹ä»¶è®¢é˜…å™¨ public class RecordBoilerInfo &#123; static void Logger(string info) &#123; Console.WriteLine(info); &#125;//end of Logger static void Main(string[] args) &#123; BoilerInfoLogger filelog = new BoilerInfoLogger(&quot;e:\\boiler.txt&quot;); DelegateBoilerEvent boilerEvent = new DelegateBoilerEvent(); boilerEvent.BoilerEventLog += new DelegateBoilerEvent.BoilerLogHandler(Logger); boilerEvent.BoilerEventLog += new DelegateBoilerEvent.BoilerLogHandler(filelog.Logger); boilerEvent.LogProcess(); Console.ReadLine(); filelog.Close(); &#125;//end of main &#125;//end of RecordBoilerInfo&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š123456Logging info:Temperature 100Pressure 12Message: O. Ké›†åˆé›†åˆï¼ˆCollectionï¼‰ç±»æ˜¯ä¸“é—¨ç”¨äºŽæ•°æ®å­˜å‚¨å’Œæ£€ç´¢çš„ç±»ã€‚è¿™äº›ç±»æä¾›äº†å¯¹æ ˆï¼ˆstackï¼‰ã€é˜Ÿåˆ—ï¼ˆqueueï¼‰ã€åˆ—è¡¨ï¼ˆlistï¼‰å’Œå“ˆå¸Œè¡¨ï¼ˆhash tableï¼‰çš„æ”¯æŒã€‚å¤§å¤šæ•°é›†åˆç±»å®žçŽ°äº†ç›¸åŒçš„æŽ¥å£ã€‚é›†åˆï¼ˆCollectionï¼‰ç±»æœåŠ¡äºŽä¸åŒçš„ç›®çš„ï¼Œå¦‚ä¸ºå…ƒç´ åŠ¨æ€åˆ†é…å†…å­˜ï¼ŒåŸºäºŽç´¢å¼•è®¿é—®åˆ—è¡¨é¡¹ç­‰ç­‰ã€‚è¿™äº›ç±»åˆ›å»º Object ç±»çš„å¯¹è±¡çš„é›†åˆã€‚åœ¨ C# ä¸­ï¼ŒObject ç±»æ˜¯æ‰€æœ‰æ•°æ®ç±»åž‹çš„åŸºç±»ã€‚å„ç§é›†åˆç±»å’Œå®ƒä»¬çš„ç”¨æ³•ä¸‹é¢æ˜¯å„ç§å¸¸ç”¨çš„ System.Collection å‘½åç©ºé—´çš„ç±»ã€‚ç‚¹å‡»ä¸‹é¢çš„é“¾æŽ¥æŸ¥çœ‹ç»†èŠ‚ã€‚ç±»æè¿°å’Œç”¨æ³•åŠ¨æ€æ•°ç»„ï¼ˆArrayListï¼‰å®ƒä»£è¡¨äº†å¯è¢«å•ç‹¬ç´¢å¼•çš„å¯¹è±¡çš„æœ‰åºé›†åˆã€‚å®ƒåŸºæœ¬ä¸Šå¯ä»¥æ›¿ä»£ä¸€ä¸ªæ•°ç»„ã€‚ä½†æ˜¯ï¼Œä¸Žæ•°ç»„ä¸åŒçš„æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ç´¢å¼•åœ¨æŒ‡å®šçš„ä½ç½®æ·»åŠ å’Œç§»é™¤é¡¹ç›®ï¼ŒåŠ¨æ€æ•°ç»„ä¼šè‡ªåŠ¨é‡æ–°è°ƒæ•´å®ƒçš„å¤§å°ã€‚å®ƒä¹Ÿå…è®¸åœ¨åˆ—è¡¨ä¸­è¿›è¡ŒåŠ¨æ€å†…å­˜åˆ†é…ã€å¢žåŠ ã€æœç´¢ã€æŽ’åºå„é¡¹ã€‚å“ˆå¸Œè¡¨ï¼ˆHashtableï¼‰å®ƒä½¿ç”¨é”®æ¥è®¿é—®é›†åˆä¸­çš„å…ƒç´ ã€‚å½“æ‚¨ä½¿ç”¨é”®è®¿é—®å…ƒç´ æ—¶ï¼Œåˆ™ä½¿ç”¨å“ˆå¸Œè¡¨ï¼Œè€Œä¸”æ‚¨å¯ä»¥è¯†åˆ«ä¸€ä¸ªæœ‰ç”¨çš„é”®å€¼ã€‚å“ˆå¸Œè¡¨ä¸­çš„æ¯ä¸€é¡¹éƒ½æœ‰ä¸€ä¸ªé”®/å€¼å¯¹ã€‚é”®ç”¨äºŽè®¿é—®é›†åˆä¸­çš„é¡¹ç›®ã€‚æŽ’åºåˆ—è¡¨ï¼ˆSortedListï¼‰å®ƒå¯ä»¥ä½¿ç”¨é”®å’Œç´¢å¼•æ¥è®¿é—®åˆ—è¡¨ä¸­çš„é¡¹ã€‚æŽ’åºåˆ—è¡¨æ˜¯æ•°ç»„å’Œå“ˆå¸Œè¡¨çš„ç»„åˆã€‚å®ƒåŒ…å«ä¸€ä¸ªå¯ä½¿ç”¨é”®æˆ–ç´¢å¼•è®¿é—®å„é¡¹çš„åˆ—è¡¨ã€‚å¦‚æžœæ‚¨ä½¿ç”¨ç´¢å¼•è®¿é—®å„é¡¹ï¼Œåˆ™å®ƒæ˜¯ä¸€ä¸ªåŠ¨æ€æ•°ç»„ï¼ˆArrayListï¼‰ï¼Œå¦‚æžœæ‚¨ä½¿ç”¨é”®è®¿é—®å„é¡¹ï¼Œåˆ™å®ƒæ˜¯ä¸€ä¸ªå“ˆå¸Œè¡¨ï¼ˆHashtableï¼‰ã€‚é›†åˆä¸­çš„å„é¡¹æ€»æ˜¯æŒ‰é”®å€¼æŽ’åºã€‚å †æ ˆï¼ˆStackï¼‰å®ƒä»£è¡¨äº†ä¸€ä¸ªåŽè¿›å…ˆå‡ºçš„å¯¹è±¡é›†åˆã€‚å½“æ‚¨éœ€è¦å¯¹å„é¡¹è¿›è¡ŒåŽè¿›å…ˆå‡ºçš„è®¿é—®æ—¶ï¼Œåˆ™ä½¿ç”¨å †æ ˆã€‚å½“æ‚¨åœ¨åˆ—è¡¨ä¸­æ·»åŠ ä¸€é¡¹ï¼Œç§°ä¸ºæŽ¨å…¥å…ƒç´ ï¼Œå½“æ‚¨ä»Žåˆ—è¡¨ä¸­ç§»é™¤ä¸€é¡¹æ—¶ï¼Œç§°ä¸ºå¼¹å‡ºå…ƒç´ ã€‚é˜Ÿåˆ—ï¼ˆQueueï¼‰å®ƒä»£è¡¨äº†ä¸€ä¸ªå…ˆè¿›å…ˆå‡ºçš„å¯¹è±¡é›†åˆã€‚å½“æ‚¨éœ€è¦å¯¹å„é¡¹è¿›è¡Œå…ˆè¿›å…ˆå‡ºçš„è®¿é—®æ—¶ï¼Œåˆ™ä½¿ç”¨é˜Ÿåˆ—ã€‚å½“æ‚¨åœ¨åˆ—è¡¨ä¸­æ·»åŠ ä¸€é¡¹ï¼Œç§°ä¸ºå…¥é˜Ÿï¼Œå½“æ‚¨ä»Žåˆ—è¡¨ä¸­ç§»é™¤ä¸€é¡¹æ—¶ï¼Œç§°ä¸ºå‡ºé˜Ÿã€‚ç‚¹é˜µåˆ—ï¼ˆBitArrayï¼‰å®ƒä»£è¡¨äº†ä¸€ä¸ªä½¿ç”¨å€¼ 1 å’Œ 0 æ¥è¡¨ç¤ºçš„äºŒè¿›åˆ¶æ•°ç»„ã€‚å½“æ‚¨éœ€è¦å­˜å‚¨ä½ï¼Œä½†æ˜¯äº‹å…ˆä¸çŸ¥é“ä½æ•°æ—¶ï¼Œåˆ™ä½¿ç”¨ç‚¹é˜µåˆ—ã€‚æ‚¨å¯ä»¥ä½¿ç”¨æ•´åž‹ç´¢å¼•ä»Žç‚¹é˜µåˆ—é›†åˆä¸­è®¿é—®å„é¡¹ï¼Œç´¢å¼•ä»Žé›¶å¼€å§‹ã€‚æ³›åž‹æ³›åž‹ï¼ˆGenericï¼‰ å…è®¸æ‚¨å»¶è¿Ÿç¼–å†™ç±»æˆ–æ–¹æ³•ä¸­çš„ç¼–ç¨‹å…ƒç´ çš„æ•°æ®ç±»åž‹çš„è§„èŒƒï¼Œç›´åˆ°å®žé™…åœ¨ç¨‹åºä¸­ä½¿ç”¨å®ƒçš„æ—¶å€™ã€‚æ¢å¥è¯è¯´ï¼Œæ³›åž‹å…è®¸æ‚¨ç¼–å†™ä¸€ä¸ªå¯ä»¥ä¸Žä»»ä½•æ•°æ®ç±»åž‹ä¸€èµ·å·¥ä½œçš„ç±»æˆ–æ–¹æ³•ã€‚æ‚¨å¯ä»¥é€šè¿‡æ•°æ®ç±»åž‹çš„æ›¿ä»£å‚æ•°ç¼–å†™ç±»æˆ–æ–¹æ³•çš„è§„èŒƒã€‚å½“ç¼–è¯‘å™¨é‡åˆ°ç±»çš„æž„é€ å‡½æ•°æˆ–æ–¹æ³•çš„å‡½æ•°è°ƒç”¨æ—¶ï¼Œå®ƒä¼šç”Ÿæˆä»£ç æ¥å¤„ç†æŒ‡å®šçš„æ•°æ®ç±»åž‹ã€‚ä¸‹é¢è¿™ä¸ªç®€å•çš„å®žä¾‹å°†æœ‰åŠ©äºŽæ‚¨ç†è§£è¿™ä¸ªæ¦‚å¿µï¼š1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556using System;using System.Collections.Generic;namespace GenericApplication&#123; public class MyGenericArray&lt;T&gt; &#123; private T[] array; public MyGenericArray(int size) &#123; array = new T[size + 1]; &#125; public T getItem(int index) &#123; return array[index]; &#125; public void setItem(int index, T value) &#123; array[index] = value; &#125; &#125; class Tester &#123; static void Main(string[] args) &#123; // å£°æ˜Žä¸€ä¸ªæ•´åž‹æ•°ç»„ MyGenericArray&lt;int&gt; intArray = new MyGenericArray&lt;int&gt;(5); // è®¾ç½®å€¼ for (int c = 0; c &lt; 5; c++) &#123; intArray.setItem(c, c*5); &#125; // èŽ·å–å€¼ for (int c = 0; c &lt; 5; c++) &#123; Console.Write(intArray.getItem(c) + &quot; &quot;); &#125; Console.WriteLine(); // å£°æ˜Žä¸€ä¸ªå­—ç¬¦æ•°ç»„ MyGenericArray&lt;char&gt; charArray = new MyGenericArray&lt;char&gt;(5); // è®¾ç½®å€¼ for (int c = 0; c &lt; 5; c++) &#123; charArray.setItem(c, (char)(c+97)); &#125; // èŽ·å–å€¼ for (int c = 0; c &lt; 5; c++) &#123; Console.Write(charArray.getItem(c) + &quot; &quot;); &#125; Console.WriteLine(); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š120 5 10 15 20a b c d eæ³›åž‹ï¼ˆGenericï¼‰çš„ç‰¹æ€§ä½¿ç”¨æ³›åž‹æ˜¯ä¸€ç§å¢žå¼ºç¨‹åºåŠŸèƒ½çš„æŠ€æœ¯ï¼Œå…·ä½“è¡¨çŽ°åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼šå®ƒæœ‰åŠ©äºŽæ‚¨æœ€å¤§é™åº¦åœ°é‡ç”¨ä»£ç ã€ä¿æŠ¤ç±»åž‹çš„å®‰å…¨ä»¥åŠæé«˜æ€§èƒ½ã€‚æ‚¨å¯ä»¥åˆ›å»ºæ³›åž‹é›†åˆç±»ã€‚.NET æ¡†æž¶ç±»åº“åœ¨ System.Collections.Generic å‘½åç©ºé—´ä¸­åŒ…å«äº†ä¸€äº›æ–°çš„æ³›åž‹é›†åˆç±»ã€‚æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›æ³›åž‹é›†åˆç±»æ¥æ›¿ä»£ System.Collections ä¸­çš„é›†åˆç±»ã€‚æ‚¨å¯ä»¥åˆ›å»ºè‡ªå·±çš„æ³›åž‹æŽ¥å£ã€æ³›åž‹ç±»ã€æ³›åž‹æ–¹æ³•ã€æ³›åž‹äº‹ä»¶å’Œæ³›åž‹å§”æ‰˜ã€‚æ‚¨å¯ä»¥å¯¹æ³›åž‹ç±»è¿›è¡Œçº¦æŸä»¥è®¿é—®ç‰¹å®šæ•°æ®ç±»åž‹çš„æ–¹æ³•ã€‚å…³äºŽæ³›åž‹æ•°æ®ç±»åž‹ä¸­ä½¿ç”¨çš„ç±»åž‹çš„ä¿¡æ¯å¯åœ¨è¿è¡Œæ—¶é€šè¿‡ä½¿ç”¨åå°„èŽ·å–ã€‚æ³›åž‹ï¼ˆGenericï¼‰æ–¹æ³•åœ¨ä¸Šé¢çš„å®žä¾‹ä¸­ï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨äº†æ³›åž‹ç±»ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç±»åž‹å‚æ•°å£°æ˜Žæ³›åž‹æ–¹æ³•ã€‚ä¸‹é¢çš„ç¨‹åºè¯´æ˜Žäº†è¿™ä¸ªæ¦‚å¿µï¼š123456789101112131415161718192021222324252627282930313233343536373839404142using System;using System.Collections.Generic;namespace GenericMethodAppl&#123; class Program &#123; static void Swap&lt;T&gt;(ref T lhs, ref T rhs) &#123; T temp; temp = lhs; lhs = rhs; rhs = temp; &#125; static void Main(string[] args) &#123; int a, b; char c, d; a = 10; b = 20; c = &apos;I&apos;; d = &apos;V&apos;; // åœ¨äº¤æ¢ä¹‹å‰æ˜¾ç¤ºå€¼ Console.WriteLine(&quot;Int values before calling swap:&quot;); Console.WriteLine(&quot;a = &#123;0&#125;, b = &#123;1&#125;&quot;, a, b); Console.WriteLine(&quot;Char values before calling swap:&quot;); Console.WriteLine(&quot;c = &#123;0&#125;, d = &#123;1&#125;&quot;, c, d); // è°ƒç”¨ swap Swap&lt;int&gt;(ref a, ref b); Swap&lt;char&gt;(ref c, ref d); // åœ¨äº¤æ¢ä¹‹åŽæ˜¾ç¤ºå€¼ Console.WriteLine(&quot;Int values after calling swap:&quot;); Console.WriteLine(&quot;a = &#123;0&#125;, b = &#123;1&#125;&quot;, a, b); Console.WriteLine(&quot;Char values after calling swap:&quot;); Console.WriteLine(&quot;c = &#123;0&#125;, d = &#123;1&#125;&quot;, c, d); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12345678Int values before calling swap:a = 10, b = 20Char values before calling swap:c = I, d = VInt values after calling swap:a = 20, b = 10Char values after calling swap:c = V, d = Iæ³›åž‹ï¼ˆGenericï¼‰å§”æ‰˜æ‚¨å¯ä»¥é€šè¿‡ç±»åž‹å‚æ•°å®šä¹‰æ³›åž‹å§”æ‰˜ã€‚ä¾‹å¦‚ï¼š1delegate T NumberChanger&lt;T&gt;(T n);ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†å§”æ‰˜çš„ä½¿ç”¨ï¼š123456789101112131415161718192021222324252627282930313233343536373839using System;using System.Collections.Generic;delegate T NumberChanger&lt;T&gt;(T n);namespace GenericDelegateAppl&#123; class TestDelegate &#123; static int num = 10; public static int AddNum(int p) &#123; num += p; return num; &#125; public static int MultNum(int q) &#123; num *= q; return num; &#125; public static int getNum() &#123; return num; &#125; static void Main(string[] args) &#123; // åˆ›å»ºå§”æ‰˜å®žä¾‹ NumberChanger&lt;int&gt; nc1 = new NumberChanger&lt;int&gt;(AddNum); NumberChanger&lt;int&gt; nc2 = new NumberChanger&lt;int&gt;(MultNum); // ä½¿ç”¨å§”æ‰˜å¯¹è±¡è°ƒç”¨æ–¹æ³• nc1(25); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); nc2(5); Console.WriteLine(&quot;Value of Num: &#123;0&#125;&quot;, getNum()); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š12Value of Num: 35Value of Num: 175åŒ¿åæ–¹æ³•æˆ‘ä»¬å·²ç»æåˆ°è¿‡ï¼Œå§”æ‰˜æ˜¯ç”¨äºŽå¼•ç”¨ä¸Žå…¶å…·æœ‰ç›¸åŒæ ‡ç­¾çš„æ–¹æ³•ã€‚æ¢å¥è¯è¯´ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å§”æ‰˜å¯¹è±¡è°ƒç”¨å¯ç”±å§”æ‰˜å¼•ç”¨çš„æ–¹æ³•ã€‚åŒ¿åæ–¹æ³•ï¼ˆAnonymous methodsï¼‰ æä¾›äº†ä¸€ç§ä¼ é€’ä»£ç å—ä½œä¸ºå§”æ‰˜å‚æ•°çš„æŠ€æœ¯ã€‚åŒ¿åæ–¹æ³•æ˜¯æ²¡æœ‰åç§°åªæœ‰ä¸»ä½“çš„æ–¹æ³•ã€‚åœ¨åŒ¿åæ–¹æ³•ä¸­æ‚¨ä¸éœ€è¦æŒ‡å®šè¿”å›žç±»åž‹ï¼Œå®ƒæ˜¯ä»Žæ–¹æ³•ä¸»ä½“å†…çš„ return è¯­å¥æŽ¨æ–­çš„ã€‚ç¼–å†™åŒ¿åæ–¹æ³•çš„è¯­æ³•åŒ¿åæ–¹æ³•æ˜¯é€šè¿‡ä½¿ç”¨ delegate å…³é”®å­—åˆ›å»ºå§”æ‰˜å®žä¾‹æ¥å£°æ˜Žçš„ã€‚ä¾‹å¦‚ï¼š123456delegate void NumberChanger(int n);...NumberChanger nc = delegate(int x)&#123; Console.WriteLine(&quot;Anonymous Method: &#123;0&#125;&quot;, x);&#125;;ä»£ç å— Console.WriteLine(&quot;Anonymous Method: {0}&quot;, x); æ˜¯åŒ¿åæ–¹æ³•çš„ä¸»ä½“ã€‚å§”æ‰˜å¯ä»¥é€šè¿‡åŒ¿åæ–¹æ³•è°ƒç”¨ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å‘½åæ–¹æ³•è°ƒç”¨ï¼Œå³ï¼Œé€šè¿‡å‘å§”æ‰˜å¯¹è±¡ä¼ é€’æ–¹æ³•å‚æ•°ã€‚ä¾‹å¦‚ï¼š1nc(10);å®žä¾‹ä¸‹é¢çš„å®žä¾‹æ¼”ç¤ºäº†åŒ¿åæ–¹æ³•çš„æ¦‚å¿µï¼š1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950using System;delegate void NumberChanger(int n);namespace DelegateAppl&#123; class TestDelegate &#123; static int num = 10; public static void AddNum(int p) &#123; num += p; Console.WriteLine(&quot;Named Method: &#123;0&#125;&quot;, num); &#125; public static void MultNum(int q) &#123; num *= q; Console.WriteLine(&quot;Named Method: &#123;0&#125;&quot;, num); &#125; public static int getNum() &#123; return num; &#125; static void Main(string[] args) &#123; // ä½¿ç”¨åŒ¿åæ–¹æ³•åˆ›å»ºå§”æ‰˜å®žä¾‹ NumberChanger nc = delegate(int x) &#123; Console.WriteLine(&quot;Anonymous Method: &#123;0&#125;&quot;, x); &#125;; // ä½¿ç”¨åŒ¿åæ–¹æ³•è°ƒç”¨å§”æ‰˜ nc(10); // ä½¿ç”¨å‘½åæ–¹æ³•å®žä¾‹åŒ–å§”æ‰˜ nc = new NumberChanger(AddNum); // ä½¿ç”¨å‘½åæ–¹æ³•è°ƒç”¨å§”æ‰˜ nc(5); // ä½¿ç”¨å¦ä¸€ä¸ªå‘½åæ–¹æ³•å®žä¾‹åŒ–å§”æ‰˜ nc = new NumberChanger(MultNum); // ä½¿ç”¨å‘½åæ–¹æ³•è°ƒç”¨å§”æ‰˜ nc(2); Console.ReadKey(); &#125; &#125;&#125;å½“ä¸Šé¢çš„ä»£ç è¢«ç¼–è¯‘å’Œæ‰§è¡Œæ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä¸‹åˆ—ç»“æžœï¼š123Anonymous Method: 10Named Method: 15Named Method: 30å¤šçº¿ç¨‹]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GRADIENTS, BATCH NORMALIZATION AND LAYER NORMALIZATION]]></title>
      <url>%2F2017%2F05%2F09%2FGRADIENTS-BATCH-NORMALIZATION-AND-LAYER-NORMALIZATION%2F</url>
      <content type="text"><![CDATA[In this post, we will take a look at common pit falls with optimization and solutions to some of these issues. The main topics that will be covered are:GradientsExploding gradientsVanishing gradientsLSTMs (pertaining to vanishing gradients)NormalizationAnd then we will see how to implement batch and layer normalization and apply them to our cells.GRADIENTSFirst, we will take a closer look at gradients and backpropagation during optimization. Our example will be a simple MLP but we will extend to an RNN later on.I want to go over what a gradient means. Letâ€™s say we have a very simple MLP with 1 set of weights W_1 which is used to calcualte some y. We devise a very simple loss function J, and our gradient becomes dJ/dW_1 (d = partials). Sure we can take the derivative and apply chain rule and get a number, but what does this value even mean? The gradient can be thought of as several things. One is that the magnitude of the gradient represents the sensitivity or impact this weight has on determining y which determines our loss. This can be seen below:CS231nWhat the gradients (dfdx, dfdy, dfdz, dfdq, dfdz) tell us is the sensitivity of each variable on our result f. In an MLP, we will produce a result (logits) and compare it with our targets to determine the deviance in what we got and what we should have gotten. From this we can use backpropagation to determine how much adjusting needs to be made for each variable along the way, all the way to the beginning.The gradient also holds another key piece of information. It repesents how much we need to change the weights in order to move towards our goal (minimizing the loss, maximizing some objective, etc.). With simple SGD, we get the gradient and we apply an update to the weights (W_i_new = W_i_old â€“ alpha * gradient). If we follow the direction of the gradient, we will be maximizing the goal function. Our loss functions (NLL or cross entropy) are functions we wish to minimize, so we subtract the gradient. We use the learning parameter alpha to control how quickly we change. This is where all of the normalization techniques in this post will come in handy.If we have an alpha that is 1 or larger, we will allow the gradient to directly impact our weights. In the beginning of training a neural net, our weight initializations are bound to be far off from the weights we actually need. This creates a large error and so, results in large gradients. If we choose to update our weights with these large gradients, we will be never reach the minimum point for our loss function. We will keep overshooting and bouncing back and forth. So, we use this alpha (small value) to control how much impact the gradient has. Eventually, the gradient will get smaller as well because of less error and we will reach our goal, but with such a small alpha, this can take a while. With techniques, such as batch normalization and layer normalization, we can afford to use large alpha because the gradients will be controlled due to controlled outputs from the neurons.Now, even with a simple RNN structure, backpropagation can pose several issues. When we get our result, we need to backpropagate all the way back to the very first cell in order to complete our updates. The main principles to really understand are: if I multiply a number greater than 1 over and over, I will reach infinity (explosion) and vice versa, if I multiply a number less than 1 over and over, I will reach 0 (vanishing).EXPLODING GRADIENTSThe first issue is that our gradients can be greater than 1. As we backpropagate the gradient through the network, we can end up with massive gradients. So far, the solution to exploding gradients is a very hacky but cheap solution; just clip the norm of the gradient at some threshold.VANISHING GRADIENTSWe could also experience the other issue where the gradient is less than 1 to start with and as we backpropagate, the effect of the gradient weakens and it will eventually be negligible. A common scenario where this occurs is when we have saturation at the tails of the sigmoidal function (0 or 1). This is problematic because now the derivative will always be near 0. During backpropagation, we will be multiplying this near zero derivative with our error repeatedly.Letâ€™s look at the sigmoidal activation function. You can replicate this example for tanh too.To solve this issue, we can use rectified linear units (ReLU) which donâ€™t suffer from this tail saturation as much. The derivative is 1 if x &gt; 0, so now error signal wonâ€™t weaken as it backpropagates through the network. But we do have the problem in the negative region (x &lt;0) where the derivative is zero. This can nullify our error signal so itâ€™s best to add a leaky factor (http://arxiv.org/abs/1502.01852) to the ReLU unit, where the negative region will have some small negative slope. This parameter can be fixed or be a randomized parameter and be fixed after training. Thereâ€™s also maxout (http://arxiv.org/abs/1302.4389) but this will have twice the amount of weights as a regular ReLU unit.LSTMS (VANISHING GRADIENTS)As for how LSTMs solve the vanishing gradient issue, they donâ€™t have to worry about the error signal weakening as with a regular basic RNN cell. Itâ€™s a bit complicated but the basic idea is that they have a forget gate that determines how much previous memory is stored in the network. This architecture allows the error signal to be transferred effectively to the previous time step. This is usually referred to as the constant error carousel (CEC).NORMALIZATIONThere are several types of normalization techniques but the idea behind all of them is the same, which is shifting our inputs to a zero mean and unit variance.Techniques like batch norm (https://arxiv.org/abs/1502.03167) may help with the gradient issues as a side effect but the main object is to improve overall optimization. When we first initialize our weights, we are bound to have very large deviances from the true weights. These outliers need to be compensated for by the gradients and this further delays convergence during training. Batchnorm helps us here by normalizing the gradients (reducing influence from weight deviances) on a batched implementation and allows us to train faster (can even safely use larger learning rates now).With batch norm, the main idea is to normalize at each layer for every minibatch. We initially may normalize our inputs, but as they travel through the layers, the inputs are operated on by weights and neurons and effectively change. As this progresses, the deviances get larger and larger and our backpropagation will need to account for these large deviances. This restricts us to using a small learning rate to prevent gradient explosion/vanishing. With batch norm, we will normalize the inputs (activations coming from the previous layer) going into each layer using the mean and variance of the activations for the entire minibatch. The normalization is a bit different during training and inference but it is beyond the scope of this post. (details in paper).Batch normalization is very nice but it is based on minibatch size and so itâ€™s a bit difficult to use with recurrent architectures. With layer normalization, we instead compute the mean and variance using ALL of the summed inputs to the neurons in a layer for EVERY single training**case. This removes the dependency on a minibatch size. Unlike batch normalization, the normalization operation for layer norm is same for training and inference. More details can be found on Hintonâ€™s paper here**.######IMPLEMENTING BATCH NORMALIZATIONAs stated above, the main goal of batch normalization is optimization. By normalizing the inputs to a layer to zero mean and unit variance, we can help our net learn faster by minimizing the effects from large errors (especially during initial training).Batch norm is given by the operation below, where \epsilon is a small random noise (for stability). When we apply batch norm on a layer, we are restricting the inputs to follow a normal distribution, which ultimately will restrict the nets ability to learn. In order to fix this, we multiply by a scale parameter (\alpha) and add a shift parameter (\beta). Both of these parameters are trainable.Note that both alpha and beta are applied element wise, so there will be a scale and shift for each neuron in the subsequent layer. With batchnorm, we compute mean and variance across an entire batch and we have a value for each neuron we are feeding our normalized inputs into.So for a given layer, the mean during BN will be 1X. Each training data gets this mean subtracted from it and divided by sqrt(var + epsilon) and then shifted and scaled. To find the mean and var, we use all the examples in the training batch.In order to accurately evaluate the effectiveness of batchnorm, we will use a simple MLP to classify MNIST digits. We will run a normal MLP and an MLP with batchnorm, both initialized with the same starting weights. Letâ€™s take a look at both the naive and TF implementations.First, the naive version:12345678910# Naive BN layerscale1 = tf.Variable(tf.ones([100]))shift1 = tf.Variable(tf.zeros([100]))W1_BN = tf.Variable(W1_init)b1_BN = tf.Variable(tf.zeros([100]))z1_BN = tf.matmul(X,W1_BN)+b1_BNmean1, var1 = tf.nn.moments(z1_BN, [0])BN1 = (z1_BN - mean1) / tf.sqrt(var1 + FLAGS.epsilon)BN1 = scale1*BN1 + shift1fc1_BN = tf.nn.relu(BN1)TF implementation:123456789# TF BN layerscale2 = tf.Variable(tf.ones([100]))shift2 = tf.Variable(tf.zeros([100]))W2_BN = tf.Variable(W2_init)b2_BN = tf.Variable(tf.zeros([100]))z2_BN = tf.matmul(fc1_BN,W2_BN)+b2_BNmean2, var2 = tf.nn.moments(z2_BN, [0])BN2 = tf.nn.batch_normalization(z2_BN,mean2,var2,shift2,scale2,FLAGS.epsilon)fc2_BN = tf.nn.relu(BN2)We first need to compute the mean and variance of the inputs coming into the layer. Then normalize them and scale/shift and then apply the activation function and pass to the next layer.Letâ€™s compare the performance of the normal MLP and the MLP with batchnorm. We will focus of the massive impact on our cost with and without BN. Other interesting features to look at would be gradient norm, neuron inputs, etc.CROSS ENTROPY LOSS###NUANCE:Training is all fine and well, but what about testing. When doing BN on our test set, with the implementation from above, we will be using the mean and variance from our test set. Now think about what will happen if our test set is very small or even size 1. This will homogenize all the outputs we get since all inputs will be close to mean 0 and variance 1. The solution to this is to calculate the population mean and variance during testing and then use those values during testing.Now there are couple ways we can try to calculate the population, even simple as taking the average of the training batch and using it for testing. This isnâ€™t the true population measure so we will calculate the unbiased mean and variance as they do in the original paper. But first, letâ€™s see the accuracy when we feed in test samples of size 1.Not exactly state of the art anymore. So letâ€™s see how to calculate population mean and variance.We will be updating the population mean and variance after each training batch and we will use them for inference. In fact we can simple replace the inference batchnorm process with a simple linear transformation:Below is the tensorflow implementation for batchnorm with the exponential moving average to use during inference. Take a look here for more implementation specifications for batch_norm but the required parameters for us is the actual input that we wish to normalize and wether or not we are training. Note: TF batchnorm with inference is in batch_norm2.py1234567891011121314from tensorflow.contrib.layers import ( batch_norm)...with tf.variable_scope('BN_1') as BN_1: self.BN1 = tf.cond(self.is_training_ph, lambda: batch_norm( self.z1_BN, is_training=True, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=BN_1), lambda: batch_norm( self.z1_BN, is_training=False, center=True, scale=True, activation_fn=tf.nn.relu, updates_collections=None, scope=BN_1, reuse=True))Here are the inference results with the population mean and variance:######IMPLEMENTING LAYER NORMALIZATIONLayernorm is very similar to batch normalization in many ways as you can see with the equation below but it usually reserved for use with recurrent architectures.Layernorm acts on a per layer per sample basis, where the mean and variance are calculated for a specific layer for a specific training point. To understand the different between layernorm and batchnorm letâ€™s see how these mean and variances are computed for both with figures.With layernorm itâ€™s a bit different from BN. We compute the mean and var for every single sample for each layer independently and then do the LN operations using those computed values.First, we will make a function that will apply batch norm given an input tensor.1234567891011121314151617181920# LN funcitiondef ln(inputs, epsilon = 1e-5, scope = None): """ Computer LN given an input tensor. We get in an input of shape [N X D] and with LN we compute the mean and var for each individual training point across all it's hidden dimensions rather than across the training batch as we do in BN. This gives us a mean and var of shape [N X 1]. """ mean, var = tf.nn.moments(inputs, [1], keep_dims=True) with tf.variable_scope(scope + 'LN'): scale = tf.get_variable('alpha', shape=[inputs.get_shape()[1]], initializer=tf.constant_initializer(1)) shift = tf.get_variable('beta', shape=[inputs.get_shape()[1]], initializer=tf.constant_initializer(0)) LN = scale * (inputs - mean) / tf.sqrt(var + epsilon) + shift return LNNow we can apply our LN function to a GRUCell class. Note that I am using tensorflowâ€™s GRUCell class but we can apply LN to all of their other RNN variants as well (LSTM, peephole LSTM, etc.)1234567891011121314151617181920212223242526272829303132333435class GRUCell(RNNCell): """Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).""" def __init__(self, num_units, input_size=None, activation=tanh): if input_size is not None: logging.warn("%s: The input_size parameter is deprecated.", self) self._num_units = num_units self._activation = activation @property def state_size(self): return self._num_units @property def output_size(self): return self._num_units def __call__(self, inputs, state, scope=None): """Gated recurrent unit (GRU) with nunits cells.""" with vs.variable_scope(scope or type(self).__name__): # "GRUCell" with vs.variable_scope("Gates"): # Reset gate and update gate. # We start with bias of 1.0 to not reset and not update. r, u = array_ops.split(1, 2, _linear([inputs, state], 2 * self._num_units, True, 1.0)) # Apply Layer Normalization to the two gates r = ln(r, scope = 'r/') u = ln(r, scope = 'u/') r, u = sigmoid(r), sigmoid(u) with vs.variable_scope("Candidate"): c = self._activation(_linear([inputs, r * state], self._num_units, True)) new_h = u * state + (1 - u) * c return new_h, new_hSHAPES:I received quite a few PMs about some confusing aspects of BN and LN, mostly centered around what is actually the input. Letâ€™s look at BN first. The input to a hidden layer will be [NXH]. Applying BN involves calculating the mean value for each H across all N samples. So we will have a mean of shape [1XH]. This â€œbatchâ€ mean will be used for BN, basically subtracting this batch mean from each sample.Now for LN, letâ€™s imagine a simple RNN situation. Batch major inputs are of shape [N, M, H], where N is the batch size, M is the max number of time steps and H is the number of hidden units. Before feeing to an RNN, we can reshape to time-major which becomes [M, N, H]. Now we feed in one time step at a time into the RNN, so the shape of each time-stepâ€™s input is [N,H]. Applying LN involves calculating the mean for sample across dimension [1], which means looking at all hidden states for each sample (for this particular time step). This gives us a mean of size [NX1]. We use this â€œlayerâ€ mean for each sample.Source page is HERE.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[What Is Local Response Normalization In Convolutional Neural Networks]]></title>
      <url>%2F2017%2F05%2F04%2FWhat-Is-Local-Response-Normalization-In-Convolutional-Neural-Networks%2F</url>
      <content type="text"><![CDATA[Convolutional Neural Networks (CNNs) have been doing wonders in the field of image recognition in recent times. CNN is a type of deep neural network in which the layers are connected using spatially organized patterns. This is in line with how the human visual cortex processes image data. Researchers have been working on coming up with better architectures over the last few years. In this blog post, we will discuss a particular type of layer that has been used consistently across many famous architectures. This layer is called Local Response Normalization layer and it plays an important role. What does it do? Whatâ€™s the advantage of having this in our network?Why do we need normalization layers in the first place?A typical CNN consists of the following layers: convolution, pooling, rectified linear unit (ReLU), fully connected, and loss. If the previous sentence didnâ€™t make sense, you may want to go through a quick CNN tutorial before proceeding further. Anyway, the reason we may want to have normalization layers in our CNN is that we want to have some kind of inhibition scheme.In neurobiology, there is a concept called â€œlateral inhibitionâ€. Now what does that mean? This refers to the capacity of an excited neuron to subdue its neighbors. We basically want a significant peak so that we have a form of local maxima. This tends to create a contrast in that area, hence increasing the sensory perception. Increasing the sensory perception is a good thing! We want to have the same thing in our CNNs.What exactly is Local Response Normalization?Local Response Normalization (LRN) layer implements the lateral inhibition we were talking about in the previous section. This layer is useful when we are dealing with ReLU neurons. Why is that? Because ReLU neurons have unbounded activations and we need LRN to normalize that. We want to detect high frequency features with a large response. If we normalize around the local neighborhood of the excited neuron, it becomes even more sensitive as compared to its neighbors.At the same time, it will dampen the responses that are uniformly large in any given local neighborhood. If all the values are large, then normalizing those values will diminish all of them. So basically we want to encourage some kind of inhibition and boost the neurons with relatively larger activations. This has been discussed nicely in Section 3.3 of the original paper by Krizhevsky et al.How is it done in practice?There are two types of normalizations available in Caffe. You can either normalize within the same channel or you can normalize across channels. Both these methods tend to amplify the excited neuron while dampening the surrounding neurons. When you are normalizing within the same channel, itâ€™s just like considering a 2D neighborhood of dimension N x N, where N is the size of the normalization window. You normalize this window using the values in this neighborhood. If you are normalizing across channels, you will consider a neighborhood along the third dimension but at a single location. You need to consider an area of shape N x 1 x 1. Here 1 x 1 refers to a single value in a 2D matrix and N refers to the normalization size.Source page is here.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The first course of the Docker]]></title>
      <url>%2F2017%2F05%2F01%2FThe-first-course-of-the-Docker%2F</url>
      <content type="text"><![CDATA[Docker command:docker images æŸ¥çœ‹æœ¬æœºæ‰€æœ‰é•œåƒdocker pull NAME ä»Žä»“åº“ä¸‹è½½é•œåƒdocker run [-d -p 8080:80] or [-P] NAME å¯åŠ¨é•œåƒï¼ˆ-d åŽå°è¿è¡Œ -p ç«¯å£æ˜ å°„ -P éšæœºæ˜ å°„ï¼‰docker exec [-i -t] NAME bash è¿›å…¥å®¹å™¨å¹¶æ‰§è¡Œbashdocker ps æŸ¥çœ‹åŽå°å®¹å™¨docker stop ID åœæ­¢dockerå®¹å™¨docker restart ID é‡å¯å®¹å™¨â€‹Docker netowrk type: bridgeDocker port map: host(eth0:80) &lt;â€“&gt; dicker0(bridge) &lt;â€“&gt; docker container(eth0:80)Build DockerDockerfiledocker build [-t] å»ºç«‹Dockerï¼ŒæŒ‡å®šTAGA Dcokerfile example (based tomcat):12345from tomcat MAINTAINER ewan ewanlee@yeah.netCOPY jpress-web-newest.war /usr/local/tomcat/webappsæ­å»ºç¬¬ä¸€ä¸ªWeb appä¸ºäº†ä»‹ç»æ–¹ä¾¿ï¼Œæ‰€ä»¥ä½¿ç”¨äº†å¼€æºçš„javaå®žçŽ°çš„wordpressï¼Œä¹Ÿå°±æ˜¯Jpress[1]ä¸‹è½½ç›¸åº”çš„waråŒ…ï¼Œå¹¶å­˜åˆ°å·¥ä½œç›®å½•ä¸‹[2]ä¸‹è½½ä¸€ä¸ªtomcatçš„Dockeré•œåƒdocker pull tomcat1234567891011121314151617Using default tag: latestlatest: Pulling from library/tomcatcd0a524342ef: Pull complete e39c3ffe4133: Pull complete aac3320edf40: Pull complete 4d9e109682f7: Pull complete 0a59efcf9553: Pull complete 43a404e523e0: Pull complete 806f07b1dce8: Pull complete 0cad96dccb4c: Pull complete 04073e2a9145: Pull complete d9e4bf4be89c: Pull complete 739005fdecc9: Pull complete 8bd03d99f1b2: Pull complete d586afbd7622: Pull complete Digest: sha256:88483873b279aaea5ced002c98dde04555584b66de29797a4476d5e94874e6deStatus: Downloaded newer image for tomcat:latest[3]å†™ä¸€ä¸ªDockerfileï¼Œä¹Ÿå°±æ˜¯ä¹‹å‰çš„example[4]å»ºç«‹é•œåƒdocker build -t jpress:latest .ç»“æžœå¦‚ä¸‹ï¼š1234567891011Sending build context to Docker daemon 20.8 MBStep 1 : FROM tomcat ---&gt; d71978506e58Step 2 : MAINTAINER ewan ewanlee@yeah.net ---&gt; Running in dfa1902d1ea4 ---&gt; 956612ba6987Removing intermediate container dfa1902d1ea4Step 3 : COPY jpress-web-newest.war /usr/local/tomcat/webapps ---&gt; dd6eecd741e7Removing intermediate container 1fe7f943071bSuccessfully built dd6eecd741e7[5]ä¸‹è½½ä¸€ä¸ªmysqlçš„dockeré•œåƒdocker pull mysql123456789101112131415Using default tag: latestlatest: Pulling from library/mysqlcd0a524342ef: Already exists d9c95f06c17e: Pull complete 46b2d578f59a: Pull complete 10fbc2bcc6e9: Pull complete 91b1a29c3956: Pull complete 5bf9316bd602: Pull complete 69bd23f08b55: Pull complete 4fb778132e94: Pull complete 6913628d7744: Pull complete a477f36dc2e0: Pull complete c954124ae935: Pull complete Digest: sha256:e44b9a3ae88db013a3e8571a89998678ba44676ed4ae9f54714fd31e108f8b58Status: Downloaded newer image for mysql:latest[6]è¿è¡Œmysqlå¹¶åˆ›å»ºä¸€ä¸ªæ•°æ®åº“1docker run -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=000000 -e MYSQL_DATABASE=jpress mysql[7]è¿è¡Œè‡ªå·±å»ºç«‹çš„jpressé•œåƒdocker run -d -p 8888:8080 jpressä¸‹é¢è¿›è¡Œæµè§ˆå™¨é¡µé¢çš„é…ç½®ï¼Œåœ¨æµè§ˆå™¨è¾“å…¥localhost:8888å°†å‡ºçŽ°ä»¥ä¸‹ç•Œé¢ï¼šåœ¨åœ°å€æ åŽåŠ å…¥åŽç¼€jpress-web-newestå¡«å†™é…ç½®ä¿¡æ¯ï¼Œæ³¨æ„æœåŠ¡å™¨åœ°å€æ˜¯docker0ç½‘å¡çš„ipç»“æžœå®‰è£…è¿‡ç¨‹ä¸­å‡ºçŽ°äº†ä¸€ä¸ªbugï¼Œå°±æ˜¯åœ¨è¿›è¡Œé…ç½®åŽæˆ‘é€€å‡ºäº†ï¼Œå†æ¬¡è¿›åŽ»é‡æ–°é…ç½®å‡ºé”™ï¼Œæœ€åŽå‘çŽ°åŽŸå› æ˜¯è¡¨å‰ç¼€éœ€è¦æ”¹ä¸€ä¸‹ï¼Œå› ä¸ºä¹‹å‰é…ç½®æˆåŠŸäº†ï¼Œæ•°æ®åº“ä¸­å·²ç»æœ‰äº†ä¸€ä¸ªç›¸åŒçš„è¡¨å‰ç¼€â€‹:Pæ˜¯ä¸æ˜¯å¾ˆæ–¹ä¾¿ï¼Œå®Œå…¨ä¸ç”¨æ‰‹åŠ¨å®‰è£…ä»»ä½•ä¸œè¥¿~Referenceshttp://www.imooc.com/learn/824]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HMM implemented by hmmlearn]]></title>
      <url>%2F2017%2F05%2F01%2FHMM-implemented-by-hmmlearn%2F</url>
      <content type="text"><![CDATA[Sampling from HMMThis script shows how to sample points from a Hidden Markov Model (HMM): we use a 4-components with specified mean and covariance.The plot show the sequence of observations generated with the transitions between them. We can see that, as specified by our transition matrix, there are no transition between component 1 and 3.123456print(__doc__)import numpy as npimport matplotlib.pyplot as pltfrom hmmlearn import hmmPrepare parameters for a 4-components HMM Initial population probability123456789101112131415161718192021222324252627282930313233343536373839startprob = np.array([0.6, 0.3, 0.1, 0.0])# The transition matrix, note that there are no transitions possible# between component 1 and 3transmat = np.array([[0.7, 0.2, 0.0, 0.1], [0.3, 0.5, 0.2, 0.0], [0.0, 0.3, 0.5, 0.2], [0.2, 0.0, 0.2, 0.6]])# The means of each componentmeans = np.array([[0.0, 0.0], [0.0, 11.0], [9.0, 10.0], [11.0, -1.0]])# The covariance of each componentcovars = .5 * np.tile(np.identity(2), (4, 1, 1))# Build an HMM instance and set parametersmodel = hmm.GaussianHMM(n_components=4, covariance_type="full")# Instead of fitting it from the data, we directly set the estimated# parameters, the means and covariance of the componentsmodel.startprob_ = startprobmodel.transmat_ = transmatmodel.means_ = meansmodel.covars_ = covars# Generate samplesX, Z = model.sample(500)# Plot the sampled dataplt.plot(X[:, 0], X[:, 1], ".-", label="observations", ms=6, mfc="orange", alpha=0.7)# Indicate the component numbersfor i, m in enumerate(means): plt.text(m[0], m[1], 'Component %i' % (i + 1), size=17, horizontalalignment='center', bbox=dict(alpha=.7, facecolor='w'))plt.legend(loc='best')plt.show()Total running time of the script: ( 0 minutes 0.676 seconds)Gaussian HMM of stock dataThis script shows how to use Gaussian HMM on stock price data from Yahoo! finance. For more information on how to visualize stock prices with matplotlib, please refer to date_demo1.py of matplotlib.12345678910111213141516171819from __future__ import print_functionimport datetimeimport numpy as npfrom matplotlib import cm, pyplot as pltfrom matplotlib.dates import YearLocator, MonthLocatortry: from matplotlib.finance import quotes_historical_yahoo_ochlexcept ImportError: # For Matplotlib prior to 1.5. from matplotlib.finance import ( quotes_historical_yahoo as quotes_historical_yahoo_ochl )from hmmlearn.hmm import GaussianHMMprint(__doc__)Get quotes from Yahoo! finance1234567891011121314151617quotes = quotes_historical_yahoo_ochl( "INTC", datetime.date(1995, 1, 1), datetime.date(2012, 1, 6))# Unpack quotesdates = np.array([q[0] for q in quotes], dtype=int)close_v = np.array([q[2] for q in quotes])volume = np.array([q[5] for q in quotes])[1:]# Take diff of close value. Note that this makes# ``len(diff) = len(close_t) - 1``, therefore, other quantities also# need to be shifted by 1.diff = np.diff(close_v)dates = dates[1:]close_v = close_v[1:]# Pack diff and volume for training.X = np.column_stack([diff, volume])Run Gaussian HMM123456789print("fitting to HMM and decoding ...", end="")# Make an HMM instance and execute fitmodel = GaussianHMM(n_components=4, covariance_type="diag", n_iter=1000).fit(X)# Predict the optimal sequence of internal hidden statehidden_states = model.predict(X)print("done")Out:1fitting to HMM and decoding ...donePrint trained parameters and plot1234567891011121314151617181920212223242526print("Transition matrix")print(model.transmat_)print()print("Means and vars of each hidden state")for i in range(model.n_components): print("&#123;0&#125;th hidden state".format(i)) print("mean = ", model.means_[i]) print("var = ", np.diag(model.covars_[i])) print()fig, axs = plt.subplots(model.n_components, sharex=True, sharey=True)colours = cm.rainbow(np.linspace(0, 1, model.n_components))for i, (ax, colour) in enumerate(zip(axs, colours)): # Use fancy indexing to plot data in each state. mask = hidden_states == i ax.plot_date(dates[mask], close_v[mask], ".-", c=colour) ax.set_title("&#123;0&#125;th hidden state".format(i)) # Format the ticks. ax.xaxis.set_major_locator(YearLocator()) ax.xaxis.set_minor_locator(MonthLocator()) ax.grid(True)plt.show()Out:12345678910111213141516171819202122Transition matrix[[ 9.79220773e-01 2.57382344e-15 2.72061945e-03 1.80586073e-02] [ 1.12216188e-12 7.73561269e-01 1.85019044e-01 4.14196869e-02] [ 3.25313504e-03 1.12692615e-01 8.83368021e-01 6.86228435e-04] [ 1.18741799e-01 4.20310643e-01 1.18670597e-18 4.60947557e-01]]Means and vars of each hidden state0th hidden statemean = [ 2.33331888e-02 4.97389989e+07]var = [ 6.97748259e-01 2.49466578e+14]1th hidden statemean = [ 2.12401671e-02 8.81882861e+07]var = [ 1.18665023e-01 5.64418451e+14]2th hidden statemean = [ 7.69658065e-03 5.43135922e+07]var = [ 5.02315562e-02 1.54569357e+14]3th hidden statemean = [ -3.53210673e-01 1.53080943e+08]var = [ 2.55544137e+00 5.88210257e+15]Total running time of the script: ( 0 minutes 2.903 seconds)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The Basic of Hidden Markov Model]]></title>
      <url>%2F2017%2F04%2F30%2FThe-basic-of-Hidden-Markov-Model%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[WGAN implemented by PyTorch]]></title>
      <url>%2F2017%2F04%2F29%2FWGAN-implemented-by-PyTorch%2F</url>
      <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139#!/usr/bin/env python# Wasserstein Generative Adversarial Networks (WGAN) example in PyTorch.import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variable# Data paramsdata_mean = 4data_stddev = 1.25# Model paramsg_input_size = 1 # Random noise dimension coming into generator, per output vectorg_hidden_size = 50 # Generator complexityg_output_size = 1 # size of generated output vectord_input_size = 100 # Minibatch size - cardinality of distributionsd_hidden_size = 50 # Discriminator complexityd_output_size = 1 # Single dimension for 'real' vs. 'fake'minibatch_size = d_input_sized_learning_rate = 2e-4 # 2e-4g_learning_rate = 2e-4# optim_betas = (0.9, 0.999)num_epochs = 30000print_interval = 200# d_steps = 1 # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generatord_steps = 5g_steps = 1# ### Uncomment only one of these#(name, preprocess, d_input_func) = ("Raw data", lambda data: data, lambda x: x)(name, preprocess, d_input_func) = ("Data and variances", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)print("Using data [%s]" % (name))# ##### DATA: Target data and generator input datadef get_distribution_sampler(mu, sigma): return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n))) # Gaussiandef get_generator_input_sampler(): return lambda m, n: torch.rand(m, n) # Uniform-dist data into generator, _NOT_ Gaussian# ##### MODELS: Generator model and discriminator modelclass Generator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Generator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.sigmoid(self.map2(x)) return self.map3(x)class Discriminator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Discriminator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.elu(self.map2(x)) # return F.sigmoid(self.map3(x)) return self.map3(x)def extract(v): return v.data.storage().tolist()def stats(d): return [np.mean(d), np.std(d)]def decorate_with_diffs(data, exponent): mean = torch.mean(data.data, 1) mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0]) diffs = torch.pow(data - Variable(mean_broadcast), exponent) return torch.cat([data, diffs], 1)d_sampler = get_distribution_sampler(data_mean, data_stddev)gi_sampler = get_generator_input_sampler()G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)# criterion = nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss# d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)# g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)for epoch in range(num_epochs): for d_index in range(d_steps): # 1. Train D on real+fake D.zero_grad() # 1A: Train D on real d_real_data = Variable(d_sampler(d_input_size)) d_real_decision = D(preprocess(d_real_data)) # d_real_error = criterion(d_real_decision, Variable(torch.ones(1))) # ones = true d_real_error = -torch.mean(d_real_decision) d_real_error.backward() # compute/store gradients, but don't change params # 1B: Train D on fake d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size)) d_fake_data = G(d_gen_input).detach() # detach to avoid training G on these labels d_fake_decision = D(preprocess(d_fake_data.t())) # d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1))) # zeros = fake d_fake_error = torch.mean(d_fake_decision) d_fake_error.backward() d_optimizer.step() # Only optimizes D's parameters; changes based on stored gradients from backward() # Weight Clipping for p in D.parameters(): p.data.clamp_(-0.01, 0.01) for g_index in range(g_steps): # 2. Train G on D's response (but DO NOT train D on these labels) G.zero_grad() gen_input = Variable(gi_sampler(minibatch_size, g_input_size)) g_fake_data = G(gen_input) dg_fake_decision = D(preprocess(g_fake_data.t())) # g_error = criterion(dg_fake_decision, Variable(torch.ones(1))) # we want to fool, so pretend it's all genuine g_error = -torch.mean(dg_fake_decision) g_error.backward() g_optimizer.step() # Only optimizes G's parameters if epoch % print_interval == 0: print("%s: D: %s/%s G: %s (Real: %s, Fake: %s) " % (epoch, extract(d_real_error)[0], extract(d_fake_error)[0], extract(g_error)[0], stats(extract(d_real_data)), stats(extract(d_fake_data))))ä¸Žä¹‹å‰çš„æ–‡ç« æ‰€åšçš„ä¿®æ”¹ä»…ä»…åªæœ‰ä»¥ä¸‹å‡ ç‚¹ï¼ˆç†è®ºæ”¯æŒå‚è€ƒæˆ‘ä¹‹å‰è½¬å‘çš„ä¸€ç¯‡åšæ–‡ï¼‰:åˆ¤åˆ«æ¨¡åž‹æœ€åŽä¸€å±‚ç›´æŽ¥ç”¨çº¿åž‹æ¿€æ´»å‡½æ•°ï¼Œè€Œä¸æ˜¯ç”¨Sigmoidå‡½æ•°123456789101112class Discriminator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Discriminator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.elu(self.map2(x)) # return F.sigmoid(self.map3(x)) return self.map3(x)ç”Ÿæˆæ¨¡åž‹ä¸Žåˆ¤åˆ«æ¨¡åž‹çš„losså‡½æ•°è¿›è¡Œä¿®æ”¹12345# ç”Ÿæˆæ¨¡åž‹# d_real_error = criterion(d_real_decision, Variable(torch.ones(1))) # ones = trued_real_error = -torch.mean(d_real_decision)# d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1))) # zeros = faked_fake_error = torch.mean(d_fake_decision)123# åˆ¤åˆ«æ¨¡åž‹# g_error = criterion(dg_fake_decision, Variable(torch.ones(1))) # we want to fool, so pretend it's all genuineg_error = -torch.mean(dg_fake_decision)æ¯æ¬¡æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°ä¹‹åŽæŠŠå®ƒä»¬çš„ç»å¯¹å€¼æˆªæ–­åˆ°ä¸è¶…è¿‡ä¸€ä¸ªå›ºå®šå¸¸æ•°c (è¿™é‡Œå–çš„æ˜¯0.01)123# Weight Clippingfor p in D.parameters(): p.data.clamp_(-0.01, 0.01)ä¸è¦ç”¨åŸºäºŽåŠ¨é‡çš„ä¼˜åŒ–ç®—æ³•ï¼ˆåŒ…æ‹¬momentumå’ŒAdamï¼‰ï¼ŒæŽ¨èRMSPropï¼ŒSGDä¹Ÿè¡Œ1234# d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)# g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate)â€‹å®žéªŒç»“æžœå¦‚ä¸‹ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152ewan@ubuntu:~/Documents/gan/pytorch-generative-adversarial-networks$ python wgan_pytorch.py Using data [Data and variances]0: D: -0.00291868206114/-0.0098686888814 G: 0.0101090818644 (Real: [3.9948547959327696, 1.1746644935894675], Fake: [-0.49681734740734101, 0.012067284766516822]) 200: D: -0.607654631138/0.150195807219 G: -0.148662015796 (Real: [3.8201908415555952, 1.2529761319208725], Fake: [1.3578049659729003, 0.068574913818859801]) 400: D: -0.463035583496/0.187745466828 G: -0.199109002948 (Real: [3.9679448902606964, 1.0966020511088672], Fake: [2.7924281167984009, 0.10128610818888226]) 600: D: -0.195529654622/-0.0762325078249 G: 0.0709114596248 (Real: [4.0289887523651124, 1.130490874393266], Fake: [3.2025665378570558, 0.11113662831727719]) 800: D: -0.267909675837/-0.0125531600788 G: 0.0149036226794 (Real: [3.8386318933963777, 1.1596351907184081], Fake: [2.9168305301666262, 0.18930262941797507]) 1000: D: -0.305421292782/0.0375043526292 G: -0.0430304855108 (Real: [4.036220012307167, 1.2074152140825467], Fake: [2.980299861431122, 0.34328656032877736]) 1200: D: -0.52364641428/0.34957420826 G: -0.336933553219 (Real: [4.2644650164060298, 1.3088487291781874], Fake: [3.5564545428752901, 0.93418534418781807]) 1400: D: 0.0167735591531/-0.0165516249835 G: 0.0153960846364 (Real: [4.005841153860092, 1.2205788960289556], Fake: [3.6258796131610871, 1.3573166859479273]) 1600: D: 0.00350501108915/-0.0680181980133 G: 0.0898797661066 (Real: [4.0096039956808092, 1.3040836884406217], Fake: [4.2868031549453738, 1.1195239069375269]) 1800: D: -0.017161777243/-0.0345846936107 G: 0.00348377227783 (Real: [3.8140131759643556, 1.2696980193364791], Fake: [3.6976867783069611, 1.3915195404268279]) 2000: D: 0.0342473760247/-0.0408688522875 G: 0.042895399034 (Real: [3.8277990472316743, 1.2935257967493754], Fake: [4.0553032100200657, 1.0920039067237071]) 2200: D: -0.0247789677233/-0.0973515734076 G: 0.0561916455626 (Real: [4.0955437314510341, 1.3877739508665123], Fake: [4.2196925377845762, 1.1830430815754616]) 2400: D: 0.0279140714556/-0.0485894307494 G: 0.051317743957 (Real: [4.1299532175064089, 1.2504224526907901], Fake: [3.6290897476673125, 1.4143234578612853]) 2600: D: -0.0277859847993/0.0174758173525 G: -0.0226532723755 (Real: [4.1205433750152585, 1.1041964193630893], Fake: [4.1067905998229977, 1.1112897398730086]) 2800: D: 0.0298485141248/-0.0404594913125 G: 0.0436173528433 (Real: [3.8474615824222567, 1.376119005659207], Fake: [4.1015409564971925, 1.1240560154112995]) 3000: D: -0.00891616754234/-0.0320432707667 G: -0.00200085714459 (Real: [4.2869654643535613, 1.2452766642692439], Fake: [4.0315418589115142, 1.1215360762164166]) 3200: D: 0.125043600798/-0.141845062375 G: 0.180229827762 (Real: [4.1041129958629607, 1.2669502216408666], Fake: [3.9350157177448271, 1.2041076720740758]) 3400: D: 0.00801010616124/-0.0085571501404 G: 0.00837498996407 (Real: [4.1750692510604859, 1.1555020360853467], Fake: [3.7647246885299683, 1.3171958013324914]) 3600: D: -0.0108975172043/0.00422720238566 G: 0.0679717883468 (Real: [4.2474800306558613, 1.1525478772018374], Fake: [3.9568253087997438, 1.2016376545965635]) 3800: D: 0.174184441566/-0.0896890684962 G: 0.132265836 (Real: [3.6444931725133212, 1.4372372290167961], Fake: [4.1011261808872224, 1.2724649929743026]) 4000: D: 0.0152352238074/-0.0211527496576 G: 0.0241769701242 (Real: [4.298748409748077, 1.2334686924805018], Fake: [3.8711180412769317, 1.2375391560481097]) 4200: D: 0.00989393051714/-0.00974932964891 G: 0.00978021323681 (Real: [3.8817882406711579, 1.2274675510251392], Fake: [4.4020989084243771, 1.1135816847780859]) 4400: D: 0.110887765884/-0.195888444781 G: 0.185447320342 (Real: [4.0501037514209743, 1.3391687317184524], Fake: [3.9222843647003174, 1.0870922014501809]) 4600: D: 0.0116609586403/0.0201185699552 G: -0.0251631941646 (Real: [4.097090389728546, 1.190104784646782], Fake: [4.0819661796092985, 1.3105115963188185]) 4800: D: 0.00524073652923/-0.00464708916843 G: 0.0057549579069 (Real: [3.8242294645309447, 1.2650652243397946], Fake: [4.1804288566112522, 1.2938617118884317]) 5000: D: -0.142288714647/0.0809833407402 G: -0.128578931093 (Real: [3.7870366251468659, 1.1074026548781364], Fake: [3.9050006806850432, 1.298625653396472]) 5200: D: 0.00282126059756/-0.000789406709373 G: 0.00220172246918 (Real: [3.8225140625238421, 1.2743034472730719], Fake: [4.1409763026237485, 1.1529764181372026]) 5400: D: 0.0688827335835/-0.143126890063 G: 0.177940413356 (Real: [3.9872682169079781, 1.3030584347635661], Fake: [4.1435868382453922, 1.1051301998899086]) 5600: D: -0.0711650624871/0.0871955379844 G: -0.134067937732 (Real: [3.9407234787940979, 1.1742557675838305], Fake: [4.2017855679988863, 1.2602829191705458]) 5800: D: 0.000587910413742/0.000934307463467 G: 0.00103192776442 (Real: [4.0573597419261933, 1.2623953329979454], Fake: [3.8340791404247283, 1.339685454959999]) 6000: D: 0.00821333751082/-0.12042221427 G: 0.0573511943221 (Real: [4.1211176145076749, 1.2369626300361085], Fake: [3.6600258636474607, 1.3520569881721223]) 6200: D: 0.00682129478082/0.001195830293 G: 0.00338123179972 (Real: [4.0544225633144375, 1.2749644040623289], Fake: [4.1039247584342959, 1.2693975476155579]) 6400: D: -0.00134055688977/0.00293467193842 G: -0.00249383598566 (Real: [4.0987548109889032, 1.4076174670922545], Fake: [3.8387181401252746, 1.0786043697026602]) 6600: D: -0.0879130512476/0.00771049968898 G: 0.0105132861063 (Real: [4.0482780200242994, 1.3183274437573238], Fake: [4.1890638065338131, 1.0659647273618436]) 6800: D: -0.0613053664565/0.00630968250334 G: 0.00345144513994 (Real: [3.9884191691875457, 1.2496578805847449], Fake: [4.0083020174503323, 1.1951200826269044]) 7000: D: -0.00451065413654/0.0126703362912 G: -0.0153036154807 (Real: [4.1685840785503387, 1.0996732796623405], Fake: [3.8199899888038633, 1.3533216043161698]) 7200: D: -0.00164794549346/-0.026672417298 G: 0.00926311034709 (Real: [3.9697488701343535, 1.1614389493998623], Fake: [4.0069102811813355, 1.332521020789126]) 7400: D: 0.0479753166437/-0.00875021051615 G: 0.0273390654474 (Real: [3.9136831092834474, 1.3941734665017038], Fake: [3.9792356503009798, 1.2934269648663987]) 7600: D: 0.0299390181899/-0.0244860406965 G: 0.0235633179545 (Real: [3.9529241484403612, 1.3003400363613378], Fake: [4.1008431494235991, 1.1966721541073959]) 7800: D: -0.106096304953/-0.00319136725739 G: 0.0128062078729 (Real: [3.8472019118070602, 1.3776392180901436], Fake: [3.9847766911983489, 1.1441746730859625]) 8000: D: -0.0541454330087/0.0360651388764 G: -0.0368629023433 (Real: [4.001156520843506, 1.2686070678293795], Fake: [3.7170648825168611, 1.2630303399418346]) 8200: D: 0.0385981723666/-0.0308057032526 G: 0.0258536860347 (Real: [4.0773776215314861, 1.1340129155680212], Fake: [4.025383379459381, 1.327217397616157]) 8400: D: 0.0323679596186/-0.0363558754325 G: 0.0379030331969 (Real: [4.068932784795761, 1.1369141540559231], Fake: [3.9889052593708039, 1.292853623065962]) 8600: D: -0.00726405344903/-0.0198955982924 G: -0.0463897511363 (Real: [4.1387977415323256, 1.2983278993502099], Fake: [3.9634271264076233, 1.2541944672524785]) 8800: D: 0.0214307252318/-0.0323143824935 G: 0.0147992642596 (Real: [3.8878944924473764, 1.2858782523769321], Fake: [3.9738967609405518, 1.2617951400969825]) 9000: D: 0.0408670082688/-0.0408971831203 G: 0.0338222235441 (Real: [3.8935359448194502, 1.2102182389881371], Fake: [4.1026345968246458, 1.1619291320679421]) 9200: D: 0.0334619283676/-0.0487795248628 G: 0.043896459043 (Real: [4.0024692767858507, 1.3035652548917089], Fake: [4.2494437253475192, 1.1284849306040097]) 9400: D: -0.0662252604961/0.0567465648055 G: -0.0975001305342 (Real: [3.9983484780788423, 1.2727864024938771], Fake: [4.1652837800979619, 1.2757452301144367]) 9600: D: -0.0437398403883/0.0547546446323 G: -0.0755473896861 (Real: [3.9568819630146028, 1.2089398910557572], Fake: [4.0577589499950406, 1.254854081501209]) 9800: D: 0.00763822672889/-0.00536214653403 G: 0.00614025257528 (Real: [4.0391950635612011, 1.3067671354062065], Fake: [3.8441065263748171, 1.3304282270617658]) 10000: D: 0.0420219749212/-0.000623900443316 G: 0.0955700650811 (Real: [4.0145307508111001, 1.2332284552616837], Fake: [4.1720886218547824, 1.3184165599194013]) 10200: D: -0.0580518990755/-0.0247586201876 G: 0.0602744668722 (Real: [3.9131186211109164, 1.1547087942243295], Fake: [3.8442363095283509, 1.3100046689992075]) 10400: D: 0.0350324884057/-0.0446610674262 G: 0.0443669557571 (Real: [3.9732863992452621, 1.0900301299537192], Fake: [4.1616083049774169, 1.1977412391369193]) 10600: D: 0.0309124011546/-0.0327286012471 G: 0.0324002951384 (Real: [4.1375643616914752, 1.3491791182650394], Fake: [4.1360740911960603, 1.2026694938475944]) 10800: D: 0.0251356009394/-0.0600365921855 G: 0.0182816889137 (Real: [3.9463955080509185, 1.209152327657528], Fake: [4.0492063975334167, 1.1931266255697688]) 11000: D: -0.0226037632674/0.0645630285144 G: -0.00730620510876 (Real: [4.0881260240077975, 1.1610880829221104], Fake: [4.1015665113925932, 1.2508656591000114]) 11200: D: -0.203874662519/0.129180550575 G: -0.137796327472 (Real: [3.9598375034332274, 1.3812077142172803], Fake: [4.0204527139663693, 1.2581185304639424]) 11400: D: -0.0908113643527/0.0762611478567 G: -0.0800914615393 (Real: [4.0449822235107424, 1.3556268019161497], Fake: [3.6170706963539123, 1.2538775159913775]) 11600: D: 0.0127945197746/-0.0136474575847 G: 0.0115108992904 (Real: [3.8434849847108126, 1.4191038384690144], Fake: [3.6834572017192841, 1.3749317238019667]) 11800: D: -0.0162955205888/0.00703074596822 G: 0.0635928660631 (Real: [4.0656388866901398, 1.1733235519103811], Fake: [4.2119219648838042, 1.2884029757138897]) 12000: D: 0.00804834254086/0.0114726442844 G: -0.0416676998138 (Real: [4.0812106788158413, 1.2768383065648503], Fake: [3.8802548873424532, 1.1682818121544778]) 12200: D: 0.00880087539554/-0.00853784382343 G: 0.00878115184605 (Real: [3.9501210238039492, 1.2609298922930623], Fake: [4.016851776838303, 1.1958214043365074]) 12400: D: -0.0908231809735/0.0565089061856 G: -0.0594271346927 (Real: [4.2189184671640394, 1.2027120432908258], Fake: [4.0232754671573643, 1.0601718488768348]) 12600: D: 0.0851941630244/-0.0584048479795 G: 0.0588090792298 (Real: [3.7772543743252753, 1.130624908263915], Fake: [3.9319257283210756, 1.2051865367836399]) 12800: D: -0.0560997053981/-0.0248175561428 G: -0.0423211455345 (Real: [4.1257915179431439, 1.3557555020469465], Fake: [3.9178791642189026, 1.1446278900771538]) 13000: D: -0.021879715845/-0.0102085536346 G: 0.049164660275 (Real: [3.8891402572393416, 1.340302981622111], Fake: [4.1098264539241791, 1.1973190716986095]) 13200: D: 0.00609071925282/0.000411780551076 G: 0.000873317010701 (Real: [4.0079734873771669, 1.0734378076269375], Fake: [4.16044829249382, 1.24589904041035]) 13400: D: 0.0619652941823/-0.0918542221189 G: 0.0685269758105 (Real: [4.0059312301874161, 1.2294789910478197], Fake: [3.935395474433899, 1.2204450041984987]) 13600: D: -0.0172225553542/0.0116953141987 G: -0.0139160379767 (Real: [3.9669277960062028, 1.2823045137798716], Fake: [3.9422059106826781, 1.1863138013678882]) 13800: D: -0.0343380719423/-0.0341883003712 G: 0.0315745696425 (Real: [3.9349321211874484, 1.3515663905606217], Fake: [4.0361522984504701, 1.1889982801815446]) 14000: D: -0.0781251713634/0.0379043146968 G: -0.0811991766095 (Real: [3.9622140777111055, 1.3270647840200485], Fake: [3.958692445755005, 1.1882249562538854]) 14200: D: -0.00332566350698/0.00831608474255 G: -0.00968919880688 (Real: [4.0868309581279751, 1.2649052154720533], Fake: [3.9996533656120299, 1.2424544463340046]) 14400: D: 0.00310544949025/-0.00344840623438 G: 0.002937767189 (Real: [3.9016156983375549, 1.3394072373207904], Fake: [3.8578492951393129, 1.2802578210924642]) 14600: D: 0.00954662263393/-0.00955961830914 G: 0.00952168926597 (Real: [3.951248247921467, 1.3720542385537113], Fake: [3.9343765902519228, 1.3196731296807518]) 14800: D: -0.118950776756/-0.0234697107226 G: -0.0475859940052 (Real: [4.224924056529999, 1.2198087928062376], Fake: [3.8152624690532684, 1.407979253312801]) 15000: D: -0.0943605676293/0.0735622048378 G: -0.104274556041 (Real: [3.8776874673366546, 1.2303474890793162], Fake: [3.8042025637626646, 1.2641632638711853]) 15200: D: -0.000172574073076/-0.0136091653258 G: -0.0342488661408 (Real: [3.9725669431686401, 1.3636566655582356], Fake: [3.7739255595207215, 1.286560381931142]) 15400: D: 0.0314685925841/-0.0321847423911 G: 0.0224884226918 (Real: [3.9619563330709933, 1.191049295263032], Fake: [3.7949125266075132, 1.144446158701051]) 15600: D: 0.00764724984765/-0.00575984269381 G: 0.0064948592335 (Real: [3.7679578655958177, 1.3149928065248815], Fake: [4.2461013138294224, 1.0951171764483221]) 15800: D: -0.0777092948556/0.0849689692259 G: -0.0924058929086 (Real: [3.932852659225464, 1.2573061632959293], Fake: [4.1913282787799835, 1.2836186853339466]) 16000: D: -0.050300322473/-0.0388206243515 G: 0.0357397347689 (Real: [4.0962446802854542, 1.4029011906591213], Fake: [4.070586755275726, 1.1271350494375147]) 16200: D: 0.0753296241164/-0.0198806431144 G: 0.0808434784412 (Real: [3.8760965394973756, 1.1409524988246751], Fake: [3.8057461333274842, 1.2098168757605468]) 16400: D: -0.0372299104929/0.0351875349879 G: -0.0454745069146 (Real: [4.0939353704452515, 1.2848196043395506], Fake: [3.9558720147609709, 1.2728235384902225]) 16600: D: -0.0101340338588/0.0110626723617 G: -0.0111222248524 (Real: [3.986977145075798, 1.3259823635587689], Fake: [3.9554380464553831, 1.2907862191410846]) 16800: D: -0.0494117587805/0.0523075163364 G: -0.0535500720143 (Real: [3.8448826253414152, 1.3117905469567066], Fake: [3.7438095784187317, 1.2535150365672076]) 17000: D: 0.0156182665378/-0.0128254238516 G: 0.0146374739707 (Real: [3.9421124708652497, 1.1052540236280552], Fake: [3.8871842885017394, 1.2453511923222738]) 17200: D: 0.0429224148393/-0.0480623096228 G: 0.0399292707443 (Real: [3.9799196243286135, 1.2941615666073001], Fake: [4.1375756561756134, 1.2109081564509361]) 17400: D: 0.00968278944492/-0.00968171562999 G: 0.00966327264905 (Real: [3.935849468111992, 1.2695645007229639], Fake: [3.8996728241443632, 1.3144268300578967]) 17600: D: -0.00301436148584/-0.000785265117884 G: 0.00103102996945 (Real: [3.9284519279003143, 1.2341036313393001], Fake: [3.6972431838512421, 1.3855687155856462]) 17800: D: 0.116903491318/-0.0937560945749 G: 0.172590240836 (Real: [4.2645069471001626, 1.3080363040531007], Fake: [3.9567726898193358, 1.2967345311449683]) 18000: D: -0.0608675032854/0.0476493611932 G: -0.00500288326293 (Real: [4.0269851100444791, 1.2116770270672328], Fake: [4.1152600276470181, 1.281199668474674]) 18200: D: -0.0734401643276/0.0987718477845 G: -0.0819599106908 (Real: [3.8394976514577865, 1.2749873300796422], Fake: [4.0419886147975923, 1.327963817546014]) 18400: D: 0.0497582927346/-0.155175164342 G: 0.13303783536 (Real: [3.7719902545213699, 1.0897407967420649], Fake: [3.7615046393871308, 1.3089916470515932]) 18600: D: 0.0239700898528/-0.0381186343729 G: 0.0276864990592 (Real: [4.188409751355648, 1.285584105229516], Fake: [4.0233318042755126, 1.2681527004757882]) 18800: D: 0.00111512281001/-0.0264507420361 G: 0.0286112166941 (Real: [3.9199141567945479, 1.2738313063627613], Fake: [4.1139781177043915, 1.330488711219485]) 19000: D: -0.0473541393876/0.111352369189 G: -0.0523310601711 (Real: [3.7932651308923959, 1.3147127405682739], Fake: [3.7947627007961273, 1.0531299503292175]) 19200: D: -0.0304779503495/0.045797213912 G: -0.0440187454224 (Real: [4.0896886540949344, 1.3392233824907658], Fake: [3.8646358847618103, 1.304593284039177]) 19400: D: 0.194737583399/-0.192367076874 G: 0.230072781444 (Real: [3.9661449289321897, 1.2822216197459986], Fake: [4.0850893747806545, 1.3070266600721223]) 19600: D: -0.195656016469/0.194369539618 G: -0.204969212413 (Real: [3.9445683220028878, 1.2908669424594961], Fake: [4.0273511683940884, 1.3484937484757897]) 19800: D: 0.276149004698/-0.262592494488 G: 0.261271834373 (Real: [3.9244625726342202, 1.2138755313418907], Fake: [3.896045311689377, 1.3239168205792633]) 20000: D: -0.037402831018/0.0541176348925 G: -0.0254273694009 (Real: [3.7887831997871397, 1.0838328443531984], Fake: [4.1803205323219297, 1.2069399210575202]) 20200: D: -0.14391182363/0.154710128903 G: -0.127932995558 (Real: [3.9718186306953429, 1.1938920103826984], Fake: [3.8623993241786958, 1.1992380687067719]) 20400: D: 0.277315825224/-0.276595175266 G: 0.280247867107 (Real: [3.9932824140787124, 1.2951435399231526], Fake: [3.9807376277446749, 1.1784780448683547]) 20600: D: -0.213297829032/0.245908752084 G: -0.243222758174 (Real: [3.8720276713371278, 1.2542419688526467], Fake: [3.8206098222732545, 1.1661960388796837]) 20800: D: 0.114619217813/-0.100926779211 G: 0.0922625884414 (Real: [3.9682870441675187, 1.3188621677189192], Fake: [3.5771069145202636, 1.1369803011602813]) 21000: D: -0.303231596947/0.294602781534 G: -0.288874447346 (Real: [3.991482014656067, 1.0697520343686426], Fake: [3.674229063987732, 1.162594834704991]) 21200: D: -0.074034973979/0.0798109993339 G: -0.0742214098573 (Real: [3.5809044003486634, 1.1568557007313405], Fake: [4.0297869884967801, 1.262183063172349]) 21400: D: 0.262162327766/-0.297971874475 G: 0.296678453684 (Real: [4.0233621561527251, 1.1153293685921177], Fake: [4.3256152606010438, 1.293378983535336]) 21600: D: 0.253285288811/-0.265974611044 G: 0.271079391241 (Real: [3.8655495065450669, 1.3046362904478612], Fake: [4.0383575105667111, 1.1593536714254398]) 21800: D: -0.668483495712/0.693548798561 G: -0.597621560097 (Real: [4.0561192989349362, 1.3785832256993071], Fake: [4.0196917986869813, 1.1727416034901368]) 22000: D: -0.247271433473/0.260498434305 G: -0.254284113646 (Real: [4.0449540507793422, 1.1182831642815363], Fake: [3.9410277414321899, 1.35662918383663]) 22200: D: 0.0106530245394/-0.0105826444924 G: 0.010412142612 (Real: [3.9709725368022917, 1.1935909496194108], Fake: [3.6618342864513398, 1.1302755516153604]) 22400: D: -0.0474079549313/0.0512998178601 G: -0.0483585894108 (Real: [4.0366528975963591, 1.255590190060166], Fake: [4.4536384451389317, 1.1817009846117434]) 22600: D: -0.322408914566/0.294503211975 G: -0.294557034969 (Real: [4.1648625326156612, 1.2910376071493044], Fake: [3.9514351594448089, 1.2428792207747439]) 22800: D: -0.0832418426871/0.0778618454933 G: -0.0830294713378 (Real: [4.1286677682399748, 1.2808552112825371], Fake: [4.0503418278694152, 1.2931609764101457]) 23000: D: -0.369321852922/0.350715816021 G: -0.379378199577 (Real: [4.0539671546220779, 1.2841527209665038], Fake: [3.7385779893398285, 1.226034767157562]) 23200: D: -0.20978730917/0.198253154755 G: -0.20125605166 (Real: [3.8997612628340721, 1.2476609639285596], Fake: [3.9131766259670258, 1.1745094337139723]) 23400: D: -0.0713088735938/0.070287771523 G: -0.0685144215822 (Real: [3.8823761761188509, 1.2554855061572396], Fake: [3.916521146297455, 1.1589148704590277]) 23600: D: 0.0427192002535/-0.0458992123604 G: 0.0468493178487 (Real: [4.2497683775424955, 1.3534774394799314], Fake: [3.7455072367191313, 1.2035723328660535]) 23800: D: 0.0886824280024/-0.089180290699 G: 0.0824339240789 (Real: [4.1368276840448379, 1.3053732424006685], Fake: [3.7440953600406646, 1.3403098424499473]) 24000: D: 0.0765529945493/-0.0702198073268 G: 0.067143753171 (Real: [4.1424573111534118, 1.1894154051554844], Fake: [3.9408028304576872, 1.311870950939225]) 24200: D: -0.0332999974489/0.0289861243218 G: -0.0238233078271 (Real: [4.0625021523237228, 1.3193496247910601], Fake: [4.0214765596389768, 1.3626613178115112]) 24400: D: 0.0116833550856/-0.0433083474636 G: 0.0294151268899 (Real: [4.155729653835297, 1.2443573708805233], Fake: [4.0276014816761014, 1.2064370896635035]) 24600: D: -0.143586605787/0.176585748792 G: -0.18224260211 (Real: [4.1486411762237552, 1.1859516848633762], Fake: [4.1132693731784817, 1.1922180729014844]) 24800: D: -0.0138712525368/0.0168411824852 G: -0.0119427125901 (Real: [4.1591709744930263, 1.2359258557380455], Fake: [4.1677398359775539, 1.3845231707709731]) 25000: D: 0.255919009447/-0.294253230095 G: 0.279962956905 (Real: [3.9463270044326784, 1.1874795319708413], Fake: [4.2903580510616299, 1.3555421660554561]) 25200: D: -0.0276325326413/0.0174208488315 G: -0.0236964281648 (Real: [3.9243721216917038, 1.0837602743237815], Fake: [3.6880193889141082, 1.3551960082382857]) 25400: D: 0.0133695462719/-0.0217840373516 G: 0.0382910817862 (Real: [3.9248281943798067, 1.3498579423514441], Fake: [3.9377611076831815, 1.3147392264391]) 25600: D: 0.0533282607794/-0.0582511797547 G: 0.0426382124424 (Real: [3.9252138528227807, 1.2343049898537437], Fake: [4.1364144349098204, 1.2410536065514364]) 25800: D: -0.00288704037666/0.00770187750459 G: -0.0114914979786 (Real: [3.9242496091127395, 1.2788150012319115], Fake: [4.0345127677917478, 1.1882337663095883]) 26000: D: -0.0608727261424/0.0541118755937 G: -0.0474198237062 (Real: [4.0897465288639072, 1.3095601996023096], Fake: [4.1400825273990627, 1.2148829163174772]) 26200: D: -0.130559697747/0.0733794793487 G: -0.104144588113 (Real: [4.2607862049341199, 1.2942193499055861], Fake: [3.8867506885528567, 1.1942672801186012]) 26400: D: -0.0439343079925/0.0573879256845 G: -0.0878697857261 (Real: [3.7808335113525389, 1.0880880845236942], Fake: [3.9782328522205352, 1.1620106342824015]) 26600: D: 0.0152015341446/0.00366508681327 G: 0.041159953922 (Real: [3.8900859886407853, 1.1779470629112894], Fake: [3.7596992158889773, 1.2139592079531667]) 26800: D: 0.0352714285254/-0.1031877473 G: 0.067874789238 (Real: [4.0695308989286421, 1.1837713697563146], Fake: [4.0929770147800442, 1.0965869589580517]) 27000: D: -0.0881021544337/0.0813493356109 G: -0.0242269244045 (Real: [3.9890777540206908, 1.2553969722414431], Fake: [3.7988330614566803, 1.2567013288504758]) 27200: D: 0.0763045027852/-0.0917293503881 G: 0.114218316972 (Real: [4.0028738850355152, 1.3423566094628674], Fake: [3.9770897746086122, 1.3219552807466088]) 27400: D: 0.0594872310758/-0.0451167076826 G: 0.0368666872382 (Real: [4.0800592017173765, 1.2152901513624952], Fake: [3.9476736617088317, 1.2989705597833583]) 27600: D: 0.0153470486403/-0.0201481245458 G: -0.000402322039008 (Real: [4.1604018148779867, 1.3359014716469342], Fake: [3.9977971708774565, 1.2944576179632961]) 27800: D: -0.00789823569357/0.00908922962844 G: -0.0111076626927 (Real: [4.0212037134170533, 1.1874018724012747], Fake: [4.1083386635780332, 1.2509297017041943]) 28000: D: 0.00757996272296/-0.00654019229114 G: 0.00611820165068 (Real: [3.7911120998859404, 1.1977103659955959], Fake: [4.0841165268421173, 1.1898253993115502]) 28200: D: 0.0131957577541/0.00322831980884 G: -0.00111622922122 (Real: [4.1888789300620557, 1.3496568725947327], Fake: [4.0611115002632143, 1.3183184144220856]) 28400: D: -0.0306499581784/0.0331647247076 G: -0.0338053703308 (Real: [4.1849153059720994, 1.3391440077022734], Fake: [3.8500063753128053, 1.3092803392722017]) 28600: D: -0.0750854164362/0.0745137408376 G: -0.0692436397076 (Real: [4.2219353467226028, 1.3228632865628431], Fake: [3.9156518685817718, 1.322625042830196]) 28800: D: 0.0400990955532/-0.0271217841655 G: 0.0072197439149 (Real: [4.1668396210670471, 1.1685380084057959], Fake: [3.8380984902381896, 1.362370341203504]) 29000: D: -0.0643707811832/0.0576644167304 G: -0.100686855614 (Real: [3.8912058281898498, 1.1764897014192157], Fake: [4.1498241519927976, 1.2432322677870791]) 29200: D: 0.0442187860608/-0.0331076569855 G: 0.0377507209778 (Real: [3.995900819301605, 1.1999502583881319], Fake: [3.9349853229522704, 1.3676764998638458]) 29400: D: -0.0614512637258/0.0583380833268 G: -0.059112302959 (Real: [4.1833238875865932, 1.4038158613161691], Fake: [4.1426575899124147, 1.2694314780433735]) 29600: D: -0.0337703973055/0.0392336845398 G: -0.0504648312926 (Real: [4.1217511665821078, 1.2264251023812502], Fake: [3.838116307258606, 1.2309841481033876]) 29800: D: 0.129453405738/-0.13672092557 G: 0.143395990133 (Real: [3.8660407388210296, 1.2221890139039508], Fake: [4.0156518769264222, 1.3044469158238432])]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The awesome Wasserstein GAN]]></title>
      <url>%2F2017%2F04%2F29%2FThe-awesome-Wasserstein-GAN%2F</url>
      <content type="text"><![CDATA[åŽŸå¸–åœ°å€ï¼šhttps://zhuanlan.zhihu.com/p/25071913æœ¬æ–‡åŽç»­ï¼šWasserstein GANæœ€æ–°è¿›å±•ï¼šä»Žweight clippingåˆ°gradient penaltyï¼Œæ›´åŠ å…ˆè¿›çš„Lipschitzé™åˆ¶æ‰‹æ³•åœ¨GANçš„ç›¸å…³ç ”ç©¶å¦‚ç«å¦‚è¼ç”šè‡³å¯ä»¥è¯´æ˜¯æ³›æ»¥çš„ä»Šå¤©ï¼Œä¸€ç¯‡æ–°é²œå‡ºç‚‰çš„arXivè®ºæ–‡ã€ŠWasserstein GANã€‹å´åœ¨Redditçš„Machine Learningé¢‘é“ç«äº†ï¼Œè¿žGoodfellowéƒ½åœ¨å¸–å­é‡Œå’Œå¤§å®¶çƒ­çƒˆè®¨è®ºï¼Œè¿™ç¯‡è®ºæ–‡ç©¶ç«Ÿæœ‰ä»€ä¹ˆäº†ä¸å¾—çš„åœ°æ–¹å‘¢ï¼Ÿè¦çŸ¥é“è‡ªä»Ž2014å¹´Ian Goodfellowæå‡ºä»¥æ¥ï¼ŒGANå°±å­˜åœ¨ç€è®­ç»ƒå›°éš¾ã€ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„lossæ— æ³•æŒ‡ç¤ºè®­ç»ƒè¿›ç¨‹ã€ç”Ÿæˆæ ·æœ¬ç¼ºä¹å¤šæ ·æ€§ç­‰é—®é¢˜ã€‚ä»Žé‚£æ—¶èµ·ï¼Œå¾ˆå¤šè®ºæ–‡éƒ½åœ¨å°è¯•è§£å†³ï¼Œä½†æ˜¯æ•ˆæžœä¸å°½äººæ„ï¼Œæ¯”å¦‚æœ€æœ‰åçš„ä¸€ä¸ªæ”¹è¿›DCGANä¾é çš„æ˜¯å¯¹åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨çš„æž¶æž„è¿›è¡Œå®žéªŒæžšä¸¾ï¼Œæœ€ç»ˆæ‰¾åˆ°ä¸€ç»„æ¯”è¾ƒå¥½çš„ç½‘ç»œæž¶æž„è®¾ç½®ï¼Œä½†æ˜¯å®žé™…ä¸Šæ˜¯æ²»æ ‡ä¸æ²»æœ¬ï¼Œæ²¡æœ‰å½»åº•è§£å†³é—®é¢˜ã€‚è€Œä»Šå¤©çš„ä¸»è§’Wasserstein GANï¼ˆä¸‹é¢ç®€ç§°WGANï¼‰æˆåŠŸåœ°åšåˆ°äº†ä»¥ä¸‹çˆ†ç‚¸æ€§çš„å‡ ç‚¹ï¼šå½»åº•è§£å†³GANè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œä¸å†éœ€è¦å°å¿ƒå¹³è¡¡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„è®­ç»ƒç¨‹åº¦åŸºæœ¬è§£å†³äº†collapse modeçš„é—®é¢˜ï¼Œç¡®ä¿äº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§è®­ç»ƒè¿‡ç¨‹ä¸­ç»ˆäºŽæœ‰ä¸€ä¸ªåƒäº¤å‰ç†µã€å‡†ç¡®çŽ‡è¿™æ ·çš„æ•°å€¼æ¥æŒ‡ç¤ºè®­ç»ƒçš„è¿›ç¨‹ï¼Œè¿™ä¸ªæ•°å€¼è¶Šå°ä»£è¡¨GANè®­ç»ƒå¾—è¶Šå¥½ï¼Œä»£è¡¨ç”Ÿæˆå™¨äº§ç”Ÿçš„å›¾åƒè´¨é‡è¶Šé«˜ï¼ˆå¦‚é¢˜å›¾æ‰€ç¤ºï¼‰ä»¥ä¸Šä¸€åˆ‡å¥½å¤„ä¸éœ€è¦ç²¾å¿ƒè®¾è®¡çš„ç½‘ç»œæž¶æž„ï¼Œæœ€ç®€å•çš„å¤šå±‚å…¨è¿žæŽ¥ç½‘ç»œå°±å¯ä»¥åšåˆ°é‚£ä»¥ä¸Šå¥½å¤„æ¥è‡ªå“ªé‡Œï¼Ÿè¿™å°±æ˜¯ä»¤äººæ‹æ¡ˆå«ç»çš„éƒ¨åˆ†äº†â€”â€”å®žé™…ä¸Šä½œè€…æ•´æ•´èŠ±äº†ä¸¤ç¯‡è®ºæ–‡ï¼Œåœ¨ç¬¬ä¸€ç¯‡ã€ŠTowards Principled Methods for Training Generative Adversarial Networksã€‹é‡Œé¢æŽ¨äº†ä¸€å †å…¬å¼å®šç†ï¼Œä»Žç†è®ºä¸Šåˆ†æžäº†åŽŸå§‹GANçš„é—®é¢˜æ‰€åœ¨ï¼Œä»Žè€Œé’ˆå¯¹æ€§åœ°ç»™å‡ºäº†æ”¹è¿›è¦ç‚¹ï¼›åœ¨è¿™ç¬¬äºŒç¯‡ã€ŠWasserstein GANã€‹é‡Œé¢ï¼Œåˆå†ä»Žè¿™ä¸ªæ”¹è¿›ç‚¹å‡ºå‘æŽ¨äº†ä¸€å †å…¬å¼å®šç†ï¼Œæœ€ç»ˆç»™å‡ºäº†æ”¹è¿›çš„ç®—æ³•å®žçŽ°æµç¨‹ï¼Œè€Œæ”¹è¿›åŽç›¸æ¯”åŽŸå§‹GANçš„ç®—æ³•å®žçŽ°æµç¨‹å´åªæ”¹äº†å››ç‚¹ï¼šåˆ¤åˆ«å™¨æœ€åŽä¸€å±‚åŽ»æŽ‰sigmoidç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„lossä¸å–logæ¯æ¬¡æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°ä¹‹åŽæŠŠå®ƒä»¬çš„ç»å¯¹å€¼æˆªæ–­åˆ°ä¸è¶…è¿‡ä¸€ä¸ªå›ºå®šå¸¸æ•°cä¸è¦ç”¨åŸºäºŽåŠ¨é‡çš„ä¼˜åŒ–ç®—æ³•ï¼ˆåŒ…æ‹¬momentumå’ŒAdamï¼‰ï¼ŒæŽ¨èRMSPropï¼ŒSGDä¹Ÿè¡Œç®—æ³•æˆªå›¾å¦‚ä¸‹ï¼šæ”¹åŠ¨æ˜¯å¦‚æ­¤ç®€å•ï¼Œæ•ˆæžœå´æƒŠäººåœ°å¥½ï¼Œä»¥è‡³äºŽRedditä¸Šä¸å°‘äººåœ¨æ„Ÿå¹ï¼šå°±è¿™æ ·ï¼Ÿæ²¡æœ‰åˆ«çš„äº†ï¼Ÿ å¤ªç®€å•äº†å§ï¼è¿™äº›ååº”è®©æˆ‘æƒ³èµ·äº†ä¸€ä¸ªé¢‡æœ‰å¹´å¤´çš„é¸¡æ±¤æ®µå­ï¼Œè¯´æ˜¯ä¸€ä¸ªå·¥ç¨‹å¸ˆåœ¨ç”µæœºå¤–å£³ä¸Šç”¨ç²‰ç¬”åˆ’äº†ä¸€æ¡çº¿æŽ’é™¤äº†æ•…éšœï¼Œè¦ä»·ä¸€ä¸‡ç¾Žå…ƒâ€”â€”ç”»ä¸€æ¡çº¿ï¼Œ1ç¾Žå…ƒï¼›çŸ¥é“åœ¨å“ªç”»çº¿ï¼Œ9999ç¾Žå…ƒã€‚ä¸Šé¢è¿™å››ç‚¹æ”¹è¿›å°±æ˜¯ä½œè€…Martin Arjovskyåˆ’çš„ç®€ç®€å•å•å››æ¡çº¿ï¼Œå¯¹äºŽå·¥ç¨‹å®žçŽ°ä¾¿å·²è¶³å¤Ÿï¼Œä½†æ˜¯çŸ¥é“åœ¨å“ªåˆ’çº¿ï¼ŒèƒŒåŽå´æ˜¯ç²¾å·§çš„æ•°å­¦åˆ†æžï¼Œè€Œè¿™ä¹Ÿæ˜¯æœ¬æ–‡æƒ³è¦æ•´ç†çš„å†…å®¹ã€‚æœ¬æ–‡å†…å®¹åˆ†ä¸ºäº”ä¸ªéƒ¨åˆ†ï¼šåŽŸå§‹GANç©¶ç«Ÿå‡ºäº†ä»€ä¹ˆé—®é¢˜ï¼Ÿï¼ˆæ­¤éƒ¨åˆ†è¾ƒé•¿ï¼‰WGANä¹‹å‰çš„ä¸€ä¸ªè¿‡æ¸¡è§£å†³æ–¹æ¡ˆWassersteinè·ç¦»çš„ä¼˜è¶Šæ€§è´¨ä»ŽWassersteinè·ç¦»åˆ°WGANæ€»ç»“ç†è§£åŽŸæ–‡çš„å¾ˆå¤šå…¬å¼å®šç†éœ€è¦å¯¹æµ‹åº¦è®ºã€ æ‹“æ‰‘å­¦ç­‰æ•°å­¦çŸ¥è¯†æœ‰æ‰€æŽŒæ¡ï¼Œæœ¬æ–‡ä¼šä»Žç›´è§‚çš„è§’åº¦å¯¹æ¯ä¸€ä¸ªé‡è¦å…¬å¼è¿›è¡Œè§£è¯»ï¼Œæœ‰æ—¶é€šè¿‡ä¸€äº›ä½Žç»´çš„ä¾‹å­å¸®åŠ©è¯»è€…ç†è§£æ•°å­¦èƒŒåŽçš„æ€æƒ³ï¼Œæ‰€ä»¥ä¸å…ä¼šå¤±äºŽä¸¥è°¨ï¼Œå¦‚æœ‰å¼•å–»ä¸å½“ä¹‹å¤„ï¼Œæ¬¢è¿Žåœ¨è¯„è®ºä¸­æŒ‡å‡ºã€‚ä»¥ä¸‹ç®€ç§°ã€ŠWassertein GANã€‹ä¸ºâ€œWGANæœ¬ä½œâ€ï¼Œç®€ç§°ã€ŠTowards Principled Methods for Training Generative Adversarial Networksã€‹ä¸ºâ€œWGANå‰ä½œâ€ã€‚WGANæºç å®žçŽ°ï¼šmartinarjovsky/WassersteinGANç¬¬ä¸€éƒ¨åˆ†ï¼šåŽŸå§‹GANç©¶ç«Ÿå‡ºäº†ä»€ä¹ˆé—®é¢˜ï¼Ÿå›žé¡¾ä¸€ä¸‹ï¼ŒåŽŸå§‹GANä¸­åˆ¤åˆ«å™¨è¦æœ€å°åŒ–å¦‚ä¸‹æŸå¤±å‡½æ•°ï¼Œå°½å¯èƒ½æŠŠçœŸå®žæ ·æœ¬åˆ†ä¸ºæ­£ä¾‹ï¼Œç”Ÿæˆæ ·æœ¬åˆ†ä¸ºè´Ÿä¾‹ï¼š$$-\mathbb{E}_{x \sim P_r}[log D(x)] - \mathbb{E}_{x \sim P_{g}}[log(1-D(x))]$$å…¶ä¸­$P_r$æ˜¯çœŸå®žæ ·æœ¬åˆ†å¸ƒï¼Œ$P_g$æ˜¯ç”±ç”Ÿæˆå™¨äº§ç”Ÿçš„æ ·æœ¬åˆ†å¸ƒã€‚å¯¹äºŽç”Ÿæˆå™¨ï¼ŒGoodfellowä¸€å¼€å§‹æå‡ºæ¥ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼ŒåŽæ¥åˆæå‡ºäº†ä¸€ä¸ªæ”¹è¿›çš„æŸå¤±å‡½æ•°ï¼Œåˆ†åˆ«æ˜¯$$ \mathbb{E}_{x \sim P_g}[log(1-D(x))]$$$$ \mathbb{E}_{x \sim P_g}[-log D(x)]$$åŽè€…åœ¨WGANä¸¤ç¯‡è®ºæ–‡ä¸­ç§°ä¸ºâ€œthe - log D alternativeâ€æˆ–â€œthe - log D trickâ€ã€‚WGANå‰ä½œåˆ†åˆ«åˆ†æžäº†è¿™ä¸¤ç§å½¢å¼çš„åŽŸå§‹GANå„è‡ªçš„é—®é¢˜æ‰€åœ¨ï¼Œä¸‹é¢åˆ†åˆ«è¯´æ˜Žã€‚ç¬¬ä¸€ç§åŽŸå§‹GANå½¢å¼çš„é—®é¢˜ä¸€å¥è¯æ¦‚æ‹¬ï¼šåˆ¤åˆ«å™¨è¶Šå¥½ï¼Œç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±è¶Šä¸¥é‡ã€‚WGANå‰ä½œä»Žä¸¤ä¸ªè§’åº¦è¿›è¡Œäº†è®ºè¯ï¼Œç¬¬ä¸€ä¸ªè§’åº¦æ˜¯ä»Žç”Ÿæˆå™¨çš„ç­‰ä»·æŸå¤±å‡½æ•°åˆ‡å…¥çš„ã€‚é¦–å…ˆä»Žå…¬å¼1å¯ä»¥å¾—åˆ°ï¼Œåœ¨ç”Ÿæˆå™¨Gå›ºå®šå‚æ•°æ—¶æœ€ä¼˜çš„åˆ¤åˆ«å™¨Dåº”è¯¥æ˜¯ä»€ä¹ˆã€‚å¯¹äºŽä¸€ä¸ªå…·ä½“çš„æ ·æœ¬$x$ï¼Œå®ƒå¯èƒ½æ¥è‡ªçœŸå®žåˆ†å¸ƒä¹Ÿå¯èƒ½æ¥è‡ªç”Ÿæˆåˆ†å¸ƒï¼Œå®ƒå¯¹å…¬å¼1æŸå¤±å‡½æ•°çš„è´¡çŒ®æ˜¯$$- P_r(x)logD(x) - p_g(x)log[1 - D(x)]$$ä»¤å…¶å…³äºŽ$D(x)$çš„å¯¼æ•°ä¸º0ï¼Œå¾—$$-\frac{P_r(x)}{D(x)} + \frac{P_g(x)}{1 - D(x)} = 0$$åŒ–ç®€å¾—æœ€ä¼˜åˆ¤åˆ«å™¨ä¸ºï¼š$$D^{\star}(x) = \frac{P_r(x)}{P_r(x) + P_g(x)}$$è¿™ä¸ªç»“æžœä»Žç›´è§‚ä¸Šå¾ˆå®¹æ˜“ç†è§£ï¼Œå°±æ˜¯çœ‹ä¸€ä¸ªæ ·æœ¬$x$æ¥è‡ªçœŸå®žåˆ†å¸ƒå’Œç”Ÿæˆåˆ†å¸ƒçš„å¯èƒ½æ€§çš„ç›¸å¯¹æ¯”ä¾‹ã€‚å¦‚æžœ$P_r(x) = 0$ä¸”$P_g(x) \neq 0$ï¼Œæœ€ä¼˜åˆ¤åˆ«å™¨å°±åº”è¯¥éžå¸¸è‡ªä¿¡åœ°ç»™å‡ºæ¦‚çŽ‡0ï¼›å¦‚æžœ$P_r(x) = P_g(x)$ï¼Œè¯´æ˜Žè¯¥æ ·æœ¬æ˜¯çœŸæ˜¯å‡çš„å¯èƒ½æ€§åˆšå¥½ä¸€åŠä¸€åŠï¼Œæ­¤æ—¶æœ€ä¼˜åˆ¤åˆ«å™¨ä¹Ÿåº”è¯¥ç»™å‡ºæ¦‚çŽ‡0.5ã€‚ç„¶è€ŒGANè®­ç»ƒæœ‰ä¸€ä¸ªtrickï¼Œå°±æ˜¯åˆ«æŠŠåˆ¤åˆ«å™¨è®­ç»ƒå¾—å¤ªå¥½ï¼Œå¦åˆ™åœ¨å®žéªŒä¸­ç”Ÿæˆå™¨ä¼šå®Œå…¨å­¦ä¸åŠ¨ï¼ˆlossé™ä¸ä¸‹åŽ»ï¼‰ï¼Œä¸ºäº†æŽ¢ç©¶èƒŒåŽçš„åŽŸå› ï¼Œæˆ‘ä»¬å°±å¯ä»¥çœ‹çœ‹åœ¨æžç«¯æƒ…å†µâ€”â€”åˆ¤åˆ«å™¨æœ€ä¼˜æ—¶ï¼Œç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°å˜æˆä»€ä¹ˆã€‚ç»™å…¬å¼2åŠ ä¸Šä¸€ä¸ªä¸ä¾èµ–äºŽç”Ÿæˆå™¨çš„é¡¹ï¼Œä½¿ä¹‹å˜æˆ$$\mathbb{E}_{x \sim P_r}[log D(x)] - \mathbb{E}_{x \sim P_{g}}[log(1-D(x))]$$æ³¨æ„ï¼Œæœ€å°åŒ–è¿™ä¸ªæŸå¤±å‡½æ•°ç­‰ä»·äºŽæœ€å°åŒ–å…¬å¼2ï¼Œè€Œä¸”å®ƒåˆšå¥½æ˜¯åˆ¤åˆ«å™¨æŸå¤±å‡½æ•°çš„åã€‚ä»£å…¥æœ€ä¼˜åˆ¤åˆ«å™¨å³å…¬å¼4ï¼Œå†è¿›è¡Œç®€å•çš„å˜æ¢å¯ä»¥å¾—åˆ°$$\mathbb{E}_{x \sim P_r} \log \frac{P_r(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} + \mathbb{E}_{x \sim P_g} \log \frac{P_g(x)}{\frac{1}{2}[P_r(x) + P_g(x)]} - 2\log 2$$å˜æ¢æˆè¿™ä¸ªæ ·å­æ˜¯ä¸ºäº†å¼•å…¥Kullbackâ€“Leibler divergenceï¼ˆç®€ç§°KLæ•£åº¦ï¼‰å’ŒJensen-Shannon divergenceï¼ˆç®€ç§°JSæ•£åº¦ï¼‰è¿™ä¸¤ä¸ªé‡è¦çš„ç›¸ä¼¼åº¦è¡¡é‡æŒ‡æ ‡ï¼ŒåŽé¢çš„ä¸»è§’ä¹‹ä¸€Wassersteinè·ç¦»ï¼Œå°±æ˜¯è¦æ¥åŠæ‰“å®ƒä»¬ä¸¤ä¸ªçš„ã€‚æ‰€ä»¥æŽ¥ä¸‹æ¥ä»‹ç»è¿™ä¸¤ä¸ªé‡è¦çš„é…è§’â€”â€”KLæ•£åº¦å’ŒJSæ•£åº¦ï¼š$$KL(P_1||P_2) = \mathbb{E}_{x \sim P_1} \log \frac{P_1}{P_2}$$$$JS(P_1 || P_2) = \frac{1}{2}KL(P_1||\frac{P_1 + P_2}{2}) + \frac{1}{2}KL(P_2||\frac{P_1 + P_2}{2})$$äºŽæ˜¯å…¬å¼5å°±å¯ä»¥ç»§ç»­å†™æˆ$$2JS(P_r || P_g) - 2\log 2$$åˆ°è¿™é‡Œè¯»è€…å¯ä»¥å…ˆå–˜ä¸€å£æ°”ï¼Œçœ‹çœ‹ç›®å‰å¾—åˆ°äº†ä»€ä¹ˆç»“è®ºï¼šæ ¹æ®åŽŸå§‹GANå®šä¹‰çš„åˆ¤åˆ«å™¨lossï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æœ€ä¼˜åˆ¤åˆ«å™¨çš„å½¢å¼ï¼›è€Œåœ¨æœ€ä¼˜åˆ¤åˆ«å™¨çš„ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠåŽŸå§‹GANå®šä¹‰çš„ç”Ÿæˆå™¨lossç­‰ä»·å˜æ¢ä¸ºæœ€å°åŒ–çœŸå®žåˆ†å¸ƒ$P_r$ä¸Žç”Ÿæˆåˆ†å¸ƒ$P_g$ä¹‹é—´çš„JSæ•£åº¦ã€‚æˆ‘ä»¬è¶Šè®­ç»ƒåˆ¤åˆ«å™¨ï¼Œå®ƒå°±è¶ŠæŽ¥è¿‘æœ€ä¼˜ï¼Œæœ€å°åŒ–ç”Ÿæˆå™¨çš„lossä¹Ÿå°±ä¼šè¶Šè¿‘ä¼¼äºŽæœ€å°åŒ–$P_r$å’Œ$P_g$ä¹‹é—´çš„JSæ•£åº¦ã€‚é—®é¢˜å°±å‡ºåœ¨è¿™ä¸ªJSæ•£åº¦ä¸Šã€‚æˆ‘ä»¬ä¼šå¸Œæœ›å¦‚æžœä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´è¶ŠæŽ¥è¿‘å®ƒä»¬çš„JSæ•£åº¦è¶Šå°ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–JSæ•£åº¦å°±èƒ½å°†$P_g$â€œæ‹‰å‘â€$P_r$ï¼Œæœ€ç»ˆä»¥å‡ä¹±çœŸã€‚è¿™ä¸ªå¸Œæœ›åœ¨ä¸¤ä¸ªåˆ†å¸ƒæœ‰æ‰€é‡å çš„æ—¶å€™æ˜¯æˆç«‹çš„ï¼Œä½†æ˜¯å¦‚æžœä¸¤ä¸ªåˆ†å¸ƒå®Œå…¨æ²¡æœ‰é‡å çš„éƒ¨åˆ†ï¼Œæˆ–è€…å®ƒä»¬é‡å çš„éƒ¨åˆ†å¯å¿½ç•¥ï¼ˆä¸‹é¢è§£é‡Šä»€ä¹ˆå«å¯å¿½ç•¥ï¼‰ï¼Œå®ƒä»¬çš„JSæ•£åº¦æ˜¯å¤šå°‘å‘¢ï¼Ÿç­”æ¡ˆæ˜¯$\log 2$ï¼Œå› ä¸ºå¯¹äºŽä»»æ„ä¸€ä¸ªxåªæœ‰å››ç§å¯èƒ½ï¼š$$P_1(x) = 0ä¸”P_2(x) = 0$$$$P_1(x) \neq 0ä¸”P_2(x) \neq 0$$$$P_1(x) = 0ä¸”P_2(x) \neq 0$$$$P_1(x) \neq ä¸”P_2(x) = 0$$ç¬¬ä¸€ç§å¯¹è®¡ç®—JSæ•£åº¦æ— è´¡çŒ®ï¼Œç¬¬äºŒç§æƒ…å†µç”±äºŽé‡å éƒ¨åˆ†å¯å¿½ç•¥æ‰€ä»¥è´¡çŒ®ä¹Ÿä¸º0ï¼Œç¬¬ä¸‰ç§æƒ…å†µå¯¹å…¬å¼7å³è¾¹ç¬¬ä¸€ä¸ªé¡¹çš„è´¡çŒ®æ˜¯$\log \frac{P_2}{\frac{1}{2}(P_2 + 0)} = \log 2$ï¼Œç¬¬å››ç§æƒ…å†µä¸Žä¹‹ç±»ä¼¼ï¼Œæ‰€ä»¥æœ€ç»ˆ$JS(P_1||P_2) = \log 2$ã€‚æ¢å¥è¯è¯´ï¼Œæ— è®º$P_r$è·Ÿ$P_g$æ˜¯è¿œåœ¨å¤©è¾¹ï¼Œè¿˜æ˜¯è¿‘åœ¨çœ¼å‰ï¼Œåªè¦å®ƒä»¬ä¿©æ²¡æœ‰ä¸€ç‚¹é‡å æˆ–è€…é‡å éƒ¨åˆ†å¯å¿½ç•¥ï¼ŒJSæ•£åº¦å°±å›ºå®šæ˜¯å¸¸æ•°$\log 2$ï¼Œè€Œè¿™å¯¹äºŽæ¢¯åº¦ä¸‹é™æ–¹æ³•æ„å‘³ç€â€”â€”æ¢¯åº¦ä¸º0ï¼æ­¤æ—¶å¯¹äºŽæœ€ä¼˜åˆ¤åˆ«å™¨æ¥è¯´ï¼Œç”Ÿæˆå™¨è‚¯å®šæ˜¯å¾—ä¸åˆ°ä¸€ä¸ç‚¹æ¢¯åº¦ä¿¡æ¯çš„ï¼›å³ä½¿å¯¹äºŽæŽ¥è¿‘æœ€ä¼˜çš„åˆ¤åˆ«å™¨æ¥è¯´ï¼Œç”Ÿæˆå™¨ä¹Ÿæœ‰å¾ˆå¤§æœºä¼šé¢ä¸´æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚ä½†æ˜¯$P_r$ä¸Ž$P_g$ä¸é‡å æˆ–é‡å éƒ¨åˆ†å¯å¿½ç•¥çš„å¯èƒ½æ€§æœ‰å¤šå¤§ï¼Ÿä¸ä¸¥è°¨çš„ç­”æ¡ˆæ˜¯ï¼šéžå¸¸å¤§ã€‚æ¯”è¾ƒä¸¥è°¨çš„ç­”æ¡ˆæ˜¯ï¼šå½“$P_r$ä¸Ž$P_g$çš„æ”¯æ’‘é›†ï¼ˆsupportï¼‰æ˜¯é«˜ç»´ç©ºé—´ä¸­çš„ä½Žç»´æµå½¢ï¼ˆmanifoldï¼‰æ—¶ï¼Œ$P_r$ä¸Ž$P_g$é‡å éƒ¨åˆ†æµ‹åº¦ï¼ˆmeasureï¼‰ä¸º0çš„æ¦‚çŽ‡ä¸º1ã€‚ä¸ç”¨è¢«å¥‡æ€ªçš„æœ¯è¯­å“å¾—å…³æŽ‰é¡µé¢ï¼Œè™½ç„¶è®ºæ–‡ç»™å‡ºçš„æ˜¯ä¸¥æ ¼çš„æ•°å­¦è¡¨è¿°ï¼Œä½†æ˜¯ç›´è§‚ä¸Šå…¶å®žå¾ˆå®¹æ˜“ç†è§£ã€‚é¦–å…ˆç®€å•ä»‹ç»ä¸€ä¸‹è¿™å‡ ä¸ªæ¦‚å¿µï¼šæ”¯æ’‘é›†ï¼ˆsupportï¼‰å…¶å®žå°±æ˜¯å‡½æ•°çš„éžé›¶éƒ¨åˆ†å­é›†ï¼Œæ¯”å¦‚ReLUå‡½æ•°çš„æ”¯æ’‘é›†å°±æ˜¯$(0, +\infty)$ï¼Œä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒçš„æ”¯æ’‘é›†å°±æ˜¯æ‰€æœ‰æ¦‚çŽ‡å¯†åº¦éžé›¶éƒ¨åˆ†çš„é›†åˆã€‚æµå½¢ï¼ˆmanifoldï¼‰æ˜¯é«˜ç»´ç©ºé—´ä¸­æ›²çº¿ã€æ›²é¢æ¦‚å¿µçš„æ‹“å¹¿ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä½Žç»´ä¸Šç›´è§‚ç†è§£è¿™ä¸ªæ¦‚å¿µï¼Œæ¯”å¦‚æˆ‘ä»¬è¯´ä¸‰ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªæ›²é¢æ˜¯ä¸€ä¸ªäºŒç»´æµå½¢ï¼Œå› ä¸ºå®ƒçš„æœ¬è´¨ç»´åº¦ï¼ˆintrinsic dimensionï¼‰åªæœ‰2ï¼Œä¸€ä¸ªç‚¹åœ¨è¿™ä¸ªäºŒç»´æµå½¢ä¸Šç§»åŠ¨åªæœ‰ä¸¤ä¸ªæ–¹å‘çš„è‡ªç”±åº¦ã€‚åŒç†ï¼Œä¸‰ç»´ç©ºé—´æˆ–è€…äºŒç»´ç©ºé—´ä¸­çš„ä¸€æ¡æ›²çº¿éƒ½æ˜¯ä¸€ä¸ªä¸€ç»´æµå½¢ã€‚æµ‹åº¦ï¼ˆmeasureï¼‰æ˜¯é«˜ç»´ç©ºé—´ä¸­é•¿åº¦ã€é¢ç§¯ã€ä½“ç§¯æ¦‚å¿µçš„æ‹“å¹¿ï¼Œå¯ä»¥ç†è§£ä¸ºâ€œè¶…ä½“ç§¯â€ã€‚å›žè¿‡å¤´æ¥çœ‹ç¬¬ä¸€å¥è¯ï¼Œâ€œå½“$P_r$ä¸Ž$P_g$çš„æ”¯æ’‘é›†æ˜¯é«˜ç»´ç©ºé—´ä¸­çš„ä½Žç»´æµå½¢æ—¶â€ï¼ŒåŸºæœ¬ä¸Šæ˜¯æˆç«‹çš„ã€‚åŽŸå› æ˜¯GANä¸­çš„ç”Ÿæˆå™¨ä¸€èˆ¬æ˜¯ä»ŽæŸä¸ªä½Žç»´ï¼ˆæ¯”å¦‚100ç»´ï¼‰çš„éšæœºåˆ†å¸ƒä¸­é‡‡æ ·å‡ºä¸€ä¸ªç¼–ç å‘é‡ï¼Œå†ç»è¿‡ä¸€ä¸ªç¥žç»ç½‘ç»œç”Ÿæˆå‡ºä¸€ä¸ªé«˜ç»´æ ·æœ¬ï¼ˆæ¯”å¦‚64x64çš„å›¾ç‰‡å°±æœ‰4096ç»´ï¼‰ã€‚å½“ç”Ÿæˆå™¨çš„å‚æ•°å›ºå®šæ—¶ï¼Œç”Ÿæˆæ ·æœ¬çš„æ¦‚çŽ‡åˆ†å¸ƒè™½ç„¶æ˜¯å®šä¹‰åœ¨4096ç»´çš„ç©ºé—´ä¸Šï¼Œä½†å®ƒæœ¬èº«æ‰€æœ‰å¯èƒ½äº§ç”Ÿçš„å˜åŒ–å·²ç»è¢«é‚£ä¸ª100ç»´çš„éšæœºåˆ†å¸ƒé™å®šäº†ï¼Œå…¶æœ¬è´¨ç»´åº¦å°±æ˜¯100ï¼Œå†è€ƒè™‘åˆ°ç¥žç»ç½‘ç»œå¸¦æ¥çš„æ˜ å°„é™ç»´ï¼Œæœ€ç»ˆå¯èƒ½æ¯”100è¿˜å°ï¼Œæ‰€ä»¥ç”Ÿæˆæ ·æœ¬åˆ†å¸ƒçš„æ”¯æ’‘é›†å°±åœ¨4096ç»´ç©ºé—´ä¸­æž„æˆä¸€ä¸ªæœ€å¤š100ç»´çš„ä½Žç»´æµå½¢ï¼Œâ€œæ’‘ä¸æ»¡â€æ•´ä¸ªé«˜ç»´ç©ºé—´ã€‚â€œæ’‘ä¸æ»¡â€å°±ä¼šå¯¼è‡´çœŸå®žåˆ†å¸ƒä¸Žç”Ÿæˆåˆ†å¸ƒéš¾ä»¥â€œç¢°åˆ°é¢â€ï¼Œè¿™å¾ˆå®¹æ˜“åœ¨äºŒç»´ç©ºé—´ä¸­ç†è§£ï¼šä¸€æ–¹é¢ï¼ŒäºŒç»´å¹³é¢ä¸­éšæœºå–ä¸¤æ¡æ›²çº¿ï¼Œå®ƒä»¬ä¹‹é—´åˆšå¥½å­˜åœ¨é‡å çº¿æ®µçš„æ¦‚çŽ‡ä¸º0ï¼›å¦ä¸€æ–¹é¢ï¼Œè™½ç„¶å®ƒä»¬å¾ˆå¤§å¯èƒ½ä¼šå­˜åœ¨äº¤å‰ç‚¹ï¼Œä½†æ˜¯ç›¸æ¯”äºŽä¸¤æ¡æ›²çº¿è€Œè¨€ï¼Œäº¤å‰ç‚¹æ¯”æ›²çº¿ä½Žä¸€ä¸ªç»´åº¦ï¼Œé•¿åº¦ï¼ˆæµ‹åº¦ï¼‰ä¸º0ï¼Œå¯å¿½ç•¥ã€‚ä¸‰ç»´ç©ºé—´ä¸­ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œéšæœºå–ä¸¤ä¸ªæ›²é¢ï¼Œå®ƒä»¬ä¹‹é—´æœ€å¤šå°±æ˜¯æ¯”è¾ƒæœ‰å¯èƒ½å­˜åœ¨äº¤å‰çº¿ï¼Œä½†æ˜¯äº¤å‰çº¿æ¯”æ›²é¢ä½Žä¸€ä¸ªç»´åº¦ï¼Œé¢ç§¯ï¼ˆæµ‹åº¦ï¼‰æ˜¯0ï¼Œå¯å¿½ç•¥ã€‚ä»Žä½Žç»´ç©ºé—´æ‹“å±•åˆ°é«˜ç»´ç©ºé—´ï¼Œå°±æœ‰äº†å¦‚ä¸‹é€»è¾‘ï¼šå› ä¸ºä¸€å¼€å§‹ç”Ÿæˆå™¨éšæœºåˆå§‹åŒ–ï¼Œæ‰€ä»¥$P_g$å‡ ä¹Žä¸å¯èƒ½ä¸Ž$P_r$æœ‰ä»€ä¹ˆå…³è”ï¼Œæ‰€ä»¥å®ƒä»¬çš„æ”¯æ’‘é›†ä¹‹é—´çš„é‡å éƒ¨åˆ†è¦ä¹ˆä¸å­˜åœ¨ï¼Œè¦ä¹ˆå°±æ¯”$P_r$å’Œ$P_g$çš„æœ€å°ç»´åº¦è¿˜è¦ä½Žè‡³å°‘ä¸€ä¸ªç»´åº¦ï¼Œæ•…è€Œæµ‹åº¦ä¸º0ã€‚æ‰€è°“â€œé‡å éƒ¨åˆ†æµ‹åº¦ä¸º0â€ï¼Œå°±æ˜¯ä¸Šæ–‡æ‰€è¨€â€œä¸é‡å æˆ–è€…é‡å éƒ¨åˆ†å¯å¿½ç•¥â€çš„æ„æ€ã€‚æˆ‘ä»¬å°±å¾—åˆ°äº†WGANå‰ä½œä¸­å…³äºŽç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±çš„ç¬¬ä¸€ä¸ªè®ºè¯ï¼šåœ¨ï¼ˆè¿‘ä¼¼ï¼‰æœ€ä¼˜åˆ¤åˆ«å™¨ä¸‹ï¼Œæœ€å°åŒ–ç”Ÿæˆå™¨çš„lossç­‰ä»·äºŽæœ€å°åŒ–$P_r$ä¸Ž$P_g$ä¹‹é—´çš„JSæ•£åº¦ï¼Œè€Œç”±äºŽ$P_r$ä¸Ž$P_g$å‡ ä¹Žä¸å¯èƒ½æœ‰ä¸å¯å¿½ç•¥çš„é‡å ï¼Œæ‰€ä»¥æ— è®ºå®ƒä»¬ç›¸è·å¤šè¿œJSæ•£åº¦éƒ½æ˜¯å¸¸æ•°$\log 2$ï¼Œæœ€ç»ˆå¯¼è‡´ç”Ÿæˆå™¨çš„æ¢¯åº¦ï¼ˆè¿‘ä¼¼ï¼‰ä¸º0ï¼Œæ¢¯åº¦æ¶ˆå¤±ã€‚æŽ¥ç€ä½œè€…å†™äº†å¾ˆå¤šå…¬å¼å®šç†ä»Žç¬¬äºŒä¸ªè§’åº¦è¿›è¡Œè®ºè¯ï¼Œä½†æ˜¯èƒŒåŽçš„æ€æƒ³ä¹Ÿå¯ä»¥ç›´è§‚åœ°è§£é‡Šï¼šé¦–å…ˆï¼Œ$P_r$ä¸Ž$P_g$ä¹‹é—´å‡ ä¹Žä¸å¯èƒ½æœ‰ä¸å¯å¿½ç•¥çš„é‡å ï¼Œæ‰€ä»¥æ— è®ºå®ƒä»¬ä¹‹é—´çš„â€œç¼éš™â€å¤šç‹­å°ï¼Œéƒ½è‚¯å®šå­˜åœ¨ä¸€ä¸ªæœ€ä¼˜åˆ†å‰²æ›²é¢æŠŠå®ƒä»¬éš”å¼€ï¼Œæœ€å¤šå°±æ˜¯åœ¨é‚£äº›å¯å¿½ç•¥çš„é‡å å¤„éš”ä¸å¼€è€Œå·²ã€‚ç”±äºŽåˆ¤åˆ«å™¨ä½œä¸ºä¸€ä¸ªç¥žç»ç½‘ç»œå¯ä»¥æ— é™æ‹Ÿåˆè¿™ä¸ªåˆ†éš”æ›²é¢ï¼Œæ‰€ä»¥å­˜åœ¨ä¸€ä¸ªæœ€ä¼˜åˆ¤åˆ«å™¨ï¼Œå¯¹å‡ ä¹Žæ‰€æœ‰çœŸå®žæ ·æœ¬ç»™å‡ºæ¦‚çŽ‡1ï¼Œå¯¹å‡ ä¹Žæ‰€æœ‰ç”Ÿæˆæ ·æœ¬ç»™å‡ºæ¦‚çŽ‡0ï¼Œè€Œé‚£äº›éš”ä¸å¼€çš„éƒ¨åˆ†å°±æ˜¯éš¾ä»¥è¢«æœ€ä¼˜åˆ¤åˆ«å™¨åˆ†ç±»çš„æ ·æœ¬ï¼Œä½†æ˜¯å®ƒä»¬çš„æµ‹åº¦ä¸º0ï¼Œå¯å¿½ç•¥ã€‚æœ€ä¼˜åˆ¤åˆ«å™¨åœ¨çœŸå®žåˆ†å¸ƒå’Œç”Ÿæˆåˆ†å¸ƒçš„æ”¯æ’‘é›†ä¸Šç»™å‡ºçš„æ¦‚çŽ‡éƒ½æ˜¯å¸¸æ•°ï¼ˆ1å’Œ0ï¼‰ï¼Œå¯¼è‡´ç”Ÿæˆå™¨çš„lossæ¢¯åº¦ä¸º0ï¼Œæ¢¯åº¦æ¶ˆå¤±ã€‚æœ‰äº†è¿™äº›ç†è®ºåˆ†æžï¼ŒåŽŸå§‹GANä¸ç¨³å®šçš„åŽŸå› å°±å½»åº•æ¸…æ¥šäº†ï¼šåˆ¤åˆ«å™¨è®­ç»ƒå¾—å¤ªå¥½ï¼Œç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±ï¼Œç”Ÿæˆå™¨lossé™ä¸ä¸‹åŽ»ï¼›åˆ¤åˆ«å™¨è®­ç»ƒå¾—ä¸å¥½ï¼Œç”Ÿæˆå™¨æ¢¯åº¦ä¸å‡†ï¼Œå››å¤„ä¹±è·‘ã€‚åªæœ‰åˆ¤åˆ«å™¨è®­ç»ƒå¾—ä¸å¥½ä¸åæ‰è¡Œï¼Œä½†æ˜¯è¿™ä¸ªç«å€™åˆå¾ˆéš¾æŠŠæ¡ï¼Œç”šè‡³åœ¨åŒä¸€è½®è®­ç»ƒçš„å‰åŽä¸åŒé˜¶æ®µè¿™ä¸ªç«å€™éƒ½å¯èƒ½ä¸ä¸€æ ·ï¼Œæ‰€ä»¥GANæ‰é‚£ä¹ˆéš¾è®­ç»ƒã€‚å®žéªŒè¾…è¯å¦‚ä¸‹ï¼šWGANå‰ä½œFigure 2ã€‚å…ˆåˆ†åˆ«å°†DCGANè®­ç»ƒ1ï¼Œ20ï¼Œ25ä¸ªepochï¼Œç„¶åŽå›ºå®šç”Ÿæˆå™¨ä¸åŠ¨ï¼Œåˆ¤åˆ«å™¨é‡æ–°éšæœºåˆå§‹åŒ–ä»Žå¤´å¼€å§‹è®­ç»ƒï¼Œå¯¹äºŽç¬¬ä¸€ç§å½¢å¼çš„ç”Ÿæˆå™¨lossäº§ç”Ÿçš„æ¢¯åº¦å¯ä»¥æ‰“å°å‡ºå…¶å°ºåº¦çš„å˜åŒ–æ›²çº¿ï¼Œå¯ä»¥çœ‹åˆ°éšç€åˆ¤åˆ«å™¨çš„è®­ç»ƒï¼Œç”Ÿæˆå™¨çš„æ¢¯åº¦å‡è¿…é€Ÿè¡°å‡ã€‚æ³¨æ„yè½´æ˜¯å¯¹æ•°åæ ‡è½´ã€‚ç¬¬äºŒç§åŽŸå§‹GANå½¢å¼çš„é—®é¢˜ä¸€å¥è¯æ¦‚æ‹¬ï¼šæœ€å°åŒ–ç¬¬äºŒç§ç”Ÿæˆå™¨losså‡½æ•°ï¼Œä¼šç­‰ä»·äºŽæœ€å°åŒ–ä¸€ä¸ªä¸åˆç†çš„è·ç¦»è¡¡é‡ï¼Œå¯¼è‡´ä¸¤ä¸ªé—®é¢˜ï¼Œä¸€æ˜¯æ¢¯åº¦ä¸ç¨³å®šï¼ŒäºŒæ˜¯collapse modeå³å¤šæ ·æ€§ä¸è¶³ã€‚WGANå‰ä½œåˆæ˜¯ä»Žä¸¤ä¸ªè§’åº¦è¿›è¡Œäº†è®ºè¯ï¼Œä¸‹é¢åªè¯´ç¬¬ä¸€ä¸ªè§’åº¦ï¼Œå› ä¸ºå¯¹äºŽç¬¬äºŒä¸ªè§’åº¦æˆ‘éš¾ä»¥æ‰¾åˆ°ä¸€ä¸ªç›´è§‚çš„è§£é‡Šæ–¹å¼ï¼Œæ„Ÿå…´è¶£çš„è¯»è€…è¿˜æ˜¯åŽ»çœ‹è®ºæ–‡å§ï¼ˆé€ƒï¼‰ã€‚å¦‚å‰æ–‡æ‰€è¯´ï¼ŒIan Goodfellowæå‡ºçš„â€œ- log D trickâ€æ˜¯æŠŠç”Ÿæˆå™¨lossæ”¹æˆ$$\mathbb{E}_{x\sim P_g}[- \log D(x)]$$ä¸Šæ–‡æŽ¨å¯¼å·²ç»å¾—åˆ°åœ¨æœ€ä¼˜åˆ¤åˆ«å™¨$D^*$ä¸‹$$\mathbb{E}_{x\sim P_r}[\log D^*(x)]$$æˆ‘ä»¬å¯ä»¥æŠŠKLæ•£åº¦ï¼ˆæ³¨æ„ä¸‹é¢æ˜¯å…ˆgåŽrï¼‰å˜æ¢æˆå«çš„å½¢å¼ï¼š$$\begin{align}KL(P_g || P_r) &amp;= \mathbb{E}_{x \sim P_g} [\log \frac{P_g(x)}{P_r(x)}] \\&amp;= \mathbb{E}_{x \sim P_g} [\log \frac{P_g(x) / (P_r(x) + P_g(x))}{P_r(x) / (P_r(x) + P_g(x))}] \\&amp;= \mathbb{E}_{x \sim P_g} [\log \frac{1 - D^(x)}{D^(x)}] \\&amp;= \mathbb{E}_{x \sim P_g} \log [1 - D^(x)] - \mathbb{E}_{x \sim P_g} \log D^(x)\end{align} \\$$å¯å¾—æœ€å°åŒ–ç›®æ ‡çš„ç­‰ä»·å˜å½¢$$\begin{align}\mathbb{E}_{x \sim P_g} [-\log D^(x)] &amp;= KL(P_g || P_r) - \mathbb{E}_{x \sim P_g} \log [1 - D^(x)] \\&amp;= KL(P_g || P_r) - 2JS(P_r || P_g) + 2\log 2 + \mathbb{E}_{x\sim P_r}[\log D^*(x)]\end{align}$$æ³¨æ„ä¸Šå¼æœ€åŽä¸¤é¡¹ä¸ä¾èµ–äºŽç”Ÿæˆå™¨Gï¼Œæœ€ç»ˆå¾—åˆ°æœ€å°åŒ–å…¬å¼3ç­‰ä»·äºŽæœ€å°åŒ–$$KL(P_g || P_r) - 2JS(P_r || P_g)$$è¿™ä¸ªç­‰ä»·æœ€å°åŒ–ç›®æ ‡å­˜åœ¨ä¸¤ä¸ªä¸¥é‡çš„é—®é¢˜ã€‚ç¬¬ä¸€æ˜¯å®ƒåŒæ—¶è¦æœ€å°åŒ–ç”Ÿæˆåˆ†å¸ƒä¸ŽçœŸå®žåˆ†å¸ƒçš„KLæ•£åº¦ï¼Œå´åˆè¦æœ€å¤§åŒ–ä¸¤è€…çš„JSæ•£åº¦ï¼Œä¸€ä¸ªè¦æ‹‰è¿‘ï¼Œä¸€ä¸ªå´è¦æŽ¨è¿œï¼è¿™åœ¨ç›´è§‚ä¸Šéžå¸¸è’è°¬ï¼Œåœ¨æ•°å€¼ä¸Šåˆ™ä¼šå¯¼è‡´æ¢¯åº¦ä¸ç¨³å®šï¼Œè¿™æ˜¯åŽé¢é‚£ä¸ªJSæ•£åº¦é¡¹çš„æ¯›ç—…ã€‚ç¬¬äºŒï¼Œå³ä¾¿æ˜¯å‰é¢é‚£ä¸ªæ­£å¸¸çš„$KL$æ•£åº¦é¡¹ä¹Ÿæœ‰æ¯›ç—…ã€‚å› ä¸º$KL$æ•£åº¦ä¸æ˜¯ä¸€ä¸ªå¯¹ç§°çš„è¡¡é‡ï¼Œ$KL(P_g || P_r)$ä¸Ž$KL(P_r || P_g)$æ˜¯æœ‰å·®åˆ«çš„ã€‚ä»¥å‰è€…ä¸ºä¾‹å½“$P_g(x)\rightarrow 0$è€Œ$P_r(x)\rightarrow 1$æ—¶ï¼Œ$P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow 0$ï¼Œå¯¹$KL(P_g || P_r)$è´¡çŒ®è¶‹è¿‘0å½“$P_g(x)\rightarrow 1$è€Œ$P_r(x)\rightarrow 0$æ—¶ï¼Œ$P_g(x) \log \frac{P_g(x)}{P_r(x)} \rightarrow +\infty$ï¼Œå¯¹$KL(P_g || P_r)$è´¡çŒ®è¶‹è¿‘æ­£æ— ç©·æ¢è¨€ä¹‹ï¼Œ$KL(P_g || P_r)$å¯¹äºŽä¸Šé¢ä¸¤ç§é”™è¯¯çš„æƒ©ç½šæ˜¯ä¸ä¸€æ ·çš„ï¼Œç¬¬ä¸€ç§é”™è¯¯å¯¹åº”çš„æ˜¯â€œç”Ÿæˆå™¨æ²¡èƒ½ç”ŸæˆçœŸå®žçš„æ ·æœ¬â€ï¼Œæƒ©ç½šå¾®å°ï¼›ç¬¬äºŒç§é”™è¯¯å¯¹åº”çš„æ˜¯â€œç”Ÿæˆå™¨ç”Ÿæˆäº†ä¸çœŸå®žçš„æ ·æœ¬â€ ï¼Œæƒ©ç½šå·¨å¤§ã€‚ç¬¬ä¸€ç§é”™è¯¯å¯¹åº”çš„æ˜¯ç¼ºä¹å¤šæ ·æ€§ï¼Œç¬¬äºŒç§é”™è¯¯å¯¹åº”çš„æ˜¯ç¼ºä¹å‡†ç¡®æ€§ã€‚è¿™ä¸€æ”¾ä¸€æ‰“ä¹‹ä¸‹ï¼Œç”Ÿæˆå™¨å®å¯å¤šç”Ÿæˆä¸€äº›é‡å¤ä½†æ˜¯å¾ˆâ€œå®‰å…¨â€çš„æ ·æœ¬ï¼Œä¹Ÿä¸æ„¿æ„åŽ»ç”Ÿæˆå¤šæ ·æ€§çš„æ ·æœ¬ï¼Œå› ä¸ºé‚£æ ·ä¸€ä¸å°å¿ƒå°±ä¼šäº§ç”Ÿç¬¬äºŒç§é”™è¯¯ï¼Œå¾—ä¸å¿å¤±ã€‚è¿™ç§çŽ°è±¡å°±æ˜¯å¤§å®¶å¸¸è¯´çš„collapse modeã€‚ç¬¬ä¸€éƒ¨åˆ†å°ç»“ï¼šåœ¨åŽŸå§‹GANçš„ï¼ˆè¿‘ä¼¼ï¼‰æœ€ä¼˜åˆ¤åˆ«å™¨ä¸‹ï¼Œç¬¬ä¸€ç§ç”Ÿæˆå™¨lossé¢ä¸´æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œç¬¬äºŒç§ç”Ÿæˆå™¨lossé¢ä¸´ä¼˜åŒ–ç›®æ ‡è’è°¬ã€æ¢¯åº¦ä¸ç¨³å®šã€å¯¹å¤šæ ·æ€§ä¸Žå‡†ç¡®æ€§æƒ©ç½šä¸å¹³è¡¡å¯¼è‡´mode collapseè¿™å‡ ä¸ªé—®é¢˜ã€‚å®žéªŒè¾…è¯å¦‚ä¸‹ï¼šWGANå‰ä½œFigure 3ã€‚å…ˆåˆ†åˆ«å°†DCGANè®­ç»ƒ1ï¼Œ20ï¼Œ25ä¸ªepochï¼Œç„¶åŽå›ºå®šç”Ÿæˆå™¨ä¸åŠ¨ï¼Œåˆ¤åˆ«å™¨é‡æ–°éšæœºåˆå§‹åŒ–ä»Žå¤´å¼€å§‹è®­ç»ƒï¼Œå¯¹äºŽç¬¬äºŒç§å½¢å¼çš„ç”Ÿæˆå™¨lossäº§ç”Ÿçš„æ¢¯åº¦å¯ä»¥æ‰“å°å‡ºå…¶å°ºåº¦çš„å˜åŒ–æ›²çº¿ï¼Œå¯ä»¥çœ‹åˆ°éšç€åˆ¤åˆ«å™¨çš„è®­ç»ƒï¼Œè“è‰²å’Œç»¿è‰²æ›²çº¿ä¸­ç”Ÿæˆå™¨çš„æ¢¯åº¦è¿…é€Ÿå¢žé•¿ï¼Œè¯´æ˜Žæ¢¯åº¦ä¸ç¨³å®šï¼Œçº¢çº¿å¯¹åº”çš„æ˜¯DCGANç›¸å¯¹æ”¶æ•›çš„çŠ¶æ€ï¼Œæ¢¯åº¦æ‰æ¯”è¾ƒç¨³å®šã€‚ç¬¬äºŒéƒ¨åˆ†ï¼šWGANä¹‹å‰çš„ä¸€ä¸ªè¿‡æ¸¡è§£å†³æ–¹æ¡ˆåŽŸå§‹GANé—®é¢˜çš„æ ¹æºå¯ä»¥å½’ç»“ä¸ºä¸¤ç‚¹ï¼Œä¸€æ˜¯ç­‰ä»·ä¼˜åŒ–çš„è·ç¦»è¡¡é‡ï¼ˆKLæ•£åº¦ã€JSæ•£åº¦ï¼‰ä¸åˆç†ï¼ŒäºŒæ˜¯ç”Ÿæˆå™¨éšæœºåˆå§‹åŒ–åŽçš„ç”Ÿæˆåˆ†å¸ƒå¾ˆéš¾ä¸ŽçœŸå®žåˆ†å¸ƒæœ‰ä¸å¯å¿½ç•¥çš„é‡å ã€‚WGANå‰ä½œå…¶å®žå·²ç»é’ˆå¯¹ç¬¬äºŒç‚¹æå‡ºäº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œå°±æ˜¯å¯¹ç”Ÿæˆæ ·æœ¬å’ŒçœŸå®žæ ·æœ¬åŠ å™ªå£°ï¼Œç›´è§‚ä¸Šè¯´ï¼Œä½¿å¾—åŽŸæœ¬çš„ä¸¤ä¸ªä½Žç»´æµå½¢â€œå¼¥æ•£â€åˆ°æ•´ä¸ªé«˜ç»´ç©ºé—´ï¼Œå¼ºè¡Œè®©å®ƒä»¬äº§ç”Ÿä¸å¯å¿½ç•¥çš„é‡å ã€‚è€Œä¸€æ—¦å­˜åœ¨é‡å ï¼ŒJSæ•£åº¦å°±èƒ½çœŸæ­£å‘æŒ¥ä½œç”¨ï¼Œæ­¤æ—¶å¦‚æžœä¸¤ä¸ªåˆ†å¸ƒè¶Šé è¿‘ï¼Œå®ƒä»¬â€œå¼¥æ•£â€å‡ºæ¥çš„éƒ¨åˆ†é‡å å¾—è¶Šå¤šï¼ŒJSæ•£åº¦ä¹Ÿä¼šè¶Šå°è€Œä¸ä¼šä¸€ç›´æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼ŒäºŽæ˜¯ï¼ˆåœ¨ç¬¬ä¸€ç§åŽŸå§‹GANå½¢å¼ä¸‹ï¼‰æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜å°±è§£å†³äº†ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ‰€åŠ çš„å™ªå£°è¿›è¡Œé€€ç«ï¼ˆannealingï¼‰ï¼Œæ…¢æ…¢å‡å°å…¶æ–¹å·®ï¼Œåˆ°åŽé¢ä¸¤ä¸ªä½Žç»´æµå½¢â€œæœ¬ä½“â€éƒ½å·²ç»æœ‰é‡å æ—¶ï¼Œå°±ç®—æŠŠå™ªå£°å®Œå…¨æ‹¿æŽ‰ï¼ŒJSæ•£åº¦ä¹Ÿèƒ½ç…§æ ·å‘æŒ¥ä½œç”¨ï¼Œç»§ç»­äº§ç”Ÿæœ‰æ„ä¹‰çš„æ¢¯åº¦æŠŠä¸¤ä¸ªä½Žç»´æµå½¢æ‹‰è¿‘ï¼Œç›´åˆ°å®ƒä»¬æŽ¥è¿‘å®Œå…¨é‡åˆã€‚ä»¥ä¸Šæ˜¯å¯¹åŽŸæ–‡çš„ç›´è§‚è§£é‡Šã€‚åœ¨è¿™ä¸ªè§£å†³æ–¹æ¡ˆä¸‹æˆ‘ä»¬å¯ä»¥æ”¾å¿ƒåœ°æŠŠåˆ¤åˆ«å™¨è®­ç»ƒåˆ°æŽ¥è¿‘æœ€ä¼˜ï¼Œä¸å¿…æ‹…å¿ƒæ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚è€Œå½“åˆ¤åˆ«å™¨æœ€ä¼˜æ—¶ï¼Œå¯¹å…¬å¼9å–åå¯å¾—åˆ¤åˆ«å™¨çš„æœ€å°lossä¸ºå…¶ä¸­$P_{r+\epsilon}$å’Œ$P_{g+\epsilon}$åˆ†åˆ«æ˜¯åŠ å™ªåŽçš„çœŸå®žåˆ†å¸ƒä¸Žç”Ÿæˆåˆ†å¸ƒã€‚åè¿‡æ¥è¯´ï¼Œä»Žæœ€ä¼˜åˆ¤åˆ«å™¨çš„losså¯ä»¥åæŽ¨å‡ºå½“å‰ä¸¤ä¸ªåŠ å™ªåˆ†å¸ƒçš„JSæ•£åº¦ã€‚ä¸¤ä¸ªåŠ å™ªåˆ†å¸ƒçš„JSæ•£åº¦å¯ä»¥åœ¨æŸç§ç¨‹åº¦ä¸Šä»£è¡¨ä¸¤ä¸ªåŽŸæœ¬åˆ†å¸ƒçš„è·ç¦»ï¼Œä¹Ÿå°±æ˜¯è¯´å¯ä»¥é€šè¿‡æœ€ä¼˜åˆ¤åˆ«å™¨çš„lossåæ˜ è®­ç»ƒè¿›ç¨‹ï¼â€¦â€¦çœŸçš„æœ‰è¿™æ ·çš„å¥½äº‹å—ï¼Ÿå¹¶æ²¡æœ‰ï¼Œå› ä¸ºåŠ å™ªJSæ•£åº¦çš„å…·ä½“æ•°å€¼å—åˆ°å™ªå£°çš„æ–¹å·®å½±å“ï¼Œéšç€å™ªå£°çš„é€€ç«ï¼Œå‰åŽçš„æ•°å€¼å°±æ²¡æ³•æ¯”è¾ƒäº†ï¼Œæ‰€ä»¥å®ƒä¸èƒ½æˆä¸º$P_r$å’Œ$P_g$è·ç¦»çš„æœ¬è´¨æ€§è¡¡é‡ã€‚å› ä¸ºæœ¬æ–‡çš„é‡ç‚¹æ˜¯WGANæœ¬èº«ï¼Œæ‰€ä»¥WGANå‰ä½œçš„åŠ å™ªæ–¹æ¡ˆç®€å•ä»‹ç»åˆ°è¿™é‡Œï¼Œæ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥é˜…è¯»åŽŸæ–‡äº†è§£æ›´å¤šç»†èŠ‚ã€‚åŠ å™ªæ–¹æ¡ˆæ˜¯é’ˆå¯¹åŽŸå§‹GANé—®é¢˜çš„ç¬¬äºŒç‚¹æ ¹æºæå‡ºçš„ï¼Œè§£å†³äº†è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œä¸éœ€è¦å°å¿ƒå¹³è¡¡åˆ¤åˆ«å™¨è®­ç»ƒçš„ç«å€™ï¼Œå¯ä»¥æ”¾å¿ƒåœ°æŠŠåˆ¤åˆ«å™¨è®­ç»ƒåˆ°æŽ¥è¿‘æœ€ä¼˜ï¼Œä½†æ˜¯ä»ç„¶æ²¡èƒ½å¤Ÿæä¾›ä¸€ä¸ªè¡¡é‡è®­ç»ƒè¿›ç¨‹çš„æ•°å€¼æŒ‡æ ‡ã€‚ä½†æ˜¯WGANæœ¬ä½œå°±ä»Žç¬¬ä¸€ç‚¹æ ¹æºå‡ºå‘ï¼Œç”¨Wassersteinè·ç¦»ä»£æ›¿JSæ•£åº¦ï¼ŒåŒæ—¶å®Œæˆäº†ç¨³å®šè®­ç»ƒå’Œè¿›ç¨‹æŒ‡æ ‡çš„é—®é¢˜ï¼ä½œè€…æœªå¯¹æ­¤æ–¹æ¡ˆè¿›è¡Œå®žéªŒéªŒè¯ã€‚ç¬¬ä¸‰éƒ¨åˆ†ï¼šWassersteinè·ç¦»çš„ä¼˜è¶Šæ€§è´¨Wassersteinè·ç¦»åˆå«Earth-Moverï¼ˆEMï¼‰è·ç¦»ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š$$ W(P_r, P_g) = \inf_{\gamma \sim \Pi (P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [||x - y||$$è§£é‡Šå¦‚ä¸‹ï¼š$ \Pi (P_r, P_g)$ æ˜¯ $P_r$ å’Œ $P_g$ ç»„åˆèµ·æ¥çš„æ‰€æœ‰å¯èƒ½çš„è”åˆåˆ†å¸ƒçš„é›†åˆï¼Œåè¿‡æ¥è¯´ï¼Œ$\Pi (P_r, P_g)$ ä¸­æ¯ä¸€ä¸ªåˆ†å¸ƒçš„è¾¹ç¼˜åˆ†å¸ƒéƒ½æ˜¯ $P_r$ å’Œ $P_g$ ã€‚å¯¹äºŽæ¯ä¸€ä¸ªå¯èƒ½çš„è”åˆåˆ†å¸ƒ $\gamma$ è€Œè¨€ï¼Œå¯ä»¥ä»Žä¸­é‡‡æ · $(x, y) \sim \gamma$ å¾—åˆ°ä¸€ä¸ªçœŸå®žæ ·æœ¬ $x$ å’Œä¸€ä¸ªç”Ÿæˆæ ·æœ¬ $y$ ï¼Œå¹¶ç®—å‡ºè¿™å¯¹æ ·æœ¬çš„è·ç¦» $||x-y||$ ï¼Œæ‰€ä»¥å¯ä»¥è®¡ç®—è¯¥è”åˆåˆ†å¸ƒ $\gamma$ ä¸‹æ ·æœ¬å¯¹è·ç¦»çš„æœŸæœ›å€¼ $\mathbb{E}_{(x, y) \sim \gamma} [||x - y||$ ã€‚åœ¨æ‰€æœ‰å¯èƒ½çš„è”åˆåˆ†å¸ƒä¸­èƒ½å¤Ÿå¯¹è¿™ä¸ªæœŸæœ›å€¼å–åˆ°çš„ä¸‹ç•Œ $\inf_{\gamma \sim \Pi (P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} [||x - y||$ ï¼Œå°±å®šä¹‰ä¸ºWassersteinè·ç¦»ã€‚ç›´è§‚ä¸Šå¯ä»¥æŠŠ$\mathbb{E}_{(x, y) \sim \gamma} [||x - y||]$ç†è§£ä¸ºåœ¨$\gamma$è¿™ä¸ªâ€œè·¯å¾„è§„åˆ’â€ä¸‹æŠŠ$P_r$è¿™å †â€œæ²™åœŸâ€æŒªåˆ°$P_g$â€œä½ç½®â€æ‰€éœ€çš„â€œæ¶ˆè€—â€ï¼Œè€Œ$W(P_r, P_g)$å°±æ˜¯â€œæœ€ä¼˜è·¯å¾„è§„åˆ’â€ä¸‹çš„â€œæœ€å°æ¶ˆè€—â€ï¼Œæ‰€ä»¥æ‰å«Earth-Moverï¼ˆæŽ¨åœŸæœºï¼‰è·ç¦»ã€‚Wassersteinè·ç¦»ç›¸æ¯”KLæ•£åº¦ã€JSæ•£åº¦çš„ä¼˜è¶Šæ€§åœ¨äºŽï¼Œå³ä¾¿ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰é‡å ï¼ŒWassersteinè·ç¦»ä»ç„¶èƒ½å¤Ÿåæ˜ å®ƒä»¬çš„è¿œè¿‘ã€‚WGANæœ¬ä½œé€šè¿‡ç®€å•çš„ä¾‹å­å±•ç¤ºäº†è¿™ä¸€ç‚¹ã€‚è€ƒè™‘å¦‚ä¸‹äºŒç»´ç©ºé—´ä¸­çš„ä¸¤ä¸ªåˆ†å¸ƒ$P_1$å’Œ$P_2$ï¼Œ$P_1$åœ¨çº¿æ®µABä¸Šå‡åŒ€åˆ†å¸ƒï¼Œ$P_2$åœ¨çº¿æ®µCDä¸Šå‡åŒ€åˆ†å¸ƒï¼Œé€šè¿‡æŽ§åˆ¶å‚æ•°$\theta$å¯ä»¥æŽ§åˆ¶ç€ä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»è¿œè¿‘ã€‚æ­¤æ—¶å®¹æ˜“å¾—åˆ°ï¼ˆè¯»è€…å¯è‡ªè¡ŒéªŒè¯ï¼‰$$KL(P_1 || P_2) = KL(P_1 || P_2) =\begin{cases}+\infty &amp; \text{if $\theta \neq 0$} \\0 &amp; \text{if $\theta = 0$}\end{cases}$$$$JS(P_1||P_2)=\begin{cases}\log 2 &amp; \text{if $\theta \neq 0$} \\0 &amp; \text{if $\theta - 0$}\end{cases}$$$$W(P_0, P_1) = |\theta|$$KLæ•£åº¦å’ŒJSæ•£åº¦æ˜¯çªå˜çš„ï¼Œè¦ä¹ˆæœ€å¤§è¦ä¹ˆæœ€å°ï¼ŒWassersteinè·ç¦»å´æ˜¯å¹³æ»‘çš„ï¼Œå¦‚æžœæˆ‘ä»¬è¦ç”¨æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–$\theta$è¿™ä¸ªå‚æ•°ï¼Œå‰ä¸¤è€…æ ¹æœ¬æä¾›ä¸äº†æ¢¯åº¦ï¼ŒWassersteinè·ç¦»å´å¯ä»¥ã€‚ç±»ä¼¼åœ°ï¼Œåœ¨é«˜ç»´ç©ºé—´ä¸­å¦‚æžœä¸¤ä¸ªåˆ†å¸ƒä¸é‡å æˆ–è€…é‡å éƒ¨åˆ†å¯å¿½ç•¥ï¼Œåˆ™KLå’ŒJSæ—¢åæ˜ ä¸äº†è¿œè¿‘ï¼Œä¹Ÿæä¾›ä¸äº†æ¢¯åº¦ï¼Œä½†æ˜¯Wassersteinå´å¯ä»¥æä¾›æœ‰æ„ä¹‰çš„æ¢¯åº¦ã€‚ç¬¬å››éƒ¨åˆ†ï¼šä»ŽWassersteinè·ç¦»åˆ°WGANæ—¢ç„¶Wassersteinè·ç¦»æœ‰å¦‚æ­¤ä¼˜è¶Šçš„æ€§è´¨ï¼Œå¦‚æžœæˆ‘ä»¬èƒ½å¤ŸæŠŠå®ƒå®šä¹‰ä¸ºç”Ÿæˆå™¨çš„lossï¼Œä¸å°±å¯ä»¥äº§ç”Ÿæœ‰æ„ä¹‰çš„æ¢¯åº¦æ¥æ›´æ–°ç”Ÿæˆå™¨ï¼Œä½¿å¾—ç”Ÿæˆåˆ†å¸ƒè¢«æ‹‰å‘çœŸå®žåˆ†å¸ƒå—ï¼Ÿæ²¡é‚£ä¹ˆç®€å•ï¼Œå› ä¸ºWassersteinè·ç¦»å®šä¹‰ä¸­çš„$\inf_{\gamma \sim \Pi (P_r, P_g)}$æ²¡æ³•ç›´æŽ¥æ±‚è§£ï¼Œä¸è¿‡æ²¡å…³ç³»ï¼Œä½œè€…ç”¨äº†ä¸€ä¸ªå·²æœ‰çš„å®šç†æŠŠå®ƒå˜æ¢ä¸ºå¦‚ä¸‹å½¢å¼$$W(P_r, P_g) = \frac{1}{K} \sup_{||f||_L \leq K} \mathbb{E}_{x \sim P_r} [f(x)$$è¯æ˜Žè¿‡ç¨‹è¢«ä½œè€…ä¸¢åˆ°è®ºæ–‡é™„å½•ä¸­äº†ï¼Œæˆ‘ä»¬ä¹Ÿå§‘ä¸”ä¸ç®¡ï¼Œå…ˆçœ‹çœ‹ä¸Šå¼ç©¶ç«Ÿè¯´äº†ä»€ä¹ˆã€‚é¦–å…ˆéœ€è¦ä»‹ç»ä¸€ä¸ªæ¦‚å¿µâ€”â€”Lipschitzè¿žç»­ã€‚å®ƒå…¶å®žå°±æ˜¯åœ¨ä¸€ä¸ªè¿žç»­å‡½æ•°$f$ä¸Šé¢é¢å¤–æ–½åŠ äº†ä¸€ä¸ªé™åˆ¶ï¼Œè¦æ±‚å­˜åœ¨ä¸€ä¸ªå¸¸æ•°$K\geq 0$ä½¿å¾—å®šä¹‰åŸŸå†…çš„ä»»æ„ä¸¤ä¸ªå…ƒç´ $x_1$å’Œ$x_2$éƒ½æ»¡è¶³æ­¤æ—¶ç§°å‡½æ•°$f$çš„Lipschitzå¸¸æ•°ä¸º$K$ã€‚ç®€å•ç†è§£ï¼Œæ¯”å¦‚è¯´$f$çš„å®šä¹‰åŸŸæ˜¯å®žæ•°é›†åˆï¼Œé‚£ä¸Šé¢çš„è¦æ±‚å°±ç­‰ä»·äºŽ$f$çš„å¯¼å‡½æ•°ç»å¯¹å€¼ä¸è¶…è¿‡$K$ã€‚å†æ¯”å¦‚è¯´$\log (x)$å°±ä¸æ˜¯Lipschitzè¿žç»­ï¼Œå› ä¸ºå®ƒçš„å¯¼å‡½æ•°æ²¡æœ‰ä¸Šç•Œã€‚Lipschitzè¿žç»­æ¡ä»¶é™åˆ¶äº†ä¸€ä¸ªè¿žç»­å‡½æ•°çš„æœ€å¤§å±€éƒ¨å˜åŠ¨å¹…åº¦ã€‚å…¬å¼13çš„æ„æ€å°±æ˜¯åœ¨è¦æ±‚å‡½æ•°$f$çš„Lipschitzå¸¸æ•°$||f||_L$ä¸è¶…è¿‡$K$çš„æ¡ä»¶ä¸‹ï¼Œå¯¹æ‰€æœ‰å¯èƒ½æ»¡è¶³æ¡ä»¶çš„$f$å–åˆ°$\mathbb{E}_{x \sim P_r} [f(x)]$çš„ä¸Šç•Œï¼Œç„¶åŽå†é™¤ä»¥$K$ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ç»„å‚æ•°$w$æ¥å®šä¹‰ä¸€ç³»åˆ—å¯èƒ½çš„å‡½æ•°$f_w$ï¼Œæ­¤æ—¶æ±‚è§£å…¬å¼13å¯ä»¥è¿‘ä¼¼å˜æˆæ±‚è§£å¦‚ä¸‹å½¢å¼$$K \cdot W(P_r, P_g) \approx \max_{w: |f_w|_L \leq K} \mathbb{E}_{x \sim P_r} [f_w(x)$$å†ç”¨ä¸Šæˆ‘ä»¬æžæ·±åº¦å­¦ä¹ çš„äººæœ€ç†Ÿæ‚‰çš„é‚£ä¸€å¥—ï¼Œä¸å°±å¯ä»¥æŠŠ$f$ç”¨ä¸€ä¸ªå¸¦å‚æ•°$w$çš„ç¥žç»ç½‘ç»œæ¥è¡¨ç¤ºå˜›ï¼ç”±äºŽç¥žç»ç½‘ç»œçš„æ‹Ÿåˆèƒ½åŠ›è¶³å¤Ÿå¼ºå¤§ï¼Œæˆ‘ä»¬æœ‰ç†ç”±ç›¸ä¿¡ï¼Œè¿™æ ·å®šä¹‰å‡ºæ¥çš„ä¸€ç³»åˆ—$f_w$è™½ç„¶æ— æ³•å›Šæ‹¬æ‰€æœ‰å¯èƒ½ï¼Œä½†æ˜¯ä¹Ÿè¶³ä»¥é«˜åº¦è¿‘ä¼¼å…¬å¼13è¦æ±‚çš„é‚£ä¸ª$sup_{||f||_L \leq K} $äº†ã€‚æœ€åŽï¼Œè¿˜ä¸èƒ½å¿˜äº†æ»¡è¶³ $||f_w||_L \leq K$ è¿™ä¸ªé™åˆ¶ã€‚æˆ‘ä»¬å…¶å®žä¸å…³å¿ƒå…·ä½“çš„Kæ˜¯å¤šå°‘ï¼Œåªè¦å®ƒä¸æ˜¯æ­£æ— ç©·å°±è¡Œï¼Œå› ä¸ºå®ƒåªæ˜¯ä¼šä½¿å¾—æ¢¯åº¦å˜å¤§Kå€ï¼Œå¹¶ä¸ä¼šå½±å“æ¢¯åº¦çš„æ–¹å‘ã€‚æ‰€ä»¥ä½œè€…é‡‡å–äº†ä¸€ä¸ªéžå¸¸ç®€å•çš„åšæ³•ï¼Œå°±æ˜¯é™åˆ¶ç¥žç»ç½‘ç»œ$f_\theta$çš„æ‰€æœ‰å‚æ•°$w_i$çš„ä¸è¶…è¿‡æŸä¸ªèŒƒå›´$[-c, c]$ï¼Œæ¯”å¦‚$w_i \in [- 0.01, 0.01]$ï¼Œæ­¤æ—¶å…³äºŽè¾“å…¥æ ·æœ¬xçš„å¯¼æ•°$\frac{\partial f_w}{\partial x}$ä¹Ÿä¸ä¼šè¶…è¿‡æŸä¸ªèŒƒå›´ï¼Œæ‰€ä»¥ä¸€å®šå­˜åœ¨æŸä¸ªä¸çŸ¥é“çš„å¸¸æ•°Kä½¿å¾—$f_w$çš„å±€éƒ¨å˜åŠ¨å¹…åº¦ä¸ä¼šè¶…è¿‡å®ƒï¼ŒLipschitzè¿žç»­æ¡ä»¶å¾—ä»¥æ»¡è¶³ã€‚å…·ä½“åœ¨ç®—æ³•å®žçŽ°ä¸­ï¼Œåªéœ€è¦æ¯æ¬¡æ›´æ–°å®Œ$w$åŽæŠŠå®ƒclipå›žè¿™ä¸ªèŒƒå›´å°±å¯ä»¥äº†ã€‚åˆ°æ­¤ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¯ä»¥æž„é€ ä¸€ä¸ªå«å‚æ•°$w$ã€æœ€åŽä¸€å±‚ä¸æ˜¯éžçº¿æ€§æ¿€æ´»å±‚çš„åˆ¤åˆ«å™¨ç½‘ç»œ$f_w$ï¼Œåœ¨é™åˆ¶$w$ä¸è¶…è¿‡æŸä¸ªèŒƒå›´çš„æ¡ä»¶ä¸‹ï¼Œä½¿å¾—$$L = \mathbb{E}_{x \sim P_r} [f_w(x)$$å°½å¯èƒ½å–åˆ°æœ€å¤§ï¼Œæ­¤æ—¶Lå°±ä¼šè¿‘ä¼¼çœŸå®žåˆ†å¸ƒä¸Žç”Ÿæˆåˆ†å¸ƒä¹‹é—´çš„Wassersteinè·ç¦»ï¼ˆå¿½ç•¥å¸¸æ•°å€æ•°Kï¼‰ã€‚æ³¨æ„åŽŸå§‹GANçš„åˆ¤åˆ«å™¨åšçš„æ˜¯çœŸå‡äºŒåˆ†ç±»ä»»åŠ¡ï¼Œæ‰€ä»¥æœ€åŽä¸€å±‚æ˜¯sigmoidï¼Œä½†æ˜¯çŽ°åœ¨WGANä¸­çš„åˆ¤åˆ«å™¨$f_w$åšçš„æ˜¯è¿‘ä¼¼æ‹ŸåˆWassersteinè·ç¦»ï¼Œå±žäºŽå›žå½’ä»»åŠ¡ï¼Œæ‰€ä»¥è¦æŠŠæœ€åŽä¸€å±‚çš„sigmoidæ‹¿æŽ‰ã€‚æŽ¥ä¸‹æ¥ç”Ÿæˆå™¨è¦è¿‘ä¼¼åœ°æœ€å°åŒ–Wassersteinè·ç¦»ï¼Œå¯ä»¥æœ€å°åŒ–$L$ï¼Œç”±äºŽWassersteinè·ç¦»çš„ä¼˜è‰¯æ€§è´¨ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ‹…å¿ƒç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚å†è€ƒè™‘åˆ°$L$çš„ç¬¬ä¸€é¡¹ä¸Žç”Ÿæˆå™¨æ— å…³ï¼Œå°±å¾—åˆ°äº†WGANçš„ä¸¤ä¸ªlossã€‚$$\mathbb{E}_{x \sim P_g} [f_w(x)]$$ï¼ˆWGANç”Ÿæˆå™¨losså‡½æ•°ï¼‰$$\mathbb{E}_{x \sim P_g} [f_w(x)$$ï¼ˆWGANåˆ¤åˆ«å™¨losså‡½æ•°ï¼‰å¯ä»¥æŒ‡ç¤ºè®­ç»ƒè¿›ç¨‹ï¼Œå…¶æ•°å€¼è¶Šå°ï¼Œè¡¨ç¤ºçœŸå®žåˆ†å¸ƒä¸Žç”Ÿæˆåˆ†å¸ƒçš„Wassersteinè·ç¦»è¶Šå°ï¼ŒGANè®­ç»ƒå¾—è¶Šå¥½ã€‚WGANå®Œæ•´çš„ç®—æ³•æµç¨‹å·²ç»è´´è¿‡äº†ï¼Œä¸ºäº†æ–¹ä¾¿è¯»è€…æ­¤å¤„å†è´´ä¸€éï¼šä¸Šæ–‡è¯´è¿‡ï¼ŒWGANä¸ŽåŽŸå§‹GANç¬¬ä¸€ç§å½¢å¼ç›¸æ¯”ï¼Œåªæ”¹äº†å››ç‚¹ï¼šåˆ¤åˆ«å™¨æœ€åŽä¸€å±‚åŽ»æŽ‰sigmoidç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„lossä¸å–logæ¯æ¬¡æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°ä¹‹åŽæŠŠå®ƒä»¬çš„ç»å¯¹å€¼æˆªæ–­åˆ°ä¸è¶…è¿‡ä¸€ä¸ªå›ºå®šå¸¸æ•°cä¸è¦ç”¨åŸºäºŽåŠ¨é‡çš„ä¼˜åŒ–ç®—æ³•ï¼ˆåŒ…æ‹¬momentumå’ŒAdamï¼‰ï¼ŒæŽ¨èRMSPropï¼ŒSGDä¹Ÿè¡Œå‰ä¸‰ç‚¹éƒ½æ˜¯ä»Žç†è®ºåˆ†æžä¸­å¾—åˆ°çš„ï¼Œå·²ç»ä»‹ç»å®Œæ¯•ï¼›ç¬¬å››ç‚¹å´æ˜¯ä½œè€…ä»Žå®žéªŒä¸­å‘çŽ°çš„ï¼Œå±žäºŽtrickï¼Œç›¸å¯¹æ¯”è¾ƒâ€œçŽ„â€ã€‚ä½œè€…å‘çŽ°å¦‚æžœä½¿ç”¨Adamï¼Œåˆ¤åˆ«å™¨çš„lossæœ‰æ—¶å€™ä¼šå´©æŽ‰ï¼Œå½“å®ƒå´©æŽ‰æ—¶ï¼ŒAdamç»™å‡ºçš„æ›´æ–°æ–¹å‘ä¸Žæ¢¯åº¦æ–¹å‘å¤¹è§’çš„coså€¼å°±å˜æˆè´Ÿæ•°ï¼Œæ›´æ–°æ–¹å‘ä¸Žæ¢¯åº¦æ–¹å‘å—è¾•åŒ—è¾™ï¼Œè¿™æ„å‘³ç€åˆ¤åˆ«å™¨çš„lossæ¢¯åº¦æ˜¯ä¸ç¨³å®šçš„ï¼Œæ‰€ä»¥ä¸é€‚åˆç”¨Adamè¿™ç±»åŸºäºŽåŠ¨é‡çš„ä¼˜åŒ–ç®—æ³•ã€‚ä½œè€…æ”¹ç”¨RMSPropä¹‹åŽï¼Œé—®é¢˜å°±è§£å†³äº†ï¼Œå› ä¸ºRMSPropé€‚åˆæ¢¯åº¦ä¸ç¨³å®šçš„æƒ…å†µã€‚å¯¹WGANä½œè€…åšäº†ä¸å°‘å®žéªŒéªŒè¯ï¼Œæœ¬æ–‡åªææ¯”è¾ƒé‡è¦çš„ä¸‰ç‚¹ã€‚ç¬¬ä¸€ï¼Œåˆ¤åˆ«å™¨æ‰€è¿‘ä¼¼çš„Wassersteinè·ç¦»ä¸Žç”Ÿæˆå™¨çš„ç”Ÿæˆå›¾ç‰‡è´¨é‡é«˜åº¦ç›¸å…³ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼ˆæ­¤å³é¢˜å›¾ï¼‰ï¼šç¬¬äºŒï¼ŒWGANå¦‚æžœç”¨ç±»ä¼¼DCGANæž¶æž„ï¼Œç”Ÿæˆå›¾ç‰‡çš„æ•ˆæžœä¸ŽDCGANå·®ä¸å¤šï¼šä½†æ˜¯åŽ‰å®³çš„åœ°æ–¹åœ¨äºŽWGANä¸ç”¨DCGANå„ç§ç‰¹æ®Šçš„æž¶æž„è®¾è®¡ä¹Ÿèƒ½åšåˆ°ä¸é”™çš„æ•ˆæžœï¼Œæ¯”å¦‚å¦‚æžœå¤§å®¶ä¸€èµ·æ‹¿æŽ‰Batch Normalizationçš„è¯ï¼Œ DCGANå°±å´©äº†ï¼šå¦‚æžœWGANå’ŒåŽŸå§‹GANéƒ½ä½¿ç”¨å¤šå±‚å…¨è¿žæŽ¥ç½‘ç»œï¼ˆMLPï¼‰ï¼Œä¸ç”¨CNNï¼ŒWGANè´¨é‡ä¼šå˜å·®äº›ï¼Œä½†æ˜¯åŽŸå§‹GANä¸ä»…è´¨é‡å˜å¾—æ›´å·®ï¼Œè€Œä¸”è¿˜å‡ºçŽ°äº†collapse modeï¼Œå³å¤šæ ·æ€§ä¸è¶³ï¼šç¬¬ä¸‰ï¼Œåœ¨æ‰€æœ‰WGANçš„å®žéªŒä¸­æœªè§‚å¯Ÿåˆ°collapse modeï¼Œä½œè€…ä¹Ÿåªè¯´åº”è¯¥æ˜¯è§£å†³äº†ï¼Œæœ€åŽè¡¥å……ä¸€ç‚¹è®ºæ–‡æ²¡æåˆ°ï¼Œä½†æ˜¯æˆ‘ä¸ªäººè§‰å¾—æ¯”è¾ƒå¾®å¦™çš„é—®é¢˜ã€‚åˆ¤åˆ«å™¨æ‰€è¿‘ä¼¼çš„Wassersteinè·ç¦»èƒ½å¤Ÿç”¨æ¥æŒ‡ç¤ºå•æ¬¡è®­ç»ƒä¸­çš„è®­ç»ƒè¿›ç¨‹ï¼Œè¿™ä¸ªæ²¡é”™ï¼›æŽ¥ç€ä½œè€…åˆè¯´å®ƒå¯ä»¥ç”¨äºŽæ¯”è¾ƒå¤šæ¬¡è®­ç»ƒè¿›ç¨‹ï¼ŒæŒ‡å¼•è°ƒå‚ï¼Œæˆ‘å€’æ˜¯è§‰å¾—éœ€è¦å°å¿ƒäº›ã€‚æ¯”å¦‚è¯´æˆ‘ä¸‹æ¬¡è®­ç»ƒæ—¶æ”¹äº†åˆ¤åˆ«å™¨çš„å±‚æ•°ã€èŠ‚ç‚¹æ•°ç­‰è¶…å‚ï¼Œåˆ¤åˆ«å™¨çš„æ‹Ÿåˆèƒ½åŠ›å°±å¿…ç„¶æœ‰æ‰€æ³¢åŠ¨ï¼Œå†æ¯”å¦‚è¯´æˆ‘ä¸‹æ¬¡è®­ç»ƒæ—¶æ”¹äº†ç”Ÿæˆå™¨ä¸¤æ¬¡è¿­ä»£ä¹‹é—´ï¼Œåˆ¤åˆ«å™¨çš„è¿­ä»£æ¬¡æ•°ï¼Œè¿™ä¸¤ç§å¸¸è§çš„å˜åŠ¨éƒ½ä¼šä½¿å¾—Wassersteinè·ç¦»çš„æ‹Ÿåˆè¯¯å·®å°±ä¸Žä¸Šæ¬¡ä¸ä¸€æ ·ã€‚é‚£ä¹ˆè¿™ä¸ªæ‹Ÿåˆè¯¯å·®çš„å˜åŠ¨ç©¶ç«Ÿæœ‰å¤šå¤§ï¼Œæˆ–è€…è¯´ä¸åŒçš„äººåšå®žéªŒæ—¶åˆ¤åˆ«å™¨çš„æ‹Ÿåˆèƒ½åŠ›æˆ–è¿­ä»£æ¬¡æ•°ç›¸å·®å®žåœ¨å¤ªå¤§ï¼Œé‚£å®ƒä»¬ä¹‹é—´è¿˜èƒ½ä¸èƒ½ç›´æŽ¥æ¯”è¾ƒä¸Šè¿°æŒ‡æ ‡ï¼Œæˆ‘éƒ½æ˜¯å­˜ç–‘çš„ã€‚è¯„è®ºåŒºçš„çŸ¥å‹@Minjie Xu è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œç›¸æ¯”äºŽåˆ¤åˆ«å™¨è¿­ä»£æ¬¡æ•°çš„æ”¹å˜ï¼Œå¯¹åˆ¤åˆ«å™¨æž¶æž„è¶…å‚çš„æ”¹å˜ä¼šç›´æŽ¥å½±å“åˆ°å¯¹åº”çš„Lipschitzå¸¸æ•°ï¼Œè¿›è€Œæ”¹å˜è¿‘ä¼¼Wassersteinè·ç¦»çš„å€æ•°ï¼Œå‰åŽä¸¤è½®è®­ç»ƒçš„æŒ‡æ ‡å°±è‚¯å®šä¸èƒ½æ¯”è¾ƒäº†ï¼Œè¿™æ˜¯éœ€è¦åœ¨å®žé™…åº”ç”¨ä¸­æ³¨æ„çš„ã€‚å¯¹æ­¤æˆ‘æƒ³åˆ°äº†ä¸€ä¸ªå·¥ç¨‹åŒ–çš„è§£å†³æ–¹å¼ï¼Œä¸æ˜¯å¾ˆä¼˜é›…ï¼šå–åŒæ ·ä¸€å¯¹ç”Ÿæˆåˆ†å¸ƒå’ŒçœŸå®žåˆ†å¸ƒï¼Œè®©å‰åŽä¸¤ä¸ªä¸åŒæž¶æž„çš„åˆ¤åˆ«å™¨å„è‡ªæ‹Ÿåˆåˆ°æ”¶æ•›ï¼Œçœ‹æ”¶æ•›åˆ°çš„æŒ‡æ ‡å·®å¤šå°‘å€ï¼Œå¯ä»¥è¿‘ä¼¼è®¤ä¸ºæ˜¯åŽé¢çš„ç›¸å¯¹å‰é¢çš„å˜åŒ–å€æ•°ï¼ŒäºŽæ˜¯å°±å¯ä»¥ç”¨è¿™ä¸ªå˜åŒ–å€æ•°æ ¡æ­£å‰åŽä¸¤è½®è®­ç»ƒçš„æŒ‡æ ‡ã€‚ç¬¬äº”éƒ¨åˆ†ï¼šæ€»ç»“WGANå‰ä½œåˆ†æžäº†Ian Goodfellowæå‡ºçš„åŽŸå§‹GANä¸¤ç§å½¢å¼å„è‡ªçš„é—®é¢˜ï¼Œç¬¬ä¸€ç§å½¢å¼ç­‰ä»·åœ¨æœ€ä¼˜åˆ¤åˆ«å™¨ä¸‹ç­‰ä»·äºŽæœ€å°åŒ–ç”Ÿæˆåˆ†å¸ƒä¸ŽçœŸå®žåˆ†å¸ƒä¹‹é—´çš„JSæ•£åº¦ï¼Œç”±äºŽéšæœºç”Ÿæˆåˆ†å¸ƒå¾ˆéš¾ä¸ŽçœŸå®žåˆ†å¸ƒæœ‰ä¸å¯å¿½ç•¥çš„é‡å ä»¥åŠJSæ•£åº¦çš„çªå˜ç‰¹æ€§ï¼Œä½¿å¾—ç”Ÿæˆå™¨é¢ä¸´æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼›ç¬¬äºŒç§å½¢å¼åœ¨æœ€ä¼˜åˆ¤åˆ«å™¨ä¸‹ç­‰ä»·äºŽæ—¢è¦æœ€å°åŒ–ç”Ÿæˆåˆ†å¸ƒä¸ŽçœŸå®žåˆ†å¸ƒç›´æŽ¥çš„KLæ•£åº¦ï¼Œåˆè¦æœ€å¤§åŒ–å…¶JSæ•£åº¦ï¼Œç›¸äº’çŸ›ç›¾ï¼Œå¯¼è‡´æ¢¯åº¦ä¸ç¨³å®šï¼Œè€Œä¸”KLæ•£åº¦çš„ä¸å¯¹ç§°æ€§ä½¿å¾—ç”Ÿæˆå™¨å®å¯ä¸§å¤±å¤šæ ·æ€§ä¹Ÿä¸æ„¿ä¸§å¤±å‡†ç¡®æ€§ï¼Œå¯¼è‡´collapse modeçŽ°è±¡ã€‚WGANå‰ä½œé’ˆå¯¹åˆ†å¸ƒé‡å é—®é¢˜æå‡ºäº†ä¸€ä¸ªè¿‡æ¸¡è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å¯¹ç”Ÿæˆæ ·æœ¬å’ŒçœŸå®žæ ·æœ¬åŠ å™ªå£°ä½¿å¾—ä¸¤ä¸ªåˆ†å¸ƒäº§ç”Ÿé‡å ï¼Œç†è®ºä¸Šå¯ä»¥è§£å†³è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œå¯ä»¥æ”¾å¿ƒè®­ç»ƒåˆ¤åˆ«å™¨åˆ°æŽ¥è¿‘æœ€ä¼˜ï¼Œä½†æ˜¯æœªèƒ½æä¾›ä¸€ä¸ªæŒ‡ç¤ºè®­ç»ƒè¿›ç¨‹çš„å¯é æŒ‡æ ‡ï¼Œä¹Ÿæœªåšå®žéªŒéªŒè¯ã€‚WGANæœ¬ä½œå¼•å…¥äº†Wassersteinè·ç¦»ï¼Œç”±äºŽå®ƒç›¸å¯¹KLæ•£åº¦ä¸ŽJSæ•£åº¦å…·æœ‰ä¼˜è¶Šçš„å¹³æ»‘ç‰¹æ€§ï¼Œç†è®ºä¸Šå¯ä»¥è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚æŽ¥ç€é€šè¿‡æ•°å­¦å˜æ¢å°†Wassersteinè·ç¦»å†™æˆå¯æ±‚è§£çš„å½¢å¼ï¼Œåˆ©ç”¨ä¸€ä¸ªå‚æ•°æ•°å€¼èŒƒå›´å—é™çš„åˆ¤åˆ«å™¨ç¥žç»ç½‘ç»œæ¥æœ€å¤§åŒ–è¿™ä¸ªå½¢å¼ï¼Œå°±å¯ä»¥è¿‘ä¼¼Wassersteinè·ç¦»ã€‚åœ¨æ­¤è¿‘ä¼¼æœ€ä¼˜åˆ¤åˆ«å™¨ä¸‹ä¼˜åŒ–ç”Ÿæˆå™¨ä½¿å¾—Wassersteinè·ç¦»ç¼©å°ï¼Œå°±èƒ½æœ‰æ•ˆæ‹‰è¿‘ç”Ÿæˆåˆ†å¸ƒä¸ŽçœŸå®žåˆ†å¸ƒã€‚WGANæ—¢è§£å†³äº†è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œä¹Ÿæä¾›äº†ä¸€ä¸ªå¯é çš„è®­ç»ƒè¿›ç¨‹æŒ‡æ ‡ï¼Œè€Œä¸”è¯¥æŒ‡æ ‡ç¡®å®žä¸Žç”Ÿæˆæ ·æœ¬çš„è´¨é‡é«˜åº¦ç›¸å…³ã€‚ä½œè€…å¯¹WGANè¿›è¡Œäº†å®žéªŒéªŒè¯ã€‚]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)]]></title>
      <url>%2F2017%2F04%2F28%2FGenerative-Adversarial-Networks-GANs-in-50-lines-of-code-PyTorch%2F</url>
      <content type="text"><![CDATA[Source BlogPyTorch Install: http://pytorch.org/The models play two distinct (literally, adversarial) roles. Given some real data set R, G is the generator, trying to create fake data that looks just like the genuine data, while D is the discriminator, getting data from either the real set or G and labeling the difference. Goodfellowâ€™s metaphor (and a fine one it is) was that G was like a team of forgers trying to match real paintings with their output, while D was the team of detectives trying to tell the difference. (Except that in this case, the forgers G never get to see the original data â€” only the judgments of D. Theyâ€™re like blind forgers.)In the ideal case, both D and G would get better over time until G had essentially become a â€œmaster forgerâ€ of the genuine article and D was at a loss, â€œunable to differentiate between the two distributions.â€In practice, what Goodfellow had shown was that G would be able to perform a form of unsupervised learning on the original dataset, finding some way of representing that data in a (possibly) much lower-dimensional manner. And as Yann LeCun famously stated, unsupervised learning is the â€œcakeâ€ of true AI.This powerful technique seems like it must require a metric ton of code just to get started, right? Nope. Using PyTorch, we can actually create a very simple GAN in under 50 lines of code. There are really only 5 components to think about:R: The original, genuine data setI: The random noise that goes into the generator as a source of entropyG: The generator which tries to copy/mimic the original data setD: The discriminator which tries to tell apart Gâ€™s output from RThe actual â€˜trainingâ€™ loop where we teach G to trick D and D to beware G.1.) R: In our case, weâ€™ll start with the simplest possible R â€” a bell curve. This function takes a mean and a standard deviation and returns a function which provides the right shape of sample data from a Gaussian with those parameters. In our sample code, weâ€™ll use a mean of 4.0 and a standard deviation of 1.25.2.) I: The input into the generator is also random, but to make our job a little bit harder, letâ€™s use a uniform distribution rather than a normal one. This means that our model G canâ€™t simply shift/scale the input to copy R, but has to reshape the data in a non-linear way.3.) G: The generator is a standard feedforward graph â€” two hidden layers, three linear maps. Weâ€™re using an ELU (exponential linear unit) becausetheyâ€™re the new black, yo. G is going to get the uniformly distributed data samples from I and somehow mimic the normally distributed samples from R.4.) D: The discriminator code is very similar to Gâ€™s generator code; a feedforward graph with two hidden layers and three linear maps. Itâ€™s going to get samples from either R or G and will output a single scalar between 0 and 1, interpreted as â€˜fakeâ€™ vs. â€˜realâ€™. This is about as milquetoast as a neural net can get.5.) Finally, the training loop alternates between two modes: first training D on real data vs. fake data, with accurate labels (think of this as Police Academy); and then training G to fool D, with inaccurate labels (this is more like those preparation montages from Oceanâ€™s Eleven). Itâ€™s a fight between good and evil, people.Even if you havenâ€™t seen PyTorch before, you can probably tell whatâ€™s going on. In the first (green) section, we push both types of data through D and apply a differentiable criterion to Dâ€™s guesses vs. the actual labels. That pushing is the â€˜forwardâ€™ step; we then call â€˜backward()â€™ explicitly in order to calculate gradients, which are then used to update Dâ€™s parameters in the d_optimizer step() call. G is used but isnâ€™t trained here.Then in the last (red) section, we do the same thing for G â€” note that we also run Gâ€™s output through D (weâ€™re essentially giving the forger a detective to practice on) but we do not optimize or change D at this step. We donâ€™t want the detective D to learn the wrong labels. Hence, we only call g_optimizer.step().Andâ€¦thatâ€™s all. Thereâ€™s some other boilerplate code but the GAN-specific stuff is just those 5 components, nothing else.After a few thousand rounds of this forbidden dance between D and G, what do we get? The discriminator D gets good very quickly (while G slowly moves up), but once it gets to a certain level of power, G has a worthy adversary and begins to improve. Really improve.Over 20,000 training rounds, the mean of Gâ€™s output overshoots 4.0 but then comes back in a fairly stable, correct range (left). Likewise, the standard deviation initially drops in the wrong direction but then rises up to the desired 1.25 range (right), matching R.Ok, so the basic stats match R, eventually. How about the higher moments? Does the shape of the distribution look right? After all, you could certainly have a uniform distribution with a mean of 4.0 and a standard deviation of 1.25, but that wouldnâ€™t really match R. Letâ€™s show the final distribution emitted by G.Not bad. The left tail is a bit longer than the right, but the skew and kurtosis are, shall we say, evocative of the original Gaussian.G recovers the original distribution R nearly perfectly â€” and D is left cowering in the corner, mumbling to itself, unable to tell fact from fiction. This is precisely the behavior we want (see Figure 1 in Goodfellow). From fewer than 50 lines of code.Goodfellow would go on to publish many other papers on GANs, including a 2016 gem describing some practical improvements, including the minibatch discrimination method adapted here. And hereâ€™s a 2-hour tutorial he presented at NIPS 2016. For TensorFlow users, hereâ€™s a parallel post from Aylien on GANs.Ok. Enough talk. Go look at the code.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130#!/usr/bin/env python# Generative Adversarial Networks (GAN) example in PyTorch.# See related blog post at https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f#.sch4xgsa9import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.autograd import Variable# Data paramsdata_mean = 4data_stddev = 1.25# Model paramsg_input_size = 1 # Random noise dimension coming into generator, per output vectorg_hidden_size = 50 # Generator complexityg_output_size = 1 # size of generated output vectord_input_size = 100 # Minibatch size - cardinality of distributionsd_hidden_size = 50 # Discriminator complexityd_output_size = 1 # Single dimension for 'real' vs. 'fake'minibatch_size = d_input_sized_learning_rate = 2e-4 # 2e-4g_learning_rate = 2e-4optim_betas = (0.9, 0.999)num_epochs = 30000print_interval = 200d_steps = 1 # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generatorg_steps = 1# ### Uncomment only one of these#(name, preprocess, d_input_func) = ("Raw data", lambda data: data, lambda x: x)(name, preprocess, d_input_func) = ("Data and variances", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)print("Using data [%s]" % (name))# ##### DATA: Target data and generator input datadef get_distribution_sampler(mu, sigma): return lambda n: torch.Tensor(np.random.normal(mu, sigma, (1, n))) # Gaussiandef get_generator_input_sampler(): return lambda m, n: torch.rand(m, n) # Uniform-dist data into generator, _NOT_ Gaussian# ##### MODELS: Generator model and discriminator modelclass Generator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Generator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.sigmoid(self.map2(x)) return self.map3(x)class Discriminator(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(Discriminator, self).__init__() self.map1 = nn.Linear(input_size, hidden_size) self.map2 = nn.Linear(hidden_size, hidden_size) self.map3 = nn.Linear(hidden_size, output_size) def forward(self, x): x = F.elu(self.map1(x)) x = F.elu(self.map2(x)) return F.sigmoid(self.map3(x))def extract(v): return v.data.storage().tolist()def stats(d): return [np.mean(d), np.std(d)]def decorate_with_diffs(data, exponent): mean = torch.mean(data.data, 1) mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0]) diffs = torch.pow(data - Variable(mean_broadcast), exponent) return torch.cat([data, diffs], 1)d_sampler = get_distribution_sampler(data_mean, data_stddev)gi_sampler = get_generator_input_sampler()G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)D = Discriminator(input_size=d_input_func(d_input_size), hidden_size=d_hidden_size, output_size=d_output_size)criterion = nn.BCELoss() # Binary cross entropy: http://pytorch.org/docs/nn.html#bcelossd_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)for epoch in range(num_epochs): for d_index in range(d_steps): # 1. Train D on real+fake D.zero_grad() # 1A: Train D on real d_real_data = Variable(d_sampler(d_input_size)) d_real_decision = D(preprocess(d_real_data)) d_real_error = criterion(d_real_decision, Variable(torch.ones(1))) # ones = true d_real_error.backward() # compute/store gradients, but don't change params # 1B: Train D on fake d_gen_input = Variable(gi_sampler(minibatch_size, g_input_size)) d_fake_data = G(d_gen_input).detach() # detach to avoid training G on these labels d_fake_decision = D(preprocess(d_fake_data.t())) d_fake_error = criterion(d_fake_decision, Variable(torch.zeros(1))) # zeros = fake d_fake_error.backward() d_optimizer.step() # Only optimizes D's parameters; changes based on stored gradients from backward() for g_index in range(g_steps): # 2. Train G on D's response (but DO NOT train D on these labels) G.zero_grad() gen_input = Variable(gi_sampler(minibatch_size, g_input_size)) g_fake_data = G(gen_input) dg_fake_decision = D(preprocess(g_fake_data.t())) g_error = criterion(dg_fake_decision, Variable(torch.ones(1))) # we want to fool, so pretend it's all genuine g_error.backward() g_optimizer.step() # Only optimizes G's parameters if epoch % print_interval == 0: print("%s: D: %s/%s G: %s (Real: %s, Fake: %s) " % (epoch, extract(d_real_error)[0], extract(d_fake_error)[0], extract(g_error)[0], stats(extract(d_real_data)), stats(extract(d_fake_data))))Resultï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152ewan@ubuntu:~/Documents/gan/pytorch-generative-adversarial-networks$ python gan_pytorch.py Using data [Data and variances]0: D: 0.636019647121/0.687892377377 G: 0.692580163479 (Real: [4.0121619534492492, 1.3228379995364423], Fake: [0.36497069358825684, 0.0040907625909989871]) 200: D: 2.92067015835e-05/0.474851727486 G: 1.00973010063 (Real: [4.0935744738578794, 1.3016500752040552], Fake: [-0.5716635638475418, 0.019948046232028654]) 400: D: 0.0014917049557/0.502498149872 G: 0.943185687065 (Real: [4.198446000814438, 1.1262929992527102], Fake: [-0.21786054879426955, 0.0067362612730766476]) 600: D: 6.4969262894e-06/0.384293109179 G: 1.15257537365 (Real: [3.8602226501703263, 1.3292726136430937], Fake: [-0.29857088595628739, 0.03924369275813562]) 800: D: 1.84774467016e-06/0.211148008704 G: 1.67116880417 (Real: [4.0269100540876392, 1.2954351206409835], Fake: [-0.32296697288751602, 0.14901211840131676]) 1000: D: 9.02455067262e-05/0.0219078511 G: 4.19585323334 (Real: [3.9491306754946707, 1.3613105655283608], Fake: [0.13110455054789782, 0.5252103421913964]) 1200: D: 0.00441630883142/0.137605398893 G: 2.78980493546 (Real: [4.238747425079346, 1.1837142728845262], Fake: [2.3851456820964811, 0.69947230698573948]) 1400: D: 0.291683584452/0.824121117592 G: 0.26126781106 (Real: [3.8486315739154815, 1.2074486225815622], Fake: [3.4868409335613251, 1.2438192602257458]) 1600: D: 0.503275632858/1.08712184429 G: 0.628099560738 (Real: [3.7856648898124696, 1.1925325100947208], Fake: [3.9149187129735945, 1.5374543372663099]) 1800: D: 0.992162883282/0.955306172371 G: 0.215137541294 (Real: [3.9097139459848402, 1.3729001379532129], Fake: [4.9751595187187192, 1.2850838287273094]) 2000: D: 0.701098382473/0.634775817394 G: 0.389043629169 (Real: [3.9641699814796447, 1.1512756986625183], Fake: [5.0374661159515384, 1.5190411587235346]) 2200: D: 0.510353624821/0.350295126438 G: 1.5988701582 (Real: [4.0406568145751951, 1.3612318676859239], Fake: [5.4763065743446351, 1.2736378899688456]) 2400: D: 0.895085930824/0.400622785091 G: 0.922062814236 (Real: [3.8292097043991089, 1.1506111704583193], Fake: [4.5642045128345492, 1.7082890861364539]) 2600: D: 0.802581310272/0.717123866081 G: 0.572393655777 (Real: [4.0654918360710148, 1.2552944260604222], Fake: [5.1286249160766602, 1.0479449058428656]) 2800: D: 0.51098883152/0.489002883434 G: 0.842381119728 (Real: [4.0405197954177856, 1.136660175398452], Fake: [3.9549839448928834, 1.1751749984899784]) 3000: D: 0.496278882027/0.97537201643 G: 0.753688693047 (Real: [4.0026307255029678, 1.2446167315972034], Fake: [3.2340782660245897, 1.2949288892421307]) 3200: D: 0.696556508541/0.829834342003 G: 0.475445389748 (Real: [3.9983750417828561, 1.2828095340103229], Fake: [3.5434492731094362, 0.98673911467128028]) 3400: D: 0.479906737804/0.477254271507 G: 1.2421528101 (Real: [4.1585888534784319, 1.2672863214247221], Fake: [3.3173918831348419, 1.156708995162234]) 3600: D: 1.36562228203/0.508370876312 G: 0.550418972969 (Real: [4.0406067597866056, 1.1363201759386616], Fake: [4.4300824308395388, 1.0639278538481793]) 3800: D: 0.538426816463/0.622343420982 G: 0.786149024963 (Real: [4.0097330248355867, 1.1609232820569348], Fake: [4.5179304122924808, 1.2347411732817635]) 4000: D: 0.350504934788/0.361344873905 G: 0.728424191475 (Real: [3.7975878280401232, 1.2378775025626094], Fake: [4.3484812033176423, 1.4327683271077338]) 4200: D: 0.912463009357/0.779066801071 G: 0.840294659138 (Real: [3.9861780107021332, 1.2293009498211762], Fake: [4.0718169224262235, 1.2044778720046834]) 4400: D: 0.814347147942/0.794115483761 G: 0.889387726784 (Real: [3.9556436133384705, 1.1131208050960595], Fake: [3.6148070895671847, 1.1790021094109027]) 4600: D: 0.637132883072/0.639598190784 G: 0.835896074772 (Real: [4.0807307386398319, 1.1590112689981971], Fake: [3.6376679444313051, 1.2540016088688517]) 4800: D: 0.816388785839/0.629823803902 G: 0.6337043643 (Real: [4.1595975148677828, 1.2996693029809485], Fake: [4.0303308999538423, 1.3050560562935769]) 5000: D: 1.38226401806/0.714248239994 G: 1.17240273952 (Real: [3.9217003214359285, 1.3408209709046912], Fake: [4.4204820060729979, 1.0378887480226417]) 5200: D: 0.752707779408/0.432243227959 G: 0.735915839672 (Real: [4.033863249272108, 1.417255801501303], Fake: [3.7434970003366472, 1.4305561672741818]) 5400: D: 0.672449588776/0.694190680981 G: 0.671269893646 (Real: [3.9849637061357499, 1.3054745436415693], Fake: [3.7987613070011137, 1.1584021967574571]) 5600: D: 0.633513212204/0.678804934025 G: 0.736048042774 (Real: [3.8742538380622862, 1.1924929483627851], Fake: [4.0905960440635685, 1.0496450658176097]) 5800: D: 0.954816102982/0.619474828243 G: 0.847522497177 (Real: [4.0848416697978971, 1.2377045321962332], Fake: [4.5059887909889218, 1.0769809353783582]) 6000: D: 0.634225904942/0.653471052647 G: 0.402414888144 (Real: [3.9909452509880068, 1.2152347623325401], Fake: [3.9412865948677065, 1.2808620107297906]) 6200: D: 0.733776032925/0.414616316557 G: 0.969770550728 (Real: [4.0096452310681343, 1.2858629342885464], Fake: [3.4776910370588303, 1.4216167469252254]) 6400: D: 0.483776688576/0.456314682961 G: 0.42595911026 (Real: [4.16927042722702, 1.2557057135387499], Fake: [3.905275868177414, 1.3509040440658031]) 6600: D: 1.06177055836/0.443961560726 G: 0.910483181477 (Real: [4.0327691116929056, 1.1752792712434861], Fake: [4.1322225379943847, 1.3041032842304898]) 6800: D: 0.911615252495/0.851063728333 G: 0.822307884693 (Real: [4.0429812586307525, 1.0149434426406105], Fake: [4.181604235172272, 1.1091966315801844]) 7000: D: 0.859644412994/0.819373309612 G: 0.683367550373 (Real: [4.0413902151584624, 1.2697299173474621], Fake: [3.6461249232292174, 1.1392232969008105]) 7200: D: 0.697537004948/1.29639554024 G: 0.567749083042 (Real: [3.9289280462265013, 1.1476723124689931], Fake: [4.3612218284606934, 1.1698644305174593]) 7400: D: 0.892510712147/0.93148213625 G: 1.18729686737 (Real: [3.9838603484630584, 1.10640478112829], Fake: [4.1228645443916321, 1.2695625804586594]) 7600: D: 0.855136275291/0.683420717716 G: 0.87994658947 (Real: [4.1161885654926298, 1.1923004904972447], Fake: [3.6958885985612868, 1.3379389180110717]) 7800: D: 0.549697399139/1.37823116779 G: 0.398991644382 (Real: [4.2173074555397037, 1.2371073094023581], Fake: [3.8741448554396629, 1.3837623378110455]) 8000: D: 1.35398185253/0.410179078579 G: 0.527717351913 (Real: [3.9588229835033415, 1.3744496473744439], Fake: [3.9429207968711855, 1.3684983506717674]) 8200: D: 0.700774013996/0.295857429504 G: 0.803082704544 (Real: [3.8515358114242555, 1.2566173136350174], Fake: [3.7108538401126863, 1.3342916614304938]) 8400: D: 0.689352571964/0.590398311615 G: 0.698961615562 (Real: [3.965521250963211, 1.2231963456729893], Fake: [4.6866454958915709, 1.1286615282559416]) 8600: D: 0.19632807374/0.604559898376 G: 0.812706291676 (Real: [3.8928249645233155, 1.3264703109197318], Fake: [3.918080286383629, 1.2016505045193488]) 8800: D: 0.595732450485/0.572122216225 G: 0.738678693771 (Real: [3.7554583859443667, 1.2011572644775179], Fake: [3.8252914756536485, 1.1905187885079342]) 9000: D: 0.232542961836/1.26930451393 G: 0.834500789642 (Real: [3.9203160056471824, 1.2725988502730134], Fake: [4.1613124001026156, 1.2681795442466237]) 9200: D: 1.257376194/0.5735257864 G: 0.554405272007 (Real: [3.8860677522420883, 1.1041807259307903], Fake: [3.9102136331796644, 1.3811967247690093]) 9400: D: 0.610212028027/0.538761377335 G: 0.558459818363 (Real: [4.0015355503559116, 0.99711450973270277], Fake: [3.8555663478374482, 1.1037480705144518]) 9600: D: 0.702151358128/0.81621837616 G: 0.706716835499 (Real: [4.0513852632045744, 1.1984303669025829], Fake: [4.2933621263504032, 1.1478353305254103]) 9800: D: 0.511451423168/0.670217812061 G: 0.873916983604 (Real: [3.935146123766899, 1.3218541944694313], Fake: [4.2863738107681275, 1.1362357473661524]) 10000: D: 0.587130308151/0.764386773109 G: 0.714644312859 (Real: [4.0829932641983033, 1.1844677307174318], Fake: [4.2149634605646131, 1.1542778585504672]) 10200: D: 0.454408079386/0.390097141266 G: 0.694087386131 (Real: [3.9480907583236693, 1.2586832917742197], Fake: [3.9525690937042235, 1.3555640918653922]) 10400: D: 0.232991695404/0.377689123154 G: 0.839949011803 (Real: [3.9636431083083155, 1.2146210496905581], Fake: [4.0022356742620468, 1.0348462356745984]) 10600: D: 0.887756228447/0.452646583319 G: 0.776298880577 (Real: [4.1107078218460087, 1.3061081296488184], Fake: [4.3001403945684435, 1.3191353715419794]) 10800: D: 0.988030552864/0.472889751196 G: 2.00703763962 (Real: [4.1303015506267551, 1.2646447231333668], Fake: [4.2425211107730867, 1.2706986066792705]) 11000: D: 0.962553679943/1.00584948063 G: 0.458068579435 (Real: [4.1017441129684444, 1.1564779436003478], Fake: [3.861787896156311, 1.2478181443952361]) 11200: D: 0.404395908117/0.560545325279 G: 0.764987766743 (Real: [3.8819530367851258, 1.1290593525971337], Fake: [4.0393019503355028, 1.1760851438968263]) 11400: D: 1.04482722282/0.170368790627 G: 0.979512214661 (Real: [4.0775347077846531, 1.1743573984958275], Fake: [4.4076948529481887, 1.1430737801156545]) 11600: D: 0.767144262791/0.419019073248 G: 0.804197788239 (Real: [4.1507718646526337, 1.2935215526943189], Fake: [4.2565110635757444, 1.1195747875890809]) 11800: D: 0.328228145838/0.192100420594 G: 0.694948136806 (Real: [4.2615561389923098, 1.3187283101366121], Fake: [3.7841238260269163, 1.2796545407667934]) 12000: D: 0.939581632614/0.512252509594 G: 0.486280798912 (Real: [4.1770594882965089, 1.2492834466325793], Fake: [4.0997331076860428, 1.0701209918243111]) 12200: D: 0.964525461197/0.397465586662 G: 1.45534229279 (Real: [3.9129967219382524, 1.3473476671217695], Fake: [4.3561846733093263, 1.1667221650406194]) 12400: D: 0.516430974007/0.255626231432 G: 0.753806650639 (Real: [3.9942912605404852, 1.3623400447216258], Fake: [4.2171517282724382, 1.2046534326031684]) 12600: D: 0.050210531801/0.567070662975 G: 0.887824892998 (Real: [3.9560802054405211, 1.3569670682588555], Fake: [3.6434229278564452, 1.2798963544271591]) 12800: D: 0.566556215286/1.45121753216 G: 2.67591071129 (Real: [4.0868541407585148, 1.1440918337515926], Fake: [3.7308121472597122, 1.2567484994327229]) 13000: D: 0.285438686609/1.26493763924 G: 0.714931368828 (Real: [4.0406689298152925, 1.2295255598171184], Fake: [4.1976348906755447, 1.2778464434389283]) 13200: D: 0.420082330704/0.20268279314 G: 1.13221895695 (Real: [4.0006502330303189, 1.1790149224725006], Fake: [4.2336275362968445, 1.2803975596845565]) 13400: D: 0.219869300723/0.733704686165 G: 1.4634616375 (Real: [3.8348834168910981, 1.240605849665303], Fake: [3.8208065938949587, 1.3042463825727604]) 13600: D: 1.35286784172/0.161317944527 G: 2.29795908928 (Real: [4.0841373348236081, 1.2295542819596996], Fake: [4.0513113558292391, 1.2789595441318489]) 13800: D: 0.188396275043/0.38589566946 G: 1.38826131821 (Real: [4.0228236329555509, 1.3524482715610078], Fake: [4.2307587480545044, 1.2042737228043698]) 14000: D: 0.0101562952623/0.363918542862 G: 1.24292945862 (Real: [4.0695835274457934, 1.4484548400603423], Fake: [4.3588982570171355, 1.2305509242343933]) 14200: D: 0.308517187834/0.687216579914 G: 0.831201374531 (Real: [4.1314239382743834, 1.2039768851618762], Fake: [4.3469831347465515, 1.1622408025070994]) 14400: D: 1.05658388138/0.777651846409 G: 0.713593065739 (Real: [3.9307258637249469, 1.3932677098843045], Fake: [3.8781710839271546, 1.3920662615905985]) 14600: D: 0.428974717855/0.430344074965 G: 0.865560889244 (Real: [4.2443156433105464, 1.4786604488020483], Fake: [3.9386759352684022, 1.2173706417721266]) 14800: D: 0.358524769545/0.631785154343 G: 1.72760403156 (Real: [4.0897545439004901, 1.3611061267905207], Fake: [4.0185626268386843, 1.2011546705663261]) 15000: D: 0.451200634241/0.451773911715 G: 1.10325527191 (Real: [3.9933083570003509, 1.0881706638388742], Fake: [3.902902855873108, 1.1771562868487595]) 15200: D: 0.756480932236/0.419855684042 G: 0.942300021648 (Real: [4.1753564620018002, 1.3629881946025171], Fake: [3.8721090507507325, 1.189488508024922]) 15400: D: 0.219109147787/0.190036550164 G: 2.20304942131 (Real: [3.9836783826351168, 1.4838718408508595], Fake: [3.9491609585285188, 1.1700151592543104]) 15600: D: 1.01965582371/0.519556045532 G: 1.10594069958 (Real: [4.1213941669464109, 1.2398676800048194], Fake: [4.1908504700660707, 1.1195751576139747]) 15800: D: 0.733263611794/0.697221815586 G: 0.84056687355 (Real: [4.0593542096018789, 1.1946663317303297], Fake: [4.3031868946552274, 1.0306412415157991]) 16000: D: 0.400649875402/0.377974271774 G: 1.2899967432 (Real: [4.0140545344352718, 1.2630515897106358], Fake: [4.1656066524982451, 1.1779954377184654]) 16200: D: 0.34089872241/0.265896707773 G: 1.11251270771 (Real: [4.0408088731765748, 1.3839176416694203], Fake: [4.0593357777595518, 1.2213436233279213]) 16400: D: 0.00472234329209/0.513436615467 G: 1.63225841522 (Real: [4.1417997646331788, 1.2449733327544124], Fake: [3.7269023895263671, 1.1296458384504016]) 16600: D: 0.756382524967/0.66779255867 G: 0.536718785763 (Real: [3.9379871004819869, 1.278594816781579], Fake: [3.8750299978256226, 1.2829775944385431]) 16800: D: 0.879319548607/0.169020995498 G: 2.33787298203 (Real: [4.2075482982397077, 1.3725696551173026], Fake: [3.6744112837314606, 1.3225226221432227]) 17000: D: 0.0482731573284/1.43823099136 G: 1.15067052841 (Real: [4.0404629743099214, 1.218948521692204], Fake: [4.0387165582180025, 1.2794767516999943]) 17200: D: 2.88490628009e-05/0.57872825861 G: 0.495411038399 (Real: [3.9901529085636138, 1.4349120434336065], Fake: [4.0573103535175328, 1.1918079188127153]) 17400: D: 0.231002807617/1.2511702776 G: 1.33606302738 (Real: [3.7472488379478452, 1.1658634335870959], Fake: [3.9354779303073881, 1.2931455406139682]) 17600: D: 0.181431129575/0.149175107479 G: 2.51311731339 (Real: [4.1270963573455814, 1.312367798822683], Fake: [4.3470913958549495, 1.1818067904116243]) 17800: D: 0.830040276051/0.415931969881 G: 1.57710897923 (Real: [3.99146986246109, 1.0836663745208763], Fake: [4.3325731372833252, 1.266683405420135]) 18000: D: 0.20047518611/0.460676729679 G: 2.56421780586 (Real: [4.3388666504621503, 1.3881540592894346], Fake: [3.9820314025878907, 1.0436684747098013]) 18200: D: 0.0659740716219/0.428199917078 G: 0.931035280228 (Real: [3.8892200005054476, 1.2217018988161374], Fake: [3.8822696304321287, 1.304586899060783]) 18400: D: 0.791511416435/0.56503880024 G: 1.98549497128 (Real: [3.7894453473389147, 1.3567878969348022], Fake: [4.0909739780426024, 1.2361544714927677]) 18600: D: 1.15297484398/0.102882102132 G: 1.85704553127 (Real: [4.2316720616817474, 1.2603607958456993], Fake: [3.7415710711479186, 1.311454258421634]) 18800: D: 1.06078708172/0.366641134024 G: 0.914008259773 (Real: [3.9394708669185636, 1.2924449902046702], Fake: [3.9466111737489702, 1.137776845711856]) 19000: D: 0.374139517546/0.448283135891 G: 0.701639294624 (Real: [3.9492650532722475, 1.2348435624999976], Fake: [3.7365686148405075, 1.215777672310739]) 19200: D: 0.209440857172/0.522395193577 G: 0.707223057747 (Real: [3.8846979635953902, 1.2146658434075039], Fake: [4.1696245861053463, 1.2979841463522084]) 19400: D: 0.15654887259/0.133351936936 G: 1.43907415867 (Real: [4.0292040088772776, 1.2291287794070285], Fake: [3.8498308193683624, 1.1121767482065514]) 19600: D: 0.329566717148/0.222448319197 G: 0.429250627756 (Real: [3.7978928279876709, 1.1554982239517226], Fake: [3.5122534275054931, 1.2462801759237472]) 19800: D: 0.0176634714007/0.480926275253 G: 0.39424943924 (Real: [4.0822606313228604, 1.2484518469881001], Fake: [4.5482089626789097, 1.1266585202489452]) 20000: D: 0.45860773325/0.517112135887 G: 0.957448124886 (Real: [4.0875282829999922, 1.2310698313795749], Fake: [4.2767848205566406, 1.1186856033319335]) 20200: D: 1.71172118187/0.240745082498 G: 0.314642876387 (Real: [3.8525538909435273, 1.2094100771830765], Fake: [3.6543397814035417, 1.2917598911679764]) 20400: D: 0.583434104919/0.703361749649 G: 1.45571947098 (Real: [4.0388400733470915, 1.2267253073862441], Fake: [3.9019298100471498, 1.0292402192122965]) 20600: D: 0.176266431808/0.55411952734 G: 0.962469100952 (Real: [4.0694609802961352, 1.2276659305759301], Fake: [3.9728190612792971, 1.1212652107309595]) 20800: D: 1.17427504063/0.212535098195 G: 0.505771696568 (Real: [3.7983859290182589, 1.3565768879920506], Fake: [4.0766829651594163, 1.1742807548541911]) 21000: D: 0.247546881437/0.242251947522 G: 2.533826828 (Real: [4.048124186992645, 1.2074367711533176], Fake: [3.8443934541940687, 1.0964556009967605]) 21200: D: 0.000996549613774/1.77280521393 G: 0.741032421589 (Real: [3.8826335191726686, 1.3432952882949609], Fake: [4.0052364200353621, 1.0658632049377181]) 21400: D: 0.0162861924618/0.202122434974 G: 0.640827775002 (Real: [3.949158318042755, 1.2312223613675215], Fake: [3.9677765011787414, 1.1984950273079937]) 21600: D: 0.494586825371/0.368914216757 G: 1.73299539089 (Real: [4.2141097390651705, 1.3170628249721785], Fake: [3.9259325069189073, 1.2402090610341174]) 21800: D: 1.72856020927/0.280478566885 G: 0.301942139864 (Real: [3.9425574642419816, 1.3421295277895979], Fake: [4.1370714265108113, 1.3135434962232824]) 22000: D: 0.316263616085/0.425417006016 G: 4.6092467308 (Real: [3.9253722500801085, 1.1573266813219236], Fake: [3.7590440094470976, 1.2176312271677099]) 22200: D: 1.70313096046/0.166758075356 G: 1.76803898811 (Real: [4.1788750314712528, 1.3796412025948377], Fake: [4.4896411395072935, 0.88890948354147137]) 22400: D: 0.00245383195579/0.618139982224 G: 0.561835348606 (Real: [4.0531666296720505, 1.3030890495946361], Fake: [3.9800510057806968, 1.2769573713555427]) 22600: D: 0.0456999950111/0.270536243916 G: 0.719259619713 (Real: [3.8036734467744826, 1.2489490089903446], Fake: [4.2525720745325089, 1.3061806069103183]) 22800: D: 0.0318684391677/0.34651991725 G: 1.3301807642 (Real: [4.0768313544988635, 1.2930152979365797], Fake: [4.4993063497543337, 1.2277717696258752]) 23000: D: 1.38112533092/0.656377196312 G: 0.700986683369 (Real: [4.0261077487468722, 1.1634786009859657], Fake: [4.1274698692560197, 1.1909195549188023]) 23200: D: 0.7532761693/0.30048418045 G: 1.24321329594 (Real: [4.0255234652757643, 1.2277433432951119], Fake: [4.0463824319839476, 1.2493841122917879]) 23400: D: 1.54497790337/0.524266302586 G: 1.88104653358 (Real: [4.1244187545776363, 1.2126284333800423], Fake: [4.0199511092901226, 1.4125067136876193]) 23600: D: 0.838026106358/1.1139113903 G: 2.2735543251 (Real: [4.0352903008460999, 1.1687086536829701], Fake: [4.5685070466995237, 1.4508884769834012]) 23800: D: 0.869914472103/0.160864800215 G: 1.42444908619 (Real: [4.1635012495517731, 1.1441051019240691], Fake: [4.1520407730340958, 1.2022442680490875]) 24000: D: 0.0401677601039/0.240127012134 G: 1.21359109879 (Real: [4.0558859372138976, 1.1263029268841764], Fake: [3.8535136532783509, 0.99055012605544335]) 24200: D: 0.444084912539/0.761975646019 G: 1.18176090717 (Real: [4.1462872040271757, 1.1670976588949802], Fake: [4.0291124176979061, 1.4000525541431663]) 24400: D: 0.259448975325/0.206390738487 G: 0.850725114346 (Real: [4.2600694203376772, 1.3260391555100224], Fake: [4.7161277580261229, 1.3763624799621637]) 24600: D: 0.821855664253/0.381440609694 G: 0.898442983627 (Real: [3.9929001557826997, 1.316718033939094], Fake: [3.659836998283863, 1.033547623133473]) 24800: D: 0.869792580605/0.143853545189 G: 1.68244981766 (Real: [3.9503055346012115, 1.1980136516743376], Fake: [4.3753550618886949, 1.4268488751378543]) 25000: D: 0.533834278584/0.944993913174 G: 1.35653877258 (Real: [3.8403973925113677, 1.1415226099240794], Fake: [4.3022644245624546, 1.277824404897737]) 25200: D: 0.57686984539/1.21011674404 G: 0.49785476923 (Real: [4.1094828593730925, 1.0606124114518727], Fake: [3.8350191235542299, 1.1822398134788241]) 25400: D: 1.30570268631/0.127069279552 G: 2.14658904076 (Real: [3.8440176880359651, 1.2759016439053388], Fake: [4.2303895175457003, 1.2478330871411345]) 25600: D: 0.163877904415/0.356351107359 G: 1.50513041019 (Real: [3.9149920016527178, 1.3322359586431274], Fake: [4.5107577931880947, 1.37733363996175]) 25800: D: 0.0257995054126/0.501479804516 G: 0.846267580986 (Real: [4.0328698861598973, 1.0891363228332751], Fake: [4.2062628841400143, 1.2707193105443095]) 26000: D: 0.4208984375/0.45090213418 G: 1.24405300617 (Real: [4.0495267909765245, 1.3629959211491509], Fake: [3.881335927248001, 1.1534035700479874]) 26200: D: 1.0977101326/0.260044932365 G: 0.274282753468 (Real: [4.0526520502567287, 1.1354404896569923], Fake: [3.7989616423845289, 1.3036229409468019]) 26400: D: 0.836492598057/0.194570705295 G: 1.25769793987 (Real: [4.2580243301391603, 1.1229754918621602], Fake: [4.9420129108428954, 1.4595622988211396]) 26600: D: 0.0381172671914/0.229116663337 G: 3.23367476463 (Real: [3.9871047949790954, 1.2891811878363044], Fake: [5.5130027627944944, 1.3531596753079107]) 26800: D: 0.33750808239/0.0588937625289 G: 2.76632380486 (Real: [4.0901136839389798, 1.2240984948711151], Fake: [5.9970619964599612, 1.3296608494175821]) 27000: D: 0.403919011354/0.025144957006 G: 5.00026988983 (Real: [3.9684947764873506, 1.1928812330565042], Fake: [5.5821900677680967, 1.5869340992569609]) 27200: D: 1.26118826866/1.14945113659 G: 0.233536079526 (Real: [4.0953157800436024, 1.2000917970554563], Fake: [3.457775202393532, 1.2362199991432059]) 27400: D: 0.842516124249/0.577941656113 G: 0.518706798553 (Real: [3.8673747038841246, 1.1826108239366226], Fake: [3.6999527400732042, 1.2050256827670227]) 27600: D: 0.459548681974/0.516558885574 G: 1.69328427315 (Real: [4.0379843235015871, 1.267741160236167], Fake: [4.3069088852405546, 1.2883256614455194]) 27800: D: 0.757292568684/0.295852422714 G: 0.82683211565 (Real: [3.6750951480865477, 1.1881818498282759], Fake: [4.3079475378990173, 1.3863961893145142]) 28000: D: 1.0311729908/0.836829304695 G: 0.54562240839 (Real: [3.8109287106990815, 1.2699445078581264], Fake: [4.0800623488426204, 1.2420579399013889]) 28200: D: 0.662180066109/0.698618113995 G: 0.430238395929 (Real: [3.8820258617401122, 1.3192879801078357], Fake: [3.8678512275218964, 1.2100339116659864]) 28400: D: 0.857332766056/0.637849986553 G: 0.443328052759 (Real: [4.0044168281555175, 1.2977773729964786], Fake: [3.77621297955513, 1.10884790779666]) 28600: D: 0.518617451191/0.676390469074 G: 0.824631929398 (Real: [3.9321113193035124, 1.189980080467403], Fake: [4.1412628889083862, 1.4110153520360829]) 28800: D: 0.924657285213/0.57682287693 G: 0.867313206196 (Real: [3.8806186806410552, 1.2663798129949515], Fake: [3.7928846073150635, 0.96599856269415929]) 29000: D: 0.681347727776/0.833830595016 G: 0.880895376205 (Real: [4.0122552135586735, 1.3382642859979685], Fake: [3.8699622356891634, 1.5246898233773196]) 29200: D: 0.690975308418/0.571468651295 G: 0.539677977562 (Real: [3.9422134029865266, 1.2798402813873653], Fake: [3.4796924066543578, 1.0078584415562459]) 29400: D: 0.600927650928/0.692537486553 G: 0.785535871983 (Real: [4.0494313037395475, 1.2729051468200046], Fake: [4.0457676327228542, 1.2121629628604733]) 29600: D: 0.662378668785/0.552553355694 G: 0.665563106537 (Real: [3.8692034566402436, 1.1988600586203602], Fake: [4.3626180648803707, 1.3098951956607312]) 29800: D: 0.844242811203/0.719559967518 G: 0.89226102829 (Real: [3.8751950478553772, 1.1053984789259368], Fake: [3.9671442759037019, 1.1584875699071935])]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LSTM by Example using Tensorflow (Text Generate)]]></title>
      <url>%2F2017%2F04%2F26%2FLSTM-by-Example-using-Tensorflow-Text-Generate%2F</url>
      <content type="text"><![CDATA[In Deep Learning, Recurrent Neural Networks (RNN) are a family of neural networks that excels in learning from sequential data. A class of RNN that has found practical applications is Long Short-Term Memory (LSTM) because it is robust against the problems of long-term dependency.What seems to be lacking is a good documentation and example on how to build an easy to understand Tensorflow application based on LSTM. This is the motivation behind this article.Suppose we want to train a LSTM to predict the next word using a sample short story, Aesopâ€™s Fables:long ago , the mice had a general council to consider what measures they could take to outwit their common enemy , the cat . some said this , and some said that but at last a young mouse got up and said he had a proposal to make , which he thought would meet the case . you will all agree , said he , that our chief danger consists in the sly and treacherous manner in which the enemy approaches us . now , if we could receive some signal of her approach , we could easily escape from her . i venture , therefore , to propose that a small bell be procured , and attached by a ribbon round the neck of the cat . by this means we should always know when she was about , and could easily retire while she was in the neighbourhood . this proposal met with general applause , until an old mouse got up and said that is all very well , but who is to bell the cat ? the mice looked at one another and nobody spoke . then the old mouse said it is easy to propose impossible remedies .If we feed a LSTM with correct sequences from the text of 3 symbols as inputs and 1 labeled symbol, eventually the neural network will learn to predict the next symbol correctly.Technically, LSTM inputs can only understand real numbers. A way to convert symbol to number is to assign a unique integer to each symbol based on frequency of occurrence. For example, there are 112 unique symbols in the text above. The function in Listing 2 builds a dictionary with the following entries [ â€œ,â€ : 0 ][ â€œtheâ€ : 1 ], â€¦, [ â€œcouncilâ€ : 37 ],â€¦,[ â€œspokeâ€ : 111 ]. The reverse dictionary is also generated since it will be used in decoding the output of LSTM.1234567def build_dataset(words): count = collections.Counter(words).most_common() dictionary = dict() for word, _ in count: dictionary[word] = len(dictionary) reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) return dictionary, reverse_dictionarySimilarly, the prediction is a unique integer identifying the index in the reverse dictionary of the predicted symbol. For example, if the prediction is 37, the predicted symbol is actually â€œcouncilâ€.The generation of output may sound simple but actually LSTM produces a 112-element vector of probabilities of prediction for the next symbol normalized by the softmax() function. The index of the element with the highest probability is the predicted index of the symbol in the reverse dictionary (ie a one-hot vector).There is the source code:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180'''A Recurrent Neural Network (LSTM) implementation example using TensorFlow..Next word prediction after n_input words learned from text file.A story is automatically generated if the predicted word is fed back as input.Author: Rowel AtienzaProject: https://github.com/roatienza/Deep-Learning-Experiments'''from __future__ import print_functionimport numpy as npimport tensorflow as tffrom tensorflow.contrib import rnnimport randomimport collectionsimport timestart_time = time.time()def elapsed(sec): if sec&lt;60: return str(sec) + " sec" elif sec&lt;(60*60): return str(sec/60) + " min" else: return str(sec/(60*60)) + " hr"# Target log pathlogs_path = '/tmp/tensorflow/rnn_words'writer = tf.summary.FileWriter(logs_path)# Text file containing words for trainingtraining_file = 'belling_the_cat.txt'def read_data(fname): with open(fname) as f: content = f.readlines() content = [x.strip() for x in content] content = [content[i].split() for i in range(len(content))] content = np.array(content) content = np.reshape(content, [-1, ]) return contenttraining_data = read_data(training_file)print("Loaded training data...")def build_dataset(words): count = collections.Counter(words).most_common() dictionary = dict() for word, _ in count: dictionary[word] = len(dictionary) reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) return dictionary, reverse_dictionarydictionary, reverse_dictionary = build_dataset(training_data)vocab_size = len(dictionary)# Parameterslearning_rate = 0.001training_iters = 50000display_step = 1000n_input = 3# number of units in RNN celln_hidden = 512# tf Graph inputx = tf.placeholder("float", [None, n_input, 1])y = tf.placeholder("float", [None, vocab_size])# RNN output node weights and biasesweights = &#123; 'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))&#125;biases = &#123; 'out': tf.Variable(tf.random_normal([vocab_size]))&#125;def RNN(x, weights, biases): # reshape to [1, n_input] x = tf.reshape(x, [-1, n_input]) # Generate a n_input-element sequence of inputs # (eg. [had] [a] [general] -&gt; [20] [6] [33]) x = tf.split(x,n_input,1) # 2-layer LSTM, each layer has n_hidden units. # Average Accuracy= 95.20% at 50k iter rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)]) # 1-layer LSTM with n_hidden units but with lower accuracy. # Average Accuracy= 90.60% 50k iter # Uncomment line below to test but comment out the 2-layer rnn.MultiRNNCell above # rnn_cell = rnn.BasicLSTMCell(n_hidden) # generate prediction outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32) # there are n_input outputs but # we only want the last output return tf.matmul(outputs[-1], weights['out']) + biases['out']pred = RNN(x, weights, biases)# Loss and optimizercost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)# Model evaluationcorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))# Initializing the variablesinit = tf.global_variables_initializer()# Launch the graphwith tf.Session() as session: session.run(init) step = 0 offset = random.randint(0,n_input+1) end_offset = n_input + 1 acc_total = 0 loss_total = 0 writer.add_graph(session.graph) while step &lt; training_iters: # Generate a minibatch. Add some randomness on selection process. if offset &gt; (len(training_data)-end_offset): offset = random.randint(0, n_input+1) symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ] symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1]) symbols_out_onehot = np.zeros([vocab_size], dtype=float) symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0 symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1]) _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \ feed_dict=&#123;x: symbols_in_keys, y: symbols_out_onehot&#125;) loss_total += loss acc_total += acc if (step+1) % display_step == 0: print("Iter= " + str(step+1) + ", Average Loss= " + \ "&#123;:.6f&#125;".format(loss_total/display_step) + ", Average Accuracy= " + \ "&#123;:.2f&#125;%".format(100*acc_total/display_step)) acc_total = 0 loss_total = 0 symbols_in = [training_data[i] for i in range(offset, offset + n_input)] symbols_out = training_data[offset + n_input] symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())] print("%s - [%s] vs [%s]" % (symbols_in,symbols_out,symbols_out_pred)) step += 1 offset += (n_input+1) print("Optimization Finished!") print("Elapsed time: ", elapsed(time.time() - start_time)) print("Run on command line.") print("\ttensorboard --logdir=%s" % (logs_path)) print("Point your web browser to: http://localhost:6006/") while True: prompt = "%s words: " % n_input sentence = input(prompt) sentence = sentence.strip() words = sentence.split(' ') if len(words) != n_input: continue try: symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))] for i in range(32): keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1]) onehot_pred = session.run(pred, feed_dict=&#123;x: keys&#125;) onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval()) sentence = "%s %s" % (sentence,reverse_dictionary[onehot_pred_index]) symbols_in_keys = symbols_in_keys[1:] symbols_in_keys.append(onehot_pred_index) print(sentence) except: print("Word not in dictionary")source blog: https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Xiaomi mini wifi cannot build the connection]]></title>
      <url>%2F2017%2F04%2F26%2FXiaomi-mini-wifi-cannot-build-the-connection%2F</url>
      <content type="text"><![CDATA[é’ˆå¯¹Win10ä¸èƒ½æ­£å¸¸ä½¿ç”¨çš„é—®é¢˜è¿›å…¥å®‰è£…ç›®å½•è¿›å…¥driversæ–‡ä»¶å¤¹è¿›å…¥Win81x64æ–‡ä»¶å¤¹æ‰¾åˆ°netr28ux.infæ–‡ä»¶ï¼Œå³é”®å®‰è£…ä¹‹]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Movie Recommendation with MLlib]]></title>
      <url>%2F2017%2F04%2F26%2FMovie-Recommendation-with-MLlib%2F</url>
      <content type="text"><![CDATA[Spark Summit 2014https://databricks-training.s3.amazonaws.com/index.htmlwe will use MLlib to make personalized movie recommendations tailored for you. We will work with 10 million ratings from 72,000 users on 10,000 movies, collected by MovieLens. This dataset is pre-loaded in your USB drive under data/movielens/large. For quick testing of your code, you may want to use a smaller dataset under data/movielens/medium, which contains 1 million ratings from 6000 users on 4000 movies.DataSetWe will use two files from this MovieLens dataset: â€œratings.datâ€ and â€œmovies.datâ€. All ratings are contained in the file â€œratings.datâ€ and are in the following format:1UserID::MovieID::Rating::TimestampMovie information is in the file â€œmovies.datâ€ and is in the following format:1MovieID::Title::GenresCollaborative filteringCollaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix, in our case, the user-movie rating matrix. MLlib currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. In particular, we implement the alternating least squares (ALS) algorithm to learn these latent factors.Create training exampleshttps://github.com/ewanlee/spark-trainingTo make recommendation for you, we are going to learn your taste by asking you to rate a few movies. We have selected a small set of movies that have received the most ratings from users in the MovieLens dataset. You can rate those movies by running bin/rateMovies:1python bin/rateMoviesWhen you run the script, you should see prompt similar to the following:12Please rate the following movie (1-5 (best), or 0 if not seen):Toy Story (1995):After youâ€™re done rating the movies, we save your ratings in personalRatings.txt in the MovieLens format, where a special user id 0 is assigned to you.rateMovies allows you to re-rate the movies if youâ€™d like to see how your ratings affect your recommendations.If you donâ€™t have python installed, please copy personalRatings.txt.template to personalRatings.txt and replace ?s with your ratings.SetupWe will be using a standalone project template for this exercise.In the training USB drive, this has been setup in1machine-learning/python/You should find the following items in the directory:MovieLensALS.py: Main Python program that you are going to edit, compile and runsolution: Directory containing the solution codeMovieLensALS.py should look as follows:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/env pythonimport sysimport itertoolsfrom math import sqrtfrom operator import addfrom os.path import join, isfile, dirnamefrom pyspark import SparkConf, SparkContextfrom pyspark.mllib.recommendation import ALSdef parseRating(line): """ Parses a rating record in MovieLens format userId::movieId::rating::timestamp . """ # ...def parseMovie(line): """ Parses a movie record in MovieLens format movieId::movieTitle . """ # ...def loadRatings(ratingsFile): """ Load ratings from file. """ # ...def computeRmse(model, data, n): """ Compute RMSE (Root Mean Squared Error). """ # ...if __name__ == "__main__": if (len(sys.argv) != 3): print "Usage: [usb root directory]/spark/bin/spark-submit --driver-memory 2g " + \ "MovieLensALS.py movieLensDataDir personalRatingsFile" sys.exit(1) # set up environment conf = SparkConf() \ .setAppName("MovieLensALS") \ .set("spark.executor.memory", "2g") sc = SparkContext(conf=conf) # load personal ratings myRatings = loadRatings(sys.argv[2]) myRatingsRDD = sc.parallelize(myRatings, 1) # load ratings and movie titles movieLensHomeDir = sys.argv[1] # ratings is an RDD of (last digit of timestamp, (userId, movieId, rating)) ratings = sc.textFile(join(movieLensHomeDir, "ratings.dat")).map(parseRating) # movies is an RDD of (movieId, movieTitle) movies = dict(sc.textFile(join(movieLensHomeDir, "movies.dat")).map(parseMovie).collect()) # your code here # clean up sc.stop()Letâ€™s first take a closer look at our template code in a text editor, then weâ€™ll start adding code to the template. Locate theMovieLensALS class and open it with a text editor.12usb/$ cd machine-learning/pythonvim MovieLensALS.py # Or your editor of choiceFor any Spark computation, we first create a SparkConf object and use it to create a SparkContext object. Since we will be using spark-submit to execute the programs in this tutorial (more on spark-submit in the next section), we only need to configure the executor memory allocation and give the program a name, e.g. â€œMovieLensALSâ€, to identify it in Sparkâ€™s web UI. In local mode, the web UI can be access at localhost:4040 during the execution of a program.This is what it looks like in our template code:1234conf = SparkConf() \ .setAppName("MovieLensALS") \ .set("spark.executor.memory", "2g") sc = SparkContext(conf=conf)Next, the code uses the SparkContext to read in ratings. Recall that the rating file is a text file with â€œ::â€ as the delimiter. The code parses each line to create a RDD for ratings that contains (Int, Rating) pairs. We only keep the last digit of the timestamp as a random key. The Rating class is a wrapper around the tuple (user: Int, product: Int, rating: Double).1234movieLensHomeDir = sys.argv[1]# ratings is an RDD of (last digit of timestamp, (userId, movieId, rating))ratings = sc.textFile(join(movieLensHomeDir, "ratings.dat")).map(parseRating)Next, the code read in movie ids and titles, collect them into a movie id to title map.12345def parseMovie(line): fields = line.split("::") return int(fields[0]), fields[1] movies = dict(sc.textFile(join(movieLensHomeDir, "movies.dat")).map(parseMovie).collect())Now, letâ€™s make our first edit to add code to get a summary of the ratings.12345numRatings = ratings.count()numUsers = ratings.values().map(lambda r: r[0]).distinct().count()numMovies = ratings.values().map(lambda r: r[1]).distinct().count()print "Got %d ratings from %d users on %d movies." % (numRatings, numUsers, numMovies)Running the programBefore we compute movie recommendations, here is a quick reminder on how you can run the program at any point during this exercise. As mentioned above, we will use spark-submit to execute your program in local mode for this tutorial.Starting with Spark 1.0, spark-submit is the recommended way for running Spark applications, both on clusters and locally in standalone mode.1234usb/$ cd machine-learning/python# change the folder name from "medium" to "large" to run on the large data set[usb root directory]/spark/bin/spark-submit MovieLensALS.py [usb root directory]/data/movielens/medium/ ../personalRatings.txtYou should see output similar to the following on your screen:1Got 1000209 ratings from 6040 users on 3706 movies.Splitting training dataWe will use MLlibâ€™s ALS to train a MatrixFactorizationModel, which takes a RDD[Rating] object as input in Scala and RDD[(user, product, rating)] in Python. ALS has training parameters such as rank for matrix factors and regularization constants. To determine a good combination of the training parameters, we split the data into three non-overlapping subsets, named training, test, and validation, based on the last digit of the timestamp, and cache them. We will train multiple models based on the training set, select the best model on the validation set based on RMSE (Root Mean Squared Error), and finally evaluate the best model on the test set. We also add your ratings to the training set to make recommendations for you. We hold the training, validation, and test sets in memory by calling cache because we need to visit them multiple times.12345678910111213141516171819numPartitions = 4training = ratings.filter(lambda x: x[0] &lt; 6) \ .values() \ .union(myRatingsRDD) \ .repartition(numPartitions) \ .cache()validation = ratings.filter(lambda x: x[0] &gt;= 6 and x[0] &lt; 8) \ .values() \ .repartition(numPartitions) \ .cache()test = ratings.filter(lambda x: x[0] &gt;= 8).values().cache()numTraining = training.count()numValidation = validation.count()numTest = test.count()print "Training: %d, validation: %d, test: %d" % (numTraining, numValidation, numTest)After the split, you should see1Training: 602251, validation: 198919, test: 199049.Training using ALSIn this section, we will use ALS.train to train a bunch of models, and select and evaluate the best. Among the training paramters of ALS, the most important ones are rank, lambda (regularization constant), and number of iterations. The trainmethod of ALS we are going to use is defined as the following:12345class ALS(object):def train(cls, ratings, rank, iterations=5, lambda_=0.01, blocks=-1): # ... return MatrixFactorizationModel(sc, mod)deally, we want to try a large number of combinations of them in order to find the best one. Due to time constraint, we will test only 8 combinations resulting from the cross product of 2 different ranks (8 and 12), 2 different lambdas (1.0 and 10.0), and two different numbers of iterations (10 and 20). We use the provided method computeRmse to compute the RMSE on the validation set for each model. The model with the smallest RMSE on the validation set becomes the one selected and its RMSE on the test set is used as the final metric.1234567891011121314151617181920212223242526ranks = [8, 12]lambdas = [1.0, 10.0]numIters = [10, 20]bestModel = NonebestValidationRmse = float("inf")bestRank = 0bestLambda = -1.0bestNumIter = -1for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters): model = ALS.train(training, rank, numIter, lmbda) validationRmse = computeRmse(model, validation, numValidation) print "RMSE (validation) = %f for the model trained with " % validationRmse + \ "rank = %d, lambda = %.1f, and numIter = %d." % (rank, lmbda, numIter) if (validationRmse &lt; bestValidationRmse): bestModel = model bestValidationRmse = validationRmse bestRank = rank bestLambda = lmbda bestNumIter = numItertestRmse = computeRmse(bestModel, test, numTest)# evaluate the best model on the test setprint "The best model was trained with rank = %d and lambda = %.1f, " % (bestRank, bestLambda) \ + "and numIter = %d, and its RMSE on the test set is %f." % (bestNumIter, testRmse)Spark might take a minute or two to train the models. You should see the following on the screen:1The best model was trained using rank 8 and lambda 10.0, and its RMSE on test is 0.8808492431998702.Recommending movies for youAs the last part of our tutorial, letâ€™s take a look at what movies our model recommends for you. This is done by generating (0, movieId) pairs for all movies you havenâ€™t rated and calling the modelâ€™s predict method to get predictions. 0 is the special user id assigned to you.12345class MatrixFactorizationModel(object): def predictAll(self, usersProducts): # ... return RDD(self._java_model.predict(usersProductsJRDD._jrdd), self._context, RatingDeserializer())After we get all predictions, let us list the top 50 recommendations and see whether they look good to you.12345678myRatedMovieIds = set([x[1] for x in myRatings])candidates = sc.parallelize([m for m in movies if m not in myRatedMovieIds])predictions = bestModel.predictAll(candidates.map(lambda x: (0, x))).collect()recommendations = sorted(predictions, key=lambda x: x[2], reverse=True)[:50]print "Movies recommended for you:"for i in xrange(len(recommendations)): print ("%2d: %s" % (i + 1, movies[recommendations[i][1]])).encode('ascii', 'ignore')The output should be similar to123456789101112Movies recommended for you: 1: Silence of the Lambs, The (1991) 2: Saving Private Ryan (1998) 3: Godfather, The (1972) 4: Star Wars: Episode IV - A New Hope (1977) 5: Braveheart (1995) 6: Schindler's List (1993) 7: Shawshank Redemption, The (1994) 8: Star Wars: Episode V - The Empire Strikes Back (1980) 9: Pulp Fiction (1994)10: Alien (1979)...Comparing to a naive baselineDoes ALS output a non-trivial model? We can compare the evaluation result with a naive baseline model that only outputs the average rating (or you may try one that outputs the average rating per movie). Computing the baselineâ€™s RMSE is straightforward:1234meanRating = training.union(validation).map(lambda x: x[2]).mean()baselineRmse = sqrt(test.map(lambda x: (meanRating - x[2]) ** 2).reduce(add) / numTest)improvement = (baselineRmse - testRmse) / baselineRmse * 100print "The best model improves the baseline by %.2f" % (improvement) + "%."The output should be similar to1The best model improves the baseline by 20.96%.It seems obvious that the trained model would outperform the naive baseline. However, a bad combination of training parameters would lead to a model worse than this naive baseline. Choosing the right set of parameters is quite important for this task.Solution code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156#!/usr/bin/env pythonimport sysimport itertoolsfrom math import sqrtfrom operator import addfrom os.path import join, isfile, dirnamefrom pyspark import SparkConf, SparkContextfrom pyspark.mllib.recommendation import ALSdef parseRating(line): """ Parses a rating record in MovieLens format userId::movieId::rating::timestamp . """ fields = line.strip().split("::") return long(fields[3]) % 10, (int(fields[0]), int(fields[1]), float(fields[2]))def parseMovie(line): """ Parses a movie record in MovieLens format movieId::movieTitle . """ fields = line.strip().split("::") return int(fields[0]), fields[1]def loadRatings(ratingsFile): """ Load ratings from file. """ if not isfile(ratingsFile): print "File %s does not exist." % ratingsFile sys.exit(1) f = open(ratingsFile, 'r') ratings = filter(lambda r: r[2] &gt; 0, [parseRating(line)[1] for line in f]) f.close() if not ratings: print "No ratings provided." sys.exit(1) else: return ratingsdef computeRmse(model, data, n): """ Compute RMSE (Root Mean Squared Error). """ predictions = model.predictAll(data.map(lambda x: (x[0], x[1]))) predictionsAndRatings = predictions.map(lambda x: ((x[0], x[1]), x[2])) \ .join(data.map(lambda x: ((x[0], x[1]), x[2]))) \ .values() return sqrt(predictionsAndRatings.map(lambda x: (x[0] - x[1]) ** 2).reduce(add) / float(n))if __name__ == "__main__": if (len(sys.argv) != 3): print "Usage: /path/to/spark/bin/spark-submit --driver-memory 2g " + \ "MovieLensALS.py movieLensDataDir personalRatingsFile" sys.exit(1) # set up environment conf = SparkConf() \ .setAppName("MovieLensALS") \ .set("spark.executor.memory", "2g") sc = SparkContext(conf=conf) # load personal ratings myRatings = loadRatings(sys.argv[2]) myRatingsRDD = sc.parallelize(myRatings, 1) # load ratings and movie titles movieLensHomeDir = sys.argv[1] # ratings is an RDD of (last digit of timestamp, (userId, movieId, rating)) ratings = sc.textFile(join(movieLensHomeDir, "ratings.dat")).map(parseRating) # movies is an RDD of (movieId, movieTitle) movies = dict(sc.textFile(join(movieLensHomeDir, "movies.dat")).map(parseMovie).collect()) numRatings = ratings.count() numUsers = ratings.values().map(lambda r: r[0]).distinct().count() numMovies = ratings.values().map(lambda r: r[1]).distinct().count() print "Got %d ratings from %d users on %d movies." % (numRatings, numUsers, numMovies) # split ratings into train (60%), validation (20%), and test (20%) based on the # last digit of the timestamp, add myRatings to train, and cache them # training, validation, test are all RDDs of (userId, movieId, rating) numPartitions = 4 training = ratings.filter(lambda x: x[0] &lt; 6) \ .values() \ .union(myRatingsRDD) \ .repartition(numPartitions) \ .cache() validation = ratings.filter(lambda x: x[0] &gt;= 6 and x[0] &lt; 8) \ .values() \ .repartition(numPartitions) \ .cache() test = ratings.filter(lambda x: x[0] &gt;= 8).values().cache() numTraining = training.count() numValidation = validation.count() numTest = test.count() print "Training: %d, validation: %d, test: %d" % (numTraining, numValidation, numTest) # train models and evaluate them on the validation set ranks = [8, 12] lambdas = [0.1, 10.0] numIters = [10, 20] bestModel = None bestValidationRmse = float("inf") bestRank = 0 bestLambda = -1.0 bestNumIter = -1 for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters): model = ALS.train(training, rank, numIter, lmbda) validationRmse = computeRmse(model, validation, numValidation) print "RMSE (validation) = %f for the model trained with " % validationRmse + \ "rank = %d, lambda = %.1f, and numIter = %d." % (rank, lmbda, numIter) if (validationRmse &lt; bestValidationRmse): bestModel = model bestValidationRmse = validationRmse bestRank = rank bestLambda = lmbda bestNumIter = numIter testRmse = computeRmse(bestModel, test, numTest) # evaluate the best model on the test set print "The best model was trained with rank = %d and lambda = %.1f, " % (bestRank, bestLambda) \ + "and numIter = %d, and its RMSE on the test set is %f." % (bestNumIter, testRmse) # compare the best model with a naive baseline that always returns the mean rating meanRating = training.union(validation).map(lambda x: x[2]).mean() baselineRmse = sqrt(test.map(lambda x: (meanRating - x[2]) ** 2).reduce(add) / numTest) improvement = (baselineRmse - testRmse) / baselineRmse * 100 print "The best model improves the baseline by %.2f" % (improvement) + "%." # make personalized recommendations myRatedMovieIds = set([x[1] for x in myRatings]) candidates = sc.parallelize([m for m in movies if m not in myRatedMovieIds]) predictions = bestModel.predictAll(candidates.map(lambda x: (0, x))).collect() recommendations = sorted(predictions, key=lambda x: x[2], reverse=True)[:50] print "Movies recommended for you:" for i in xrange(len(recommendations)): print ("%2d: %s" % (i + 1, movies[recommendations[i][1]])).encode('ascii', 'ignore') # clean up sc.stop()]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning with MLlib of Spark]]></title>
      <url>%2F2017%2F04%2F25%2FMachine-Learning-with-MLlib-of-Spark%2F</url>
      <content type="text"><![CDATA[Example: Spam ClassificationThis program uses two MLlib algorithms: HashingTF, which builds term frequency feature vectors from text data, and LogisticRegressionWithSGD, which implements the logistic regression procedure using stochastic gradient descent (SGD). We assume that we start with two files, spam.txt an normal.txt, each of which contains examples of spam and non-spam emails, one per line. We then turn the text in each file into a feature vector with TF, and train a logistic regression model to separate the two types of messages.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657## Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the "License"); you may not use this file except in compliance with# the License. You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.#from pyspark import SparkContextfrom pyspark.mllib.regression import LabeledPointfrom pyspark.mllib.classification import LogisticRegressionWithSGDfrom pyspark.mllib.feature import HashingTFif __name__ == "__main__": sc = SparkContext(appName="PythonBookExample") # Load 2 types of emails from text files: spam and ham (non-spam). # Each line has text from one email. spam = sc.textFile("file:///home/hduser/learning-spark/files/spam.txt") ham = sc.textFile("file:///home/hduser/learning-spark/files/ham.txt") # Create a HashingTF instance to map email text to vectors of 100 features. tf = HashingTF(numFeatures = 100) # Each email is split into words, and each word is mapped to one feature. spamFeatures = spam.map(lambda email: tf.transform(email.split(" "))) hamFeatures = ham.map(lambda email: tf.transform(email.split(" "))) # Create LabeledPoint datasets for positive (spam) and negative (ham) examples. positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features)) negativeExamples = hamFeatures.map(lambda features: LabeledPoint(0, features)) training_data = positiveExamples.union(negativeExamples) training_data.cache() # Cache data since Logistic Regression is an iterative algorithm. # Run Logistic Regression using the SGD optimizer. # regParam is model regularization, which can make models more robust. model = LogisticRegressionWithSGD.train(training_data) # Test on a positive example (spam) and a negative one (ham). # First apply the same HashingTF feature transformation used on the training data. posTestExample = tf.transform("O M G GET cheap stuff by sending money to ...".split(" ")) negTestExample = tf.transform("Hi Dad, I started studying Spark the other ...".split(" ")) # Now use the learned model to predict spam/ham for new emails. print "Prediction for positive test example: %g" % model.predict(posTestExample) print "Prediction for negative test example: %g" % model.predict(negTestExample) sc.stop()AlgorithmsHere only has some usual APIs.Feature ExtractionScalingMost machine learning algorithms consider the magnitude of each element in the feature vector, and thus work best when the features are scaled so they weigh equally (e.g., all features have a mean of 0 and standard deviation of 1). Once you have built feature vectors, you can use the StandardScaler class in MLlib to do this scaling, both for the mean and the standard deviation. You create a StandardScaler, call fit() on a dataset to obtain a StandardScalerModel (i.e., compute the mean and variance of each column), and then call transform() on the model to scale a dataset.123456789from pyspark.mllib.feature import StandardScalervectors = [Vectors.dense([-2.0, 5.0, 1.0]), Vectors.dense([2.0, 0.0, 1.0])]dataset = sc.parallelize(vectors)scaler = StandardScaler(withMean=True, withStd=True)model = scaler.fit(dataset)result = model.transform(dataset)# Result: &#123;[-0.7071, 0.7071, 0.0], [0.7071, -0.7071, 0.0]&#125;NormalizationSimply use Normalizer().transform(rdd). By default Normalizer uses the L 2 norm (i.e, Euclidean length), but you can also pass a power pto Normalizer to use the L p norm.123456789101112131415from pyspark.mllib.feature import Normalizerfrom pyspark.mllib.util import MLUtilsdata = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")labels = data.map(lambda x: x.label)features = data.map(lambda x: x.features)normalizer1 = Normalizer()normalizer2 = Normalizer(p=float("inf"))# Each sample in data1 will be normalized using $L^2$ norm.data1 = labels.zip(normalizer1.transform(features))# Each sample in data2 will be normalized using $L^\infty$ norm.data2 = labels.zip(normalizer2.transform(features))Word2VecOnce you have trained the model (withWord2Vec.fit(rdd)), you will receive a Word2VecModel that can be used to transform() each word into a vector. Note that the size of the models in Word2Vec will be equal to the number of words in your vocabulary times the size of a vector (by default, 100). You may wish to filter out words that are not in a standard dictionary to limit the size. In general, a good size for the vocabulary is 100,000 words.1234567891011from pyspark.mllib.feature import Word2Vecinp = sc.textFile("data/mllib/sample_lda_data.txt").map(lambda row: row.split(" "))word2vec = Word2Vec()model = word2vec.fit(inp)synonyms = model.findSynonyms('1', 5)for word, cosine_distance in synonyms: print("&#123;&#125;: &#123;&#125;".format(word, cosine_distance))StatisticsStatistics.colStats(rdd)Computes a statistical summary of an RDD of vectors, which stores the min, max, mean, and variance for each column in the set of vectors. This can be used to obtain a wide variety of statistics in one pass.Statistics.corr(rdd, method)Computes the correlation matrix between columns in an RDD of vectors, using either the Pearson or Spearman correlation (method must be one of pearson and spearman).1234567891011121314151617from pyspark.mllib.stat import StatisticsseriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0]) # a series# seriesY must have the same number of partitions and cardinality as seriesXseriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])# Compute the correlation using Pearson's method. Enter "spearman" for Spearman's method.# If a method is not specified, Pearson's method will be used by default.print("Correlation is: " + str(Statistics.corr(seriesX, seriesY, method="pearson")))data = sc.parallelize( [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([5.0, 33.0, 366.0])]) # an RDD of Vectors# calculate the correlation matrix using Pearson's method. Use "spearman" for Spearman's method.# If a method is not specified, Pearson's method will be used by default.print(Statistics.corr(data, method="pearson"))Statistics.corr(rdd1, rdd2, method)Computes the correlation between two RDDs of floating-point values, using either the Pearson or Spearman correlation (method must be one of pearson and spearman).Statistics.chiSqTest(rdd)Computes Pearsonâ€™s independence test for every feature with the label on an RDD of LabeledPoint objects. Returns an array of ChiSqTestResult objects that capture the p-value, test statistic, and degrees of freedom for each feature. Label and feature values must be categorical (i.e., discrete values).123456789101112131415161718192021222324252627282930313233343536from pyspark.mllib.linalg import Matrices, Vectorsfrom pyspark.mllib.regression import LabeledPointfrom pyspark.mllib.stat import Statisticsvec = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25) # a vector composed of the frequencies of events# compute the goodness of fit. If a second vector to test against# is not supplied as a parameter, the test runs against a uniform distribution.goodnessOfFitTestResult = Statistics.chiSqTest(vec)# summary of the test including the p-value, degrees of freedom,# test statistic, the method used, and the null hypothesis.print("%s\n" % goodnessOfFitTestResult)mat = Matrices.dense(3, 2, [1.0, 3.0, 5.0, 2.0, 4.0, 6.0]) # a contingency matrix# conduct Pearson's independence test on the input contingency matrixindependenceTestResult = Statistics.chiSqTest(mat)# summary of the test including the p-value, degrees of freedom,# test statistic, the method used, and the null hypothesis.print("%s\n" % independenceTestResult)obs = sc.parallelize( [LabeledPoint(1.0, [1.0, 0.0, 3.0]), LabeledPoint(1.0, [1.0, 2.0, 0.0]), LabeledPoint(1.0, [-1.0, 0.0, -0.5])]) # LabeledPoint(feature, label)# The contingency table is constructed from an RDD of LabeledPoint and used to conduct# the independence test. Returns an array containing the ChiSquaredTestResult for every feature# against the label.featureTestResults = Statistics.chiSqTest(obs)for i, result in enumerate(featureTestResults): print("Column %d:\n%s" % (i + 1, result))Classification and RegressionMLlib includes a variety of methods for classification and regression, including simple linear methods and decision trees and forests.Linear regression123456from pyspark.mllib.regression import LabeledPointfrom pyspark.mllib.regression import LinearRegressionWithSGDpoints = # (create RDD of LabeledPoint)model = LinearRegressionWithSGD.train(points, iterations=200, intercept=True)print "weights: %s, intercept: %s" % (model.weights, model.intercept)Logistic regressionThe logistic regression algorithm has a very similar API to linear regression, covered in the previous section. One difference is that there are two algorithms available for solving it: SGD and LBFGS. LBFGS is generally the best choice, but is not available in some earlier versions of MLlib (before Spark 1.2). These algorithms are available in the mllib.classification.LogisticRegressionWithLBFGS and WithSGD classes, which have interfaces similar to LinearRegressionWithSGD. They take all the same parameters as linear regression.1234567891011121314151617181920212223from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModelfrom pyspark.mllib.regression import LabeledPoint# Load and parse the datadef parsePoint(line): values = [float(x) for x in line.split(' ')] return LabeledPoint(values[0], values[1:])data = sc.textFile("data/mllib/sample_svm_data.txt")parsedData = data.map(parsePoint)# Build the modelmodel = LogisticRegressionWithLBFGS.train(parsedData)# Evaluating the model on training datalabelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())print("Training Error = " + str(trainErr))# Save and load modelmodel.save(sc, "target/tmp/pythonLogisticRegressionWithLBFGSModel")sameModel = LogisticRegressionModel.load(sc, "target/tmp/pythonLogisticRegressionWithLBFGSModel")Support Vector MachinesThey are available through the SVMWithSGD class, with similar parameters to linear and logisitic regression. The returned SVMModel uses a threshold for prediction like LogisticRegressionModel.12345678910111213141516171819202122from pyspark.mllib.classification import SVMWithSGD, SVMModelfrom pyspark.mllib.regression import LabeledPoint# Load and parse the datadef parsePoint(line): values = [float(x) for x in line.split(' ')] return LabeledPoint(values[0], values[1:])data = sc.textFile("data/mllib/sample_svm_data.txt")parsedData = data.map(parsePoint)# Build the modelmodel = SVMWithSGD.train(parsedData, iterations=100)# Evaluating the model on training datalabelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())print("Training Error = " + str(trainErr))# Save and load modelmodel.save(sc, "target/tmp/pythonSVMWithSGDModel")sameModel = SVMModel.load(sc, "target/tmp/pythonSVMWithSGDModel")Naive BayesIn MLlib, you can use Naive Bayes through themllib.classification.NaiveBayes class. It supports one parameter, lambda (or lambda_ in Python), used for smoothing. You can call it on an RDD of LabeledPoints, where the labels are between 0 and Câ€“1 for C classes.123456789101112131415161718192021222324252627from pyspark.mllib.classification import NaiveBayes, NaiveBayesModelfrom pyspark.mllib.util import MLUtils# Load and parse the data file.data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")# Split data approximately into training (60%) and test (40%)training, test = data.randomSplit([0.6, 0.4])# Train a naive Bayes model.model = NaiveBayes.train(training, 1.0)# Make prediction and test accuracy.predictionAndLabel = test.map(lambda p: (model.predict(p.features), p.label))accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()print('model accuracy &#123;&#125;'.format(accuracy))# Save and load modeloutput_dir = 'target/tmp/myNaiveBayesModel'shutil.rmtree(output_dir, ignore_errors=True)model.save(sc, output_dir)sameModel = NaiveBayesModel.load(sc, output_dir)predictionAndLabel = test.map(lambda p: (sameModel.predict(p.features), p.label))accuracy = 1.0 * predictionAndLabel.filter(lambda (x, v): x == v).count() / test.count()print('sameModel accuracy &#123;&#125;'.format(accuracy))Decision trees and random forestsIn MLlib, you can train trees using the mllib.tree.DecisionTree class, through the static methods trainClassifier() and trainRegressor(). Unlike in some of the other algorithms, the Java and Scala APIs also use static methods instead of a DecisionTree object with setters.123456789101112131415161718192021222324from pyspark.mllib.tree import DecisionTree, DecisionTreeModelfrom pyspark.mllib.util import MLUtils# Load and parse the data file into an RDD of LabeledPoint.data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')# Split the data into training and test sets (30% held out for testing)(trainingData, testData) = data.randomSplit([0.7, 0.3])# Train a DecisionTree model.# Empty categoricalFeaturesInfo indicates all features are continuous.model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo=&#123;&#125;, impurity='gini', maxDepth=5, maxBins=32)# Evaluate model on test instances and compute test errorpredictions = model.predict(testData.map(lambda x: x.features))labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())print('Test Error = ' + str(testErr))print('Learned classification tree model:')print(model.toDebugString())# Save and load modelmodel.save(sc, "target/tmp/myDecisionTreeClassificationModel")sameModel = DecisionTreeModel.load(sc, "target/tmp/myDecisionTreeClassificationModel")ClusteringK-means1234567891011121314151617181920212223from numpy import arrayfrom math import sqrtfrom pyspark.mllib.clustering import KMeans, KMeansModel# Load and parse the datadata = sc.textFile("data/mllib/kmeans_data.txt")parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))# Build the model (cluster the data)clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode="random")# Evaluate clustering by computing Within Set Sum of Squared Errorsdef error(point): center = clusters.centers[clusters.predict(point)] return sqrt(sum([x**2 for x in (point - center)]))WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)print("Within Set Sum of Squared Error = " + str(WSSSE))# Save and load modelclusters.save(sc, "target/org/apache/spark/PythonKMeansExample/KMeansModel")sameModel = KMeansModel.load(sc, "target/org/apache/spark/PythonKMeansExample/KMeansModel")Collaborative Filtering and Recommendation12345678910111213141516171819202122from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating# Load and parse the datadata = sc.textFile("data/mllib/als/test.data")ratings = data.map(lambda l: l.split(','))\ .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))# Build the recommendation model using Alternating Least Squaresrank = 10numIterations = 10model = ALS.train(ratings, rank, numIterations)# Evaluate the model on training datatestdata = ratings.map(lambda p: (p[0], p[1]))predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()print("Mean Squared Error = " + str(MSE))# Save and load modelmodel.save(sc, "target/tmp/myCollaborativeFilter")sameModel = MatrixFactorizationModel.load(sc, "target/tmp/myCollaborativeFilter")The training exercises from the Spark Summit 2014 include a hands-on tutorial for personalized movie recommendation with spark.mllib.Dimensionality ReductionPrincipal component analysisPCA in Scala123456789101112import org.apache.spark.mllib.linalg.Matriximport org.apache.spark.mllib.linalg.distributed.RowMatrixval points: RDD[Vector] = // ...val mat: RowMatrix = new RowMatrix(points)val pc: Matrix = mat.computePrincipalComponents(2)// Project points to low-dimensional spaceval projected = mat.multiply(pc).rows// Train a k-means model on the projected 2-dimensional dataval model = KMeans.train(projected, 10)Singular value decompositionSVD in Scala1234567// Compute the top 20 singular values of a RowMatrix mat and their singular vectors.val svd: SingularValueDecomposition[RowMatrix, Matrix] = mat.computeSVD(20, computeU=true)val U: RowMatrix = svd.U // U is a distributed RowMatrix.val s: Vector = svd.s // Singular values are a local dense vector.val V: Matrix = svd.V // V is a local dense matrix.Pipeline APIPipeline API version of spam classification in Scala1234567891011121314151617181920212223242526272829303132333435363738394041import org.apache.spark.sql.SQLContextimport org.apache.spark.ml.Pipelineimport org.apache.spark.ml.classification.LogisticRegressionimport org.apache.spark.ml.feature.&#123;HashingTF, Tokenizer&#125;import org.apache.spark.ml.tuning.&#123;CrossValidator, ParamGridBuilder&#125;import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator// A class to represent documents -- will be turned into a SchemaRDDcase class LabeledDocument(id: Long, text: String, label: Double)val documents = // (load RDD of LabeledDocument)val sqlContext = new SQLContext(sc)import sqlContext._// Configure an ML pipeline with three stages: tokenizer, tf, and lr; each stage// outputs a column in a SchemaRDD and feeds it to the next stage's input columnval tokenizer = new Tokenizer() // Splits each email into words .setInputCol("text") .setOutputCol("words")val tf = new HashingTF() // Maps email words to vectors of 10000 features .setNumFeatures(10000) .setInputCol(tokenizer.getOutputCol) .setOutputCol("features")val lr = new LogisticRegression() // Uses "features" as inputCol by defaultval pipeline = new Pipeline().setStages(Array(tokenizer, tf, lr))// Fit the pipeline to the training documentsval model = pipeline.fit(documents)// Alternatively, instead of fitting once with the parameters above, we can do a// grid search over some parameters and pick the best model via cross-validationval paramMaps = new ParamGridBuilder() .addGrid(tf.numFeatures, Array(10000, 20000)) .addGrid(lr.maxIter, Array(100, 200)) .build() // Builds all combinations of parametersval eval = new BinaryClassificationEvaluator()val cv = new CrossValidator() .setEstimator(lr) .setEstimatorParamMaps(paramMaps) .setEvaluator(eval)val bestModel = cv.fit(documents)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Solution for Bracket in markdown link address]]></title>
      <url>%2F2017%2F04%2F23%2F%C2%96Solution-for-Bracket-in-markdown-link-address%2F</url>
      <content type="text"><![CDATA[Markdownåˆ›é€ ä¸€ä¸ªé“¾æŽ¥æˆ–è€…å›¾ç‰‡æ˜¯ä½¿ç”¨ [title](link) å’Œ ![title](link).æˆ‘ä»¬å¯ä»¥é¿å…[]å†…å‡ºçŽ°ä¸­æ‹¬å·, æˆ–è€…ä½¿ç”¨è½¬ä¹‰.ä½†æ˜¯åœ¨å°æ‹¬å·çš„é“¾æŽ¥é‡Œé¢å°±å¯èƒ½ä¼šå‡ºé—®é¢˜. æœ‰äº›ç½‘å€ä¸Šé¢ä¼šå…·æœ‰å°æ‹¬å·. ä¾‹å¦‚,https://github.com/CjTouzi/Learning-RSpark/blob/master/Zaharia%20M.%2C%20et%20al.%20Learning%20Spark%20%28O%27Reilly%2C%202015%29%28274s%29.pdfè§£å†³æ–¹æ³•:%28 ä»£æ›¿(, %29ä»£æ›¿) ä¸»è¦æ˜¯åŽè€…ä¼šæ­§ä¹‰é“¾æŽ¥éƒ¨åˆ†çš„ç»“æŸ. è¿™æ˜¯ä½¿ç”¨urlç¬¦å·ç åŽ»ä»£æ›¿asciiçš„ç¬¦å·. èƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[The first course of the Spark]]></title>
      <url>%2F2017%2F04%2F23%2FThe-first-course-of-the-Spark%2F</url>
      <content type="text"><![CDATA[ç®€ä»‹æ‰©å……äº†MapReduceè®¡ç®—æ¨¡åž‹åŸºäºŽå†…å­˜çš„è®¡ç®—èƒ½å¤Ÿè¿›è¡Œæ‰¹å¤„ç†ã€è¿­ä»£å¼è®¡ç®—ã€äº¤äº’æŸ¥è¯¢å’Œæµå¤„ç†é™ä½Žé‡Œç»´æŠ¤æˆæœ¬æä¾›äº†Pythonã€Javaã€Scalaã€SQLçš„APIå’Œä¸°å¯Œçš„å†…ç½®åº“å¯ä»¥ä¸ŽHadoopã€Kafkaç­‰æ•´åˆç»„ä»¶Spark CoreSpark Core contains the basic functionality of Spark, including components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. Spark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Sparkâ€™s main programming abstraction. RDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.Spark SQLSpark SQL is Sparkâ€™s package for working with structured data. It allows querying data via SQL as well as the Apache Hive variant of SQLâ€”called the Hive Query Language (HQL)â€”and it supports many sources of data, including Hive tables, Parquet, and JSON. Beyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java, and Scala, all within a single application, thus combining SQL with complex analytics. This tight integration with the rich computing environment provided by Spark makes Spark SQL unlike any other open source data warehouse tool. Spark SQL was added to Spark in version 1.0.Shark was an older SQL-on-Spark project out of the University of California, Berkeley, that modified Apache Hive to run on Spark. It has now been replaced by Spark SQL to provide better integration with the Spark engine and language APIs.Spark StreamingSpark Streaming is a Spark component that enables processing of live streams of data. Examples of data streams include logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service. Spark Streaming provides an API for manipulating data streams that closely matches the Spark Coreâ€™s RDD API, making it easy for programmers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. Underneath its API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.MLlibSpark comes with a library containing common machine learning (ML) functionality, called MLlib. MLlib provides multiple types of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as supporting functionality such as model evaluation and data import. It also provides some lower-level ML primitives, including a generic gradient descent optimization algorithm. All of these methods are designed to scale out across a cluster.GraphXGraphX is a library for manipulating graphs (e.g., a social networkâ€™s friend graph) and performing graph-parallel computations. Like Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. GraphX also provides various operators for manipulating graphs (e.g., subgraph and mapVertices) and a library of common graph algorithms (e.g., PageRank and triangle counting).Cluster ManagersUnder the hood, Spark is designed to efficiently scale up from one to many thousands of compute nodes. To achieve this while maximizing flexibility, Spark can run over a variety of cluster managers, including Hadoop YARN, Apache Mesos, and a simple cluster manager included in Spark itself called the Standalone Scheduler. If you are just installing Spark on an empty set of machines, the Standalone Scheduler provides an easy way to get started; if you already have a Hadoop YARN or Mesos cluster, however, Sparkâ€™s support for these cluster managers allows your applications to also run on them.å®‰è£…Sparkç”±Scalaç¼–å†™ï¼Œè¿è¡ŒäºŽJVMä¸Šï¼Œè¿è¡ŒçŽ¯å¢ƒä¸ºJava 7+å¦‚æžœä½¿ç”¨Python APIï¼Œéœ€è¦å®‰è£…Python 2.6+ æˆ–è€…Python 3.4+Spark 1.6.2 â€“ Scala 2.10 / Spark 2.0.0 â€“ Scala 2.11ä¸‹è½½http://spark.apache.org/downloads.htmlä¸éœ€è¦Hadoopé›†ç¾¤ï¼›å¦‚æžœå·²ç»æ­å»ºå¥½Hadoopé›†ç¾¤ï¼Œå¯ä¸‹è½½ç›¸åº”ç‰ˆæœ¬è§£åŽ‹ç›®å½•README.mdContains short instructions for getting started with Spark.binContains executable files that can be used to interact with Spark in various ways (e.g., the Spark shell, which we will cover later in this chapter).core, streaming, python, â€¦Contains the source code of major components of the Spark project.examplesContains some helpful Spark standalone jobs that you can look at and run tolearn about the Spark API.ShellPython Shellbin/pysparkScala Shellbin/spark-shellå¼€å‘çŽ¯å¢ƒæ­å»ºScalaå®‰è£…https://www.scala-lang.org/download/æ³¨æ„ç‰ˆæœ¬å¯¹åº”IntelliJ IDEAå®‰è£…https://www.jetbrains.com/idea/#chooseYourEditionå¯ä»¥ç”³è¯·æ•™è‚²è´¦å·æ’ä»¶å®‰è£…File-Settings-Plugins æœç´¢Scalaï¼Œå®‰è£…é¡¹ç›®åˆ›å»ºFile-New-Project-Scala-SBTåŒæ ·æ³¨æ„ç‰ˆæœ¬åŒ¹é…ï¼ˆè¿™é‡Œç”¨çš„æ˜¯Spark 2.1.0, Scala 2.11.11ï¼‰é…ç½®æ–‡ä»¶éœ€è¦å®šä¹‰ä½¿ç”¨çš„Sparkç‰ˆæœ¬build.sbtè¿½åŠ 123libraryDependencies ++= Seq( "org.apache.spark" %% "spark-core" % "2.1.0")é‡å»ºé¡¹ç›®å³å¯æºç¨‹åºç¼–å†™New-Scala Class-Class to ObjectWordCount.scala1234567891011121314151617import org.apache.spark.&#123;SparkContext, SparkConf&#125;/** * Created by root on 4/23/17. */object WordCount &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setAppName("wordcount") val sc = new SparkContext(conf) val input = sc.textFile("/home/hduser/Anaconda2-4.3.1-Linux-x86_64.sh") val lines = input.flatMap(line =&gt; line.split(" ")) val count = lines.map(word =&gt; (word, 1)).reduceByKey&#123;case (x, y) =&gt; x + y&#125; val output = count.saveAsTextFile("/home/hduser/scala_wordcount_demo_output") &#125;&#125;æ‰“åŒ…File-Project Structure-Project Setting-Artifacts-Add-JAR-From modules with dependenciesBuild-Build Artifacts-Buildå¯åŠ¨é›†ç¾¤å¯åŠ¨mastersbin/start-master.shå¯åŠ¨workerbin/spark-class org.apache.spark.deploy.worker.Worker spark://Ubuntu:7077æ³¨æ„è¿™é‡Œçš„sparkæœåŠ¡å™¨åœ°å€å¯ä»¥é€šè¿‡æµè§ˆå™¨è¾“å…¥localhost:8080æ¥æŸ¥çœ‹æäº¤ä½œä¸šbin/spark-submit --master spark://Ubuntu:7077 --class WordCount /home/hduser/scala_demo.jaræ³¨æ„è¿™é‡Œçš„scala_demo.jaræ–‡ä»¶ä¸ºæ‰“åŒ…é˜¶æ®µç”ŸæˆTODO: RDDsZaharia M., et al. Learning Spark (Oâ€™Reilly, 2015)(274s).pdfReferencesæ…•è¯¾ç½‘ http://www.imooc.com/learn/814Zaharia M., et al. Learning Spark (Oâ€™Reilly, 2015)(274s).pdf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Implement k-means on the hadoop platform]]></title>
      <url>%2F2017%2F04%2F21%2FImplement-k-means-on-the-hadoop-platform%2F</url>
      <content type="text"><![CDATA[é¦–å…ˆåœ¨å•æœºä¸Šæ­ä¸€ä¸ªä¼ªåˆ†å¸ƒå¼çŽ¯å¢ƒï¼Œä¸»è¦æ˜¯å¯¹*-site.xmlé…ç½®æ–‡ä»¶è¿›è¡Œä¿®æ”¹ï¼Œå…·ä½“ä¿®æ”¹å¦‚ä¸‹ï¼šcore-site.xml12345678&lt;?xml version="1.0"?&gt;&lt;!-- core-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost/&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;hdfs-site.xml12345678&lt;?xml version="1.0"?&gt;&lt;!-- hdfs-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;yarn-site.xml123456789101112&lt;?xml version="1.0"?&gt;&lt;!-- yarn-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;mapred-site.xml12345678&lt;?xml version="1.0"?&gt;&lt;!-- mapred-site.xml --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;ç„¶åŽå¯åŠ¨hadoopï¼Œå¯åŠ¨çš„æµç¨‹å¦‚ä¸‹ï¼šstart-dfs.shstart-yarn.shmr-jobhistory-daemon.sh start historyserveræ³¨æ„ï¼Œä»¥ä¸Šå‘½ä»¤èƒ½å¤Ÿå¾—åˆ°æ­£ç¡®æ‰§è¡Œçš„å‰ææ˜¯å·²ç»å°†hadoopçš„å®‰è£…ç›®å½•ä¸‹çš„binç›®å½•åŠ åˆ°çŽ¯å¢ƒå˜é‡ä¸­ç”±äºŽå¾ˆä¹…æ²¡æœ‰ä½¿ç”¨Javaï¼Œæ‰€ä»¥é‡‡ç”¨Pythonå®žçŽ°ï¼Œè¿™é‡Œéœ€è¦ç”¨åˆ°ä¸€ä¸ªpackageï¼Œä¹Ÿå°±æ˜¯mrjobé¦–å…ˆè®¡åˆ’ä¸€ä¸‹å®žçŽ°æ­¥éª¤ï¼š[ Mapper ]Acceptsdataglobal constant representing the list of centersComputesthe nearest center for each data instanceEmitsnearest centers (key) and points (value).[ Reducer ]Acceptscenter instance / coordinate (key)points (value)Computesthe new centers based on clustersEmitsnew centersYou will provide the next epoch of K-Means with:the same data from your initial epochthe centers emitted from the reducer as global constantsRepeat until your stopping criteria are met.å¦‚æžœè¦ç”¨Pythonè¿›è¡Œç›¸å…³çš„Hadoopæ“ä½œçš„è¯ï¼Œè‚¯å®šæ˜¯è¦ä½¿ç”¨hadoop streamingçš„ï¼Œä½†æ˜¯å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œä¹Ÿå°±æ˜¯streamingæµç¨‹åªèƒ½è·‘ä¸€éï¼Œä½†æ˜¯å¾ˆæ˜¾ç„¶ï¼Œä½œä¸ºä¸€ä¸ªmachine learningç®—æ³•ï¼Œk-meansæ˜¯ç±»ä¼¼äºŽEMç®—æ³•è¦ç»è¿‡å¤šæ­¥è¿­ä»£çš„ï¼Œé‚£ä¹ˆæœ€å®¹æ˜“æƒ³åˆ°çš„å°±æ˜¯ä½¿ç”¨shellè„šæœ¬å¤šæ¬¡è°ƒç”¨ç›¸å…³å‘½ä»¤ï¼Œä½†æ˜¯è¿™æ ·æ˜¾å¾—ååˆ†uglyï¼Œå› æ­¤å¯ä»¥é‡‡ç”¨mrjobåŒ…æ¥å¸®åŠ©æˆ‘ä»¬å®Œæˆè¿™ä¸ªå·¥ä½œã€‚ä»Žä¸Šé¢çœ‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªæ˜¯pythonå®žçŽ°çš„map-reduceï¼Œå¦ä¸€ä¸ªæ˜¯mrjobçš„jobæ–‡ä»¶ï¼Œç›¸å½“äºŽmasterï¼Œä¸‹é¢åˆ—å‡ºè¿™ä¸¤ä¸ªæ–‡ä»¶ï¼Œå› ä¸ºå®žçŽ°æ¯”è¾ƒç®€å•ï¼Œå› æ­¤ä¸ä½œè¿‡å¤šè§£é‡Š.kmeans.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788from mrjob.job import MRJobimport mrjob# MRJob is a python class which will be overloadedfrom math import sqrtclass MRKMeans(MRJob): SORT_VALUES = True OUTPUT_PROTOCOL = mrjob.protocol.RawProtocol def dist_vec(self, v1, v2): # calculate the ditance between two vectors (in two dimensions) return sqrt((v2[0] - v1[0]) * (v2[0] - v1[0]) + (v2[1] - v1[1]) * (v2[1] - v1[1])) def configure_options(self): super(MRKMeans, self).configure_options() # the line below define that the file folowing the --c option is the # centroid and is loadable self.add_file_option('--c') def get_centroids(self): """ Definition : extracts centroids from the centroids file define afetr --c flag Out : Return the list of centroids """ # self.options.c is the name of the file following --c option f = open(self.options.c, 'r') centroids = [] for line in f.read().split('\n'): if line: x, y = line.split(', ') centroids.append([float(x), float(y)]) f.close() return centroids def mapper(self, _, lines): """ Definition : Mapper take centroids extract form get_centroids() and the point cloud and for each point, calculate the distance to the centroids, find the mininum of it Out : yield the point with it's class """ centroids = self.get_centroids() for l in lines.split('\n'): x, y = l.split(', ') point = [float(x), float(y)] min_dist = 100000000.0 classe = 0 # iterate over the centroids (Here we know that we are doing a 3means) for i in range(3): dist = self.dist_vec(point, centroids[i]) if dist &lt; min_dist: min_dist = dist classe = i yield classe, point def combiner(self, k, v): """ Definition : Calculate for each class, at the end of the mapper, before reducer, the medium point of each class Out: return for each class, the centroids for each mapper """ count = 0 moy_x = moy_y = 0.0 for t in v: count += 1 moy_x += t[0] moy_y += t[1] yield k, (moy_x / count, moy_y / count) def reducer(self, k, v): """ Definition : for each class, get all the tmp centroids from each combiner and calculate the new centroids. """ # k is class and v are medium points linked to the class count = 0 moy_x = moy_y = 0.0 for t in v: count += 1 moy_x += t[0] moy_y += t[1] print str(k) + ", " + str(moy_x / count) + ", " + str(moy_y / count)if __name__ == '__main__': # just run mapreduce ! MRKMeans.run()main.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889from mrjob.job import MRJobfrom kmeans import MRKMeansimport sysimport os.pathimport shutilfrom math import sqrtimport timeinput_c = "centroids"CENTROIDS_FILE = "/home/hduser/tmp/centroid"def get_c(job, runner): c = [] for line in runner.stream_output(): # print "stream_output: ", line key, value = job.parse_output_line(line) c.append(key) return cdef get_first_c(fname): f = open(fname, 'r') centroids = [] for line in f.read().split('\n'): if line: x, y = line.split(', ') centroids.append([float(x), float(y)]) f.close() return centroidsdef write_c(centroids): f = open(CENTROIDS_FILE, "w") centroids.sort() for c in centroids: k, cx, cy = c.split(', ') # print c f.write("%s, %s\n" % (cx, cy)) f.close()def dist_vec(v1, v2): return sqrt((v2[0] - v1[0]) * (v2[0] - v1[0]) + (v2[1] - v1[1]) * (v2[1] - v1[1]))def diff(cs1, cs2): max_dist = 0.0 for i in range(3): dist = dist_vec(cs1[i], cs2[i]) if dist &gt; max_dist: max_dist = dist return max_distif __name__ == '__main__': args = sys.argv[1:] if not os.path.isfile(CENTROIDS_FILE): shutil.copy(input_c, CENTROIDS_FILE) old_c = get_first_c(input_c) i = 1 start = time.time() while True: print "Iteration #%i" % i mr_job = MRKMeans(args=args + ['--c=' + CENTROIDS_FILE]) # print "start runner.." with mr_job.make_runner() as runner: runner.run() centroids = get_c(mr_job, runner) # print "mr result: ", centroids write_c(centroids) n_c = get_first_c(CENTROIDS_FILE) # print "old_c", old_c # print "n_c", n_c max_d = diff(n_c, old_c) # print "dist max = "+str(max_d) if max_d &lt; 0.01: break else: old_c = n_c i = i + 1 print "used time: ", time.time() - start, 's'æ ¹æ®ä¸Šé¢å†™çš„å®žçŽ°æ­¥éª¤å¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªå­˜å‚¨è¾“å…¥æ•°æ®ï¼Œå¦ä¸€ä¸ªå­˜å‚¨centroidsï¼Œç”±äºŽåªæ˜¯ä¸€ä¸ªdemoï¼Œå› æ­¤åœ¨è¿™é‡Œæˆ‘ç®€åŒ–äº†å…·ä½“é—®é¢˜ã€‚è®¾æ‰€æœ‰çš„æ•°æ®éƒ½æ˜¯äºŒç»´æ•°æ®ç‚¹ï¼Œå¹¶ä¸”èšç±»ä¸ªæ•°ä¸º3ã€‚å½“ç„¶ï¼Œå¦‚æžœçœŸçš„æ˜¯åœ¨å¤§æ•°æ®ä¸Šè¿›è¡Œå·¥ä¸šçº§çš„å¤„ç†çš„è¯ï¼Œè¿˜æ˜¯æŽ¨èä½¿ç”¨Sparkã€‚ä¸‹é¢åˆ—å‡ºè¿™ä¸¤ä¸ªæ–‡ä»¶ï¼škmeans_data12345678910111213141516171, 22, 31, 3.54, 3.53, 4.22, 1.65, 2.31.5, 2.33, 5.22, 31, 3.54, 3.53, 4.22, 1.65, 2.31.5, 2.33, 5centroids1231, 22, 31, 3.5æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è¿è¡Œï¼š1python main.py kmeans_data -r hadoopç»“æžœæ˜¾ç¤ºå¦‚ä¸‹ï¼š1234567891011Iteration #1No handlers could be found for logger "mrjob.hadoop"old_c [[1.0, 2.0], [2.0, 3.0], [1.0, 3.5]]n_c [[1.625, 1.95833333333], [3.4, 3.62], [1.0, 3.5]]Iteration #2old_c [[1.625, 1.95833333333], [3.4, 3.62], [1.0, 3.5]]n_c [[1.72916666667, 2.2625], [3.75, 3.775], [1.0, 3.5]]Iteration #3old_c [[1.72916666667, 2.2625], [3.75, 3.775], [1.0, 3.5]]n_c [[1.72916666667, 2.2625], [3.75, 3.775], [1.0, 3.5]]time: 148.277868032æœ€åŽç”Ÿæˆç»“æžœæ–‡ä»¶ï¼šcentroid1231.72916666667, 2.26253.75, 3.7751.0, 3.5æ ¹æ®ä»¥ä¸Šå¯ä»¥çœ‹å‡ºï¼Œå¯¹äºŽå°æ•°æ®é›†ï¼Œæ•ˆçŽ‡åè€Œä¼šæ¯”è¾ƒä½Žï¼Œå› ä¸ºæ•´ä¸ªç¨‹åºè¿è¡Œè¿‡ç¨‹ä¸­å¤§éƒ¨åˆ†çš„æ—¶é—´éƒ½æ²¡æœ‰èŠ±åœ¨å®žé™…çš„ç®—æ³•è¿è¡Œä¸Šã€‚TODOï¼šç”¨å¸¸è§„æ–¹æ³•å®žçŽ°ï¼Œä½œä¸ºbaselineåœ¨å¤§æ•°æ®é›†ä¸Šç»§ç»­å®žéªŒï¼Œè§‚å¯Ÿç»“æžœ]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Something about cloudera]]></title>
      <url>%2F2017%2F04%2F20%2FSomething-about-cloudera%2F</url>
      <content type="text"><![CDATA[æœ€è¿‘å‘çŽ°äº†ä¸€ä¸ªç¥žå™¨ï¼Œclouderahttps://www.cloudera.com/å®ƒå…¶å®žæ˜¯ä¸€ä¸ªé›†æˆäº†Hadoopç”Ÿæ€ç³»ç»Ÿçš„CentOS 6.7çš„VMï¼Œå¯ä»¥è·‘åœ¨Dockerã€Virtual Boxæˆ–è€…VMwareä¸Šhttps://www.cloudera.com/downloads/quickstart_vms/5-10.htmlè™šæ‹Ÿæœºé…ç½®çš„æ—¶å€™éœ€è¦åˆ†é…è‡³å°‘8Gçš„RAMä»¥åŠ2ä¸ªCoresã€‚å¦å¤–ï¼Œç¬¬ä¸€æ¬¡å¯åŠ¨ä¼šæœ‰äº›æ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop Distributed Filesystem notes]]></title>
      <url>%2F2017%2F04%2F18%2FHadoop-Distributed-Filesystem-notes%2F</url>
      <content type="text"><![CDATA[HDFS ConceptsBlocks128 MB by defaultHDFS blocks are large compared to disk blocks, and the reason is to minimize the costof seeks.Having a block abstraction for a distributed filesystem brings several benefitsA file can be larger than any single disk in the network.Simplifies the storage subsystem.Providing fault tolerance and availability.% hdfs fsck / -files -blocksNamenodes and DatanodesAn HDFS cluster has two types of nodes operating in a masterâˆ’worker patternnamenode (the master)datanodes (workers)Without the namenode, the filesystem cannot be usedFor this reason, it is important to make the namenode resilient to failureThe first way is to back up the files that make up the persistent state of the filesystemmetadata.It is also possible to run a secondary namenodeBlock CachingFor frequently accessed files the blocks may be explicitly cached in the datanodeâ€™s memory, in an off-heap block cache. By default, a block is cached in only one datanodeâ€™s memory.HDFS Federationone namenode might manage all the files rooted under /user, say, and a second nameâ€node might handle files under /share.namespace volumeblock poolnamenodes do not communicate with one anotherHDFS High AvalibilityThe new namenode is not able to serve requests until it hasloaded its namespace image into memoryreplayed its edit logreceived enough block reports from the datanodes to leave safe mode.On large clusters with many files and blocks, the time it takes for a namenode to start from cold can be 30 minutes or more.Hadoop 2 remedied this situation by adding support for HDFS high availability (HA).there are a pair of namenodes in an active-standby configuration. In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption.There are two choices for the highly available shared storageNFS filerquorum journal manager (QJM)The actual observed failover time will be longer in practice (around a minute or so)The transition from the active namenode to the standby is managed by a new entity inthe system called the failover controllerdefault implementation uses ZooKeeper to ensure that only one namenode is active.The QJM only allows one namenode to write to the edit log at one timeThe Command-Line InterfaceBasic Filesystem Operationscopying a file from the local filesystem to HDFS12% hadoop fs -copyFromLocal input/docs/quangle.txt \ hdfs://localhost/user/tom/quangle.txtcopy the file back to the local filesystem and check whether itâ€™s the same1234% hadoop fs -copyToLocal quangle.txt quangle.copy.txt% md5 input/docs/quangle.txt quangle.copy.txte7891a2627cf263a079fb0f18256ffb2 input/docs/quangle.txtMD5 (quangle.copy.txt) = e7891a2627cf263a079fb0f18256ffb2create a directory first just to see how it is displayed in the listing12345% hadoop fs -mkdir books% hadoop fs -ls .Found 2 itemsdrwxr-xr-x - tom supergroup 0 2014-10-04 13:22 books-rw-r--r-- 1 tom supergroup 119 2014-10-04 13:21 quangle.txtThe Java InterfaceReading Data from a Hadoop URLExample. Displaying files from a Hadoop filesystem on standard output using a URLStreamHandler1234567891011121314151617181920212223242526// cc URLCat Displays files from a Hadoop filesystem on standard output using a URLStreamHandlerimport java.io.InputStream;import java.net.URL;import org.apache.hadoop.fs.FsUrlStreamHandlerFactory;import org.a pache.hadoop.io.IOUtils;// vv URLCatpublic class URLCat &#123; static &#123; URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory()); &#125; public static void main(String[] args) throws Exception &#123; InputStream in = null; try &#123; in = new URL(args[0]).openStream(); IOUtils.copyBytes(in, System.out, 4096, false); &#125; finally &#123; IOUtils.closeStream(in); &#125; &#125;&#125;// ^^ URLCatThereâ€™s a little bit more work required to make Java recognize Hadoopâ€™s hdfs URL scheme. This is achieved by calling the setURLStreamHandlerFactory() method on URL with an instance of FsUrlStreamHandlerFactory. This method can be called only once per JVM, so it is typically executed in a static block.Hereâ€™s a sample run:123456% export HADOOP_CLASSPATH=hadoop-examples.jar% hadoop URLCat hdfs://localhost/user/tom/quangle.txtOn the top of the Crumpetty TreeThe Quangle Wangle sat,But his face you could not see,On account of his Beaver Hat.Reading Data Using the FileSystem APIExample. Displaying files from a Hadoop filesystem on standard output by using the FileSystem directly1234567891011121314151617181920212223242526// cc FileSystemCat Displays files from a Hadoop filesystem on standard output by using the FileSystem directlyimport java.io.InputStream;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;// vv FileSystemCatpublic class FileSystemCat &#123; public static void main(String[] args) throws Exception &#123; String uri = args[0]; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), conf); InputStream in = null; try &#123; in = fs.open(new Path(uri)); IOUtils.copyBytes(in, System.out, 4096, false); &#125; finally &#123; IOUtils.closeStream(in); &#125; &#125;&#125;// ^^ FileSystemCatThe program runs as follows:12345% hadoop FileSystemCat hdfs://localhost/user/tom/quangle.txtOn the top of the Crumpetty TreeThe Quangle Wangle sat,But his face you could not see,On account of his Beaver Hat.FSDataInputStreamExample. Displaying files from a Hadoop filesystem on standard output twice, by using seek()12345678910111213141516171819202122232425262728// cc FileSystemDoubleCat Displays files from a Hadoop filesystem on standard output twice, by using seekimport java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;// vv FileSystemDoubleCatpublic class FileSystemDoubleCat &#123; public static void main(String[] args) throws Exception &#123; String uri = args[0]; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), conf); FSDataInputStream in = null; try &#123; in = fs.open(new Path(uri)); IOUtils.copyBytes(in, System.out, 4096, false); in.seek(0); // go back to the start of the file IOUtils.copyBytes(in, System.out, 4096, false); &#125; finally &#123; IOUtils.closeStream(in); &#125; &#125;&#125;// ^^ FileSystemDoubleCatHereâ€™s the result of running it on a small file:123456789% hadoop FileSystemDoubleCat hdfs://localhost/user/tom/quangle.txtOn the top of the Crumpetty TreeThe Quangle Wangle sat,But his face you could not see,On account of his Beaver Hat.On the top of the Crumpetty TreeThe Quangle Wangle sat,But his face you could not see,On account of his Beaver Hat.Writing DataExample. Copying a local file to a Hadoop filesystem123456789101112131415161718192021222324252627282930313233// cc FileCopyWithProgress Copies a local file to a Hadoop filesystem, and shows progressimport java.io.BufferedInputStream;import java.io.FileInputStream;import java.io.InputStream;import java.io.OutputStream;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.util.Progressable;// vv FileCopyWithProgresspublic class FileCopyWithProgress &#123; public static void main(String[] args) throws Exception &#123; String localSrc = args[0]; String dst = args[1]; InputStream in = new BufferedInputStream(new FileInputStream(localSrc)); Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(dst), conf); OutputStream out = fs.create(new Path(dst), new Progressable() &#123; public void progress() &#123; System.out.print("."); &#125; &#125;); IOUtils.copyBytes(in, out, 4096, true); &#125;&#125;// ^^ FileCopyWithProgressTypical usage:123% hadoop FileCopyWithProgress input/docs/1400-8.txthdfs://localhost/user/tom/1400-8.txt.................Querying the FilesystemThe FileStatus class encapsulates filesystem metadata for files and directories, including file length, block size, replication, modification time, ownership, and permission information.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class ShowFileStatusTest &#123; private MiniDFSCluster cluster; // use an in-process HDFS cluster for testing private FileSystem fs; @Before public void setUp() throws IOException &#123; Configuration conf = new Configuration(); if (System.getProperty("test.build.data") == null) &#123; System.setProperty("test.build.data", "/tmp"); &#125; cluster = new MiniDFSCluster.Builder(conf).build(); fs = cluster.getFileSystem(); OutputStream out = fs.create(new Path("/dir/file")); out.write("content".getBytes("UTF-8")); out.close(); &#125; @After public void tearDown() throws IOException &#123; if (fs != null) &#123; fs.close(); &#125; if (cluster != null) &#123; cluster.shutdown(); &#125; &#125; @Test(expected = FileNotFoundException.class) public void throwsFileNotFoundForNonExistentFile() throws IOException &#123; fs.getFileStatus(new Path("no-such-file")); &#125; @Test public void fileStatusForFile() throws IOException &#123; Path file = new Path("/dir/file"); FileStatus stat = fs.getFileStatus(file); assertThat(stat.getPath().toUri().getPath(), is("/dir/file")); assertThat(stat.isDirectory(), is(false)); assertThat(stat.getLen(), is(7L)); assertThat(stat.getModificationTime(), is(lessThanOrEqualTo(System.currentTimeMillis()))); assertThat(stat.getReplication(), is((short) 1)); assertThat(stat.getBlockSize(), is(128 * 1024 * 1024L)); assertThat(stat.getOwner(), is(System.getProperty("user.name"))); assertThat(stat.getGroup(), is("supergroup")); assertThat(stat.getPermission().toString(), is("rw-r--r--")); &#125; @Test public void fileStatusForDirectory() throws IOException &#123; Path dir = new Path("/dir"); FileStatus stat = fs.getFileStatus(dir); assertThat(stat.getPath().toUri().getPath(), is("/dir")); assertThat(stat.isDirectory(), is(true)); assertThat(stat.getLen(), is(0L)); assertThat(stat.getModificationTime(), is(lessThanOrEqualTo(System.currentTimeMillis()))); assertThat(stat.getReplication(), is((short) 0)); assertThat(stat.getBlockSize(), is(0L)); assertThat(stat.getOwner(), is(System.getProperty("user.name"))); assertThat(stat.getGroup(), is("supergroup")); assertThat(stat.getPermission().toString(), is("rwxr-xr-x")); &#125; &#125;Listing filesExample. Showing the file statuses for a collection of paths in a Hadoop filesystem123456789101112131415161718192021222324252627282930// cc ListStatus Shows the file statuses for a collection of paths in a Hadoop filesystem import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.FileUtil;import org.apache.hadoop.fs.Path;// vv ListStatuspublic class ListStatus &#123; public static void main(String[] args) throws Exception &#123; String uri = args[0]; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), conf); Path[] paths = new Path[args.length]; for (int i = 0; i &lt; paths.length; i++) &#123; paths[i] = new Path(args[i]); &#125; FileStatus[] status = fs.listStatus(paths); Path[] listedPaths = FileUtil.stat2Paths(status); for (Path p : listedPaths) &#123; System.out.println(p); &#125; &#125;&#125;// ^^ ListStatusWe can use this program to find the union of directory listings for a collection of paths:1234% hadoop ListStatus hdfs://localhost/ hdfs://localhost/user/tomhdfs://localhost/userhdfs://localhost/user/tom/bookshdfs://localhost/user/tom/quangle.txtDataFlowAnatomy of a File ReadNetwork Topology and HadoopMathematically inclined readers will notice that this is an example of a distance metric.Anatomy of a File WriteA typical replica pipeline:Coherency ModelAfter creating a file, it is visible in the filesystem namespace, as expected:123Path p = new Path("p");fs.create(p);assertThat(fs.exists(p), is(true));However, any content written to the file is not guaranteed to be visible, even if the stream is flushed. So, the file appears to have a length of zero:12345Path p = new Path("p");OutputStream out = fs.create(p);out.write("content".getBytes("UTF-8"));out.flush();assertThat(fs.getFileStatus(p).getLen(), is(0L));HDFS provides a way to force all buffers to be flushed to the datanodes via the hflush() method on FSDataOutputStream. After a successful return from hflush(), HDFS guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers:12345Path p = new Path("p");FSDataOutputStream out = fs.create(p);out.write("content".getBytes("UTF-8"));out.hflush();assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));Note that hflush() does not guarantee that the datanodes have written the data to disk, only that itâ€™s in the datanodesâ€™ memory (so in the event of a data center power outage, for example, data could be lost). For this stronger guarantee, use hsync() instead.12345FileOutputStream out = new FileOutputStream(localFile);out.write("content".getBytes("UTF-8"));out.flush(); // flush to operating systemout.getFD().sync(); // sync to diskassertThat(localFile.length(), is(((long) "content".length())));Closing a file in HDFS performs an implicit hflush(), too:12345Path p = new Path("p");OutputStream out = fs.create(p);out.write("content".getBytes("UTF-8"));out.close();assertThat(fs.getFileStatus(p).getLen(), is(((long) "content".length())));Parallel Copying with distcpOne use for distcp is as an efficient replacement for hadoop fs -cp. For example, you can copy one file to another with:% hadoop distcp file1 file2You can also copy directories:% hadoop distcp dir1 dir2]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop namenode not getting started]]></title>
      <url>%2F2017%2F04%2F18%2FHadoop-namenode-not-getting-started%2F</url>
      <content type="text"><![CDATA[First delete all contents from temporary folder: rm -rf &lt;tmp dir&gt; (my was /usr/local/hadoop/tmp)Format the namenode: bin/hadoop namenode -formatStart all processes againbin/start-dfs.shbin/start-yarn.shbin/mr-jobhistory-daemon.sh start historyserverYou may consider rolling back as well using checkpoint (if you had it enabled).]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hadoop ch02 MapReduce notes]]></title>
      <url>%2F2017%2F04%2F17%2FHadoop-ch02-MapReduce-notes%2F</url>
      <content type="text"><![CDATA[MapReduceé¦–å…ˆæˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå…³äºŽå¤©æ°”çš„ï¼Œç„¶åŽå®ƒçš„æ¯ä¸€æ¡è®°å½•æ˜¯è¿™æ ·çš„ï¼š123456789101112131415161718192021222324252627282930310057332130 # USAF weather station identifier99999 # WBAN weather station identifier19500101 # observation date0300 # observation time4+51317 # latitude (degrees x 1000)+028783 # longitude (degrees x 1000)FM-12+0171 # elevation (meters)99999V020320 # wind direction (degrees)1 # quality codeN0072100450 # sky ceiling height (meters)1 # quality codeCN010000 # visibility distance (meters)1 # quality codeN9-0128 # air temperature (degrees Celsius x 10)1 # quality code-0139 # dew point temperature (degrees Celsius x 10)1 # quality code10268 # atmospheric pressure (hectopascals x 10)1 # quality codeå½“ç„¶ä»¥ä¸Šæ•°æ®æ˜¯ç»è¿‡å¤„ç†ä¹‹åŽçš„ï¼Œä¸€å¼€å§‹å®ƒé•¿è¿™æ ·ï¼š123450067011990999991950051507004...9999999N9+00001+99999999999...0043011990999991950051512004...9999999N9+00221+99999999999...0043011990999991950051518004...9999999N9-00111+99999999999...0043012650999991949032412004...0500001N9+01111+99999999999...0043012650999991949032418004...0500001N9+00781+99999999999...Hmmmâ€¦.è¿™ä¸ªå¤©æ°”æ•°æ®é›†æŒ‰ç…§æ°”è±¡ç«™ç¼–å·-å¹´ä»½çš„å½¢å¼æ¥ç»„ç»‡çš„ï¼š12345678910010010-99999-1990.gz010014-99999-1990.gz010015-99999-1990.gz010016-99999-1990.gz010017-99999-1990.gz010030-99999-1990.gz010040-99999-1990.gz010080-99999-1990.gz010100-99999-1990.gz010150-99999-1990.gzè¿™ä¸ªåŽŸå§‹æ•°æ®æ˜¾ç„¶ç”¨èµ·æ¥ä¸æ–¹ä¾¿ï¼Œæ‰€ä»¥æŒ‰ç…§å¹´ä»½ç»™å®ƒèšä¸ªç±»ï¼Œç”¨äº†å¦‚ä¸‹æ–¹æ³•ï¼š123456789hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \ -D mapred.reduce.tasks=0 \ -D mapred.map.tasks.speculative.execution=false \ -D mapred.task.timeout=12000000 \ -input ncdc_files.txt \ -inputformat org.apache.hadoop.mapred.lib.NLineInputFormat \ -output output \ -mapper load_ncdc_map.sh \ -file load_ncdc_map.shç„¶åŽé‡Œé¢ç”¨åˆ°çš„ncdc_filesä»¥åŠload_ncdc_map.shè¿™ä¸¤ä¸ªæ–‡ä»¶æ˜¯è¿™æ ·çš„ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100s3n://hadoopbook/ncdc/raw/isd-1901.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1902.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1903.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1904.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1905.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1906.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1907.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1908.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1909.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1910.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1911.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1912.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1913.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1914.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1915.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1916.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1917.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1918.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1919.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1920.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1921.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1922.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1923.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1924.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1925.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1926.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1927.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1928.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1929.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1930.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1931.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1932.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1933.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1934.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1935.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1936.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1937.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1938.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1939.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1940.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1941.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1942.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1943.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1944.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1945.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1946.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1947.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1948.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1949.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1950.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1951.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1952.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1953.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1954.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1955.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1956.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1957.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1958.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1959.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1960.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1961.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1962.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1963.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1964.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1965.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1966.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1967.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1968.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1969.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1970.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1971.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1972.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1973.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1974.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1975.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1976.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1977.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1978.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1979.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1980.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1981.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1982.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1983.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1984.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1985.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1986.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1987.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1988.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1989.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1990.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1991.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1992.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1993.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1994.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1995.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1996.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1997.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1998.tar.bz2s3n://hadoopbook/ncdc/raw/isd-1999.tar.bz2s3n://hadoopbook/ncdc/raw/isd-2000.tar.bz21234567891011121314151617181920212223242526#!/usr/bin/env bash# NLineInputFormat gives a single line: key is offset, value is S3 URIread offset s3file# Retrieve file from S3 to local diskecho "reporter:status:Retrieving $s3file" &gt;&amp;2$HADOOP_INSTALL/bin/hadoop fs -get $s3file .# Un-bzip and un-tar the local filetarget=`basename $s3file .tar.bz2`mkdir -p $targetecho "reporter:status:Un-tarring $s3file to $target" &gt;&amp;2tar jxf `basename $s3file` -C $target# Un-gzip each station file and concat into one fileecho "reporter:status:Un-gzipping $target" &gt;&amp;2for file in $target/*/*do gunzip -c $file &gt;&gt; $target.all echo "reporter:status:Processed $file" &gt;&amp;2done# Put gzipped version into HDFSecho "reporter:status:Gzipping $target and putting in HDFS" &gt;&amp;2gzip -c $target.all | $HADOOP_INSTALL/bin/hadoop fs -put - gz/$target.gzå—¯â€¦é¡ºä¾¿è¯´ä¸€å¥ï¼Œè¿™ä¸ªæ–‡ä»¶æ˜¯å­˜åœ¨AWSä¸Šçš„ï¼Œæ‰€ä»¥æƒ³ç”¨çš„è¯è¦æœ‰ä¸€ä¸ªAWSè´¦å·ï¼Œæƒ³è¦æœ‰ä¸ªè´¦å·å‘¢ï¼Œä½ å¾—å…ˆæœ‰ä¸ªå¯ä»¥æ”¯ä»˜ç¾Žåˆ€çš„ä¿¡ç”¨å¡ã€‚Hmmmmmâ€¦å…¶å®žä½œè€…ç»™çš„sample dataä¹ŸæŒºå¥½çš„æˆ‘è§‰å¾—ï¼Œåœ¨è¿™é‡Œ.é‚£ä¹ˆæˆ‘ä»¬çš„é—®é¢˜å°±æ˜¯è¯´ï¼Œæ‰¾å‡ºæ¯ä¸€å¹´çš„æœ€é«˜çš„æ¸©åº¦ã€‚å…ˆçœ‹çœ‹ä¸ç”¨Hadoopçš„å®žçŽ°æ–¹æ³•ï¼Œäº‹å®žè¯æ˜Žæˆ‘shellè„šæœ¬è¿˜æ˜¯å®åˆ€æœªè€çš„ã€‚12345678910#!/usr/bin/env bashfor year in all/*do echo -ne `basename $year .gz`"\t" gunzip -c $year | \ awk '&#123; temp = substr($0, 88, 5) + 0; q = substr($0, 93, 1); if (temp !=9999 &amp;&amp; q ~ /[01459]/ &amp;&amp; temp &gt; max) max = temp &#125; END &#123; print max &#125;'doneç»“æžœå¦‚ä¸‹ï¼š1234567% ./max_temperature.sh1901 3171902 2441903 2891904 2561905 283...å•Šå˜žï¼Œè¿˜ä¸é”™çš„æ ·å­ï¼Œä½†æ˜¯å¯¹äºŽå¤§æ•°æ®é€Ÿåº¦è¿˜æ˜¯æ…¢äº†ç‚¹å„¿ï¼Œæ‰€ä»¥ç›´æŽ¥ä¸ŠHadoopçœ‹çœ‹ã€‚å¯¹äºŽä»¥ä¸Šçš„é—®é¢˜å‘¢ï¼ŒMapReduceæ˜¯è¿™æ ·è§£å†³çš„æ³¨æ„äº†ï¼Œä¸Šé¢ä¸€è¡Œæ˜¯hadoopçš„æœ¯è¯­ï¼Œä¸‹é¢å‘¢ï¼Œå…¶å®žå°±æ˜¯Unixçš„pipeäº†ï¼Œè¿™ç»™æˆ‘ä»¬ä¸ç”¨Javaæ¥å®žçŽ°æä¾›äº†å¯èƒ½ã€‚å¥½äº†ä¸‹é¢å¼€å§‹codingäº†ï¼Œæ‹¿èµ·é”®ç›˜å°±æ˜¯GANä¸ºäº†å®žçŽ°æˆ‘ä»¬çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦ä¸‰ä¸ªjavaæ–‡ä»¶ï¼Œä¸€ä¸ªmapperï¼Œä¸€ä¸ªreducerã€‚è¿™ä¿©æ˜¯è‹¦å·¥ï¼Œè¿˜è¦ä¸€ä¸ªç›‘å·¥ã€‚Mapper123456789101112131415161718192021222324252627282930313233// cc MaxTemperatureMapper Mapper for maximum temperature example// vv MaxTemperatureMapperimport java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;public class MaxTemperatureMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private static final int MISSING = 9999; @Override public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String year = line.substring(15, 19); int airTemperature; if (line.charAt(87) == '+') &#123; // parseInt doesn't like leading plus signs airTemperature = Integer.parseInt(line.substring(88, 92)); &#125; else &#123; airTemperature = Integer.parseInt(line.substring(87, 92)); &#125; String quality = line.substring(92, 93); if (airTemperature != MISSING &amp;&amp; quality.matches("[01459]")) &#123; context.write(new Text(year), new IntWritable(airTemperature)); &#125; &#125;&#125;// ^^ MaxTemperatureMapperReducer123456789101112131415161718192021222324// cc MaxTemperatureReducer Reducer for maximum temperature example// vv MaxTemperatureReducerimport java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class MaxTemperatureReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int maxValue = Integer.MIN_VALUE; for (IntWritable value : values) &#123; maxValue = Math.max(maxValue, value.get()); &#125; context.write(key, new IntWritable(maxValue)); &#125;&#125;// ^^ MaxTemperatureReducerJob12345678910111213141516171819202122232425262728293031323334// cc MaxTemperature Application to find the maximum temperature in the weather dataset// vv MaxTemperatureimport org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MaxTemperature &#123; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("Usage: MaxTemperature &lt;input path&gt; &lt;output path&gt;"); System.exit(-1); &#125; Job job = new Job(); job.setJarByClass(MaxTemperature.class); job.setJobName("Max temperature"); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTemperatureMapper.class); job.setReducerClass(MaxTemperatureReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;// ^^ MaxTemperatureç„¶åŽè¿™ä¹ˆè¿è¡Œï¼š12% export HADOOP_CLASSPATH=hadoop-examples.jar% hadoop MaxTemperature input/ncdc/sample.txt outputä½†æ˜¯å¦‚æžœæ•°æ®é‡éžå¸¸å¤§çš„è¯ï¼Œéœ€è¦åœ¨Mapperå’ŒReducerä¹‹é—´ä¼ é€’å¤§é‡çš„æ•°æ®ï¼Œè¿™ä¸ªæ—¶å€™å¯ä»¥å¼•å…¥Combinerï¼Œå®ƒçš„æœºç†æ˜¯è¿™æ ·çš„ã€‚å‡å¦‚æˆ‘æœ‰ä¸¤ä¸ªmapperï¼Œå®ƒä»¬çš„è¾“å‡ºç»“æžœæ˜¯è¿™æ ·å­çš„ï¼š123(1950, 0)(1950, 20)(1950, 10)ä»¥åŠè¿™æ ·å­çš„ï¼š12(1950, 25)(1950, 15)å¦‚æžœæ²¡æœ‰combinerçš„è¯ï¼Œå®ƒä»¬ä¼šå…ˆå˜æˆè¿™æ ·å­ï¼š1(1950, [0, 20, 10, 25, 15])ç„¶åŽä½œä¸ºreducerçš„è¾“å…¥ï¼Œä½†æ˜¯å¦‚æžœåŠ å…¥äº†combinerçš„è¯ï¼Œç›¸å½“äºŽä¸Šé¢çš„é—®é¢˜å˜æˆäº†è¿™æ ·max(0, 20, 10, 25, 15) = max(max(0, 20, 10), max(25, 15)) = max(20, 25) = 25æ˜¯ä¸æ˜¯ç®€å•å¤šäº†ã€‚ä½†æ˜¯æ³¨æ„äº†ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„é—®é¢˜éƒ½æ˜¯è¿™æ ·ï¼Œæ¯”å¦‚ä¸‹é¢è¿™ä¸ªé—®é¢˜ï¼šmean(0, 20, 10, 25, 15) = 14mean(mean(0, 20, 10), mean(25, 15)) = mean(10, 20) = 15æ‰€ä»¥è¯´è¦æ ¹æ®å…·ä½“æƒ…å†µæ¥å®šï¼Œä¸èƒ½ç›´æŽ¥å¥—ç”¨ã€‚å¥½äº†æˆ‘ä»¬ç»§ç»­combinerçš„è¯é¢˜ï¼Œæˆ‘ä»¬æ€Žä¹ˆæŠŠè¿™è´§åŠ åˆ°hadoopçš„æµç¨‹ä¸­åŽ»å‘¢ï¼Œå…¶å®žå¾ˆç®€å•ï¼Œè¿™æ ·å°±å¯ä»¥ï¼š123456789101112131415161718192021222324252627282930313233343536// cc MaxTemperatureWithCombiner Application to find the maximum temperature, using a combiner function for efficiencyimport org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;// vv MaxTemperatureWithCombinerpublic class MaxTemperatureWithCombiner &#123; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.println("Usage: MaxTemperatureWithCombiner &lt;input path&gt; " + "&lt;output path&gt;"); System.exit(-1); &#125; Job job = new Job(); job.setJarByClass(MaxTemperatureWithCombiner.class); job.setJobName("Max temperature"); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(MaxTemperatureMapper.class); /*[*/job.setCombinerClass(MaxTemperatureReducer.class)/*]*/; job.setReducerClass(MaxTemperatureReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;// ^^ MaxTemperatureWithCombineræ²¡é”™ï¼Œcombinerå’Œreduceræ˜¯ä¸€æ ·çš„ã€‚å…¶å®žä»”ç»†æƒ³æƒ³è¿™ä¹Ÿå¾ˆè‡ªç„¶ï¼Œå› ä¸ºå®ƒä»¬ä¿©å®žé™…å®žçŽ°çš„åŠŸèƒ½æ˜¯ä¸€æ ·çš„ã€‚Hadoop Streamingä½œä¸ºä¸€ä¸ªmachine learningä¸“ä¸šçš„ï¼Œæœ‰æ—¶å€™ç”¨Javaè¿˜æ˜¯æ„Ÿè§‰æŒºä¸çˆ½çš„ï¼Œå“ªæœ‰Pythonå•Šï¼ŒRubyå•Šè¿™ç§è„šæœ¬è¯­è¨€æ–¹ä¾¿å˜›ã€‚æ‰€ä»¥hadoopè¿˜æ˜¯å¾ˆäººæ€§åœ°æä¾›äº†è§£å†³æ–¹æ³•ï¼Œå°±æ˜¯æ ‡é¢˜æ‰€è¡¨ç¤ºçš„æŠ€æœ¯ã€‚ç›´æŽ¥çœ‹ä»£ç æ€Žä¹ˆç”¨å§ã€‚RubyMap1234567#!/usr/bin/env rubySTDIN.each_line do |line| val = line year, temp, q = val[15,4], val[87,5], val[92,1] puts "#&#123;year&#125;\t#&#123;temp&#125;" if (temp != "+9999" &amp;&amp; q =~ /[01459]/)endReduce12345678910111213#!/usr/bin/env rubylast_key, max_val = nil, -1000000STDIN.each_line do |line| key, val = line.split("\t") if last_key &amp;&amp; last_key != key puts "#&#123;last_key&#125;\t#&#123;max_val&#125;" last_key, max_val = key, val.to_i else last_key, max_val = key, [max_val, val.to_i].max endendputs "#&#123;last_key&#125;\t#&#123;max_val&#125;" if last_keyç„¶åŽè¿™æ ·è°ƒç”¨ï¼š12345% hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \ -input input/ncdc/sample.txt \ -output output \ -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \ -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rbæ˜¯ä¸æ˜¯å¾ˆæ–¹ä¾¿ï¼Ÿå¦‚æžœè¦åŠ ä¸Šcombinerçš„è¯ï¼Œæ›´æ–¹ä¾¿äº†ï¼Œéƒ½ä¸ç”¨å†å†™é¢å¤–çš„æ–‡ä»¶ï¼š12345678% hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \ -files ch02-mr-intro/src/main/ruby/max_temperature_map.rb,\ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \ -input input/ncdc/all \ -output output \ -mapper ch02-mr-intro/src/main/ruby/max_temperature_map.rb \ -combiner ch02-mr-intro/src/main/ruby/max_temperature_reduce.rb \ -reducer ch02-mr-intro/src/main/ruby/max_temperature_reduce.rbæ³¨æ„ï¼Œä»¥ä¸Šçš„-fileså‘½ä»¤æ˜¯ä¸ºäº†åœ¨é›†ç¾¤çŽ¯å¢ƒä¸‹è¿è¡Œæ—¶ï¼Œå°†è„šæœ¬å¤åˆ¶åˆ°å„å­èŠ‚ç‚¹ä¸Šã€‚Pythonå•Šï¼ŒPythonå¤§å¤§å‡ºåœºï¼Œå…¶å®žå’ŒRubyæ²¡å•¥åŒºåˆ«ã€‚Map12345678910#!/usr/bin/env pythonimport reimport sysfor line in sys.stdin: val = line.strip() (year, temp, q) = (val[15:19], val[87:92], val[92:93]) if (temp != "+9999" and re.match("[01459]", q)): print "%s\t%s" % (year, temp)Reduce123456789101112131415#!/usr/bin/env pythonimport sys(last_key, max_val) = (None, -sys.maxint)for line in sys.stdin: (key, val) = line.strip().split("\t") if last_key and last_key != key: print "%s\t%s" % (last_key, max_val) (last_key, max_val) = (key, int(val)) else: (last_key, max_val) = (key, max(max_val, int(val)))if last_key: print "%s\t%s" % (last_key, max_val)è¿è¡Œéƒ½æ˜¯ä¸€æ ·çš„ï¼Œå°±ä¸å¤šåšèµ˜è¿°äº†ã€‚]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine Learning [ECNU] Assignment 1]]></title>
      <url>%2F2017%2F04%2F16%2FMachine-Learning-ECNU-Assignment-1%2F</url>
      <content type="text"><![CDATA[Use a crawler to get at least 20 webpages from a website.Count theoccurrences of words in the webpages on Hadoop.Hand in:Each one should crawl different websites, list the website URL, as well as the URLsof the crawled webpages.Count the word occurrence on Hadoop, code in both JAVA and another language such asPig Latin. print out your code.Print out your result.Home work due: 4/12You are allowed toform a group of no more than 4 fellow students.https://github.com/ewanlee/machine-learning-ECNU-/blob/master/Hadoop%20wordcount%20demo_cutted.pdf]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignments [2 & 3]]]></title>
      <url>%2F2017%2F04%2F16%2Fcs231n-Assignments-2-3%2F</url>
      <content type="text"><![CDATA[Assignment 2In this assignment you will practice writing backpropagation code, and training Neural Networks and Convolutional Neural Networks. The goals of this assignment are as follows:understand Neural Networks and how they are arranged in layered architecturesunderstand and be able to implement (vectorized) backpropagationimplement various update rules used to optimize Neural Networksimplement batch normalization for training deep networksimplement dropout to regularize networkseffectively cross-validate and find the best hyperparameters for Neural Network architectureunderstand the architecture of Convolutional Neural Networks and train gain experience with training these models on dataSetupYou can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine through Terminal.com.Working in the cloud on TerminalTerminal has created a separate subdomain to serve our class, www.stanfordterminalcloud.com. Register your account there. The Assignment 2 snapshot can then be found HERE. If you are registered in the class you can contact the TA (see Piazza for more information) to request Terminal credits for use on the assignment. Once you boot up the snapshot everything will be installed for you, and you will be ready to start on your assignment right away. We have written a small tutorial on Terminal here.Working locallyGet the code as a zip file here. As for the dependencies:[Option 1] Use Anaconda: The preferred approach for installing all the assignment dependencies is to useAnaconda, which is a Python distribution that includes many of the most popular Python packages for science, math, engineering and data analysis. Once you install it you can skip all mentions of requirements and you are ready to go directly to working on the assignment.[Option 2] Manual install, virtual environment: If you do not want to use Anaconda and want to go with a more manual and risky installation route you will likely want to create a virtual environment for the project. If you choose not to use a virtual environment, it is up to you to make sure that all dependencies for the code are installed globally on your machine. To set up a virtual environment, run the following:1234567cd assignment2sudo pip install virtualenv # This may already be installedvirtualenv .env # Create a virtual environmentsource .env/bin/activate # Activate the virtual environmentpip install -r requirements.txt # Install dependencies# Work on the assignment for a while ...deactivate # Exit the virtual environmentDownload data: Once you have the starter code, you will need to download the CIFAR-10 dataset. Run the following from the assignment2 directory:12cd cs231n/datasets./get_datasets.shCompile the Cython extension: Convolutional Neural Networks require a very efficient implementation. We have implemented of the functionality using Cython; you will need to compile the Cython extension before you can run the code. From the cs231n directory, run the following command:1python setup.py build_ext --inplaceStart IPython: After you have the CIFAR-10 data, you should start the IPython notebook server from the assignment2 directory. If you are unfamiliar with IPython, you should read our IPython tutorial.NOTE: If you are working in a virtual environment on OSX, you may encounter errors with matplotlib due to theissues described here. You can work around this issue by starting the IPython server using thestart_ipython_osx.sh script from the assignment2 directory; the script assumes that your virtual environment is named .env.Submitting your work:Whether you work on the assignment locally or using Terminal, once you are done working run the collectSubmission.sh script; this will produce a file called assignment2.zip. Upload this file under the Assignments tab on the coursework page for the course.Q1: Fully-connected Neural Network (30 points)The IPython notebook FullyConnectedNets.ipynb will introduce you to our modular layer design, and then use those layers to implement fully-connected networks of arbitrary depth. To optimize these models you will implement several popular update rules.Q2: Batch Normalization (30 points)In the IPython notebook BatchNormalization.ipynb you will implement batch normalization, and use it to train deep fully-connected networks.Q3: Dropout (10 points)The IPython notebook Dropout.ipynb will help you implement Dropout and explore its effects on model generalization.Q4: ConvNet on CIFAR-10 (30 points)In the IPython Notebook ConvolutionalNetworks.ipynb you will implement several new layers that are commonly used in convolutional networks. You will train a (shallow) convolutional network on CIFAR-10, and it will then be up to you to train the best network that you can.Q5: Do something extra! (up to +10 points)In the process of training your network, you should feel free to implement anything that you want to get better performance. You can modify the solver, implement additional layers, use different types of regularization, use an ensemble of models, or anything else that comes to mind. If you implement these or other ideas not covered in the assignment then you will be awarded some bonus points.](https://github.com/ewanlee/cs231n/tree/master/cs231n-assignments/assignmentAssignment 3In this assignment you will implement recurrent networks, and apply them to image captioning on Microsoft COCO. We will also introduce the TinyImageNet dataset, and use a pretrained model on this dataset to explore different applications of image gradients.The goals of this assignment are as follows:Understand the architecture of recurrent neural networks (RNNs) and how they operate on sequences by sharing weights over timeUnderstand the difference between vanilla RNNs and Long-Short Term Memory (LSTM) RNNsUnderstand how to sample from an RNN at test-timeUnderstand how to combine convolutional neural nets and recurrent nets to implement an image captioning systemUnderstand how a trained convolutional network can be used to compute gradients with respect to the input imageImplement and different applications of image gradients, including saliency maps, fooling images, class visualizations, feature inversion, and DeepDream.SetupYou can work on the assignment in one of two ways: locally on your own machine, or on a virtual machine through Terminal.com.Working in the cloud on TerminalTerminal has created a separate subdomain to serve our class, www.stanfordterminalcloud.com. Register your account there. The Assignment 3 snapshot can then be found HERE. If you are registered in the class you can contact the TA (see Piazza for more information) to request Terminal credits for use on the assignment. Once you boot up the snapshot everything will be installed for you, and you will be ready to start on your assignment right away. We have written a small tutorial on Terminal here.Working locallyGet the code as a zip file here. As for the dependencies:[Option 1] Use Anaconda: The preferred approach for installing all the assignment dependencies is to useAnaconda, which is a Python distribution that includes many of the most popular Python packages for science, math, engineering and data analysis. Once you install it you can skip all mentions of requirements and you are ready to go directly to working on the assignment.[Option 2] Manual install, virtual environment: If you do not want to use Anaconda and want to go with a more manual and risky installation route you will likely want to create a virtual environment for the project. If you choose not to use a virtual environment, it is up to you to make sure that all dependencies for the code are installed globally on your machine. To set up a virtual environment, run the following:1234567cd assignment3sudo pip install virtualenv # This may already be installedvirtualenv .env # Create a virtual environmentsource .env/bin/activate # Activate the virtual environmentpip install -r requirements.txt # Install dependencies# Work on the assignment for a while ...deactivate # Exit the virtual environmentDownload data: Once you have the starter code, you will need to download the processed MS-COCO dataset, the TinyImageNet dataset, and the pretrained TinyImageNet model. Run the following from the assignment3directory:1234cd cs231n/datasets./get_coco_captioning.sh./get_tiny_imagenet_a.sh./get_pretrained_model.shCompile the Cython extension: Convolutional Neural Networks require a very efficient implementation. We have implemented of the functionality using Cython; you will need to compile the Cython extension before you can run the code. From the cs231n directory, run the following command:1python setup.py build_ext --inplaceStart IPython: After you have the data, you should start the IPython notebook server from the assignment3directory. If you are unfamiliar with IPython, you should read our IPython tutorial.NOTE: If you are working in a virtual environment on OSX, you may encounter errors with matplotlib due to theissues described here. You can work around this issue by starting the IPython server using thestart_ipython_osx.sh script from the assignment3 directory; the script assumes that your virtual environment is named .env.Submitting your work:Whether you work on the assignment locally or using Terminal, once you are done working run the collectSubmission.sh script; this will produce a file called assignment3.zip. Upload this file under the Assignments tab on the coursework page for the course.Q1: Image Captioning with Vanilla RNNs (40 points)The IPython notebook RNN_Captioning.ipynb will walk you through the implementation of an image captioning system on MS-COCO using vanilla recurrent networks.Q2: Image Captioning with LSTMs (35 points)The IPython notebook LSTM_Captioning.ipynbwill walk you through the implementation of Long-Short Term Memory (LSTM) RNNs, and apply them to image captioning on MS-COCO.Q3: Image Gradients: Saliency maps and Fooling Images (10 points)The IPython notebook ImageGradients.ipynb will introduce the TinyImageNet dataset. You will use a pretrained model on this dataset to compute gradients with respect to the image, and use them to produce saliency maps and fooling images.Q4: Image Generation: Classes, Inversion, DeepDream (15 points)In the IPython notebook ImageGeneration.ipynb you will use the pretrained TinyImageNet model to generate images. In particular you will generate class visualizations and implement feature inversion and DeepDream.Q5: Do something extra! (up to +10 points)Given the components of the assignment, try to do something cool. Maybe there is some way to generate images that we did not implement in the assignment?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Software Packages notes]]></title>
      <url>%2F2017%2F04%2F13%2Fcs231n-Software-Packages-notes%2F</url>
      <content type="text"><![CDATA[Software PackagesCaffehttp://caffe.berkeleyvision.orgOverviewFrom U.C. BerkeleyWritten in C++Has Python and Matlab bindingsGood for training or finetuning feedforward modelsTipDonâ€™t be afraid to read the code!Main classesBlob: Stores data and derivativesLayer: Transforms bottom blobs to top blobsNet:Many layersComputes gradients via forward / backwardSolver: Uses gradients to update weightsProtocol Buffersâ€œTyped JSONâ€ from GoogleDefine â€œmessage typesâ€ in .proto files12345message Person &#123; required string name = 1; required int32 id = 2; optional string email = 3;&#125;Serialize instances to text files (.prototxt)123name: "John Doe"id: 1234email: "jdoe@example.com"Compile classes for different languagesTraining / FinetuningConvert data (run a script)Define net (edit prototxt)Define solver (edit prototxt)Train (with pretrained weights) (run a script)Step1: Convert DataDataLayer reading from LMDB is the easiestCreate LMDB using convert_imagesetNeed text file where each line isâ€œ[path/to/image.jpeg][label]â€Create HDF5 file yourself using h5py[extras] some methods:ImageDataLayer: Read from image filesWindowDataLayer: For detectionHDF5Layer: Read from HDF5 fileFrom memory, using Python interfaceAll of these are harder to use (except Python)Step2: Define Net123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127name: "ResNet-152"input: "data"input_dim: 1input_dim: 3input_dim: 224input_dim: 224layer &#123; bottom: "data" top: "conv1" name: "conv1" type: "Convolution" convolution_param &#123; num_output: 64 kernel_size: 7 pad: 3 stride: 2 bias_term: false &#125;&#125;layer &#123; bottom: "conv1" top: "conv1" name: "bn_conv1" type: "BatchNorm" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: "conv1" top: "conv1" name: "scale_conv1" type: "Scale" scale_param &#123; bias_term: true &#125;&#125;layer &#123; top: "conv1" bottom: "conv1" name: "conv1_relu" type: "ReLU"&#125;layer &#123; bottom: "conv1" top: "pool1" name: "pool1" type: "Pooling" pooling_param &#123; kernel_size: 3 stride: 2 pool: MAX &#125;&#125;layer &#123; bottom: "pool1" top: "res2a_branch1" name: "res2a_branch1" type: "Convolution" convolution_param &#123; num_output: 256 kernel_size: 1 pad: 0 stride: 1 bias_term: false &#125;&#125;layer &#123; bottom: "res2a_branch1" top: "res2a_branch1" name: "bn2a_branch1" type: "BatchNorm" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: "res2a_branch1" top: "res2a_branch1" name: "scale2a_branch1" type: "Scale" scale_param &#123; bias_term: true &#125;&#125;layer &#123; bottom: "pool1" top: "res2a_branch2a" name: "res2a_branch2a" type: "Convolution" convolution_param &#123; num_output: 64 kernel_size: 1 pad: 0 stride: 1 bias_term: false &#125;&#125;layer &#123; bottom: "res2a_branch2a" top: "res2a_branch2a" name: "bn2a_branch2a" type: "BatchNorm" batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: "res2a_branch2a" top: "res2a_branch2a" name: "scale2a_branch2a" type: "Scale" scale_param &#123; bias_term: true &#125;&#125;.prototxt can get ugly for big modelsResNet-152 prototxt is 6775 lines long!Not â€œcompositionalâ€; canâ€™t easily define a residual block and reuseStep2: Define Net (finetuning)Same name: weights copiedDifferent name: weights reinitializedStep3: Define SolverWrite a prototxt file defining a SolverParameter123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166message SolverParameter &#123; ////////////////////////////////////////////////////////////////////////////// // Specifying the train and test networks // // Exactly one train net must be specified using one of the following fields: // train_net_param, train_net, net_param, net // One or more test nets may be specified using any of the following fields: // test_net_param, test_net, net_param, net // If more than one test net field is specified (e.g., both net and // test_net are specified), they will be evaluated in the field order given // above: (1) test_net_param, (2) test_net, (3) net_param/net. // A test_iter must be specified for each test_net. // A test_level and/or a test_stage may also be specified for each test_net. ////////////////////////////////////////////////////////////////////////////// // Proto filename for the train net, possibly combined with one or more // test nets. optional string net = 24; // Inline train net param, possibly combined with one or more test nets. optional NetParameter net_param = 25; optional string train_net = 1; // Proto filename for the train net. repeated string test_net = 2; // Proto filenames for the test nets. optional NetParameter train_net_param = 21; // Inline train net params. repeated NetParameter test_net_param = 22; // Inline test net params. // The states for the train/test nets. Must be unspecified or // specified once per net. // // By default, all states will have solver = true; // train_state will have phase = TRAIN, // and all test_state's will have phase = TEST. // Other defaults are set according to the NetState defaults. optional NetState train_state = 26; repeated NetState test_state = 27; // The number of iterations for each test net. repeated int32 test_iter = 3; // The number of iterations between two testing phases. optional int32 test_interval = 4 [default = 0]; optional bool test_compute_loss = 19 [default = false]; // If true, run an initial test pass before the first iteration, // ensuring memory availability and printing the starting value of the loss. optional bool test_initialization = 32 [default = true]; optional float base_lr = 5; // The base learning rate // the number of iterations between displaying info. If display = 0, no info // will be displayed. optional int32 display = 6; // Display the loss averaged over the last average_loss iterations optional int32 average_loss = 33 [default = 1]; optional int32 max_iter = 7; // the maximum number of iterations optional string lr_policy = 8; // The learning rate decay policy. optional float gamma = 9; // The parameter to compute the learning rate. optional float power = 10; // The parameter to compute the learning rate. optional float momentum = 11; // The momentum value. optional float weight_decay = 12; // The weight decay. // regularization types supported: L1 and L2 // controlled by weight_decay optional string regularization_type = 29 [default = "L2"]; // the stepsize for learning rate policy "step" optional int32 stepsize = 13; // the stepsize for learning rate policy "multistep" repeated int32 stepvalue = 34; // Set clip_gradients to &gt;= 0 to clip parameter gradients to that L2 norm, // whenever their actual L2 norm is larger. optional float clip_gradients = 35 [default = -1]; optional int32 snapshot = 14 [default = 0]; // The snapshot interval optional string snapshot_prefix = 15; // The prefix for the snapshot. // whether to snapshot diff in the results or not. Snapshotting diff will help // debugging but the final protocol buffer size will be much larger. optional bool snapshot_diff = 16 [default = false]; // the mode solver will use: 0 for CPU and 1 for GPU. Use GPU in default. enum SolverMode &#123; CPU = 0; GPU = 1; &#125; optional SolverMode solver_mode = 17 [default = GPU]; // the device_id will that be used in GPU mode. Use device_id = 0 in default. optional int32 device_id = 18 [default = 0]; // If non-negative, the seed with which the Solver will initialize the Caffe // random number generator -- useful for reproducible results. Otherwise, // (and by default) initialize using a seed derived from the system clock. optional int64 random_seed = 20 [default = -1]; // Solver type enum SolverType &#123; SGD = 0; NESTEROV = 1; ADAGRAD = 2; &#125; optional SolverType solver_type = 30 [default = SGD]; // numerical stability for AdaGrad optional float delta = 31 [default = 1e-8]; // If true, print information about the state of the net that may help with // debugging learning problems. optional bool debug_info = 23 [default = false]; // If false, don't save a snapshot after training finishes. optional bool snapshot_after_train = 28 [default = true];&#125;// A message that stores the solver snapshotsmessage SolverState &#123; optional int32 iter = 1; // The current iteration optional string learned_net = 2; // The file that stores the learned net. repeated BlobProto history = 3; // The history for sgd solvers optional int32 current_step = 4 [default = 0]; // The current step for learning rate&#125;enum Phase &#123; TRAIN = 0; TEST = 1;&#125;message NetState &#123; optional Phase phase = 1 [default = TEST]; optional int32 level = 2 [default = 0]; repeated string stage = 3;&#125;message NetStateRule &#123; // Set phase to require the NetState have a particular phase (TRAIN or TEST) // to meet this rule. optional Phase phase = 1; // Set the minimum and/or maximum levels in which the layer should be used. // Leave undefined to meet the rule regardless of level. optional int32 min_level = 2; optional int32 max_level = 3; // Customizable sets of stages to include or exclude. // The net must have ALL of the specified stages and NONE of the specified // "not_stage"s to meet the rule. // (Use multiple NetStateRules to specify conjunctions of stages.) repeated string stage = 4; repeated string not_stage = 5;&#125;// Specifies training parameters (multipliers on global learning constants,// and the name and other settings used for weight sharing).message ParamSpec &#123; // The names of the parameter blobs -- useful for sharing parameters among // layers, but never required otherwise. To share a parameter between two // layers, give it a (non-empty) name. optional string name = 1; // Whether to require shared weights to have the same shape, or just the same // count -- defaults to STRICT if unspecified. optional DimCheckMode share_mode = 2; enum DimCheckMode &#123; // STRICT (default) requires that num, channels, height, width each match. STRICT = 0; // PERMISSIVE requires only the count (num*channels*height*width) to match. PERMISSIVE = 1; &#125; // The multiplier on the global learning rate for this parameter. optional float lr_mult = 3 [default = 1.0]; // The multiplier on the global weight decay for this parameter. optional float decay_mult = 4 [default = 1.0];&#125;If finetuning, copy existing solver.prototxt fileChange net to be your netChange snapshot_prefix to your outputReduce base learning rate (divide by 100)Maybe change max_iter and snapshotStep 4: Train12345678./build/tools/caffe train \ -gpu 0 \ -model path/to/trainval.prototxt \ -solver path/to/solver.prototxt \ -weights path/to/pretrained_weights.caffemodel # -gpu -1 for CPU mode # -gpu all for multi-GPU data parallelismModel Zoohttps://github.com/BVLC/caffe/wiki/Model-ZooPython InterfaceRead the code! Two most important files:caffe/python/caffe/_caffe.cppExports Blob, Layer, Net, and Solver classescaffe/python/caffe/pycaffe.pyAdds extra methods to Net classGood for:Interfacing with numpyExtract features: Run net forwardCompute gradients: Run net backward (DeepDream, etc)Define layers in Python with numpy (CPU only)Pros / Cons(+) Good for feedforward networks(+) Good for finetuning existing networks(+) Train models without writing any code!(+) Python interface is pretty useful!(-) Need to write C++ / CUDA for new GPU layers(-) Not good for recurrent networks(-) Cumbersome for big networks (GoogLeNet, ResNet)Torchhttp://torch.chOverviewFrom NYU + IDIAPWritten in C and LuaUsed a lot a Facebook, DeepMindLuaLearn Lua in 15 MinutesHigh level scripting language, easy to interface with CSimilar to Javascript:One data structure: table == JS objectPrototypical inheritance: metatable == JS prototypeFirst-class functionsSome gotchas:1-indexed =(Variables global by default =(Small standard libraryTensorTorch tensors are just like numpy arraysDocumentation on GitHub:https://github.com/torch/torch7/blob/master/doc/tensor.mdhttps://github.com/torch/torch7/blob/master/doc/maths.mdnnnn module lets you easily build and train neural nets123456789101112131415-- our optimization procedure will iterate over the modules, so only share-- the parametersmlp = nn.Sequential()linear = nn.Linear(2,2)linear_clone = linear:clone('weight','bias') -- clone sharing the parametersmlp:add(linear)mlp:add(linear_clone)function gradUpdate(mlp, x, y, criterion, learningRate) local pred = mlp:forward(x) local err = criterion:forward(pred, y) local gradCriterion = criterion:backward(pred, y) mlp:zeroGradParameters() mlp:backward(x, gradCriterion) mlp:updateParameters(learningRate)end12345678910111213141516171819-- our optimization procedure will use all the parameters at once, because-- it requires the flattened parameters and gradParameters Tensors. Thus,-- we need to share both the parameters and the gradParametersmlp = nn.Sequential()linear = nn.Linear(2,2)-- need to share the parameters and the gradParameters as welllinear_clone = linear:clone('weight','bias','gradWeight','gradBias')mlp:add(linear)mlp:add(linear_clone)params, gradParams = mlp:getParameters()function gradUpdate(mlp, x, y, criterion, learningRate, params, gradParams) local pred = mlp:forward(x) local err = criterion:forward(pred, y) local gradCriterion = criterion:backward(pred, y) mlp:zeroGradParameters() mlp:backward(x, gradCriterion) -- adds the gradients to all the parameters at once params:add(-learningRate, gradParams)endcunnRunning on GPU is easy123456789101112local model = nn.Sequential()model:add(nn.Linear(2,2))model:add(nn.LogSoftMax())model:cuda() -- convert model to CUDAlocal input = torch.Tensor(32,2):uniform()input = input:cuda()local output = model:forward(input)local input = torch.CudaTensor(32,2):uniform()local output = model:forward(input)optimoptim package implements different update rules: momentum, Adam, etc123456789101112131415161718192021require 'optim'for epoch = 1, 50 do -- local function we give to optim -- it takes current weights as input, and outputs the loss -- and the gradient of the loss with respect to the weights -- gradParams is calculated implicitly by calling 'backward', -- because the model's weight and bias gradient tensors -- are simply views onto gradParams function feval(params) gradParams:zero() local outputs = model:forward(batchInputs) local loss = criterion:forward(outputs, batchLabels) local dloss_doutputs = criterion:backward(outputs, batchLabels) model:backward(batchInputs, dloss_doutputs) return loss, gradParams end optim.sgd(feval, params, optimState)endModulesCaffe has Nets and Layers; Torch just has ModulesModules are classes written in Lua; easy to read and writeForward / backward written in Lua using Tensor methodsSame code runs on CPU / GPU123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122local Linear, parent = torch.class('nn.Linear', 'nn.Module')function Linear:__init(inputSize, outputSize, bias) parent.__init(self) local bias = ((bias == nil) and true) or bias self.weight = torch.Tensor(outputSize, inputSize) self.gradWeight = torch.Tensor(outputSize, inputSize) if bias then self.bias = torch.Tensor(outputSize) self.gradBias = torch.Tensor(outputSize) end self:reset()endfunction Linear:noBias() self.bias = nil self.gradBias = nil return selfendfunction Linear:reset(stdv) if stdv then stdv = stdv * math.sqrt(3) else stdv = 1./math.sqrt(self.weight:size(2)) end if nn.oldSeed then for i=1,self.weight:size(1) do self.weight:select(1, i):apply(function() return torch.uniform(-stdv, stdv) end) end if self.bias then for i=1,self.bias:nElement() do self.bias[i] = torch.uniform(-stdv, stdv) end end else self.weight:uniform(-stdv, stdv) if self.bias then self.bias:uniform(-stdv, stdv) end end return selfendlocal function updateAddBuffer(self, input) local nframe = input:size(1) self.addBuffer = self.addBuffer or input.new() if self.addBuffer:nElement() ~= nframe then self.addBuffer:resize(nframe):fill(1) endendfunction Linear:updateOutput(input) if input:dim() == 1 then self.output:resize(self.weight:size(1)) if self.bias then self.output:copy(self.bias) else self.output:zero() end self.output:addmv(1, self.weight, input) elseif input:dim() == 2 then local nframe = input:size(1) local nElement = self.output:nElement() self.output:resize(nframe, self.weight:size(1)) if self.output:nElement() ~= nElement then self.output:zero() end updateAddBuffer(self, input) self.output:addmm(0, self.output, 1, input, self.weight:t()) if self.bias then self.output:addr(1, self.addBuffer, self.bias) end else error('input must be vector or matrix') end return self.outputendfunction Linear:updateGradInput(input, gradOutput) if self.gradInput then local nElement = self.gradInput:nElement() self.gradInput:resizeAs(input) if self.gradInput:nElement() ~= nElement then self.gradInput:zero() end if input:dim() == 1 then self.gradInput:addmv(0, 1, self.weight:t(), gradOutput) elseif input:dim() == 2 then self.gradInput:addmm(0, 1, gradOutput, self.weight) end return self.gradInput endendfunction Linear:accGradParameters(input, gradOutput, scale) scale = scale or 1 if input:dim() == 1 then self.gradWeight:addr(scale, gradOutput, input) if self.bias then self.gradBias:add(scale, gradOutput) end elseif input:dim() == 2 then self.gradWeight:addmm(scale, gradOutput:t(), input) if self.bias then -- update the size of addBuffer if the input is not the same size as the one we had in last updateGradInput updateAddBuffer(self, input) self.gradBias:addmv(scale, gradOutput:t(), self.addBuffer) end endendfunction Linear:sharedAccUpdateGradParameters(input, gradOutput, lr) -- we do not need to accumulate parameters when sharing: self:defaultAccUpdateGradParameters(input, gradOutput, lr)endfunction Linear:clearState() if self.addBuffer then self.addBuffer:set() end return parent.clearState(self)endfunction Linear:__tostring__() return torch.type(self) .. string.format('(%d -&gt; %d)', self.weight:size(2), self.weight:size(1)) .. (self.bias == nil and ' without bias' or '')endTons of built-in modules and loss functionshttps://github.com/torch/nnContainerContainer modules allow you to combine multiple modulesnngraphA multi-layer network where each layer takes output of previous two layers as input.1234567891011121314input = nn.Identity()()L1 = nn.Tanh()(nn.Linear(10, 20)(input))L2 = nn.Tanh()(nn.Linear(30, 60)(nn.JoinTable(1)(&#123;input, L1&#125;)))L3 = nn.Tanh()(nn.Linear(80, 160)(nn.JoinTable(1)(&#123;L1, L2&#125;)))g = nn.gModule(&#123;input&#125;, &#123;L3&#125;)indata = torch.rand(10)gdata = torch.rand(160)g:forward(indata)g:backward(indata, gdata)graph.dot(g.fg, 'Forward Graph')graph.dot(g.bg, 'Backward Graph')More InfoPretrained Modelsloadcaffe: Load pretrained Caffe models: AlexNet, VGG, some othershttps://github.com/szagoruyko/loadcaffeGoogLeNet v1: https://github.com/soumith/inception.torchGoogLeNet v3: https://github.com/Moodstocks/inception-v3.torchResNet: https://github.com/facebook/fb.resnet.torchPackage ManagementAfter installing torch, use luarocks to install or update Lua packages(Similar to pip install from Python)Other useful packagestorch.cudnn: Bindings for NVIDIA cuDNN kernelshttps://github.com/soumith/cudnn.torchtorch-hdf5: Read and write HDF5 files from Torchhttps://github.com/deepmind/torch-hdf5lua-cjson: Read and write JSON files from Luahttps://luarocks.org/modules/luarocks/lua-cjsoncltorch, clnn: OpenCL backend for Torch, and port of nnhttps://github.com/hughperkins/cltorch, https://github.com/hughperkins/clnntorch-autograd: Automatic differentiation; sort of like more powerful nngraph, similar to Theano or TensorFlowhttps://github.com/twitter/torch-autogradfbcunn: Facebook: FFT conv, multi-GPU (DataParallel, ModelParallel)https://github.com/facebook/fbcunnTypical WorkflowPreprocess data; usually use a Python script to dump data to HDF5Train a model in Lua / Torch; read from HDF5 datafile, save trained model to diskUse trained model for something, often with an evaluation scriptExample: https://github.com/jcjohnson/torch-rnnStep 1: Preprocess data; usually use a Python script to dump data to HDF5 (https://github.com/jcjohnson/torch-rnn/blob/master/scripts/preprocess.py)Step 2: Train a model in Lua / Torch; read from HDF5 datafile, save trained model to disk (https://github.com/jcjohnson/torch-rnn/blob/master/train.lua )Step 3: Use trained model for something, often with an evaluation script (https://github.com/jcjohnson/torch-rnn/blob/master/sample.lua)Pros / Cons(-) Lua(-) Less plug-and-play than CaffeYou usually write your own training code(+) Lots of modular pieces that are easy to combine(+) Easy to write your own layer types and run on GPU(+) Most of the library code is in Lua, easy to read(+) Lots of pretrained models!(-) Not great for RNNsTheanohttp://deeplearning.net/software/theano/OverviewFrom Yoshua Bengioâ€™s group at University of MontrealEmbracing computation graphs, symbolic computationHigh-level wrappers: Keras, LasagneOther TopicsConditionals: The ifelse and switch functions allow conditional control flow in the graphLoops: The scan function allows for (some types) of loops in the computational graph; good for RNNsDerivatives: Efficient Jacobian / vector products with R and L operators, symbolic hessians (gradient of gradient)Sparse matrices, optimizations, etcMulti-GPUExperimental model parallelism:http://deeplearning.net/software/theano/tutorial/using_multi_gpu.htmlData parallelism using platoon:https://github.com/mila-udem/platoonHigh level wrapperLasagneKerasPretrained ModelsLasagne Model Zoo has pretrained common architectures:https://github.com/Lasagne/Recipes/tree/master/modelzooAlexNet with weights: https://github.com/uoguelph-mlrg/theano_alexnetsklearn-theano: Run OverFeat and GoogLeNet forward, but no fine-tuning? http://sklearn-theano.github.iocaffe-theano-conversion: CS 231n project from last year: load models and weights from caffe! Not sure if full-featured https://github.com/kitofans/caffe-theano-conversionPros / Cons(+) Python + numpy(+) Computational graph is nice abstraction(+) RNNs fit nicely in computational graph(-) Raw Theano is somewhat low-level(+) High level wrappers (Keras, Lasagne) ease the pain(-) Error messages can be unhelpful(-) Large models can have long compile times(-) Much â€œfatterâ€ than Torch; more magic(-) Patchy support for pretrained modelsTensorFlowhttps://www.tensorflow.orgOverviewFrom GoogleVery similar to Theano - all about computation graphsEasy visualizations (TensorBoard)Multi-GPU and multi-node trainingTensorboardTensorboard makes it easy to visualize whatâ€™s happening inside your modelsMulti-GPUDistributedPretrained ModelsYou can get a pretrained version of Inception here:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md(In an Android example?? Very well-hidden)The only one I could find =(Pros / Cons(+) Python + numpy(+) Computational graph abstraction, like Theano; great for RNNs(+) Much faster compile times than Theano(+) Slightly more convenient than raw Theano?(+) TensorBoard for visualization(+) Data AND model parallelism; best of all frameworks(+/-) Distributed models, but not open-source yet(-) Slower than other frameworks right now(-) Much â€œfatterâ€ than Torch; more magic(-) Not many pretrained modelsUse CasesExtract AlexNet or VGG features? Use CaffeFine-tune AlexNet for new classes? Use CaffeImage Captioning with finetuning?-&gt; Need pretrained models (Caffe, Torch, Lasagne)-&gt; Need RNNs (Torch or Lasagne)-&gt; Use Torch or LasagnaSegmentation? (Classify every pixel)-&gt; Need pretrained model (Caffe, Torch, Lasagna)-&gt; Need funny loss function-&gt; If loss function exists in Caffe: Use Caffe-&gt; If you want to write your own loss: Use TorchObject Detection?-&gt; Need pretrained model (Torch, Caffe, Lasagne)-&gt; Need lots of custom imperative code (NOT Lasagne)-&gt; Use Caffe + Python or TorchLanguage modeling with new RNN structure?-&gt; Need easy recurrent nets (NOT Caffe, Torch)-&gt; No need for pretrained models-&gt; Use Theano or TensorFlowImplement BatchNorm?-&gt; Donâ€™t want to derive gradient? Theano or TensorFlow-&gt; Implement efficient backward pass? Use TorchRecommendation:Feature extraction / finetuning existing models: Use CaffeComplex uses of pretrained models: Use Lasagne or TorchWrite your own layers: Use TorchCrazy RNNs: Use Theano or TensorflowHuge model, need model parallelism: Use TensorFlow]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[usr/bin/ld: cannot find -lxxx Solutions]]></title>
      <url>%2F2017%2F04%2F11%2Fusr-bin-ld-cannot-find-lxxx-Solutions%2F</url>
      <content type="text"><![CDATA[åœ¨Ubuntuä¸Šè¿è¡ŒQt5çš„è¿‡ç¨‹ä¸­æŠ¥é”™ï¼š1usr/bin/ld: cannot find -lGLæœ€åŽå‘çŽ°é—®é¢˜æ˜¯ç³»ç»Ÿä¸­æ²¡æœ‰å¯¹åº”çš„åº“æ–‡ä»¶ libgl.soé‚£ä¹ˆè§£å†³æ–¹å¼ä¹Ÿå¾ˆç®€å•ï¼Œå®‰è£…å³å¯ï¼š1sudo apt-get install libgl-devTada =)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Reinforcement Learning (GT) Notes]]></title>
      <url>%2F2017%2F04%2F10%2FReinforcement-Learning-GT-Notes%2F</url>
      <content type="text"><![CDATA[Decision Making &amp; Reinforcement LearningSupervised Learning: $y = f(x)$Unsupervised Learning: $f(x)$Reinforcement Learning: $y = f(x), z$Markov Decision ProcessStates: $S$Model: $T(s, a, s^{\prime}) \sim Pr(s^{\prime} | s, a)$Actions: $A(s), A$Reword: $R(s), R(s, a), R(s, a, s^{\prime})$Policy: $\pi(s) \rightarrow a$â€‹ $\pi^{*}$Sequences of Rewards: AssumptionInfinite HorizonsUtility of sequencesif $U(s_0, s_1, s_2, \cdots) &gt; U(s_0, s^{\prime}_1, s^{\prime}_2, \cdots)$then $U(s_1, s_2, \cdots) &gt; U(s^{\prime}_1, s^{\prime}_2, \cdots)$$$U(s_0, s_1, s_2, \cdots)=\sum_{t=0}^{\infty}\gamma^{t}R(s_t), 0 \leq \gamma \leq 1$$$$U\leq\frac{R_{max}}{1 - \gamma}$$Policies$$\pi^{\star}=argmax_{\pi} E[\sum_{t=0}^{\infty}\gamma^{t}R(S_t)|\pi]$$$$U^{\pi}(s)=E[\sum_{t=0}^{\infty}\gamma^{t}R(s_t)|\pi,s_0=s]$$$$\pi^{\star}(s)=argmax_{a}\sum_{s^{\prime}}T(s, a, s^{\prime})U(s^{\prime})$$$$U(s)=R(s)+\gamma \max_{a}\sum_{s^{\prime}}T(s, a, s^{\prime})U(s^{\prime})$$Above is the Bellman Equation.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Lecture 11 Recap]]></title>
      <url>%2F2017%2F04%2F10%2Fcs231n-Lecture-11-Recap%2F</url>
      <content type="text"><![CDATA[Working with CNNs in practiceMaking the most of your dataData augmentationTransfer learningAll about convolutionsHow to arrange themHow to compute them fastImplementation detailsGPU / CPU, bottlenecks, ditributed trainingData AugmentationHorizontal flipsRandom crops/scalesTraining: sample random crops /scalesResNet:Pick random L in range [256, 480]Resize training image, short side = LSample random 224 x 224 patchTesting: average a fixed set of cropsResNet:Resize image at 5 scales: {224, 256, 384, 480, 640}For each size, use 10 224 x 224 crops: 4 corners + center, + flipsColor jitterSimple:Randomly jitter contrastComplex:Apply PCA to all [R, G, B] pixels in training setSample a â€œcolor offsetâ€ along principal component directionsAdd offset to all pixels of a training image(As seen in [Krizhevsky et al. 2012], ResNet, etc)Transfer Learningâ€œYou need a lot of a data if you want to train/use CNNsâ€some tricks:very similar datasetvery different datasetvery little dataUse Linear Classifer on top layerTry linear classifer from different stagesquite a lot of dataFinetune a few layersFinetune a larger number of layersAll about ConvolutionsHow to stack themReplace large convolutions (5 x 5, 7 x 7) with stacks of 3 x 3 convolutions1 x 1 â€œbottleneckâ€ convolutions are very efficientCan factor N x N convolutions into 1 x N and N x 1All of the above give fewer parameters, less compute, more nonlinearityHow to compute themim2colBLASFFTCompute FFT of weights: F(W)Compute FFT of image: F(X)Compute elementwise product: F(W) â—‹ F(X)Compute inverse FFT: Y = F-1(F(W) â—‹ F(X))FFT convolutions get a big speedup for larger filtersNot much speedup for 3x3 filters =(Fast algorithmsStrassenâ€™s AlgorithmAnd so onâ€¦Implementation DetailsGPUs much faster than CPUsDistributed training is sometimes usedNot needed for small problemsBe aware of bottlenecks: CPU / GPU, CPU / diskLow precison makes things faster and still works32 bit is standard now, 16 bit soonIn the future: binary nets?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dynet xor demo [python version]]]></title>
      <url>%2F2017%2F04%2F09%2FDynet-xor-demo-python-version%2F</url>
      <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546import dynet as dyimport random# Parameters of the model and trainingHIDDEN_SIZE = 20NUM_EPOCHS = 20# Define the model and SGD optimizermodel = dy.Model()W_xh_p = model.add_parameters((HIDDEN_SIZE, 2))b_h_p = model.add_parameters(HIDDEN_SIZE)W_hy_p = model.add_parameters((1, HIDDEN_SIZE))b_y_p = model.add_parameters(1)trainer = dy.SimpleSGDTrainer(model)# Define the training data, consisting of (x,y) tuplesdata = [([1,1],1), ([-1,1],-1), ([1,-1],-1), ([-1,-1],1)]# Define the function we would like to calculatedef calc_function(x): dy.renew_cg() w_xh = dy.parameter(W_xh_p) b_h = dy.parameter(b_h_p) W_hy = dy.parameter(W_hy_p) b_y = dy.parameter(b_y_p) x_val = dy.inputVector(x) h_val = dy.tanh(w_xh * x_val + b_h) y_val = W_hy * h_val + b_y return y_val# Perform trainingfor epoch in range(NUM_EPOCHS): epoch_loss = 0 random.shuffle(data) for x, ystar in data: y = calc_function(x) loss = dy.squared_distance(y, dy.scalarInput(ystar)) epoch_loss += loss.value() loss.backward() trainer.update() print("Epoch %d: loss=%f" % (epoch, epoch_loss))# Print results of predictionfor x, ystar in data: y = calc_function(x) print("%r -&gt; %f" % (x, y.value()))Output:123456789101112131415161718192021222324252627[dynet] random seed: 1174664263[dynet] allocating memory: 512MB[dynet] memory allocation done.Epoch 0: loss=12.391680Epoch 1: loss=8.196088Epoch 2: loss=8.103037Epoch 3: loss=8.636450Epoch 4: loss=7.573008Epoch 5: loss=4.910318Epoch 6: loss=3.079966Epoch 7: loss=1.328273Epoch 8: loss=1.171368Epoch 9: loss=0.515850Epoch 10: loss=1.885216Epoch 11: loss=0.568994Epoch 12: loss=0.278629Epoch 13: loss=0.025215Epoch 14: loss=0.018466Epoch 15: loss=0.055305Epoch 16: loss=0.014131Epoch 17: loss=0.010476Epoch 18: loss=0.003893Epoch 19: loss=0.003332[1, 1] -&gt; 1.049703[-1, 1] -&gt; -0.996379[1, -1] -&gt; -0.974599[-1, -1] -&gt; 0.995763]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Installing the Python Dynet module]]></title>
      <url>%2F2017%2F04%2F09%2FInstalling-the-Python-Dynet-module%2F</url>
      <content type="text"><![CDATA[Installing the Python Dynet module(for instructions on installing on a computer with GPU, see below)Python bindings to DyNet are supported for both Python 2.x and 3.x.TL;DR(see below for the details)123456789101112131415161718192021222324# Installing Python DyNet:pip install cython # if you don&apos;t have it already.mkdir dynet-basecd dynet-base# getting dynet and eigengit clone https://github.com/clab/dynet.githg clone https://bitbucket.org/eigen/eigen -r 346ecdb # -r NUM specified a known working revisioncd dynetmkdir buildcd build# without GPU support:cmake .. -DEIGEN3_INCLUDE_DIR=../../eigen -DPYTHON=`which python`# or with GPU support:cmake .. -DEIGEN3_INCLUDE_DIR=../../eigen -DPYTHON=`which python` -DBACKEND=cudamake -j 2 # replace 2 with the number of available corescd pythonpython setup.py install # or `python setup.py install --user` for a user-local install.# this should suffice, but on some systems you may need to add the following line to your# init files in order for the compiled .so files be accessible to Python.# /path/to/dynet/build/dynet is the location in which libdynet.dylib resides.export DYLD_LIBRARY_PATH=/path/to/dynet/build/dynet/:$DYLD_LIBRARY_PATHDetailed InstructionsFirst, get DyNet:1234567cd $HOMEmkdir dynet-basecd dynet-basegit clone https://github.com/clab/dynet.gitcd dynetgit submodule init # To be consistent with DyNet&apos;s installation instructions.git submodule update # To be consistent with DyNet&apos;s installation instructions.Then get Eigen:123cd $HOMEcd dynet-basehg clone https://bitbucket.org/eigen/eigen/ -r 346ecdb(-r NUM specifies a known working revision of Eigen. You can remove this in order to get the bleeding edge Eigen, with the risk of some compile breaks, and the possible benefit of added optimizations.)We also need to make sure the cython module is installed. (you can replace pip with your favorite package manager, such as conda, or install within a virtual environment)1pip install cythonTo simplify the following steps, we can set a bash variable to hold where we have saved the main directories of DyNet and Eigen. In case you have gotten DyNet and Eigen differently from the instructions above and saved them in different location(s), these variables will be helpful:12PATH_TO_DYNET=$HOME/dynet-base/dynet/PATH_TO_EIGEN=$HOME/dynet-base/eigen/Compile DyNet.This is pretty much the same process as compiling DyNet, with the addition of the -DPYTHON= flag, pointing to the location of your Python interpreter.If Boost is installed in a non-standard location, you should add the corresponding flags to the cmake commandline, see the DyNet installation instructions page.123456cd $PATH_TO_DYNETPATH_TO_PYTHON=`which python`mkdir buildcd buildcmake .. -DEIGEN3_INCLUDE_DIR=$PATH_TO_EIGEN -DPYTHON=$PATH_TO_PYTHONmake -j 2Assuming that the cmake command found all the needed libraries and didnâ€™t fail, the make command will take a while, and compile DyNet as well as the Python bindings. You can change make -j 2 to a higher number, depending on the available cores you want to use while compiling.You now have a working Python binding inside of build/dynet. To verify this is working:12cd $PATH_TO_DYNET/build/pythonpythonthen, within Python:123import dynet as dyprint dy.__version__model = dy.Model()In order to install the module so that it is accessible from everywhere in the system, run the following:12cd $PATH_TO_DYNET/build/pythonpython setup.py install --userThe --user switch will install the module in your local site-packages, and works without root privileges. To install the module to the system site-packages (for all users), or to the current virtualenv (if you are on one), run python setup.py installwithout this switch.You should now have a working python binding (the dynet module).Note however that the installation relies on the compiled DyNet library being in $PATH_TO_DYNET/build/dynet, so make sure not to move it from there.Now, check that everything works:1234cd $PATH_TO_DYNETcd examples/pythonpython xor.pypython rnnlm.py rnnlm.pyAlternatively, if the following script works for you, then your installation is likely to be working:12from dynet import *model = Model()If it doesnâ€™t work and you get an error similar to the following:123ImportError: dlopen(/Users/sneharajana/.python-eggs/dyNET-0.0.0-py2.7-macosx-10.11-intel.egg-tmp/_dynet.so, 2): Library not loaded: @rpath/libdynet.dylibReferenced from: /Users/sneharajana/.python-eggs/dyNET-0.0.0-py2.7-macosx-10.11-intel.egg-tmp/_dynet.soReason: image not found``then you may need to run the following (and add it to your shell init files):export DYLD_LIBRARY_PATH=/path/to/dynet/build/dynet/:$DYLD_LIBRARY_PATHUsageThere are two ways to import the dynet module :1import dynetimports dynet and automatically initializes the global dynet parameters with the command line arguments (see the documentation). The amount of memory allocated, GPU/CPU usage is fixed from there on.123import _dynet# orimport _gdynet # For GPUImports dynet for CPU (resp. GPU) and doesnâ€™t initialize the global parameters. These must be initialized manually before using dynet, using one of the following :123# Same as import dynet as dyimport _dynet as dydy.init()123456789101112131415# Same as import dynet as dyimport _dynet as dy# Declare a DynetParams objectdyparams = dy.DynetParams()# Fetch the command line arguments (optional)dyparams.from_args()# Set some parameters manualy (see the command line arguments documentation)dyparams.set_mem(2048)dyparams.set_random_seed(666)dyparams.set_weight_decay(1e-7)dyparams.set_shared_parameters(False)dyparams.set_requested_gpus(1)dyparams.set_gpu_mask([0,1,1,0])# Initialize with the given parametersdyparams.init() # or init_from_params(dyparams)Anaconda SupportAnaconda is a popular package management system for Python. DyNet can be used from within an Anaconda environment, but be sure to activate the environmentsource activate my_environment_namethen install some necessary packages as follows:conda install gcc cmake boost cythonAfter this, the build process should be the same as normal.Note that on some conda environments, people have reported build errors related to the interaction between the icu and boost packages. If you encounter this, try the solution in this comment.Windows SupportYou can also use Python on Windows by following similar steps to the above. For simplicity, we recommend using a Python distribution that already has Cython installed. The following has been tested to work:Install WinPython 2.7.10 (comes with Cython already installed).Run CMake as above with -DPYTHON=/path/to/your/python.exe.Open a command prompt and set VS90COMNTOOLS to the path to your Visual Studio â€œCommon7/Toolsâ€ directory. One easy way to do this is a command such as:1set VS90COMNTOOLS=%VS140COMNTOOLS%Open dynet.sln from this command prompt and build the â€œReleaseâ€ version of the solution.Follow the rest of the instructions above for testing the build and installing it for other usersNote, currently only the Release version works.GPU/MKL SupportInstalling/running on GPUFor installing on a computer with GPU, first install CUDA. The following instructions assume CUDA is installed.The installation process is pretty much the same, while adding the -DBACKEND=cuda flag to the cmake stage:1cmake .. -DEIGEN3_INCLUDE_DIR=$PATH_TO_EIGEN -DPYTHON=$PATH_TO_PYTHON -DBACKEND=cuda(if CUDA is installed in a non-standard location and cmake cannot find it, you can specify also -DCUDA_TOOLKIT_ROOT_DIR=/path/to/cuda.)Now, build the Python modules (as above, we assume Cython is installed):After running make -j 2, you should have the files _dynet.so and _gdynet.so in the build/python folder.As before, cd build/python followed by python setup.py install --user will install the module.In order to use the GPU support, you can either:Use import _gdynet as dy instead of import dynet as dyOr, (preferred), import dynet as usual, but use the commandline switch --dynet-gpu or the GPU switches detailed herewhen invoking the program. This option lets the same code work with either the GPU or the CPU version depending on how it is invoked.Running with MKLIf youâ€™ve built DyNet to use MKL (using -DMKL or -DMKL_ROOT), Python sometimes has difficulty finding the MKL shared libraries. You can try setting LD_LIBRARY_PATH to point to your MKL library directory. If that doesnâ€™t work, try setting the following environment variable (supposing, for example, your MKL libraries are located at /opt/intel/mkl/lib/intel64):1export LD_PRELOAD=/opt/intel/mkl/lib/intel64/libmkl_def.so:/opt/intel/mkl/lib/intel64/libmkl_avx2.so:/opt/intel/mkl/libSome Errors and correspond Solutions12345678910111213import dynet as dy-------------------------------------------Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;dynet.py&quot;, line 17, in &lt;module&gt; from _dynet import *ImportError: /home/ewan/anaconda2/lib/libstdc++.so.6: version `GLIBCXX_3.4.20&apos; not found (required by /home/ewan/dynet-base/dynet/build/dynet/libdynet.so)-------------------------------------------Solution:conda install libgcc]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[lda for news classification]]></title>
      <url>%2F2017%2F03%2F26%2Flda-for-news-classification%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[netease news spider]]></title>
      <url>%2F2017%2F03%2F26%2Fnetease-news-spider%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[LDA]]></title>
      <url>%2F2017%2F03%2F24%2FLDA%2F</url>
      <content type="text"><![CDATA[Scikit-learn example1%matplotlib inlineTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet AllocationThis is an example of applying Non-negative Matrix Factorization and Latent Dirichlet Allocation on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).The default parameters (n_samples / n_features / n_topics) should make the example runnable in a couple of tens of seconds. You can try to increase the dimensions of the problem, but be aware that the time complexity is polynomial in NMF. In LDA, the time complexity is proportional to (n_samples * iterations).1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# Author: Olivier Grisel &lt;olivier.grisel@ensta.org&gt;# Lars Buitinck# Chyi-Kwei Yau &lt;chyikwei.yau@gmail.com&gt;# License: BSD 3 clausefrom __future__ import print_functionfrom time import timefrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizerfrom sklearn.decomposition import NMF, LatentDirichletAllocationfrom sklearn.datasets import fetch_20newsgroupsn_samples = 2000n_features = 1000n_topics = 10n_top_words = 20def print_top_words(model, feature_names, n_top_words): for topic_idx, topic in enumerate(model.components_): print("Topic #%d:" % topic_idx) print(" ".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])) print()# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics# to filter out useless terms early on: the posts are stripped of headers,# footers and quoted replies, and common English words, words occurring in# only one document or in at least 95% of the documents are removed.print("Loading dataset...")t0 = time()dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))data_samples = dataset.data[:n_samples]print("done in %0.3fs." % (time() - t0))# Use tf-idf features for NMF.print("Extracting tf-idf features for NMF...")tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')t0 = time()tfidf = tfidf_vectorizer.fit_transform(data_samples)print("done in %0.3fs." % (time() - t0))# Use tf (raw term count) features for LDA.print("Extracting tf features for LDA...")tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')t0 = time()tf = tf_vectorizer.fit_transform(data_samples)print("done in %0.3fs." % (time() - t0))# Fit the NMF modelprint("Fitting the NMF model with tf-idf features, " "n_samples=%d and n_features=%d..." % (n_samples, n_features))t0 = time()nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)print("done in %0.3fs." % (time() - t0))print("\nTopics in NMF model:")tfidf_feature_names = tfidf_vectorizer.get_feature_names()print_top_words(nmf, tfidf_feature_names, n_top_words)print("Fitting LDA models with tf features, " "n_samples=%d and n_features=%d..." % (n_samples, n_features))lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0)t0 = time()lda.fit(tf)print("done in %0.3fs." % (time() - t0))print("\nTopics in LDA model:")tf_feature_names = tf_vectorizer.get_feature_names()print_top_words(lda, tf_feature_names, n_top_words)1Loading dataset...â€‹1No handlers could be found for logger "sklearn.datasets.twenty_newsgroups"â€‹123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354done in 691.647s.Extracting tf-idf features for NMF...done in 0.454s.Extracting tf features for LDA...done in 0.416s.Fitting the NMF model with tf-idf features, n_samples=2000 and n_features=1000...done in 0.367s.Topics in NMF model:Topic #0:just people don think like know time good make way really say right ve want did ll new use yearsTopic #1:windows use dos using window program os drivers application help software pc running ms screen files version card code workTopic #2:god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religionTopic #3:thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video needTopic #4:car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used boughtTopic #5:edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact bloodTopic #6:file problem files format win sound ftp pub read save site help image available create copy running memory self versionTopic #7:game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teamsTopic #8:drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internalTopic #9:key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standardFitting LDA models with tf features, n_samples=2000 and n_features=1000...done in 2.169s.Topics in LDA model:Topic #0:edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mitTopic #1:don like just know think ve way use right good going make sure ll point got need really time doesnTopic #2:christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believeTopic #3:drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16Topic #4:hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drugTopic #5:god people does just good don jesus say israel way life know true fact time law want believe make thinkTopic #6:55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16Topic #7:car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performanceTopic #8:people said did just didn know time like went think children came come don took years say dead told startedTopic #9:key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technologyâ€‹1data_samples[0]1u"Well i'm not sure about the story nad it did seem biased. What\nI disagree with is your statement that the U.S. Media is out to\nruin Israels reputation. That is rediculous. The U.S. media is\nthe most pro-israeli media in the world. Having lived in Europe\nI realize that incidences such as the one described in the\nletter have occured. The U.S. media as a whole seem to try to\nignore them. The U.S. is subsidizing Israels existance and the\nEuropeans are not (at least not to the same degree). So I think\nthat might be a reason they report more clearly on the\natrocities.\n\tWhat is a shame is that in Austria, daily reports of\nthe inhuman acts commited by Israeli soldiers and the blessing\nreceived from the Government makes some of the Holocaust guilt\ngo away. After all, look how the Jews are treating other races\nwhen they got power. It is unfortunate.\n"1tfidf_vectorizer.get_feature_names()[-10:]12345678910[u'worth', u'wouldn', u'write', u'written', u'wrong', u'xfree86', u'year', u'years', u'yes', u'young']1tfidf.toarray().shape1(2000L, 1000L)1dataset.target_names1234567891011121314151617181920['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']1lda.transform(tf)[1934]12array([ 0.3587206 , 0.00227337, 0.00227317, 0.50146046, 0.00227288, 0.12390701, 0.00227282, 0.00227329, 0.00227343, 0.00227299])ExtrasSome materials can find from Github.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Excel merge the same value that in a column to a cell]]></title>
      <url>%2F2017%2F03%2F16%2FExcel-merge-the-same-value-that-in-a-column-to-a-cell%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Installing ptproc on Ubuntu 16.04 LTS]]></title>
      <url>%2F2017%2F03%2F16%2FInstalling-ptproc-on-Ubuntu-16-04-LTS%2F</url>
      <content type="text"><![CDATA[æƒ³å¯¹äº‹ä»¶æ•°æ®ç”¨ç‚¹è¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ï¼Œä¸ºäº†ä¸ç”¨é‡å¤é€ è½®å­ï¼Œæ‰€ä»¥æ‰¾åˆ°äº†ä¸€ä¸ªRåŒ…ptprocï¼Œä½†æ˜¯å®‰è£…æ—¶æŠ¥é”™1234install.packages("ptproc")-------------------------------package â€˜ptprocâ€™ is not available (as a binary package for R version 3.2.3)æ—¢ç„¶ä»“åº“é‡Œæ²¡æœ‰ï¼Œé‚£å°±åªå¥½ç”¨æºç å®‰è£…äº†ï¼Œæºç ä¸‹è½½åœ°å€, ä½†æ˜¯ä¾æ—§æŠ¥é”™ï¼š123456789101112131415&gt; install.packages("ptproc", repos="http://www.biostat.jhsph.edu/~rpeng/software", type="source")trying URL 'http://www.biostat.jhsph.edu/~rpeng/software/src/contrib/ptproc_1.5-1.tar.gz'Content type 'application/x-gzip' length 282002 bytes (275 KB)opened URL==================================================downloaded 275 KB * installing *source* package â€˜ptprocâ€™ ...ERROR: a 'NAMESPACE' file is required* removing â€˜/Library/Frameworks/R.framework/Versions/3.1/Resources/library/ptprocâ€™ The downloaded source packages are in â€˜/private/var/folders/0b/qdw3f3zn0gq5yy2cjjpm8cgw0000gn/T/RtmpuW1EPA/downloaded_packagesâ€™Warning message:In install.packages("ptproc", repos = "http://www.biostat.jhsph.edu/~rpeng/software", : installation of package â€˜ptprocâ€™ had non-zero exit statusçœ‹æ¥åªæœ‰æ‰‹åŠ¨æ·»åŠ ä¸€ä¸ªNAMESPACE,12345cd ptprocecho &apos;exportPattern( &quot;.&quot; )&apos; &gt; NAMESPACEcd ../rm ptproc_1.5-1.tar.gztar cvzf ptproc/ ptproc_1.5-1.tar.gzç»§ç»­æºç å®‰è£…ï¼š1R CMD INSTALL -l &lt;ourRlibrarylocation&gt; &lt;path where I saved the packagename.tar.gz file&gt;Got it.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note Ch12]]></title>
      <url>%2F2017%2F03%2F09%2Fpython-data-analysis-learning-note-Ch12%2F</url>
      <content type="text"><![CDATA[Numpyé«˜çº§åº”ç”¨123456from __future__ import divisionfrom numpy.random import randnfrom pandas import Seriesimport numpy as npnp.set_printoptions(precision=4)import sys12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"ndarrayå¯¹è±¡çš„å†…éƒ¨æœºåˆ¶NumPy æ•°æ®ç±»åž‹ä½“ç³»æ£€æµ‹ç±»åž‹æ˜¯å¦æ˜¯æŸç§ç±»åž‹çš„å­ç±»1234ints = np.ones(10, dtype=np.uint16)floats = np.ones(10, dtype=np.float32)np.issubdtype(ints.dtype, np.integer)np.issubdtype(floats.dtype, np.floating)True True è¾“å‡ºæŸç§ç±»åž‹çš„æ‰€æœ‰çˆ¶ç±»1np.float64.mro()[numpy.float64, numpy.floating, numpy.inexact, numpy.number, numpy.generic, float, object] é«˜çº§æ•°ç»„æ“ä½œæ•°ç»„é‡å¡‘123arr = np.arange(8)arrarr.reshape((4, 2))array([0, 1, 2, 3, 4, 5, 6, 7]) array([[0, 1], [2, 3], [4, 5], [6, 7]]) 1arr.reshape((4, 2)).reshape((2, 4))array([[0, 1, 2, 3], [4, 5, 6, 7]]) -1ä»£è¡¨è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ç»´åº¦12arr = np.arange(15)arr.reshape((5, -1))array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) ç”¨å…¶ä»–æ•°ç»„çš„shapeè¿›è¡Œé‡å¡‘123other_arr = np.ones((3, 5))other_arr.shapearr.reshape(other_arr.shape)(3, 5) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) æ‹‰ç›´123arr = np.arange(15).reshape((5, 3))arrarr.ravel()array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) ä¼šäº§ç”Ÿä¸€ä¸ªå‰¯æœ¬1arr.flatten()array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) C vs. Fortran é¡ºåº1234arr = np.arange(12).reshape((3, 4))arrarr.ravel()arr.ravel('F')array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) array([ 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11]) æ•°ç»„çš„åˆå¹¶ä»¥åŠæ‹†åˆ†1234arr1 = np.array([[1, 2, 3], [4, 5, 6]])arr2 = np.array([[7, 8, 9], [10, 11, 12]])np.concatenate([arr1, arr2], axis=0)np.concatenate([arr1, arr2], axis=1)array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]]) array([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]]) æ›´æ–¹ä¾¿çš„æ–¹æ³•12np.vstack((arr1, arr2))np.hstack((arr1, arr2))array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]]) array([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]]) 1234567from numpy.random import randnarr = randn(5, 2)arrfirst, second, third = np.split(arr, [1, 3])firstsecondthirdarray([[ 0.9659, 1.3079], [-1.7632, 0.0904], [-0.6033, 0.2266], [-0.4417, -1.8609], [-1.2463, -0.6249]]) array([[ 0.9659, 1.3079]]) array([[-1.7632, 0.0904], [-0.6033, 0.2266]]) array([[-0.4417, -1.8609], [-1.2463, -0.6249]]) å †å è¾…åŠ©ç±»æ›´â€¦ç®€æ´â€¦12345arr = np.arange(6)arr1 = arr.reshape((3, 2))arr2 = randn(3, 2)np.r_[arr1, arr2]np.c_[np.r_[arr1, arr2], arr]array([[ 0. , 1. ], [ 2. , 3. ], [ 4. , 5. ], [ 0.0376, 1.8236], [ 0.9025, -0.053 ], [-0.6849, 1.6728]]) array([[ 0. , 1. , 0. ], [ 2. , 3. , 1. ], [ 4. , 5. , 2. ], [ 0.0376, 1.8236, 3. ], [ 0.9025, -0.053 , 4. ], [-0.6849, 1.6728, 5. ]]) 1np.c_[1:6, -10:-5]array([[ 1, -10], [ 2, -9], [ 3, -8], [ 4, -7], [ 5, -6]]) å…ƒç´ çš„é‡å¤æ“ä½œ: tile and repeatå…ƒç´ çº§é‡å¤12arr = np.arange(3)arr.repeat(3)array([0, 0, 0, 1, 1, 1, 2, 2, 2]) æŒ‡å®šé‡å¤æ¬¡æ•°1arr.repeat([2, 3, 4])array([0, 0, 1, 1, 1, 2, 2, 2, 2]) å¤šç»´æ•°ç»„éœ€è¦æŒ‡å®šaxis123arr = randn(2, 2)arrarr.repeat(2, axis=0)array([[-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142], [-0.4628, 1.1142], [ 0.3637, 0.4341], [ 0.3637, 0.4341]]) 12arr.repeat([2, 3], axis=0)arr.repeat([2, 3], axis=1)array([[-0.4628, 1.1142], [-0.4628, 1.1142], [ 0.3637, 0.4341], [ 0.3637, 0.4341], [ 0.3637, 0.4341]]) array([[-0.4628, -0.4628, 1.1142, 1.1142, 1.1142], [ 0.3637, 0.3637, 0.4341, 0.4341, 0.4341]]) å—çº§é‡å¤12arrnp.tile(arr, 2)array([[-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341]]) 123arrnp.tile(arr, (2, 1))np.tile(arr, (3, 2))array([[-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142], [ 0.3637, 0.4341], [-0.4628, 1.1142], [ 0.3637, 0.4341]]) array([[-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341], [-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341], [-0.4628, 1.1142, -0.4628, 1.1142], [ 0.3637, 0.4341, 0.3637, 0.4341]]) èŠ±å¼ç´¢å¼•çš„ç­‰ä»·å‡½æ•°: take and put123arr = np.arange(10) * 100inds = [7, 1, 2, 6]arr[inds]array([700, 100, 200, 600]) 12345arr.take(inds)arr.put(inds, 42)arrarr.put(inds, [40, 41, 42, 43])arrarray([700, 100, 200, 600]) array([ 0, 42, 42, 300, 400, 500, 42, 42, 800, 900]) array([ 0, 41, 42, 300, 400, 500, 43, 40, 800, 900]) 1234inds = [2, 0, 2, 1]arr = randn(2, 4)arrarr.take(inds, axis=1)array([[ 0.2772, -1.3059, -1.4607, -0.4856], [ 1.5585, -0.4521, -1.6259, -1.6644]]) array([[-1.4607, 0.2772, -1.4607, -1.3059], [-1.6259, 1.5585, -1.6259, -0.4521]]) å¹¿æ’­æ¯ä¸€ä¸ªå…ƒç´ éƒ½ä¹˜ä»¥4123arr = np.arange(5)arrarr * 4array([0, 1, 2, 3, 4]) array([ 0, 4, 8, 12, 16]) æ¯ä¸€ç»´å¯¹åº”å‡åŽ»å‡å€¼12345arr = randn(4, 3)arr.mean(0)demeaned = arr - arr.mean(0)demeaneddemeaned.mean(0)array([-0.1556, 0.3494, -0.2545]) array([[-0.3753, 0.5353, 1.3534], [-0.4282, 0.5606, 0.8935], [-0.0956, -0.9767, -1.2444], [ 0.899 , -0.1192, -1.0024]]) array([ -5.5511e-17, -1.3878e-17, 0.0000e+00]) 12345arrrow_means = arr.mean(1)row_means.reshape((4, 1))demeaned = arr - row_means.reshape((4, 1))demeaned.mean(1)array([[-0.5308, 0.8848, 1.0989], [-0.5837, 0.91 , 0.639 ], [-0.2511, -0.6273, -1.4989], [ 0.7434, 0.2302, -1.2569]]) array([[ 0.4843], [ 0.3218], [-0.7924], [-0.0944]]) array([ 7.4015e-17, 0.0000e+00, 0.0000e+00, 0.0000e+00]) æ²¿å…¶ä»–è½´å‘å¹¿æ’­ç»´åº¦ä¸å¯¹åº”1arr - arr.mean(1)--------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-31-7b87b85a20b2&gt; in &lt;module&gt;() ----&gt; 1 arr - arr.mean(1) ValueError: operands could not be broadcast together with shapes (4,3) (4,) 1arr - arr.mean(1).reshape((4, 1))array([[-1.0151, 0.4005, 0.6146], [-0.9055, 0.5882, 0.3173], [ 0.5413, 0.1652, -0.7065], [ 0.8378, 0.3246, -1.1625]]) 1234arr = np.zeros((4, 4))arr_3d = arr[:, np.newaxis]arr_3darr_3d.shapearray([[[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]]]) (4, 1, 4) 1234arr_1d = np.random.normal(size=3)arr_1darr_1d[:, np.newaxis]arr_1d[np.newaxis, :]array([-1.1083, 0.5576, 1.2277]) array([[-1.1083], [ 0.5576], [ 1.2277]]) array([[-1.1083, 0.5576, 1.2277]]) 123456arr = randn(3, 4, 5)arrdepth_means = arr.mean(2)depth_meansdemeaned = arr - depth_means[:, :, np.newaxis]demeaned.mean(2)array([[[-1.9966, -0.2431, -0.992 , 0.8283, -0.5073], [-0.3938, -0.1332, -0.7427, 0.3094, -0.9241], [ 1.1069, -0.5383, -0.9288, 0.0233, -0.4678], [-1.2015, 0.6905, 1.6706, -0.1703, -1.3975]], [[-0.3048, -1.7181, -0.189 , 0.6263, 1.1194], [ 0.0823, -0.7132, -0.5162, 1.5305, -1.199 ], [ 0.5777, 1.2935, 0.1547, -1.3637, 0.4251], [ 0.4923, 1.4004, 0.3646, 0.1594, -0.7334]], [[ 1.3836, -0.5313, 0.2826, 0.4739, -1.3435], [-1.141 , -0.3084, 1.1364, 1.1326, 0.3064], [-0.9692, 1.0229, -0.0246, 1.4484, -1.137 ], [ 1.7033, -1.8358, 1.2087, -0.5463, 0.5904]]]) array([[-0.5822, -0.3769, -0.1609, -0.0816], [-0.0932, -0.1631, 0.2174, 0.3367], [ 0.0531, 0.2252, 0.0681, 0.2241]]) array([[ 8.8818e-17, 0.0000e+00, -4.4409e-17, -8.8818e-17], [ 0.0000e+00, 0.0000e+00, 2.7756e-17, 8.8818e-17], [ 4.4409e-17, 5.5511e-17, 4.4409e-17, 0.0000e+00]]) 1234567def demean_axis(arr, axis=0): means = arr.mean(axis) # This generalized things like [:, :, np.newaxis] to N dimensions indexer = [slice(None)] * arr.ndim # like : indexer[axis] = np.newaxis return arr - means[indexer]é€šè¿‡å¹¿æ’­è®¾ç½®æ•°ç»„çš„å€¼123arr = np.zeros((4, 3))arr[:] = 5arrarray([[ 5., 5., 5.], [ 5., 5., 5.], [ 5., 5., 5.], [ 5., 5., 5.]]) 12345col = np.array([1.28, -0.42, 0.44, 1.6])arr[:] = col[:, np.newaxis]arrarr[:2] = [[-1.37], [0.509]]arrarray([[ 1.28, 1.28, 1.28], [-0.42, -0.42, -0.42], [ 0.44, 0.44, 0.44], [ 1.6 , 1.6 , 1.6 ]]) array([[-1.37 , -1.37 , -1.37 ], [ 0.509, 0.509, 0.509], [ 0.44 , 0.44 , 0.44 ], [ 1.6 , 1.6 , 1.6 ]]) ufuncé«˜çº§åº”ç”¨ufuncå®žä¾‹æ–¹æ³•reduceé€šè¿‡ä¸€ç³»åˆ—çš„äºŒå…ƒè¿ç®—å¯¹å…¶å€¼è¿›è¡Œèšåˆï¼ˆå¯æŒ‡æ˜Žè½´å‘ï¼‰123arr = np.arange(10)np.add.reduce(arr)arr.sum()45 45 1np.random.seed(12346)è¿™é‡Œèšåˆçš„æ˜¯é€»è¾‘ä¸Žæ“ä½œ123456arr = randn(5, 5)arrarr[::2].sort(1) # sort a few rowsarrarr[:, :-1] &lt; arr[:, 1:]np.logical_and.reduce(arr[:, :-1] &lt; arr[:, 1:], axis=1)array([[-0.7066, 0.4268, -0.2776, -0.8283, -2.7628], [ 0.9835, 0.4378, -0.8496, 0.7188, 0.7329], [ 0.5047, -0.7893, 0.5392, 1.2907, 0.8676], [ 0.4113, 0.4459, -0.3172, -1.0493, 1.3459], [ 0.356 , -0.0915, -0.535 , -0.036 , -0.2591]]) array([[-2.7628, -0.8283, -0.7066, -0.2776, 0.4268], [ 0.9835, 0.4378, -0.8496, 0.7188, 0.7329], [-0.7893, 0.5047, 0.5392, 0.8676, 1.2907], [ 0.4113, 0.4459, -0.3172, -1.0493, 1.3459], [-0.535 , -0.2591, -0.0915, -0.036 , 0.356 ]]) array([[ True, True, True, True], [False, False, True, True], [ True, True, True, True], [ True, False, False, True], [ True, True, True, True]], dtype=bool) array([ True, False, True, False, True], dtype=bool) ç›¸å¯¹äºŽreduceåªè¾“å‡ºæœ€åŽç»“æžœï¼Œaccumulateä¿ç•™ä¸­é—´ç»“æžœ12arr = np.arange(15).reshape((3, 5))np.add.accumulate(arr, axis=1)array([[ 0, 1, 3, 6, 10], [ 5, 11, 18, 26, 35], [10, 21, 33, 46, 60]], dtype=int32) outerè®¡ç®—ä¸¤ä¸ªæ•°ç»„çš„å‰ç§¯123arr = np.arange(3).repeat([1, 2, 2])arrnp.multiply.outer(arr, np.arange(5))array([0, 1, 1, 2, 2]) array([[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8], [0, 2, 4, 6, 8]]) outerè¾“å‡ºç»“æžœçš„ç»´åº¦æ˜¯è¾“å…¥ä¸¤ä¸ªæ•°ç»„çš„ç»´åº¦ä¹‹å’Œ12result = np.subtract.outer(randn(3, 4), randn(5))result.shape(3, 4, 5) 12arr = np.arange(10)np.add.reduceat(arr, [0, 5, 8])array([10, 18, 17], dtype=int32) 123arr = np.multiply.outer(np.arange(4), np.arange(5))arrnp.add.reduceat(arr, [0, 2, 4], axis=1)array([[ 0, 0, 0, 0, 0], [ 0, 1, 2, 3, 4], [ 0, 2, 4, 6, 8], [ 0, 3, 6, 9, 12]]) array([[ 0, 0, 0], [ 1, 5, 4], [ 2, 10, 8], [ 3, 15, 12]], dtype=int32) è‡ªå®šä¹‰ ufuncsä¸¤ç§ä¸åŒçš„è°ƒç”¨æ–¹å¼1234def add_elements(x, y): return x + yadd_them = np.frompyfunc(add_elements, 2, 1) # 2 input and 1 outputadd_them(np.arange(8), np.arange(8))array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object) 12add_them = np.vectorize(add_elements, otypes=[np.float64])add_them(np.arange(8), np.arange(8))array([ 0., 2., 4., 6., 8., 10., 12., 14.]) è‡ªå·±å®žçŽ°çš„è¿˜æ˜¯æ¯”ä¸ä¸Šå†…ç½®ä¼˜åŒ–è¿‡çš„å‡½æ•°123arr = randn(10000)%timeit add_them(arr, arr)%timeit np.add(arr, arr)100 loops, best of 3: 1.81 ms per loop The slowest run took 16.51 times longer than the fastest. This could mean that an intermediate result is being cached. 100000 loops, best of 3: 3.65 Âµs per loop ç»“æž„åŒ–å’Œè®°å½•å¼æ•°ç»„123dtype = [('x', np.float64), ('y', np.int32)]sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)sarrarray([(1.5, 6), (3.141592653589793, -2)], dtype=[(&apos;x&apos;, &apos;&lt;f8&apos;), (&apos;y&apos;, &apos;&lt;i4&apos;)]) 12sarr[0]sarr[0]['y'](1.5, 6) 6 1sarr['x']array([ 1.5 , 3.1416]) åµŒå¥—dtypeå’Œå¤šç»´å­—æ®µ123dtype = [('x', np.int64, 3), ('y', np.int32)]arr = np.zeros(4, dtype=dtype)arrarray([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0)], dtype=[(&apos;x&apos;, &apos;&lt;i8&apos;, (3,)), (&apos;y&apos;, &apos;&lt;i4&apos;)]) 1arr[0]['x']array([0, 0, 0], dtype=int64) 1arr['x']array([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=int64) 12345dtype = [('x', [('a', 'f8'), ('b', 'f4')]), ('y', np.int32)]data = np.array([((1, 2), 5), ((3, 4), 6)], dtype=dtype)data['x']data['y']data['x']['a']array([(1.0, 2.0), (3.0, 4.0)], dtype=[(&apos;a&apos;, &apos;&lt;f8&apos;), (&apos;b&apos;, &apos;&lt;f4&apos;)]) array([5, 6]) array([ 1., 3.]) æ›´å¤šæœ‰å…³æŽ’åºçš„è¯é¢˜123arr = randn(6)arr.sort()arrarray([-1.3918, -0.2089, 0.2316, 0.728 , 0.8356, 1.9956]) 1234arr = randn(3, 5)arrarr[:, 0].sort() # Sort first column values in-placearrarray([[ -2.9812e-01, 1.2037e+00, -1.5768e-02, 7.4395e-01, 8.6880e-01], [ -4.2865e-01, 7.1886e-01, -1.4510e+00, 1.0510e-01, -1.7942e+00], [ -2.8792e-04, 6.1168e-01, -9.1210e-02, -1.2799e+00, -4.0230e-02]]) array([[ -4.2865e-01, 1.2037e+00, -1.5768e-02, 7.4395e-01, 8.6880e-01], [ -2.9812e-01, 7.1886e-01, -1.4510e+00, 1.0510e-01, -1.7942e+00], [ -2.8792e-04, 6.1168e-01, -9.1210e-02, -1.2799e+00, -4.0230e-02]]) 1234arr = randn(5)arrnp.sort(arr)arrarray([-0.9699, -0.5626, 1.1172, 0.2791, -1.1148]) array([-1.1148, -0.9699, -0.5626, 0.2791, 1.1172]) array([-0.9699, -0.5626, 1.1172, 0.2791, -1.1148]) 1234arr = randn(3, 5)arrarr.sort(axis=1)arrarray([[ 0.2266, 0.3405, 2.6439, -1.6262, -0.3976], [-1.4821, 1.068 , -0.252 , -0.9331, 2.2639], [-0.2311, 1.1472, 0.9287, -0.9023, 1.1761]]) array([[-1.6262, -0.3976, 0.2266, 0.3405, 2.6439], [-1.4821, -0.9331, -0.252 , 1.068 , 2.2639], [-0.9023, -0.2311, 0.9287, 1.1472, 1.1761]]) 1arr[:, ::-1]array([[ 2.6439, 0.3405, 0.2266, -0.3976, -1.6262], [ 2.2639, 1.068 , -0.252 , -0.9331, -1.4821], [ 1.1761, 1.1472, 0.9287, -0.2311, -0.9023]]) é—´æŽ¥æŽ’åº: argsort and lexsort1234values = np.array([5, 0, 1, 3, 2])indexer = values.argsort()indexervalues[indexer]array([1, 2, 4, 3, 0], dtype=int64) array([0, 1, 2, 3, 5]) 1234arr = randn(3, 5)arr[0] = valuesarrarr[:, arr[0].argsort()]array([[ 5. , 0. , 1. , 3. , 2. ], [ 0.422 , 0.1187, 1.1352, 1.4363, -1.2487], [ 0.1909, -1.0984, 0.7886, -0.5827, 1.1592]]) array([[ 0. , 1. , 2. , 3. , 5. ], [ 0.1187, 1.1352, -1.2487, 1.4363, 0.422 ], [-1.0984, 0.7886, 1.1592, -0.5827, 0.1909]]) 1234first_name = np.array(['Bob', 'Jane', 'Steve', 'Bill', 'Barbara'])last_name = np.array(['Jones', 'Arnold', 'Arnold', 'Jones', 'Walters'])sorter = np.lexsort((first_name, last_name))zip(last_name[sorter], first_name[sorter])&lt;zip at 0x1d1284f87c8&gt; å…¶ä»–æŽ’åºç®—æ³•12345values = np.array(['2:first', '2:second', '1:first', '1:second', '1:third'])key = np.array([2, 2, 1, 1, 1])indexer = key.argsort(kind='mergesort')indexervalues.take(indexer)array([2, 3, 4, 0, 1], dtype=int64) array([&apos;1:first&apos;, &apos;1:second&apos;, &apos;1:third&apos;, &apos;2:first&apos;, &apos;2:second&apos;], dtype=&apos;&lt;U8&apos;) numpy.searchsorted: åœ¨æœ‰åºæ•°ç»„ä¸­æŸ¥æ‰¾å…ƒç´ 12arr = np.array([0, 1, 7, 12, 15])arr.searchsorted(9)3 1arr.searchsorted([0, 8, 11, 16])array([0, 3, 3, 5], dtype=int64) 123arr = np.array([0, 0, 0, 1, 1, 1, 1])arr.searchsorted([0, 1])arr.searchsorted([0, 1], side='right')array([0, 3], dtype=int64) array([3, 7], dtype=int64) 123data = np.floor(np.random.uniform(0, 10000, size=50))bins = np.array([0, 100, 1000, 5000, 10000])dataarray([ 143., 8957., 309., 2349., 5503., 2754., 4408., 4259., 3313., 3364., 2492., 9977., 4704., 5538., 6089., 5864., 6926., 3677., 8698., 1832., 8931., 6631., 5322., 3712., 9350., 3945., 9514., 3683., 8568., 8247., 7087., 7630., 3392., 8320., 1973., 982., 1672., 7052., 6230., 3894., 1832., 9488., 755., 8522., 1858., 5417., 6162., 7517., 9827., 4458.]) 12labels = bins.searchsorted(data)labelsarray([2, 4, 2, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 2, 3, 4, 4, 3, 3, 4, 2, 4, 3, 4, 4, 4, 4, 3], dtype=int64) 1Series(data).groupby(labels).mean()2 547.250000 3 3178.550000 4 7591.038462 dtype: float64 1np.digitize(data, bins)array([2, 4, 2, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 2, 3, 4, 4, 3, 3, 4, 2, 4, 3, 4, 4, 4, 4, 3], dtype=int64) NumPy matrix class12345678X = np.array([[ 8.82768214, 3.82222409, -1.14276475, 2.04411587], [ 3.82222409, 6.75272284, 0.83909108, 2.08293758], [-1.14276475, 0.83909108, 5.01690521, 0.79573241], [ 2.04411587, 2.08293758, 0.79573241, 6.24095859]])X[:, 0] # one-dimensionaly = X[:, :1] # two-dimensional by slicingXyarray([ 8.8277, 3.8222, -1.1428, 2.0441]) array([[ 8.8277, 3.8222, -1.1428, 2.0441], [ 3.8222, 6.7527, 0.8391, 2.0829], [-1.1428, 0.8391, 5.0169, 0.7957], [ 2.0441, 2.0829, 0.7957, 6.241 ]]) array([[ 8.8277], [ 3.8222], [-1.1428], [ 2.0441]]) 1np.dot(y.T, np.dot(X, y))array([[ 1195.468]]) 12345Xm = np.matrix(X)ym = Xm[:, 0]Xmymym.T * Xm * ymmatrix([[ 8.8277, 3.8222, -1.1428, 2.0441], [ 3.8222, 6.7527, 0.8391, 2.0829], [-1.1428, 0.8391, 5.0169, 0.7957], [ 2.0441, 2.0829, 0.7957, 6.241 ]]) matrix([[ 8.8277], [ 3.8222], [-1.1428], [ 2.0441]]) matrix([[ 1195.468]]) 1Xm.I * Xmatrix([[ 1.0000e+00, 6.9616e-17, -4.0136e-17, 8.1258e-17], [ -2.3716e-17, 1.0000e+00, 2.2230e-17, -2.5721e-17], [ 1.0957e-16, 5.0783e-18, 1.0000e+00, 7.8658e-18], [ -5.7092e-17, -3.7777e-18, 6.2391e-18, 1.0000e+00]]) é«˜çº§æ•°ç»„è¾“å…¥è¾“å‡ºå†…å­˜æ˜ åƒæ–‡ä»¶12mmap = np.memmap('mymmap', dtype='float64', mode='w+', shape=(10000, 10000))mmapmemmap([[ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], ..., [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.], [ 0., 0., 0., ..., 0., 0., 0.]]) 1section = mmap[:5]1234section[:] = np.random.randn(5, 10000)mmap.flush()mmapdel mmapmemmap([[-1.273 , -0.1547, 0.7817, ..., 0.3421, 1.0272, -1.8742], [-0.3544, -3.1195, 0.1256, ..., -0.4476, 0.4863, -0.8311], [-1.1117, 0.8186, 2.3934, ..., 0.1061, 1.4123, 0.6489], ..., [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ]]) 12mmap = np.memmap('mymmap', dtype='float64', shape=(10000, 10000))mmapmemmap([[-1.273 , -0.1547, 0.7817, ..., 0.3421, 1.0272, -1.8742], [-0.3544, -3.1195, 0.1256, ..., -0.4476, 0.4863, -0.8311], [-1.1117, 0.8186, 2.3934, ..., 0.1061, 1.4123, 0.6489], ..., [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ]]) 12%xdel mmap!del mymmapNameError: name &apos;mmap&apos; is not defined C:\Users\Ewan\Downloads\pydata-book-master\mymmap The process cannot access the file because it is being used by another process. â€‹æ€§èƒ½å»ºè®®è¿žç»­å†…å­˜çš„é‡è¦æ€§12345arr_c = np.ones((1000, 1000), order='C')arr_f = np.ones((1000, 1000), order='F')arr_c.flagsarr_f.flagsarr_f.flags.f_contiguous C_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False C_CONTIGUOUS : False F_CONTIGUOUS : True OWNDATA : True WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False True 12%timeit arr_c.sum(1)%timeit arr_f.sum(1)1000 loops, best of 3: 848 Âµs per loop 1000 loops, best of 3: 582 Âµs per loop 1arr_f.copy('C').flagsC_CONTIGUOUS : True F_CONTIGUOUS : False OWNDATA : True WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False 12arr_c[:50].flags.contiguousarr_c[:, :50].flagsTrue C_CONTIGUOUS : False F_CONTIGUOUS : False OWNDATA : False WRITEABLE : True ALIGNED : True UPDATEIFCOPY : False 123%xdel arr_c%xdel arr_f%cd ..C:\Users\Ewan\Downloads â€‹å…¶ä»–åŠ é€Ÿæ‰‹æ®µ: Cython, f2py, C12345678910from numpy cimport ndarray, float64_tdef sum_elements(ndarray[float64_t] arr): cdef Py_ssize_t i, n = len(arr) cdef float64_t result = 0 for i in range(n): result += arr[i] return result]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GFW Break]]></title>
      <url>%2F2017%2F03%2F08%2FGFW-Break%2F</url>
      <content type="text"><![CDATA[ï¼ˆBetaç‰ˆæœ¬ï¼Œ ç•™å¾…ä»¥åŽå®Œå–„ï¼‰è½¯ä»¶ä¸‹è½½ç™¾åº¦ç½‘ç›˜é“¾æŽ¥ï¼šhttp://pan.baidu.com/s/1gftCmd1ä½¿ç”¨æ–¹æ³•è§£åŽ‹è§£åŽ‹åŽçš„ç›®å½•å¦‚ä¸‹ï¼šâ€‹æ‰“å¼€è½¯ä»¶ï¼šâ€‹ç•Œé¢å¦‚ä¸‹ï¼šâ€‹â€‹â€‹è¿™é‡Œéœ€è¦å¡«ä¸€äº›ä¸œè¥¿ï¼šServer IPServer PortPasswordå…·ä½“å€¼ï¼ˆsscat.txtï¼‰æˆ‘æ”¾åœ¨äº†ç½‘ç›˜é‡Œï¼Œé“¾æŽ¥ï¼šhttp://pan.baidu.com/s/1gftCmd1å¯åŠ¨è½¯ä»¶ï¼šâ€‹â€‹å³é”®è¿™ä¸ªå°é£žæœºå›¾æ ‡ï¼ˆå¯èƒ½ä½ çš„é¢œè‰²çœ‹èµ·æ¥ä¸ä¸€æ ·ï¼Œæ˜¯æš—è“è‰²ï¼‰ï¼Œä¼šå‡ºçŽ°å¦‚ä¸‹ç•Œé¢ï¼šå‹¾é€‰ä¸Šç¬¬ä¸€é¡¹ï¼Œç„¶åŽå°†é¼ æ ‡ç§»åˆ°ç¬¬äºŒé¡¹ä¸Šï¼šé€‰æ‹©PACæ¨¡å¼ï¼ˆè¿™ä¸ªæ¨¡å¼ä¼šè‡ªåŠ¨æ£€æµ‹ä½ æ‰€è¿›å…¥çš„ç½‘ç«™æ˜¯å¦éœ€è¦ç¿»å¢™ï¼Œæ‰€ä»¥é€‰æ‹©è¿™ä¸ªæ¨¡å¼å°±å¯ä»¥äº†ï¼Œå¦‚æžœä¸è¡Œçš„è¯ï¼Œå‹¾é€‰ä¸‹é¢çš„Globalå³å¯ï¼‰]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note Ch10]]></title>
      <url>%2F2017%2F03%2F08%2Fpython-data-analysis-learning-note-Ch10%2F</url>
      <content type="text"><![CDATA[æ—¶é—´åºåˆ—123456789from __future__ import divisionfrom pandas import Series, DataFrameimport pandas as pdfrom numpy.random import randnimport numpy as nppd.options.display.max_rows = 12np.set_printoptions(precision=4, suppress=True)import matplotlib.pyplot as pltplt.rc('figure', figsize=(12, 4))12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inlineæ—¥æœŸå’Œæ—¶é—´æ•°æ®ç±»åž‹åŠå·¥å…·123from datetime import datetimenow = datetime.now()nowdatetime.datetime(2017, 3, 8, 14, 47, 50, 32019) 1now.year, now.month, now.day(2017, 3, 8) è¿”å›žå€¼ï¼ˆå¤©æ•°ï¼Œç§’æ•°ï¼‰12delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)deltadatetime.timedelta(926, 56700) 1delta.days926 1delta.seconds56700 timedelta å¤©æ•°123from datetime import timedeltastart = datetime(2011, 1, 7)start + timedelta(12)datetime.datetime(2011, 1, 19, 0, 0) 1start - 2 * timedelta(12)datetime.datetime(2010, 12, 14, 0, 0) å­—ç¬¦ä¸²å’Œdatatimeçš„ç›¸äº’è½¬æ¢1stamp = datetime(2011, 1, 3)ä½¿ç”¨strç›´æŽ¥è½¬æ¢1str(stamp)&apos;2011-01-03 00:00:00&apos; æ ¼å¼åŒ–è½¬æ¢1stamp.strftime('%Y-%m-%d')&apos;2011-01-03&apos; é€†è½¬æ¢12value = '2011-01-03'datetime.strptime(value, '%Y-%m-%d')datetime.datetime(2011, 1, 3, 0, 0) æ‰¹é‡è½¬æ¢12datestrs = ['7/6/2011', '8/6/2011'][datetime.strptime(x, '%m/%d/%Y') for x in datestrs][datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)] æ€»æ˜¯å†™æ ¼å¼å¾ˆéº»çƒ¦ï¼Œç›´æŽ¥è°ƒç”¨parserè§£æž12from dateutil.parser import parseparse('2011-01-03')datetime.datetime(2011, 1, 3, 0, 0) å¯ä»¥è§£æžä»»æ„æ ¼å¼1parse('Jan 31, 1997 10:45 PM')datetime.datetime(1997, 1, 31, 22, 45) æŒ‡å®šæ ¼å¼1parse('6/12/2011', dayfirst=True)datetime.datetime(2011, 12, 6, 0, 0) 1datestrs[&apos;7/6/2011&apos;, &apos;8/6/2011&apos;] pandasçš„API12pd.to_datetime(datestrs)# note: output changed (no '00:00:00' anymore)DatetimeIndex([&apos;2011-07-06&apos;, &apos;2011-08-06&apos;], dtype=&apos;datetime64[ns]&apos;, freq=None) Noneä¹Ÿå¯ä»¥è½¬æ¢ï¼Œåªä¸è¿‡ä¼šå˜æˆç¼ºå¤±å€¼12idx = pd.to_datetime(datestrs + [None])idxDatetimeIndex([&apos;2011-07-06&apos;, &apos;2011-08-06&apos;, &apos;NaT&apos;], dtype=&apos;datetime64[ns]&apos;, freq=None) 1idx[2]NaT 1pd.isnull(idx)array([False, False, True], dtype=bool) æ—¶é—´åºåˆ—åŸºç¡€å°†è¡Œç´¢å¼•å˜æˆæ—¶é—´ç±»åž‹ï¼Œä¹Ÿå°±æ˜¯æ—¶é—´æˆ³12345from datetime import datetimedates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7), datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]ts = Series(np.random.randn(6), index=dates)ts2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 12type(ts)# note: output changed to "pandas.core.series.Series"pandas.core.series.Series æ‹¥æœ‰ä¸€ä¸ªç‰¹å®šçš„ç±»åž‹1ts.indexDatetimeIndex([&apos;2011-01-02&apos;, &apos;2011-01-05&apos;, &apos;2011-01-07&apos;, &apos;2011-01-08&apos;, &apos;2011-01-10&apos;, &apos;2011-01-12&apos;], dtype=&apos;datetime64[ns]&apos;, freq=None) å¯ä»¥ç›´æŽ¥è¿›è¡ŒåŠ æ³•è¿ç®—ï¼Œç›¸åŒçš„æ—¶é—´æˆ³ä¼šè¿›è¡ŒåŒ¹é…1ts + ts[::2]2011-01-02 -0.593708 2011-01-05 NaN 2011-01-07 -0.968984 2011-01-08 NaN 2011-01-10 -0.697394 2011-01-12 NaN dtype: float64 ä»¥çº³ç§’å½¢å¼å­˜å‚¨æ—¶é—´æˆ³12ts.index.dtype# note: output changed from dtype('datetime64[ns]') to dtype('&lt;M8[ns]')dtype(&apos;&lt;M8[ns]&apos;) è¡Œç´¢å¼•å°±ä¼šå˜æˆæ—¶é—´æˆ³ç±»åž‹123stamp = ts.index[0]stamp# note: output changed from &lt;Timestamp: 2011-01-02 00:00:00&gt; to Timestamp('2011-01-02 00:00:00')Timestamp(&apos;2011-01-02 00:00:00&apos;) ç´¢å¼•ã€é€‰å–ã€å­é›†æž„é€ æ—¶é—´æˆ³ç´¢å¼•ä¸Žæ­£å¸¸ç´¢å¼•è¡Œä¸ºä¸€æ ·12stamp = ts.index[2]ts[stamp]-0.4844920247591406 å¯ä»¥ç›´æŽ¥é€šè¿‡ä¼ å…¥ä¸Žè¡Œç´¢å¼•ç›¸åŒ¹é…çš„æ—¶é—´æˆ³è¿›è¡Œç´¢å¼•1ts['1/10/2011']-0.34869693931763396 æ¢ä¸ªæ ¼å¼ä¹Ÿå¯ä»¥ï¼Œä¼šè‡ªåŠ¨è½¬æ¢ä¸ºdatatimeï¼Œåªè¦æœ€åŽè½¬æ¢æˆçš„æ—¶é—´æˆ³æ˜¯ç›¸åŒçš„ï¼Œä»»æ„æ ¼å¼éƒ½å¯ä»¥1ts['20110110']-0.34869693931763396 é€šè¿‡periodså‚æ•°æ¥æŒ‡å®šå¾€åŽé¡ºå»¶çš„æ—¶é—´é•¿çŸ­123longer_ts = Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))longer_ts2000-01-01 0.871808 2000-01-02 -0.025158 2000-01-03 0.132813 2000-01-04 -2.006494 2000-01-05 -0.988423 2000-01-06 0.775930 ... 2002-09-21 -0.186519 2002-09-22 0.881745 2002-09-23 -1.335826 2002-09-24 0.418774 2002-09-25 0.970405 2002-09-26 0.636320 Freq: D, dtype: float64 æ—¶é—´æˆ³çš„ç‰¹æ®Šä¹‹å¤„åœ¨äºŽå¯ä»¥è¿›è¡Œå¹´ä»½ä»¥åŠæœˆä»½ç­‰çš„é€‰å–ï¼Œç›¸å½“äºŽä¸€ä¸ªå¤šç»´ç´¢å¼•1longer_ts['2001']2001-01-01 -1.799866 2001-01-02 0.499890 2001-01-03 -0.409970 2001-01-04 -0.808111 2001-01-05 -1.220433 2001-01-06 0.581235 ... 2001-12-26 -0.312186 2001-12-27 -0.804940 2001-12-28 -0.572741 2001-12-29 -0.175605 2001-12-30 0.693675 2001-12-31 -0.196274 Freq: D, dtype: float64 1longer_ts['2001-05']2001-05-01 -2.783535 2001-05-02 1.386292 2001-05-03 0.153705 2001-05-04 -0.571590 2001-05-05 -0.933012 2001-05-06 0.579244 ... 2001-05-26 0.080809 2001-05-27 0.652650 2001-05-28 0.862616 2001-05-29 -0.967580 2001-05-30 0.907069 2001-05-31 0.551137 Freq: D, dtype: float64 åŒæ ·å¯ä»¥è¿›è¡Œåˆ‡ç‰‡ï¼Œåªä¸è¿‡æ˜¯æŒ‰ç…§æ—¶é—´çš„å…ˆåŽåº¦é‡1ts[datetime(2011, 1, 7):]2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 1ts2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 è€Œä¸”åˆ‡ç‰‡ä¸éœ€è¦è¿›è¡Œç´¢å¼•åŒ¹é…ï¼Œåªéœ€è¦æŒ‡å®šæ—¶é—´èŒƒå›´å³å¯åˆ‡ç‰‡1ts['1/6/2011':'1/11/2011']2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 dtype: float64 ä¸€ä¸ªå¯ä»¥å®žçŽ°åŒæ ·åŠŸèƒ½çš„å†…ç½®æ–¹æ³•1ts.truncate(after='1/9/2011')2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 dtype: float64 è¿™é‡Œçš„freqå‚æ•°æŒ‡å®šäº†é€‰å–çš„é¢‘çŽ‡ï¼Œè¿™é‡Œçš„æ˜¯æ¯ä¸€ä¸ªæ˜ŸæœŸä¸‰12345dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')long_df = DataFrame(np.random.randn(100, 4), index=dates, columns=['Colorado', 'Texas', 'New York', 'Ohio'])long_df.ix['5-2001']ColoradoTexasNew YorkOhio2001-05-020.506207-1.1162180.6565750.2126062001-05-09-1.306963-0.054373-1.165053-1.3193612001-05-160.891692-0.4639001.6422670.6449722001-05-23-0.0252832.363886-0.3679880.8278822001-05-30-1.501301-2.5345530.2563690.268207å¸¦æœ‰é‡å¤ç´¢å¼•çš„æ—¶é—´åºåˆ—ç›´æŽ¥åˆ›å»ºæ—¶é—´æˆ³ç´¢å¼•1234dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000', '1/2/2000', '1/3/2000'])dup_ts = Series(np.arange(5), index=dates)dup_ts2000-01-01 0 2000-01-02 1 2000-01-02 2 2000-01-02 3 2000-01-03 4 dtype: int32 1dup_ts.index.is_uniqueFalse 1dup_ts['1/3/2000'] # not duplicated4 å¦‚æžœæœ‰é‡å¤çš„æ—¶é—´ç´¢å¼•ï¼Œåˆ™ä¼šå°†æ»¡è¶³æ¡ä»¶çš„å…¨éƒ¨è¾“å‡º1dup_ts['1/2/2000'] # duplicated2000-01-02 1 2000-01-02 2 2000-01-02 3 dtype: int32 å› æ­¤å¯ä»¥ç›´æŽ¥æ ¹æ®æ—¶é—´æˆ³è¿›è¡Œç´¢å¼•12grouped = dup_ts.groupby(level=0)grouped.mean()2000-01-01 0 2000-01-02 2 2000-01-03 4 dtype: int32 1grouped.count()2000-01-01 1 2000-01-02 3 2000-01-03 1 dtype: int64 æ—¥æœŸçš„èŒƒå›´ã€é¢‘çŽ‡ä»¥åŠç§»åŠ¨pandasä¸­çš„æ—¶é—´åºåˆ—ä¸€èˆ¬è¢«è®¤ä¸ºæ˜¯ä¸è§„åˆ™çš„ï¼Œä¹Ÿå°±æ˜¯è¯´æ²¡æœ‰å›ºå®šçš„é¢‘çŽ‡ã€‚ä½†æ˜¯æœ‰æ—¶å€™éœ€è¦ä»¥æŸç§ç›¸å¯¹å›ºå®šçš„é¢‘çŽ‡è¿›è¡Œåˆ†æžï¼Œæ¯”å¦‚æ¯æ—¥ã€æ¯æœˆã€æ¯15åˆ†é’Ÿç­‰ï¼ˆè¿™æ ·è‡ªç„¶ä¼šåœ¨æ—¶é—´åºåˆ—ä¸­å¼•å…¥ç¼ºå¤±å€¼ï¼‰ã€‚pandasæ‹¥æœ‰ä¸€æ•´å¥—æ ‡å‡†æ—¶é—´åºåˆ—é¢‘çŽ‡ä»¥åŠç”¨äºŽé‡é‡‡æ ·ã€é¢‘çŽ‡æŽ¨æ–­ã€ç”Ÿæˆå›ºå®šé¢‘çŽ‡æ—¥æœŸèŒƒå›´çš„å·¥å…·1ts2011-01-02 -0.296854 2011-01-05 -1.968663 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-10 -0.348697 2011-01-12 0.102276 dtype: float64 ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¹‹å‰é‚£ä¸ªæ—¶é—´åºåˆ—è½¬æ¢ä¸ºä¸€ä¸ªå…·æœ‰å›ºå®šé¢‘çŽ‡ï¼ˆæ¯æ—¥ï¼‰çš„æ—¶é—´åºåˆ—ã€‚åªéœ€è¦è°ƒç”¨resampleå³å¯1ts.resample('D').mean()2011-01-02 -0.296854 2011-01-03 NaN 2011-01-04 NaN 2011-01-05 -1.968663 2011-01-06 NaN 2011-01-07 -0.484492 2011-01-08 -0.517927 2011-01-09 NaN 2011-01-10 -0.348697 2011-01-11 NaN 2011-01-12 0.102276 Freq: D, dtype: float64 ç”Ÿæˆæ—¥æœŸèŒƒå›´data_rangeå‡½æ•°ï¼Œ æŒ‡å®šå§‹æœ«12index = pd.date_range('4/1/2012', '6/1/2012')indexDatetimeIndex([&apos;2012-04-01&apos;, &apos;2012-04-02&apos;, &apos;2012-04-03&apos;, &apos;2012-04-04&apos;, &apos;2012-04-05&apos;, &apos;2012-04-06&apos;, &apos;2012-04-07&apos;, &apos;2012-04-08&apos;, &apos;2012-04-09&apos;, &apos;2012-04-10&apos;, &apos;2012-04-11&apos;, &apos;2012-04-12&apos;, &apos;2012-04-13&apos;, &apos;2012-04-14&apos;, &apos;2012-04-15&apos;, &apos;2012-04-16&apos;, &apos;2012-04-17&apos;, &apos;2012-04-18&apos;, &apos;2012-04-19&apos;, &apos;2012-04-20&apos;, &apos;2012-04-21&apos;, &apos;2012-04-22&apos;, &apos;2012-04-23&apos;, &apos;2012-04-24&apos;, &apos;2012-04-25&apos;, &apos;2012-04-26&apos;, &apos;2012-04-27&apos;, &apos;2012-04-28&apos;, &apos;2012-04-29&apos;, &apos;2012-04-30&apos;, &apos;2012-05-01&apos;, &apos;2012-05-02&apos;, &apos;2012-05-03&apos;, &apos;2012-05-04&apos;, &apos;2012-05-05&apos;, &apos;2012-05-06&apos;, &apos;2012-05-07&apos;, &apos;2012-05-08&apos;, &apos;2012-05-09&apos;, &apos;2012-05-10&apos;, &apos;2012-05-11&apos;, &apos;2012-05-12&apos;, &apos;2012-05-13&apos;, &apos;2012-05-14&apos;, &apos;2012-05-15&apos;, &apos;2012-05-16&apos;, &apos;2012-05-17&apos;, &apos;2012-05-18&apos;, &apos;2012-05-19&apos;, &apos;2012-05-20&apos;, &apos;2012-05-21&apos;, &apos;2012-05-22&apos;, &apos;2012-05-23&apos;, &apos;2012-05-24&apos;, &apos;2012-05-25&apos;, &apos;2012-05-26&apos;, &apos;2012-05-27&apos;, &apos;2012-05-28&apos;, &apos;2012-05-29&apos;, &apos;2012-05-30&apos;, &apos;2012-05-31&apos;, &apos;2012-06-01&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) åªæŒ‡å®šèµ·å§‹ï¼Œ ä»¥åŠé•¿åº¦1pd.date_range(start='4/1/2012', periods=20)DatetimeIndex([&apos;2012-04-01&apos;, &apos;2012-04-02&apos;, &apos;2012-04-03&apos;, &apos;2012-04-04&apos;, &apos;2012-04-05&apos;, &apos;2012-04-06&apos;, &apos;2012-04-07&apos;, &apos;2012-04-08&apos;, &apos;2012-04-09&apos;, &apos;2012-04-10&apos;, &apos;2012-04-11&apos;, &apos;2012-04-12&apos;, &apos;2012-04-13&apos;, &apos;2012-04-14&apos;, &apos;2012-04-15&apos;, &apos;2012-04-16&apos;, &apos;2012-04-17&apos;, &apos;2012-04-18&apos;, &apos;2012-04-19&apos;, &apos;2012-04-20&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) åªæŒ‡å®šç»“å°¾ï¼Œä»¥åŠé•¿åº¦1pd.date_range(end='6/1/2012', periods=20)DatetimeIndex([&apos;2012-05-13&apos;, &apos;2012-05-14&apos;, &apos;2012-05-15&apos;, &apos;2012-05-16&apos;, &apos;2012-05-17&apos;, &apos;2012-05-18&apos;, &apos;2012-05-19&apos;, &apos;2012-05-20&apos;, &apos;2012-05-21&apos;, &apos;2012-05-22&apos;, &apos;2012-05-23&apos;, &apos;2012-05-24&apos;, &apos;2012-05-25&apos;, &apos;2012-05-26&apos;, &apos;2012-05-27&apos;, &apos;2012-05-28&apos;, &apos;2012-05-29&apos;, &apos;2012-05-30&apos;, &apos;2012-05-31&apos;, &apos;2012-06-01&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) æŒ‡å®šå§‹æœ«ï¼Œä»¥åŠé‡‡æ ·é¢‘çŽ‡ï¼Œ BM = business end of month1pd.date_range('1/1/2000', '12/1/2000', freq='BM')DatetimeIndex([&apos;2000-01-31&apos;, &apos;2000-02-29&apos;, &apos;2000-03-31&apos;, &apos;2000-04-28&apos;, &apos;2000-05-31&apos;, &apos;2000-06-30&apos;, &apos;2000-07-31&apos;, &apos;2000-08-31&apos;, &apos;2000-09-29&apos;, &apos;2000-10-31&apos;, &apos;2000-11-30&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;BM&apos;) é»˜è®¤peroidsæŒ‡çš„æ˜¯å¤©æ•°1pd.date_range('5/2/2012 12:56:31', periods=5)DatetimeIndex([&apos;2012-05-02 12:56:31&apos;, &apos;2012-05-03 12:56:31&apos;, &apos;2012-05-04 12:56:31&apos;, &apos;2012-05-05 12:56:31&apos;, &apos;2012-05-06 12:56:31&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) å¯ä»¥çœç•¥æ—¶é—´æˆ³1pd.date_range('5/2/2012 12:56:31', periods=5, normalize=True)DatetimeIndex([&apos;2012-05-02&apos;, &apos;2012-05-03&apos;, &apos;2012-05-04&apos;, &apos;2012-05-05&apos;, &apos;2012-05-06&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) é¢‘çŽ‡å’Œæ—¥æœŸåç§»é‡åç§»é‡å¯ä»¥é‡‡ç”¨ç‰¹å®šå•ä½çš„æ—¶é—´å¯¹è±¡123from pandas.tseries.offsets import Hour, Minutehour = Hour()hour&lt;Hour&gt; 4ä¸ªå°æ—¶ï¼Œç®€å•ç²—æš´12four_hours = Hour(4)four_hours&lt;4 * Hours&gt; æ¯éš”å››ä¸ªå°æ—¶è¿›è¡Œé‡‡æ ·1pd.date_range('1/1/2000', '1/3/2000 23:59', freq='4h')DatetimeIndex([&apos;2000-01-01 00:00:00&apos;, &apos;2000-01-01 04:00:00&apos;, &apos;2000-01-01 08:00:00&apos;, &apos;2000-01-01 12:00:00&apos;, &apos;2000-01-01 16:00:00&apos;, &apos;2000-01-01 20:00:00&apos;, &apos;2000-01-02 00:00:00&apos;, &apos;2000-01-02 04:00:00&apos;, &apos;2000-01-02 08:00:00&apos;, &apos;2000-01-02 12:00:00&apos;, &apos;2000-01-02 16:00:00&apos;, &apos;2000-01-02 20:00:00&apos;, &apos;2000-01-03 00:00:00&apos;, &apos;2000-01-03 04:00:00&apos;, &apos;2000-01-03 08:00:00&apos;, &apos;2000-01-03 12:00:00&apos;, &apos;2000-01-03 16:00:00&apos;, &apos;2000-01-03 20:00:00&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;4H&apos;) ä¸¤ä¸ªåŠå°æ—¶1Hour(2) + Minute(30)&lt;150 * Minutes&gt; ä¹Ÿå¯ä»¥ç›´æŽ¥ä½¿ç”¨è¿™ç§ç±»ä¼¼äºŽè‡ªç„¶è¯­è¨€çš„å½¢å¼1pd.date_range('1/1/2000', periods=10, freq='1h30min')DatetimeIndex([&apos;2000-01-01 00:00:00&apos;, &apos;2000-01-01 01:30:00&apos;, &apos;2000-01-01 03:00:00&apos;, &apos;2000-01-01 04:30:00&apos;, &apos;2000-01-01 06:00:00&apos;, &apos;2000-01-01 07:30:00&apos;, &apos;2000-01-01 09:00:00&apos;, &apos;2000-01-01 10:30:00&apos;, &apos;2000-01-01 12:00:00&apos;, &apos;2000-01-01 13:30:00&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;90T&apos;) Week of month dates ï¼ˆWOMæ—¥æœŸï¼‰æ¯æœˆç¬¬ä¸‰ä¸ªæ˜ŸæœŸäº”12rng = pd.date_range('1/1/2012', '9/1/2012', freq='WOM-3FRI')list(rng)[Timestamp(&apos;2012-01-20 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-02-17 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-03-16 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-04-20 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-05-18 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-06-15 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-07-20 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;), Timestamp(&apos;2012-08-17 00:00:00&apos;, offset=&apos;WOM-3FRI&apos;)] ç§»åŠ¨ï¼ˆè¶…å‰æˆ–æ»žåŽï¼‰æ•°æ®123ts = Series(np.random.randn(4), index=pd.date_range('1/1/2000', periods=4, freq='M'))ts2000-01-31 1.294798 2000-02-29 -1.907732 2000-03-31 -1.407750 2000-04-30 0.544825 Freq: M, dtype: float64 æ•´ä½“æ•°æ®å‰ç§»1ts.shift(2)2000-01-31 NaN 2000-02-29 NaN 2000-03-31 1.294798 2000-04-30 -1.907732 Freq: M, dtype: float64 æ•´ä½“æ•°æ®åŽç§»ï¼Œæœ‰ç‚¹ç±»ä¼¼äºŽä½è¿ç®—ä¸­çš„ç§»ä½æ“ä½œ1ts.shift(-2)2000-01-31 -1.407750 2000-02-29 0.544825 2000-03-31 NaN 2000-04-30 NaN Freq: M, dtype: float64 ç§»ä½ä¹‹åŽæ•°æ®å¯¹é½1ts / ts.shift(1) - 12000-01-31 NaN 2000-02-29 -2.473382 2000-03-31 -0.262082 2000-04-30 -1.387018 Freq: M, dtype: float64 åŠ å…¥freqä¹‹åŽå°±æ˜¯åœ¨è¡Œç´¢å¼•ä¸Šè¿›è¡Œæ—¶é—´å‰ç§»1ts.shift(2, freq='M')2000-03-31 1.294798 2000-04-30 -1.907732 2000-05-31 -1.407750 2000-06-30 0.544825 Freq: M, dtype: float64 åœ¨å¤©æ•°ä¸Šè¿›è¡Œå‰ç§»1ts.shift(3, freq='D')2000-02-03 1.294798 2000-03-03 -1.907732 2000-04-03 -1.407750 2000-05-03 0.544825 dtype: float64 å¦ä¸€ç§å®žçŽ°æ–¹å¼1ts.shift(1, freq='3D')2000-02-03 1.294798 2000-03-03 -1.907732 2000-04-03 -1.407750 2000-05-03 0.544825 dtype: float64 æ¢ä¸€ä¸ªé¢‘çŽ‡1ts.shift(1, freq='90T')2000-01-31 01:30:00 1.294798 2000-02-29 01:30:00 -1.907732 2000-03-31 01:30:00 -1.407750 2000-04-30 01:30:00 0.544825 Freq: M, dtype: float64 é€šè¿‡åç§»é‡å¯¹æ—¥æœŸè¿›è¡Œä½ç§»123from pandas.tseries.offsets import Day, MonthEndnow = datetime(2011, 11, 17)now + 3 * Day()Timestamp(&apos;2011-11-20 00:00:00&apos;) ç›´æŽ¥ç§»ä½åˆ°æœˆæœ«ï¼Œæ˜¯ä¸€ä¸ªç›¸å¯¹ä½ç§»1now + MonthEnd()Timestamp(&apos;2011-11-30 00:00:00&apos;) ä¼ å…¥çš„å‚æ•°è¡¨ç¤ºç¬¬å‡ ä¸ªæœˆçš„æœˆæœ«1now + MonthEnd(2)Timestamp(&apos;2011-12-31 00:00:00&apos;) æ¢ä¸€ç§æ–¹å¼å®žçŽ°ï¼Œâ€œä¸»è¯­â€ä¸åŒ12offset = MonthEnd()offset.rollforward(now)Timestamp(&apos;2011-11-30 00:00:00&apos;) å¾€å›žèµ°ï¼Œä¸Šä¸€ä¸ªæœˆçš„æœˆæœ«1offset.rollback(now)Timestamp(&apos;2011-10-31 00:00:00&apos;) å¯¹æ—¥æœŸè¿›è¡Œç§»ä½ä¹‹åŽåˆ†ç»„123ts = Series(np.random.randn(20), index=pd.date_range('1/15/2000', periods=20, freq='4d'))ts.groupby(offset.rollforward).mean()2000-01-31 -0.610639 2000-02-29 0.029121 2000-03-31 -0.089587 dtype: float64 å¦ä¸€ç§æ–¹å¼ä¹Ÿå¯ä»¥è¾¾åˆ°ç›¸åŒçš„æ•ˆæžœ1ts.resample('M', how='mean')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: how in .resample() is deprecated the new syntax is .resample(...).mean() if __name__ == &apos;__main__&apos;: 2000-01-31 -0.610639 2000-02-29 0.029121 2000-03-31 -0.089587 Freq: M, dtype: float64 æ—¶åŒºå¤„ç†æ˜¾ç¤ºä¸€äº›æ—¶åŒº12import pytzpytz.common_timezones[-5:][&apos;US/Eastern&apos;, &apos;US/Hawaii&apos;, &apos;US/Mountain&apos;, &apos;US/Pacific&apos;, &apos;UTC&apos;] æ˜¾ç¤ºæŸä¸ªæ—¶åŒºçš„å…·ä½“ä¿¡æ¯12tz = pytz.timezone('US/Eastern')tz&lt;DstTzInfo &apos;US/Eastern&apos; LMT-1 day, 19:04:00 STD&gt; æœ¬åœ°åŒ–å’Œè½¬æ¢123rng = pd.date_range('3/9/2012 9:30', periods=6, freq='D')ts = Series(np.random.randn(len(rng)), index=rng)ts2012-03-09 09:30:00 0.065144 2012-03-10 09:30:00 -0.391505 2012-03-11 09:30:00 1.207495 2012-03-12 09:30:00 1.516354 2012-03-13 09:30:00 -0.253149 2012-03-14 09:30:00 -0.768138 Freq: D, dtype: float64 æ²¡æœ‰æŒ‡å®šæ—¶åŒºçš„æ—¶å€™é»˜è®¤æ—¶åŒºä¸ºNone1print(ts.index.tz)None â€‹æŒ‡å®šæ—¶åŒº1pd.date_range('3/9/2012 9:30', periods=10, freq='D', tz='UTC')DatetimeIndex([&apos;2012-03-09 09:30:00+00:00&apos;, &apos;2012-03-10 09:30:00+00:00&apos;, &apos;2012-03-11 09:30:00+00:00&apos;, &apos;2012-03-12 09:30:00+00:00&apos;, &apos;2012-03-13 09:30:00+00:00&apos;, &apos;2012-03-14 09:30:00+00:00&apos;, &apos;2012-03-15 09:30:00+00:00&apos;, &apos;2012-03-16 09:30:00+00:00&apos;, &apos;2012-03-17 09:30:00+00:00&apos;, &apos;2012-03-18 09:30:00+00:00&apos;], dtype=&apos;datetime64[ns, UTC]&apos;, freq=&apos;D&apos;) è¿›è¡Œæ—¶åŒºçš„è½¬æ¢12ts_utc = ts.tz_localize('UTC')ts_utc2012-03-09 09:30:00+00:00 0.065144 2012-03-10 09:30:00+00:00 -0.391505 2012-03-11 09:30:00+00:00 1.207495 2012-03-12 09:30:00+00:00 1.516354 2012-03-13 09:30:00+00:00 -0.253149 2012-03-14 09:30:00+00:00 -0.768138 Freq: D, dtype: float64 1ts_utc.indexDatetimeIndex([&apos;2012-03-09 09:30:00+00:00&apos;, &apos;2012-03-10 09:30:00+00:00&apos;, &apos;2012-03-11 09:30:00+00:00&apos;, &apos;2012-03-12 09:30:00+00:00&apos;, &apos;2012-03-13 09:30:00+00:00&apos;, &apos;2012-03-14 09:30:00+00:00&apos;], dtype=&apos;datetime64[ns, UTC]&apos;, freq=&apos;D&apos;) ç»§ç»­è½¬æ¢1ts_utc.tz_convert('US/Eastern')2012-03-09 04:30:00-05:00 0.065144 2012-03-10 04:30:00-05:00 -0.391505 2012-03-11 05:30:00-04:00 1.207495 2012-03-12 05:30:00-04:00 1.516354 2012-03-13 05:30:00-04:00 -0.253149 2012-03-14 05:30:00-04:00 -0.768138 Freq: D, dtype: float64 ä¾æ—§æ˜¯è½¬æ¢12ts_eastern = ts.tz_localize('US/Eastern')ts_eastern.tz_convert('UTC')2012-03-09 14:30:00+00:00 0.065144 2012-03-10 14:30:00+00:00 -0.391505 2012-03-11 13:30:00+00:00 1.207495 2012-03-12 13:30:00+00:00 1.516354 2012-03-13 13:30:00+00:00 -0.253149 2012-03-14 13:30:00+00:00 -0.768138 Freq: D, dtype: float64 è½¬è½¬è½¬ts_eastern.tz_convert(â€˜Europe/Berlinâ€™)è½¬æ¢ä¹‹å‰å¿…é¡»è¦è¿›è¡Œæœ¬åœ°åŒ–1ts.index.tz_localize('Asia/Shanghai')æ“ä½œæ—¶åŒºæ„è¯†åž‹TimeStampå¯¹è±¡åˆå§‹åŒ–æ—¶é—´æˆ³ï¼Œæœ¬åœ°åŒ–ï¼Œæ—¶åŒºè½¬æ¢123stamp = pd.Timestamp('2011-03-12 04:00')stamp_utc = stamp.tz_localize('utc')stamp_utc.tz_convert('US/Eastern')Timestamp(&apos;2011-03-11 23:00:00-0500&apos;, tz=&apos;US/Eastern&apos;) æ˜¾å¼åœ°åˆå§‹åŒ–12stamp_moscow = pd.Timestamp('2011-03-12 04:00', tz='Europe/Moscow')stamp_moscowTimestamp(&apos;2011-03-12 04:00:00+0300&apos;, tz=&apos;Europe/Moscow&apos;) è‡ª1970å¹´1æœˆ1æ—¥èµ·è®¡ç®—çš„çº³ç§’æ•°1stamp_utc.value1299902400000000000 è¿™ä¸ªå€¼æ˜¯ç»å¯¹çš„1stamp_utc.tz_convert('US/Eastern').value1299902400000000000 1234# 30 minutes before DST transitionfrom pandas.tseries.offsets import Hourstamp = pd.Timestamp('2012-03-12 01:30', tz='US/Eastern')stampTimestamp(&apos;2012-03-12 01:30:00-0400&apos;, tz=&apos;US/Eastern&apos;) è¿›è¡Œæ—¶é—´çš„ä½ç§»1stamp + Hour()Timestamp(&apos;2012-03-12 02:30:00-0400&apos;, tz=&apos;US/Eastern&apos;) 123# 90 minutes before DST transitionstamp = pd.Timestamp('2012-11-04 00:30', tz='US/Eastern')stampTimestamp(&apos;2012-11-04 00:30:00-0400&apos;, tz=&apos;US/Eastern&apos;) 1stamp + 2 * Hour()Timestamp(&apos;2012-11-04 01:30:00-0500&apos;, tz=&apos;US/Eastern&apos;) ä¸åŒæ—¶åŒºä¹‹é—´çš„è¿ç®—123rng = pd.date_range('3/7/2012 9:30', periods=10, freq='B')ts = Series(np.random.randn(len(rng)), index=rng)ts2012-03-07 09:30:00 -0.461750 2012-03-08 09:30:00 0.947394 2012-03-09 09:30:00 0.703239 2012-03-12 09:30:00 0.266519 2012-03-13 09:30:00 0.302334 2012-03-14 09:30:00 -0.000725 2012-03-15 09:30:00 0.305446 2012-03-16 09:30:00 -1.605358 2012-03-19 09:30:00 1.306474 2012-03-20 09:30:00 0.865511 Freq: B, dtype: float64 æœ€ç»ˆç»“æžœä¼šå˜æˆUTC1234ts1 = ts[:7].tz_localize('Europe/London')ts2 = ts1[2:].tz_convert('Europe/Moscow')result = ts1 + ts2result.indexDatetimeIndex([&apos;2012-03-07 09:30:00+00:00&apos;, &apos;2012-03-08 09:30:00+00:00&apos;, &apos;2012-03-09 09:30:00+00:00&apos;, &apos;2012-03-12 09:30:00+00:00&apos;, &apos;2012-03-13 09:30:00+00:00&apos;, &apos;2012-03-14 09:30:00+00:00&apos;, &apos;2012-03-15 09:30:00+00:00&apos;], dtype=&apos;datetime64[ns, UTC]&apos;, freq=&apos;B&apos;) æ—¶æœŸåŠå…¶ç®—æœ¯è¿ç®—12p = pd.Period(2007, freq='A-DEC')pPeriod(&apos;2007&apos;, &apos;A-DEC&apos;) 1p + 5Period(&apos;2012&apos;, &apos;A-DEC&apos;) 1p - 2Period(&apos;2005&apos;, &apos;A-DEC&apos;) 1pd.Period('2014', freq='A-DEC') - p7 12rng = pd.period_range('1/1/2000', '6/30/2000', freq='M')rngPeriodIndex([&apos;2000-01&apos;, &apos;2000-02&apos;, &apos;2000-03&apos;, &apos;2000-04&apos;, &apos;2000-05&apos;, &apos;2000-06&apos;], dtype=&apos;int64&apos;, freq=&apos;M&apos;) 1Series(np.random.randn(6), index=rng)2000-01 0.061389 2000-02 0.059265 2000-03 0.779627 2000-04 -0.068995 2000-05 -0.451276 2000-06 -1.531821 Freq: M, dtype: float64 123values = ['2001Q3', '2002Q2', '2003Q1']index = pd.PeriodIndex(values, freq='Q-DEC')indexPeriodIndex([&apos;2001Q3&apos;, &apos;2002Q2&apos;, &apos;2003Q1&apos;], dtype=&apos;int64&apos;, freq=&apos;Q-DEC&apos;) æ—¶åŒºçš„é¢‘çŽ‡è½¬æ¢ä»¥åäºŒæœˆä¸ºç»“å°¾çš„ä¸€ä¸ªå¹´æ—¶æœŸ12p = pd.Period('2007', freq='A-DEC')p.asfreq('M', how='start')Period(&apos;2007-01&apos;, &apos;M&apos;) 1p.asfreq('M', how='end')Period(&apos;2007-12&apos;, &apos;M&apos;) ä»¥å…­æœˆä»½ç»“å°¾çš„ä¸€ä¸ªå¹´æ—¶æœŸ12p = pd.Period('2007', freq='A-JUN')p.asfreq('M', 'start')Period(&apos;2006-07&apos;, &apos;M&apos;) 1p.asfreq('M', 'end')Period(&apos;2007-06&apos;, &apos;M&apos;) 2007å¹´8æœˆæ˜¯å±žäºŽä»¥å…­æœˆç»“å°¾çš„2008å¹´çš„æ—¶æœŸä¸­12p = pd.Period('Aug-2007', 'M')p.asfreq('A-JUN')Period(&apos;2008&apos;, &apos;A-JUN&apos;) ç›¸å½“äºŽä¸€ä¸ªæ‰¹é‡æ“ä½œ123rng = pd.period_range('2006', '2009', freq='A-DEC')ts = Series(np.random.randn(len(rng)), index=rng)ts2006 0.634252 2007 -0.738716 2008 0.398145 2009 -1.226529 Freq: A-DEC, dtype: float64 1ts.asfreq('M', how='start')2006-01 0.634252 2007-01 -0.738716 2008-01 0.398145 2009-01 -1.226529 Freq: M, dtype: float64 1ts.asfreq('B', how='end')2006-12-29 0.634252 2007-12-31 -0.738716 2008-12-31 0.398145 2009-12-31 -1.226529 Freq: B, dtype: float64 æŒ‰å­£åº¦è®¡ç®—çš„æ—¶é—´é¢‘çŽ‡ä»¥ä¸€æœˆä¸ºæˆªæ­¢çš„ç¬¬å››ä¸ªå­£åº¦12p = pd.Period('2012Q4', freq='Q-JAN')pPeriod(&apos;2012Q4&apos;, &apos;Q-JAN&apos;) ç¬¬å››ä¸ªå­£åº¦çš„èµ·å§‹æ—¥1p.asfreq('D', 'start')Period(&apos;2011-11-01&apos;, &apos;D&apos;) ç»“æŸæ—¥1p.asfreq('D', 'end')Period(&apos;2012-01-31&apos;, &apos;D&apos;) æˆªæ­¢æ—¥å‰ä¸€å¤©çš„ä¸‹åˆå››ç‚¹12p4pm = (p.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60p4pmPeriod(&apos;2012-01-30 16:00&apos;, &apos;T&apos;) è½¬åŒ–æˆæ—¶é—´æˆ³å¯¹è±¡1p4pm.to_timestamp()Timestamp(&apos;2012-01-30 16:00:00&apos;) 123rng = pd.period_range('2011Q3', '2012Q4', freq='Q-JAN')ts = Series(np.arange(len(rng)), index=rng)ts2011Q3 0 2011Q4 1 2012Q1 2 2012Q2 3 2012Q3 4 2012Q4 5 Freq: Q-JAN, dtype: int32 æ‰¹é‡è½¬åŒ–ä¸ºæ—¶é—´æˆ³123new_rng = (rng.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60ts.index = new_rng.to_timestamp()ts2010-10-28 16:00:00 0 2011-01-28 16:00:00 1 2011-04-28 16:00:00 2 2011-07-28 16:00:00 3 2011-10-28 16:00:00 4 2012-01-30 16:00:00 5 dtype: int32 å°†æ—¶é—´æˆ³è½¬åŒ–ä¸ºæ—¶æœŸï¼ˆä»¥åŠå…¶é€†è¿‡ç¨‹ï¼‰1234rng = pd.date_range('1/1/2000', periods=3, freq='M')ts = Series(randn(3), index=rng)pts = ts.to_period()ts2000-01-31 0.239752 2000-02-29 -0.469201 2000-03-31 2.835243 Freq: M, dtype: float64 é»˜è®¤ä»¥æœˆä»½ä¸ºå•ä½è¿›è¡Œè½¬åŒ–1pts2000-01 0.239752 2000-02 -0.469201 2000-03 2.835243 Freq: M, dtype: float64 è½¬åŒ–ä¸ºæœˆä»½ä¸ºå•ä½çš„æ—¶æœŸ123rng = pd.date_range('1/29/2000', periods=6, freq='D')ts2 = Series(randn(6), index=rng)ts2.to_period('M')2000-01 1.126773 2000-01 -0.979309 2000-01 -0.784376 2000-02 -1.490820 2000-02 1.125043 2000-02 0.421830 Freq: M, dtype: float64 12pts = ts.to_period()pts2000-01 0.239752 2000-02 -0.469201 2000-03 2.835243 Freq: M, dtype: float64 é€†å‘è½¬æ¢1pts.to_timestamp(how='end')2000-01-31 0.239752 2000-02-29 -0.469201 2000-03-31 2.835243 Freq: M, dtype: float64 é€šè¿‡æ•°ç»„åˆ›å»ºPeriodIndex12data = pd.read_csv('ch08/macrodata.csv')data.year0 1959.0 1 1959.0 2 1959.0 3 1959.0 4 1960.0 5 1960.0 ... 197 2008.0 198 2008.0 199 2008.0 200 2009.0 201 2009.0 202 2009.0 Name: year, dtype: float64 1data.quarter0 1.0 1 2.0 2 3.0 3 4.0 4 1.0 5 2.0 ... 197 2.0 198 3.0 199 4.0 200 1.0 201 2.0 202 3.0 Name: quarter, dtype: float64 å°†å¹´ä»½å’Œå­£åº¦æ•°æ®ç»Ÿä¸€èµ·æ¥è½¬åŒ–ä¸ºæ—¶æœŸç´¢å¼•æ•°æ®12index = pd.PeriodIndex(year=data.year, quarter=data.quarter, freq='Q-DEC')indexPeriodIndex([&apos;1959Q1&apos;, &apos;1959Q2&apos;, &apos;1959Q3&apos;, &apos;1959Q4&apos;, &apos;1960Q1&apos;, &apos;1960Q2&apos;, &apos;1960Q3&apos;, &apos;1960Q4&apos;, &apos;1961Q1&apos;, &apos;1961Q2&apos;, ... &apos;2007Q2&apos;, &apos;2007Q3&apos;, &apos;2007Q4&apos;, &apos;2008Q1&apos;, &apos;2008Q2&apos;, &apos;2008Q3&apos;, &apos;2008Q4&apos;, &apos;2009Q1&apos;, &apos;2009Q2&apos;, &apos;2009Q3&apos;], dtype=&apos;int64&apos;, length=203, freq=&apos;Q-DEC&apos;) 12data.index = indexdata.infl1959Q1 0.00 1959Q2 2.34 1959Q3 2.74 1959Q4 0.27 1960Q1 2.31 1960Q2 0.14 ... 2008Q2 8.53 2008Q3 -3.16 2008Q4 -8.79 2009Q1 0.94 2009Q2 3.37 2009Q3 3.56 Freq: Q-DEC, Name: infl, dtype: float64 é‡é‡‡æ ·ä»¥åŠé¢‘çŽ‡è½¬æ¢ç›¸å½“äºŽè¿›è¡Œäº†ä¸€æ¬¡åˆ†ç»„æ“ä½œ123rng = pd.date_range('1/1/2000', periods=100, freq='D')ts = Series(randn(len(rng)), index=rng)ts.resample('M', how='mean')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: how in .resample() is deprecated the new syntax is .resample(...).mean() app.launch_new_instance() 2000-01-31 -0.055153 2000-02-29 0.189412 2000-03-31 -0.075940 2000-04-30 -0.239036 Freq: M, dtype: float64 æ¢ä¸ªç´¢å¼•çš„å½¢å¼1ts.resample('M', how='mean', kind='period')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: how in .resample() is deprecated the new syntax is .resample(...).mean() if __name__ == &apos;__main__&apos;: 2000-01 -0.055153 2000-02 0.189412 2000-03 -0.075940 2000-04 -0.239036 Freq: M, dtype: float64 é™é‡‡æ ·æŒ‰ç…§åˆ†é’Ÿè¿›è¡Œé‡‡æ ·123rng = pd.date_range('1/1/2000', periods=12, freq='T')ts = Series(np.arange(12), index=rng)ts2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 2000-01-01 00:09:00 9 2000-01-01 00:10:00 10 2000-01-01 00:11:00 11 Freq: T, dtype: int32 æ¯5åˆ†é’Ÿé™é‡‡æ ·12ts.resample('5min').sum()# note: output changed (as the default changed from closed='right', label='right' to closed='left', label='left'2000-01-01 00:00:00 10 2000-01-01 00:05:00 35 2000-01-01 00:10:00 21 Freq: 5T, dtype: int32 1ts.resample('5min', closed='left').sum()2000-01-01 00:00:00 10 2000-01-01 00:05:00 35 2000-01-01 00:10:00 21 Freq: 5T, dtype: int32 1ts.resample('5min', closed='left', label='left').sum()2000-01-01 00:00:00 10 2000-01-01 00:05:00 35 2000-01-01 00:10:00 21 Freq: 5T, dtype: int32 åŠ äº†ä¸ªæ—¶é—´çš„åç§»1ts.resample('5min', loffset='-1s').sum()1999-12-31 23:59:59 10 2000-01-01 00:04:59 35 2000-01-01 00:09:59 21 Freq: 5T, dtype: int32 Open-High-Low-Close (OHLC) é™é‡‡æ ·1ts2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 2000-01-01 00:09:00 9 2000-01-01 00:10:00 10 2000-01-01 00:11:00 11 Freq: T, dtype: int32 ä»¥5åˆ†é’Ÿä¸ºå•ä½12ts.resample('5min').ohlc()# note: output changed because of changed defaultsopenhighlowclose2000-01-01 00:00:0004042000-01-01 00:05:0059592000-01-01 00:10:0010111011é€šè¿‡GroupByè¿›è¡Œé‡é‡‡æ ·123rng = pd.date_range('1/1/2000', periods=100, freq='D')ts = Series(np.arange(100), index=rng)ts.groupby(lambda x: x.month).mean()1 15 2 45 3 75 4 95 dtype: int32 1ts.groupby(lambda x: x.weekday).mean()0 47.5 1 48.5 2 49.5 3 50.5 4 51.5 5 49.0 6 50.0 dtype: float64 å‡é‡‡æ ·å’Œæ’å€¼1234frame = DataFrame(np.random.randn(2, 4), index=pd.date_range('1/1/2000', periods=2, freq='W-WED'), columns=['Colorado', 'Texas', 'New York', 'Ohio'])frameColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-12-0.5871240.612993-0.796000-0.34113812df_daily = frame.resample('D').mean()df_dailyColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-06NaNNaNNaNNaN2000-01-07NaNNaNNaNNaN2000-01-08NaNNaNNaNNaN2000-01-09NaNNaNNaNNaN2000-01-10NaNNaNNaNNaN2000-01-11NaNNaNNaNNaN2000-01-12-0.5871240.612993-0.796000-0.3411381frame.resample('D').ffill()ColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-060.3607730.5064291.1664241.4023362000-01-070.3607730.5064291.1664241.4023362000-01-080.3607730.5064291.1664241.4023362000-01-090.3607730.5064291.1664241.4023362000-01-100.3607730.5064291.1664241.4023362000-01-110.3607730.5064291.1664241.4023362000-01-12-0.5871240.612993-0.796000-0.3411381frame.resample('D').ffill(limit=2)ColoradoTexasNew YorkOhio2000-01-050.3607730.5064291.1664241.4023362000-01-060.3607730.5064291.1664241.4023362000-01-070.3607730.5064291.1664241.4023362000-01-08NaNNaNNaNNaN2000-01-09NaNNaNNaNNaN2000-01-10NaNNaNNaNNaN2000-01-11NaNNaNNaNNaN2000-01-12-0.5871240.612993-0.796000-0.3411381frame.resample('W-THU').ffill()ColoradoTexasNew YorkOhio2000-01-060.3607730.5064291.1664241.4023362000-01-13-0.5871240.612993-0.796000-0.341138é€šè¿‡æ—¶æœŸè¿›è¡Œé‡é‡‡æ ·1234frame = DataFrame(np.random.randn(24, 4), index=pd.period_range('1-2000', '12-2001', freq='M'), columns=['Colorado', 'Texas', 'New York', 'Ohio'])frame[:5]ColoradoTexasNew YorkOhio2000-01-0.2543400.401110-0.931350-0.8725522000-020.390968-0.815357-1.656213-2.2516212000-030.2062970.1973940.927518-0.6572572000-04-0.4517090.908598-0.187902-0.4980822000-05-0.215150-0.042141-0.7387332.499246ä»¥å¹´ä¸ºå•ä½12annual_frame = frame.resample('A-DEC').mean()annual_frameColoradoTexasNew YorkOhio2000-0.0493830.037021-0.272851-0.1409842001-0.183766-0.2919930.3409410.209276ä»¥å­£åº¦ä¸ºå•ä½1234# Q-DEC: Quarterly, year ending in Decemberannual_frame.resample('Q-DEC').ffill()# note: output changed, default value changed from convention='end' to convention='start' + 'start' changed to span-like# also the following cellsColoradoTexasNew YorkOhio2000Q1-0.0493830.037021-0.272851-0.1409842000Q2-0.0493830.037021-0.272851-0.1409842000Q3-0.0493830.037021-0.272851-0.1409842000Q4-0.0493830.037021-0.272851-0.1409842001Q1-0.183766-0.2919930.3409410.2092762001Q2-0.183766-0.2919930.3409410.2092762001Q3-0.183766-0.2919930.3409410.2092762001Q4-0.183766-0.2919930.3409410.2092761annual_frame.resample('Q-DEC', fill_method='ffill', convention='start')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() if __name__ == &apos;__main__&apos;: ColoradoTexasNew YorkOhio2000Q1-0.0493830.037021-0.272851-0.1409842000Q2-0.0493830.037021-0.272851-0.1409842000Q3-0.0493830.037021-0.272851-0.1409842000Q4-0.0493830.037021-0.272851-0.1409842001Q1-0.183766-0.2919930.3409410.2092762001Q2-0.183766-0.2919930.3409410.2092762001Q3-0.183766-0.2919930.3409410.2092762001Q4-0.183766-0.2919930.3409410.2092761annual_frame.resample('Q-MAR', fill_method='ffill')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() if __name__ == &apos;__main__&apos;: ColoradoTexasNew YorkOhio2000Q4-0.0493830.037021-0.272851-0.1409842001Q1-0.0493830.037021-0.272851-0.1409842001Q2-0.0493830.037021-0.272851-0.1409842001Q3-0.0493830.037021-0.272851-0.1409842001Q4-0.183766-0.2919930.3409410.2092762002Q1-0.183766-0.2919930.3409410.2092762002Q2-0.183766-0.2919930.3409410.2092762002Q3-0.183766-0.2919930.3409410.209276æ—¶é—´åºåˆ—ç»˜å›¾1234close_px_all = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)close_px = close_px_all[['AAPL', 'MSFT', 'XOM']]close_px = close_px.resample('B', fill_method='ffill')close_px.info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; DatetimeIndex: 2292 entries, 2003-01-02 to 2011-10-14 Freq: B Data columns (total 3 columns): AAPL 2292 non-null float64 MSFT 2292 non-null float64 XOM 2292 non-null float64 dtypes: float64(3) memory usage: 71.6 KB C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() app.launch_new_instance() æŒ‰å¹´ç»˜å›¾1close_px['AAPL'].plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb1f85a080&gt; æŒ‰æœˆç»˜å›¾1close_px.ix['2009'].plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb20c4d550&gt; æŒ‰å¤©ç»˜å›¾1close_px['AAPL'].ix['01-2011':'03-2011'].plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21235668&gt; æŒ‰å­£åº¦ç»˜å›¾12appl_q = close_px['AAPL'].resample('Q-DEC', fill_method='ffill')appl_q.ix['2009':].plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: fill_method is deprecated to .resample() the new syntax is .resample(...).ffill() if __name__ == &apos;__main__&apos;: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21346c50&gt; ç§»åŠ¨çª—å£å‡½æ•°1close_px = close_px.asfreq('B').fillna(method='ffill')12close_px.AAPL.plot()pd.rolling_mean(close_px.AAPL, 250).plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb213b95f8&gt; C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:2: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=250,center=False).mean() from ipykernel import kernelapp as app &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb213b95f8&gt; 1plt.figure()&lt;matplotlib.figure.Figure at 0x1fb212fb550&gt; &lt;matplotlib.figure.Figure at 0x1fb212fb550&gt; 12appl_std250 = pd.rolling_std(close_px.AAPL, 250, min_periods=10)appl_std250[5:12]C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pd.rolling_std is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=250,min_periods=10,center=False).std() if __name__ == &apos;__main__&apos;: 2003-01-09 NaN 2003-01-10 NaN 2003-01-13 NaN 2003-01-14 NaN 2003-01-15 0.077496 2003-01-16 0.074760 2003-01-17 0.112368 Freq: B, Name: AAPL, dtype: float64 1appl_std250.plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21466ba8&gt; 12# Define expanding mean in terms of rolling_meanexpanding_mean = lambda x: rolling_mean(x, len(x), min_periods=1)1pd.rolling_mean(close_px, 60).plot(logy=True)C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pd.rolling_mean is deprecated for DataFrame and will be removed in a future version, replace with DataFrame.rolling(window=60,center=False).mean() if __name__ == &apos;__main__&apos;: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21571208&gt; 1plt.close('all')æŒ‡æ•°åŠ æƒå‡½æ•°æ›´å¥½çš„æ‹Ÿåˆ1234567891011121314fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(12, 7))aapl_px = close_px.AAPL['2005':'2009']ma60 = pd.rolling_mean(aapl_px, 60, min_periods=50)ewma60 = pd.ewma(aapl_px, span=60)aapl_px.plot(style='k-', ax=axes[0])ma60.plot(style='k--', ax=axes[0])aapl_px.plot(style='k-', ax=axes[1])ewma60.plot(style='k--', ax=axes[1])axes[0].set_title('Simple MA')axes[1].set_title('Exponentially-weighted MA')C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:6: FutureWarning: pd.rolling_mean is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=60,min_periods=50,center=False).mean() C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:7: FutureWarning: pd.ewm_mean is deprecated for Series and will be removed in a future version, replace with Series.ewm(span=60,ignore_na=False,min_periods=0,adjust=True).mean() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21983b70&gt; &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21983b70&gt; &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb219c9a20&gt; &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb219c9a20&gt; &lt;matplotlib.text.Text at 0x1fb219b10b8&gt; &lt;matplotlib.text.Text at 0x1fb219efcc0&gt; äºŒå…ƒç§»åŠ¨çª—å£å‡½æ•°12close_pxspx_px = close_px_all['SPX']AAPLMSFTXOM2003-01-027.4021.1129.222003-01-037.4521.1429.242003-01-067.4521.5229.962003-01-077.4321.9328.952003-01-087.2821.3128.832003-01-097.3421.9329.44â€¦â€¦â€¦â€¦2011-10-07369.8026.2573.562011-10-10388.8126.9476.282011-10-11400.2927.0076.272011-10-12402.1926.9677.162011-10-13408.4327.1876.372011-10-14422.0027.2778.112292 rows Ã— 3 columns1234spx_rets = spx_px / spx_px.shift(1) - 1returns = close_px.pct_change()corr = pd.rolling_corr(returns.AAPL, spx_rets, 125, min_periods=100)corr.plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: pd.rolling_corr is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=125,min_periods=100).corr(other=&lt;Series&gt;) app.launch_new_instance() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb21a93438&gt; 12corr = pd.rolling_corr(returns, spx_rets, 125, min_periods=100)corr.plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: pd.rolling_corr is deprecated for DataFrame and will be removed in a future version, replace with DataFrame.rolling(window=125,min_periods=100).corr(other=&lt;Series&gt;) if __name__ == &apos;__main__&apos;: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb22b21438&gt; ç”¨æˆ·è‡ªå®šä¹‰ç§»åŠ¨çª—å£å‡½æ•°1234from scipy.stats import percentileofscorescore_at_2percent = lambda x: percentileofscore(x, 0.02)result = pd.rolling_apply(returns.AAPL, 250, score_at_2percent)result.plot()C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: FutureWarning: pd.rolling_apply is deprecated for Series and will be removed in a future version, replace with Series.rolling(window=250,center=False).apply(kwargs=&lt;dict&gt;,args=&lt;tuple&gt;,func=&lt;function&gt;) app.launch_new_instance() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1fb3dbdd2e8&gt; æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨æ–¹é¢çš„æ³¨æ„äº‹é¡¹123rng = pd.date_range('1/1/2000', periods=10000000, freq='10ms')ts = Series(np.random.randn(len(rng)), index=rng)ts2000-01-01 00:00:00.000 -0.428577 2000-01-01 00:00:00.010 1.650203 2000-01-01 00:00:00.020 -0.064777 2000-01-01 00:00:00.030 -0.219433 2000-01-01 00:00:00.040 1.907433 2000-01-01 00:00:00.050 0.103347 ... 2000-01-02 03:46:39.940 0.989446 2000-01-02 03:46:39.950 2.333137 2000-01-02 03:46:39.960 0.354455 2000-01-02 03:46:39.970 0.353224 2000-01-02 03:46:39.980 -0.862868 2000-01-02 03:46:39.990 2.007468 Freq: 10L, dtype: float64 1ts.resample('15min').ohlc().info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; DatetimeIndex: 11112 entries, 2000-01-01 00:00:00 to 2000-04-25 17:45:00 Freq: 15T Data columns (total 4 columns): open 11112 non-null float64 high 11112 non-null float64 low 11112 non-null float64 close 11112 non-null float64 dtypes: float64(4) memory usage: 434.1 KB 1%timeit ts.resample('15min').ohlc()10 loops, best of 3: 123 ms per loop â€‹123rng = pd.date_range('1/1/2000', periods=10000000, freq='1s')ts = Series(np.random.randn(len(rng)), index=rng)%timeit ts.resample('15s').ohlc()1 loop, best of 3: 192 ms per loop â€‹]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 two layer net]]></title>
      <url>%2F2017%2F03%2F08%2Fcs231n-Assignment-1-two-layer-net%2F</url>
      <content type="text"><![CDATA[Implementing a Neural NetworkIn this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.1234567891011121314151617181920# A bit of setupimport numpy as npimport matplotlib.pyplot as pltfrom cs231n.classifiers.neural_net import TwoLayerNet%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# for auto-reloading external modules# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2def rel_error(x, y): """ returns relative error """ return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))We will use the class TwoLayerNet in the file cs231n/classifiers/neural_net.py to represent instances of our network. The network parameters are stored in the instance variable self.params where keys are string parameter names and values are numpy arrays. Below, we initialize toy data and a toy model that we will use to develop your implementation.1234567891011121314151617181920# Create a small net and some toy data to check your implementations.# Note that we set the random seed for repeatable experiments.input_size = 4hidden_size = 10num_classes = 3num_inputs = 5def init_toy_model(): np.random.seed(0) return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)def init_toy_data(): np.random.seed(1) X = 10 * np.random.randn(num_inputs, input_size) y = np.array([0, 1, 2, 2, 1]) return X, ynet = init_toy_model()X, y = init_toy_data()Forward pass: compute scoresOpen the file cs231n/classifiers/neural_net.py and look at the method TwoLayerNet.loss. This function is very similar to the loss functions you have written for the SVM and Softmax exercises: It takes the data and weights and computes the class scores, the loss, and the gradients on the parameters.Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs.1234567891011121314151617scores = net.loss(X)print 'Your scores:'print scoresprintprint 'correct scores:'correct_scores = np.asarray([ [-0.81233741, -1.27654624, -0.70335995], [-0.17129677, -1.18803311, -0.47310444], [-0.51590475, -1.01354314, -0.8504215 ], [-0.15419291, -0.48629638, -0.52901952], [-0.00618733, -0.12435261, -0.15226949]])print correct_scoresprint# The difference should be very small. We get &lt; 1e-7print 'Difference between your scores and correct scores:'print np.sum(np.abs(scores - correct_scores))Your scores: [[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]] correct scores: [[-0.81233741 -1.27654624 -0.70335995] [-0.17129677 -1.18803311 -0.47310444] [-0.51590475 -1.01354314 -0.8504215 ] [-0.15419291 -0.48629638 -0.52901952] [-0.00618733 -0.12435261 -0.15226949]] Difference between your scores and correct scores: 3.68027204961e-08 Forward pass: compute lossIn the same function, implement the second part that computes the data and regularizaion loss.123456loss, _ = net.loss(X, y, reg=0.1)correct_loss = 1.30378789133# should be very small, we get &lt; 1e-12print 'Difference between your loss and correct loss:'print np.sum(np.abs(loss - correct_loss))Difference between your loss and correct loss: 1.79412040779e-13 Backward passImplement the rest of the function. This will compute the gradient of the loss with respect to the variables W1, b1, W2, and b2. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check:12345678910111213from cs231n.gradient_check import eval_numerical_gradient# Use numeric gradient checking to check your implementation of the backward pass.# If your implementation is correct, the difference between the numeric and# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.loss, grads = net.loss(X, y, reg=0.1)# these should all be less than 1e-8 or sofor param_name in grads: f = lambda W: net.loss(X, y, reg=0.1)[0] param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False) print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))W1 max relative error: 3.669858e-09 W2 max relative error: 3.440708e-09 b2 max relative error: 3.865028e-11 b1 max relative error: 2.738422e-09 Train the networkTo train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers. Look at the function TwoLayerNet.train and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the SVM and Softmax classifiers. You will also have to implement TwoLayerNet.predict, as the training process periodically performs prediction to keep track of accuracy over time while the network trains.Once you have implemented the method, run the code below to train a two-layer network on toy data. You should achieve a training loss less than 0.2.12345678910111213net = init_toy_model()stats = net.train(X, y, X, y, learning_rate=1e-1, reg=1e-5, num_iters=100, verbose=False)print 'Final training loss: ', stats['loss_history'][-1]# plot the loss historyplt.plot(stats['loss_history'])plt.xlabel('iteration')plt.ylabel('training loss')plt.title('Training Loss history')plt.show()Final training loss: 0.0171496079387 â€‹Load the dataNow that you have implemented a two-layer network that passes gradient checks and works on toy data, itâ€™s time to load up our favorite CIFAR-10 data so we can use it to train a classifier on a real dataset.123456789101112131415161718192021222324252627282930313233343536373839404142434445from cs231n.data_utils import load_CIFAR10def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000): """ Load the CIFAR-10 dataset from disk and perform preprocessing to prepare it for the two-layer neural net classifier. These are the same steps as we used for the SVM, but condensed to a single function. """ # Load the raw CIFAR-10 data cifar10_dir = 'cs231n/datasets/cifar-10-batches-py' X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir) # Subsample the data mask = range(num_training, num_training + num_validation) X_val = X_train[mask] y_val = y_train[mask] mask = range(num_training) X_train = X_train[mask] y_train = y_train[mask] mask = range(num_test) X_test = X_test[mask] y_test = y_test[mask] # Normalize the data: subtract the mean image mean_image = np.mean(X_train, axis=0) X_train -= mean_image X_val -= mean_image X_test -= mean_image # Reshape data to rows X_train = X_train.reshape(num_training, -1) X_val = X_val.reshape(num_validation, -1) X_test = X_test.reshape(num_test, -1) return X_train, y_train, X_val, y_val, X_test, y_test# Invoke the above function to get our data.X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTrain data shape: (49000L, 3072L) Train labels shape: (49000L,) Validation data shape: (1000L, 3072L) Validation labels shape: (1000L,) Test data shape: (1000L, 3072L) Test labels shape: (1000L,) Train a networkTo train our network we will use SGD with momentum. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate.1234567891011121314input_size = 32 * 32 * 3hidden_size = 50num_classes = 10net = TwoLayerNet(input_size, hidden_size, num_classes)# Train the networkstats = net.train(X_train, y_train, X_val, y_val, num_iters=1000, batch_size=200, learning_rate=1e-4, learning_rate_decay=0.95, reg=0.5, verbose=True)# Predict on the validation setval_acc = (net.predict(X_val) == y_val).mean()print 'Validation accuracy: ', val_acciteration 0 / 1000: loss 2.302954 iteration 100 / 1000: loss 2.302550 iteration 200 / 1000: loss 2.297648 iteration 300 / 1000: loss 2.259602 iteration 400 / 1000: loss 2.204170 iteration 500 / 1000: loss 2.118565 iteration 600 / 1000: loss 2.051535 iteration 700 / 1000: loss 1.988466 iteration 800 / 1000: loss 2.006591 iteration 900 / 1000: loss 1.951473 Validation accuracy: 0.287 Debug the trainingWith the default parameters we provided above, you should get a validation accuracy of about 0.29 on the validation set. This isnâ€™t very good.One strategy for getting insight into whatâ€™s wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized.1234567891011121314# Plot the loss function and train / validation accuraciesplt.subplot(3, 1, 1)plt.plot(stats['loss_history'])plt.title('Loss history')plt.xlabel('Iteration')plt.ylabel('Loss')plt.subplot(3, 1, 3)plt.plot(stats['train_acc_history'], label='train')plt.plot(stats['val_acc_history'], label='val')plt.title('Classification accuracy history')plt.xlabel('Epoch')plt.ylabel('Clasification accuracy')plt.show()123456789101112from cs231n.vis_utils import visualize_grid# Visualize the weights of the networkdef show_net_weights(net): W1 = net.params['W1'] W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2) plt.imshow(visualize_grid(W1, padding=3).astype('uint8')) plt.gca().axis('off') plt.show()show_net_weights(net)Tune your hyperparametersWhatâ€™s wrong?. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.Tuning. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, numer of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.Approximate results. You should be aim to achieve a classification accuracy of greater than 48% on the validation set. Our best network gets over 52% on the validation set.Experiment: You goal in this exercise is to get as good of a result on CIFAR-10 as you can, with a fully-connected Neural Network. For every 1% above 52% on the Test set we will award you with one extra bonus point. Feel free implement your own techniques (e.g. PCA to reduce dimensionality, or adding dropout, or adding features to the solver, etc.).123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960best_net = None # store the best model into this ################################################################################## TODO: Tune hyperparameters using the validation set. Store your best trained ## model in best_net. ## ## To help debug your network, it may help to use visualizations similar to the ## ones we used above; these visualizations will have significant qualitative ## differences from the ones we saw above for the poorly tuned network. ## ## Tweaking hyperparameters by hand can be fun, but you might find it useful to ## write code to sweep through possible combinations of hyperparameters ## automatically like we did on the previous exercises. ##################################################################################best_val = -1best_stats = Nonelearning_rates = [1e-1, 1e-2, 1e-3, 1e-4]regularization_strengths = [1e-1, 1e-2, 1e-3, 1e-4]batch_sizes = [200, 400, 800]hidden_sizes = [80, 160, 320]results = &#123;&#125;iters = 2000total_size = 144i = 0for lr in learning_rates: for rs in regularization_strengths: for bs in batch_sizes: for hs in hidden_sizes: i += 1 print i, '/', total_size net = TwoLayerNet(input_size, hs, num_classes) # Train the network stats = net.train(X_train, y_train, X_val, y_val, num_iters=iters, batch_size=bs, learning_rate=lr, learning_rate_decay=0.95, reg=rs) y_train_pred = net.predict(X_train) acc_train = np.mean(y_train == y_train_pred) y_val_pred = net.predict(X_val) acc_val = np.mean(y_val == y_val_pred) results[(lr, rs, bs, hs)] = (acc_train, acc_val) if best_val &lt; acc_val: best_stats = stats best_val = acc_val best_net = net# Print out results.# for lr, reg, bs, hs in sorted(results):# train_accuracy, val_accuracy = results[(lr, reg, bs, hs)]# print 'lr %e reg %e bs %e hs %e train accuracy: %f val accuracy: %f' % (# lr, reg, bs, hs, train_accuracy, val_accuracy)print 'best validation accuracy achieved during cross-validation: %f' % best_val################################################################################## END OF YOUR CODE ##################################################################################1 / 144 â€‹cs231n\classifiers\neural_net.py:104: RuntimeWarning: overflow encountered in exp exp_scores = np.exp(scores) cs231n\classifiers\neural_net.py:105: RuntimeWarning: invalid value encountered in divide a2 = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) cs231n\classifiers\neural_net.py:107: RuntimeWarning: divide by zero encountered in log correct_log_probs = -np.log(a2[range(N), y]) cs231n\classifiers\neural_net.py:81: RuntimeWarning: invalid value encountered in maximum a1 = np.maximum(0, z1) cs231n\classifiers\neural_net.py:131: RuntimeWarning: invalid value encountered in less_equal dhidden[z1 &lt;= 0] = 0 cs231n\classifiers\neural_net.py:247: RuntimeWarning: invalid value encountered in maximum a1 = np.maximum(0, z1) # pass through ReLU activation function 2 / 144 3 / 144 4 / 144 5 / 144 6 / 144 7 / 144 8 / 144 9 / 144 10 / 144 11 / 144 12 / 144 13 / 144 14 / 144 15 / 144 16 / 144 17 / 144 18 / 144 19 / 144 20 / 144 21 / 144 22 / 144 23 / 144 24 / 144 25 / 144 26 / 144 27 / 144 28 / 144 29 / 144 30 / 144 31 / 144 32 / 144 33 / 144 34 / 144 35 / 144 36 / 144 37 / 144 38 / 144 39 / 144 40 / 144 41 / 144 42 / 144 43 / 144 44 / 144 45 / 144 46 / 144 47 / 144 48 / 144 49 / 144 50 / 144 51 / 144 52 / 144 53 / 144 54 / 144 55 / 144 56 / 144 57 / 144 58 / 144 59 / 144 60 / 144 61 / 144 62 / 144 63 / 144 64 / 144 65 / 144 66 / 144 67 / 144 68 / 144 69 / 144 70 / 144 71 / 144 72 / 144 73 / 144 74 / 144 75 / 144 76 / 144 77 / 144 78 / 144 79 / 144 80 / 144 81 / 144 82 / 144 83 / 144 84 / 144 85 / 144 86 / 144 87 / 144 88 / 144 89 / 144 90 / 144 91 / 144 92 / 144 93 / 144 94 / 144 95 / 144 96 / 144 97 / 144 98 / 144 99 / 144 100 / 144 101 / 144 102 / 144 103 / 144 104 / 144 105 / 144 106 / 144 107 / 144 108 / 144 109 / 144 110 / 144 111 / 144 112 / 144 113 / 144 114 / 144 115 / 144 116 / 144 117 / 144 118 / 144 119 / 144 120 / 144 121 / 144 122 / 144 123 / 144 124 / 144 125 / 144 126 / 144 127 / 144 128 / 144 129 / 144 130 / 144 131 / 144 132 / 144 133 / 144 134 / 144 135 / 144 136 / 144 137 / 144 138 / 144 139 / 144 140 / 144 141 / 144 142 / 144 143 / 144 144 / 144 best validation accuracy achieved during cross-validation: 0.540000 12# visualize the weights of the best networkshow_net_weights(best_net)Run on the test setWhen you are done experimenting, you should evaluate your final trained network on the test set; you should get above 48%.We will give you extra bonus point for every 1% of accuracy above 52%.12test_acc = (best_net.predict(X_test) == y_test).mean()print 'Test accuracy: ', test_accTest accuracy: 0.531 â€‹Codes123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254import numpy as npimport matplotlib.pyplot as pltclass TwoLayerNet(object): """ A two-layer fully-connected neural network. The net has an input dimension of N, a hidden layer dimension of H, and performs classification over C classes. We train the network with a softmax loss function and L2 regularization on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture: input - fully connected layer - ReLU - fully connected layer - softmax The outputs of the second fully-connected layer are the scores for each class. """ def __init__(self, input_size, hidden_size, output_size, std=1e-4): """ Initialize the model. Weights are initialized to small random values and biases are initialized to zero. Weights and biases are stored in the variable self.params, which is a dictionary with the following keys: W1: First layer weights; has shape (D, H) b1: First layer biases; has shape (H,) W2: Second layer weights; has shape (H, C) b2: Second layer biases; has shape (C,) Inputs: - input_size: The dimension D of the input data. - hidden_size: The number of neurons H in the hidden layer. - output_size: The number of classes C. """ self.params = &#123;&#125; self.params['W1'] = std * np.random.randn(input_size, hidden_size) self.params['b1'] = np.zeros(hidden_size) self.params['W2'] = std * np.random.randn(hidden_size, output_size) self.params['b2'] = np.zeros(output_size) def loss(self, X, y=None, reg=0.0): """ Compute the loss and gradients for a two layer fully connected neural network. Inputs: - X: Input data of shape (N, D). Each X[i] is a training sample. - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it is not passed then we only return scores, and if it is passed then we instead return the loss and gradients. - reg: Regularization strength. Returns: If y is None, return a matrix scores of shape (N, C) where scores[i, c] is the score for class c on input X[i]. If y is not None, instead return a tuple of: - loss: Loss (data loss and regularization loss) for this batch of training samples. - grads: Dictionary mapping parameter names to gradients of those parameters with respect to the loss function; has the same keys as self.params. """ # Unpack variables from the params dictionary W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] N, D = X.shape # Compute the forward pass scores = None ############################################################################# # TODO: Perform the forward pass, computing the class scores for the input. # # Store the result in the scores variable, which should be an array of # # shape (N,C). # ############################################################################# # First layer pre-activation z1 = X.dot(W1) + b1 # First layer activation a1 = np.maximum(0, z1) # Second layer pre-activation z2 = a1.dot(W2) + b2 scores = z2 ############################################################################# # END OF YOUR CODE # ############################################################################# # If the targets are not given then jump out, we're done if y is None: return scores # Compute the loss loss = None ############################################################################# # TODO: Finish the forward pass, and compute the loss. This should include # # both the data loss and L2 regularization for W1 and W2. Store the result # # in the variable loss, which should be a scalar. Use the Softmax # # classifier loss. So that your results match ours, multiply the # # regularization loss by 0.5 # ############################################################################# exp_scores = np.exp(scores) a2 = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) correct_log_probs = -np.log(a2[range(N), y]) data_loss = np.sum(correct_log_probs) / N reg_loss = 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2)) loss = data_loss + reg_loss ############################################################################# # END OF YOUR CODE # ############################################################################# # Backward pass: compute gradients grads = &#123;&#125; ############################################################################# # TODO: Compute the backward pass, computing the derivatives of the weights # # and biases. Store the results in the grads dictionary. For example, # # grads['W1'] should store the gradient on W1, and be a matrix of same size # ############################################################################# dscores = a2 dscores[range(N), y] -= 1 dscores /= N grads['W2'] = np.dot(a1.T, dscores) grads['b2'] = np.sum(dscores, axis=0) dhidden = np.dot(dscores, W2.T) dhidden[z1 &lt;= 0] = 0 grads['W1'] = np.dot(X.T, dhidden) grads['b1'] = np.sum(dhidden, axis=0) grads['W2'] += reg * W2 grads['W1'] += reg * W1 ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, grads def train(self, X, y, X_val, y_val, learning_rate=1e-3, learning_rate_decay=0.95, reg=1e-5, num_iters=100, batch_size=200, verbose=False): """ Train this neural network using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) giving training data. - y: A numpy array f shape (N,) giving training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - X_val: A numpy array of shape (N_val, D) giving validation data. - y_val: A numpy array of shape (N_val,) giving validation labels. - learning_rate: Scalar giving learning rate for optimization. - learning_rate_decay: Scalar giving factor used to decay the learning rate after each epoch. - reg: Scalar giving regularization strength. - num_iters: Number of steps to take when optimizing. - batch_size: Number of training examples to use per step. - verbose: boolean; if true print progress during optimization. """ num_train = X.shape[0] iterations_per_epoch = max(num_train / batch_size, 1) # Use SGD to optimize the parameters in self.model loss_history = [] train_acc_history = [] val_acc_history = [] for it in xrange(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: Create a random minibatch of training data and labels, storing # # them in X_batch and y_batch respectively. # ######################################################################### sample_indices = np.random.choice(num_train, batch_size) X_batch = X[sample_indices] y_batch = y[sample_indices] ######################################################################### # END OF YOUR CODE # ######################################################################### # Compute loss and gradients using the current minibatch loss, grads = self.loss(X_batch, y=y_batch, reg=reg) loss_history.append(loss) ######################################################################### # TODO: Use the gradients in the grads dictionary to update the # # parameters of the network (stored in the dictionary self.params) # # using stochastic gradient descent. You'll need to use the gradients # # stored in the grads dictionary defined above. # ######################################################################### self.params['W1'] += -learning_rate * grads['W1'] self.params['b1'] += -learning_rate * grads['b1'] self.params['W2'] += -learning_rate * grads['W2'] self.params['b2'] += -learning_rate * grads['b2'] ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print 'iteration %d / %d: loss %f' % (it, num_iters, loss) # Every epoch, check train and val accuracy and decay learning rate. if it % iterations_per_epoch == 0: # Check accuracy train_acc = (self.predict(X_batch) == y_batch).mean() val_acc = (self.predict(X_val) == y_val).mean() train_acc_history.append(train_acc) val_acc_history.append(val_acc) # Decay learning rate learning_rate *= learning_rate_decay return &#123; 'loss_history': loss_history, 'train_acc_history': train_acc_history, 'val_acc_history': val_acc_history, &#125; def predict(self, X): """ Use the trained weights of this two-layer network to predict labels for data points. For each data point we predict scores for each of the C classes, and assign each data point to the class with the highest score. Inputs: - X: A numpy array of shape (N, D) giving N D-dimensional data points to classify. Returns: - y_pred: A numpy array of shape (N,) giving predicted labels for each of the elements of X. For all i, y_pred[i] = c means that X[i] is predicted to have class c, where 0 &lt;= c &lt; C. """ y_pred = None ########################################################################### # TODO: Implement this function; it should be VERY simple! # ########################################################################### z1 = X.dot(self.params['W1']) + self.params['b1'] a1 = np.maximum(0, z1) # pass through ReLU activation function scores = a1.dot(self.params['W2']) + self.params['b2'] y_pred = np.argmax(scores, axis=1) ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note 09]]></title>
      <url>%2F2017%2F03%2F07%2Fpython-data-analysis-learning-note-09%2F</url>
      <content type="text"><![CDATA[æ•°æ®èšåˆä¸Žåˆ†ç»„è¿ç®—12345678910from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)123pd.options.display.notebook_repr_html = Falsefrom IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inlineGroupBy æœºåˆ¶12345df = DataFrame(&#123;'key1' : ['a', 'a', 'b', 'b', 'a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randn(5), 'data2' : np.random.randn(5)&#125;)df data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one 12grouped = df['data1'].groupby(df['key1'])grouped&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000008BAFA90&gt; å˜é‡groupbyæ˜¯ä¸€ä¸ªGroupByå¯¹è±¡ã€‚å®ƒå®žé™…è¿˜æ²¡æœ‰è¿›è¡Œä»»ä½•è®¡ç®—ï¼Œåªæœ‰è¿›è¡Œè®¡ç®—ä¹‹åŽæ‰èƒ½æ˜¾ç¤ºç»“æžœ1grouped.mean()key1 a 0.746672 b -0.537585 Name: data1, dtype: float64 12means = df['data1'].groupby([df['key1'], df['key2']]).mean()meanskey1 key2 a one 0.880536 two 0.478943 b one -0.519439 two -0.555730 Name: data1, dtype: float64 1means.unstack()key2 one two key1 a 0.880536 0.478943 b -0.519439 -0.555730 åªè¦é•¿åº¦ç›¸åŒå³å¯123states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])years = np.array([2005, 2005, 2006, 2005, 2006])df['data1'].groupby([states, years]).mean()California 2005 0.478943 2006 -0.519439 Ohio 2005 -0.380219 2006 1.965781 Name: data1, dtype: float64 åªè¦æ•°å€¼åž‹æ•°æ®æ‰ä¼šå‡ºçŽ°åœ¨ç»“æžœä¸­1df.groupby('key1').mean() data1 data2 key1 a 0.746672 0.910916 b -0.537585 0.525384 1df.groupby(['key1', 'key2']).mean() data1 data2 key1 key2 a one 0.880536 1.319920 two 0.478943 0.092908 b one -0.519439 0.281746 two -0.555730 0.769023 1df.groupby(['key1', 'key2']).size()key1 key2 a one 2 two 1 b one 1 two 1 dtype: int64 å¯¹åˆ†ç»„è¿›è¡Œè¿­ä»£æ˜¾ç¤ºåˆ†ç»„æ•°æ®ï¼ˆè¦é€šè¿‡è¿™ç§è¿­ä»£çš„æ–¹å¼æ‰èƒ½æ˜¾ç¤ºï¼‰1df.groupby('key1')&lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000034BC8CF8&gt; 1234dffor name, group in df.groupby('key1'): print(name) print(group) data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one a data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 4 1.965781 1.246435 a one b data1 data2 key1 key2 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two åŒæ ·è¿›è¡Œè¿­ä»£æ‰èƒ½æ˜¾ç¤ºç»“æžœ123for (k1, k2), group in df.groupby(['key1', 'key2']): print((k1, k2)) print(group)(&apos;a&apos;, &apos;one&apos;) data1 data2 key1 key2 0 -0.204708 1.393406 a one 4 1.965781 1.246435 a one (&apos;a&apos;, &apos;two&apos;) data1 data2 key1 key2 1 0.478943 0.092908 a two (&apos;b&apos;, &apos;one&apos;) data1 data2 key1 key2 2 -0.519439 0.281746 b one (&apos;b&apos;, &apos;two&apos;) data1 data2 key1 key2 3 -0.55573 0.769023 b two å°†åˆ†ç»„ç»“æžœè½¬åŒ–æˆä¸€ä¸ªå­—å…¸ï¼ˆè¦å…ˆè½¬åŒ–ä¸ºä¸€ä¸ªåˆ—è¡¨ï¼‰12pieces = dict(list(df.groupby('key1')))pieces['b'] data1 data2 key1 key2 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two æ˜¾ç¤ºæ¯ä¸€é¡¹çš„æ•°æ®ç±»åž‹1df.dtypesdata1 float64 data2 float64 key1 object key2 object dtype: object å¯¹åˆ—è¿›è¡Œåˆ†ç»„â€¦æŒ‰ç…§æ•°æ®ç±»åž‹æ¥ï¼Ÿï¼12grouped = df.groupby(df.dtypes, axis=1)dict(list(grouped)){dtype(&apos;float64&apos;): data1 data2 0 -0.204708 1.393406 1 0.478943 0.092908 2 -0.519439 0.281746 3 -0.555730 0.769023 4 1.965781 1.246435, dtype(&apos;O&apos;): key1 key2 0 a one 1 a two 2 b one 3 b two 4 a one} é€‰æ‹©ä¸€åˆ—æˆ–ä¸€ç»„åˆ—12df.groupby('key1')['data1']df.groupby('key1')[['data2']]&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000034BDC2E8&gt; &lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000015EDF2B0&gt; ä¸Šè¿°ä»£ç æ˜¯ä»¥ä¸‹ä»£ç çš„è¯­æ³•ç³–12df['data1'].groupby(df['key1'])df[['data2']].groupby(df['key1'])&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000034BDC8D0&gt; &lt;pandas.core.groupby.DataFrameGroupBy object at 0x0000000034BDC898&gt; 1df.groupby(['key1', 'key2'])[['data2']].mean() data2 key1 key2 a one 1.319920 two 0.092908 b one 0.281746 two 0.769023 12s_grouped = df.groupby(['key1', 'key2'])['data2']s_grouped&lt;pandas.core.groupby.SeriesGroupBy object at 0x0000000034BDC6A0&gt; 1s_grouped.mean()key1 key2 a one 1.319920 two 0.092908 b one 0.281746 two 0.769023 Name: data2, dtype: float64 é€šè¿‡å­—å…¸æˆ–Seriesè¿›è¡Œåˆ†ç»„12345people = DataFrame(np.random.randn(5, 5), columns=['a', 'b', 'c', 'd', 'e'], index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])people.ix[2:3, ['b', 'c']] = np.nan # Add a few NA valuespeople a b c d e Joe 1.007189 -1.296221 0.274992 0.228913 1.352917 Steve 0.886429 -2.001637 -0.371843 1.669025 -0.438570 Wes -0.539741 NaN NaN -1.021228 -0.577087 Jim 0.124121 0.302614 0.523772 0.000940 1.343810 Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 12mapping = &#123;'a': 'red', 'b': 'red', 'c': 'blue', 'd': 'blue', 'e': 'red', 'f' : 'orange'&#125;ä¼šè·³è¿‡NAå€¼12by_column = people.groupby(mapping, axis=1)by_column.sum() blue red Joe 0.503905 1.063885 Steve 1.297183 -1.553778 Wes -1.021228 -1.116829 Jim 0.524712 1.770545 Travis -4.230992 -2.405455 ä¸Šè¿°åŠŸèƒ½åŒæ ·å¯ä»¥é€šè¿‡Serieså®žçŽ°12map_series = Series(mapping)map_seriesa red b red c blue d blue e red f orange dtype: object 1people.groupby(map_series, axis=1).count() blue red Joe 2 3 Steve 2 3 Wes 1 2 Jim 2 3 Travis 2 3 é€šè¿‡å‡½æ•°è¿›è¡Œåˆ†ç»„æ ¹æ®äººåé•¿åº¦è¿›è¡Œåˆ†ç»„1people.groupby(len).sum() a b c d e 3 0.591569 -0.993608 0.798764 -0.791374 2.119639 5 0.886429 -2.001637 -0.371843 1.669025 -0.438570 6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 å†åŠ ä¸€ä¸ªåˆ†ç»„åº¦é‡12key_list = ['one', 'one', 'one', 'two', 'two']people.groupby([len, key_list]).min() a b c d e 3 one -0.539741 -1.296221 0.274992 -1.021228 -0.577087 two 0.124121 0.302614 0.523772 0.000940 1.343810 5 one 0.886429 -2.001637 -0.371843 1.669025 -0.438570 6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 æ ¹æ®ç´¢å¼•çº§åˆ«åˆ†ç»„1234columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'], [1, 3, 5, 1, 3]], names=['cty', 'tenor'])hier_df = DataFrame(np.random.randn(4, 5), columns=columns)hier_dfcty US JP tenor 1 3 5 1 3 0 0.560145 -1.265934 0.119827 -1.063512 0.332883 1 -2.359419 -0.199543 -1.541996 -0.970736 -1.307030 2 0.286350 0.377984 -0.753887 0.331286 1.349742 3 0.069877 0.246674 -0.011862 1.004812 1.327195 1hier_df.groupby(level='cty', axis=1).count()cty JP US 0 2 3 1 2 3 2 2 3 3 2 3 æ•°æ®èšåˆ1df data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one å¯¹åˆ†ç»„åŽçš„æ•°æ®è¿›è¡Œç›¸åº”æ“ä½œ12grouped = df.groupby('key1')grouped['data1'].quantile(0.9)key1 a 1.668413 b -0.523068 Name: data1, dtype: float64 é€šè¿‡å‡½æ•°è¿›è¡Œèšåˆæ“ä½œ123def peak_to_peak(arr): return arr.max() - arr.min()grouped.agg(peak_to_peak) data1 data2 key1 a 2.170488 1.300498 b 0.036292 0.487276 åˆ—å‡ºåˆ†ç»„åŽæ•°æ®çš„ä¸€äº›å¸¸ç”¨å±žæ€§1grouped.describe() data1 data2 key1 a count 3.000000 3.000000 mean 0.746672 0.910916 std 1.109736 0.712217 min -0.204708 0.092908 25% 0.137118 0.669671 50% 0.478943 1.246435 75% 1.222362 1.319920 max 1.965781 1.393406 b count 2.000000 2.000000 mean -0.537585 0.525384 std 0.025662 0.344556 min -0.555730 0.281746 25% -0.546657 0.403565 50% -0.537585 0.525384 75% -0.528512 0.647203 max -0.519439 0.769023 å¯¼å…¥ä¸€ä¸ªæ•°æ®é›†ç”¨äºŽæŽ¥ä¸‹æ¥æ›´åŠ é«˜çº§çš„èšåˆæ“ä½œ1234tips = pd.read_csv('ch08/tips.csv')# Add tip percentage of total billtips['tip_pct'] = tips['tip'] / tips['total_bill']tips[:6] total_bill tip sex smoker day time size_ tip_pct 0 16.99 1.01 Female No Sun Dinner 2 0.059447 1 10.34 1.66 Male No Sun Dinner 3 0.160542 2 21.01 3.50 Male No Sun Dinner 3 0.166587 3 23.68 3.31 Male No Sun Dinner 2 0.139780 4 24.59 3.61 Female No Sun Dinner 4 0.146808 5 25.29 4.71 Male No Sun Dinner 4 0.186240 é¢å‘åˆ—çš„å¤šå‡½æ•°åº”ç”¨æ ¹æ®æ€§åˆ«ä»¥åŠæ˜¯å¦å¸çƒŸè¿›è¡Œåˆ†ç±»1grouped = tips.groupby(['sex', 'smoker'])ç®—å‡ºä¸åŒç±»åž‹çš„é¡¾å®¢æ‰€ç»™çš„å°è´¹å æ€»èŠ±è´¹çš„æ¯”ä¾‹çš„å¹³å‡å€¼12grouped_pct = grouped['tip_pct']grouped_pct.agg('mean')sex smoker Female No 0.156921 Yes 0.182150 Male No 0.160669 Yes 0.152771 Name: tip_pct, dtype: float64 åŒæ—¶ç®—å‡ºæ¯”ä¾‹çš„å‡å€¼ã€æ ‡å‡†å·®ä»¥åŠèŒƒå›´å¤§å°1grouped_pct.agg(['mean', 'std', peak_to_peak]) mean std peak_to_peak sex smoker Female No 0.156921 0.036421 0.195876 Yes 0.182150 0.071595 0.360233 Male No 0.160669 0.041849 0.220186 Yes 0.152771 0.090588 0.674707 èµ·ä¸€ä¸ªåˆ«å1grouped_pct.agg([('foo', 'mean'), ('bar', np.std)]) foo bar sex smoker Female No 0.156921 0.036421 Yes 0.182150 0.071595 Male No 0.160669 0.041849 Yes 0.152771 0.090588 å¯¹åˆ†ç»„åŽçš„æ•°æ®çš„ä¸¤ä¸ªå±žæ€§åˆ†åˆ«åšä¸‰ä¸ªä¸åŒçš„æ“ä½œ123functions = ['count', 'mean', 'max']result = grouped['tip_pct', 'total_bill'].agg(functions)result tip_pct total_bill count mean max count mean max sex smoker Female No 54 0.156921 0.252672 54 18.105185 35.83 Yes 33 0.182150 0.416667 33 17.977879 44.30 Male No 97 0.160669 0.291990 97 19.791237 48.33 Yes 60 0.152771 0.710345 60 22.284500 50.81 æå–å‡ºä¸Šè¿°ä¸¤ä¸ªå±žæ€§ä¸­çš„ä¸€ä¸ª1result['tip_pct'] count mean max sex smoker Female No 54 0.156921 0.252672 Yes 33 0.182150 0.416667 Male No 97 0.160669 0.291990 Yes 60 0.152771 0.710345 å¯¹å¤šä¸ªå±žæ€§è¿›è¡Œå¤šä¸ªæ“ä½œçš„åŒæ—¶è¿›è¡Œèµ·åˆ«åçš„æ“ä½œ12ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]grouped['tip_pct', 'total_bill'].agg(ftuples) tip_pct total_bill Durchschnitt Abweichung Durchschnitt Abweichung sex smoker Female No 0.156921 0.001327 18.105185 53.092422 Yes 0.182150 0.005126 17.977879 84.451517 Male No 0.160669 0.001751 19.791237 76.152961 Yes 0.152771 0.008206 22.284500 98.244673 å¯¹ä¸åŒçš„åˆ—è¿›è¡Œä¸åŒçš„æ“ä½œ1grouped.agg(&#123;'tip' : np.max, 'size_' : 'sum'&#125;) size_ tip sex smoker Female No 140 5.2 Yes 74 6.5 Male No 263 9.0 Yes 150 10.0 å¯¹ä¸åŒçš„åˆ—è¿›è¡Œæ•°é‡ä¸åŒç±»åž‹ä¸åŒçš„æ“ä½œ12grouped.agg(&#123;'tip_pct' : ['min', 'max', 'mean', 'std'], 'size_' : 'sum'&#125;) tip_pct size_ min max mean std sum sex smoker Female No 0.056797 0.252672 0.156921 0.036421 140 Yes 0.056433 0.416667 0.182150 0.071595 74 Male No 0.071804 0.291990 0.160669 0.041849 263 Yes 0.035638 0.710345 0.152771 0.090588 150 ä»¥æ— ç´¢å¼•çš„å½¢å¼è¿”å›žèšåˆæ•°æ®1tips.groupby(['sex', 'smoker'], as_index=False).mean() sex smoker total_bill tip size_ tip_pct 0 Female No 18.105185 2.773519 2.592593 0.156921 1 Female Yes 17.977879 2.931515 2.242424 0.182150 2 Male No 19.791237 3.113402 2.711340 0.160669 3 Male Yes 22.284500 3.051167 2.500000 0.152771 1tips.groupby(['sex', 'smoker'], as_index=True).mean() total_bill tip size_ tip_pct sex smoker Female No 18.105185 2.773519 2.592593 0.156921 Yes 17.977879 2.931515 2.242424 0.182150 Male No 19.791237 3.113402 2.711340 0.160669 Yes 22.284500 3.051167 2.500000 0.152771 åˆ†ç»„çº§è¿ç®—å’Œè½¬æ¢1df data1 data2 key1 key2 0 -0.204708 1.393406 a one 1 0.478943 0.092908 a two 2 -0.519439 0.281746 b one 3 -0.555730 0.769023 b two 4 1.965781 1.246435 a one 12k1_means = df.groupby('key1').mean().add_prefix('mean_')k1_means mean_data1 mean_data2 key1 a 0.746672 0.910916 b -0.537585 0.525384 ä¿ç•™åŽŸç´¢å¼•1pd.merge(df, k1_means, left_on='key1', right_index=True) data1 data2 key1 key2 mean_data1 mean_data2 0 -0.204708 1.393406 a one 0.746672 0.910916 1 0.478943 0.092908 a two 0.746672 0.910916 4 1.965781 1.246435 a one 0.746672 0.910916 2 -0.519439 0.281746 b one -0.537585 0.525384 3 -0.555730 0.769023 b two -0.537585 0.525384 å¦ä¸€ä¸ªä¾‹å­ï¼Œä»¥æ›´ç®€æ´çš„æ–¹å¼å®žçŽ°ä¸Šè¿°åŠŸèƒ½1people a b c d e Joe 1.007189 -1.296221 0.274992 0.228913 1.352917 Steve 0.886429 -2.001637 -0.371843 1.669025 -0.438570 Wes -0.539741 NaN NaN -1.021228 -0.577087 Jim 0.124121 0.302614 0.523772 0.000940 1.343810 Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757 12key = ['one', 'two', 'one', 'two', 'one']people.groupby(key).mean() a b c d e one -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 two 0.505275 -0.849512 0.075965 0.834983 0.452620 å°†èšåˆåŽçš„ç»“æžœæ”¾å›žåŽŸæ¥æ•°æ®ä¸­åˆé€‚çš„ä½ç½®ï¼ˆæ ‡é‡è¿›è¡Œå¹¿æ’­ï¼‰1people.groupby(key).transform(np.mean) a b c d e Joe -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 Steve 0.505275 -0.849512 0.075965 0.834983 0.452620 Wes -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 Jim 0.505275 -0.849512 0.075965 0.834983 0.452620 Travis -0.082032 -1.063687 -1.047620 -0.884358 -0.028309 åŒæ—¶å‡åŽ»å‡å€¼1234def demean(arr): return arr - arr.mean()demeaned = people.groupby(key).transform(demean)demeaned a b c d e Joe 1.089221 -0.232534 1.322612 1.113271 1.381226 Steve 0.381154 -1.152125 -0.447807 0.834043 -0.891190 Wes -0.457709 NaN NaN -0.136869 -0.548778 Jim -0.381154 1.152125 0.447807 -0.834043 0.891190 Travis -0.631512 0.232534 -1.322612 -0.976402 -0.832448 æ£€éªŒä¸€ä¸‹1demeaned.groupby(key).mean() a b c d e one 0.000000e+00 -1.110223e-16 0.0 7.401487e-17 0.0 two -2.775558e-17 0.000000e+00 0.0 0.000000e+00 0.0 Apply: ä¸€èˆ¬æ€§çš„ â€œæ‹†åˆ†-åº”ç”¨-åˆå¹¶â€å°è´¹æ•°æ®ï¼Œæ ¹æ®æŸä¸€ä¸ªå±žæ€§ä»Žå¤§åˆ°å°è¿›è¡ŒæŽ’åº123def top(df, n=5, column='tip_pct'): return df.sort_values(by=column)[-n:]top(tips, n=6) total_bill tip sex smoker day time size_ tip_pct 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 232 11.61 3.39 Male No Sat Dinner 2 0.291990 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 åœ¨åˆ†ç»„åŽçš„æ•°æ®é›†ä¸Šè¿›è¡Œä¸Šè¿°æŽ’åºæ“ä½œï¼Œè¯´æ˜Žåˆ†ç»„åŽçš„æ¯ä¸€ç»„éƒ½æ˜¯ä¸€ä¸ªDataFrameå¯¹è±¡1tips.groupby('smoker').apply(top) total_bill tip sex smoker day time size_ tip_pct smoker No 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 1tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill') total_bill tip sex smoker day time size_ \ smoker day No Fri 94 22.75 3.25 Female No Fri Dinner 2 Sat 212 48.33 9.00 Male No Sat Dinner 4 Sun 156 48.17 5.00 Male No Sun Dinner 6 Thur 142 41.19 5.00 Male No Thur Lunch 5 Yes Fri 95 40.17 4.73 Male Yes Fri Dinner 4 Sat 170 50.81 10.00 Male Yes Sat Dinner 3 Sun 182 45.35 3.50 Male Yes Sun Dinner 3 Thur 197 43.11 5.00 Female Yes Thur Lunch 4 tip_pct smoker day No Fri 94 0.142857 Sat 212 0.186220 Sun 156 0.103799 Thur 142 0.121389 Yes Fri 95 0.117750 Sat 170 0.196812 Sun 182 0.077178 Thur 197 0.115982 èŽ·å–åˆ†ç»„åŽæ•°æ®æŸä¸€åˆ—çš„ç»Ÿè®¡æ•°æ®12result = tips.groupby('smoker')['tip_pct'].describe()resultsmoker No count 151.000000 mean 0.159328 std 0.039910 min 0.056797 25% 0.136906 50% 0.155625 75% 0.185014 max 0.291990 Yes count 93.000000 mean 0.163196 std 0.085119 min 0.035638 25% 0.106771 50% 0.153846 75% 0.195059 max 0.710345 Name: tip_pct, dtype: float64 1result.unstack('smoker')smoker No Yes count 151.000000 93.000000 mean 0.159328 0.163196 std 0.039910 0.085119 min 0.056797 0.035638 25% 0.136906 0.106771 50% 0.155625 0.153846 75% 0.185014 0.195059 max 0.291990 0.710345 grouped æ ¹æ®æ€§åˆ«ä»¥åŠæ˜¯å¦å¸çƒŸè¿›è¡Œåˆ†ç»„12f = lambda x: x.describe()grouped.apply(f) total_bill tip size_ tip_pct sex smoker Female No count 54.000000 54.000000 54.000000 54.000000 mean 18.105185 2.773519 2.592593 0.156921 std 7.286455 1.128425 1.073146 0.036421 min 7.250000 1.000000 1.000000 0.056797 25% 12.650000 2.000000 2.000000 0.139708 50% 16.690000 2.680000 2.000000 0.149691 75% 20.862500 3.437500 3.000000 0.181630 max 35.830000 5.200000 6.000000 0.252672 Yes count 33.000000 33.000000 33.000000 33.000000 mean 17.977879 2.931515 2.242424 0.182150 std 9.189751 1.219916 0.613917 0.071595 min 3.070000 1.000000 1.000000 0.056433 25% 12.760000 2.000000 2.000000 0.152439 50% 16.270000 2.880000 2.000000 0.173913 75% 22.120000 3.500000 2.000000 0.198216 max 44.300000 6.500000 4.000000 0.416667 Male No count 97.000000 97.000000 97.000000 97.000000 mean 19.791237 3.113402 2.711340 0.160669 std 8.726566 1.489559 0.989094 0.041849 min 7.510000 1.250000 2.000000 0.071804 25% 13.810000 2.000000 2.000000 0.131810 50% 18.240000 2.740000 2.000000 0.157604 75% 22.820000 3.710000 3.000000 0.186220 max 48.330000 9.000000 6.000000 0.291990 Yes count 60.000000 60.000000 60.000000 60.000000 mean 22.284500 3.051167 2.500000 0.152771 std 9.911845 1.500120 0.892530 0.090588 min 7.250000 1.000000 1.000000 0.035638 25% 15.272500 2.000000 2.000000 0.101845 50% 20.390000 3.000000 2.000000 0.141015 75% 28.572500 3.820000 3.000000 0.191697 max 50.810000 10.000000 5.000000 0.710345 ç¦æ­¢åˆ†ç»„é”®1tips.groupby('smoker', group_keys=True).apply(top) total_bill tip sex smoker day time size_ tip_pct smoker No 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 1tips.groupby('smoker', group_keys=False).apply(top) total_bill tip sex smoker day time size_ tip_pct 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 åˆ†ä½æ•°ä¸Žæ¡¶åˆ†æž1234frame = DataFrame(&#123;'data1': np.random.randn(1000), 'data2': np.random.randn(1000)&#125;)factor = pd.cut(frame.data1, 4)factor[:10]0 (-1.23, 0.489] 1 (-2.956, -1.23] 2 (-1.23, 0.489] 3 (0.489, 2.208] 4 (-1.23, 0.489] 5 (0.489, 2.208] 6 (-1.23, 0.489] 7 (-1.23, 0.489] 8 (0.489, 2.208] 9 (0.489, 2.208] Name: data1, dtype: category Categories (4, object): [(-2.956, -1.23] &lt; (-1.23, 0.489] &lt; (0.489, 2.208] &lt; (2.208, 3.928]] 12345678def get_stats(group): return &#123;'min': group.min(), 'max': group.max(), 'count': group.count(), 'mean': group.mean()&#125;grouped = frame.data2.groupby(factor)grouped.apply(get_stats).unstack()#ADAPT the output is not sorted in the book while this is the case now (swap first two lines) count max mean min data1 (-2.956, -1.23] 95.0 1.670835 -0.039521 -3.399312 (-1.23, 0.489] 598.0 3.260383 -0.002051 -2.989741 (0.489, 2.208] 297.0 2.954439 0.081822 -3.745356 (2.208, 3.928] 10.0 1.765640 0.024750 -1.929776 12345# Return quantile numbersgrouping = pd.qcut(frame.data1, 10, labels=False)grouped = frame.data2.groupby(grouping)grouped.apply(get_stats).unstack() count max mean min data1 0 100.0 1.670835 -0.049902 -3.399312 1 100.0 2.628441 0.030989 -1.950098 2 100.0 2.527939 -0.067179 -2.925113 3 100.0 3.260383 0.065713 -2.315555 4 100.0 2.074345 -0.111653 -2.047939 5 100.0 2.184810 0.052130 -2.989741 6 100.0 2.458842 -0.021489 -2.223506 7 100.0 2.954439 -0.026459 -3.056990 8 100.0 2.735527 0.103406 -3.745356 9 100.0 2.377020 0.220122 -2.064111 Example: ç”¨ç‰¹å®šåˆ†ç»„çš„å€¼å¡«å……ç¼ºå¤±å€¼å¡«ä¸€äº›ç¼ºå¤±å€¼è¿›åŽ»123s = Series(np.random.randn(6))s[::2] = np.nans0 NaN 1 -0.125921 2 NaN 3 -0.884475 4 NaN 5 0.227290 dtype: float64 ç”¨å‡å€¼å¡«å……ç¼ºå¤±å€¼1s.fillna(s.mean())0 -0.261035 1 -0.125921 2 -0.261035 3 -0.884475 4 -0.261035 5 0.227290 dtype: float64 åŒæ ·ï¼Œå¡«ä¸€äº›ç¼ºå¤±å€¼123456states = ['Ohio', 'New York', 'Vermont', 'Florida', 'Oregon', 'Nevada', 'California', 'Idaho']group_key = ['East'] * 4 + ['West'] * 4data = Series(np.random.randn(8), index=states)data[['Vermont', 'Nevada', 'Idaho']] = np.nandataOhio 0.922264 New York -2.153545 Vermont NaN Florida -0.375842 Oregon 0.329939 Nevada NaN California 1.105913 Idaho NaN dtype: float64 è®¡ç®—åˆ†ç»„å‡å€¼1data.groupby(group_key).mean()East -0.535707 West 0.717926 dtype: float64 è¿™é‡Œçš„gæŒ‡ä»£è°ƒç”¨applyçš„ä¸»ä½“ï¼Œä¹Ÿå°±æ˜¯data.groupby(group_key)åˆ†ç»„åŽçš„ç»“æžœ12fill_mean = lambda g: g.fillna(g.mean())data.groupby(group_key).apply(fill_mean)Ohio 0.922264 New York -2.153545 Vermont -0.535707 Florida -0.375842 Oregon 0.329939 Nevada 0.717926 California 1.105913 Idaho 0.717926 dtype: float64 ç”±äºŽgroupbyæ“ä½œåŽå¾—åˆ°çš„ç»“æžœç±»ä¼¼äºŽä¸€ä¸ªå­—å…¸ï¼Œå­—å…¸keyæ˜¯ç»„åï¼Œvalueæ˜¯ä¸€ä¸ªDataFrame Object1234fill_values = &#123;'East': 0.5, 'West': -1&#125;fill_func = lambda g: g.fillna(fill_values[g.name])data.groupby(group_key).apply(fill_func)Ohio 0.922264 New York -2.153545 Vermont 0.500000 Florida -0.375842 Oregon 0.329939 Nevada -1.000000 California 1.105913 Idaho -1.000000 dtype: float64 Example: éšæœºé‡‡æ ·å’ŒæŽ’åˆ—æž„é€ æ‰‘å…‹ç‰Œçº¢æ¡ƒHearts, é»‘æ¡ƒSpades, æ¢…èŠ±Clubs, æ–¹ç‰‡Diamonds123456789# Hearts, Spades, Clubs, Diamondssuits = ['H', 'S', 'C', 'D']card_val = (list(range(1, 11)) + [10] * 3) * 4base_names = ['A'] + range(2, 11) + ['J', 'K', 'Q']cards = []for suit in ['H', 'S', 'C', 'D']: cards.extend(str(num) + suit for num in base_names)deck = Series(card_val, index=cards)1deck[:13]AH 1 2H 2 3H 3 4H 4 5H 5 6H 6 7H 7 8H 8 9H 9 10H 10 JH 10 KH 10 QH 10 dtype: int64 éšæœºæŠ½ç‰Œ123def draw(deck, n=5): return deck.take(np.random.permutation(len(deck))[:n])draw(deck)AD 1 8C 8 5H 5 KC 10 2C 2 dtype: int64 åˆ†ç±»æŠ½ç‰Œ12get_suit = lambda card: card[-1] # last letter is suitdeck.groupby(get_suit).apply(draw, n=2)C 2C 2 3C 3 D KD 10 8D 8 H KH 10 3H 3 S 2S 2 4S 4 dtype: int64 åŽ»æŽ‰åˆ†ç»„é”®12# alternativelydeck.groupby(get_suit, group_keys=False).apply(draw, n=2)KC 10 JC 10 AD 1 5D 5 5H 5 6H 6 7S 7 KS 10 dtype: int64 Example: åˆ†ç»„åŠ æƒå¹³å‡æ•°å’Œç›¸å…³ç³»æ•°1234df = DataFrame(&#123;'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], 'data': np.random.randn(8), 'weights': np.random.rand(8)&#125;)df category data weights 0 a 1.561587 0.957515 1 a 1.219984 0.347267 2 a -0.482239 0.581362 3 a 0.315667 0.217091 4 b -0.047852 0.894406 5 b -0.454145 0.918564 6 b -0.556774 0.277825 7 b 0.253321 0.955905 123grouped = df.groupby('category')get_wavg = lambda g: np.average(g['data'], weights=g['weights'])grouped.apply(get_wavg)category a 0.811643 b -0.122262 dtype: float64 stockæ•°æ®é›†12close_px = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)close_px.info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; DatetimeIndex: 2214 entries, 2003-01-02 to 2011-10-14 Data columns (total 4 columns): AAPL 2214 non-null float64 MSFT 2214 non-null float64 XOM 2214 non-null float64 SPX 2214 non-null float64 dtypes: float64(4) memory usage: 86.5 KB 1close_px[-4:] AAPL MSFT XOM SPX 2011-10-11 400.29 27.00 76.27 1195.54 2011-10-12 402.19 26.96 77.16 1207.25 2011-10-13 408.43 27.18 76.37 1203.66 2011-10-14 422.00 27.27 78.11 1224.58 è®¡ç®—ç›¸å…³ç³»æ•°1234rets = close_px.pct_change().dropna()spx_corr = lambda x: x.corrwith(x['SPX'])by_year = rets.groupby(lambda x: x.year)by_year.apply(spx_corr) AAPL MSFT XOM SPX 2003 0.541124 0.745174 0.661265 1.0 2004 0.374283 0.588531 0.557742 1.0 2005 0.467540 0.562374 0.631010 1.0 2006 0.428267 0.406126 0.518514 1.0 2007 0.508118 0.658770 0.786264 1.0 2008 0.681434 0.804626 0.828303 1.0 2009 0.707103 0.654902 0.797921 1.0 2010 0.710105 0.730118 0.839057 1.0 2011 0.691931 0.800996 0.859975 1.0 lambdaçœ‹æ¥æœ‰å¾ˆå¤§ç”¨å¤„12# Annual correlation of Apple with Microsoftby_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))2003 0.480868 2004 0.259024 2005 0.300093 2006 0.161735 2007 0.417738 2008 0.611901 2009 0.432738 2010 0.571946 2011 0.581987 dtype: float64 Example: åˆ†ç»„çº§çº¿åž‹å›žå½’1234567import statsmodels.api as smdef regress(data, yvar, xvars): Y = data[yvar] X = data[xvars] X['intercept'] = 1. result = sm.OLS(Y, X).fit() return result.paramsè¿™æ ·ä¼ å‚1by_year.apply(regress, 'AAPL', ['SPX']) SPX intercept 2003 1.195406 0.000710 2004 1.363463 0.004201 2005 1.766415 0.003246 2006 1.645496 0.000080 2007 1.198761 0.003438 2008 0.968016 -0.001110 2009 0.879103 0.002954 2010 1.052608 0.001261 2011 0.806605 0.001514 é€è§†è¡¨å’Œäº¤å‰è¡¨1tips[:10] total_bill tip sex smoker day time size_ tip_pct 0 16.99 1.01 Female No Sun Dinner 2 0.059447 1 10.34 1.66 Male No Sun Dinner 3 0.160542 2 21.01 3.50 Male No Sun Dinner 3 0.166587 3 23.68 3.31 Male No Sun Dinner 2 0.139780 4 24.59 3.61 Female No Sun Dinner 4 0.146808 5 25.29 4.71 Male No Sun Dinner 4 0.186240 6 8.77 2.00 Male No Sun Dinner 2 0.228050 7 26.88 3.12 Male No Sun Dinner 4 0.116071 8 15.04 1.96 Male No Sun Dinner 2 0.130319 9 14.78 3.23 Male No Sun Dinner 2 0.218539 pivot_tableé»˜è®¤æƒ…å†µç›¸å½“äºŽåˆ†ç»„åŽè¿›è¡Œmean()æ“ä½œ1tips.pivot_table(index=['sex', 'smoker']) size_ tip tip_pct total_bill sex smoker Female No 2.592593 2.773519 0.156921 18.105185 Yes 2.242424 2.931515 0.182150 17.977879 Male No 2.711340 3.113402 0.160669 19.791237 Yes 2.500000 3.051167 0.152771 22.284500 æŒ‡å®šåˆ†ç»„åº¦é‡12tips.pivot_table(['tip_pct', 'size_'], index=['sex', 'day'], columns='smoker') tip_pct size_ smoker No Yes No Yes sex day Female Fri 0.165296 0.209129 2.500000 2.000000 Sat 0.147993 0.163817 2.307692 2.200000 Sun 0.165710 0.237075 3.071429 2.500000 Thur 0.155971 0.163073 2.480000 2.428571 Male Fri 0.138005 0.144730 2.000000 2.125000 Sat 0.162132 0.139067 2.656250 2.629630 Sun 0.158291 0.173964 2.883721 2.600000 Thur 0.165706 0.164417 2.500000 2.300000 å¢žåŠ ALLåˆ—12tips.pivot_table(['tip_pct', 'size_'], index=['sex', 'day'], columns='smoker', margins=True) tip_pct size_ smoker No Yes All No Yes All sex day Female Fri 0.165296 0.209129 0.199388 2.500000 2.000000 2.111111 Sat 0.147993 0.163817 0.156470 2.307692 2.200000 2.250000 Sun 0.165710 0.237075 0.181569 3.071429 2.500000 2.944444 Thur 0.155971 0.163073 0.157525 2.480000 2.428571 2.468750 Male Fri 0.138005 0.144730 0.143385 2.000000 2.125000 2.100000 Sat 0.162132 0.139067 0.151577 2.656250 2.629630 2.644068 Sun 0.158291 0.173964 0.162344 2.883721 2.600000 2.810345 Thur 0.165706 0.164417 0.165276 2.500000 2.300000 2.433333 All 0.159328 0.163196 0.160803 2.668874 2.408602 2.569672 æ›´æ¢ä¸€ä¸ªåˆ†ç»„åº¦é‡12tips.pivot_table('tip_pct', index=['sex', 'smoker'], columns='day', aggfunc=len, margins=True)day Fri Sat Sun Thur All sex smoker Female No 2.0 13.0 14.0 25.0 54.0 Yes 7.0 15.0 4.0 7.0 33.0 Male No 2.0 32.0 43.0 20.0 97.0 Yes 8.0 27.0 15.0 10.0 60.0 All 19.0 87.0 76.0 62.0 244.0 åˆ†ç»„è®¡æ•°å¹¶å¡«å……ï¼ˆå¯èƒ½å­˜åœ¨ç©ºç»„åˆï¼‰12tips.pivot_table('size_', index=['time', 'sex', 'smoker'], columns='day', aggfunc='sum', fill_value=0)day Fri Sat Sun Thur time sex smoker Dinner Female No 2 30 43 2 Yes 8 33 10 0 Male No 4 85 124 0 Yes 12 71 39 0 Lunch Female No 3 0 0 60 Yes 6 0 0 17 Male No 0 0 0 50 Yes 5 0 0 23 äº¤å‰è¡¨: crosstab1234567891011121314from StringIO import StringIOdata = """\Sample Gender Handedness1 Female Right-handed2 Male Left-handed3 Female Right-handed4 Male Right-handed5 Male Left-handed6 Male Right-handed7 Female Right-handed8 Female Left-handed9 Male Right-handed10 Female Right-handed"""data = pd.read_table(StringIO(data), sep='\s+')1data Sample Gender Handedness 0 1 Female Right-handed 1 2 Male Left-handed 2 3 Female Right-handed 3 4 Male Right-handed 4 5 Male Left-handed 5 6 Male Right-handed 6 7 Female Right-handed 7 8 Female Left-handed 8 9 Male Right-handed 9 10 Female Right-handed äº¤å‰è¡¨å°±æ˜¯åœ¨â€¦è®¡æ•°â€¦1pd.crosstab(data.Gender, data.Handedness, margins=True)Handedness Left-handed Right-handed All Gender Female 1 4 5 Male 2 3 5 All 3 7 10 1pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)smoker No Yes All time day Dinner Fri 3 9 12 Sat 45 42 87 Sun 57 19 76 Thur 1 0 1 Lunch Fri 1 6 7 Thur 44 17 61 All 151 93 244 Example: 2012 è”é‚¦é€‰ä¸¾å§”å‘˜ä¼šæ•°æ®åº“è¿™ä¸ªæ•°æ®åº“åŒ…æ‹¬èµžåŠ©äººçš„å§“åï¼ŒèŒä¸šã€é›‡ä¸»ã€åœ°å€ä»¥åŠå‡ºèµ„é¢ç­‰ä¿¡æ¯1fec = pd.read_csv('ch09/P00000001-ALL.csv')1fec.info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 1001731 entries, 0 to 1001730 Data columns (total 16 columns): cmte_id 1001731 non-null object cand_id 1001731 non-null object cand_nm 1001731 non-null object contbr_nm 1001731 non-null object contbr_city 1001712 non-null object contbr_st 1001727 non-null object contbr_zip 1001620 non-null object contbr_employer 988002 non-null object contbr_occupation 993301 non-null object contb_receipt_amt 1001731 non-null float64 contb_receipt_dt 1001731 non-null object receipt_desc 14166 non-null object memo_cd 92482 non-null object memo_text 97770 non-null object form_tp 1001731 non-null object file_num 1001731 non-null int64 dtypes: float64(1), int64(1), object(14) memory usage: 122.3+ MB 1fec.ix[123456]cmte_id C00431445 cand_id P80003338 cand_nm Obama, Barack contbr_nm ELLMAN, IRA contbr_city TEMPE contbr_st AZ contbr_zip 852816719 contbr_employer ARIZONA STATE UNIVERSITY contbr_occupation PROFESSOR contb_receipt_amt 50 contb_receipt_dt 01-DEC-11 receipt_desc NaN memo_cd NaN memo_text NaN form_tp SA17A file_num 772372 Name: 123456, dtype: object è¾“å‡ºå…¨éƒ¨å€™é€‰äººåå•12unique_cands = fec.cand_nm.unique()unique_candsarray([&apos;Bachmann, Michelle&apos;, &apos;Romney, Mitt&apos;, &apos;Obama, Barack&apos;, &quot;Roemer, Charles E. &apos;Buddy&apos; III&quot;, &apos;Pawlenty, Timothy&apos;, &apos;Johnson, Gary Earl&apos;, &apos;Paul, Ron&apos;, &apos;Santorum, Rick&apos;, &apos;Cain, Herman&apos;, &apos;Gingrich, Newt&apos;, &apos;McCotter, Thaddeus G&apos;, &apos;Huntsman, Jon&apos;, &apos;Perry, Rick&apos;], dtype=object) 1unique_cands[2]&apos;Obama, Barack&apos; æ”¿å…šæ˜ å°„12345678910111213parties = &#123;'Bachmann, Michelle': 'Republican', 'Cain, Herman': 'Republican', 'Gingrich, Newt': 'Republican', 'Huntsman, Jon': 'Republican', 'Johnson, Gary Earl': 'Republican', 'McCotter, Thaddeus G': 'Republican', 'Obama, Barack': 'Democrat', 'Paul, Ron': 'Republican', 'Pawlenty, Timothy': 'Republican', 'Perry, Rick': 'Republican', "Roemer, Charles E. 'Buddy' III": 'Republican', 'Romney, Mitt': 'Republican', 'Santorum, Rick': 'Republican'&#125;1fec.cand_nm[123456:123461]123456 Obama, Barack 123457 Obama, Barack 123458 Obama, Barack 123459 Obama, Barack 123460 Obama, Barack Name: cand_nm, dtype: object 1fec.cand_nm[123456:123461].map(parties)123456 Democrat 123457 Democrat 123458 Democrat 123459 Democrat 123460 Democrat Name: cand_nm, dtype: object æ ¹æ®ä»¥ä¸Šåˆ›å»ºçš„æ˜ å°„ï¼Œåœ¨åŽŸæ•°æ®é›†ä¸­æ·»åŠ ä¸€åˆ—party12# Add it as a columnfec['party'] = fec.cand_nm.map(parties)1fec['party'].value_counts()Democrat 593746 Republican 407985 Name: party, dtype: int64 çœ‹çœ‹å‡ºèµ„é¢æ˜¯æ­£æ˜¯è´Ÿ1(fec.contb_receipt_amt &gt; 0).value_counts()True 991475 False 10256 Name: contb_receipt_amt, dtype: int64 è°ƒæ•´å‡ºèµ„é¢ä¸ºæ­£1fec = fec[fec.contb_receipt_amt &gt; 0]ç­›é€‰å€™é€‰äºº1fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack', 'Romney, Mitt'])]æ ¹æ®èŒä¸šå’Œé›‡ä¸»ç»Ÿè®¡èµžåŠ©ä¿¡æ¯ç»Ÿè®¡èŒä¸šä¿¡æ¯1fec.contbr_occupation.value_counts()[:10]RETIRED 233990 INFORMATION REQUESTED 35107 ATTORNEY 34286 HOMEMAKER 29931 PHYSICIAN 23432 INFORMATION REQUESTED PER BEST EFFORTS 21138 ENGINEER 14334 TEACHER 13990 CONSULTANT 13273 PROFESSOR 12555 Name: contbr_occupation, dtype: int64 ç­›é€‰å‡ºä¸€äº›ä¸ç¬¦åˆè§„æ ¼çš„ä¿¡æ¯æ˜ å°„åˆ°æ­£å¸¸ä¿¡æ¯12345678910occ_mapping = &#123; 'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED', 'INFORMATION REQUESTED' : 'NOT PROVIDED', 'INFORMATION REQUESTED (BEST EFFORTS)' : 'NOT PROVIDED', 'C.E.O.': 'CEO'&#125;# If no mapping provided, return xf = lambda x: occ_mapping.get(x, x)fec.contbr_occupation = fec.contbr_occupation.map(f)ä»¥ä¸Šå·§å¦™è¿ç”¨äº†getæ–¹æ³•12345678910emp_mapping = &#123; 'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED', 'INFORMATION REQUESTED' : 'NOT PROVIDED', 'SELF' : 'SELF-EMPLOYED', 'SELF EMPLOYED' : 'SELF-EMPLOYED',&#125;# If no mapping provided, return xf = lambda x: emp_mapping.get(x, x)fec.contbr_employer = fec.contbr_employer.map(f)æ ¹æ®èŒä¸šä»¥åŠå€™é€‰äººæ”¿å…šåˆ†ç»„ï¼Œç»Ÿè®¡å‡ºèµ„é¢æ€»å’Œ123by_occupation = fec.pivot_table('contb_receipt_amt', index='contbr_occupation', columns='party', aggfunc='sum')12over_2mm = by_occupation[by_occupation.sum(1) &gt; 2000000]over_2mmparty Democrat Republican contbr_occupation ATTORNEY 11141982.97 7.477194e+06 CEO 2074974.79 4.211041e+06 CONSULTANT 2459912.71 2.544725e+06 ENGINEER 951525.55 1.818374e+06 EXECUTIVE 1355161.05 4.138850e+06 HOMEMAKER 4248875.80 1.363428e+07 INVESTOR 884133.00 2.431769e+06 LAWYER 3160478.87 3.912243e+05 MANAGER 762883.22 1.444532e+06 NOT PROVIDED 4866973.96 2.056547e+07 OWNER 1001567.36 2.408287e+06 PHYSICIAN 3735124.94 3.594320e+06 PRESIDENT 1878509.95 4.720924e+06 PROFESSOR 2165071.08 2.967027e+05 REAL ESTATE 528902.09 1.625902e+06 RETIRED 25305116.38 2.356124e+07 SELF-EMPLOYED 672393.40 1.640253e+06 1over_2mm.plot(kind='barh')&lt;matplotlib.axes._subplots.AxesSubplot at 0x340fb4e0&gt; 12345def get_top_amounts(group, key, n=5): totals = group.groupby(key)['contb_receipt_amt'].sum() # Order totals by key in descending order return totals.sort_values()[-n:]12grouped = fec_mrbo.groupby('cand_nm')grouped.apply(get_top_amounts, 'contbr_occupation', n=7)cand_nm contbr_occupation Obama, Barack CONSULTANT 2459912.71 LAWYER 3160478.87 PHYSICIAN 3735124.94 HOMEMAKER 4248875.80 INFORMATION REQUESTED 4866973.96 ATTORNEY 11141982.97 RETIRED 25305116.38 Romney, Mitt C.E.O. 1968386.11 EXECUTIVE 2300947.03 PRESIDENT 2491244.89 ATTORNEY 5364718.82 HOMEMAKER 8147446.22 INFORMATION REQUESTED PER BEST EFFORTS 11396894.84 RETIRED 11508473.59 Name: contb_receipt_amt, dtype: float64 1grouped.apply(get_top_amounts, 'contbr_employer', n=10)cand_nm contbr_employer Obama, Barack MICROSOFT 215585.36 VOLUNTEER 257104.00 STUDENT 318831.45 SELF EMPLOYED 469290.00 SELF 1076531.20 HOMEMAKER 2605408.54 INFORMATION REQUESTED 5053480.37 NOT EMPLOYED 8586308.70 SELF-EMPLOYED 17080985.96 RETIRED 22694358.85 Romney, Mitt H.I.G. CAPITAL 139500.00 BARCLAYS CAPITAL 162750.00 GOLDMAN SACH &amp; CO. 238250.00 MORGAN STANLEY 267266.00 CREDIT SUISSE 281150.00 STUDENT 496490.94 SELF-EMPLOYED 7409860.98 HOMEMAKER 8147196.22 RETIRED 11506225.71 INFORMATION REQUESTED PER BEST EFFORTS 12059527.24 Name: contb_receipt_amt, dtype: float64 æ ¹æ®å‡ºèµ„é¢åˆ†ç»„ä¸å‡ºæ„å¤–æžœç„¶è¦ç”¨åˆ°æ¡¶123bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000])labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)labels[:10]411 (10, 100] 412 (100, 1000] 413 (100, 1000] 414 (10, 100] 415 (10, 100] 416 (10, 100] 417 (100, 1000] 418 (10, 100] 419 (100, 1000] 420 (10, 100] Name: contb_receipt_amt, dtype: category Categories (8, object): [(0, 1] &lt; (1, 10] &lt; (10, 100] &lt; (100, 1000] &lt; (1000, 10000] &lt; (10000, 100000] &lt; (100000, 1000000] &lt; (1000000, 10000000]] 12grouped = fec_mrbo.groupby(['cand_nm', labels])grouped.size().unstack(0)cand_nm Obama, Barack Romney, Mitt contb_receipt_amt (0, 1] 493.0 77.0 (1, 10] 40070.0 3681.0 (10, 100] 372280.0 31853.0 (100, 1000] 153991.0 43357.0 (1000, 10000] 22284.0 26186.0 (10000, 100000] 2.0 1.0 (100000, 1000000] 3.0 NaN (1000000, 10000000] 4.0 NaN 12bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)bucket_sumscand_nm Obama, Barack Romney, Mitt contb_receipt_amt (0, 1] 318.24 77.00 (1, 10] 337267.62 29819.66 (10, 100] 20288981.41 1987783.76 (100, 1000] 54798531.46 22363381.69 (1000, 10000] 51753705.67 63942145.42 (10000, 100000] 59100.00 12700.00 (100000, 1000000] 1490683.08 NaN (1000000, 10000000] 7148839.76 NaN è®¡ç®—æ¯”ä¾‹12normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)normed_sumscand_nm Obama, Barack Romney, Mitt contb_receipt_amt (0, 1] 0.805182 0.194818 (1, 10] 0.918767 0.081233 (10, 100] 0.910769 0.089231 (100, 1000] 0.710176 0.289824 (1000, 10000] 0.447326 0.552674 (10000, 100000] 0.823120 0.176880 (100000, 1000000] 1.000000 NaN (1000000, 10000000] 1.000000 NaN ç”»ä¸ªå›¾çœ‹çœ‹1normed_sums[:-2].plot(kind='barh', stacked=True)&lt;matplotlib.axes._subplots.AxesSubplot at 0x14c4db00&gt; æ ¹æ®å·žç»Ÿè®¡èµžåŠ©ä¿¡æ¯1234grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)totals = totals[totals.sum(1) &gt; 100000]totals[:10]cand_nm Obama, Barack Romney, Mitt contbr_st AK 281840.15 86204.24 AL 543123.48 527303.51 AR 359247.28 105556.00 AZ 1506476.98 1888436.23 CA 23824984.24 11237636.60 CO 2132429.49 1506714.12 CT 2068291.26 3499475.45 DC 4373538.80 1025137.50 DE 336669.14 82712.00 FL 7318178.58 8338458.81 Mitt is so...poorly...12percent = totals.div(totals.sum(1), axis=0)percent[:10]cand_nm Obama, Barack Romney, Mitt contbr_st AK 0.765778 0.234222 AL 0.507390 0.492610 AR 0.772902 0.227098 AZ 0.443745 0.556255 CA 0.679498 0.320502 CO 0.585970 0.414030 CT 0.371476 0.628524 DC 0.810113 0.189887 DE 0.802776 0.197224 FL 0.467417 0.532583]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Lecture4 note]]></title>
      <url>%2F2017%2F03%2F06%2Fcs231n-Lecture4-note%2F</url>
      <content type="text"><![CDATA[BackPropIntroductionMotivation. In this section we will develop expertise with an intuitive understanding of backpropagation, which is a way of computing gradients of expressions through recursive application of chain rule. Understanding of this process and its subtleties is critical for you to understand, and effectively develop, design and debug Neural Networks.Problem statement. The core problem studied in this section is as follows: We are given some function $f(x)$ where $x$ is a vector of inputs and we are interested in computing the gradient of $f$ at $x$ (i.e. $\Delta f(x)$ ).Modularity: Sigmoid example$$f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$The function is made up of multiple gates. In addition to the ones described already above (add, mul, max), there are four more:$$f(x) = \frac{1}{x}\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = -1/x^2\\\\f_c(x) = c + x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = 1\\\\f(x) = e^x\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = e^x\\\\f_a(x) = ax\hspace{1in} \rightarrow \hspace{1in}\frac{df}{dx} = a$$The full circuit then looks as follows:In the example above, we see a long chain of function applications that operates on the result of the dot product between w,x. The function that these operations implement is called the sigmoid function $\sigma (x)$. It turns out that the derivative of the sigmoid function with respect to its input simplifies if you perform the derivation (after a fun tricky part where we add and subtract a 1 in the numerator):$$\sigma(x) = \frac{1}{1+e^{-x}} \\\\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right)= \left( 1 - \sigma(x) \right) \sigma(x)$$As we see, the gradient turns out to simplify and becomes surprisingly simple. For example, the sigmoid expression receives the input 1.0 and computes the output 0.73 during the forward pass. The derivation above shows that the local gradient would simply be (1 - 0.73) * 0.73 ~= 0.2, as the circuit computed before (see the image above), except this way it would be done with a single, simple and efficient expression (and with less numerical issues). Therefore, in any real practical application it would be very useful to group these operations into a single gate. Lets see the backprop for this neuron in code:123456789101112w = [2,-3,-3] # assume some random weights and datax = [-1, -2]# forward passdot = w[0]*x[0] + w[1]*x[1] + w[2]f = 1.0 / (1 + math.exp(-dot)) # sigmoid function# backward pass through the neuron (backpropagation)ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivationdx = [w[0] * ddot, w[1] * ddot] # backprop into xdw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w# we're done! we have the gradients on the inputs to the circuitImplementation protip: staged backpropagation. As shown in the code above, in practice it is always helpful to break down the forward pass into stages that are easily backpropped through. For example here we created an intermediate variable dot which holds the output of the dot product between w and x. During backward pass we then successively compute (in reverse order) the corresponding variables (e.g. ddot, and ultimately dw, dx) that hold the gradients of those variables.Backprop in practice: Staged computationSuppose that we have a function of the form:$$f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}$$Here is how we would structure the forward pass of such expression:123456789101112x = 3 # example valuesy = -4# forward passsigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator #(1)num = x + sigy # numerator #(2)sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)xpy = x + y #(4)xpysqr = xpy**2 #(5)den = sigx + xpysqr # denominator #(6)invden = 1.0 / den #(7)f = num * invden # done! #(8)For each row, we also highlight which part of the forward pass it refers to:123456789101112131415161718192021# backprop f = num * invdendnum = invden # gradient on numerator #(8)dinvden = num #(8)# backprop invden = 1.0 / den dden = (-1.0 / (den**2)) * dinvden #(7)# backprop den = sigx + xpysqrdsigx = (1) * dden #(6)dxpysqr = (1) * dden #(6)# backprop xpysqr = xpy**2dxpy = (2 * xpy) * dxpysqr #(5)# backprop xpy = x + ydx = (1) * dxpy #(4)dy = (1) * dxpy #(4)# backprop sigx = 1.0 / (1 + math.exp(-x))dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below #(3)# backprop num = x + sigydx += (1) * dnum #(2)dsigy = (1) * dnum #(2)# backprop sigy = 1.0 / (1 + math.exp(-y))dy += ((1 - sigy) * sigy) * dsigy #(1)# done! phewNotice a few things:Cache forward pass variables. To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them.Gradients add up at forks. The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add.Patterns in backward flowIt is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (add,mul,max), all have very simple interpretations in terms of how they act during backpropagation. Consider this example circuit:Looking at the diagram above as an example, we can see that:The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass. This follows from the fact that the local gradient for the add operation is simply +1.0, so the gradients on all inputs will exactly equal the gradients on the output because it will be multiplied by x1.0 (and remain unchanged). In the example circuit above, note that the + gate routed the gradient of 2.00 to both of its inputs, equally and unchanged.The max gate routes the gradient. Unlike the add gate which distributed the gradient unchanged to all its inputs, the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass). This is because the local gradient for a max gate is 1.0 for the highest value, and 0.0 for all other values. In the example circuit above, the max operation routed the gradient of 2.00 to the z variable, which had a higher value than w, and the gradient on w remains zero.The multiply gate is a little less easy to interpret. Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. In the example above, the gradient on x is -8.00, which is -4.00 x 2.00.Unintuitive effects and their consequences. Notice that if one of the inputs to the multiply gate is very small and the other is very big, then the multiply gate will do something slightly unintuitive: it will assign a relatively huge gradient to the small input and a tiny gradient to the large input. Note that in linear classifiers where the weights are dot producted $w^T x_i$ (multiplied) with the inputs, this implies that the scale of the data has an effect on the magnitude of the gradient for the weights. For example, if you multiplied all input data examples xixi by 1000 during preprocessing, then the gradient on the weights will be 1000 times larger, and youâ€™d have to lower the learning rate by that factor to compensate. This is why preprocessing matters a lot, sometimes in subtle ways! And having intuitive understanding for how the gradients flow can help you debug some of these cases.ExtrasSome extra materials provided in here and here.Optional: [1, 2, 3]Neural Network#1Quick introIt is possible to introduce neural networks without appealing to brain analogies. In the section on linear classification we computed scores for different visual categories given the image using the formula $s=Wx$, where $W$ was a matrix and $x$ was an input column vector containing all pixel data of the image. In the case of CIFAR-10, $x$ is a [3072x1] column vector, and $W$ is a [10x3072] matrix, so that the output scores is a vector of 10 class scores.An example neural network would instead compute $s=W_2 \max(0, W_1x)$. Here, $W_1$ could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. The function $\max(0,âˆ’)$is a non-linearity that is applied elementwise. There are several choices we could make for the non-linearity (which weâ€™ll study below), but this one is a common choice and simply thresholds all activations that are below zero to zero. Finally, the matrix $W_2$ would then be of size [10x100], so that we again get 10 numbers out that we interpret as the class scores. Notice that the non-linearity is critical computationally - if we left it out, the two matrices could be collapsed to a single matrix, and therefore the predicted class scores would again be a linear function of the input. The non-linearity is where we get the wiggle. The parameters $W_2, W_1$ are learned with stochastic gradient descent, and their gradients are derived with chain rule (and computed with backpropagation).A three-layer neural network could analogously look like $s = W_3 \max(0, W_2 \max(0, W_1 x))$, where all of $W_3, W_2, W_1$ are parameters to be learned. The sizes of the intermediate hidden vectors are hyperparameters of the network and weâ€™ll see how we can set them later. Lets now look into how we can interpret these computations from the neuron/network perspective.Modeling one neuronAn example code for forward-propagating a single neuron might look as follows:1234567class Neuron(object): # ... def forward(inputs): """ assume inputs and weights are 1-D numpy arrays and bias is a number """ cell_body_sum = np.sum(inputs * self.weights) + self.bias firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function return firing_rateSingle neuron as a linear classifierThe mathematical form of the model Neuronâ€™s forward computation might look familiar to you. As we saw with linear classifiers, a neuron has the capacity to â€œlikeâ€ (activation near one) or â€œdislikeâ€ (activation near zero) certain linear regions of its input space. Hence, with an appropriate loss function on the neuronâ€™s output, we can turn a single neuron into a linear classifier:Binary Softmax classifier. For example, we can interpret $\sigma(\sum_iw_ix_i + b)$ to be the probability of one of the classes $P(y_i = 1 \mid x_i; w)$. The probability of the other class would be $P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w)$, since they must sum to one. With this interpretation, we can formulate the cross-entropy loss as we have seen in the Linear Classification section, and optimizing it would lead to a binary Softmax classifier (also known as logistic regression). Since the sigmoid function is restricted to be between 0-1, the predictions of this classifier are based on whether the output of the neuron is greater than 0.5.Binary SVM classifier. Alternatively, we could attach a max-margin hinge loss to the output of the neuron and train it to become a binary Support Vector Machine.Regularization interpretation. The regularization loss in both SVM/Softmax cases could in this biological view be interpreted as gradual forgetting, since it would have the effect of driving all synaptic weights ww towards zero after every parameter update.A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers)Commonly used activation functionsEvery activation function (or non-linearity) takes a single number and performs a certain fixed mathematical operation on it. There are several activation functions you may encounter in practice:Sigmoid. The sigmoid non-linearity has the mathematical form $\sigma(x) = 1 / (1 + e^{-x})$ and is shown in the image. As alluded to in the previous section, it takes a real-valued number and â€œsquashesâ€ it into range between 0 and 1. In particular, large negative numbers become 0 and large positive numbers become 1. The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks:Sigmoids saturate and kill gradients. A very undesirable property of the sigmoid neuron is that when the neuronâ€™s activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gateâ€™s output for the whole objective. Therefore, if the local gradient is very small, it will effectively â€œkillâ€ the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on this soon) would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $x &gt; 0$ elementwise in $f = w^Tx + b$), then the gradient on the weights ww will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression $f$). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem above.â€‹Tanh. The tanh non-linearity is shown on the image. It squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: $\tanh(x) = 2 \sigma(2x) -1$.ReLU. The Rectified Linear Unit has become very popular in the last few years. It computes the function $f(x) = \max(0, x)$. In other words, the activation is simply thresholded at zero (see image). There are several pros and cons to using the ReLUs:(+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.(+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.(-) Unfortunately, ReLU units can be fragile during training and can â€œdieâ€. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be â€œdeadâ€ (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.Leaky ReLU. Leaky ReLUs are one attempt to fix the â€œdying ReLUâ€ problem. Instead of the function being zero when x &lt; 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes $f(x) = \mathbb{1}(x &lt; 0) (\alpha x) + \mathbb{1}(x&gt;=0) (x)$ where $\alpha$ is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear.Maxout. Other types of units have been proposed that do not have the functional form $f(w^Tx + b)$ where a non-linearity is applied on the dot product between the weights and the data. One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. The Maxout neuron computes the function $\max(w_1^Tx+b_1, w_2^Tx + b_2)$. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have $w_1, b_1 = 0$). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters.This concludes our discussion of the most common types of neurons and their activation functions. As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.TLDR: â€œWhat neuron type should I use?â€ Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of â€œdeadâ€ units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.Neural Network architecturesLayer-wise organizationSizing neural networks. The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with the two example networks in the above picture:The first network (left) has 4 + 2 = 6 neurons (not counting the inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a total of 26 learnable parameters.The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters.To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing. More on this in the Convolutional Neural Networks module.Example feed-forward computation123456# forward-pass of a 3-layer neural network:f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)x = np.random.randn(3, 1) # random input vector of three numbers (3x1)h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)out = np.dot(W3, h2) + b3 # output neuron (1x1)The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function.Representational powerOne way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. A natural question that arises is: What is the representational power of this family of functions? In particular, are there functions that cannot be modeled with a Neural Network?It turns out that Neural Networks with at least one hidden layer are universal approximators. That is, it can be shown (e.g. see Approximation by Superpositions of Sigmoidal Function from 1989 (pdf), or this intuitive explanation from Michael Nielsen) that given any continuous function $f(x)$ and some $\epsilon &gt; 0$, there exists a Neural Network $g(x)$ with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that $\forall x, \mid f(x) - g(x) \mid &lt; \epsilon$. In other words, the neural network can approximate any continuous function.If one hidden layer suffices to approximate any function, why use more layers and go deeper? The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice. In one dimension, the â€œsum of indicator bumpsâ€ function $g(x) = \sum_i c_i \mathbb{1}(a_i &lt; x &lt; b_i)$ where $a, b, c$ are parameter vectors is also a universal approximator, but noone would suggest that we use this functional form in Machine Learning. Neural Networks work well in practice because they compactly express nice, smooth functions that fit well with the statistical properties of data we encounter in practice, and are also easy to learn using our optimization algorithms (e.g. gradient descent). Similarly, the fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal.As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.The full story is, of course, much more involved and a topic of much recent research. If you are interested in these topics we recommend for further reading:Deep Learning book in press by Bengio, Goodfellow, Courville, in practicular Chapter 6.4.Do Deep Nets Really Need to be Deep?FitNets: Hints for Thin Deep NetsSetting number of layers and their sizesWe increase the size and number of layers in a Neural Network, the capacity of the network increases:In practice, it is always better to use these methods to control overfitting instead of the number of neurons.The subtle reason behind this is that smaller networks are harder to train with local methods such as Gradient Descent: Itâ€™s clear that their loss functions have relatively few local minima, but it turns out that many of these minima are easier to converge to, and that they are bad (i.e. with high loss). Conversely, bigger neural networks contain significantly more local minima, but these minima turn out to be much better in terms of their actual loss. Since Neural Networks are non-convex, it is hard to study these properties mathematically, but some attempts to understand these objective functions have been made, e.g. in a recent paper The Loss Surfaces of Multilayer Networks. In practice, what you find is that if you train a small network the final loss can display a good amount of variance - in some cases you get lucky and converge to a good place but in some cases you get trapped in one of the bad minima. On the other hand, if you train a large network youâ€™ll start to find many different solutions, but the variance in the final achieved loss will be much smaller. In other words, all solutions are about equally as good, and rely less on the luck of random initialization.To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network. We can look at the results achieved by three different settings:You can play with these examples in this ConvNetsJS demo.The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.Additional Referencesdeeplearning.net tutorial with TheanoConvNetJS demos for intuitionsMichael Nielsenâ€™s tutorials]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python data analysis learning note ch08]]></title>
      <url>%2F2017%2F03%2F05%2Fpython-data-analysis-learning-note-ch08%2F</url>
      <content type="text"><![CDATA[ç»˜å›¾å’Œå¯è§†åŒ–12345678910from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inlinematplotlib API å…¥é—¨1import matplotlib.pyplot as pltFigure å’Œ Subplot1fig = plt.figure()1ax1 = fig.add_subplot(2, 2, 1)12ax2 = fig.add_subplot(2, 2, 2)ax3 = fig.add_subplot(2, 2, 3)12from numpy.random import randnplt.plot(randn(50).cumsum(), 'k--')12_ = ax1.hist(randn(100), bins=20, color='k', alpha=0.3)ax2.scatter(np.arange(30), np.arange(30) + 3 * randn(30))1plt.close('all')12fig, axes = plt.subplots(2, 3)axesè°ƒæ•´subplotå‘¨å›´çš„é—´è·12plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)12345fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)for i in range(2): for j in range(2): axes[i, j].hist(randn(500), bins=50, color='k', alpha=0.5)plt.subplots_adjust(wspace=0, hspace=0)(array([ 2., 0., 3., 2., 1., 1., 0., 3., 5., 8., 9., 9., 10., 18., 34., 13., 24., 30., 24., 24., 25., 20., 34., 20., 30., 30., 19., 14., 14., 8., 19., 14., 7., 3., 7., 2., 7., 2., 2., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.]), array([-2.9493, -2.8118, -2.6743, -2.5367, -2.3992, -2.2617, -2.1241, -1.9866, -1.849 , -1.7115, -1.574 , -1.4364, -1.2989, -1.1614, -1.0238, -0.8863, -0.7487, -0.6112, -0.4737, -0.3361, -0.1986, -0.0611, 0.0765, 0.214 , 0.3516, 0.4891, 0.6266, 0.7642, 0.9017, 1.0392, 1.1768, 1.3143, 1.4519, 1.5894, 1.7269, 1.8645, 2.002 , 2.1395, 2.2771, 2.4146, 2.5522, 2.6897, 2.8272, 2.9648, 3.1023, 3.2398, 3.3774, 3.5149, 3.6525, 3.79 , 3.9275]), &lt;a list of 50 Patch objects&gt;) (array([ 1., 1., 0., 2., 0., 1., 1., 5., 7., 4., 5., 8., 12., 12., 13., 15., 17., 13., 22., 30., 21., 24., 17., 20., 20., 20., 18., 26., 16., 24., 19., 8., 14., 15., 7., 11., 5., 4., 9., 7., 6., 1., 6., 2., 4., 2., 0., 2., 1., 2.]), array([-2.595 , -2.4898, -2.3845, -2.2793, -2.1741, -2.0688, -1.9636, -1.8584, -1.7531, -1.6479, -1.5427, -1.4374, -1.3322, -1.227 , -1.1217, -1.0165, -0.9112, -0.806 , -0.7008, -0.5955, -0.4903, -0.3851, -0.2798, -0.1746, -0.0694, 0.0359, 0.1411, 0.2463, 0.3516, 0.4568, 0.562 , 0.6673, 0.7725, 0.8777, 0.983 , 1.0882, 1.1935, 1.2987, 1.4039, 1.5092, 1.6144, 1.7196, 1.8249, 1.9301, 2.0353, 2.1406, 2.2458, 2.351 , 2.4563, 2.5615, 2.6667]), &lt;a list of 50 Patch objects&gt;) (array([ 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 4., 1., 4., 5., 11., 8., 6., 11., 13., 13., 17., 18., 20., 27., 32., 29., 31., 22., 21., 31., 29., 19., 22., 18., 10., 18., 11., 12., 9., 6., 2., 3., 3., 3., 2., 1., 1., 1., 0., 1.]), array([-3.7454, -3.6052, -3.4651, -3.325 , -3.1849, -3.0448, -2.9047, -2.7646, -2.6244, -2.4843, -2.3442, -2.2041, -2.064 , -1.9239, -1.7837, -1.6436, -1.5035, -1.3634, -1.2233, -1.0832, -0.9431, -0.8029, -0.6628, -0.5227, -0.3826, -0.2425, -0.1024, 0.0377, 0.1779, 0.318 , 0.4581, 0.5982, 0.7383, 0.8784, 1.0185, 1.1587, 1.2988, 1.4389, 1.579 , 1.7191, 1.8592, 1.9994, 2.1395, 2.2796, 2.4197, 2.5598, 2.6999, 2.84 , 2.9802, 3.1203, 3.2604]), &lt;a list of 50 Patch objects&gt;) (array([ 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 2., 5., 9., 8., 6., 2., 11., 17., 10., 13., 10., 14., 12., 27., 17., 28., 27., 25., 14., 24., 25., 38., 13., 24., 15., 10., 17., 14., 13., 8., 7., 10., 3., 7., 2., 5., 2., 0., 1., 1.]), array([-3.4283, -3.3066, -3.185 , -3.0633, -2.9417, -2.8201, -2.6984, -2.5768, -2.4551, -2.3335, -2.2119, -2.0902, -1.9686, -1.847 , -1.7253, -1.6037, -1.482 , -1.3604, -1.2388, -1.1171, -0.9955, -0.8739, -0.7522, -0.6306, -0.5089, -0.3873, -0.2657, -0.144 , -0.0224, 0.0993, 0.2209, 0.3425, 0.4642, 0.5858, 0.7074, 0.8291, 0.9507, 1.0724, 1.194 , 1.3156, 1.4373, 1.5589, 1.6806, 1.8022, 1.9238, 2.0455, 2.1671, 2.2887, 2.4104, 2.532 , 2.6537]), &lt;a list of 50 Patch objects&gt;) é¢œè‰²ã€æ ‡è®°å’Œçº¿åž‹1plt.figure()1plt.plot(randn(30).cumsum(), 'ko--')1plt.close('all')1234data = randn(30).cumsum()plt.plot(data, 'k--', label='Default')plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')plt.legend(loc='best')åˆ»åº¦ã€æ ‡ç­¾å’Œå›¾ä¾‹è®¾ç½®æ ‡é¢˜ã€è½´æ ‡ç­¾ã€åˆ»åº¦ä»¥åŠåˆ»åº¦æ ‡ç­¾12345678fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)ax.plot(randn(1000).cumsum())ticks = ax.set_xticks([0, 250, 500, 750, 1000])labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'], rotation=30, fontsize='small')ax.set_title('My first matplotlib plot')ax.set_xlabel('Stages')æ·»åŠ å›¾ä¾‹123456fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)ax.plot(randn(1000).cumsum(), 'k', label='one')ax.plot(randn(1000).cumsum(), 'k--', label='two')ax.plot(randn(1000).cumsum(), 'k.', label='three')ax.legend(loc='best')æ³¨è§£ä»¥åŠåœ¨subplotä¸Šç»˜å›¾123456789101112131415161718192021222324252627from datetime import datetimefig = plt.figure()ax = fig.add_subplot(1, 1, 1)data = pd.read_csv('ch08/spx.csv', index_col=0, parse_dates=True)spx = data['SPX']spx.plot(ax=ax, style='k-')crisis_data = [ (datetime(2007, 10, 11), 'Peak of bull market'), (datetime(2008, 3, 12), 'Bear Stearns Fails'), (datetime(2008, 9, 15), 'Lehman Bankruptcy')]for date, label in crisis_data: ax.annotate(label, xy=(date, spx.asof(date) + 50), xytext=(date, spx.asof(date) + 200), arrowprops=dict(facecolor='black'), horizontalalignment='left', verticalalignment='top')# Zoom in on 2007-2010ax.set_xlim(['1/1/2007', '1/1/2011'])ax.set_ylim([600, 1800])ax.set_title('Important dates in 2008-2009 financial crisis')1234567891011fig = plt.figure()ax = fig.add_subplot(1, 1, 1)rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]], color='g', alpha=0.5)ax.add_patch(rect)ax.add_patch(circ)ax.add_patch(pgon)å°†å›¾è¡¨ä¿å­˜åˆ°æ–‡ä»¶1fig1fig.savefig('figpath.svg')1fig.savefig('figpath.png', dpi=400, bbox_inches='tight')1234from io import BytesIObuffer = BytesIO()plt.savefig(buffer)plot_data = buffer.getvalue()&lt;matplotlib.figure.Figure at 0xaebe550&gt; matplotlib é…ç½®1plt.rc('figure', figsize=(10, 10))pandasä¸­çš„ç»˜å›¾å‡½æ•°çº¿åž‹å›¾1plt.close('all')12s = Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))s.plot()1234df = DataFrame(np.random.randn(10, 4).cumsum(0), columns=['A', 'B', 'C', 'D'], index=np.arange(0, 100, 10))df.plot()æŸ±çŠ¶å›¾1234fig, axes = plt.subplots(2, 1)data = Series(np.random.rand(16), index=list('abcdefghijklmnop'))data.plot(kind='bar', ax=axes[0], color='k', alpha=0.7)data.plot(kind='barh', ax=axes[1], color='k', alpha=0.7)12345df = DataFrame(np.random.rand(6, 4), index=['one', 'two', 'three', 'four', 'five', 'six'], columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))dfdf.plot(kind='bar')GenusABCDone0.3016860.1563330.3719430.270731two0.7505890.5255870.6894290.358974three0.3815040.6677070.4737720.632528four0.9424080.1801860.7082840.641783five0.8402780.9095890.0100410.653207six0.0628540.5898130.8113180.0602171plt.figure()1df.plot(kind='barh', stacked=True, alpha=0.5)123456tips = pd.read_csv('ch08/tips.csv')party_counts = pd.crosstab(tips.day, tips.size_)party_counts# Not many 1- and 6-person partiesparty_counts = party_counts.ix[:, 2:5]party_countssize_123456dayFri1161100Sat253181310Sun039151831Thur1484513size_2345dayFri16110Sat5318131Sun3915183Thur4845112345# Normalize to sum to 1party_pcts = party_counts.div(party_counts.sum(1).astype(float), axis=0)party_pctsparty_pcts.plot(kind='bar', stacked=True)size_2345dayFri0.8888890.0555560.0555560.000000Sat0.6235290.2117650.1529410.011765Sun0.5200000.2000000.2400000.040000Thur0.8275860.0689660.0862070.017241ç›´æ–¹å›¾å’Œå¯†åº¦å›¾1plt.figure()12tips['tip_pct'] = tips['tip'] / tips['total_bill']tips['tip_pct'].hist(bins=50)1plt.figure()1tips['tip_pct'].plot(kind='kde')1plt.figure()12345comp1 = np.random.normal(0, 1, size=200) # N(0, 1)comp2 = np.random.normal(10, 2, size=200) # N(10, 4)values = Series(np.concatenate([comp1, comp2]))values.hist(bins=100, alpha=0.3, color='k', normed=True)values.plot(kind='kde', style='k--')æ•£ç‚¹å›¾1234macro = pd.read_csv('ch08/macrodata.csv')data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]trans_data = np.log(data).diff().dropna()trans_data[-5:]cpim1tbilrateunemp198-0.0079040.045361-0.3968810.105361199-0.0219790.066753-2.2772670.1397622000.0023400.0102860.6061360.1603432010.0084190.037461-0.2006710.1273392020.0088940.012202-0.4054650.0425601plt.figure()12plt.scatter(trans_data['m1'], trans_data['unemp'])plt.title('Changes in log %s vs. log %s' % ('m1', 'unemp'))1pd.scatter_matrix(trans_data, diagonal='kde', c='k', alpha=0.3)ç»˜åˆ¶åœ°å›¾ï¼šå›¾å½¢åŒ–æ˜¾ç¤ºæµ·åº•åœ°éœ‡å±æœºæ•°æ®12data = pd.read_csv('ch08/Haiti.csv')data.info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 3593 entries, 0 to 3592 Data columns (total 10 columns): Serial 3593 non-null int64 INCIDENT TITLE 3593 non-null object INCIDENT DATE 3593 non-null object LOCATION 3592 non-null object DESCRIPTION 3593 non-null object CATEGORY 3587 non-null object LATITUDE 3593 non-null float64 LONGITUDE 3593 non-null float64 APPROVED 3593 non-null object VERIFIED 3593 non-null object dtypes: float64(2), int64(1), object(7) memory usage: 280.8+ KB 1data[['INCIDENT DATE', 'LATITUDE', 'LONGITUDE']][:10]INCIDENT DATELATITUDELONGITUDE005/07/2010 17:2618.233333-72.533333128/06/2010 23:0650.2260295.729886224/06/2010 16:2122.278381114.174287320/06/2010 21:5944.4070628.933989418/05/2010 16:2618.571084-72.334671526/04/2010 13:1418.593707-72.310079626/04/2010 14:1918.482800-73.638800726/04/2010 14:2718.415000-73.195000815/03/2010 10:5818.517443-72.236841915/03/2010 11:0018.547790-72.4100101data['CATEGORY'][:6]0 1. Urgences | Emergency, 3. Public Health, 1 1. Urgences | Emergency, 2. Urgences logistiqu... 2 2. Urgences logistiques | Vital Lines, 8. Autr... 3 1. Urgences | Emergency, 4 1. Urgences | Emergency, 5 5e. Communication lines down, Name: CATEGORY, dtype: object 1data.describe()SerialLATITUDELONGITUDEcount3593.0000003593.0000003593.000000mean2080.27748418.611495-72.322680std1171.1003600.7385723.650776min4.00000018.041313-74.45275725%1074.00000018.524070-72.41750050%2163.00000018.539269-72.33500075%3088.00000018.561820-72.293570max4052.00000050.226029114.174287123data = data[(data.LATITUDE &gt; 18) &amp; (data.LATITUDE &lt; 20) &amp; (data.LONGITUDE &gt; -75) &amp; (data.LONGITUDE &lt; -70) &amp; data.CATEGORY.notnull()]12345678910111213def to_cat_list(catstr): stripped = (x.strip() for x in catstr.split(',')) return [x for x in stripped if x]def get_all_categories(cat_series): cat_sets = (set(to_cat_list(x)) for x in cat_series) return sorted(set.union(*cat_sets))def get_english(cat): code, names = cat.split('.') if '|' in names: names = names.split(' | ')[1] return code, names.strip()1get_english('2. Urgences logistiques | Vital Lines')(&apos;2&apos;, &apos;Vital Lines&apos;) 12345all_cats = get_all_categories(data.CATEGORY)# Generator expressionenglish_mapping = dict(get_english(x) for x in all_cats)english_mapping['2a']english_mapping['6c']&apos;Food Shortage&apos; &apos;Earthquake and aftershocks&apos; 1234567def get_code(seq): return [x.split('.')[0] for x in seq if x]all_codes = get_code(all_cats)code_index = pd.Index(np.unique(all_codes))dummy_frame = DataFrame(np.zeros((len(data), len(code_index))), index=data.index, columns=code_index)1dummy_frame.ix[:, :6].info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; Int64Index: 3569 entries, 0 to 3592 Data columns (total 6 columns): 1 3569 non-null float64 1a 3569 non-null float64 1b 3569 non-null float64 1c 3569 non-null float64 1d 3569 non-null float64 2 3569 non-null float64 dtypes: float64(6) memory usage: 195.2 KB 12345for row, cat in zip(data.index, data.CATEGORY): codes = get_code(to_cat_list(cat)) dummy_frame.ix[row, codes] = 1data = data.join(dummy_frame.add_prefix('category_'))1data.ix[:, 10:15].info()&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; Int64Index: 3569 entries, 0 to 3592 Data columns (total 5 columns): category_1 3569 non-null float64 category_1a 3569 non-null float64 category_1b 3569 non-null float64 category_1c 3569 non-null float64 category_1d 3569 non-null float64 dtypes: float64(5) memory usage: 167.3 KB 1234567891011121314151617from mpl_toolkits.basemap import Basemapimport matplotlib.pyplot as pltdef basic_haiti_map(ax=None, lllat=17.25, urlat=20.25, lllon=-75, urlon=-71): # create polar stereographic Basemap instance. m = Basemap(ax=ax, projection='stere', lon_0=(urlon + lllon) / 2, lat_0=(urlat + lllat) / 2, llcrnrlat=lllat, urcrnrlat=urlat, llcrnrlon=lllon, urcrnrlon=urlon, resolution='f') # draw coastlines, state and country boundaries, edge of map. m.drawcoastlines() m.drawstates() m.drawcountries() return m123456789101112131415161718fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))fig.subplots_adjust(hspace=0.05, wspace=0.05)to_plot = ['2a', '1', '3c', '7a']lllat=17.25; urlat=20.25; lllon=-75; urlon=-71for code, ax in zip(to_plot, axes.flat): m = basic_haiti_map(ax, lllat=lllat, urlat=urlat, lllon=lllon, urlon=urlon) cat_data = data[data['category_%s' % code] == 1] # compute map proj coordinates. x, y = m(cat_data.LONGITUDE.values, cat_data.LATITUDE.values) m.plot(x, y, 'k.', alpha=0.5) ax.set_title('%s: %s' % (code, english_mapping[code]))C:\Users\Ewan\Anaconda3\envs\ipykernel_py2\lib\site-packages\mpl_toolkits\basemap\__init__.py:3260: MatplotlibDeprecationWarning: The ishold function was deprecated in version 2.0. b = ax.ishold() C:\Users\Ewan\Anaconda3\envs\ipykernel_py2\lib\site-packages\mpl_toolkits\basemap\__init__.py:3269: MatplotlibDeprecationWarning: axes.hold is deprecated. See the API Changes document (http://matplotlib.org/api/api_changes.html) for more details. ax.hold(b) 12345678910111213141516171819202122#è¡—é“æ•°æ®çš„è·¯å¾„shapefilepath = 'ch08/PortAuPrince_Roads/PortAuPrince_Roads'fig = plt.figure()ax = fig.add_subplot(1,1,1)lat0 = 18.533333;lon0 = -72.333333;change = 0.13;lllat=lat0-change; urlat=lat0+change; lllon=lon0-change; urlon=lon0+change;m = basic_haiti_map(ax, lllat=lllat, urlat=urlat,lllon=lllon, urlon=urlon)m.readshapefile(shapefilepath,'roads') #æ·»åŠ è¡—é“æ•°æ®code = '2a'cat_data = data[data['category_%s' % code] == 1]# compute map proj coordinates.x, y = m(cat_data.LONGITUDE.values, cat_data.LATITUDE.values)m.plot(x, y, 'k.', alpha=0.5)ax.set_title('Food shortages reported in Port-au-Prince')# plt.savefig('myfig.png',dpi=400,bbox_inches='tight')(1583, 3, [-72.749246, 18.409952, 0.0, 0.0], [-71.973789, 18.7147105, 0.0, 0.0],]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 softmax]]></title>
      <url>%2F2017%2F03%2F05%2Fcs231n-Assignment-1-softmax%2F</url>
      <content type="text"><![CDATA[Softmax exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website.This exercise is analogous to the SVM exercise. You will:implement a fully-vectorized loss function for the Softmax classifierimplement the fully-vectorized expression for its analytic gradientcheck your implementation with numerical gradientuse a validation set to tune the learning rate and regularization strengthoptimize the loss function with SGDvisualize the final learned weights12345678910111213import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# for auto-reloading extenrnal modules# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500): """ Load the CIFAR-10 dataset from disk and perform preprocessing to prepare it for the linear classifier. These are the same steps as we used for the SVM, but condensed to a single function. """ # Load the raw CIFAR-10 data cifar10_dir = 'cs231n/datasets/cifar-10-batches-py' X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir) # subsample the data mask = range(num_training, num_training + num_validation) X_val = X_train[mask] y_val = y_train[mask] mask = range(num_training) X_train = X_train[mask] y_train = y_train[mask] mask = range(num_test) X_test = X_test[mask] y_test = y_test[mask] mask = np.random.choice(num_training, num_dev, replace=False) X_dev = X_train[mask] y_dev = y_train[mask] # Preprocessing: reshape the image data into rows X_train = np.reshape(X_train, (X_train.shape[0], -1)) X_val = np.reshape(X_val, (X_val.shape[0], -1)) X_test = np.reshape(X_test, (X_test.shape[0], -1)) X_dev = np.reshape(X_dev, (X_dev.shape[0], -1)) # Normalize the data: subtract the mean image mean_image = np.mean(X_train, axis = 0) X_train -= mean_image X_val -= mean_image X_test -= mean_image X_dev -= mean_image # add bias dimension and transform into columns X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]) X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]) X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]) X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))]) return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev# Invoke the above function to get our data.X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeprint 'dev data shape: ', X_dev.shapeprint 'dev labels shape: ', y_dev.shapeTrain data shape: (49000L, 3073L) Train labels shape: (49000L,) Validation data shape: (1000L, 3073L) Validation labels shape: (1000L,) Test data shape: (1000L, 3073L) Test labels shape: (1000L,) dev data shape: (500L, 3073L) dev labels shape: (500L,) Softmax ClassifierYour code for this section will all be written inside cs231n/classifiers/softmax.py.1234567891011121314# First implement the naive softmax loss function with nested loops.# Open the file cs231n/classifiers/softmax.py and implement the# softmax_loss_naive function.from cs231n.classifiers.softmax import softmax_loss_naiveimport time# Generate a random softmax weight matrix and use it to compute the loss.W = np.random.randn(3073, 10) * 0.0001loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)# As a rough sanity check, our loss should be something close to -log(0.1).print 'loss: %f' % lossprint 'sanity check: %f' % (-np.log(0.1))loss: 2.395985 sanity check: 2.302585 Inline Question 1:Why do we expect our loss to be close to -log(0.1)? Explain briefly.Your answer: Because the W is selected by random, so the probability of select the true class is 1/10. That is, 0.1.1234567891011121314# Complete the implementation of softmax_loss_naive and implement a (naive)# version of the gradient that uses nested loops.loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)# As we did for the SVM, use numeric gradient checking as a debugging tool.# The numeric gradient should be close to the analytic gradient.from cs231n.gradient_check import grad_check_sparsef = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]grad_numerical = grad_check_sparse(f, W, grad, 10)# similar to SVM case, do another gradient check with regularizationloss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]grad_numerical = grad_check_sparse(f, W, grad, 10)numerical: 2.368141 analytic: 2.368141, relative error: 2.349797e-08 numerical: 1.324690 analytic: 1.324690, relative error: 7.140560e-08 numerical: 3.170412 analytic: 3.170411, relative error: 1.324741e-08 numerical: 0.249509 analytic: 0.249509, relative error: 2.647240e-08 numerical: 1.536095 analytic: 1.536095, relative error: 4.345856e-08 numerical: 1.075819 analytic: 1.075819, relative error: 3.902323e-08 numerical: -0.198098 analytic: -0.198098, relative error: 5.737134e-08 numerical: -0.089902 analytic: -0.089902, relative error: 8.604010e-07 numerical: -0.339487 analytic: -0.339487, relative error: 3.992996e-08 numerical: -4.819781 analytic: -4.819781, relative error: 3.465667e-09 numerical: 1.869922 analytic: 1.869921, relative error: 7.536693e-08 numerical: 0.783465 analytic: 0.783465, relative error: 6.960291e-08 numerical: -3.206007 analytic: -3.206007, relative error: 2.337350e-09 numerical: 0.532183 analytic: 0.532183, relative error: 1.498128e-07 numerical: 0.900500 analytic: 0.900500, relative error: 6.954913e-09 numerical: -0.353224 analytic: -0.353224, relative error: 1.836960e-07 numerical: -1.331470 analytic: -1.331470, relative error: 2.726426e-08 numerical: -0.082452 analytic: -0.082452, relative error: 7.712355e-07 numerical: -1.322133 analytic: -1.322133, relative error: 5.516628e-09 numerical: 0.345814 analytic: 0.345814, relative error: 1.251858e-07 1234567891011121314151617181920# Now that we have a naive implementation of the softmax loss function and its gradient,# implement a vectorized version in softmax_loss_vectorized.# The two versions should compute the same results, but the vectorized version should be# much faster.tic = time.time()loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)from cs231n.classifiers.softmax import softmax_loss_vectorizedtic = time.time()loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)# As we did for the SVM, we use the Frobenius norm to compare the two versions# of the gradient.grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)print 'Gradient difference: %f' % grad_differencenaive loss: 2.395985e+00 computed in 0.080000s vectorized loss: 2.395985e+00 computed in 0.003000s Loss difference: 0.000000 Gradient difference: 0.000000 1234567891011121314151617181920212223242526272829303132333435363738394041424344# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of over 0.35 on the validation set.from cs231n.classifiers import Softmaxresults = &#123;&#125;best_val = -1best_softmax = Nonelearning_rates = [1e-8, 1e-7, 2e-7]regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]################################################################################# TODO: ## Use the validation set to set the learning rate and regularization strength. ## This should be identical to the validation that you did for the SVM; save ## the best trained softmax classifer in best_softmax. #################################################################################iters = 2000for lr in learning_rates: for rs in regularization_strengths: softmax = Softmax() softmax.train(X_train, y_train, learning_rate=lr, reg=rs, num_iters=iters) y_train_pred = softmax.predict(X_train) acc_train = np.mean(y_train == y_train_pred) y_val_pred = softmax.predict(X_val) acc_val = np.mean(y_val == y_val_pred) results[(lr, rs)] = (acc_train, acc_val) if best_val &lt; acc_val: best_val = acc_val best_softmax = softmax################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print 'lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy) print 'best validation accuracy achieved during cross-validation: %f' % best_vallr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.175633 val accuracy: 0.179000 lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.174102 val accuracy: 0.161000 lr 1.000000e-08 reg 3.000000e+04 train accuracy: 0.203490 val accuracy: 0.210000 lr 1.000000e-08 reg 4.000000e+04 train accuracy: 0.191367 val accuracy: 0.202000 lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.208000 val accuracy: 0.197000 lr 1.000000e-08 reg 6.000000e+04 train accuracy: 0.203571 val accuracy: 0.215000 lr 1.000000e-08 reg 7.000000e+04 train accuracy: 0.213551 val accuracy: 0.215000 lr 1.000000e-08 reg 8.000000e+04 train accuracy: 0.238347 val accuracy: 0.229000 lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.245102 val accuracy: 0.242000 lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.358265 val accuracy: 0.362000 lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.356306 val accuracy: 0.374000 lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.347327 val accuracy: 0.362000 lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.336347 val accuracy: 0.354000 lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.331490 val accuracy: 0.348000 lr 1.000000e-07 reg 6.000000e+04 train accuracy: 0.320163 val accuracy: 0.336000 lr 1.000000e-07 reg 7.000000e+04 train accuracy: 0.314551 val accuracy: 0.325000 lr 1.000000e-07 reg 8.000000e+04 train accuracy: 0.313082 val accuracy: 0.324000 lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.303000 val accuracy: 0.315000 lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.374163 val accuracy: 0.389000 lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.353184 val accuracy: 0.365000 lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.340265 val accuracy: 0.359000 lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.334673 val accuracy: 0.351000 lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.326531 val accuracy: 0.337000 lr 2.000000e-07 reg 6.000000e+04 train accuracy: 0.319857 val accuracy: 0.336000 lr 2.000000e-07 reg 7.000000e+04 train accuracy: 0.317878 val accuracy: 0.329000 lr 2.000000e-07 reg 8.000000e+04 train accuracy: 0.310449 val accuracy: 0.329000 lr 2.000000e-07 reg 1.000000e+05 train accuracy: 0.316286 val accuracy: 0.315000 best validation accuracy achieved during cross-validation: 0.389000 12345# evaluate on test set# Evaluate the best softmax on test sety_test_pred = best_softmax.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )softmax on raw pixels final test set accuracy: 0.375000 â€‹123456789101112131415# Visualize the learned weights for each classw = best_softmax.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in xrange(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])Codes123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899import numpy as npfrom random import shuffledef softmax_loss_naive(W, X, y, reg): """ Softmax loss function, naive implementation (with loops) Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# num_train = X.shape[0] num_classes = W.shape[1] for i in xrange(num_train): f = X[i, :].dot(W) f -= np.max(f) correct_f = f[y[i]] denom = np.sum(np.exp(f)) p = np.exp(correct_f) / denom loss += -np.log(p) for j in xrange(num_classes): if j == y[i]: dW[:, y[i]] += (np.exp(f[j]) / denom - 1) * X[i, :] else: dW[:, j] += (np.exp(f[j]) / denom) * X[i, :] loss /= num_train loss += 0.5 * reg * np.sum(W * W) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dWdef softmax_loss_vectorized(W, X, y, reg): """ Softmax loss function, vectorized version. Inputs and outputs are the same as softmax_loss_naive. """ # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) ############################################################################# # TODO: Compute the softmax loss and its gradient using no explicit loops. # # Store the loss in loss and the gradient in dW. If you are not careful # # here, it is easy to run into numeric instability. Don't forget the # # regularization! # ############################################################################# num_train = X.shape[0] f = X.dot(W) f = f - np.max(f, axis=1)[:, np.newaxis] loss = -np.sum( np.log(np.exp(f[np.arange(num_train), y]) / np.sum(np.exp(f), axis=1))) loss /= num_train loss += 0.5 * reg * np.sum(W * W) ind = np.zeros_like(f) ind[np.arange(num_train), y] = 1 dW = X.T.dot(np.exp(f) / np.sum(np.exp(f), axis=1, keepdims=True) - ind) dW /= num_train dW += reg * W ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dW]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 svm]]></title>
      <url>%2F2017%2F03%2F03%2Fcs231n-Assignment-1-svm%2F</url>
      <content type="text"><![CDATA[Multiclass Support Vector Machine exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website.In this exercise you will:â€‹implement a fully-vectorized loss function for the SVMimplement the fully-vectorized expression for its analytic gradientcheck your implementation using numerical gradientuse a validation set to tune the learning rate and regularization strengthoptimize the loss function with SGDvisualize the final learned weights123456789101112131415161718# Run some setup code for this notebook.import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the# notebook rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2CIFAR-10 Data Loading and Preprocessing123456789# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print 'Training data shape: ', X_train.shapeprint 'Training labels shape: ', y_train.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTraining data shape: (50000L, 32L, 32L, 3L) Training labels shape: (50000L,) Test data shape: (10000L, 32L, 32L, 3L) Test labels shape: (10000L,) 12345678910111213141516# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show()1?np.random.choice1234567891011121314151617181920212223242526272829303132333435363738# Split the data into train, val, and test sets. In addition we will# create a small development set as a subset of the training data;# we can use this for development so our code runs faster.num_training = 49000num_validation = 1000num_test = 1000num_dev = 500# Our validation set will be num_validation points from the original# training set.mask = range(num_training, num_training + num_validation)X_val = X_train[mask]y_val = y_train[mask]# Our training set will be the first num_train points from the original# training set.mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]# We will also make a development set, which is a small subset of# the training set.mask = np.random.choice(num_training, num_dev, replace=False)X_dev = X_train[mask]y_dev = y_train[mask]# We use the first num_test points of the original test set as our# test set.mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask]print 'Train data shape: ', X_train.shapeprint 'Train labels shape: ', y_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Validation labels shape: ', y_val.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTrain data shape: (49000L, 32L, 32L, 3L) Train labels shape: (49000L,) Validation data shape: (1000L, 32L, 32L, 3L) Validation labels shape: (1000L,) Test data shape: (1000L, 32L, 32L, 3L) Test labels shape: (1000L,) 1234567891011# Preprocessing: reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1))X_val = np.reshape(X_val, (X_val.shape[0], -1))X_test = np.reshape(X_test, (X_test.shape[0], -1))X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))# As a sanity check, print out the shapes of the dataprint 'Training data shape: ', X_train.shapeprint 'Validation data shape: ', X_val.shapeprint 'Test data shape: ', X_test.shapeprint 'dev data shape: ', X_dev.shapeTraining data shape: (49000L, 3072L) Validation data shape: (1000L, 3072L) Test data shape: (1000L, 3072L) dev data shape: (500L, 3072L) 1234567# Preprocessing: subtract the mean image# first: compute the image mean based on the training datamean_image = np.mean(X_train, axis=0)print mean_image[:10] # print a few of the elementsplt.figure(figsize=(4,4))plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean imageplt.show()[ 130.64189796 135.98173469 132.47391837 130.05569388 135.34804082 131.75402041 130.96055102 136.14328571 132.47636735 131.48467347] 12345# second: subtract the mean image from train and test dataX_train -= mean_imageX_val -= mean_imageX_test -= mean_imageX_dev -= mean_image12345678# third: append (at the last) the bias dimension of ones (i.e. bias trick) so that our SVM# only has to worry about optimizing a single weight matrix W.X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])print X_train.shape, X_val.shape, X_test.shape, X_dev.shape(49000L, 3073L) (1000L, 3073L) (1000L, 3073L) (500L, 3073L) â€‹SVM ClassifierYour code for this section will all be written inside cs231n/classifiers/linear_svm.py.As you can see, we have prefilled the function compute_loss_naive which uses for loops to evaluate the multiclass SVM loss function.123456789# Evaluate the naive implementation of the loss we provided for you:from cs231n.classifiers.linear_svm import svm_loss_naiveimport time# generate a random SVM weight matrix of small numbersW = np.random.randn(3073, 10) * 0.0001 loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.00001)print 'loss: %f' % (loss, )loss: 8.831645 â€‹The grad returned from the function above is right now all zero. Derive and implement the gradient for the SVM cost function and implement it inline inside the function svm_loss_naive. You will find it helpful to interleave your new code inside the existing function.To check that you have correctly implemented the gradient correctly, you can numerically estimate the gradient of the loss function and compare the numeric estimate to the gradient that you computed. We have provided code that does this for you:123456789101112131415161718# Once you've implemented the gradient, recompute it with the code below# and gradient check it with the function we provided for you# Compute the loss and its gradient at W.loss, grad = svm_loss_naive(W, X_dev, y_dev, 0.0)# Numerically compute the gradient along several randomly chosen dimensions, and# compare them with your analytically computed gradient. The numbers should match# almost exactly along all dimensions.from cs231n.gradient_check import grad_check_sparsef = lambda w: svm_loss_naive(w, X_dev, y_dev, 0.0)[0]grad_numerical = grad_check_sparse(f, W, grad)# do the gradient check once again with regularization turned on# you didn't forget the regularization gradient did you?loss, grad = svm_loss_naive(W, X_dev, y_dev, 1e2)f = lambda w: svm_loss_naive(w, X_dev, y_dev, 1e2)[0]grad_numerical = grad_check_sparse(f, W, grad)numerical: -13.865929 analytic: -13.865929, relative error: 1.283977e-12 numerical: 7.842142 analytic: 7.735021, relative error: 6.876784e-03 numerical: 3.464393 analytic: 3.464393, relative error: 9.040092e-11 numerical: -23.034911 analytic: -23.034911, relative error: 6.876266e-12 numerical: -0.185311 analytic: -0.185311, relative error: 2.538774e-10 numerical: 25.825504 analytic: 25.825504, relative error: 1.336035e-11 numerical: 4.457836 analytic: 4.457836, relative error: 1.015819e-10 numerical: 3.184691 analytic: 3.184691, relative error: 8.849109e-11 numerical: 10.428446 analytic: 10.374317, relative error: 2.601982e-03 numerical: 12.479957 analytic: 12.479957, relative error: 6.825191e-12 numerical: 12.237949 analytic: 12.326308, relative error: 3.597051e-03 numerical: 4.377103 analytic: 4.377103, relative error: 3.904758e-11 numerical: -1.951930 analytic: -1.951930, relative error: 1.432276e-10 numerical: 33.752503 analytic: 33.752503, relative error: 4.254520e-12 numerical: 11.367149 analytic: 11.367149, relative error: 1.682727e-11 numerical: 16.461879 analytic: 16.461879, relative error: 4.766805e-12 numerical: 3.814562 analytic: 3.814562, relative error: 1.087469e-10 numerical: 13.931226 analytic: 13.931226, relative error: 9.578349e-12 numerical: -27.291095 analytic: -27.395406, relative error: 1.907445e-03 numerical: -7.610407 analytic: -7.610407, relative error: 1.015282e-12 Inline Question 1:It is possible that once in a while a dimension in the gradcheck will not match exactly. What could such a discrepancy be caused by? Is it a reason for concern? What is a simple example in one dimension where a gradient check could fail? Hint: the SVM loss function is not strictly speaking differentiableYour Answer: Maybe the SVM loss function is not differentiable on that dimension1?np.max1np.sum(np.maximum(0, X_dev.dot(W) - X_dev.dot(W)[np.arange(len(y_dev)), [y_dev]].T + 1))4915.822409730994 123456789101112131415# Next implement the function svm_loss_vectorized; for now only compute the loss;# we will implement the gradient in a moment.tic = time.time()loss_naive, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Naive loss: %e computed in %fs' % (loss_naive, toc - tic)from cs231n.classifiers.linear_svm import svm_loss_vectorizedtic = time.time()loss_vectorized, _ = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)# The losses should match but your vectorized implementation should be much faster.print 'difference: %f' % (loss_naive - loss_vectorized)Naive loss: 8.831645e+00 computed in 0.071000s Vectorized loss: 8.831645e+00 computed in 0.000000s difference: 0.000000 1234567891011121314151617181920# Complete the implementation of svm_loss_vectorized, and compute the gradient# of the loss function in a vectorized way.# The naive implementation and the vectorized implementation should match, but# the vectorized version should still be much faster.tic = time.time()_, grad_naive = svm_loss_naive(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Naive loss and gradient: computed in %fs' % (toc - tic)tic = time.time()_, grad_vectorized = svm_loss_vectorized(W, X_dev, y_dev, 0.00001)toc = time.time()print 'Vectorized loss and gradient: computed in %fs' % (toc - tic)# The loss is a single number, so it is easy to compare the values computed# by the two implementations. The gradient on the other hand is a matrix, so# we use the Frobenius norm to compare them.difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')print 'difference: %f' % differenceNaive loss and gradient: computed in 0.084000s Vectorized loss and gradient: computed in 0.005000s difference: 0.000000 Stochastic Gradient DescentWe now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss.123456789# In the file linear_classifier.py, implement SGD in the function# LinearClassifier.train() and then run it with the code below.from cs231n.classifiers import LinearSVMsvm = LinearSVM()tic = time.time()loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4, num_iters=1500, verbose=True)toc = time.time()print 'That took %fs' % (toc - tic)iteration 0 / 1500: loss 791.772037 iteration 100 / 1500: loss 286.021346 iteration 200 / 1500: loss 107.673095 iteration 300 / 1500: loss 41.812791 iteration 400 / 1500: loss 18.665578 iteration 500 / 1500: loss 10.614984 iteration 600 / 1500: loss 6.664814 iteration 700 / 1500: loss 6.509693 iteration 800 / 1500: loss 5.792204 iteration 900 / 1500: loss 4.986855 iteration 1000 / 1500: loss 5.914691 iteration 1100 / 1500: loss 5.058078 iteration 1200 / 1500: loss 5.491475 iteration 1300 / 1500: loss 5.609450 iteration 1400 / 1500: loss 5.376595 That took 5.454000s 123456# A useful debugging strategy is to plot the loss as a function of# iteration number:plt.plot(loss_hist)plt.xlabel('Iteration number')plt.ylabel('Loss value')plt.show()123456# Write the LinearSVM.predict function and evaluate the performance on both the# training and validation sety_train_pred = svm.predict(X_train)print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )y_val_pred = svm.predict(X_val)print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )training accuracy: 0.364980 validation accuracy: 0.378000 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# Use the validation set to tune hyperparameters (regularization strength and# learning rate). You should experiment with different ranges for the learning# rates and regularization strengths; if you are careful you should be able to# get a classification accuracy of about 0.4 on the validation set.learning_rates = [1e-8, 1e-7, 2e-7]regularization_strengths = [1e4, 2e4, 3e4, 4e4, 5e4, 6e4, 7e4, 8e4, 1e5]# results is dictionary mapping tuples of the form# (learning_rate, regularization_strength) to tuples of the form# (training_accuracy, validation_accuracy). The accuracy is simply the fraction# of data points that are correctly classified.results = &#123;&#125;best_val = -1 # The highest validation accuracy that we have seen so far.best_svm = None # The LinearSVM object that achieved the highest validation rate.################################################################################# TODO: ## Write code that chooses the best hyperparameters by tuning on the validation ## set. For each combination of hyperparameters, train a linear SVM on the ## training set, compute its accuracy on the training and validation sets, and ## store these numbers in the results dictionary. In addition, store the best ## validation accuracy in best_val and the LinearSVM object that achieves this ## accuracy in best_svm. ## ## Hint: You should use a small value for num_iters as you develop your ## validation code so that the SVMs don't take much time to train; once you are ## confident that your validation code works, you should rerun the validation ## code with a larger value for num_iters. #################################################################################for learning_rate in learning_rates: for regularization_strength in regularization_strengths: svm = LinearSVM() loss_hist = svm.train( X_train, y_train, learning_rate, \ regularization_strength, num_iters=1500, batch_size=200) y_train_pred = svm.predict(X_train) y_val_pred = svm.predict(X_val) training_accuracy = np.mean(y_train == y_train_pred) validation_accuracy = np.mean(y_val == y_val_pred) results[(learning_rate, regularization_strength)] = \ (training_accuracy, validation_accuracy) if validation_accuracy &gt; best_val: best_val = validation_accuracy best_svm = svm################################################################################# END OF YOUR CODE ################################################################################# # Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print 'lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy) print 'best validation accuracy achieved during cross-validation: %f' % best_vallr 1.000000e-08 reg 1.000000e+04 train accuracy: 0.221898 val accuracy: 0.247000 lr 1.000000e-08 reg 2.000000e+04 train accuracy: 0.233653 val accuracy: 0.258000 lr 1.000000e-08 reg 3.000000e+04 train accuracy: 0.234694 val accuracy: 0.225000 lr 1.000000e-08 reg 4.000000e+04 train accuracy: 0.255959 val accuracy: 0.249000 lr 1.000000e-08 reg 5.000000e+04 train accuracy: 0.259755 val accuracy: 0.273000 lr 1.000000e-08 reg 6.000000e+04 train accuracy: 0.267408 val accuracy: 0.269000 lr 1.000000e-08 reg 7.000000e+04 train accuracy: 0.269102 val accuracy: 0.287000 lr 1.000000e-08 reg 8.000000e+04 train accuracy: 0.277102 val accuracy: 0.285000 lr 1.000000e-08 reg 1.000000e+05 train accuracy: 0.295306 val accuracy: 0.301000 lr 1.000000e-07 reg 1.000000e+04 train accuracy: 0.369388 val accuracy: 0.374000 lr 1.000000e-07 reg 2.000000e+04 train accuracy: 0.380265 val accuracy: 0.390000 lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.375490 val accuracy: 0.378000 lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.375633 val accuracy: 0.385000 lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.369694 val accuracy: 0.375000 lr 1.000000e-07 reg 6.000000e+04 train accuracy: 0.372469 val accuracy: 0.383000 lr 1.000000e-07 reg 7.000000e+04 train accuracy: 0.356000 val accuracy: 0.370000 lr 1.000000e-07 reg 8.000000e+04 train accuracy: 0.352816 val accuracy: 0.355000 lr 1.000000e-07 reg 1.000000e+05 train accuracy: 0.356796 val accuracy: 0.377000 lr 2.000000e-07 reg 1.000000e+04 train accuracy: 0.393510 val accuracy: 0.395000 lr 2.000000e-07 reg 2.000000e+04 train accuracy: 0.377020 val accuracy: 0.382000 lr 2.000000e-07 reg 3.000000e+04 train accuracy: 0.363857 val accuracy: 0.373000 lr 2.000000e-07 reg 4.000000e+04 train accuracy: 0.368714 val accuracy: 0.372000 lr 2.000000e-07 reg 5.000000e+04 train accuracy: 0.361531 val accuracy: 0.364000 lr 2.000000e-07 reg 6.000000e+04 train accuracy: 0.354714 val accuracy: 0.368000 lr 2.000000e-07 reg 7.000000e+04 train accuracy: 0.348306 val accuracy: 0.365000 lr 2.000000e-07 reg 8.000000e+04 train accuracy: 0.358082 val accuracy: 0.378000 lr 2.000000e-07 reg 1.000000e+05 train accuracy: 0.347898 val accuracy: 0.358000 best validation accuracy achieved during cross-validation: 0.395000 123456789101112131415161718192021222324# Visualize the cross-validation resultsimport mathx_scatter = [math.log10(x[0]) for x in results]y_scatter = [math.log10(x[1]) for x in results]# plot training accuracymarker_size = 100colors = [results[x][0] for x in results]plt.subplot(3, 1, 1)plt.scatter(x_scatter, y_scatter, marker_size, c=colors)plt.colorbar()plt.xlabel('log learning rate')plt.ylabel('log regularization strength')plt.title('CIFAR-10 training accuracy')# plot validation accuracycolors = [results[x][1] for x in results] # default size of markers is 20plt.subplot(3, 1, 3)plt.scatter(x_scatter, y_scatter, marker_size, c=colors)plt.colorbar()plt.xlabel('log learning rate')plt.ylabel('log regularization strength')plt.title('CIFAR-10 validation accuracy')plt.show()1234# Evaluate the best svm on test sety_test_pred = best_svm.predict(X_test)test_accuracy = np.mean(y_test == y_test_pred)print 'linear SVM on raw pixels final test set accuracy: %f' % test_accuracylinear SVM on raw pixels final test set accuracy: 0.383000 â€‹12x = np.array([[[0], [1], [2]]])np.squeeze(x)array([0, 1, 2]) 123456789101112131415# Visualize the learned weights for each class.# Depending on your choice of learning rate and regularization strength, these may# or may not be nice to look at.w = best_svm.W[:-1,:] # strip out the biasw = w.reshape(32, 32, 3, 10)w_min, w_max = np.min(w), np.max(w)classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']for i in xrange(10): plt.subplot(2, 5, i + 1) # Rescale the weights to be between 0 and 255 wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min) plt.imshow(wimg.astype('uint8')) plt.axis('off') plt.title(classes[i])Inline question 2:Describe what your visualized SVM weights look like, and offer a brief explanation for why they look they way that they do.Your answer: fill this inCodeslinear_svm.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import numpy as npfrom random import shuffledef svm_loss_naive(W, X, y, reg): """ Structured SVM loss function, naive implementation (with loops). Inputs have dimension D, there are C classes, and we operate on minibatches of N examples. Inputs: - W: A numpy array of shape (D, C) containing weights. - X: A numpy array of shape (N, D) containing a minibatch of data. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label c, where 0 &lt;= c &lt; C. - reg: (float) regularization strength Returns a tuple of: - loss as single float - gradient with respect to weights W; an array of same shape as W """ dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in xrange(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in xrange(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: dW[:, y[i]] -= X[i, :] dW[:, j] += X[i, :] loss += margin # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += 0.5 * reg * np.sum(W * W) dW += reg * W ############################################################################# # TODO: # # Compute the gradient of the loss function and store it dW. # # Rather that first computing the loss and then computing the derivative, # # it may be simpler to compute the derivative at the same time that the # # loss is being computed. As a result you may need to modify some of the # # code above to compute the gradient. # ############################################################################# return loss, dWdef svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero ############################################################################# # TODO: # # Implement a vectorized version of the structured SVM loss, storing the # # result in loss. # ############################################################################# num_train = X.shape[0] delta = 1.0 scores = X.dot(W) correct_class_score = scores[np.arange(num_train), y] margins = np.maximum( 0, scores - correct_class_score[:, np.newaxis] + delta) margins[np.arange(num_train), y] = 0 loss = np.sum(margins) loss /= num_train loss += 0.5 * reg * np.sum(W.T.dot(W)) ############################################################################# # END OF YOUR CODE # ############################################################################# ############################################################################# # TODO: # # Implement a vectorized version of the gradient for the structured SVM # # loss, storing the result in dW. # # # # Hint: Instead of computing the gradient from scratch, it may be easier # # to reuse some of the intermediate values that you used to compute the # # loss. # ############################################################################# X_mask = np.zeros(margins.shape) X_mask[margins &gt; 0] = 1 count = np.sum(X_mask, axis=1) X_mask[np.arange(num_train), y] = -count dW = X.T.dot(X_mask) dW /= num_train dW += np.multiply(W, reg) ############################################################################# # END OF YOUR CODE # ############################################################################# return loss, dWlinear_classifier123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import numpy as npfrom cs231n.classifiers.linear_svm import *from cs231n.classifiers.softmax import *class LinearClassifier(object): def __init__(self): self.W = None def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): """ Train this linear classifier using stochastic gradient descent. Inputs: - X: A numpy array of shape (N, D) containing training data; there are N training samples each of dimension D. - y: A numpy array of shape (N,) containing training labels; y[i] = c means that X[i] has label 0 &lt;= c &lt; C for C classes. - learning_rate: (float) learning rate for optimization. - reg: (float) regularization strength. - num_iters: (integer) number of steps to take when optimizing - batch_size: (integer) number of training examples to use at each step. - verbose: (boolean) If true, print progress during optimization. Outputs: A list containing the value of the loss function at each training iteration. """ num_train, dim = X.shape num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in xrange(num_iters): X_batch = None y_batch = None ######################################################################### # TODO: # # Sample batch_size elements from the training data and their # # corresponding labels to use in this round of gradient descent. # # Store the data in X_batch and their corresponding labels in # # y_batch; after sampling X_batch should have shape (dim, batch_size) # # and y_batch should have shape (batch_size,) # # # # Hint: Use np.random.choice to generate indices. Sampling with # # replacement is faster than sampling without replacement. # ######################################################################### mask = np.random.choice(num_train, batch_size, replace=True) X_batch = X[mask] y_batch = y[mask] ######################################################################### # END OF YOUR CODE # ######################################################################### # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) # perform parameter update ######################################################################### # TODO: # # Update the weights using the gradient and the learning rate. # ######################################################################### self.W = self.W - learning_rate * grad ######################################################################### # END OF YOUR CODE # ######################################################################### if verbose and it % 100 == 0: print 'iteration %d / %d: loss %f' % (it, num_iters, loss) return loss_history def predict(self, X): """ Use the trained weights of this linear classifier to predict labels for data points. Inputs: - X: D x N array of training data. Each column is a D-dimensional point. Returns: - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional array of length N, and each element is an integer giving the predicted class. """ X = X.T y_pred = np.zeros(X.shape[1]) ########################################################################### # TODO: # # Implement this method. Store the predicted labels in y_pred. # ########################################################################### scores = X.T.dot(self.W) y_pred = np.argsort(scores, axis=1)[:, -1] ########################################################################### # END OF YOUR CODE # ########################################################################### return y_pred def loss(self, X_batch, y_batch, reg): """ Compute the loss function and its derivative. Subclasses will override this. Inputs: - X_batch: A numpy array of shape (N, D) containing a minibatch of N data points; each point has dimension D. - y_batch: A numpy array of shape (N,) containing labels for the minibatch. - reg: (float) regularization strength. Returns: A tuple containing: - loss as a single float - gradient with respect to self.W; an array of the same shape as W """ passclass LinearSVM(LinearClassifier): """ A subclass that uses the Multiclass SVM loss function """ def loss(self, X_batch, y_batch, reg): return svm_loss_vectorized(self.W, X_batch, y_batch, reg)class Softmax(LinearClassifier): """ A subclass that uses the Softmax + Cross-entropy loss function """ def loss(self, X_batch, y_batch, reg): return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n Assignment#1 kNN]]></title>
      <url>%2F2017%2F03%2F02%2Fcs231n-Assignment-1-kNN%2F</url>
      <content type="text"><![CDATA[k-Nearest Neighbor (kNN) exerciseComplete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the assignments page on the course website.The kNN classifier consists of two stages:During training, the classifier takes the training data and simply remembers itDuring testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examplesThe value of k is cross-validatedIn this exercise you will implement these steps and understand the basic Image Classification pipeline, cross-validation, and gain proficiency in writing efficient, vectorized code.123456789101112131415161718# Run some setup code for this notebook.import randomimport numpy as npfrom cs231n.data_utils import load_CIFAR10import matplotlib.pyplot as plt# This is a bit of magic to make matplotlib figures appear inline in the notebook# rather than in a new window.%matplotlib inlineplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'# Some more magic so that the notebook will reload external python modules;# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython%load_ext autoreload%autoreload 2123456789# Load the raw CIFAR-10 data.cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)# As a sanity check, we print out the size of the training and test data.print 'Training data shape: ', X_train.shapeprint 'Training labels shape: ', y_train.shapeprint 'Test data shape: ', X_test.shapeprint 'Test labels shape: ', y_test.shapeTraining data shape: (50000L, 32L, 32L, 3L) Training labels shape: (50000L,) Test data shape: (10000L, 32L, 32L, 3L) Test labels shape: (10000L,) 12345678910111213141516# Visualize some examples from the dataset.# We show a few examples of training images from each class.classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']num_classes = len(classes)samples_per_class = 7for y, cls in enumerate(classes): idxs = np.flatnonzero(y_train == y) idxs = np.random.choice(idxs, samples_per_class, replace=False) for i, idx in enumerate(idxs): plt_idx = i * num_classes + y + 1 plt.subplot(samples_per_class, num_classes, plt_idx) plt.imshow(X_train[idx].astype('uint8')) plt.axis('off') if i == 0: plt.title(cls)plt.show()12345678910# Subsample the data for more efficient code execution in this exercisenum_training = 5000mask = range(num_training)X_train = X_train[mask]y_train = y_train[mask]num_test = 500mask = range(num_test)X_test = X_test[mask]y_test = y_test[mask]1234# Reshape the image data into rowsX_train = np.reshape(X_train, (X_train.shape[0], -1)) # Wow~X_test = np.reshape(X_test, (X_test.shape[0], -1))print X_train.shape, X_test.shape(5000L, 3072L) (500L, 3072L) â€‹1234567from cs231n.classifiers import KNearestNeighbor# Create a kNN classifier instance. # Remember that training a kNN classifier is a noop: # the Classifier simply remembers the data and does no further processing classifier = KNearestNeighbor()classifier.train(X_train, y_train)We would now like to classify the test data with the kNN classifier. Recall that we can break down this process into two steps:First we must compute the distances between all test examples and all train examples.Given these distances, for each test example we find the k nearest examples and have them vote for the labelLets begin with computing the distance matrix between all training and test examples. For example, if there are Ntr training examples and Nte test examples, this stage should result in a Nte x Ntr matrix where each element (i,j) is the distance between the i-th test and j-th train example.First, open cs231n/classifiers/k_nearest_neighbor.py and implement the function compute_distances_two_loops that uses a (very inefficient) double loop over all pairs of (test, train) examples and computes the distance matrix one element at a time.123456# Open cs231n/classifiers/k_nearest_neighbor.py and implement# compute_distances_two_loops.# Test your implementation:dists = classifier.compute_distances_two_loops(X_test)print dists.shape(500L, 5000L) â€‹1234# We can visualize the distance matrix: each row is a single test example and# its distances to training examplesplt.imshow(dists, interpolation='none')plt.show()Inline Question #1: Notice the structured patterns in the distance matrix, where some rows or columns are visible brighter. (Note that with the default color scheme black indicates low distances while white indicates high distances.)What in the data is the cause behind the distinctly bright rows?What causes the columns?Your Answer: Maybe exists noises in test data set and train data set.12345678# Now implement the function predict_labels and run the code below:# We use k = 1 (which is Nearest Neighbor).y_test_pred = classifier.predict_labels(dists, k=1)# Compute and print the fraction of correctly predicted examplesnum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)Got 137 / 500 correct =&gt; accuracy: 0.274000 â€‹You should expect to see approximately 27% accuracy. Now lets try out a larger k, say k = 5:1234y_test_pred = classifier.predict_labels(dists, k=5)num_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)Got 142 / 500 correct =&gt; accuracy: 0.284000 â€‹You should expect to see a slightly better performance than with k = 1.1234567891011121314151617# Now lets speed up distance matrix computation by using partial vectorization# with one loop. Implement the function compute_distances_one_loop and run the# code below:dists_one = classifier.compute_distances_one_loop(X_test)# To ensure that our vectorized implementation is correct, we make sure that it# agrees with the naive implementation. There are many ways to decide whether# two matrices are similar; one of the simplest is the Frobenius norm. In case# you haven't seen it before, the Frobenius norm of two matrices is the square# root of the squared sum of differences of all elements; in other words, reshape# the matrices into vectors and compute the Euclidean distance between them.difference = np.linalg.norm(dists - dists_one, ord='fro')print 'Difference was: %f' % (difference, )if difference &lt; 0.001: print 'Good! The distance matrices are the same'else: print 'Uh-oh! The distance matrices are different'Difference was: 0.000000 Good! The distance matrices are the same 1234567891011# Now implement the fully vectorized version inside compute_distances_no_loops# and run the codedists_two = classifier.compute_distances_no_loops(X_test)# check that the distance matrix agrees with the one we computed before:difference = np.linalg.norm(dists - dists_two, ord='fro')print 'Difference was: %f' % (difference, )if difference &lt; 0.001: print 'Good! The distance matrices are the same'else: print 'Uh-oh! The distance matrices are different'Difference was: 0.000000 Good! The distance matrices are the same 123456789101112131415161718192021# Let's compare how fast the implementations aredef time_function(f, *args): """ Call a function f with args and return the time (in seconds) that it took to execute. """ import time tic = time.time() f(*args) toc = time.time() return toc - tictwo_loop_time = time_function(classifier.compute_distances_two_loops, X_test)print 'Two loop version took %f seconds' % two_loop_timeone_loop_time = time_function(classifier.compute_distances_one_loop, X_test)print 'One loop version took %f seconds' % one_loop_timeno_loop_time = time_function(classifier.compute_distances_no_loops, X_test)print 'No loop version took %f seconds' % no_loop_time# you should see significantly faster performance with the fully vectorized implementationTwo loop version took 27.001000 seconds One loop version took 59.630000 seconds No loop version took 0.205000 seconds Cross-validationWe have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]X_train_folds = []y_train_folds = []################################################################################# TODO: ## Split up the training data into folds. After splitting, X_train_folds and ## y_train_folds should each be lists of length num_folds, where ## y_train_folds[i] is the label vector for the points in X_train_folds[i]. ## Hint: Look up the numpy array_split function. #################################################################################X_train_folds = np.array_split(X_train, num_folds)y_train_folds = np.array_split(y_train, num_folds)################################################################################# END OF YOUR CODE ################################################################################## A dictionary holding the accuracies for different values of k that we find# when running cross-validation. After running cross-validation,# k_to_accuracies[k] should be a list of length num_folds giving the different# accuracy values that we found when using that value of k.k_to_accuracies = &#123;&#125;################################################################################# TODO: ## Perform k-fold cross validation to find the best value of k. For each ## possible value of k, run the k-nearest-neighbor algorithm num_folds times, ## where in each case you use all but one of the folds as training data and the ## last fold as a validation set. Store the accuracies for all fold and all ## values of k in the k_to_accuracies dictionary. #################################################################################for k in k_choices: k_to_accuracies[k] = [] for fold in xrange(num_folds): train_X = np.append( X_train_folds[:fold], X_train_folds[fold+1:]).reshape( (X_train.shape[0] - X_train.shape[0]/num_folds, -1)) train_y = np.append( y_train_folds[:fold], y_train_folds[fold+1:]).reshape( (y_train.shape[0] - y_train.shape[0]/num_folds, -1)).flatten() classifier.train(train_X, train_y) dists = classifier.compute_distances_no_loops(X_train_folds[fold]) y_test_pred = classifier.predict_labels(dists, k) num_correct = np.sum(y_test_pred == y_train_folds[fold]) accuracy = float(num_correct) / len(y_train_folds[fold]) k_to_accuracies[k].append(accuracy) ################################################################################# END OF YOUR CODE ################################################################################## Print out the computed accuraciesfor k in sorted(k_to_accuracies): for accuracy in k_to_accuracies[k]: print 'k = %d, accuracy = %f' % (k, accuracy)k = 1, accuracy = 0.263000 k = 1, accuracy = 0.257000 k = 1, accuracy = 0.264000 k = 1, accuracy = 0.278000 k = 1, accuracy = 0.266000 k = 3, accuracy = 0.241000 k = 3, accuracy = 0.249000 k = 3, accuracy = 0.243000 k = 3, accuracy = 0.273000 k = 3, accuracy = 0.264000 k = 5, accuracy = 0.258000 k = 5, accuracy = 0.273000 k = 5, accuracy = 0.281000 k = 5, accuracy = 0.290000 k = 5, accuracy = 0.272000 k = 8, accuracy = 0.263000 k = 8, accuracy = 0.288000 k = 8, accuracy = 0.278000 k = 8, accuracy = 0.285000 k = 8, accuracy = 0.277000 k = 10, accuracy = 0.265000 k = 10, accuracy = 0.296000 k = 10, accuracy = 0.278000 k = 10, accuracy = 0.284000 k = 10, accuracy = 0.286000 k = 12, accuracy = 0.260000 k = 12, accuracy = 0.294000 k = 12, accuracy = 0.281000 k = 12, accuracy = 0.282000 k = 12, accuracy = 0.281000 k = 15, accuracy = 0.255000 k = 15, accuracy = 0.290000 k = 15, accuracy = 0.281000 k = 15, accuracy = 0.281000 k = 15, accuracy = 0.276000 k = 20, accuracy = 0.270000 k = 20, accuracy = 0.281000 k = 20, accuracy = 0.280000 k = 20, accuracy = 0.282000 k = 20, accuracy = 0.284000 k = 50, accuracy = 0.271000 k = 50, accuracy = 0.288000 k = 50, accuracy = 0.278000 k = 50, accuracy = 0.269000 k = 50, accuracy = 0.266000 k = 100, accuracy = 0.256000 k = 100, accuracy = 0.270000 k = 100, accuracy = 0.263000 k = 100, accuracy = 0.256000 k = 100, accuracy = 0.263000 12345678910111213# plot the raw observationsfor k in k_choices: accuracies = k_to_accuracies[k] plt.scatter([k] * len(accuracies), accuracies)# plot the trend line with error bars that correspond to standard deviationaccuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)plt.title('Cross-validation on k')plt.xlabel('k')plt.ylabel('Cross-validation accuracy')plt.show()12345678910111213# Based on the cross-validation results above, choose the best value for k, # retrain the classifier using all the training data, and test it on the test# data. You should be able to get above 28% accuracy on the test data.best_k = 10classifier = KNearestNeighbor()classifier.train(X_train, y_train)y_test_pred = classifier.predict(X_test, k=best_k)# Compute and display the accuracynum_correct = np.sum(y_test_pred == y_test)accuracy = float(num_correct) / num_testprint 'Got %d / %d correct =&gt; accuracy: %f' % (num_correct, num_test, accuracy)Got 139 / 500 correct =&gt; accuracy: 0.278000 â€‹k_nearest_neighbor.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186import numpy as npimport operatorclass KNearestNeighbor(object): """ a kNN classifier with L2 distance """ def __init__(self): pass def train(self, X, y): """ Train the classifier. For k-nearest neighbors this is just memorizing the training data. Inputs: - X: A numpy array of shape (num_train, D) containing the training data consisting of num_train samples each of dimension D. - y: A numpy array of shape (N,) containing the training labels, where y[i] is the label for X[i]. """ self.X_train = X self.y_train = y def predict(self, X, k=1, num_loops=0): """ Predict labels for test data using this classifier. Inputs: - X: A numpy array of shape (num_test, D) containing test data consisting of num_test samples each of dimension D. - k: The number of nearest neighbors that vote for the predicted labels. - num_loops: Determines which implementation to use to compute distances between training points and testing points. Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. """ if num_loops == 0: dists = self.compute_distances_no_loops(X) elif num_loops == 1: dists = self.compute_distances_one_loop(X) elif num_loops == 2: dists = self.compute_distances_two_loops(X) else: raise ValueError('Invalid value %d for num_loops' % num_loops) return self.predict_labels(dists, k=k) def compute_distances_two_loops(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using a nested loop over both the training data and the test data. Inputs: - X: A numpy array of shape (num_test, D) containing test data. Returns: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] is the Euclidean distance between the ith test point and the jth training point. """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): for j in xrange(num_train): ############################################################### # TODO: # # Compute the l2 distance between the ith test point and the jth # # training point, and store the result in dists[i, j]. You should # # not use a loop over dimension. # ############################################################### dists[i, j] = np.sqrt(np.sum((X[i, :] - self.X_train[j, :]) ** 2)) ############################################################### # END OF YOUR CODE # ############################################################### return dists def compute_distances_one_loop(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using a single loop over the test data. Input / Output: Same as compute_distances_two_loops """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in xrange(num_test): ################################################################### # TODO: # # Compute the l2 distance between the ith test point and all training # # points, and store the result in dists[i, :]. # ################################################################### dists[i, :] = np.sqrt(np.sum(np.square(X[i, :] - self.X_train), axis=1)) ################################################################### # END OF YOUR CODE # ################################################################### return dists def compute_distances_no_loops(self, X): """ Compute the distance between each test point in X and each training point in self.X_train using no explicit loops. Input / Output: Same as compute_distances_two_loops """ num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) ####################################################################### # TODO: # # Compute the l2 distance between all test points and all training # # points without using any explicit loops, and store the result in # # dists. # # # # You should implement this function using only basic array operations; # # in particular you should not use functions from scipy. # # # # HINT: Try to formulate the l2 distance using matrix multiplication # # and two broadcast sums. # ####################################################################### dists = np.sqrt(np.multiply(np.dot(X, self.X_train.T), -2) + np.sum(self.X_train ** 2, axis=1) + np.sum(X ** 2, axis=1)[:, np.newaxis]) ####################################################################### # END OF YOUR CODE # ####################################################################### return dists def predict_labels(self, dists, k=1): """ Given a matrix of distances between test points and training points, predict a label for each test point. Inputs: - dists: A numpy array of shape (num_test, num_train) where dists[i, j] gives the distance betwen the ith test point and the jth training point. Returns: - y: A numpy array of shape (num_test,) containing predicted labels for the test data, where y[i] is the predicted label for the test point X[i]. """ num_test = dists.shape[0] y_pred = np.zeros(num_test) for i in xrange(num_test): # A list of length k storing the labels of the k nearest neighbors to # the ith test point. closest_y = [] ################################################################### # TODO: # # Use the distance matrix to find the k nearest neighbors of the ith # # testing point, and use self.y_train to find the labels of these # # neighbors. Store these labels in closest_y. # # Hint: Look up the function numpy.argsort. # ################################################################### k_nearest_index = np.argsort(dists[i, :])[:k] ################################################################### # TODO: # # Now that you have found the labels of the k nearest neighbors, you # # need to find the most common label in the list closest_y of labels. # # Store this label in y_pred[i]. Break ties by choosing the smaller # # label. # ################################################################### closest_y = self.y_train[k_nearest_index] labels_counts = &#123;&#125; for label in closest_y: if label in labels_counts.keys(): labels_counts[label] += 1 else: labels_counts[label] = 0 sorted_labels_counts = sorted( labels_counts.items(), key=operator.itemgetter(1), reverse=True) y_pred[i] = sorted_labels_counts[0][0] ################################################################### # END OF YOUR CODE # ################################################################### return y_pred]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS231n Lecture3 note]]></title>
      <url>%2F2017%2F03%2F02%2FCS231n-Lecture3-note%2F</url>
      <content type="text"><![CDATA[Loss functionSource is here.Multiclass Support Vector Machine lossThe SVM loss is set up so that the SVM â€œwantsâ€ the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$The Multiclass SVM loss for the i-th example is formalized as follows:Example$s = [13, -7, 11]$ and $\Delta = 10$, then,$$L_i = \max(0, -7 - 13 + 10) + \max(0, 11 - 13 + 10)$$In summary, the SVM loss function wants the score of the correct class $y_i$ to be larger than the incorrect class scores by at least by $\Delta$ (delta). If this is not the case, we will accumulate loss.Note that $f(x_i; W) = W x_i$, so we can also rewrite the loss function in this equivalent form:$$L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)$$A last piece of terminology weâ€™ll mention before we finish with this section is that the threshold at zero $\max(0,âˆ’)$ function is often called the hinge loss. Youâ€™ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form $\max(0,âˆ’)^2$ that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation.The follow image shows the motivation of the SVM loss functionâ€RegularizationThere is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters W that correctly classify every example (i.e. all scores are so that all the margins are met, and $L_i=0$ for all i). The issue is that this set of W is not necessarily unique: there might be many similar W that correctly classify the examples. One easy way to see this is that if some parameters W correctly classify all examples (so loss is zero for each example), then any multiple of these parameters $\lambda W$ where $\lambda &gt; 1$ will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30.We can avoid this by extending the loss function with a regularization penalty $R(W)$. The most common regularization penalty is the L2 norm:$$R(W) = \sum_k\sum_l W_{k,l}^2$$That is, the full Multiclass SVM loss becomes:$$L = \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss} \\\\$$Or expanding this out in its full form:$$L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2$$Including the L2 penalty leads to the appealing max margin property in SVMs (See CS229 lecture notes for full details if you are interested).The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself. For example, suppose that we have some input vector $x=[1,1,1,1]$ and two weight vectors $w_1=[1,0,0,0]$, $w_2=[0.25,0.25,0.25,0.25]$. Then $w^T_1x=w^T_2x=1$ so both weight vectors lead to the same dot product, but the L2 penalty of $w_1$ is 1.0 while the L2 penalty of $w_2$ is only 0.25. Therefore, according to the L2 penalty the weight vector $w_2$ would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in $w_2$ are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly. As we will see later in the class, this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting.Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546def L_i(x, y, W): """ unvectorized version. Compute the multiclass svm loss for a single example (x,y) - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick) - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10) - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10) """ delta = 1.0 # see notes about delta later in this section scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class correct_class_score = scores[y] D = W.shape[0] # number of classes, e.g. 10 loss_i = 0.0 for j in xrange(D): # iterate over all wrong classes if j == y: # skip for the true class to only loop over incorrect classes continue # accumulate loss for the i-th example loss_i += max(0, scores[j] - correct_class_score + delta) return loss_idef L_i_vectorized(x, y, W): """ A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function) """ delta = 1.0 scores = W.dot(x) # compute the margins for all classes in one vector operation margins = np.maximum(0, scores - scores[y] + delta) # on y-th position scores[y] - scores[y] canceled and gave delta. We want # to ignore the y-th position and only consider margin on max wrong class margins[y] = 0 loss_i = np.sum(margins) return loss_idef L(X, y, W): """ fully-vectorized implementation : - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10) - y is array of integers specifying correct class (e.g. 50,000-D array) - W are weights (e.g. 10 x 3073) """ # evaluate loss over all examples in X without using any for loops # left as exercise to reader in the assignmentPractice ConsiderationsSetting DeltaNote that we brushed over the hyperparameter $\Delta$ and its setting. What value should it be set to, and do we have to cross-validate it? It turns out that this hyperparameter can safely be set to $\Delta = 1.0$ in all cases. The hyperparameters $\Delta$ and $\lambda$ seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights W has direct effect on the scores (and hence also their differences): As we shrink all values inside W the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. $\Delta = 10$, or$\Delta = 100$) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength $\lambda$).Softmax classifierIn the Softmax classifier, the function mapping $f(x_i; W) = W x_i$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entropy loss that has the form:$$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}$$The function $f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}$ is called the softmax function.Information theory viewThe cross-entropy between a â€œtrueâ€ distribution $p$ and an estimated distribution $q$ is defined as:$$H(p,q) = - \sum_x p(x) \log q(x)$$The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ( $q = e^{f_{y_i}} / \sum_j e^{f_j}$ as seen above) and the â€œtrueâ€ distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. $p = [0, \ldots 1, \ldots, 0]$ contains a single 1 at the $y_i$ -th position.).Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q) = H(p) + D_{KL}(p||q)$, and the entropy of the delta function $p$ is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer.Probabilistic interpretation$$P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }$$can be interpreted as the (normalized) probability assigned to the correct label $y_i$given the image $x_i$ and parameterized by W. To see this, remember that the Softmax classifier interprets the scores inside the output vector $f$ as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE). A nice feature of this view is that we can now also interpret the regularization term $R(W)$ in the full loss function as coming from a Gaussian prior over the weight matrix W, where instead of MLE we are performing the Maximum a posteriori (MAP) estimation.Numeric stabilityWhen youâ€™re writing code for computing the Softmax function in practice, the intermediate term $e^{f_{y_i}}$ and $\sum_j e^{f_j}$ may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant C and push it into the sum, we get the following (mathematically equivalent) expression:$$\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}$$We are free to choose the value of C. This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for C is to set $\log C = -\max_j f_j$. This simply states that we should shift the values inside the vector ff so that the highest value is zero. In code:123456f = np.array([123, 456, 789]) # example with 3 classes and each having large scoresp = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup# instead: first shift the values of f so that the highest number is 0:f -= np.max(f) # f becomes [-666, -333, 0]p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answerSome tricksHow peaky or diffuse these probabilities are depends directly on the regularization strength $\lambda$ - which you are in charge of as input to the system. For example, suppose that the unnormalized log-probabilities for some three classes come out to be [1, -2, 0]. The softmax function would then compute:$$[1, -2, 0] \rightarrow [e^1, e^{-2}, e^0] = [2.71, 0.14, 1] \rightarrow [0.7, 0.04, 0.26]$$Where the steps taken are to exponentiate and normalize to sum to one. Now, if the regularization strength $\lambda$ was higher, the weights W would be penalized more and this would lead to smaller weights. For example, suppose that the weights became one half smaller ([0.5, -1, 0]). The softmax would now compute:$$[0.5, -1, 0] \rightarrow [e^{0.5}, e^{-1}, e^0] = [1.65, 0.37, 1] \rightarrow [0.55, 0.12, 0.33]$$where the probabilites are now more diffuse.Futher ReadingDeep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.OptimizationStrategy #1: A first very bad idea solution: Random search12345678910111213141516171819202122# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)# assume Y_train are the labels (e.g. 1D array of 50,000)# assume the function L evaluates the loss functionbestloss = float("inf") # Python assigns the highest possible float valuefor num in xrange(1000): W = np.random.randn(10, 3073) * 0.0001 # generate random parameters loss = L(X_train, Y_train, W) # get the loss over the entire training set if loss &lt; bestloss: # keep track of the best solution bestloss = loss bestW = W print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)# prints:# in attempt 0 the loss was 9.401632, best 9.401632# in attempt 1 the loss was 8.959668, best 8.959668# in attempt 2 the loss was 9.044034, best 8.959668# in attempt 3 the loss was 9.278948, best 8.959668# in attempt 4 the loss was 8.857370, best 8.857370# in attempt 5 the loss was 8.943151, best 8.857370# in attempt 6 the loss was 8.605604, best 8.605604# ... (trunctated: continues for 1000 lines)We can take the best weights W found by this search and try it out on the test set:1234567# Assume X_test is [3073 x 10000], Y_test [10000 x 1]scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples# find the index with max score in each column (the predicted class)Yte_predict = np.argmax(scores, axis = 0)# and calculate accuracy (fraction of predictions that are correct)np.mean(Yte_predict == Yte)# returns 0.1555With the best W this gives an accuracy of about 15.5%.Core idea: iterative refinement. Of course, it turns out that we can do much better. The core idea is that finding the best set of weights W is a very difficult or even impossible problem (especially once W contains weights for entire complex neural networks), but the problem of refining a specific set of weights W to be slightly better is significantly less difficult. In other words, our approach will be to start with a random W and then iteratively refine it, making it slightly better each time.Our strategy will be to start with random weights and iteratively refine them over time to get lower lossStrategy #2: Random Local SearchConcretely, we will start out with a random WW, generate random perturbations $\delta W$ to it and if the loss at the perturbed $W + \delta W$ is lower, we will perform an update. The code for this procedure is as follows:12345678910W = np.random.randn(10, 3073) * 0.001 # generate random starting Wbestloss = float("inf")for i in xrange(1000): step_size = 0.0001 Wtry = W + np.random.randn(10, 3073) * step_size loss = L(Xtr_cols, Ytr, Wtry) if loss &lt; bestloss: W = Wtry bestloss = loss print 'iter %d loss is %f' % (i, bestloss)This approach achieves test set classification accuracy of 21.4%.Strategy #3: Following the GradientThe mathematical expression for the derivative of a 1-D function with respect its input is:$$\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}$$When the functions of interest take a vector of numbers instead of a single number, we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension.Computing the gradientThere are two ways to compute the gradient: A slow, approximate but easy way (numerical gradient), and a fast, exact but more error-prone way that requires calculus (analytic gradient). We will now present both.Computing the gradient numerically with finite differences123456789101112131415161718192021222324252627def eval_numerical_gradient(f, x): """ a naive implementation of numerical gradient of f at x - f should be a function that takes a single argument - x is the point (numpy array) to evaluate the gradient at """ fx = f(x) # evaluate function value at original point grad = np.zeros(x.shape) h = 0.00001 # iterate over all indexes in x it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: # evaluate function at x+h ix = it.multi_index old_value = x[ix] x[ix] = old_value + h # increment by h fxh = f(x) # evalute f(x + h) x[ix] = old_value # restore to previous value (very important!) # compute the partial derivative grad[ix] = (fxh - fx) / h # the slope it.iternext() # step to next dimension return gradPractical considerations.Note that in the mathematical formulation the gradient is defined in the limit as h goes towards zero, but in practice it is often sufficient to use a very small value (such as 1e-5 as seen in the example). Ideally, you want to use the smallest step size that does not lead to numerical issues. Additionally, in practice it often works better to compute the numeric gradient using the centered difference formula:$$[f(x+h) - f(x-h)] / 2 h$$See wiki for details.1234567# to use the generic code above we want a function that takes a single argument# (the weights in our case) so we close over X_train and Y_traindef CIFAR10_loss_fun(W): return L(X_train, Y_train, W)W = np.random.rand(10, 3073) * 0.001 # random weight vectordf = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradientThen we can use to make an update:12345678910111213141516171819202122loss_original = CIFAR10_loss_fun(W) # the original lossprint 'original loss: %f' % (loss_original, )# lets see the effect of multiple step sizesfor step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]: step_size = 10 ** step_size_log W_new = W - step_size * df # new position in the weight space loss_new = CIFAR10_loss_fun(W_new) print 'for step size %f new loss: %f' % (step_size, loss_new)# prints:# original loss: 2.200718# for step size 1.000000e-10 new loss: 2.200652# for step size 1.000000e-09 new loss: 2.200057# for step size 1.000000e-08 new loss: 2.194116# for step size 1.000000e-07 new loss: 2.135493# for step size 1.000000e-06 new loss: 1.647802# for step size 1.000000e-05 new loss: 2.844355# for step size 1.000000e-04 new loss: 25.558142# for step size 1.000000e-03 new loss: 254.086573# for step size 1.000000e-02 new loss: 2539.370888# for step size 1.000000e-01 new loss: 25392.214036Effect of step sizeComputing the gradient analytically with CalculusHowever, unlike the numerical gradient it can be more error prone to implement, which is why in practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a gradient check.Lets use the example of the SVM loss function for a single datapoint:$$L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]$$$$\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i$$when youâ€™re implementing this in code youâ€™d simply count the number of classes that didnâ€™t meet the desired margin (and hence contributed to the loss function) and then the data vector $x_i$ scaled by this number is the gradient.$$\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i$$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note Ch07]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-Ch07%2F</url>
      <content type="text"><![CDATA[æ•°æ®è§„æ•´åŒ–ï¼šæ¸…ç†ã€è½¬æ¢ã€åˆå¹¶ã€é‡å¡‘1234567891011121314from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandasimport pandas as pdnp.set_printoptions(precision=4, threshold=500)pd.options.display.max_rows = 100from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%matplotlib inlineåˆå¹¶æ•°æ®é›†æ•°æ®åº“é£Žæ ¼çš„DataFrameåˆå¹¶12345df1 = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)&#125;)df2 = DataFrame(&#123;'key': ['a', 'b', 'd'], 'data2': range(3)&#125;)df1data1key00b11b22a33c44a55a66b1df2data2key00a11b22dé»˜è®¤æƒ…å†µä¸‹æ ¹æ®é‡å çš„åˆ—åè¿›è¡Œåˆå¹¶1pd.merge(df1, df2)data1keydata200b111b126b132a044a055a0æœ€å¥½è¿›è¡Œæ˜¾å¼åœ°æŒ‡å®š1pd.merge(df1, df2, on='key')data1keydata200b111b126b132a044a055a01234df3 = DataFrame(&#123;'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)&#125;)df4 = DataFrame(&#123;'rkey': ['a', 'b', 'd'], 'data2': range(3)&#125;)1df3data1lkey00b11b22a33c44a55a66b1df4data2rkey00a11b22då¦‚æžœä¸¤ä¸ªå¯¹è±¡çš„åˆ—åä¸åŒï¼Œé‚£ä¹ˆå°±éœ€è¦åˆ†åˆ«æŒ‡å®š1pd.merge(df3, df4, left_on='lkey', right_on='rkey')data1lkeydata2rkey00b1b11b1b26b1b32a0a44a0a55a0aé»˜è®¤æ˜¯è¿›è¡Œinnerè¿žæŽ¥ï¼ˆäº¤é›†ï¼‰ï¼Œ outeræ˜¯æ±‚å–å¹¶é›†1pd.merge(df1, df2, how='outer')data1keydata200.0b1.011.0b1.026.0b1.032.0a0.044.0a0.055.0a0.063.0cNaN7NaNd2.01234df1 = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)&#125;)df2 = DataFrame(&#123;'key': ['a', 'b', 'a', 'b', 'd'], 'data2': range(5)&#125;)1df1data1key00b11b22a33c44a55b1df2data2key00a11b22a33b44d1pd.merge(df1, df2, on='key', how='left')data1keydata200b1.010b3.021b1.031b3.042a0.052a2.063cNaN74a0.084a2.095b1.0105b3.01pd.merge(df1, df2, how='inner')data1keydata200b110b321b131b345b155b362a072a284a094a2123456left = DataFrame(&#123;'key1': ['foo', 'foo', 'bar'], 'key2': ['one', 'two', 'one'], 'lval': [1, 2, 3]&#125;)right = DataFrame(&#123;'key1': ['foo', 'foo', 'bar', 'bar'], 'key2': ['one', 'one', 'one', 'two'], 'rval': [4, 5, 6, 7]&#125;)121leftkey1key2lval0fooone11footwo22barone31rightkey1key2rval0fooone41fooone52barone63bartwo71pd.merge(left, right, on=['key1', 'key2'], how='outer')key1key2lvalrval0fooone1.04.01fooone1.05.02footwo2.0NaN3barone3.06.04bartwoNaN7.0åˆ—åé‡å¤é—®é¢˜1pd.merge(left, right, on='key1')key1key2_xlvalkey2_yrval0fooone1one41fooone1one52footwo2one43footwo2one54barone3one65barone3two71pd.merge(left, right, on='key1', suffixes=('_left', '_right'))key1key2_leftlvalkey2_rightrval0fooone1one41fooone1one52footwo2one43footwo2one54barone3one65barone3two7ç´¢å¼•ä¸Šçš„åˆå¹¶123left1 = DataFrame(&#123;'key': ['a', 'b', 'a', 'a', 'b', 'c'], 'value': range(6)&#125;)right1 = DataFrame(&#123;'group_val': [3.5, 7]&#125;, index=['a', 'b'])1left1keyvalue0a01b12a23a34b45c51right1group_vala3.5b7.01pd.merge(left1, right1, left_on='key', right_index=True)keyvaluegroup_val0a03.52a23.53a33.51b17.04b47.01pd.merge(left1, right1, left_on='key', right_index=True, how='outer')keyvaluegroup_val0a03.52a23.53a33.51b17.04b47.05c5NaN12345678lefth = DataFrame(&#123;'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'key2': [2000, 2001, 2002, 2001, 2002], 'data': np.arange(5.)&#125;)righth = DataFrame(np.arange(12).reshape((6, 2)), index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'], [2001, 2000, 2000, 2000, 2001, 2002]], columns=['event1', 'event2'])lefthdatakey1key200.0Ohio200011.0Ohio200122.0Ohio200233.0Nevada200144.0Nevada20021righthevent1event2Nevada200101200023Ohio200045200067200189200210111pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)datakey1key2event1event200.0Ohio20004500.0Ohio20006711.0Ohio20018922.0Ohio2002101133.0Nevada20010112pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True, how='outer')datakey1key2event1event200.0Ohio2000.04.05.000.0Ohio2000.06.07.011.0Ohio2001.08.09.022.0Ohio2002.010.011.033.0Nevada2001.00.01.044.0Nevada2002.0NaNNaN4NaNNevada2000.02.03.01234left2 = DataFrame([[1., 2.], [3., 4.], [5., 6.]], index=['a', 'c', 'e'], columns=['Ohio', 'Nevada'])right2 = DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]], index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])1left2OhioNevadaa1.02.0c3.04.0e5.06.01right2MissouriAlabamab7.08.0c9.010.0d11.012.0e13.014.01pd.merge(left2, right2, how='outer', left_index=True, right_index=True)OhioNevadaMissouriAlabamaa1.02.0NaNNaNbNaNNaN7.08.0c3.04.09.010.0dNaNNaN11.012.0e5.06.013.014.01left2.join(right2, how='outer')OhioNevadaMissouriAlabamaa1.02.0NaNNaNbNaNNaN7.08.0c3.04.09.010.0dNaNNaN11.012.0e5.06.013.014.01left1.join(right1, on='key')keyvaluegroup_val0a03.51b17.02a23.53a33.54b47.05c5NaN12another = DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]], index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])New YorkOregona7.08.0c9.010.0e11.012.0f16.017.0ç›¸å½“äºŽä¸‰ä¸ªè¡¨è¿›è¡Œåˆå¹¶1234left2right2anotherleft2.join([right2, another])OhioNevadaa1.02.0c3.04.0e5.06.0MissouriAlabamab7.08.0c9.010.0d11.012.0e13.014.0New YorkOregona7.08.0c9.010.0e11.012.0f16.017.0OhioNevadaMissouriAlabamaNew YorkOregona1.02.0NaNNaN7.08.0c3.04.09.010.09.010.0e5.06.013.014.011.012.01left2.join([right2, another], how='outer')è½´å‘è¿žæŽ¥ä¹‹å‰æŒ‡çš„éƒ½æ˜¯è¡Œçº§åˆ«çš„è¿žæŽ¥æ“ä½œ1arr = np.arange(12).reshape((3, 4))1arrarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) 1np.concatenate([arr, arr], axis=1)array([[ 0, 1, 2, 3, 0, 1, 2, 3], [ 4, 5, 6, 7, 4, 5, 6, 7], [ 8, 9, 10, 11, 8, 9, 10, 11]]) 123s1 = Series([0, 1], index=['a', 'b'])s2 = Series([2, 3, 4], index=['c', 'd', 'e'])s3 = Series([5, 6], index=['f', 'g'])1pd.concat([s1, s2, s3])a 0 b 1 c 2 d 3 e 4 f 5 g 6 dtype: int64 1pd.concat([s1, s2, s3], axis=1)012a0.0NaNNaNb1.0NaNNaNcNaN2.0NaNdNaN3.0NaNeNaN4.0NaNfNaNNaN5.0gNaNNaN6.012s4 = pd.concat([s1 * 5, s3])s4a 0 b 5 f 5 g 6 dtype: int64 1pd.concat([s1, s4], axis=1)01a0.00b1.05fNaN5gNaN61pd.concat([s1, s4], axis=1, join='inner')01a00b151pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])01a0.00.0cNaNNaNb1.05.0eNaNNaNåœ¨è¿žæŽ¥è½´ä¸Šå»ºç«‹ä¸€ä¸ªå±‚æ¬¡åŒ–ç´¢å¼•123s1s3result = pd.concat([s1, s1, s3], keys=['one', 'two', 'three'])a 0 b 1 dtype: int64 f 5 g 6 dtype: int64 1resultone a 0 b 1 two a 0 b 1 three f 5 g 6 dtype: int64 12# Much more on the unstack function laterresult.unstack()abfgone0.01.0NaNNaNtwo0.01.0NaNNaNthreeNaNNaN5.06.01pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three'])onetwothreea0.0NaNNaNb1.0NaNNaNcNaN2.0NaNdNaN3.0NaNeNaN4.0NaNfNaNNaN5.0gNaNNaN6.012345678df1 = DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'], columns=['one', 'two'])df2 = DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'], columns=['three', 'four'])df1df2pd.concat([df1, df2], keys=['level1', 'level2'])pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])onetwoa01b23c45threefoura56c78fouronethreetwolevel1aNaN0.0NaN1.0bNaN2.0NaN3.0cNaN4.0NaN5.0level2a6.0NaN5.0NaNc8.0NaN7.0NaNlevel1level2onetwothreefoura015.06.0b23NaNNaNc457.08.012pd.concat(&#123;'level1': df1, 'level2': df2&#125;, axis=0)pd.concat(&#123;'level1': df1, 'level2': df2&#125;, axis=1)fouronethreetwolevel1aNaN0.0NaN1.0bNaN2.0NaN3.0cNaN4.0NaN5.0level2a6.0NaN5.0NaNc8.0NaN7.0NaNlevel1level2onetwothreefoura015.06.0b23NaNNaNc457.08.012pd.concat([df1, df2], axis=1, keys=['level1', 'level2'], names=['upper', 'lower'])upperlevel1level2loweronetwothreefoura015.06.0b23NaNNaNc457.08.012df1 = DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])df2 = DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])1df1abcd0-0.2047080.478943-0.519439-0.55573011.9657811.3934060.0929080.28174620.7690231.2464351.007189-1.2962211df2bda00.2749920.2289131.35291710.886429-2.001637-0.371843åŽ»é™¤æ— å…³çš„è¡Œç´¢å¼•12pd.concat([df1, df2], ignore_index=False)pd.concat([df1, df2], ignore_index=True)abcd0-0.2047080.478943-0.519439-0.55573011.9657811.3934060.0929080.28174620.7690231.2464351.007189-1.29622101.3529170.274992NaN0.2289131-0.3718430.886429NaN-2.001637abcd0-0.2047080.478943-0.519439-0.55573011.9657811.3934060.0929080.28174620.7690231.2464351.007189-1.29622131.3529170.274992NaN0.2289134-0.3718430.886429NaN-2.001637åˆå¹¶é‡å æ•°æ®12345a = Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan], index=['f', 'e', 'd', 'c', 'b', 'a'])b = Series(np.arange(len(a), dtype=np.float64), index=['f', 'e', 'd', 'c', 'b', 'a'])b[-1] = np.nan1af NaN e 2.5 d NaN c 3.5 b 4.5 a NaN dtype: float64 1bf 0.0 e 1.0 d 2.0 c 3.0 b 4.0 a NaN dtype: float64 1np.where(pd.isnull(a), b, a)array([ 0. , 2.5, 2. , 3.5, 4.5, nan]) combine_first, é‡å å€¼åˆå¹¶ï¼Œä¸”è¿›è¡Œæ•°æ®å¯¹å…¶1b[:-2].combine_first(a[2:])a NaN b 4.5 c 3.0 d 2.0 e 1.0 f 0.0 dtype: float64 12345678df1 = DataFrame(&#123;'a': [1., np.nan, 5., np.nan], 'b': [np.nan, 2., np.nan, 6.], 'c': range(2, 18, 4)&#125;)df2 = DataFrame(&#123;'a': [5., 4., np.nan, 3., 7.], 'b': [np.nan, 3., 4., 6., 8.]&#125;)df1df2df1.combine_first(df2)abc01.0NaN21NaN2.0625.0NaN103NaN6.014ab05.0NaN14.03.02NaN4.033.06.047.08.0abc01.0NaN2.014.02.06.025.04.010.033.06.014.047.08.0NaNé‡å¡‘å’Œè½´å‘æ—‹è½¬é‡å¡‘å±‚æ¬¡åŒ–ç´¢å¼•1234data = DataFrame(np.arange(6).reshape((2, 3)), index=pd.Index(['Ohio', 'Colorado'], name='state'), columns=pd.Index(['one', 'two', 'three'], name='number'))datanumberonetwothreestateOhio012Colorado345stackå°†åˆ—æ—‹è½¬ä¸ºè¡Œ12result = data.stack()resultstate number Ohio one 0 two 1 three 2 Colorado one 3 two 4 three 5 dtype: int32 unstackå°†è¡Œæ—‹è½¬ä¸ºåˆ—ï¼Œé»˜è®¤æ“ä½œæœ€å†…å±‚1result.unstack()numberonetwothreestateOhio012Colorado3451result.unstack(0)stateOhioColoradonumberone03two14three251result.unstack('state')stateOhioColoradonumberone03two14three251234567s1 = Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])s2 = Series([4, 5, 6], index=['c', 'd', 'e'])s1s2data2 = pd.concat([s1, s2], keys=['one', 'two'])data2data2.unstack()a 0 b 1 c 2 d 3 dtype: int64 c 4 d 5 e 6 dtype: int64 one a 0 b 1 c 2 d 3 two c 4 d 5 e 6 dtype: int64 abcdeone0.01.02.03.0NaNtwoNaNNaN4.05.06.0stacké»˜è®¤ä¼šæ»¤é™¤ç¼ºå¤±å€¼ï¼Œå› æ­¤ä¸¤è€…å¯é€†1data2.unstack().stack()one a 0.0 b 1.0 c 2.0 d 3.0 two c 4.0 d 5.0 e 6.0 dtype: float64 1data2.unstack().stack(dropna=False)one a 0.0 b 1.0 c 2.0 d 3.0 e NaN two a NaN b NaN c 4.0 d 5.0 e 6.0 dtype: float64 1234resultdf = DataFrame(&#123;'left': result, 'right': result + 5&#125;, columns=pd.Index(['left', 'right'], name='side'))dfstate number Ohio one 0 two 1 three 2 Colorado one 3 two 4 three 5 dtype: int32 sideleftrightstatenumberOhioone05two16three27Coloradoone38two49three510DataFrameä½œä¸ºæ—‹è½¬è½´çš„çº§åˆ«å°†æˆä¸ºç»“æžœä¸­çš„æœ€ä½Žçº§åˆ«ï¼ˆaxis=MAXï¼‰1df.unstack('state')sideleftrightstateOhioColoradoOhioColoradonumberone0358two1469three25710stackæ“ä½œå°†axis-1?1df.unstack('state').stack('side')stateOhioColoradonumbersideoneleft03right58twoleft14right69threeleft25right71012df.unstack('state')df.unstack('state').stack('state')sideleftrightstateOhioColoradoOhioColoradonumberone0358two1469three25710sideleftrightnumberstateoneOhio05Colorado38twoOhio16Colorado49threeOhio27Colorado510å°†é•¿æ ¼å¼æ—‹è½¬ä¸ºå®½æ ¼å¼12345678910111213data = pd.read_csv('ch07/macrodata.csv')data[:10]periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name='date')periods[:10]data = DataFrame(data.to_records(), columns=pd.Index(['realgdp', 'infl', 'unemp'], name='item'), index=periods.to_timestamp('D', 'end'))data[:10]data.to_records()[:10]data.stack()[:10]ldata = data.stack().reset_index().rename(columns=&#123;0: 'value'&#125;)wdata = ldata.pivot('date', 'item', 'value')yearquarterrealgdprealconsrealinvrealgovtrealdpicpim1tbilrateunemppopinflrealint01959.01.02710.3491707.4286.898470.0451886.928.98139.72.825.8177.1460.000.0011959.02.02778.8011733.7310.859481.3011919.729.15141.73.085.1177.8302.340.7421959.03.02775.4881751.8289.226491.2601916.429.35140.53.825.3178.6572.741.0931959.04.02785.2041753.7299.356484.0521931.329.37140.04.335.6179.3860.274.0641960.01.02847.6991770.5331.722462.1991955.529.54139.63.505.2180.0072.311.1951960.02.02834.3901792.9298.152460.4001966.129.55140.22.685.2180.6710.142.5561960.03.02839.0221785.8296.375474.6761967.829.75140.92.365.6181.5282.70-0.3471960.04.02802.6161788.2259.764476.4341966.629.84141.12.296.3182.2871.211.0881961.01.02819.2641787.7266.405475.8541984.529.81142.12.376.8182.992-0.402.7791961.02.02872.0051814.3286.246480.3282014.429.92142.92.297.0183.6911.470.81PeriodIndex([&apos;1959Q1&apos;, &apos;1959Q2&apos;, &apos;1959Q3&apos;, &apos;1959Q4&apos;, &apos;1960Q1&apos;, &apos;1960Q2&apos;, &apos;1960Q3&apos;, &apos;1960Q4&apos;, &apos;1961Q1&apos;, &apos;1961Q2&apos;], dtype=&apos;int64&apos;, name=&apos;date&apos;, freq=&apos;Q-DEC&apos;) itemrealgdpinflunempdate1959-03-312710.3490.005.81959-06-302778.8012.345.11959-09-302775.4882.745.31959-12-312785.2040.275.61960-03-312847.6992.315.21960-06-302834.3900.145.21960-09-302839.0222.705.61960-12-312802.6161.216.31961-03-312819.264-0.406.81961-06-302872.0051.477.0rec.array([(datetime.datetime(1959, 3, 31, 0, 0), 2710.349, 0.0, 5.8), (datetime.datetime(1959, 6, 30, 0, 0), 2778.801, 2.34, 5.1), (datetime.datetime(1959, 9, 30, 0, 0), 2775.488, 2.74, 5.3), (datetime.datetime(1959, 12, 31, 0, 0), 2785.204, 0.27, 5.6), (datetime.datetime(1960, 3, 31, 0, 0), 2847.699, 2.31, 5.2), (datetime.datetime(1960, 6, 30, 0, 0), 2834.39, 0.14, 5.2), (datetime.datetime(1960, 9, 30, 0, 0), 2839.022, 2.7, 5.6), (datetime.datetime(1960, 12, 31, 0, 0), 2802.616, 1.21, 6.3), (datetime.datetime(1961, 3, 31, 0, 0), 2819.264, -0.4, 6.8), (datetime.datetime(1961, 6, 30, 0, 0), 2872.005, 1.47, 7.0)], dtype=[(&apos;date&apos;, &apos;O&apos;), (&apos;realgdp&apos;, &apos;&lt;f8&apos;), (&apos;infl&apos;, &apos;&lt;f8&apos;), (&apos;unemp&apos;, &apos;&lt;f8&apos;)]) date item 1959-03-31 realgdp 2710.349 infl 0.000 unemp 5.800 1959-06-30 realgdp 2778.801 infl 2.340 unemp 5.100 1959-09-30 realgdp 2775.488 infl 2.740 unemp 5.300 1959-12-31 realgdp 2785.204 dtype: float64 1ldata[:10]dateitemvalue01959-03-31realgdp2710.34911959-03-31infl0.00021959-03-31unemp5.80031959-06-30realgdp2778.80141959-06-30infl2.34051959-06-30unemp5.10061959-09-30realgdp2775.48871959-09-30infl2.74081959-09-30unemp5.30091959-12-31realgdp2785.20412pivoted = ldata.pivot('date', 'item', 'value')pivoted.head()iteminflrealgdpunempdate1959-03-310.002710.3495.81959-06-302.342778.8015.11959-09-302.742775.4885.31959-12-310.272785.2045.61960-03-312.312847.6995.212ldata['value2'] = np.random.randn(len(ldata))ldata[:10]dateitemvaluevalue201959-03-31realgdp2710.349-0.20470811959-03-31infl0.0000.47894321959-03-31unemp5.800-0.51943931959-06-30realgdp2778.801-0.55573041959-06-30infl2.3401.96578151959-06-30unemp5.1001.39340661959-09-30realgdp2775.4880.09290871959-09-30infl2.7400.28174681959-09-30unemp5.3000.76902391959-12-31realgdp2785.2041.24643512pivoted = ldata.pivot('date', 'item')pivoted[:5]valuevalue2iteminflrealgdpunempinflrealgdpunempdate1959-03-310.002710.3495.80.478943-0.204708-0.5194391959-06-302.342778.8015.11.965781-0.5557301.3934061959-09-302.742775.4885.30.2817460.0929080.7690231959-12-310.272785.2045.61.0071891.246435-1.2962211960-03-312.312847.6995.20.2289130.2749921.3529171pivoted['value'][:5]iteminflrealgdpunempdate1959-03-310.002710.3495.81959-06-302.342778.8015.11959-09-302.742775.4885.31959-12-310.272785.2045.61960-03-312.312847.6995.2123ldata.set_index(['date', 'item'])[:9]unstacked = ldata.set_index(['date', 'item']).unstack('item')unstacked[:7]valuevalue2dateitem1959-03-31realgdp2710.349-0.204708infl0.0000.478943unemp5.800-0.5194391959-06-30realgdp2778.801-0.555730infl2.3401.965781unemp5.1001.3934061959-09-30realgdp2775.4880.092908infl2.7400.281746unemp5.3000.769023valuevalue2iteminflrealgdpunempinflrealgdpunempdate1959-03-310.002710.3495.80.478943-0.204708-0.5194391959-06-302.342778.8015.11.965781-0.5557301.3934061959-09-302.742775.4885.30.2817460.0929080.7690231959-12-310.272785.2045.61.0071891.246435-1.2962211960-03-312.312847.6995.20.2289130.2749921.3529171960-06-300.142834.3905.2-2.0016370.886429-0.3718431960-09-302.702839.0225.6-0.4385701.669025-0.539741æ•°æ®è½¬æ¢ç§»é™¤é‡å¤å€¼123data = DataFrame(&#123;'k1': ['one'] * 3 + ['two'] * 4, 'k2': [1, 1, 2, 3, 3, 4, 4]&#125;)datak1k20one11one12one23two34two35two46two41data.duplicated()0 False 1 True 2 False 3 False 4 True 5 False 6 True dtype: bool 1data.drop_duplicates()k1k20one12one23two35two4123data['v1'] = range(7)datadata.drop_duplicates(['k1'])k1k2v10one101one112one223two334two345two456two46k1k2v10one103two3312data.drop_duplicates(['k1', 'k2'], keep='last')data.drop_duplicates(['k1', 'k2'], keep='first')k1k2v11one112one224two346two46k1k2v10one102one223two335two45åˆ©ç”¨å‡½æ•°æˆ–æ˜ å°„è¿›è¡Œæ•°æ®è½¬æ¢12345data = DataFrame(&#123;'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami', 'corned beef', 'Bacon', 'pastrami', 'honey ham', 'nova lox'], 'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]&#125;)datafoodounces0bacon4.01pulled pork3.02bacon12.03Pastrami6.04corned beef7.55Bacon8.06pastrami3.07honey ham5.08nova lox6.012345678meat_to_animal = &#123; 'bacon': 'pig', 'pulled pork': 'pig', 'pastrami': 'cow', 'corned beef': 'cow', 'honey ham': 'pig', 'nova lox': 'salmon'&#125;12data['animal'] = data['food'].map(str.lower).map(meat_to_animal)datafoodouncesanimal0bacon4.0pig1pulled pork3.0pig2bacon12.0pig3Pastrami6.0cow4corned beef7.5cow5Bacon8.0pig6pastrami3.0cow7honey ham5.0pig8nova lox6.0salmon1data['food'].map(lambda x: meat_to_animal[x.lower()])0 pig 1 pig 2 pig 3 cow 4 cow 5 pig 6 cow 7 pig 8 salmon Name: food, dtype: object æ›¿æ¢å€¼12data = Series([1., -999., 2., -999., -1000., 3.])data0 1.0 1 -999.0 2 2.0 3 -999.0 4 -1000.0 5 3.0 dtype: float64 1data.replace(-999, np.nan)0 1.0 1 NaN 2 2.0 3 NaN 4 -1000.0 5 3.0 dtype: float64 1data.replace([-999, -1000], np.nan)0 1.0 1 NaN 2 2.0 3 NaN 4 NaN 5 3.0 dtype: float64 1data.replace([-999, -1000], [np.nan, 0])0 1.0 1 NaN 2 2.0 3 NaN 4 0.0 5 3.0 dtype: float64 1data.replace(&#123;-999: np.nan, -1000: 0&#125;)0 1.0 1 NaN 2 2.0 3 NaN 4 0.0 5 3.0 dtype: float64 é‡å‘½åè½´ç´¢å¼•1234data = DataFrame(np.arange(12).reshape((3, 4)), index=['Ohio', 'Colorado', 'New York'], columns=['one', 'two', 'three', 'four'])dataonetwothreefourOhio0123Colorado4567New York8910111data.index.map(str.upper)array([&apos;OHIO&apos;, &apos;COLORADO&apos;, &apos;NEW YORK&apos;], dtype=object) 12data.index = data.index.map(str.upper)dataonetwothreefourOHIO0123COLORADO4567NEW YORK8910111data.rename(index=str.title, columns=str.upper)ONETWOTHREEFOUROhio0123Colorado4567New York89101112data.rename(index=&#123;'OHIO': 'INDIANA'&#125;, columns=&#123;'three': 'peekaboo'&#125;)onetwopeekaboofourINDIANA0123COLORADO4567NEW YORK891011123# Always returns a reference to a DataFrame_ = data.rename(index=&#123;'OHIO': 'INDIANA'&#125;, inplace=True)dataonetwothreefourINDIANA0123COLORADO4567NEW YORK891011ç¦»æ•£åŒ–å’Œé¢å…ƒåˆ’åˆ†1ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]123bins = [18, 25, 35, 60, 100]cats = pd.cut(ages, bins)cats[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]] Length: 12 Categories (4, object): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]] 1cats.codesarray([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8) 1cats.categoriesIndex([&apos;(18, 25]&apos;, &apos;(25, 35]&apos;, &apos;(35, 60]&apos;, &apos;(60, 100]&apos;], dtype=&apos;object&apos;) 1pd.value_counts(cats)(18, 25] 5 (35, 60] 3 (25, 35] 3 (60, 100] 1 dtype: int64 1pd.cut(ages, [18, 26, 36, 61, 100], right=False)[[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), ..., [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)] Length: 12 Categories (4, object): [[18, 26) &lt; [26, 36) &lt; [36, 61) &lt; [61, 100)] 12group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']pd.cut(ages, bins, labels=group_names)[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult] Length: 12 Categories (4, object): [Youth &lt; YoungAdult &lt; MiddleAged &lt; Senior] 12data = np.random.rand(20)pd.cut(data, 4, precision=2)[(0.25, 0.49], (0.25, 0.49], (0.73, 0.98], (0.25, 0.49], (0.25, 0.49], ..., (0.25, 0.49], (0.73, 0.98], (0.49, 0.73], (0.49, 0.73], (0.49, 0.73]] Length: 20 Categories (4, object): [(0.0032, 0.25] &lt; (0.25, 0.49] &lt; (0.49, 0.73] &lt; (0.73, 0.98]] 123data = np.random.randn(1000) # Normally distributedcats = pd.qcut(data, 4) # Cut into quartilescats[(0.636, 3.26], [-3.745, -0.648], (0.636, 3.26], (-0.022, 0.636], (-0.648, -0.022], ..., (0.636, 3.26], (-0.022, 0.636], [-3.745, -0.648], (-0.022, 0.636], (-0.022, 0.636]] Length: 1000 Categories (4, object): [[-3.745, -0.648] &lt; (-0.648, -0.022] &lt; (-0.022, 0.636] &lt; (0.636, 3.26]] 1pd.value_counts(cats)(0.636, 3.26] 250 (-0.022, 0.636] 250 (-0.648, -0.022] 250 [-3.745, -0.648] 250 dtype: int64 1pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])[(-0.022, 1.298], [-3.745, -1.274], (-0.022, 1.298], (-0.022, 1.298], (-1.274, -0.022], ..., (-0.022, 1.298], (-0.022, 1.298], [-3.745, -1.274], (-0.022, 1.298], (-0.022, 1.298]] Length: 1000 Categories (4, object): [[-3.745, -1.274] &lt; (-1.274, -0.022] &lt; (-0.022, 1.298] &lt; (1.298, 3.26]] æ£€æµ‹å’Œè¿‡æ»¤å¼‚å¸¸å€¼123np.random.seed(12345)data = DataFrame(np.random.randn(1000, 4))data.describe()0123count1000.0000001000.0000001000.0000001000.000000mean-0.0676840.0679240.025598-0.002298std0.9980350.9921061.0068350.996794min-3.428254-3.548824-3.184377-3.74535625%-0.774890-0.591841-0.641675-0.64414450%-0.1164010.1011430.002073-0.01361175%0.6163660.7802820.6803910.654328max3.3666262.6536563.2603833.92752812col = data[3]col[np.abs(col) &gt; 3]97 3.927528 305 -3.399312 400 -3.745356 Name: 3, dtype: float64 1data[(np.abs(data) &gt; 3).any(1)]01235-0.5397410.4769853.248944-1.02122897-0.7743630.5529360.1060613.927528102-0.655054-0.5652303.1768730.959533305-2.3155550.457246-0.025907-3.3993123240.0501881.9513123.2603830.9633014000.1463260.508391-0.196713-3.745356499-0.293333-0.242459-3.0569901.918403523-3.428254-0.296336-0.439938-0.8671655860.2751441.179227-3.1843771.369891808-0.362528-3.5488241.553205-2.1863019003.366626-2.3722140.8510101.33284612data[np.abs(data) &gt; 3] = np.sign(data) * 3data.describe()0123count1000.0000001000.0000001000.0000001000.000000mean-0.0676230.0684730.025153-0.002081std0.9954850.9902531.0039770.989736min-3.000000-3.000000-3.000000-3.00000025%-0.774890-0.591841-0.641675-0.64414450%-0.1164010.1011430.002073-0.01361175%0.6163660.7802820.6803910.654328max3.0000002.6536563.0000003.000000æŽ’åˆ—å’Œéšæœºé‡‡æ ·123df = DataFrame(np.arange(5 * 4).reshape((5, 4)))sampler = np.random.permutation(5)samplerarray([1, 0, 2, 3, 4]) 1df0123001231456728910113121314154161718191df.take(sampler)0123145670012328910113121314154161718191df.take(np.random.permutation(len(df))[:3])0123145670012341617181912bag = np.array([5, 7, -1, 6, 4])sampler = np.random.randint(0, len(bag), size=10)1samplerarray([3, 0, 4, 1, 1, 2, 3, 0, 1, 2]) 12draws = bag.take(sampler)drawsarray([ 6, 5, 4, 7, 7, -1, 6, 5, 7, -1]) è®¡ç®—æŒ‡æ ‡ / å“‘å˜é‡1234df = DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)&#125;)dfpd.get_dummies(df['key'])data1key00b11b22a33c44a55babc00.01.00.010.01.00.021.00.00.030.00.01.041.00.00.050.01.00.01234dummies = pd.get_dummies(df['key'], prefix='key')dummiesdf_with_dummy = df[['data1']].join(dummies)df_with_dummykey_akey_bkey_c00.01.00.010.01.00.021.00.00.030.00.01.041.00.00.050.01.00.0data1key_akey_bkey_c000.01.00.0110.01.00.0221.00.00.0330.00.01.0441.00.00.0550.01.00.01234mnames = ['movie_id', 'title', 'genres']movies = pd.read_table('ch02/movielens/movies.dat', sep='::', header=None, names=mnames)movies[:10]C:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:3: ParserWarning: Falling back to the &apos;python&apos; engine because the &apos;c&apos; engine does not support regex separators (separators &gt; 1 char and different from &apos;\s+&apos; are interpreted as regex); you can avoid this warning by specifying engine=&apos;python&apos;. app.launch_new_instance() movie_idtitlegenres01Toy Story (1995)Animation|Childrenâ€™s|Comedy12Jumanji (1995)Adventure|Childrenâ€™s|Fantasy23Grumpier Old Men (1995)Comedy|Romance34Waiting to Exhale (1995)Comedy|Drama45Father of the Bride Part II (1995)Comedy56Heat (1995)Action|Crime|Thriller67Sabrina (1995)Comedy|Romance78Tom and Huck (1995)Adventure|Childrenâ€™s89Sudden Death (1995)Action910GoldenEye (1995)Action|Adventure|Thriller123genre_iter = (set(x.split('|')) for x in movies.genres)genres = sorted(set.union(*genre_iter))genres[&apos;Action&apos;, &apos;Adventure&apos;, &apos;Animation&apos;, &quot;Children&apos;s&quot;, &apos;Comedy&apos;, &apos;Crime&apos;, &apos;Documentary&apos;, &apos;Drama&apos;, &apos;Fantasy&apos;, &apos;Film-Noir&apos;, &apos;Horror&apos;, &apos;Musical&apos;, &apos;Mystery&apos;, &apos;Romance&apos;, &apos;Sci-Fi&apos;, &apos;Thriller&apos;, &apos;War&apos;, &apos;Western&apos;] 12dummies = DataFrame(np.zeros((len(movies), len(genres))), columns=genres)dummies[:10].ix[:, :5]ActionAdventureAnimationChildrenâ€™sComedy00.00.00.00.00.010.00.00.00.00.020.00.00.00.00.030.00.00.00.00.040.00.00.00.00.050.00.00.00.00.060.00.00.00.00.070.00.00.00.00.080.00.00.00.00.090.00.00.00.00.0123for i, gen in enumerate(movies.genres): dummies.ix[i, gen.split('|')] = 1dummies[:10].ix[:, :5]ActionAdventureAnimationChildrenâ€™sComedy00.00.01.01.01.010.01.00.01.00.020.00.00.00.01.030.00.00.00.01.040.00.00.00.01.051.00.00.00.00.060.00.00.00.01.070.01.00.01.00.081.00.00.00.00.091.01.00.00.00.012movies_windic = movies.join(dummies.add_prefix('Genre_'))movies_windic.ix[0]movie_id 1 title Toy Story (1995) genres Animation|Children&apos;s|Comedy Genre_Action 0 Genre_Adventure 0 Genre_Animation 1 Genre_Children&apos;s 1 Genre_Comedy 1 Genre_Crime 0 Genre_Documentary 0 Genre_Drama 0 Genre_Fantasy 0 Genre_Film-Noir 0 Genre_Horror 0 Genre_Musical 0 Genre_Mystery 0 Genre_Romance 0 Genre_Sci-Fi 0 Genre_Thriller 0 Genre_War 0 Genre_Western 0 Name: 0, dtype: object 1np.random.seed(12345)12values = np.random.rand(10)valuesarray([ 0.9296, 0.3164, 0.1839, 0.2046, 0.5677, 0.5955, 0.9645, 0.6532, 0.7489, 0.6536]) 12bins = [0, 0.2, 0.4, 0.6, 0.8, 1]pd.get_dummies(pd.cut(values, bins))(0, 0.2](0.2, 0.4](0.4, 0.6](0.6, 0.8](0.8, 1]00.00.00.00.01.010.01.00.00.00.021.00.00.00.00.030.01.00.00.00.040.00.01.00.00.050.00.01.00.00.060.00.00.00.01.070.00.00.01.00.080.00.00.01.00.090.00.00.01.00.0å­—ç¬¦ä¸²æ“ä½œå­—ç¬¦ä¸²å¯¹è±¡æ–¹æ³•12val = 'a,b, guido'val.split(',')[&apos;a&apos;, &apos;b&apos;, &apos; guido&apos;] 12pieces = [x.strip() for x in val.split(',')]pieces[&apos;a&apos;, &apos;b&apos;, &apos;guido&apos;] 12first, second, third = piecesfirst + '::' + second + '::' + third&apos;a::b::guido&apos; Surprise :P1'::'.join(pieces)&apos;a::b::guido&apos; 1'guido' in valTrue 1val.index(',')1 1val.find(':')-1 1val.index(':')--------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-110-280f8b2856ce&gt; in &lt;module&gt;() ----&gt; 1 val.index(&apos;:&apos;) ValueError: substring not found 1val.count(',')2 1val.replace(',', '::')&apos;a::b:: guido&apos; 1val.replace(',', '')&apos;ab guido&apos; æ­£åˆ™è¡¨è¾¾å¼123import retext = "foo bar\t baz \tqux"re.split('\s+', text)[&apos;foo&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;qux&apos;] 12regex = re.compile('\s+')regex.split(text)[&apos;foo&apos;, &apos;bar&apos;, &apos;baz&apos;, &apos;qux&apos;] 1regex.findall(text)[&apos; &apos;, &apos;\t &apos;, &apos; \t&apos;] 123456789text = """Dave dave@google.comSteve steve@gmail.comRob rob@gmail.comRyan ryan@yahoo.com"""pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]&#123;2,4&#125;'# re.IGNORECASE makes the regex case-insensitiveregex = re.compile(pattern, flags=re.IGNORECASE)1regex.findall(text)[&apos;dave@google.com&apos;, &apos;steve@gmail.com&apos;, &apos;rob@gmail.com&apos;, &apos;ryan@yahoo.com&apos;] Searchåªè¿”å›žç¬¬ä¸€é¡¹12m = regex.search(text)m&lt;_sre.SRE_Match object; span=(5, 20), match=&apos;dave@google.com&apos;&gt; 1text[m.start():m.end()]&apos;dave@google.com&apos; åªåŒ¹é…å‡ºçŽ°åœ¨å­—ç¬¦ä¸²å¼€å¤´çš„æ¨¡å¼1print(regex.match(text))None â€‹æ›¿æ¢1print(regex.sub('REDACTED', text))Dave REDACTED Steve REDACTED Rob REDACTED Ryan REDACTED â€‹12pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]&#123;2,4&#125;)'regex = re.compile(pattern, flags=re.IGNORECASE)12m = regex.match('wesm@bright.net')m.groups()(&apos;wesm&apos;, &apos;bright&apos;, &apos;net&apos;) 1regex.findall(text)[(&apos;dave&apos;, &apos;google&apos;, &apos;com&apos;), (&apos;steve&apos;, &apos;gmail&apos;, &apos;com&apos;), (&apos;rob&apos;, &apos;gmail&apos;, &apos;com&apos;), (&apos;ryan&apos;, &apos;yahoo&apos;, &apos;com&apos;)] 1print(regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text))Dave Username: dave, Domain: google, Suffix: com Steve Username: steve, Domain: gmail, Suffix: com Rob Username: rob, Domain: gmail, Suffix: com Ryan Username: ryan, Domain: yahoo, Suffix: com â€‹123456regex = re.compile(r""" (?P&lt;username&gt;[A-Z0-9._%+-]+) @ (?P&lt;domain&gt;[A-Z0-9.-]+) \. (?P&lt;suffix&gt;[A-Z]&#123;2,4&#125;)""", flags=re.IGNORECASE|re.VERBOSE)12m = regex.match('wesm@bright.net')m.groupdict(){&apos;domain&apos;: &apos;bright&apos;, &apos;suffix&apos;: &apos;net&apos;, &apos;username&apos;: &apos;wesm&apos;} pandasä¸­çŸ¢é‡åŒ–çš„å­—ç¬¦ä¸²å‡½æ•°123data = &#123;'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com', 'Rob': 'rob@gmail.com', 'Wes': np.nan&#125;data = Series(data)1dataDave dave@google.com Rob rob@gmail.com Steve steve@gmail.com Wes NaN dtype: object 1data.isnull()Dave False Rob False Steve False Wes True dtype: bool 1data.str.contains('gmail')Dave False Rob True Steve True Wes NaN dtype: object 1pattern&apos;([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})&apos; 1data.str.findall(pattern, flags=re.IGNORECASE)Dave [(dave, google, com)] Rob [(rob, gmail, com)] Steve [(steve, gmail, com)] Wes NaN dtype: object 12matches = data.str.match(pattern, flags=re.IGNORECASE)matchesC:\Users\Ewan\Anaconda3\lib\site-packages\ipykernel\__main__.py:1: FutureWarning: In future versions of pandas, match will change to always return a bool indexer. if __name__ == &apos;__main__&apos;: Dave (dave, google, com) Rob (rob, gmail, com) Steve (steve, gmail, com) Wes NaN dtype: object 1matches.str.get(1)Dave google Rob gmail Steve gmail Wes NaN dtype: object 1matches.str[0]Dave dave Rob rob Steve steve Wes NaN dtype: object 1data.str[:5]Dave dave@ Rob rob@g Steve steve Wes NaN dtype: object Example: USDA Food Database123456789101112131415161718192021222324252627&#123; "id": 21441, "description": "KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,Wing, meat and skin with breading", "tags": ["KFC"], "manufacturer": "Kentucky Fried Chicken", "group": "Fast Foods", "portions": [ &#123; "amount": 1, "unit": "wing, with skin", "grams": 68.0 &#125;, ... ], "nutrients": [ &#123; "value": 20.8, "units": "g", "description": "Protein", "group": "Composition" &#125;, ... ]&#125;123import jsondb = json.load(open('ch07/foods-2011-10-03.json'))len(db)6636 1db[0].keys()dict_keys([&apos;id&apos;, &apos;tags&apos;, &apos;portions&apos;, &apos;nutrients&apos;, &apos;description&apos;, &apos;group&apos;, &apos;manufacturer&apos;]) 1db[0]['nutrients'][0]{&apos;description&apos;: &apos;Protein&apos;, &apos;group&apos;: &apos;Composition&apos;, &apos;units&apos;: &apos;g&apos;, &apos;value&apos;: 25.18} 12nutrients = DataFrame(db[0]['nutrients'])nutrients[:7]descriptiongroupunitsvalue0ProteinCompositiong25.181Total lipid (fat)Compositiong29.202Carbohydrate, by differenceCompositiong3.063AshOtherg3.284EnergyEnergykcal376.005WaterCompositiong39.286EnergyEnergykJ1573.0012info_keys = ['description', 'group', 'id', 'manufacturer']info = DataFrame(db, columns=info_keys)1info[:5]descriptiongroupidmanufacturer0Cheese, carawayDairy and Egg Products10081Cheese, cheddarDairy and Egg Products10092Cheese, edamDairy and Egg Products10183Cheese, fetaDairy and Egg Products10194Cheese, mozzarella, part skim milkDairy and Egg Products10281pd.value_counts(info.group)[:10]Vegetables and Vegetable Products 812 Beef Products 618 Baked Products 496 Breakfast Cereals 403 Legumes and Legume Products 365 Fast Foods 365 Lamb, Veal, and Game Products 345 Sweets 341 Pork Products 328 Fruits and Fruit Juices 328 Name: group, dtype: int64 12345678nutrients = []for rec in db: fnuts = DataFrame(rec['nutrients']) fnuts['id'] = rec['id'] nutrients.append(fnuts)nutrients = pd.concat(nutrients, ignore_index=True)1nutrients[:10]descriptiongroupunitsvalueid0ProteinCompositiong25.1810081Total lipid (fat)Compositiong29.2010082Carbohydrate, by differenceCompositiong3.0610083AshOtherg3.2810084EnergyEnergykcal376.0010085WaterCompositiong39.2810086EnergyEnergykJ1573.0010087Fiber, total dietaryCompositiong0.0010088Calcium, CaElementsmg673.0010089Iron, FeElementsmg0.6410081nutrients.duplicated().sum()14179 1nutrients = nutrients.drop_duplicates()1234col_mapping = &#123;'description' : 'food', 'group' : 'fgroup'&#125;info = info.rename(columns=col_mapping, copy=False)info[:10]foodfgroupidmanufacturer0Cheese, carawayDairy and Egg Products10081Cheese, cheddarDairy and Egg Products10092Cheese, edamDairy and Egg Products10183Cheese, fetaDairy and Egg Products10194Cheese, mozzarella, part skim milkDairy and Egg Products10285Cheese, mozzarella, part skim milk, low moistureDairy and Egg Products10296Cheese, romanoDairy and Egg Products10387Cheese, roquefortDairy and Egg Products10398Cheese spread, pasteurized process, american, â€¦Dairy and Egg Products10489Cream, fluid, half and halfDairy and Egg Products10491234col_mapping = &#123;'description' : 'nutrient', 'group' : 'nutgroup'&#125;nutrients = nutrients.rename(columns=col_mapping, copy=False)nutrients[:10]nutrientnutgroupunitsvalueid0ProteinCompositiong25.1810081Total lipid (fat)Compositiong29.2010082Carbohydrate, by differenceCompositiong3.0610083AshOtherg3.2810084EnergyEnergykcal376.0010085WaterCompositiong39.2810086EnergyEnergykJ1573.0010087Fiber, total dietaryCompositiong0.0010088Calcium, CaElementsmg673.0010089Iron, FeElementsmg0.6410081ndata = pd.merge(nutrients, info, on='id', how='outer')1ndata[:10]nutrientnutgroupunitsvalueidfoodfgroupmanufacturer0ProteinCompositiong25.181008Cheese, carawayDairy and Egg Products1Total lipid (fat)Compositiong29.201008Cheese, carawayDairy and Egg Products2Carbohydrate, by differenceCompositiong3.061008Cheese, carawayDairy and Egg Products3AshOtherg3.281008Cheese, carawayDairy and Egg Products4EnergyEnergykcal376.001008Cheese, carawayDairy and Egg Products5WaterCompositiong39.281008Cheese, carawayDairy and Egg Products6EnergyEnergykJ1573.001008Cheese, carawayDairy and Egg Products7Fiber, total dietaryCompositiong0.001008Cheese, carawayDairy and Egg Products8Calcium, CaElementsmg673.001008Cheese, carawayDairy and Egg Products9Iron, FeElementsmg0.641008Cheese, carawayDairy and Egg Products1ndata.ix[30000]nutrient Glycine nutgroup Amino Acids units g value 0.04 id 6158 food Soup, tomato bisque, canned, condensed fgroup Soups, Sauces, and Gravies manufacturer Name: 30000, dtype: object 123result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)result[:10]result['Zinc, Zn'].sort_values().plot(kind='barh')nutrient fgroup Adjusted Protein Sweets 12.900 Vegetables and Vegetable Products 2.180 Alanine Baby Foods 0.085 Baked Products 0.248 Beef Products 1.550 Beverages 0.003 Breakfast Cereals 0.311 Cereal Grains and Pasta 0.373 Dairy and Egg Products 0.271 Ethnic Foods 1.290 Name: value, dtype: float64 &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ba08c9e780&gt; 123456789by_nutrient = ndata.groupby(['nutgroup', 'nutrient'])get_maximum = lambda x: x.xs(x.value.idxmax())get_minimum = lambda x: x.xs(x.value.idxmin())max_foods = by_nutrient.apply(get_maximum)[['value', 'food']]# make the food a little smallermax_foods.food = max_foods.food.str[:50]1max_foods.ix['Amino Acids']['food']nutrient Alanine Gelatins, dry powder, unsweetened Arginine Seeds, sesame flour, low-fat Aspartic acid Soy protein isolate Cystine Seeds, cottonseed flour, low fat (glandless) Glutamic acid Soy protein isolate Glycine Gelatins, dry powder, unsweetened Histidine Whale, beluga, meat, dried (Alaska Native) Hydroxyproline KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINA... Isoleucine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Leucine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Lysine Seal, bearded (Oogruk), meat, dried (Alaska Na... Methionine Fish, cod, Atlantic, dried and salted Phenylalanine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Proline Gelatins, dry powder, unsweetened Serine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Threonine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Tryptophan Sea lion, Steller, meat with fat (Alaska Native) Tyrosine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Valine Soy protein isolate, PROTEIN TECHNOLOGIES INTE... Name: food, dtype: object]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note Ch06]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-Ch06%2F</url>
      <content type="text"><![CDATA[Data loading, storage, and file formats12345678910111213from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport sysimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"1%pwd&apos;C:\\Users\\Ewan\\Downloads\\pydata-book-master&apos; Reading and Writing Data in Text Format1!more ch06\ex1.csva,b,c,d,message 1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo 12df = pd.read_csv('ch06/ex1.csv')dfabcdmessage01234hello15678world29101112foo1pd.read_table('ch06/ex1.csv', sep=',')abcdmessage01234hello15678world29101112foo1!more ch06\ex2.csv1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo 12pd.read_csv('ch06/ex2.csv', header=None)pd.read_csv('ch06/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])0123401234hello15678world29101112fooabcdmessage01234hello15678world29101112foo12names = ['a', 'b', 'c', 'd', 'message']pd.read_csv('ch06/ex2.csv', names=names, index_col='message')abcdmessagehello1234world5678foo9101112123!more ch06\csv_mindex.csvparsed = pd.read_csv('ch06/csv_mindex.csv', index_col=['key1', 'key2'])parsedkey1,key2,value1,value2 one,a,1,2 one,b,3,4 one,c,5,6 one,d,7,8 two,a,9,10 two,b,11,12 two,c,13,14 two,d,15,16 value1value2key1key2onea12b34c56d78twoa910b1112c1314d15161list(open('ch06/ex3.txt'))[&apos; A B C\n&apos;, &apos;aaa -0.264438 -1.026059 -0.619500\n&apos;, &apos;bbb 0.927272 0.302904 -0.032399\n&apos;, &apos;ccc -0.264273 -0.386314 -0.217601\n&apos;, &apos;ddd -0.871858 -0.348382 1.100491\n&apos;] é‡‡ç”¨æ­£åˆ™è¡¨è¾¾å¼ä½œä¸ºåˆ†éš”ç¬¦12result = pd.read_table('ch06/ex3.txt', sep='\s+')resultABCaaa-0.264438-1.026059-0.619500bbb0.9272720.302904-0.032399ccc-0.264273-0.386314-0.217601ddd-0.871858-0.3483821.10049112!more ch06\ex4.csvpd.read_csv('ch06/ex4.csv', skiprows=[0, 2, 3])# hey! a,b,c,d,message # just wanted to make things more difficult for you # who reads CSV files with computers, anyway? 1,2,3,4,hello 5,6,7,8,world 9,10,11,12,foo abcdmessage01234hello15678world29101112foo1234!more ch06\ex5.csvresult = pd.read_csv('ch06/ex5.csv')resultpd.isnull(result)something,a,b,c,d,message one,1,2,3,4,NA two,5,6,,8,world three,9,10,11,12,foo somethingabcdmessage0one123.04NaN1two56NaN8world2three91011.012foosomethingabcdmessage0FalseFalseFalseFalseFalseTrue1FalseFalseFalseTrueFalseFalse2FalseFalseFalseFalseFalseFalse12result = pd.read_csv('ch06/ex5.csv', na_values=['NULL'])resultsomethingabcdmessage0one123.04NaN1two56NaN8world2three91011.012foo12sentinels = &#123;'message': ['foo', 'NA'], 'something': ['two']&#125;pd.read_csv('ch06/ex5.csv', na_values=sentinels)somethingabcdmessage0one123.04NaN1NaN56NaN8world2three91011.012NaNé€å—è¯»å–æ–‡æœ¬æ–‡ä»¶12result = pd.read_csv('ch06/ex6.csv')resultonetwothreefourkey00.467976-0.038649-0.295344-1.824726L1-0.3588931.4044530.704965-0.200638B2-0.5018400.659254-0.421691-0.057688G30.2048861.0741341.388361-0.982404R40.354628-0.1331160.283763-0.837063Q51.8174800.7422730.419395-2.251035Q6-0.7767640.935518-0.332872-1.875641U7-0.9131351.530624-0.5726570.477252K80.358480-0.497572-0.3670160.507702S9-1.740877-1.160417-1.6378302.172201G100.240564-0.3282491.2521551.0727968110.7640181.165476-0.6395441.495258R120.571035-0.3105370.582437-0.2987651132.3176580.430710-1.3342160.199679P141.547771-1.119753-2.2776340.329586J15-1.3106080.401719-1.0009871.156708E16-0.0884960.6347120.1533240.415335B17-0.018663-0.247487-1.4465220.750938A18-0.070127-1.5790970.1208920.671432F19-0.194678-0.4920392.3596050.319810H20-0.2486180.868707-0.492226-0.717959W21-1.091549-0.867110-0.647760-0.832562C220.641404-0.138822-0.621963-0.284839C231.2164080.9926870.165162-0.069619V24-0.5644740.7928320.7470530.571675I251.759879-0.515666-0.2304811.362317S260.1262660.3092810.382820-0.239199L271.334360-0.100152-0.840731-0.643967628-0.7376200.278087-0.053235-0.950972J29-1.148486-0.986292-0.1449630.124362Yâ€¦â€¦â€¦â€¦â€¦â€¦99700.633495-0.1865240.9276270.143164499710.308636-0.1128570.762842-1.07297719972-1.627051-0.9781510.154745-1.229037Z99730.3148470.0979890.1996080.955193P99741.6669070.9920050.496128-0.686391S99750.0106030.708540-1.2587110.226541K99760.118693-0.714455-0.501342-0.254764K99770.302616-2.011527-0.6280850.768827H9978-0.0985721.769086-0.215027-0.053076A9979-0.0190581.9649940.738538-0.883776F9980-0.5953490.001781-1.423355-1.458477M99811.392170-1.396560-1.425306-0.847535H9982-0.896029-0.1522871.9244830.36518469983-2.274642-0.9018741.5003520.996541N9984-0.3018981.0199061.1021602.624526I9985-2.548389-0.5853741.496201-0.718815D9986-0.0645880.759292-1.568415-0.420933E9987-0.143365-1.111760-1.8155810.43527429988-0.070412-1.0559210.338017-0.440763X99890.6491480.994273-1.3842270.485120Q9990-0.3707690.404356-1.051628-1.05089989991-0.4099800.155627-0.8189901.277350W99920.301214-1.1112030.6682580.671922A99931.8211170.4164450.1738740.505118X99940.0688041.3227590.8023460.223618H99952.311896-0.417070-1.409599-0.515821L9996-0.479893-0.6504190.745152-0.646038E99970.5233310.7871120.4860661.093156K9998-0.3625590.598894-1.8432010.887292G9999-0.096376-1.012999-0.657431-0.573315010000 rows Ã— 5 columns1pd.read_csv('ch06/ex6.csv', nrows=5)onetwothreefourkey00.467976-0.038649-0.295344-1.824726L1-0.3588931.4044530.704965-0.200638B2-0.5018400.659254-0.421691-0.057688G30.2048861.0741341.388361-0.982404R40.354628-0.1331160.283763-0.837063Q12chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)chunker&lt;pandas.io.parsers.TextFileReader at 0x2035229de80&gt; 1234567chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)tot = Series([])for piece in chunker: tot = tot.add(piece['key'].value_counts(), fill_value=0)tot = tot.sort_values(ascending=False)1tot[:10]E 368.0 X 364.0 L 346.0 O 343.0 Q 340.0 M 338.0 J 337.0 F 335.0 K 334.0 H 330.0 dtype: float64 å°†æ•°æ®å†™å‡ºåˆ°æ–‡æœ¬æ ¼å¼12data = pd.read_csv('ch06/ex5.csv')datasomethingabcdmessage0one123.04NaN1two56NaN8world2three91011.012foo12data.to_csv('ch06/out.csv')!more ch06\out.csv,something,a,b,c,d,message 0,one,1,2,3.0,4, 1,two,5,6,,8,world 2,three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, sep='|')|something|a|b|c|d|message 0|one|1|2|3.0|4| 1|two|5|6||8|world 2|three|9|10|11.0|12|foo 1data.to_csv(sys.stdout, na_rep='NULL'),something,a,b,c,d,message 0,one,1,2,3.0,4,NULL 1,two,5,6,NULL,8,world 2,three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, index=False, header=False)one,1,2,3.0,4, two,5,6,,8,world three,9,10,11.0,12,foo 1data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])a,b,c 1,2,3.0 5,6, 9,10,11.0 123456dates = pd.date_range('1/1/2000', periods=7)datests = Series(np.arange(7), index=dates)tsts.to_csv('ch06/tseries.csv')!more ch06\tseries.csvDatetimeIndex([&apos;2000-01-01&apos;, &apos;2000-01-02&apos;, &apos;2000-01-03&apos;, &apos;2000-01-04&apos;, &apos;2000-01-05&apos;, &apos;2000-01-06&apos;, &apos;2000-01-07&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;) 2000-01-01 0 2000-01-02 1 2000-01-03 2 2000-01-04 3 2000-01-05 4 2000-01-06 5 2000-01-07 6 Freq: D, dtype: int32 2000-01-01,0 2000-01-02,1 2000-01-03,2 2000-01-04,3 2000-01-05,4 2000-01-06,5 2000-01-07,6 1Series.from_csv('ch06/tseries.csv', parse_dates=True)2000-01-01 0 2000-01-02 1 2000-01-03 2 2000-01-04 3 2000-01-05 4 2000-01-06 5 2000-01-07 6 dtype: int64 æ‰‹åŠ¨å¤„ç†åˆ†éš”ç¬¦æ ¼å¼1!more ch06\ex7.csv&quot;a&quot;,&quot;b&quot;,&quot;c&quot; &quot;1&quot;,&quot;2&quot;,&quot;3&quot; &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot; 1234import csvf = open('ch06/ex7.csv')reader = csv.reader(f)12for line in reader: print(line)[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;] [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;] [&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;] 12345678910lines = list(csv.reader(open('ch06/ex7.csv')))header, values = lines[0], lines[1:]for item in zip(*values): print(item) for h, v in zip(header, zip(*values)): print(h, v)data_dict = &#123;h: v for h, v in zip(header, zip(*values))&#125;data_dict(&apos;1&apos;, &apos;1&apos;) (&apos;2&apos;, &apos;2&apos;) (&apos;3&apos;, &apos;3&apos;) a (&apos;1&apos;, &apos;1&apos;) b (&apos;2&apos;, &apos;2&apos;) c (&apos;3&apos;, &apos;3&apos;) {&apos;a&apos;: (&apos;1&apos;, &apos;1&apos;), &apos;b&apos;: (&apos;2&apos;, &apos;2&apos;), &apos;c&apos;: (&apos;3&apos;, &apos;3&apos;)} 12345class my_dialect(csv.Dialect): lineterminator = '\n' delimiter = ';' quotechar = '"' quoting = csv.QUOTE_MINIMAL123456with open('mydata.csv', 'w') as f: writer = csv.writer(f, dialect=my_dialect) writer.writerow(('one', 'two', 'three')) writer.writerow(('1', '2', '3')) writer.writerow(('4', '5', '6')) writer.writerow(('7', '8', '9'))14 6 6 6 1!more mydata.csvone;two;three 1;2;3 4;5;6 7;8;9 JSONæ•°æ®12345678obj = """&#123;"name": "Wes", "places_lived": ["United States", "Spain", "Germany"], "pet": null, "siblings": [&#123;"name": "Scott", "age": 25, "pet": "Zuko"&#125;, &#123;"name": "Katie", "age": 33, "pet": "Cisco"&#125;]&#125;"""123import jsonresult = json.loads(obj)result{&apos;name&apos;: &apos;Wes&apos;, &apos;pet&apos;: None, &apos;places_lived&apos;: [&apos;United States&apos;, &apos;Spain&apos;, &apos;Germany&apos;], &apos;siblings&apos;: [{&apos;age&apos;: 25, &apos;name&apos;: &apos;Scott&apos;, &apos;pet&apos;: &apos;Zuko&apos;}, {&apos;age&apos;: 33, &apos;name&apos;: &apos;Katie&apos;, &apos;pet&apos;: &apos;Cisco&apos;}]} 1asjson = json.dumps(result) # convert to json12siblings = DataFrame(result['siblings'], columns=['name', 'age'])siblingsnameage0Scott251Katie33XMLå’ŒHTMLï¼š Webä¿¡æ¯æ”¶é›†NB. The Yahoo! Finance API has changed and this example no longer works123456from lxml.html import parsefrom urllib.request import urlopenparsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options'))doc = parsed.getroot()12links = doc.findall('.//a')links[15:20][&lt;Element a at 0x20352cad598&gt;, &lt;Element a at 0x20352cad5e8&gt;, &lt;Element a at 0x20352cad638&gt;, &lt;Element a at 0x20352cad688&gt;, &lt;Element a at 0x20352cad6d8&gt;] 1234lnk = links[28]lnklnk.get('href')lnk.text_content()&lt;Element a at 0x20352cad9a8&gt; &apos;/quote/NFLX?p=NFLX&apos; &apos;NFLX&apos; 12urls = [lnk.get('href') for lnk in doc.findall('.//a')]urls[-10:][&apos;//finance.yahoo.com/broker-comparison?bypass=true&apos;, &apos;https://help.yahoo.com/kb/index?page=content&amp;y=PROD_MAIL_ML&amp;locale=en_US&amp;id=SLN2310&amp;actp=productlink&apos;, &apos;http://help.yahoo.com/l/us/yahoo/finance/&apos;, &apos;https://yahoo.uservoice.com/forums/382977&apos;, &apos;http://info.yahoo.com/privacy/us/yahoo/&apos;, &apos;http://info.yahoo.com/relevantads/&apos;, &apos;http://info.yahoo.com/legal/us/yahoo/utos/utos-173.html&apos;, &apos;http://twitter.com/YahooFinance&apos;, &apos;http://facebook.com/yahoofinance&apos;, &apos;http://yahoofinance.tumblr.com&apos;] 123tables = doc.findall('.//table')len(tables)calls = tables[0]1 1rows = calls.findall('.//tr')123def _unpack(row, kind='td'): elts = row.findall('.//%s' % kind) return [val.text_content() for val in elts]12_unpack(rows[0], kind='th')_unpack(rows[1], kind='td')[] --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-87-7d371ed47023&gt; in &lt;module&gt;() 1 _unpack(rows[0], kind=&apos;th&apos;) ----&gt; 2 _unpack(rows[1], kind=&apos;td&apos;) IndexError: list index out of range 1234567from pandas.io.parsers import TextParserdef parse_options_data(table): rows = table.findall('.//tr') header = _unpack(rows[0], kind='th') data = [_unpack(r) for r in rows[1:]] return TextParser(data, names=header).get_chunk()123call_data = parse_options_data(calls)put_data = parse_options_data(puts)call_data[:10]Parsing XML with lxml.objectify12345from lxml import objectifypath = '.\ch06\mta_perf\Performance_MNR.xml'parsed = objectify.parse(open(path))root = parsed.getroot()12345678910111213data = []skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ', 'DESIRED_CHANGE', 'DECIMAL_PLACES']for elt in root.INDICATOR: el_data = &#123;&#125; for child in elt.getchildren(): if child.tag in skip_fields: continue el_data[child.tag] = child.pyval data.append(el_data)12perf = DataFrame(data)perf.ix[:10, 5:]INDICATOR_UNITMONTHLY_ACTUALMONTHLY_TARGETPERIOD_MONTHPERIOD_YEARYTD_ACTUALYTD_TARGET0%96.9951200896.9951%95952200896952%96.9953200896.3953%98.3954200896.8954%95.8955200896.6955%94.4956200896.2956%96957200896.2957%96.4958200896.2958%93.7959200895.9959%96.495102008969510%96.99511200896.195äºŒè¿›åˆ¶æ•°æ®æ ¼å¼123frame = pd.read_csv('ch06/ex1.csv')frameframe.to_pickle('ch06/frame_pickle')abcdmessage01234hello15678world29101112foo1pd.read_pickle('ch06/frame_pickle')abcdmessage01234hello15678world29101112fooä½¿ç”¨HDF5æ ¼å¼1234store = pd.HDFStore('mydata.h5')store['obj1'] = framestore['obj1_col'] = frame['a']store&lt;class &apos;pandas.io.pytables.HDFStore&apos;&gt; File path: mydata.h5 /obj1 frame (shape-&gt;[3,5]) /obj1_col series (shape-&gt;[3]) 1store['obj1']abcdmessage01234hello15678world29101112foo12store.close()os.remove('mydata.h5')ä½¿ç”¨æ•°æ®åº“1234567891011import sqlite3query = """CREATE TABLE test(a VARCHAR(20), b VARCHAR(20), c REAL, d INTEGER);"""con = sqlite3.connect(':memory:')con.execute(query)con.commit()&lt;sqlite3.Cursor at 0x2035487c880&gt; 1234567data = [('Atlanta', 'Georgia', 1.25, 6), ('Tallahassee', 'Florida', 2.6, 3), ('Sacramento', 'California', 1.7, 5)]stmt = "INSERT INTO test VALUES(?, ?, ?, ?)"con.executemany(stmt, data)con.commit()&lt;sqlite3.Cursor at 0x2035487c810&gt; 123cursor = con.execute('select * from test')rows = cursor.fetchall()rows[(&apos;Atlanta&apos;, &apos;Georgia&apos;, 1.25, 6), (&apos;Tallahassee&apos;, &apos;Florida&apos;, 2.6, 3), (&apos;Sacramento&apos;, &apos;California&apos;, 1.7, 5)] 12import pandas.io.sql as sqlsql.read_sql('select * from test', con)abcd0AtlantaGeorgia1.2561TallahasseeFlorida2.6032SacramentoCalifornia1.705]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis Learning note ch05]]></title>
      <url>%2F2017%2F02%2F28%2FPython-data-analysis-Learning-note-ch05%2F</url>
      <content type="text"><![CDATA[pandaså…¥é—¨æŒ‰ç…§ä»¥ä¸‹çº¦å®šå¼•ç”¨ç›¸å…³package12from pandas import Series, DataFrameimport pandas as pd123456789101112from __future__ import divisionfrom numpy.random import randnimport numpy as npimport osimport matplotlib.pyplot as pltnp.random.seed(12345)plt.rc('figure', figsize=(10, 6))from pandas import Series, DataFrameimport pandas as pdnp.set_printoptions(precision=4)from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"pandasæ•°æ®ç»“æž„ä»‹ç»SeriesSeriesæ˜¯ä¸€ç§ç±»ä¼¼äºŽä¸€ç»´æ•°ç»„çš„å¯¹è±¡ï¼Œç”±ä¸€ç»„æ•°æ®ä»¥åŠä¸€ç»„ä¸Žä¹‹ç›¸å…³çš„æ•°æ®æ ‡ç­¾ï¼ˆç±»ä¼¼äºŽå­—å…¸çš„é”®ï¼‰ç»„æˆï¼Œæ‰€ä»¥å¯ä»¥çœ‹æˆæ˜¯ä¸€ä¸ªæœ‰åºçš„å­—å…¸12obj = Series([4, 7, -5, 3])obj0 4 1 7 2 -5 3 3 dtype: int64 12obj.valuesobj.indexarray([ 4, 7, -5, 3], dtype=int64) RangeIndex(start=0, stop=4, step=1) 12obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])obj2d 4 b 7 a -5 c 3 dtype: int64 1obj2.indexIndex([&apos;d&apos;, &apos;b&apos;, &apos;a&apos;, &apos;c&apos;], dtype=&apos;object&apos;) 1obj2['a']-5 12obj2['d'] = 6obj2[['c', 'a', 'd']]c 3 a -5 d 6 dtype: int64 å„ç§Numpyè¿ç®—éƒ½æ˜¯ä½œç”¨åœ¨æ•°æ®ä¸Šï¼ŒåŒæ—¶ç´¢å¼•ä¸Žæ•°æ®çš„é“¾æŽ¥ä¼šä¸€ç›´ä¿æŒ1obj2[obj2 &gt; 0]d 6 b 7 c 3 dtype: int64 1obj2 * 2d 12 b 14 a -10 c 6 dtype: int64 1np.exp(obj2)d 403.428793 b 1096.633158 a 0.006738 c 20.085537 dtype: float64 1'b' in obj2True 1'e' in obj2False å› æ­¤å¯ä»¥ç›´æŽ¥æ ¹æ®Numpyçš„Dictæ¥åˆ›å»ºSeries123sdata = &#123;'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000&#125;obj3 = Series(sdata)obj3Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 å¦‚æžœä¼ å…¥äº†indexå‚æ•°çš„è¯ï¼Œé‚£ä¹ˆå°±ä¼šä¸Žä¼ å…¥çš„Dictåšé”®åŒ¹é…ï¼Œæ²¡æœ‰åŒ¹é…ä¸Šçš„å°±è®¾ä¸ºNaN123states = ['California', 'Ohio', 'Oregon', 'Texas']obj4 = Series(sdata, index=states)obj4California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64 1pd.isnull(obj4)California True Ohio False Oregon False Texas False dtype: bool 1pd.notnull(obj4)California False Ohio True Oregon True Texas True dtype: bool 1obj4.isnull()California True Ohio False Oregon False Texas False dtype: bool Seriesæœ‰ä¸€ä¸ªéžå¸¸é‡è¦çš„æ•°æ®å¯¹é½çš„åŠŸèƒ½1obj3Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 1obj4California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64 1obj3 + obj4California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 Seriesæœ¬èº«ä»¥åŠå…¶ç´¢å¼•éƒ½æœ‰ä¸€ä¸ªå«åšnameçš„å±žæ€§ï¼Œè¿™ä¸ªå±žæ€§ååˆ†é‡è¦ï¼Œä»¥åŽå¾ˆå¤šé«˜çº§åŠŸèƒ½éƒ½ä¼šç”¨åˆ°123obj4.name = 'population'obj4.index.name = 'state'obj4state California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 Name: population, dtype: float64 å¯ä»¥é€šè¿‡ç›´æŽ¥èµ‹å€¼çš„æ–¹å¼ä¿®æ”¹indexå±žæ€§12obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']objBob 4 Steve 7 Jeff -5 Ryan 3 dtype: int64 DataFrameDataFrameæ˜¯ä¸€ä¸ªè¡¨æ ¼åž‹çš„æ•°æ®ç»“æž„ï¼Œå¯ä»¥çœ‹æˆç”±Seriesç»„æˆçš„å­—å…¸ï¼Œåªä¸è¿‡è¿™äº›Serieså…±ç”¨ä¸€å¥—ç´¢å¼•1234data = &#123;'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2002, 2001, 2002], 'pop': [1.5, 1.7, 3.6, 2.4, 2.9]&#125;frame = DataFrame(data)æ•°æ®ä¼šè¢«æŽ’åº1framepopstateyear01.5Ohio200011.7Ohio200123.6Ohio200232.4Nevada200142.9Nevada20021DataFrame(data, columns=['year', 'state', 'pop'])yearstatepop02000Ohio1.512001Ohio1.722002Ohio3.632001Nevada2.442002Nevada2.9123frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'], index=['one', 'two', 'three', 'four', 'five'])frame2yearstatepopdebtone2000Ohio1.5NaNtwo2001Ohio1.7NaNthree2002Ohio3.6NaNfour2001Nevada2.4NaNfive2002Nevada2.9NaN1frame2.columnsIndex([&apos;year&apos;, &apos;state&apos;, &apos;pop&apos;, &apos;debt&apos;], dtype=&apos;object&apos;) DataFrameæ¯ä¸€ä¸ªKeyå¯¹åº”çš„Valueéƒ½æ˜¯ä¸€ä¸ªSeries1frame2['state']one Ohio two Ohio three Ohio four Nevada five Nevada Name: state, dtype: object 1frame2.yearone 2000 two 2001 three 2002 four 2001 five 2002 Name: year, dtype: int64 æ³¨æ„åˆ°nameå±žæ€§ä¹Ÿå·²ç»è¢«è®¾ç½®å¥½äº†ixç›¸å½“äºŽä¸€ä¸ªè¡Œç´¢å¼•ï¼Ÿ1frame2.ix['three']year 2002 state Ohio pop 3.6 debt NaN Name: three, dtype: object å¯ä»¥åˆ©ç”¨Numpyçš„å¹¿æ’­åŠŸèƒ½12frame2['debt'] = 16.5frame2yearstatepopdebtone2000Ohio1.516.5two2001Ohio1.716.5three2002Ohio3.616.5four2001Nevada2.416.5five2002Nevada2.916.5ä¹Ÿå¯ä»¥èµ‹å€¼ä¸€ä¸ªåˆ—è¡¨ï¼Œä½†æ˜¯é•¿åº¦å¿…é¡»åŒ¹é…12frame2['debt'] = np.arange(5.)frame2yearstatepopdebtone2000Ohio1.50.0two2001Ohio1.71.0three2002Ohio3.62.0four2001Nevada2.43.0five2002Nevada2.94.0å¦‚æžœæ˜¯èµ‹å€¼ä¸€ä¸ªSeriesï¼Œåˆ™ä¼šåŒ¹é…ä¸Šç´¢å¼•ï¼Œæ²¡æœ‰åŒ¹é…ä¸Šçš„å°±æ˜¯ç½®ä¸ºNaN123val = Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])frame2['debt'] = valframe2yearstatepopdebtone2000Ohio1.5NaNtwo2001Ohio1.7-1.2three2002Ohio3.6NaNfour2001Nevada2.4-1.5five2002Nevada2.9-1.7ä¹Ÿå¯ä»¥è¿›è¡Œé€»è¾‘æ“ä½œ12frame2['eastern'] = frame2.state == 'Ohio'frame2yearstatepopdebteasternone2000Ohio1.5NaNTruetwo2001Ohio1.7-1.2Truethree2002Ohio3.6NaNTruefour2001Nevada2.4-1.5Falsefive2002Nevada2.9-1.7Falsedelç”¨äºŽåˆ é™¤ä¸€åˆ—12del frame2['eastern']frame2.columnsIndex([&apos;year&apos;, &apos;state&apos;, &apos;pop&apos;, &apos;debt&apos;], dtype=&apos;object&apos;) åªè¦æ˜¯é€šè¿‡ç´¢å¼•æ–¹å¼è¿›è¡Œçš„æ“ä½œï¼Œéƒ½æ˜¯ç›´æŽ¥åœ¨åŽŸæ•°æ®ä¸Šè¿›è¡Œçš„æ“ä½œï¼Œä¸æ˜¯ä¸€ä¸ªå‰¯æœ¬åµŒå¥—çš„å­—å…¸ä¹Ÿå¯ç›´æŽ¥ç”ŸæˆDataFrameï¼Œåªä¸è¿‡å†…å±‚çš„é”®è¢«å½“ä½œindexï¼Œå¤–å±‚çš„é”®è¢«å½“ä½œcolums12pop = &#123;'Nevada': &#123;2001: 2.4, 2002: 2.9&#125;, 'Ohio': &#123;2000: 1.5, 2001: 1.7, 2002: 3.6&#125;&#125;12frame3 = DataFrame(pop)frame3NevadaOhio2000NaN1.520012.41.720022.93.6åŒæ ·å¯ä»¥è¿›è¡Œè½¬ç½®ï¼Œè¿™æ ·çš„è¯indexå’Œcolumnå°±ä¼šäº’æ¢1frame3.T200020012002NevadaNaN2.42.9Ohio1.51.73.6æ˜¾å¼åœ°æŒ‡å®šç´¢å¼•ï¼Œä¸åŒ¹é…çš„ä¼šç½®ä¸ºNaN1DataFrame(pop, index=[2001, 2002, 2003])NevadaOhio20012.41.720022.93.62003NaNNaN1frame3NevadaOhio2000NaN1.520012.41.720022.93.6è¿˜å¯ä»¥è¿™æ ·æž„å»º123pdata = &#123;'Ohio': frame3['Ohio'][:-1], 'Nevada': frame3['Nevada'][:2]&#125;DataFrame(pdata)NevadaOhio2000NaN1.520012.41.71frame3NevadaOhio2000NaN1.520012.41.720022.93.6nameå±žæ€§ä¹Ÿä¼šåœ¨è¡¨æ ¼ä¸­æ˜¾ç¤ºå‡ºæ¥12frame3.index.name = 'year'; frame3.columns.name = 'state'frame3stateNevadaOhioyear2000NaN1.520012.41.720022.93.6valuesæ–¹æ³•åªè¿”å›žæ•°æ®ï¼Œä¸è¿”å›žindexä»¥åŠkey1frame3.valuesarray([[ nan, 1.5], [ 2.4, 1.7], [ 2.9, 3.6]]) 1frame2yearstatepopdebtone2000Ohio1.5NaNtwo2001Ohio1.7-1.2three2002Ohio3.6NaNfour2001Nevada2.4-1.5five2002Nevada2.9-1.71frame2.valuesarray([[2000, &apos;Ohio&apos;, 1.5, nan], [2001, &apos;Ohio&apos;, 1.7, -1.2], [2002, &apos;Ohio&apos;, 3.6, nan], [2001, &apos;Nevada&apos;, 2.4, -1.5], [2002, &apos;Nevada&apos;, 2.9, -1.7]], dtype=object) ç´¢å¼•å¯¹è±¡Indexæ˜¯ä¸€ä¸ªå¯ä»¥å•ç‹¬æå–å‡ºæ¥çš„å¯¹è±¡123obj = Series(range(3), index=['a', 'b', 'c'])index = obj.indexindexIndex([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], dtype=&apos;object&apos;) 1index[1:]Index([&apos;b&apos;, &apos;c&apos;], dtype=&apos;object&apos;) ä¸å¯ä¿®æ”¹~ï¼1index[1] = 'd'--------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-52-676fdeb26a68&gt; in &lt;module&gt;() ----&gt; 1 index[1] = &apos;d&apos; C:\Users\Ewan\Anaconda3\lib\site-packages\pandas\indexes\base.py in __setitem__(self, key, value) 1243 1244 def __setitem__(self, key, value): -&gt; 1245 raise TypeError(&quot;Index does not support mutable operations&quot;) 1246 1247 def __getitem__(self, key): TypeError: Index does not support mutable operations ç›´æŽ¥åˆ›å»ºIndexå¯¹è±¡123index = pd.Index(np.arange(3))obj2 = Series([1.5, -2.5, 0], index=index)obj2.index is indexTrue 1frame3stateNevadaOhioyear2000NaN1.520012.41.720022.93.61'Ohio' in frame3.columnsTrue 12003 in frame3.indexFalse åŸºæœ¬åŠŸèƒ½é‡æ–°ç´¢å¼•12obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])objd 4.5 b 7.2 a -5.3 c 3.6 dtype: float64 é‡æŽ’ç´¢å¼•å½¢æˆæ–°å¯¹è±¡12obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])obj2a -5.3 b 7.2 c 3.6 d 4.5 e NaN dtype: float64 1obj.reindex(['a', 'b', 'c', 'd', 'e'], fill_value=0)a -5.3 b 7.2 c 3.6 d 4.5 e 0.0 dtype: float64 12obj3 = Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])obj3.reindex(range(6), method='ffill') # å‰å‘å¡«å……0 blue 1 blue 2 purple 3 purple 4 yellow 5 yellow dtype: object 123frame = DataFrame(np.arange(9).reshape((3, 3)), index=['a', 'c', 'd'], columns=['Ohio', 'Texas', 'California'])frameOhioTexasCaliforniaa012c345d67812frame2 = frame.reindex(['a', 'b', 'c', 'd'])frame2OhioTexasCaliforniaa0.01.02.0bNaNNaNNaNc3.04.05.0d6.07.08.012states = ['Texas', 'Utah', 'California']frame.reindex(columns=states)TexasUtahCaliforniaa1NaN2c4NaN5d7NaN8æ’å€¼åªèƒ½æŒ‰è¡Œ12frame.reindex(index=['a', 'b', 'c', 'd'], method='ffill', columns=states)TexasUtahCaliforniaa1NaN2b1NaN2c4NaN5d7NaN8ç”¨ixæ–¹æ³•è¿›è¡Œé‡æ–°ç´¢å¼•æ“ä½œä¼šä½¿å¾—ä»£ç å¾ˆç®€æ´1frame.ix[['a', 'b', 'c', 'd'], states]TexasUtahCaliforniaa1.0NaN2.0bNaNNaNNaNc4.0NaN5.0d7.0NaN8.0ä¸¢å¼ƒæŒ‡å®šè½´ä¸Šçš„é¡¹1frame.ix[['a', 'b', 'c', 'd'], states]TexasUtahCaliforniaa1.0NaN2.0bNaNNaNNaNc4.0NaN5.0d7.0NaN8.0123obj = Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])new_obj = obj.drop('c')new_obja 0.0 b 1.0 d 3.0 e 4.0 dtype: float64 1obj.drop(['d', 'c'])a 0.0 b 1.0 e 4.0 dtype: float64 1234data = DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])dataonetwothreefourOhio0123Colorado4567Utah891011New York121314151data.drop(['Colorado', 'Ohio'])onetwothreefourUtah891011New York121314151data.drop('two', axis=1)onethreefourOhio023Colorado467Utah81011New York1214151data.drop(['two', 'four'], axis=1)onethreeOhio02Colorado46Utah810New York1214ç´¢å¼•ï¼Œé€‰å–å’Œè¿‡æ»¤å¤šç§ç´¢å¼•æ–¹å¼123obj = Series(np.arange(4.), index=['a', 'b', 'c', 'd'])objobj['b']a 0.0 b 1.0 c 2.0 d 3.0 dtype: float64 1.0 1obj[1]1.0 1obj[2:4]c 2.0 d 3.0 dtype: float64 1obj[['b', 'a', 'd']]b 1.0 a 0.0 d 3.0 dtype: float64 1obj[[1, 3]]b 1.0 d 3.0 dtype: float64 1obj[obj &lt; 2] # ç›´æŽ¥å¯¹dataè¿›è¡Œæ“ä½œa 0.0 b 1.0 dtype: float64 è¿™ç§åˆ‡ç‰‡æ–¹å¼â€¦æœ«ç«¯åŒ…å«1obj['b':'c']b 1.0 c 2.0 dtype: float64 12obj['b':'c'] = 5obja 0.0 b 5.0 c 5.0 d 3.0 dtype: float64 1234data = DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])dataonetwothreefourOhio0123Colorado4567Utah891011New York121314151data['two']Ohio 1 Colorado 5 Utah 9 New York 13 Name: two, dtype: int32 1data[['three', 'one']]threeoneOhio20Colorado64Utah108New York14121data[:2] # axis=0onetwothreefourOhio0123Colorado45671data[data['three'] &gt; 5]onetwothreefourColorado4567Utah891011New York121314151data &lt; 5onetwothreefourOhioTrueTrueTrueTrueColoradoTrueFalseFalseFalseUtahFalseFalseFalseFalseNew YorkFalseFalseFalseFalse1data[data &lt; 5] = 01dataonetwothreefourOhio0000Colorado0567Utah891011New York12131415ç´¢å¼•çš„å¦å¤–ä¸€ç§é€‰æ‹©1data.ix['Colorado', ['two', 'three']]two 5 three 6 Name: Colorado, dtype: int32 1data.ix[['Colorado', 'Utah'], [3, 0, 1]]fouronetwoColorado705Utah11891data.ix[2]one 8 two 9 three 10 four 11 Name: Utah, dtype: int32 1data.ix[:'Utah', 'two']Ohio 0 Colorado 5 Utah 9 Name: two, dtype: int32 1data.ix[data.three &gt; 5, :3]onetwothreeColorado056Utah8910New York121314æ€»ç»“ä¸€ä¸‹å°±æ˜¯è¯´ï¼ŒDataFrameæ˜¯ä¸€ä¸ªäºŒç»´çš„æ•°ç»„ï¼Œåªä¸è¿‡æ¯ä¸€ç»´çš„ç´¢å¼•æ–¹å¼é™¤äº†åºå·ä¹‹å¤–ï¼Œè¿˜å¯ä»¥ç”¨nameå±žæ€§æ¥è¿›è¡Œç´¢å¼•ï¼Œä¸”ä¸€åˆ‡è¡Œä¸ºä¸Žåºå·æ— å¼‚ç®—æœ¯è¿ç®—å’Œæ•°æ®å¯¹é½12s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])1s1a 7.3 c -2.5 d 3.4 e 1.5 dtype: float64 1s2a -2.1 c 3.6 e -1.5 f 4.0 g 3.1 dtype: float64 æ•°æ®å¯¹é½æ“ä½œå°±æ˜¯ä¸€ç§ç‰¹æ®Šçš„å¹¶é›†æ“ä½œ1s1 + s2a 5.2 c 1.1 d NaN e 0.0 f NaN g NaN dtype: float64 12345df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'), index=['Ohio', 'Texas', 'Colorado'])df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])df1bcdOhio0.01.02.0Texas3.04.05.0Colorado6.07.08.01df2bdeUtah0.01.02.0Ohio3.04.05.0Texas6.07.08.0Oregon9.010.011.0å¹¶ä¸”æ•°æ®å¯¹é½æ“ä½œæ˜¯åœ¨æ‰€æœ‰ç»´åº¦ä¸ŠåŒæ—¶è¿›è¡Œçš„1df1 + df2bcdeColoradoNaNNaNNaNNaNOhio3.0NaN6.0NaNOregonNaNNaNNaNNaNTexas9.0NaN12.0NaNUtahNaNNaNNaNNaNåœ¨ç®—æœ¯æ–¹æ³•ä¸­å¡«å……è¯ä¸‹é¢è¿™ç§å®šä¹‰columnçš„æ–¹å¼å€¼å¾—æ³¨æ„123df1 = DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))df2 = DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))df1abcd00.01.02.03.014.05.06.07.028.09.010.011.01df2abcde00.01.02.03.04.015.06.07.08.09.0210.011.012.013.014.0315.016.017.018.019.01df1 + df2abcde00.02.04.06.0NaN19.011.013.015.0NaN218.020.022.024.0NaN3NaNNaNNaNNaNNaNè¦æƒ³å¡«å……å€¼å¿…é¡»ä½¿ç”¨addæ–¹æ³•1df1.add(df2, fill_value=0)abcde00.02.04.06.04.019.011.013.015.09.0218.020.022.024.014.0315.016.017.018.019.0reindexæ–¹æ³•ä¸Žaddæ–¹æ³•è¿˜æ˜¯å­˜åœ¨å·®å¼‚çš„1df1.reindex(columns=df2.columns, fill_value=0)abcde00.01.02.03.0014.05.06.07.0028.09.010.011.00DataFrameå’ŒSeriesä¹‹é—´çš„è¿ç®—12arr = np.arange(12.).reshape((3, 4))arrarray([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) 1arr[0]array([ 0., 1., 2., 3.]) å¹¿æ’­æ“ä½œ1arr - arr[0]array([[ 0., 0., 0., 0.], [ 4., 4., 4., 4.], [ 8., 8., 8., 8.]]) 1234frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])series = frame.ix[0]framebdeUtah0.01.02.0Ohio3.04.05.0Texas6.07.08.0Oregon9.010.011.0Seriesçš„nameç­‰äºŽDataFrameçš„åˆ‡ç‰‡å±žæ€§1seriesb 0.0 d 1.0 e 2.0 Name: Utah, dtype: float64 é»˜è®¤æƒ…å†µä¸‹ï¼ŒDataFrameå’ŒSeriesä¹‹é—´çš„ç®—æœ¯è¿ç®—ä¼šå°†Seriesçš„indexåŒ¹é…åˆ°DataFrameçš„column, ç„¶åŽæ²¿ç€è¡Œå‘ä¸‹å¹¿æ’­1frame - seriesbdeUtah0.00.00.0Ohio3.03.03.0Texas6.06.06.0Oregon9.09.09.0å¦‚æžœSeriesçš„indexä¸ŽDataFrameçš„columnä¸åŒ¹é…ï¼Œåˆ™è¿›è¡Œæ•°æ®å¯¹é½12series2 = Series(range(3), index=['b', 'e', 'f'])frame + series2bdefUtah0.0NaN3.0NaNOhio3.0NaN6.0NaNTexas6.0NaN9.0NaNOregon9.0NaN12.0NaN12series3 = frame['d']framebdeUtah0.01.02.0Ohio3.04.05.0Texas6.07.08.0Oregon9.010.011.01series3Utah 1.0 Ohio 4.0 Texas 7.0 Oregon 10.0 Name: d, dtype: float64 åŒ¹é…è¡Œå¹¶ä¸”åœ¨åˆ—ä¸Šè¿›è¡Œå¹¿æ’­ï¼Œ å°±å¿…é¡»è¦æŒ‡å®šaxis12frame.sub(series3, axis=0)frame.sub(series3, axis=1)bdeUtah-1.00.01.0Ohio-1.00.01.0Texas-1.00.01.0Oregon-1.00.01.0OhioOregonTexasUtahbdeUtahNaNNaNNaNNaNNaNNaNNaNOhioNaNNaNNaNNaNNaNNaNNaNTexasNaNNaNNaNNaNNaNNaNNaNOregonNaNNaNNaNNaNNaNNaNNaNå‡½æ•°åº”ç”¨å’Œæ˜ å°„12frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])1framebdeUtah-0.2047080.478943-0.519439Ohio-0.5557301.9657811.393406Texas0.0929080.2817460.769023Oregon1.2464351.007189-1.296221Numpyçš„å…ƒç´ çº§æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨åˆ°DataFrameä¸Šï¼Œç›´æŽ¥æŠŠDataFrameå½“ä½œäºŒç»´çš„Numpy.arrayå³å¯1np.abs(frame)bdeUtah0.2047080.4789430.519439Ohio0.5557301.9657811.393406Texas0.0929080.2817460.769023Oregon1.2464351.0071891.2962211f = lambda x: x.max() - x.min()applyæ–¹æ³•å°†å‡½æ•°æ˜ å°„åˆ°ç”±å„è¡Œæˆ–è€…å„åˆ—å½¢æˆçš„ä¸€ç»´æ•°ç»„ä¸Š1frame.apply(f) # axis=0b 1.802165 d 1.684034 e 2.689627 dtype: float64 1frame.apply(f, axis=1)Utah 0.998382 Ohio 2.521511 Texas 0.676115 Oregon 2.542656 dtype: float64 1234def f(x): return Series([x.min(), x.max()], index=['min', 'max'])frame.apply(f)frame.apply(f, axis=1)bdemin-0.5557300.281746-1.296221max1.2464351.9657811.393406minmaxUtah-0.5194390.478943Ohio-0.5557301.965781Texas0.0929080.769023Oregon-1.2962211.246435å…ƒç´ çº§çš„å‡½æ•°æ˜ å°„applymapï¼Œ ä¹‹æ‰€ä»¥å«è¿™ä¸ªåå­—æ˜¯å› ä¸ºSeriesæœ‰ä¸€ä¸ªå…ƒç´ çº§çš„æ˜ å°„å‡½æ•°map12format = lambda x: '%.2f' % xframe.applymap(format)bdeUtah-0.200.48-0.52Ohio-0.561.971.39Texas0.090.280.77Oregon1.251.01-1.301frame['e'].map(format)Utah -0.52 Ohio 1.39 Texas 0.77 Oregon -1.30 Name: e, dtype: object æŽ’åºå’ŒæŽ’åå¯¹ç´¢å¼•æˆ–è€…columnè¿›è¡Œï¼ˆå­—å…¸ï¼‰æŽ’åº12obj = Series(range(4), index=['d', 'a', 'b', 'c'])obj.sort_index()a 1 b 2 c 3 d 0 dtype: int32 123frame = DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'], columns=['d', 'a', 'b', 'c'])frame.sort_index()dabcone4567three01231frame.sort_index(axis=1)abcdthree1230one5674é™åº1frame.sort_index(axis=1, ascending=False)dcbathree0321one4765æŒ‰ç…§dataè¿›è¡ŒæŽ’åº12obj = Series([4, 7, -3, 2])obj.sort_values()2 -3 3 2 0 4 1 7 dtype: int64 åœ¨æŽ’åºæ—¶ï¼Œä»»ä½•ç¼ºå¤±å€¼é»˜è®¤éƒ½ä¼šè¢«æ”¾åˆ°Seriesçš„æœ«å°¾12obj = Series([4, np.nan, 7, np.nan, -3, 2])obj.sort_values()4 -3.0 5 2.0 0 4.0 2 7.0 1 NaN 3 NaN dtype: float64 12frame = DataFrame(&#123;'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]&#125;)frameab00411720-3312å¯¹æŒ‡å®šindexæˆ–è€…columnè¿›è¡ŒæŽ’åº1frame.sort_values(by='b')ab20-3312004117æˆ–è€…æ ¹æ®multi-indexäº¦æˆ–multi-columnè¿›è¡ŒæŽ’åº1frame.sort_values(by=['a', 'b'])ab20-3004312117é»˜è®¤æƒ…å†µä¸‹ï¼Œrankæ–¹æ³•é€šè¿‡â€œä¸ºå„ç»„åˆ†é…ä¸€ä¸ªå¹³å‡æŽ’åâ€çš„æ–¹å¼ç ´åå¹³çº§å…³ç³»ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æžœæœ‰å¤šä¸ªç›¸åŒçš„å€¼ï¼Œåˆ™è¿™äº›å€¼çš„rankå°±æ˜¯è¿™äº›ç›¸åŒå€¼randçš„ç®—æœ¯å¹³å‡ã€‚12obj = Series([7, -5, 7, 4, 2, 0, 4, 4])obj.rank()0 7.5 1 1.0 2 7.5 3 5.0 4 3.0 5 2.0 6 5.0 7 5.0 dtype: float64 å¦‚æžœæƒ³æŒ‰ç…§ä¸€èˆ¬æ–¹å¼æŽ’å1obj.rank(method='first')0 7.0 1 1.0 2 8.0 3 4.0 4 3.0 5 2.0 6 5.0 7 6.0 dtype: float64 ä½¿ç”¨æ¯ä¸ªåˆ†ç»„çš„æœ€å¤§æŽ’å1obj.rank(ascending=False, method='max')0 2.0 1 8.0 2 2.0 3 5.0 4 6.0 5 7.0 6 5.0 7 5.0 dtype: float64 123frame = DataFrame(&#123;'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1], 'c': [-2, 5, 8, -2.5]&#125;)frameabc004.3-2.0117.05.020-3.08.0312.0-2.5æŒ‡å®šç»´åº¦1frame.rank(axis=1)abc02.03.01.011.03.02.022.01.03.032.03.01.0å¸¦æœ‰é‡å¤å€¼çš„è½´ç´¢å¼•12obj = Series(range(5), index=['a', 'a', 'b', 'b', 'c'])obja 0 a 1 b 2 b 3 c 4 dtype: int32 1obj.index.is_uniqueFalse è¿”å›žä¸€ä¸ªSeries1obj['a']a 0 a 1 dtype: int32 1obj['c']4 12df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])df012a0.2749920.2289131.352917a0.886429-2.001637-0.371843b1.669025-0.438570-0.539741b0.4769853.248944-1.0212281df.ix['b']012b1.669025-0.438570-0.539741b0.4769853.248944-1.021228æ±‡æ€»å’Œè®¡ç®—æè¿°ç»Ÿè®¡12345df = DataFrame([[1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]], index=['a', 'b', 'c', 'd'], columns=['one', 'two'])dfonetwoa1.40NaNb7.10-4.5cNaNNaNd0.75-1.31df.sum() # axis=0 skipna=Trueone 9.25 two -5.80 dtype: float64 1df.sum(axis=1) # skipna=Truea 1.40 b 2.60 c 0.00 d -0.55 dtype: float64 1df.mean(axis=1, skipna=False)a NaN b 1.300 c NaN d -0.275 dtype: float64 è¿”å›žçš„æ˜¯ç´¢å¼•1df.idxmax()one b two d dtype: object 1df.cumsum()onetwoa1.40NaNb8.50-4.5cNaNNaNd9.25-5.8describeå¯¹äºŽæ•°å€¼åž‹å’Œéžæ•°å€¼åž‹æ•°æ®çš„è¡Œä¸ºä¸ä¸€æ ·1df.describe()C:\Users\Ewan\Anaconda3\lib\site-packages\numpy\lib\function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile RuntimeWarning) onetwocount3.0000002.000000mean3.083333-2.900000std3.4936852.262742min0.750000-4.50000025%NaNNaN50%NaNNaN75%NaNNaNmax7.100000-1.300000123obj = Series(['a', 'a', 'b', 'c'] * 4)objobj.describe()0 a 1 a 2 b 3 c 4 a 5 a 6 b 7 c 8 a 9 a 10 b 11 c 12 a 13 a 14 b 15 c dtype: object count 16 unique 3 top a freq 8 dtype: object ç›¸å…³ç³»æ•°å’Œxi12345678910import pandas_datareader.data as weball_data = &#123;&#125;for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']: all_data[ticker] = web.get_data_yahoo(ticker)price = DataFrame(&#123;tic: data['Adj Close'] for tic, data in all_data.items()&#125;)volume = DataFrame(&#123;tic: data['Volume'] for tic, data in all_data.items()&#125;)1price[:10]AAPLGOOGIBMMSFTDate2010-01-0427.727039313.062468111.40500025.5554852010-01-0527.774976311.683844110.05923225.5637412010-01-0627.333178303.826685109.34428325.4068592010-01-0727.282650296.753749108.96578625.1426342010-01-0827.464034300.709808110.05923225.3160312010-01-1127.221758300.255255108.90690324.9940072010-01-1226.912110294.945572109.77324524.8288662010-01-1327.291720293.252243109.53773525.0600642010-01-1427.133657294.630868111.28724525.5637412010-01-1526.680198289.710772110.84145825.4811721volume[:10]AAPLGOOGIBMMSFTDate2010-01-0412343240039270006155300384091002010-01-0515047620060319006841400497496002010-01-0613804000079871005605300581824002010-01-07119282800128766005840600505597002010-01-0811190270094839004197200511974002010-01-11115557400144798005730400687547002010-01-1214861490097429008081500659121002010-01-13151473000130418006455400518635002010-01-1410822350085119007111800632281002010-01-1514851690010909600849440079913200price.pct_changeSignature: price.pct_change(periods=1, fill_method=â€™padâ€™, limit=None, freq=None, **kwargs)Docstring:Percent change over given number of periods.Parametersperiods : int, default 1Periods to shift for forming percent changefill_method : str, default â€˜padâ€™How to handle NAs before computing percent changeslimit : int, default NoneThe number of consecutive NAs to fill before stoppingfreq : DateOffset, timedelta, or offset alias string, optionalIncrement to use from time series API (e.g. â€˜Mâ€™ or BDay())Returnschg : NDFrameNotesBy default, the percentage change is calculated along the stataxis: 0, or Index, for DataFrame and 1, or minor forPanel. You can change this with the axis keyword argument.12returns = price.pct_change()returns.tail()AAPLGOOGIBMMSFTDate2017-02-210.0072210.004335-0.002269-0.0020122017-02-220.002999-0.0010820.004937-0.0020162017-02-23-0.0042300.0006860.0027600.0040402017-02-240.000952-0.003236-0.0016510.0000002017-02-270.0019760.000772-0.010753-0.0060351returns.MSFT.corr(returns.IBM)0.49525655865062668 1returns.MSFT.cov(returns.IBM)8.5880535146740545e-05 1returns.corr()AAPLGOOGIBMMSFTAAPL1.0000000.4095230.3813740.388875GOOG0.4095231.0000000.4027810.470781IBM0.3813740.4027811.0000000.495257MSFT0.3888750.4707810.4952571.0000001returns.cov()AAPLGOOGIBMMSFTAAPL0.0002690.0001050.0000750.000092GOOG0.0001050.0002440.0000750.000106IBM0.0000750.0000750.0001440.000086MSFT0.0000920.0001060.0000860.0002091returns.corrwith(returns.IBM)AAPL 0.381374 GOOG 0.402781 IBM 1.000000 MSFT 0.495257 dtype: float64 1returns.corrwith(volume)AAPL -0.074055 GOOG -0.009543 IBM -0.194107 MSFT -0.090724 dtype: float64 å”¯ä¸€å€¼ï¼Œ å€¼è®¡æ•°ä»¥åŠæˆå‘˜èµ„æ ¼1obj = Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])12uniques = obj.unique()uniquesarray([&apos;c&apos;, &apos;a&apos;, &apos;d&apos;, &apos;b&apos;], dtype=object) 1obj.value_counts()c 3 a 3 b 2 d 1 dtype: int64 1pd.value_counts(obj.values, sort=False)a 3 d 1 b 2 c 3 dtype: int64 12mask = obj.isin(['b', 'c'])mask0 True 1 False 2 False 3 False 4 False 5 True 6 True 7 True 8 True dtype: bool 1obj[mask]0 c 5 b 6 b 7 c 8 c dtype: object 1234data = DataFrame(&#123;'Qu1': [1, 3, 4, 3, 4], 'Qu2': [2, 3, 1, 2, 3], 'Qu3': [1, 5, 2, 4, 4]&#125;)dataQu1Qu2Qu30121133524123324443412result = data.apply(pd.value_counts).fillna(0)resultQu1Qu2Qu311.01.01.020.02.01.032.02.00.042.00.02.050.00.01.0å¤„ç†ç¼ºå¤±æ•°æ®12string_data = Series(['aardvark', 'artichoke', np.nan, 'avocado'])string_data0 aardvark 1 artichoke 2 NaN 3 avocado dtype: object 1string_data.isnull()0 False 1 False 2 True 3 False dtype: bool 12string_data[0] = Nonestring_data.isnull()0 True 1 False 2 True 3 False dtype: bool æ»¤é™¤ç¼ºå¤±æ•°æ®123from numpy import nan as NAdata = Series([1, NA, 3.5, NA, 7])data.dropna()0 1.0 2 3.5 4 7.0 dtype: float64 1data[data.notnull()]0 1.0 2 3.5 4 7.0 dtype: float64 1234data = DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])cleaned = data.dropna()data01201.06.53.011.0NaNNaN2NaNNaNNaN3NaN6.53.01cleaned01201.06.53.01data.dropna(how='all')01201.06.53.011.0NaNNaN3NaN6.53.012data[4] = NAdata012401.06.53.0NaN11.0NaNNaNNaN2NaNNaNNaNNaN3NaN6.53.0NaN1data.dropna(axis=1, how='all')01201.06.53.011.0NaNNaN2NaNNaNNaN3NaN6.53.0123df = DataFrame(np.random.randn(7, 3))df.ix[:4, 1] = NA; df.ix[:2, 2] = NAdf0120-0.204708NaNNaN1-0.555730NaNNaN20.092908NaNNaN31.246435NaN-1.29622140.274992NaN1.35291750.886429-2.001637-0.37184361.669025-0.438570-0.5397411df.dropna(thresh=2)01231.246435NaN-1.29622140.274992NaN1.35291750.886429-2.001637-0.37184361.669025-0.438570-0.539741å¡«å……ç¼ºå¤±æ•°æ®1df.fillna(0)0120-0.2047080.0000000.0000001-0.5557300.0000000.00000020.0929080.0000000.00000031.2464350.000000-1.29622140.2749920.0000001.35291750.886429-2.001637-0.37184361.669025-0.438570-0.5397411df.fillna(&#123;1: 0.5, 3: -1&#125;)0120-0.2047080.500000NaN1-0.5557300.500000NaN20.0929080.500000NaN31.2464350.500000-1.29622140.2749920.5000001.35291750.886429-2.001637-0.37184361.669025-0.438570-0.539741fillnaé»˜è®¤è¿”å›žæ–°å¯¹è±¡ï¼Œä½†ä¹Ÿå¯ä»¥å¯¹çŽ°æœ‰å¯¹è±¡å°±åœ°ä¿®æ”¹123# always returns a reference to the filled object_ = df.fillna(0, inplace=True)df0120-0.2047080.0000000.0000001-0.5557300.0000000.00000020.0929080.0000000.00000031.2464350.000000-1.29622140.2749920.0000001.35291750.886429-2.001637-0.37184361.669025-0.438570-0.539741123df = DataFrame(np.random.randn(6, 3))df.ix[2:, 1] = NA; df.ix[4:, 2] = NAdf01200.4769853.248944-1.0212281-0.5770870.1241210.30261420.523772NaN1.3438103-0.713544NaN-2.3702324-1.860761NaNNaN5-1.265934NaNNaN1df.fillna(method='ffill')01200.4769853.248944-1.0212281-0.5770870.1241210.30261420.5237720.1241211.3438103-0.7135440.124121-2.3702324-1.8607610.124121-2.3702325-1.2659340.124121-2.3702321df.fillna(method='ffill', limit=2)01200.4769853.248944-1.0212281-0.5770870.1241210.30261420.5237720.1241211.3438103-0.7135440.124121-2.3702324-1.860761NaN-2.3702325-1.265934NaN-2.37023212data = Series([1., NA, 3.5, NA, 7])data.fillna(data.mean())0 1.000000 1 3.833333 2 3.500000 3 3.833333 4 7.000000 dtype: float64 å±‚æ¬¡ç´¢å¼•1234data = Series(np.random.randn(10), index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd', 'd'], [1, 2, 3, 1, 2, 3, 1, 2, 2, 3]])dataa 1 0.332883 2 -2.359419 3 -0.199543 b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 d 2 -0.753887 3 0.331286 dtype: float64 1data.indexMultiIndex(levels=[[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;], [1, 2, 3]], labels=[[0, 0, 0, 1, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 1, 2]]) 1data['b']1 -1.541996 2 -0.970736 3 -1.307030 dtype: float64 1data['b':'c']b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 dtype: float64 1data.ix[['b', 'd']]b 1 -1.541996 2 -0.970736 3 -1.307030 d 2 -0.753887 3 0.331286 dtype: float64 1data[:, 2]a -2.359419 b -0.970736 c 0.377984 d -0.753887 dtype: float64 1data.unstack()123a0.332883-2.359419-0.199543b-1.541996-0.970736-1.307030c0.2863500.377984NaNdNaN-0.7538870.3312861data.unstack().stack()a 1 0.332883 2 -2.359419 3 -0.199543 b 1 -1.541996 2 -0.970736 3 -1.307030 c 1 0.286350 2 0.377984 d 2 -0.753887 3 0.331286 dtype: float64 12345frame = DataFrame(np.arange(12).reshape((4, 3)), index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]], columns=[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])frameOhioColoradoGreenRedGreena10122345b1678291011123frame.index.names = ['key1', 'key2']frame.columns.names = ['state', 'color']framestateOhioColoradocolorGreenRedGreenkey1key2a10122345b16782910111frame['Ohio']colorGreenRedkey1key2a101234b1672910åˆ›å»ºMultiIndexå¯¹è±¡å¤ç”¨12MultiIndex.from_arrays([[&apos;Ohio&apos;, &apos;Ohio&apos;, &apos;Colorado&apos;], [&apos;Green&apos;, &apos;Red&apos;, &apos;Green&apos;]], names=[&apos;state&apos;, &apos;color&apos;])é‡æŽ’åˆ†çº§é¡ºåº1frame.swaplevel('key1', 'key2')stateOhioColoradocolorGreenRedGreenkey2key11a0122a3451b6782b910111frame.sortlevel(1)stateOhioColoradocolorGreenRedGreenkey1key2a1012b1678a2345b2910111frame.swaplevel(0, 1).sortlevel(0)stateOhioColoradocolorGreenRedGreenkey2key11a012b6782a345b91011æ ¹æ®çº§åˆ«æ±‡æ€»ç»Ÿè®¡1frame.sum(level='key2')stateOhioColoradocolorGreenRedGreenkey21681021214161frame.sum(level='color', axis=1)colorGreenRedkey1key2a121284b114722010ä½¿ç”¨DataFrameçš„åˆ—1234frame = DataFrame(&#123;'a': range(7), 'b': range(7, 0, -1), 'c': ['one', 'one', 'one', 'two', 'two', 'two', 'two'], 'd': [0, 1, 2, 0, 1, 2, 3]&#125;)frameabcd007one0116one1225one2334two0443two1552two2661two312frame2 = frame.set_index(['c', 'd'])frame2abcdone007116225two0341432523611frame.set_index(['c', 'd'], drop=False)abcdcdone007one0116one1225one2two034two0143two1252two2361two31frame2.reset_index()cdab0one0071one1162one2253two0344two1435two2526two361æ‹“å±•è¯é¢˜æ•´æ•°ç´¢å¼•12ser = Series(np.arange(3.))ser.iloc[-1]2.0 1ser0 0.0 1 1.0 2 2.0 dtype: float64 12ser2 = Series(np.arange(3.), index=['a', 'b', 'c'])ser2[-1]2.0 1ser.ix[:1]0 0.0 1 1.0 dtype: float64 12ser3 = Series(range(3), index=[-5, 1, 3])ser3.iloc[2]2 123frame = DataFrame(np.arange(6).reshape((3, 2)), index=[2, 0, 1])frameframe.iloc[0]012010231450 0 1 1 Name: 2, dtype: int32 é¢æ¿æ•°æ®1234import pandas_datareader.data as webpdata = pd.Panel(dict((stk, web.get_data_yahoo(stk)) for stk in ['AAPL', 'GOOG', 'MSFT', 'DELL']))1pdata&lt;class &apos;pandas.core.panel.Panel&apos;&gt; Dimensions: 4 (items) x 1820 (major_axis) x 6 (minor_axis) Items axis: AAPL to MSFT Major_axis axis: 2010-01-04 00:00:00 to 2017-02-27 00:00:00 Minor_axis axis: Open to Adj Close 12pdata = pdata.swapaxes('items', 'minor')pdata['Adj Close'].iloc[:10]AAPLDELLGOOGMSFTDate2010-01-0427.72703914.06528313.06246825.5554852010-01-0527.77497614.38450311.68384425.5637412010-01-0627.33317814.10397303.82668525.4068592010-01-0727.28265014.23940296.75374925.1426342010-01-0827.46403414.36516300.70980825.3160312010-01-1127.22175814.37483300.25525524.9940072010-01-1226.91211014.56830294.94557224.8288662010-01-1327.29172014.57797293.25224325.0600642010-01-1427.13365714.22005294.63086825.5637412010-01-1526.68019813.92985289.71077225.4811721pdata.ix[:, '6/1/2012', :]OpenHighLowCloseVolumeAdj CloseAAPL569.159996572.650009560.520012560.989983130246900.072.681610DELL12.15000012.30000012.04500012.07000019397600.011.675920GOOG571.790972572.650996568.350996570.9810006138700.0285.205295MSFT28.76000028.95999928.44000128.45000156634300.024.9422391pdata.ix['Adj Close', '5/22/2012':, :].iloc[:10]AAPLDELLGOOGMSFTDate2012-05-2272.16078614.58765300.10041226.0907212012-05-2373.92149412.08221304.42610625.5208642012-05-2473.24260712.04351301.52897825.4857952012-05-2572.85003812.05319295.47005025.4770282012-05-28NaN12.05319NaNNaN2012-05-2974.14304112.24666296.87364525.9153802012-05-3075.03700512.14992293.82167425.7225052012-05-3174.85044211.92743290.14035425.5910002012-06-0172.68161011.67592285.20529524.9422392012-06-0473.10915611.60821289.00648025.02990812stacked = pdata.ix[:, '5/30/2012':, :].to_frame()stackedOpenHighLowCloseVolumeAdj CloseDateminor2012-05-30AAPL569.199997579.989990566.559990579.169998132357400.075.037005DELL12.59000012.70000012.46000012.56000019787800.012.149920GOOG588.161028591.901014583.530999588.2309923827600.0293.821674MSFT29.35000029.48000029.12000129.34000041585500.025.7225052012-05-31AAPL580.740021581.499985571.460022577.730019122918600.074.850442DELL12.53000012.54000012.33000012.33000019955600.011.927430GOOG588.720982590.001032579.001013580.8609905958800.0290.140354MSFT29.29999929.42000028.94000129.19000139134000.025.5910002012-06-01AAPL569.159996572.650009560.520012560.989983130246900.072.681610DELL12.15000012.30000012.04500012.07000019397600.011.675920GOOG571.790972572.650996568.350996570.9810006138700.0285.205295MSFT28.76000028.95999928.44000128.45000156634300.024.9422392012-06-04AAPL561.500008567.499985548.499977564.289978139248900.073.109156DELL12.11000012.11250011.80000012.00000017015700.011.608210GOOG570.220958580.491016570.011006578.5909734883500.0289.006480MSFT28.62000128.78000128.32000028.54999947926300.025.0299082012-06-05AAPL561.269989566.470001558.330002562.83002597053600.072.920005DELL11.95000012.24000011.95000012.16000015620900.011.762980GOOG575.451008578.131003566.470986570.4109994697200.0284.920579MSFT28.51000028.75000028.38999928.51000045715400.024.9948412012-06-06AAPL567.770004573.849983565.499992571.460022100363900.074.038104DELL12.21000012.28000012.09000012.21500020779900.011.816190GOOG576.480979581.970971573.611004580.5709664207200.0289.995487MSFT28.87999929.37000128.80999929.35000046860500.025.7312732012-06-07AAPL577.290009577.320023570.500000571.72000194941700.074.071787DELL12.32000012.41000012.12000012.13000020074000.011.733960GOOG587.601014587.891038577.251006578.2309863530100.0288.826666MSFT29.63999929.70000129.17000029.23000037792800.025.6260672012-06-08AAPL571.599998580.580017568.999992580.31998486879100.075.185997DELL12.13000012.22500012.02000012.12000018155600.011.724290â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦2017-02-13AAPL133.080002133.820007132.750000133.28999323035400.0133.289993GOOG816.000000820.958984815.489990819.2399901198100.0819.239990MSFT64.23999864.86000164.12999764.72000122920100.064.3300002017-02-14AAPL133.470001135.089996133.250000135.02000432815500.0135.020004GOOG819.000000823.000000816.000000820.4500121053600.0820.450012MSFT64.41000464.72000164.01999764.57000023065900.064.5700002017-02-15AAPL135.520004136.270004134.619995135.50999535501600.0135.509995GOOG819.359985823.000000818.469971818.9799801304000.0818.979980MSFT64.50000064.57000064.16000464.52999916917000.064.5299992017-02-16AAPL135.669998135.899994134.839996135.35000622118000.0135.350006GOOG819.929993824.400024818.979980824.1599731281700.0824.159973MSFT64.73999865.23999864.44000264.51999720524700.064.5199972017-02-17AAPL135.100006135.830002135.100006135.72000122084500.0135.720001GOOG823.020020828.070007821.655029828.0700071597800.0828.070007MSFT64.47000164.69000264.30000364.62000321234600.064.6200032017-02-21AAPL136.229996136.750000135.979996136.69999724265100.0136.699997GOOG828.659973833.450012828.349976831.6599731247700.0831.659973MSFT64.61000164.94999764.44999764.48999819384900.064.4899982017-02-22AAPL136.429993137.119995136.110001137.11000120745300.0137.110001GOOG828.659973833.250000828.640015830.760010982900.0830.760010MSFT64.33000264.38999964.05000364.36000119259700.064.3600012017-02-23AAPL137.380005137.479996136.300003136.52999920704100.0136.529999GOOG830.119995832.460022822.880005831.3300171470100.0831.330017MSFT64.41999864.73000364.19000264.62000320235200.064.6200032017-02-24AAPL135.910004136.660004135.279999136.66000421690900.0136.660004GOOG827.729980829.000000824.200012828.6400151386600.0828.640015MSFT64.52999964.80000364.13999964.62000321705200.064.6200032017-02-27AAPL137.139999137.440002136.279999136.92999320196400.0136.929993GOOG824.549988830.500000824.000000829.2800291099500.0829.280029MSFT64.54000164.54000164.05000364.23000315850400.064.2300033952 rows Ã— 6 columns1stacked.to_panel()&lt;class &apos;pandas.core.panel.Panel&apos;&gt; Dimensions: 6 (items) x 1207 (major_axis) x 4 (minor_axis) Items axis: Open to Adj Close Major_axis axis: 2012-05-30 00:00:00 to 2017-02-27 00:00:00 Minor_axis axis: AAPL to MSFT]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis-Learning note-Ch04]]></title>
      <url>%2F2017%2F02%2F26%2FPython-data-analysis-Learning-note-Ch04%2F</url>
      <content type="text"><![CDATA[NumpyåŸºç¡€ï¼šæ•°ç»„å’ŒçŸ¢é‡è®¡ç®—1%matplotlib inline1234from __future__ import divisionfrom numpy.random import randnimport numpy as npnp.set_printoptions(precision=4, suppress=True)12from IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"NumPy ndarray: ä¸€ç§å¤šç»´æ•°ç»„å¯¹è±¡1data = randn(2, 3)123datadata * 10data + dataarray([[ 0.1584, 0.299 , -0.2555], [ 0.3277, -0.6934, 1.3191]]) array([[ 1.5842, 2.9896, -2.5545], [ 3.2767, -6.9342, 13.1913]]) array([[ 0.3168, 0.5979, -0.5109], [ 0.6553, -1.3868, 2.6383]]) 12data.shapedata.dtype(2, 3) dtype(&apos;float64&apos;) åˆ›å»ºndarray123data1 = [6, 7.5, 8, 0, 1]arr1 = np.array(data1)arr1array([ 6. , 7.5, 8. , 0. , 1. ]) 12345data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]arr2 = np.array(data2)arr2arr2.ndimarr2.shapearray([[1, 2, 3, 4], [5, 6, 7, 8]]) 2 (2, 4) é™¤éžæ˜¾ç¤ºè¯´æ˜Žï¼Œnp.arrayä¼šå°è¯•ä¸ºæ–°å»ºçš„æ•°ç»„é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ç±»åž‹12arr1.dtypearr2.dtypedtype(&apos;float64&apos;) dtype(&apos;int32&apos;) 123np.zeros(10)np.zeros((3, 6))np.empty((2, 3, 2))array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) array([[ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.], [ 0., 0., 0., 0., 0., 0.]]) array([[[ 0., 0.], [ 0., 0.], [ 0., 0.]], [[ 0., 0.], [ 0., 0.], [ 0., 0.]]]) 1np.arange(15)array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) ones_like, zeros_like, empty_likeè¿™ä¸‰ä¸ªæ–¹æ³•æŽ¥å—ä¸€ä¸ªæ•°ç»„ä¸ºå¯¹è±¡ï¼Œåˆ›å»ºå’Œè¿™ä¸ªæ•°ç»„å½¢çŠ¶å’Œdtypeä¸€æ ·çš„å…¨1ï¼Œ å…¨0å’Œåˆ†é…çš„åˆå§‹ç©ºé—´ndarrayçš„æ•°æ®ç±»åž‹1234arr1 = np.array([1, 2, 3], dtype=np.float64)arr2 = np.array([1, 2, 3], dtype=np.int32)arr1.dtypearr2.dtypedtype(&apos;float64&apos;) dtype(&apos;int32&apos;) å½“éœ€è¦æŽ§åˆ¶æ•°æ®åœ¨å†…å­˜å’Œç£ç›˜ä¸­çš„å­˜å‚¨æ–¹å¼æ—¶ï¼ˆå°¤å…¶æ˜¯å¯¹å¤§æ•°æ®é›†ï¼‰ï¼Œé‚£å°±å¾—äº†è§£å¦‚ä½•æŽ§åˆ¶å­˜å‚¨ç±»åž‹1234arr = np.array([1, 2, 3, 4, 5])arr.dtypefloat_arr = arr.astype(np.float64) float_arr.dtypedtype(&apos;int32&apos;) dtype(&apos;float64&apos;) 123arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])arrarr.astype(np.int32)array([ 3.7, -1.2, -2.6, 0.5, 12.9, 10.1]) array([ 3, -1, -2, 0, 12, 10]) 12numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_)numeric_strings.astype(float)array([ 1.25, -9.6 , 42. ]) 123int_array = np.arange(10)calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)int_array.astype(calibers.dtype)array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) 12empty_uint32 = np.empty(8, dtype='u4')empty_uint32array([1, 2, 3, 4, 5, 6, 7, 8], dtype=uint32) è°ƒç”¨astypeä¼šåˆ›å»ºåŽŸæ•°ç»„çš„ä¸€ä»½æ‹·è´æ•°ç»„å’Œæ ‡é‡ä¹‹é—´çš„è¿ç®—1234arr = np.array([[1., 2., 3.], [4., 5., 6.]])arrarr * arrarr - arrarray([[ 1., 2., 3.], [ 4., 5., 6.]]) array([[ 1., 4., 9.], [ 16., 25., 36.]]) array([[ 0., 0., 0.], [ 0., 0., 0.]]) 121 / arrarr ** 0.5array([[ 1. , 0.5 , 0.3333], [ 0.25 , 0.2 , 0.1667]]) array([[ 1. , 1.4142, 1.7321], [ 2. , 2.2361, 2.4495]]) åŸºæœ¬çš„ç´¢å¼•å’Œåˆ‡ç‰‡123456arr = np.arange(10)arrarr[5]arr[5:8]arr[5:8] = 12arrarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 5 array([5, 6, 7]) array([ 0, 1, 2, 3, 4, 12, 12, 12, 8, 9]) åˆ‡ç‰‡ç›´æŽ¥åœ¨åŽŸæ•°ç»„ä¸Šæ“ä½œå¦‚æžœæƒ³è¦å¾—åˆ°ä¸€ä¸ªå¤åˆ¶çš„ç‰ˆæœ¬ï¼Œéœ€è¦æ˜¾ç¤ºåœ°è°ƒç”¨copy()æ–¹æ³•12345arr_slice = arr[5:8]arr_slice[1] = 12345arrarr_slice[:] = 64arrarray([ 0, 1, 2, 3, 4, 12, 12345, 12, 8, 9]) array([ 0, 1, 2, 3, 4, 64, 64, 64, 8, 9]) 12arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])arr2d[2]array([7, 8, 9]) æ³¨æ„ä¸‹é¢è¿™ç§ç´¢å¼•æ–¹å¼12arr2d[0][2]arr2d[0, 2]3 3 12arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])arr3darray([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[0]array([[1, 2, 3], [4, 5, 6]]) 12345old_values = arr3d[0].copy()arr3d[0] = 42arr3darr3d[0] = old_valuesarr3darray([[[42, 42, 42], [42, 42, 42]], [[ 7, 8, 9], [10, 11, 12]]]) array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 1arr3d[1, 0]array([7, 8, 9]) åˆ‡ç‰‡ç´¢å¼•1arr[1:6]array([ 1, 2, 3, 4, 64]) 12arr2darr2d[:2]array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) array([[1, 2, 3], [4, 5, 6]]) 1arr2d[:2, 1:]array([[2, 3], [5, 6]]) 12arr2d[1, :2]arr2d[2, :1]array([4, 5]) array([7]) 1arr2d[:, :1]array([[1], [4], [7]]) 1arr2d[:2, 1:] = 0å¸ƒå°”åž‹ç´¢å¼•1234names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])data = randn(7, 4)namesdataarray([&apos;Bob&apos;, &apos;Joe&apos;, &apos;Will&apos;, &apos;Bob&apos;, &apos;Will&apos;, &apos;Joe&apos;, &apos;Joe&apos;], dtype=&apos;&lt;U4&apos;) array([[-2.9033, 1.4721, 0.9512, 1.7727], [ 2.2303, -1.0259, 1.0664, 0.534 ], [-0.9725, 0.2226, -0.1538, -0.4994], [-1.4289, 0.1665, -1.2874, -1.0817], [ 1.3581, -1.0734, -0.1387, 0.1673], [ 1.2816, 1.8883, 0.5699, -0.5843], [-0.0464, -0.9633, 0.2855, -0.6473]]) 1names == 'Bob'array([ True, False, False, True, False, False, False], dtype=bool) 1data[names == 'Bob']array([[-2.9033, 1.4721, 0.9512, 1.7727], [-1.4289, 0.1665, -1.2874, -1.0817]]) 12data[names == 'Bob', 2:]data[names == 'Bob', 3]array([[ 0.9512, 1.7727], [-1.2874, -1.0817]]) array([ 1.7727, -1.0817]) 12names != 'Bob'data[~(names == 'Bob')]array([False, True, True, False, True, True, True], dtype=bool) array([[ 2.2303, -1.0259, 1.0664, 0.534 ], [-0.9725, 0.2226, -0.1538, -0.4994], [ 1.3581, -1.0734, -0.1387, 0.1673], [ 1.2816, 1.8883, 0.5699, -0.5843], [-0.0464, -0.9633, 0.2855, -0.6473]]) 123mask = (names == 'Bob') | (names == 'Will')maskdata[mask]array([ True, False, True, True, True, False, False], dtype=bool) array([[-2.9033, 1.4721, 0.9512, 1.7727], [-0.9725, 0.2226, -0.1538, -0.4994], [-1.4289, 0.1665, -1.2874, -1.0817], [ 1.3581, -1.0734, -0.1387, 0.1673]]) Pythonå…³é”®å­—andå’Œoråœ¨å¸ƒå°”åž‹æ•°ç»„ä¸­æ— æ•ˆ12data[data &lt; 0] = 0dataarray([[ 0. , 1.4721, 0.9512, 1.7727], [ 2.2303, 0. , 1.0664, 0.534 ], [ 0. , 0.2226, 0. , 0. ], [ 0. , 0.1665, 0. , 0. ], [ 1.3581, 0. , 0. , 0.1673], [ 1.2816, 1.8883, 0.5699, 0. ], [ 0. , 0. , 0.2855, 0. ]]) 12data[names != 'Joe'] = 7dataarray([[ 7. , 7. , 7. , 7. ], [ 2.2303, 0. , 1.0664, 0.534 ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 7. , 7. , 7. , 7. ], [ 1.2816, 1.8883, 0.5699, 0. ], [ 0. , 0. , 0.2855, 0. ]]) èŠ±å¼ç´¢å¼•èŠ±å¼ç´¢å¼•åˆ›å»ºæ–°çš„æ•°ç»„1234arr = np.empty((8, 4))for i in range(8): arr[i] = iarrarray([[ 0., 0., 0., 0.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], [ 3., 3., 3., 3.], [ 4., 4., 4., 4.], [ 5., 5., 5., 5.], [ 6., 6., 6., 6.], [ 7., 7., 7., 7.]]) 1arr[[4, 3, 0, 6]]array([[ 4., 4., 4., 4.], [ 3., 3., 3., 3.], [ 0., 0., 0., 0.], [ 6., 6., 6., 6.]]) 1arr[[-3, -5, -7]]array([[ 5., 5., 5., 5.], [ 3., 3., 3., 3.], [ 1., 1., 1., 1.]]) 1234# more on reshape in Chapter 12arr = np.arange(32).reshape((8, 4))arrarr[[1, 5, 7, 2], [0, 3, 1, 2]]array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]) array([ 4, 23, 29, 10]) æ ¹æ®ä»¥ä¸Šå¯çŸ¥ï¼Œä¼ å…¥ä¸¤ä¸ªç´¢å¼•æ•°ç»„ç›¸å½“äºŽè¿›è¡Œäº†åŒä½ç½®ç»„åˆæ³¨æ„ä»¥ä¸‹è¿™ç§æ–¹å¼1arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) np.ix_æ–¹æ³•å°†ä¸¤ä¸ªä¸€ç»´æ•°ç»„è½¬æ¢ä¸ºä¸€ä¸ªçŸ©å½¢åŒºåŸŸçš„ç´¢å¼•é€‰æ‹©å™¨1arr[np.ix_([1, 5, 7, 2], [0, 3, 1, 2])]array([[ 4, 7, 5, 6], [20, 23, 21, 22], [28, 31, 29, 30], [ 8, 11, 9, 10]]) æ•°ç»„è½¬ç½®å’Œè½´å¯¹æ¢123arr = np.arange(15).reshape((3, 5))arrarr.Tarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]]) 12arr = np.random.randn(6, 3)np.dot(arr.T, arr)array([[ 3.6804, 0.0133, 1.0388], [ 0.0133, 1.6074, 0.1836], [ 1.0388, 0.1836, 3.5281]]) 123arr = np.arange(16).reshape((2, 2, 4))arrarr.transpose((1, 0, 2))array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]]) Refered from here.In short: transposing an array means that NumPy just needs to permute the stride and shape information for each axis:&gt;&gt;&gt; arr.strides (64, 32, 8) &gt;&gt;&gt; arr.transpose(1, 0, 2).strides (32, 64, 8) Notice that the strides for the first and second axes were swapped here. This means that no data needs to be copied; NumPy can simply change how it looks at the memory to construct the array.What are strides?The values in a 3D array arr are stored in a contiguous block of memory like this:[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] In the case of arr, each integer takes up 8 bytes of memory (i.e. weâ€™re using the int64 dtype).A stride tells NumPy how many bytes to skip in order to move to the next value along an axis. For example, to get the next value in a row in arr (axis 2), we just need to move 8 bytes (1 number).The strides for arr.transpose(1, 0, 2) are (32, 64, 8). To move along the first axis, instead of 64 bytes (8 numbers) NumPy will now only skip 32 bytes (4 numbers) each time:[[[0 ...] [... ...]] [[4 ...] [... ...]]] Similarly, NumPy will now skip 64 bytes (8 numbers) in order to move along axis 1:[[[0 ...] [8 ...]] [[4 ...] [12 ...]]] The actual code that does the transposing is written in C and can be found here.ä¹Ÿå¯ä»¥ä½¿ç”¨swapaxesæ–¹æ³•123arrarr.swapaxes(1, 2)arrarray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) array([[[ 0, 4], [ 1, 5], [ 2, 6], [ 3, 7]], [[ 8, 12], [ 9, 13], [10, 14], [11, 15]]]) array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]]) é€šç”¨å‡½æ•°ï¼šå¿«é€Ÿçš„å…ƒç´ çº§æ•°ç»„å‡½æ•°123arr = np.arange(10)np.sqrt(arr)np.exp(arr)array([ 0. , 1. , 1.4142, 1.7321, 2. , 2.2361, 2.4495, 2.6458, 2.8284, 3. ]) array([ 1. , 2.7183, 7.3891, 20.0855, 54.5982, 148.4132, 403.4288, 1096.6332, 2980.958 , 8103.0839]) 12345x = randn(8)y = randn(8)xynp.maximum(x, y) # å¯¹åº”å…ƒç´ è¿›è¡Œæ¯”è¾ƒarray([ 0.811 , -0.0214, -0.3702, -0.4856, 1.1449, -0.4246, 0.9396, 0.0382]) array([-1.223 , 0.3271, -1.7197, -2.2636, -0.1154, -1.4122, -0.0989, 0.4477]) array([ 0.811 , 0.3271, -0.3702, -0.4856, 1.1449, -0.4246, 0.9396, 0.4477]) modfå‡½æ•°æŒºæœ‰æ„æ€123arr = randn(7) * 5arrnp.modf(arr)array([ 10.3171, -4.733 , -6.3358, 3.2457, -7.3823, 2.7036, -2.6173]) (array([ 0.3171, -0.733 , -0.3358, 0.2457, -0.3823, 0.7036, -0.6173]), array([ 10., -4., -6., 3., -7., 2., -2.])) åˆ©ç”¨æ•°ç»„è¿›è¡Œæ•°æ®å¤„ç†meshgridäº§ç”Ÿä¸¤ä¸ªäºŒç»´æ•°ç»„ï¼Œå¯¹åº”pointsä¸­æ‰€æœ‰çš„äºŒå…ƒç»„1234points = np.arange(-5, 5, 0.01) # 1000 equally spaced pointsxs, ys = np.meshgrid(points, points)xsysarray([[-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], ..., [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99]]) array([[-5. , -5. , -5. , ..., -5. , -5. , -5. ], [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99], [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98], ..., [ 4.97, 4.97, 4.97, ..., 4.97, 4.97, 4.97], [ 4.98, 4.98, 4.98, ..., 4.98, 4.98, 4.98], [ 4.99, 4.99, 4.99, ..., 4.99, 4.99, 4.99]]) 1from matplotlib.pyplot import imshow, title12345import matplotlib.pyplot as pltz = np.sqrt(xs ** 2 + ys ** 2)zplt.imshow(z, cmap=plt.cm.gray); plt.colorbar()plt.title("Image plot of $\sqrt&#123;x^2 + y^2&#125;$ for a grid of values")array([[ 7.0711, 7.064 , 7.0569, ..., 7.0499, 7.0569, 7.064 ], [ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569], [ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499], ..., [ 7.0499, 7.0428, 7.0357, ..., 7.0286, 7.0357, 7.0428], [ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499], [ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569]]) &lt;matplotlib.image.AxesImage at 0x23400a22b38&gt; &lt;matplotlib.colorbar.Colorbar at 0x23400a7c7b8&gt; &lt;matplotlib.text.Text at 0x23400a03da0&gt; 1plt.draw()&lt;matplotlib.figure.Figure at 0x23401396eb8&gt; å°†æ¡ä»¶é€»è¾‘è¡¨è¿°ä¸ºæ•°ç»„è¿ç®—123xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])cond = np.array([True, False, True, True, False])æ³¨æ„ä¸‹é¢åˆ—è¡¨ç”Ÿæˆå¼çš„å†™æ³•123result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]result[1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5] ä¸Šè¿°æ–¹æ³•å…·æœ‰ä¸€äº›ç¼ºç‚¹ï¼šå¤§æ•°ç»„å¤„ç†é€Ÿåº¦æ…¢ï¼ˆçº¯Pythonå®žçŽ°ï¼‰æ— æ³•å¤„ç†å¤šç»´æ•°ç»„æ‰€ä»¥å¯ä»¥ä½¿ç”¨ä¸‹é¢è¿™ç§æ–¹æ³•ï¼š12result = np.where(cond, xarr, yarr)resultarray([ 1.1, 2.2, 1.3, 1.4, 2.5]) 1234arr = randn(4, 4)arrnp.where(arr &gt; 0, 2, -2)np.where(arr &gt; 0, 2, arr) # set only positive values to 2array([[-0.7355, -0.3188, -0.2358, 0.3137], [-0.6196, -0.5803, -0.5504, -1.1508], [ 0.1719, -1.1599, -0.7115, 1.7869], [-0.2306, 0.2068, 1.5366, 1.6154]]) array([[-2, -2, -2, 2], [-2, -2, -2, -2], [ 2, -2, -2, 2], [-2, 2, 2, 2]]) array([[-0.7355, -0.3188, -0.2358, 2. ], [-0.6196, -0.5803, -0.5504, -1.1508], [ 2. , -1.1599, -0.7115, 2. ], [-0.2306, 2. , 2. , 2. ]]) æ˜¾ç„¶whereè¿˜å¯ä»¥åº”ç”¨äºŽæ›´å¤æ‚çš„æ“ä½œã€‚è€ƒè™‘ä¸‹é¢è¿™ç§é€»è¾‘ï¼š12345678910result = []for i in range(n): if cond1[i] and cond2[i]: result.append(0) elif cond1[i]: result.append(1) elif cond2[i]: result.append(2) else: result.append(3)ç”¨whereå¯ä»¥è¿™æ ·å®žçŽ°ï¼š123np.where(cond1 &amp; cond2, 0, np.where(cond1, 1, np.where(cond2, 2, 3)))æ›´åŠ magicä¸€ç‚¹ï¼š1result = 1 * cond1 + 2 * cond2 + 3 * -(cond1 | cond2)æ•°å­¦å’Œç»Ÿè®¡æ–¹æ³•123456arr = np.random.randn(5, 4) # æ­£æ€åˆ†å¸ƒarr# ä¸‹é¢ä¸¤ç§æ–¹å¼éƒ½å¯ä»¥ä½¿ç”¨arr.mean()np.mean(arr)arr.sum()array([[ 1.4513, -0.8225, 0.7011, -0.617 ], [ 1.5872, 1.2937, 1.0151, 0.7123], [-0.2012, -0.0168, -0.3847, 0.5274], [-0.6312, -0.2762, 0.4869, 0.0462], [-0.5268, -1.1071, 1.8642, 0.2282]]) 0.26650934393195791 0.26650934393195791 5.3301868786391582 12arr.mean(axis=1)arr.sum(0) # axis=0array([ 0.1782, 1.1521, -0.0188, -0.0936, 0.1146]) array([ 1.6793, -0.9289, 3.6826, 0.8971]) 1234arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])arrarr.cumsum(0) # axis=0arr.cumprod(1) # axis=1array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) array([[ 0, 1, 2], [ 3, 5, 7], [ 9, 12, 15]], dtype=int32) array([[ 0, 0, 0], [ 3, 12, 60], [ 6, 42, 336]], dtype=int32) ç”¨äºŽå¸ƒå°”æ•°ç»„çš„æ–¹æ³•123arr = randn(100)arr(arr &gt; 0).sum() # Number of positive valuesarray([ 0.7828, 0.1372, -0.6264, 1.8927, -0.2104, 0.2822, -0.3672, -0.3601, 0.5918, 0.9285, 0.1808, -0.4021, 0.4086, -0.2949, 0.5633, -0.7462, -0.1635, 0.1482, -0.3226, -1.2127, -0.9821, 0.0536, -0.1772, -0.4714, -0.9002, -0.0037, 0.7352, 0.5675, -1.1612, 0.5288, 0.3319, 0.7315, 0.6841, -0.6881, 1.5654, -0.4605, -0.5423, 0.0184, -0.8153, -0.1313, 0.4594, 0.0228, 0.255 , -2.2361, 0.8703, -1.5153, -0.9458, 0.2769, 0.9986, 0.7699, -0.7948, -1.2508, 1.7059, 0.1805, -1.0265, -0.0181, -0.9415, 0.1265, -0.2576, 0.6791, 0.3969, 0.8027, -0.6792, -0.7487, -1.9949, -0.9595, 0.5706, -0.5727, -1.0204, 0.1521, -0.9755, -0.4094, 0.67 , 0.212 , 0.4081, -0.1435, 0.3964, -0.1865, -0.6018, -2.6185, -0.5073, -0.6328, -0.2631, 0.6637, -0.5586, 1.3346, -0.5317, 0.8572, 1.1159, 0.9563, -0.0434, -1.0534, 0.5869, 0.0502, -0.0479, -0.8673, 0.1531, 1.0646, -0.2624, -0.3726]) 47 1234bools = np.array([False, False, True, False])boolsbools.any()bools.all()array([False, False, True, False], dtype=bool) True False æŽ’åº1234arr = randn(8)arrarr.sort()arrarray([ 1.0584, 1.9062, -0.2923, 0.7169, 0.5186, -0.6089, -2.0444, -0.5661]) array([-2.0444, -0.6089, -0.5661, -0.2923, 0.5186, 0.7169, 1.0584, 1.9062]) 1234arr = randn(5, 3)arrarr.sort(1) # axis=1arrarray([[ 0.0118, -2.8916, -0.4477], [-1.9768, 1.859 , -1.128 ], [-2.6262, 0.5791, 0.7594], [-0.5254, -0.9059, 0.0203], [-1.4029, -1.8566, 0.1892]]) array([[-2.8916, -0.4477, 0.0118], [-1.9768, -1.128 , 1.859 ], [-2.6262, 0.5791, 0.7594], [-0.9059, -0.5254, 0.0203], [-1.8566, -1.4029, 0.1892]]) 12345large_arr = randn(1000)large_arrlarge_arr.sort()large_arrlarge_arr[int(0.05 * len(large_arr))] # 5% quantilearray([ 1.2296, 0.3794, -0.1526, 2.1223, -0.0675, 0.6867, -0.5742, -1.4291, 0.6856, 0.1364, -0.3966, -0.7793, 0.4965, 0.2447, -0.7487, 0.7695, 0.5358, -0.4813, 0.9949, -0.6489, -0.3656, 1.9551, 0.8327, 1.497 , -0.4431, -0.8357, -0.821 , -0.7348, 1.9294, -0.3144, 0.1396, -0.9111, 0.0943, 0.8043, 1.067 , 0.9362, -2.2574, 0.7475, -1.0152, -1.1234, -0.3774, 1.076 , 0.8743, 1.1864, 0.0801, 0.3995, 0.2536, -0.9371, -1.669 , -2.2444, 1.2544, 1.0539, -0.7579, 0.2963, 0.7496, -1.3655, 0.1552, -0.6259, 0.2621, -1.5415, -0.1036, -0.5794, 1.2098, 1.3388, 0.3159, 1.0998, 0.5109, -0.3927, 1.4797, -1.4891, 0.3624, 0.966 , 0.0756, -0.4703, 0.1859, 1.6091, 0.662 , -0.4808, 0.8744, 0.4738, 1.1351, 0.0251, -1.017 , -0.849 , -0.1602, -1.5392, 0.0601, 1.7323, 1.1837, 0.4657, 0.8858, -0.211 , 0.1865, 0.673 , 0.3086, -1.2527, -0.7802, 0.407 , -1.118 , -0.2058, 0.7921, 0.5284, -2.3038, -0.4038, -1.1087, -0.827 , -2.6518, 0.3711, -0.0244, 1.1103, 0.2748, -0.7962, 1.9456, 0.5347, 0.1862, -0.3734, -0.3036, 0.6831, -0.9419, 1.4848, -0.1247, -0.4138, -0.601 , 0.6138, 1.1334, 0.4386, 0.0466, -0.0588, 0.6883, -1.2912, -0.2381, 0.3934, 0.2132, -0.4143, 1.0844, -0.5258, -0.9944, 1.0977, 0.3528, 1.9928, 1.421 , 0.8634, 0.1973, -1.1799, -2.9433, 2.697 , 0.4778, 0.6464, 0.049 , -0.2339, 1.6945, -0.6568, -0.5972, -0.8324, -0.6443, 0.0882, -0.3686, 0.0419, 0.5119, -0.641 , 1.1545, 1.0735, -0.5329, -0.1126, 0.0375, -1.0699, -1.3153, -1.6097, 2.5671, -0.9516, -0.388 , -0.0129, -0.0171, -1.0763, -0.7125, 0.767 , 0.2254, -0.7638, -0.2065, 1.2797, 0.0784, -0.7762, 1.7106, -0.0136, -0.4435, 1.2946, -2.5489, 0.4241, 0.5675, -0.7596, 0.6128, 1.1161, -1.2456, -0.131 , -0.2684, 1.6461, -0.2497, -0.4294, 1.122 , 0.5969, 0.3335, -0.0453, 1.1567, 0.0216, -0.7277, -2.5465, -2.4542, -1.5895, 0.4607, -0.8303, 0.0263, 0.0301, -1.2365, -0.146 , -0.8632, 0.6449, 0.1958, -0.6914, -0.3223, 0.4037, 0.9918, -0.3542, 0.8442, 0.7751, -1.6248, 2.6081, 0.3524, 1.5298, 0.4421, 1.5228, -1.5263, -1.3994, 0.0285, -0.5389, 1.4047, -2.1117, -1.0397, 0.6495, 0.9073, 1.8738, 0.2913, -1.069 , -0.7835, -0.6437, 0.6739, 0.3272, -0.8483, -0.2971, 0.2882, 0.1778, -0.6705, -1.4129, -0.1935, 0.6615, -0.4423, -1.2472, -0.9816, 0.927 , -2.2774, 0.5736, 1.3996, 1.1653, -0.3253, -0.2074, -0.2447, 0.4925, 1.8415, -1.1551, -0.5131, -0.6407, 0.5033, -0.817 , 0.0479, -0.9106, 1.4391, -1.5824, -0.4652, 1.253 , -0.6051, 0.6699, 0.3803, 1.0767, 1.5449, 0.106 , -0.7215, -0.354 , 0.1016, -1.3191, -0.6596, -0.9632, -0.3655, 0.8411, -0.2314, 1.9493, -0.6966, -1.2598, 0.4023, 0.1704, -0.452 , 1.5924, 0.381 , -0.4731, -1.2467, -0.4264, -0.2298, -0.1792, -0.5009, -1.0032, 1.0126, 0.5436, 1.1366, -1.0318, 1.3289, 0.3218, -0.2828, 0.5597, -0.0213, -0.078 , 0.7667, -0.3984, -1.0263, -0.5557, -2.0724, -0.9343, -0.6877, 1.0567, -0.605 , 1.7923, 0.6351, -1.769 , 0.4175, 0.8266, 0.3767, -0.1508, -0.4301, -0.3397, 0.7248, 0.188 , 1.1632, -1.0831, -0.5726, -0.475 , 0.092 , -0.1566, 1.9074, -1.4261, 1.8589, -0.7534, -1.0767, -0.2704, 0.7567, 0.5903, -1.5612, -1.1097, 0.3504, -0.9086, -0.1691, 0.6714, -0.6033, 1.8315, -0.8141, 0.5968, -0.408 , -1.1843, 0.5146, 0.6201, 0.4293, 0.9797, 1.066 , -1.3325, -1.733 , 0.8545, 0.3993, -0.2041, -0.4624, 0.0272, -0.005 , 0.9237, -0.5523, 0.9975, -0.4374, 0.1351, -0.6148, 0.3185, 0.0572, -0.3002, 0.0889, -0.0894, -0.5617, -2.0553, -0.2923, 0.7227, 0.604 , -0.6623, -0.6126, -0.4991, 0.0923, -0.6982, 0.2099, -0.6853, -0.4752, -1.625 , 0.0443, 2.5507, -1.1597, 0.3504, -0.7654, -1.4366, -1.3755, 0.3702, -1.7853, -0.7326, -1.2803, -0.6089, -0.4472, 0.462 , 0.7799, 0.3141, 0.8064, -1.0487, 0.7317, -0.2446, 0.3061, 0.1384, -0.572 , -0.0311, 0.3572, -0.6371, -0.2236, 0.0806, 0.6648, -0.148 , -0.2547, 1.3649, -0.1595, 1.3632, -0.8858, 1.1801, 0.5533, 2.3306, 0.2724, 0.7073, -0.5605, -0.8849, 0.9533, 0.3683, -0.2901, -0.0453, 0.1064, 1.3342, -0.7036, 0.7127, 1.2156, 0.9017, 1.2378, -1.1017, 1.0558, 1.4273, 0.7003, 1.1649, 0.0334, 0.3433, -0.3997, -0.1195, 1.3725, -0.3746, 0.8444, 0.961 , -0.2644, 0.3245, -1.3583, 0.387 , 1.2944, 0.0274, -0.5057, 0.15 , 0.6 , -0.5752, 0.3746, 1.7114, -0.0026, -0.1221, -0.8084, -0.9521, -0.6332, 0.7254, 1.7032, -0.0879, 0.3329, -1.9525, -0.7083, -0.4113, 1.163 , 0.9018, -0.3667, 0.8419, 0.4417, 0.2904, 0.1666, 1.3722, -0.4455, -1.4876, 0.4103, 2.3672, 0.3569, -0.8546, 0.5152, 0.9623, 1.1777, 1.6789, -1.7793, -0.7797, -1.0923, 0.07 , -0.8974, -0.3151, -0.3675, -1.9851, -2.3352, 0.3566, 1.1929, 1.5275, 1.4349, -1.4742, -0.1913, 1.5874, -0.7264, -0.5594, 0.3166, -0.9377, -0.6452, 0.394 , -0.2238, -1.1239, -0.0324, 1.3866, -0.6174, -0.1301, -0.0328, -0.92 , 1.8067, 0.2576, -0.5248, 0.4114, 0.1655, -0.1674, 0.2743, 0.0835, -0.145 , 1.1658, 1.2624, 0.0404, 2.0929, 0.6047, 1.0317, -0.4956, -1.5666, 1.1729, 0.484 , 0.955 , 1.0546, 0.0106, 0.5062, 0.3211, 0.8503, 0.4706, 1.9953, -0.9362, 0.6326, -0.3154, 1.4987, -0.1695, 1.0906, -0.686 , 0.2501, -0.316 , 0.3032, 0.4873, 0.6402, -0.1209, -0.1857, -0.3707, -0.3082, -0.4769, -0.858 , -0.1521, -0.3403, -0.9853, -0.5049, 0.3338, -0.3197, -0.5789, -0.7124, -0.8867, -0.0228, -1.5519, 1.8517, 0.5229, 0.7613, -0.5586, 0.4827, -1.3011, -0.5284, -0.3806, -0.7719, 1.6304, 0.0375, -0.9122, -0.1006, 0.382 , 0.0969, 1.7784, 0.1831, -1.8866, 0.2996, 0.4778, -0.2491, -1.6537, 0.022 , -0.101 , 0.5912, -0.2249, -1.1422, -0.6436, -1.4096, -0.7446, 0.8055, 1.0727, 0.2426, -0.8079, -1.4692, 0.062 , -0.4466, 0.3786, -2.0461, 0.7238, -1.6195, 1.4005, 0.4881, -0.8161, -0.582 , 0.3456, 1.2922, 0.2469, 1.9035, 0.9072, -0.0729, -0.9424, -1.1129, 0.8922, -0.5628, 1.6215, -0.7022, 0.8395, -0.3423, 0.6048, -0.248 , 0.7411, 0.3546, 0.6176, -0.8221, -0.338 , -2.1051, -1.0049, -0.0659, 0.0917, -0.6661, -0.5234, 0.9574, -0.6316, -0.0047, -0.4773, 0.1562, -0.116 , -1.6255, -0.9108, -1.4767, -0.7765, -1.7101, 0.0557, 0.8112, 0.7382, 1.8806, 0.9239, 1.8638, 0.8426, 0.0359, 0.2743, 1.9204, 1.2223, 0.4575, -0.3408, 0.3727, 0.5036, 0.5392, -1.3331, -0.4008, -0.1341, -1.5197, 0.1923, 0.2128, 1.1533, -1.4284, -0.7483, -0.4092, 1.2843, -0.4489, -0.6624, 0.9255, -0.0895, 0.3199, -0.2564, -0.1166, -1.4701, 1.1799, -1.6238, 0.0508, 0.2312, 0.7322, -1.3623, -0.232 , -0.2206, 0.566 , 1.2411, -1.1563, 1.1777, -1.1481, -0.6716, 0.4596, -0.2422, -0.8654, 0.4441, 0.1869, 1.4626, 0.7621, 0.4249, 0.252 , 0.632 , 0.5626, -0.7925, 1.1995, 1.5665, 0.6096, 0.4821, -0.7324, -0.7624, 1.858 , -0.8434, -0.4408, 0.2011, 0.7552, -0.8955, -1.3255, 0.7022, 0.1507, 0.662 , -1.2229, 0.5199, 0.9837, -0.3947, -0.5262, -1.0424, -1.4582, 0.5126, -0.3606, 0.4427, -2.3922, 1.2784, -0.8382, -0.0198, 1.2136, -0.4212, -0.7798, -1.3387, -0.7141, 0.9581, -0.8575, -0.2255, 0.8436, -2.2162, 0.0742, 0.9683, -0.3633, -0.0227, -1.2176, 1.1482, -0.6697, 0.9643, -1.2802, -0.3651, -1.29 , 0.851 , 1.0167, 1.0011, -1.3014, -0.7205, 1.3621, -0.692 , 1.0637, 0.5637, 0.0851, 2.1514, -0.272 , 0.3136, 0.2179, 0.7035, -1.3028, -0.1032, 0.0611, 1.2002, -0.7346, 0.9991, -0.3747, 0.7908, -0.9573, -0.5114, -0.8607, -0.6711, 1.3335, -0.6671, -0.1687, 0.4601, 0.5747, -0.0767, -0.8428, 0.3372, -1.7756, -2.5264, -1.503 , -0.5669, 0.0167, -1.961 , 0.8861, 1.1902, 2.239 , 0.2481, 0.7361, -1.1103, 0.8368, -1.0434, 0.6809, -0.0839, -0.6972, -1.5492, -1.4129, 0.5889, 0.2138, 1.7689, -0.4861, -0.1124, 0.2032, 1.0664, -0.369 , 2.3793, 0.4406, -1.1741, 1.0812, 1.3965, -0.149 , 0.8793, 1.3494, 1.2159, -0.0001, 1.1929, -0.1966, -0.1666, 1.7097, -0.4273, 0.4831, -0.2411, -1.4517, -0.7317, 0.099 , 1.7922, 0.2313, -0.5031, -0.0849, 0.7331, -0.1483, -0.8003, 1.1897, 0.031 , -0.3624, -1.1133, 1.4647, 2.5653, -1.9536, -0.4528, -1.693 , 0.4847, 0.1368, 0.6859, -0.9872, 0.8425, -0.1492, -0.1335, -0.0229, -0.0903, -0.4381, 1.2552, 1.5763, 0.2375, -0.7597, 0.0845, 0.0894, -1.6022, -0.1988, 0.3095, -1.0785, -1.6044, -0.4922, 0.4583, 0.3168, -2.0485, -1.2147, -0.2803, -0.2071, 0.0767, 1.9544, -1.7648, 0.2873, -0.4029, -0.8128, -0.1081, 0.0332, 2.5288, 0.9933, 0.4378, -0.8208, -0.2451, 0.3472, -0.2917, 2.0775, 1.7381, -0.467 , -0.8943, -1.4171, -0.3905, 0.2591, 0.8118, -0.643 , 1.0387, 0.0049, 1.7299, -0.6882, -1.4132, -1.0893, 0.4606, -1.546 , -2.87 , 0.3492, -1.5968, 0.9858, 0.1384, -0.6016, -0.9632, -0.9088, 0.3711, 1.3509, 0.4601, -1.4963, -0.043 , 0.5588, 0.2638, -1.1118, -0.9376, -0.9139, 0.6551, 0.4876, -1.7039, -0.2915, 0.3867, -0.1795, 1.2298, 0.0893, -0.6019, 1.4109, -1.1918, 0.5009, 0.0157, -1.1307, 1.0407, 1.9742, -1.0377, -0.6151, -0.8398, 1.4096, -0.012 , -1.5323, 0.3323, 0.0539, 0.2383, -0.4059, 2.285 , 0.1536, 0.3838, 0.3623, -0.4326, -0.0975, -1.8119]) array([-2.9433, -2.87 , -2.6518, -2.5489, -2.5465, -2.5264, -2.4542, -2.3922, -2.3352, -2.3038, -2.2774, -2.2574, -2.2444, -2.2162, -2.1117, -2.1051, -2.0724, -2.0553, -2.0485, -2.0461, -1.9851, -1.961 , -1.9536, -1.9525, -1.8866, -1.8119, -1.7853, -1.7793, -1.7756, -1.769 , -1.7648, -1.733 , -1.7101, -1.7039, -1.693 , -1.669 , -1.6537, -1.6255, -1.625 , -1.6248, -1.6238, -1.6195, -1.6097, -1.6044, -1.6022, -1.5968, -1.5895, -1.5824, -1.5666, -1.5612, -1.5519, -1.5492, -1.546 , -1.5415, -1.5392, -1.5323, -1.5263, -1.5197, -1.503 , -1.4963, -1.4891, -1.4876, -1.4767, -1.4742, -1.4701, -1.4692, -1.4582, -1.4517, -1.4366, -1.4291, -1.4284, -1.4261, -1.4171, -1.4132, -1.4129, -1.4129, -1.4096, -1.3994, -1.3755, -1.3655, -1.3623, -1.3583, -1.3387, -1.3331, -1.3325, -1.3255, -1.3191, -1.3153, -1.3028, -1.3014, -1.3011, -1.2912, -1.29 , -1.2803, -1.2802, -1.2598, -1.2527, -1.2472, -1.2467, -1.2456, -1.2365, -1.2229, -1.2176, -1.2147, -1.1918, -1.1843, -1.1799, -1.1741, -1.1597, -1.1563, -1.1551, -1.1481, -1.1422, -1.1307, -1.1239, -1.1234, -1.118 , -1.1133, -1.1129, -1.1118, -1.1103, -1.1097, -1.1087, -1.1017, -1.0923, -1.0893, -1.0831, -1.0785, -1.0767, -1.0763, -1.0699, -1.069 , -1.0487, -1.0434, -1.0424, -1.0397, -1.0377, -1.0318, -1.0263, -1.017 , -1.0152, -1.0049, -1.0032, -0.9944, -0.9872, -0.9853, -0.9816, -0.9632, -0.9632, -0.9573, -0.9521, -0.9516, -0.9424, -0.9419, -0.9377, -0.9376, -0.9371, -0.9362, -0.9343, -0.92 , -0.9139, -0.9122, -0.9111, -0.9108, -0.9106, -0.9088, -0.9086, -0.8974, -0.8955, -0.8943, -0.8867, -0.8858, -0.8849, -0.8654, -0.8632, -0.8607, -0.858 , -0.8575, -0.8546, -0.849 , -0.8483, -0.8434, -0.8428, -0.8398, -0.8382, -0.8357, -0.8324, -0.8303, -0.827 , -0.8221, -0.821 , -0.8208, -0.817 , -0.8161, -0.8141, -0.8128, -0.8084, -0.8079, -0.8003, -0.7962, -0.7925, -0.7835, -0.7802, -0.7798, -0.7797, -0.7793, -0.7765, -0.7762, -0.7719, -0.7654, -0.7638, -0.7624, -0.7597, -0.7596, -0.7579, -0.7534, -0.7487, -0.7483, -0.7446, -0.7348, -0.7346, -0.7326, -0.7324, -0.7317, -0.7277, -0.7264, -0.7215, -0.7205, -0.7141, -0.7125, -0.7124, -0.7083, -0.7036, -0.7022, -0.6982, -0.6972, -0.6966, -0.692 , -0.6914, -0.6882, -0.6877, -0.686 , -0.6853, -0.6716, -0.6711, -0.6705, -0.6697, -0.6671, -0.6661, -0.6624, -0.6623, -0.6596, -0.6568, -0.6489, -0.6452, -0.6443, -0.6437, -0.6436, -0.643 , -0.641 , -0.6407, -0.6371, -0.6332, -0.6316, -0.6259, -0.6174, -0.6151, -0.6148, -0.6126, -0.6089, -0.6051, -0.605 , -0.6033, -0.6019, -0.6016, -0.601 , -0.5972, -0.582 , -0.5794, -0.5789, -0.5752, -0.5742, -0.5726, -0.572 , -0.5669, -0.5628, -0.5617, -0.5605, -0.5594, -0.5586, -0.5557, -0.5523, -0.5389, -0.5329, -0.5284, -0.5262, -0.5258, -0.5248, -0.5234, -0.5131, -0.5114, -0.5057, -0.5049, -0.5031, -0.5009, -0.4991, -0.4956, -0.4922, -0.4861, -0.4813, -0.4808, -0.4773, -0.4769, -0.4752, -0.475 , -0.4731, -0.4703, -0.467 , -0.4652, -0.4624, -0.4528, -0.452 , -0.4489, -0.4472, -0.4466, -0.4455, -0.4435, -0.4431, -0.4423, -0.4408, -0.4381, -0.4374, -0.4326, -0.4301, -0.4294, -0.4273, -0.4264, -0.4212, -0.4143, -0.4138, -0.4113, -0.4092, -0.408 , -0.4059, -0.4038, -0.4029, -0.4008, -0.3997, -0.3984, -0.3966, -0.3947, -0.3927, -0.3905, -0.388 , -0.3806, -0.3774, -0.3747, -0.3746, -0.3734, -0.3707, -0.369 , -0.3686, -0.3675, -0.3667, -0.3656, -0.3655, -0.3651, -0.3633, -0.3624, -0.3606, -0.3542, -0.354 , -0.3423, -0.3408, -0.3403, -0.3397, -0.338 , -0.3253, -0.3223, -0.3197, -0.316 , -0.3154, -0.3151, -0.3144, -0.3082, -0.3036, -0.3002, -0.2971, -0.2923, -0.2917, -0.2915, -0.2901, -0.2828, -0.2803, -0.272 , -0.2704, -0.2684, -0.2644, -0.2564, -0.2547, -0.2497, -0.2491, -0.248 , -0.2451, -0.2447, -0.2446, -0.2422, -0.2411, -0.2381, -0.2339, -0.232 , -0.2314, -0.2298, -0.2255, -0.2249, -0.2238, -0.2236, -0.2206, -0.211 , -0.2074, -0.2071, -0.2065, -0.2058, -0.2041, -0.1988, -0.1966, -0.1935, -0.1913, -0.1857, -0.1795, -0.1792, -0.1695, -0.1691, -0.1687, -0.1674, -0.1666, -0.1602, -0.1595, -0.1566, -0.1526, -0.1521, -0.1508, -0.1492, -0.149 , -0.1483, -0.148 , -0.146 , -0.145 , -0.1341, -0.1335, -0.131 , -0.1301, -0.1247, -0.1221, -0.1209, -0.1195, -0.1166, -0.116 , -0.1126, -0.1124, -0.1081, -0.1036, -0.1032, -0.101 , -0.1006, -0.0975, -0.0903, -0.0895, -0.0894, -0.0879, -0.0849, -0.0839, -0.078 , -0.0767, -0.0729, -0.0675, -0.0659, -0.0588, -0.0453, -0.0453, -0.043 , -0.0328, -0.0324, -0.0311, -0.0244, -0.0229, -0.0228, -0.0227, -0.0213, -0.0198, -0.0171, -0.0136, -0.0129, -0.012 , -0.005 , -0.0047, -0.0026, -0.0001, 0.0049, 0.0106, 0.0157, 0.0167, 0.0216, 0.022 , 0.0251, 0.0263, 0.0272, 0.0274, 0.0285, 0.0301, 0.031 , 0.0332, 0.0334, 0.0359, 0.0375, 0.0375, 0.0404, 0.0419, 0.0443, 0.0466, 0.0479, 0.049 , 0.0508, 0.0539, 0.0557, 0.0572, 0.0601, 0.0611, 0.062 , 0.07 , 0.0742, 0.0756, 0.0767, 0.0784, 0.0801, 0.0806, 0.0835, 0.0845, 0.0851, 0.0882, 0.0889, 0.0893, 0.0894, 0.0917, 0.092 , 0.0923, 0.0943, 0.0969, 0.099 , 0.1016, 0.106 , 0.1064, 0.1351, 0.1364, 0.1368, 0.1384, 0.1384, 0.1396, 0.15 , 0.1507, 0.1536, 0.1552, 0.1562, 0.1655, 0.1666, 0.1704, 0.1778, 0.1831, 0.1859, 0.1862, 0.1865, 0.1869, 0.188 , 0.1923, 0.1958, 0.1973, 0.2011, 0.2032, 0.2099, 0.2128, 0.2132, 0.2138, 0.2179, 0.2254, 0.2312, 0.2313, 0.2375, 0.2383, 0.2426, 0.2447, 0.2469, 0.2481, 0.2501, 0.252 , 0.2536, 0.2576, 0.2591, 0.2621, 0.2638, 0.2724, 0.2743, 0.2743, 0.2748, 0.2873, 0.2882, 0.2904, 0.2913, 0.2963, 0.2996, 0.3032, 0.3061, 0.3086, 0.3095, 0.3136, 0.3141, 0.3159, 0.3166, 0.3168, 0.3185, 0.3199, 0.3211, 0.3218, 0.3245, 0.3272, 0.3323, 0.3329, 0.3335, 0.3338, 0.3372, 0.3433, 0.3456, 0.3472, 0.3492, 0.3504, 0.3504, 0.3524, 0.3528, 0.3546, 0.3566, 0.3569, 0.3572, 0.3623, 0.3624, 0.3683, 0.3702, 0.3711, 0.3711, 0.3727, 0.3746, 0.3767, 0.3786, 0.3794, 0.3803, 0.381 , 0.382 , 0.3838, 0.3867, 0.387 , 0.3934, 0.394 , 0.3993, 0.3995, 0.4023, 0.4037, 0.407 , 0.4103, 0.4114, 0.4175, 0.4241, 0.4249, 0.4293, 0.4378, 0.4386, 0.4406, 0.4417, 0.4421, 0.4427, 0.4441, 0.4575, 0.4583, 0.4596, 0.4601, 0.4601, 0.4606, 0.4607, 0.462 , 0.4657, 0.4706, 0.4738, 0.4778, 0.4778, 0.4821, 0.4827, 0.4831, 0.484 , 0.4847, 0.4873, 0.4876, 0.4881, 0.4925, 0.4965, 0.5009, 0.5033, 0.5036, 0.5062, 0.5109, 0.5119, 0.5126, 0.5146, 0.5152, 0.5199, 0.5229, 0.5284, 0.5347, 0.5358, 0.5392, 0.5436, 0.5533, 0.5588, 0.5597, 0.5626, 0.5637, 0.566 , 0.5675, 0.5736, 0.5747, 0.5889, 0.5903, 0.5912, 0.5968, 0.5969, 0.6 , 0.604 , 0.6047, 0.6048, 0.6096, 0.6128, 0.6138, 0.6176, 0.6201, 0.632 , 0.6326, 0.6351, 0.6402, 0.6449, 0.6464, 0.6495, 0.6551, 0.6615, 0.662 , 0.662 , 0.6648, 0.6699, 0.6714, 0.673 , 0.6739, 0.6809, 0.6831, 0.6856, 0.6859, 0.6867, 0.6883, 0.7003, 0.7022, 0.7035, 0.7073, 0.7127, 0.7227, 0.7238, 0.7248, 0.7254, 0.7317, 0.7322, 0.7331, 0.7361, 0.7382, 0.7411, 0.7475, 0.7496, 0.7552, 0.7567, 0.7613, 0.7621, 0.7667, 0.767 , 0.7695, 0.7751, 0.7799, 0.7908, 0.7921, 0.8043, 0.8055, 0.8064, 0.8112, 0.8118, 0.8266, 0.8327, 0.8368, 0.8395, 0.8411, 0.8419, 0.8425, 0.8426, 0.8436, 0.8442, 0.8444, 0.8503, 0.851 , 0.8545, 0.8634, 0.8743, 0.8744, 0.8793, 0.8858, 0.8861, 0.8922, 0.9017, 0.9018, 0.9072, 0.9073, 0.9237, 0.9239, 0.9255, 0.927 , 0.9362, 0.9533, 0.955 , 0.9574, 0.9581, 0.961 , 0.9623, 0.9643, 0.966 , 0.9683, 0.9797, 0.9837, 0.9858, 0.9918, 0.9933, 0.9949, 0.9975, 0.9991, 1.0011, 1.0126, 1.0167, 1.0317, 1.0387, 1.0407, 1.0539, 1.0546, 1.0558, 1.0567, 1.0637, 1.066 , 1.0664, 1.067 , 1.0727, 1.0735, 1.076 , 1.0767, 1.0812, 1.0844, 1.0906, 1.0977, 1.0998, 1.1103, 1.1161, 1.122 , 1.1334, 1.1351, 1.1366, 1.1482, 1.1533, 1.1545, 1.1567, 1.163 , 1.1632, 1.1649, 1.1653, 1.1658, 1.1729, 1.1777, 1.1777, 1.1799, 1.1801, 1.1837, 1.1864, 1.1897, 1.1902, 1.1929, 1.1929, 1.1995, 1.2002, 1.2098, 1.2136, 1.2156, 1.2159, 1.2223, 1.2296, 1.2298, 1.2378, 1.2411, 1.253 , 1.2544, 1.2552, 1.2624, 1.2784, 1.2797, 1.2843, 1.2922, 1.2944, 1.2946, 1.3289, 1.3335, 1.3342, 1.3388, 1.3494, 1.3509, 1.3621, 1.3632, 1.3649, 1.3722, 1.3725, 1.3866, 1.3965, 1.3996, 1.4005, 1.4047, 1.4096, 1.4109, 1.421 , 1.4273, 1.4349, 1.4391, 1.4626, 1.4647, 1.4797, 1.4848, 1.497 , 1.4987, 1.5228, 1.5275, 1.5298, 1.5449, 1.5665, 1.5763, 1.5874, 1.5924, 1.6091, 1.6215, 1.6304, 1.6461, 1.6789, 1.6945, 1.7032, 1.7097, 1.7106, 1.7114, 1.7299, 1.7323, 1.7381, 1.7689, 1.7784, 1.7922, 1.7923, 1.8067, 1.8315, 1.8415, 1.8517, 1.858 , 1.8589, 1.8638, 1.8738, 1.8806, 1.9035, 1.9074, 1.9204, 1.9294, 1.9456, 1.9493, 1.9544, 1.9551, 1.9742, 1.9928, 1.9953, 2.0775, 2.0929, 2.1223, 2.1514, 2.239 , 2.285 , 2.3306, 2.3672, 2.3793, 2.5288, 2.5507, 2.5653, 2.5671, 2.6081, 2.697 ]) -1.5519406239259821 å”¯ä¸€åŒ–ä»¥åŠå…¶ä»–çš„é›†åˆé€»è¾‘1234names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])np.unique(names)ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])np.unique(ints)array([&apos;Bob&apos;, &apos;Joe&apos;, &apos;Will&apos;], dtype=&apos;&lt;U4&apos;) array([1, 2, 3, 4]) 1sorted(set(names))[&apos;Bob&apos;, &apos;Joe&apos;, &apos;Will&apos;] in1dæ„Ÿè§‰æŒºæœ‰ç”¨12values = np.array([6, 0, 0, 3, 2, 5, 6])np.in1d(values, [2, 3, 6])array([ True, False, False, True, True, False, True], dtype=bool) ç”¨äºŽæ•°ç»„çš„æ–‡ä»¶è¾“å…¥è¾“å‡ºå°†æ•°ç»„ä»¥äºŒè¿›åˆ¶çš„å½¢å¼ä¿å­˜åˆ°ç£ç›˜12arr = np.arange(10)np.save('some_array', arr)1np.load('some_array.npy')array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) åŽ‹ç¼©å­˜å‚¨ï¼Œå¹¶ä¸”å¯ä»¥å­˜å‚¨å¤šä¸ª1np.savez('array_archive.npz', a=arr[:4], b=arr)123arch = np.load('array_archive.npz')arch['a']arch['b']array([0, 1, 2, 3]) array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) å­˜å–æ–‡æœ¬æ–‡ä»¶1!more ch04\array_ex.txt0.580052,0.186730,1.040717,1.134411 0.194163,-0.636917,-0.938659,0.124094 -0.126410,0.268607,-0.695724,0.047428 -1.484413,0.004176,-0.744203,0.005487 2.302869,0.200131,1.670238,-1.881090 -0.193230,1.047233,0.482803,0.960334 12arr = np.loadtxt('.\\ch04\\array_ex.txt', delimiter=',')arrarray([[ 0.5801, 0.1867, 1.0407, 1.1344], [ 0.1942, -0.6369, -0.9387, 0.1241], [-0.1264, 0.2686, -0.6957, 0.0474], [-1.4844, 0.0042, -0.7442, 0.0055], [ 2.3029, 0.2001, 1.6702, -1.8811], [-0.1932, 1.0472, 0.4828, 0.9603]]) çº¿æ€§ä»£æ•°12345x = np.array([[1., 2., 3.], [4., 5., 6.]])y = np.array([[6., 23.], [-1, 7], [8, 9]])xyx.dot(y) # equivalently np.dot(x, y)array([[ 1., 2., 3.], [ 4., 5., 6.]]) array([[ 6., 23.], [ -1., 7.], [ 8., 9.]]) array([[ 28., 64.], [ 67., 181.]]) 1np.dot(x, np.ones(3))array([ 6., 15.]) 1np.random.seed(12345)123456789from numpy.linalg import inv, qrX = randn(5, 5)Xmat = X.T.dot(X)matinv(mat)mat.dot(inv(mat))q, r = qr(mat) # QRåˆ†è§£rarray([[-0.5031, -0.6223, -0.9212, -0.7262, 0.2229], [ 0.0513, -1.1577, 0.8167, 0.4336, 1.0107], [ 1.8249, -0.9975, 0.8506, -0.1316, 0.9124], [ 0.1882, 2.1695, -0.1149, 2.0037, 0.0296], [ 0.7953, 0.1181, -0.7485, 0.585 , 0.1527]]) array([[ 4.2538, -1.0645, 1.4407, 0.9898, 1.7318], [-1.0645, 7.4431, -1.5585, 4.4972, -2.1367], [ 1.4407, -1.5585, 2.8126, 0.243 , 1.2786], [ 0.9898, 4.4972, 0.243 , 5.0897, 0.305 ], [ 1.7318, -2.1367, 1.2786, 0.305 , 1.928 ]]) array([[ 0.4057, -0.1875, -0.0764, 0.1229, -0.541 ], [-0.1875, 2.462 , 0.2537, -2.3367, 3.0984], [-0.0764, 0.2537, 0.5435, -0.2369, 0.0268], [ 0.1229, -2.3367, -0.2369, 2.4239, -2.9264], [-0.541 , 3.0984, 0.0268, -2.9264, 4.8837]]) array([[ 1., 0., -0., -0., -0.], [ 0., 1., -0., -0., -0.], [ 0., 0., 1., 0., -0.], [ 0., -0., 0., 1., 0.], [ 0., 0., -0., 0., 1.]]) array([[-5.0281, 2.7734, -2.8428, -1.0619, -3.0078], [ 0. , -8.7212, 1.2925, -6.5614, 1.622 ], [ 0. , 0. , -2.0873, -1.0487, -0.6291], [ 0. , 0. , 0. , -1.408 , -0.955 ], [ 0. , 0. , 0. , 0. , 0.1537]]) éšæœºæ•°ç”Ÿæˆ12samples = np.random.normal(size=(4, 4))samplesarray([[-0.5196, 1.297 , 0.9062, 0.5809], [ 1.2233, -1.3301, 1.0483, 0.357 ], [-0.7935, -0.406 , -0.0096, -0.596 ], [ 1.3833, -0.2029, -1.0547, -0.9795]]) 1234from random import normalvariateN = 1000000%timeit samples = [normalvariate(0, 1) for _ in range(N)]%timeit np.random.normal(size=N)1 loop, best of 3: 814 ms per loop 10 loops, best of 3: 28.4 ms per loop å¯ä»¥çœ‹å‡ºnumpyç¡®å®žè¦å¿«å¾ˆå¤šExample: éšæœºæ¸¸èµ°123456789import randomposition = 0walk = [position]steps = 1000for i in range(steps): step = 1 if random.randint(0, 1) else -1 position += step walk.append(position)é€šè¿‡numpyæ¥å®žçŽ°ä¸Šè¿°è¿‡ç¨‹1np.random.seed(12345)1234nsteps = 1000draws = np.random.randint(0, 2, size=nsteps)steps = np.where(draws &gt; 0, 1, -1)walk = steps.cumsum()1234import matplotlib.pyplot as pltindex = [x + 1 for x in range(len(walk))]plt.plot(index, walk)[&lt;matplotlib.lines.Line2D at 0x234014e9f98&gt;] 12walk.min()walk.max()-3 31 1(np.abs(walk) &gt;= 10).argmax() # the first index37 ä¸€æ¬¡æ¨¡æ‹Ÿå¤šæ¬¡éšæœºæ¼«æ­¥123456nwalks = 5000 # 5000 random walknsteps = 1000draws = np.random.randint(0, 2, size=(nwalks, nsteps)) # 0 or 1steps = np.where(draws &gt; 0, 1, -1)walks = steps.cumsum(1)walksarray([[ -1, 0, -1, ..., 24, 23, 22], [ -1, 0, -1, ..., -36, -37, -36], [ 1, 2, 3, ..., -42, -41, -40], ..., [ 1, 0, -1, ..., 48, 49, 50], [ -1, -2, -3, ..., -38, -39, -40], [ -1, 0, 1, ..., -48, -47, -48]], dtype=int32) 12walks.max()walks.min()130 -117 123hits30 = (np.abs(walks) &gt;= 30).any(1)hits30hits30.sum() # Number that hit 30 or -30array([ True, True, True, ..., True, True, True], dtype=bool) 3412 12crossing_times = (np.abs(walks[hits30]) &gt;= 30).argmax(1)crossing_times.mean()497.04103165298943]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Naive Bayes]]></title>
      <url>%2F2017%2F02%2F25%2FNaive-Bayes%2F</url>
      <content type="text"><![CDATA[åˆ©ç”¨æœ´ç´ è´å¶æ–¯è¿›è¡Œæ–‡æœ¬åˆ†ç±»å‡†å¤‡æ•°æ®ä»Žæ–‡æœ¬ä¸­æž„å»ºè¯å‘é‡ä¸‹é¢å†™ä¸€ä¸ªè¯è¡¨åˆ°å‘é‡çš„è½¬æ¢å‡½æ•°123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def LoadDataSet(): """ Load a vector-like data set tranfered by a data set list that generated by artifical. Returns: return_vec: the vector-like data set. class_vec: the class label corresponds to the data items. """ posting_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] class_vec = [0, 1, 0, 1, 0, 1] # 1 is abusive, 0 not return posting_list, class_vecdef CreateVocabList(data_set): """ Create vocabulary list from vector-like data set. Arguments: data_set: the data source. Returns: vocab_list: the vocabulary list. """ vocab_set = set([]) for document in data_set: vocab_set = vocab_set | set(document) return list(vocab_set)def SetOfWords2Vec(vocab_list, input_set): """ Transfer the words list to vector for each posting. Arguments: vocab_list: The vocabulary list. input_set: The posting that ready to transfer to vector. Returns: return_vec: the result vector. """ # Initialize return_vec = [0] * len(vocab_list) for word in input_set: if word in vocab_list: return_vec[vocab_list.index(word)] = 1 else: print("the word: %s is not in my Vocabulary" % word) return return_vecä¸‹é¢å¯¹å‡½æ•°çš„åŠŸèƒ½è¿›è¡Œæµ‹è¯•123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110In [13]: import bayesIn [14]: list_of_posts, list_classes = bayes.LoadDataSet()In [15]: my_vocab_list = bayes.CreateVocabList(list_of_posts)In [16]: my_vocab_listOut[16]:['help', 'worthless', 'I', 'take', 'love', 'maybe', 'stupid', 'to', 'not', 'please', 'quit', 'park', 'posting', 'dog', 'dalmation', 'steak', 'my', 'how', 'food', 'so', 'stop', 'is', 'garbage', 'flea', 'problems', 'has', 'buying', 'ate', 'him', 'licks', 'mr', 'cute']In [17]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[0])Out[17]:[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]In [18]: bayes.SetOfWords2Vec(my_vocab_list, list_of_posts[3])Out[18]:[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]çœ‹ä¸ŠåŽ»ä¸€åˆ‡éƒ½workï¼Œå¯ä»¥è¿›å…¥ä¸‹ä¸€æ­¥äº†ã€‚è®­ç»ƒä»Žè¯å‘é‡è®¡ç®—æ¦‚çŽ‡æˆ‘ä»¬æ¥çœ‹çœ‹è´å¶æ–¯å…¬å¼ï¼š$$p(c_i | w) = \frac{p(w | c_i) p(c_i)}{p(w)}$$è¿™é‡Œ$w$ä»£è¡¨è¯å‘é‡ï¼Œ å¯ä»¥çœ‹å‡º$c_i$çš„è®¡ç®—ååˆ†ç®€å•ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ ¹æ®æœ´ç´ è´å¶æ–¯çš„å‡è®¾ï¼Œæœ‰ï¼š$$p(w | c_i) = p(w_0, w_1, w_2, \cdots, w_N | c_i) = p(w_0 | c_i)p(w_1 | c_i)p(w_2 | c_i) \cdots p(w_N | c_i)$$å½“éœ€è¦é¢„æµ‹æ–°æ ·æœ¬çš„ç±»åˆ«æ—¶ï¼šè¿™æ ·ä¸€åˆ‡å°±å¾ˆæ¸…æ¥šäº†ï¼Œä¸‹é¢ç»™å‡ºä¼ªä»£ç ï¼š12345678910è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ–‡æ¡£æ•°ç›®for æ¯ä¸€ç¯‡æ–‡æ¡£ï¼š for æ¯ä¸€ä¸ªç±»åˆ«ï¼š if è¯æ¡å‡ºçŽ°åœ¨æ–‡æ¡£ä¸­ï¼š å¢žåŠ è¯¥è¯æ¡çš„è®¡æ•°å€¼ å¢žåŠ æ€»è¯æ¡æ•°çš„è®¡æ•°å€¼ for æ¯ä¸€ä¸ªç±»åˆ«ï¼š for æ¯ä¸€ä¸ªè¯æ¡ï¼š å°†è¯¥è¯æ¡çš„æ•°ç›®é™¤ä»¥æ€»è¯æ¡æ•°ç›®å¾—åˆ°æ¡ä»¶æ¦‚çŽ‡ è¿”å›žæ¯ä¸ªç±»åˆ«çš„æ¡ä»¶æ¦‚çŽ‡æ³¨æ„ï¼Œè¿™é‡Œçš„$p(w_j | c_i)$æ˜¯è¦æ ¹æ®æ•´ä¸ªè®­ç»ƒé›†æ¥ç®—ï¼Œä»£ç å®žçŽ°å¦‚ä¸‹ï¼š123456789101112131415161718192021222324252627282930313233def TrainNaiveBayes0(train_matrix, train_category): """ the training method. Arguments: train_matrix: The train data. train_category: The train label. Returns: p0_vect: The conditional probability of w by c0 p1_vect: The conditional probability of w by c1 p_abusive: The conditional probability of c1 """ num_train_docs = len(train_matrix) num_words = len(train_matrix[0]) p_abusive = sum(train_category) / num_train_docs p0_num = zeros(num_words) p1_num = zeros(num_words) p0_denom = 0.0 p1_denom = 0.0 for i in range(num_train_docs): if train_category[i] == 1: p1_num += train_matrix[i] p1_denom += sum(train_matrix[i]) else: p0_num += train_matrix[i] p0_denom += sum(train_matrix[i]) p1_vect = p1_num / p1_denom p0_vect = p0_num / p0_denom return p0_vect, p1_vect, p_abusiveåŒæ ·è¿›è¡Œä¸€ä¸‹æµ‹è¯•ï¼š123456789101112131415161718192021222324252627282930In [25]: train_mat = []In [26]: for post_in_doc in list_of_posts: ...: train_mat.append(bayes.SetOfWords2Vec(my_vocab_list, post_in_doc)) ...:In [27]: p0_v, p1_v, p_ab = bayes.TrainNaiveBayes0(train_mat, list_classes)In [28]: p_abOut[28]: 0.5In [29]: p0_vOut[29]:array([ 0.04166667, 0. , 0.04166667, 0. , 0.04166667, 0. , 0. , 0.04166667, 0. , 0.04166667, 0. , 0. , 0. , 0.04166667, 0.04166667, 0.04166667, 0.125 , 0.04166667, 0. , 0.04166667, 0.04166667, 0.04166667, 0. , 0.04166667, 0.04166667, 0.04166667, 0. , 0.04166667, 0.08333333, 0.04166667, 0.04166667, 0.04166667])In [30]: p1_vOut[30]:array([ 0. , 0.10526316, 0. , 0.05263158, 0. , 0.05263158, 0.15789474, 0.05263158, 0.05263158, 0. , 0.05263158, 0.05263158, 0.05263158, 0.10526316, 0. , 0. , 0. , 0. , 0.05263158, 0. , 0.05263158, 0. , 0.05263158, 0. , 0. , 0. , 0.05263158, 0. , 0.05263158, 0. , 0. , 0. ])ä½†æ˜¯ä¸Šè¿°ä»£ç å­˜åœ¨ä¸€äº›ç¼ºé™·ï¼Œé¦–å…ˆï¼Œè®¡ç®—$p(w_j | c_i)$å¯èƒ½ä¼šå‡ºçŽ°ç»“æžœä¸º0çš„æƒ…å†µï¼Œé‚£ä¹ˆæœ€åŽçš„ç»“æžœå°±ä¼šä¸º0ï¼Œé‚£ä¹ˆéœ€è¦è¿›è¡Œä¸€äº›ä¿®æ”¹ (ä¸ºä»€ä¹ˆæ˜¯2ï¼Ÿ)ï¼š1234p0_num = ones(num_words)p1_num = ones(num_words)p0_denom = 2.0p1_denom = 2.0å¦å¤–ä¸€ä¸ªå°±æ˜¯ä¸‹æº¢çš„é—®é¢˜ï¼Œæ‰€ä»¥è¦æ”¹ç”¨logå‡½æ•°12p1_vect = log(p1_num / p1_denom)p0_vect = log(p0_num / p0_denom)æµ‹è¯•æœ€åŽå†™ä¸€ä¸ªåˆ†ç±»å’Œæµ‹è¯•å‡½æ•°ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142def ClassifyNaiveBayes(vec_to_classify, p0_vect, p1_vect, p_abusive): """ Classify. Arguments: vec_to_classify: The vector to classify. p0_vect: The conditional probability of w by c0. p1_vect: The conditional probability of w by c1. p_abusive: The conditional probability of c1. Returns: 0: The predict class is 0 1: The predict class is 1 """ p1 = sum(vec_to_classify * p1_vect) + log(p_abusive) p0 = sum(vec_to_classify * p0_vect) + log(1 - p_abusive) if p1 &gt; p0: return 1 else: return 0def TestNaiveBayes(): """ A test method. """ list_of_posts, list_of_classes = LoadDataSet() my_vocab_list = CreateVocabList(list_of_posts) train_mat = [] for post_in_doc in list_of_posts: train_mat.append(SetOfWords2Vec(my_vocab_list, post_in_doc)) p0_v, p1_v, p_ab = TrainNaiveBayes0( array(train_mat), array(list_of_classes)) test_entry = ['love', 'my', 'dalmation'] this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry)) print(test_entry, 'classified as: ', ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab)) test_entry = ['stupid', 'garbage'] this_doc = array(SetOfWords2Vec(my_vocab_list, test_entry)) print(test_entry, 'classified as: ', ClassifyNaiveBayes(this_doc, p0_v, p1_v, p_ab))Test123In [32]: bayes.TestNaiveBayes()['love', 'my', 'dalmation'] classified as: 0['stupid', 'garbage'] classified as: 1Ok, bravo!è¯è¢‹æ¨¡åž‹çŽ°åœ¨æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªè¯æ˜¯å¦å‡ºçŽ°ä½œä¸ºç‰¹å¾ï¼Œè¿™è¢«ç§°ä¸ºè¯é›†æ¨¡åž‹ã€‚ä½†æ˜¯å¦‚æžœæœ‰ä¸€ä¸ªè¯åœ¨æ–‡æ¡£ä¸­ä¸æ­¢å‡ºçŽ°ä¸€æ¬¡ï¼Œé‚£ä¹ˆå°±éœ€è¦è¯è¢‹æ¨¡åž‹è¿›è¡Œå»ºæ¨¡ã€‚1234567891011121314151617181920def BagOfWords2Vec(vocab_list, input_set): """ Transfer the words list to vector for each posting. Arguments: vocab_list: The vocabulary list. input_set: The posting that ready to transfer to vector. Returns: return_vec: the result vector. """ # Initialize return_vec = [0] * len(vocab_list) for word in input_set: if word in vocab_list: return_vec[vocab_list.index(word)] += 1 else: print("the word: %s is not in my Vocabulary" % word) return return_vecé«˜æ–¯æœ´ç´ è´å¶æ–¯ä¸€èˆ¬çš„æœ´ç´ è´å¶æ–¯ç®—æ³•çš„è¾“å…¥ç‰¹å¾ä¸ºç¦»æ•£å€¼ï¼Œé‚£ä¹ˆå½“è¾“å…¥å˜é‡ä¸ºè¿žç»­å€¼æ—¶å°±ä¸èƒ½å¤„ç†äº†ï¼Œä¸€èˆ¬è¿™æ—¶å€™å‡è®¾è¾“å…¥å˜é‡æœä»Žä¸€ä¸ªæ­£æ€åˆ†å¸ƒï¼Œè¿™æ ·$p(w_j | c_i)$å°±å¯ä»¥è®¡ç®—äº†ï¼Œæ‰€ä»¥æ•´ä¸ªçš„æµç¨‹å¦‚ä¸‹ï¼šé‡‡ç”¨sk-learnè¿›è¡Œä¸‹å®žéªŒ123456789In [33]: from sklearn import datasets ...: iris = datasets.load_iris() ...: from sklearn.naive_bayes import GaussianNB ...: gnb = GaussianNB() ...: y_pred = gnb.fit(iris.data, iris.target).predict(iris.data) ...: print("Number of mislabeled points out of a total %d points : %d" ...: % (iris.data.shape[0],(iris.target != y_pred).sum())) ...:Number of mislabeled points out of a total 150 points : 6]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS231n Lecture2 note]]></title>
      <url>%2F2017%2F02%2F24%2FCS231n-Lecture2-note%2F</url>
      <content type="text"><![CDATA[å›¾åƒåˆ†ç±»ç›®æ ‡ç»™ä¸€å¼ è¾“å…¥å›¾ç‰‡èµ‹äºˆä¸€ä¸ªæ ‡ç­¾ï¼Œ è¿™ä¸ªæ ‡ç­¾å±žäºŽäº‹å…ˆå®šä¹‰å¥½çš„ç±»åˆ«é›†åˆä¸­åœ°ä½è®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒé—®é¢˜ä¾‹å­éš¾ç‚¹æ‹æ‘„ç‚¹çš„è§†è§’å¤šæ ·æ‹æ‘„ç‚¹çš„è¿œè¿‘è·ç¦»å¤šæ ·ç‰©ä½“çš„å˜å½¢ç‰©ä½“éƒ¨åˆ†é®æŒ¡å…‰çº¿èƒŒæ™¯ç›¸ä¼¼å“ç§å¤šæ ·æ–¹æ³•æ•°æ®é©±åŠ¨ï¼ˆå³åŒ…å«è®­ç»ƒæ•°æ®ï¼‰pipelineè¾“å…¥ -&gt; å­¦ä¹  -&gt; è¯„ä¼°æœ€è¿‘é‚»åˆ†ç±»å™¨è·ç¦»åº¦é‡L1 distance$$d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|$$L2 distance$$d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$$ç¤ºä¾‹ä»£ç æ•°æ®è¯»å–1234Xtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide# flatten out all images to be one-dimensionalXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072é¢„æµ‹åŠè¯„ä¼°123456nn = NearestNeighbor() # create a Nearest Neighbor classifier classnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labelsYte_predict = nn.predict(Xte_rows) # predict labels on the test images# and now print the classification accuracy, which is the average number# of examples that are correctly predicted (i.e. label matches)print 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )åŸºæœ¬æ–¹æ³•123456789101112131415161718192021222324252627import numpy as npclass NearestNeighbor(object): def __init__(self): pass def train(self, X, y): """ X is N x D where each row is an example. Y is 1-dimension of size N """ # the nearest neighbor classifier simply remembers all the training data self.Xtr = X self.ytr = y def predict(self, X): """ X is N x D where each row is an example we wish to predict label for """ num_test = X.shape[0] # lets make sure that the output type matches the input type Ypred = np.zeros(num_test, dtype = self.ytr.dtype) # loop over all test rows for i in xrange(num_test): # find the nearest training image to the i'th test image # using the L1 distance (sum of absolute value differences) distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1) min_index = np.argmin(distances) # get the index with smallest distance Ypred[i] = self.ytr[min_index] # predict the label of the nearest example return Ypredå®žéªŒç»“æžœL1-distance 38.6% on CIFAR-10L2-distance 35.4% on CIFAR-10L1 vs. L2 L2æ¯”L1çš„å·®å¼‚å®¹å¿åº¦æ›´å°Kè¿‘é‚»åˆ†ç±»å™¨é‡‡ç”¨éªŒè¯é›†è¿›è¡Œè¶…å‚è°ƒå‚å®žä¾‹ä»£ç 123456789101112131415161718192021# assume we have Xtr_rows, Ytr, Xte_rows, Yte as before# recall Xtr_rows is 50,000 x 3072 matrixXval_rows = Xtr_rows[:1000, :] # take first 1000 for validationYval = Ytr[:1000]Xtr_rows = Xtr_rows[1000:, :] # keep last 49,000 for trainYtr = Ytr[1000:]# find hyperparameters that work best on the validation setvalidation_accuracies = []for k in [1, 3, 5, 10, 20, 50, 100]: # use a particular value of k and evaluation on validation data nn = NearestNeighbor() nn.train(Xtr_rows, Ytr) # here we assume a modified NearestNeighbor class that can take a k as input Yval_predict = nn.predict(Xval_rows, k = k) acc = np.mean(Yval_predict == Yval) print 'accuracy: %f' % (acc,) # keep track of what works on the validation set validation_accuracies.append((k, acc))äº¤å‰éªŒè¯æœ€è¿‘é‚»åˆ†ç±»å™¨çš„ä¼˜ç¼ºç‚¹ä¼˜ç‚¹è®­ç»ƒé€Ÿåº¦å¿«ç¼ºç‚¹æµ‹è¯•é€Ÿåº¦å¾ˆæ…¢â€‹ è§£å†³æ–¹æ³•ï¼š1. ANN 2. FANNè·ç¦»åº¦é‡ä¸åˆé€‚â€‹(http://cs231n.github.io/assets/pixels_embed_cifar10_big.jpg)æŽ¥ä¸‹æ¥â€¦t-SNE http://lvdmaaten.github.io/tsne/random projection http://scikit-learn.org/stable/modules/random_projection.htmlINTUITION FAILS IN HIGH DIMENSIONS http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdfRecognizing and Learning Object Categories http://people.csail.mit.edu/torralba/shortCourseRLOC/index.htmlçº¿æ€§åˆ†ç±»ä¸€ä¸ªä»Žå›¾åƒåˆ°æ ‡ç­¾çš„æ˜ å°„å‡½æ•°$$f(x_i, W, b) = W x_i + b$$x shape is [D x 1], W shape is [K x D], b shape is [K x 1]æ³¨æ„ç‚¹ï¼šWä»£è¡¨Kä¸ªåˆ†ç±»å™¨çš„å‚æ•°æ”¾åœ¨ä¸€èµ·ï¼Œå› æ­¤æ•´ä¸ªæ¨¡åž‹æ˜¯Kä¸ªåˆ†ç±»å™¨çš„ä¸€ä¸ªæ•´åˆå‘é‡åŒ–èƒ½å¤Ÿå¤§å¤§æå‡è®¡ç®—é€Ÿåº¦çº¿æ€§åˆ†ç±»å™¨çš„è§£é‡Šæƒé‡Wè¡¨ç¤ºä¸åŒçš„æ ‡ç­¾å¯¹äºŽå›¾åƒä¸åŒä½ç½®ä¸åŒé¢œè‰²çš„é‡è§†ç¨‹åº¦ã€‚æ¯”å¦‚å¤ªé˜³å¯èƒ½å¯¹äºŽåœ†å½¢çš„åŒºåŸŸä»¥åŠé»„é¢œè‰²æ¯”è¾ƒçœ‹é‡å°†å›¾åƒçœ‹æˆé«˜ç»´ç©ºé—´ä¸­çš„ç‚¹å°†çº¿æ€§åˆ†ç±»å™¨çœ‹æˆæ¨¡æ¿åŒ¹é…å°†Wçš„æ¯ä¸€è¡Œçœ‹æˆä¸€ä¸ªæ¨¡æ¿ï¼Œé€šè¿‡å†…ç§¯è®¡ç®—ï¼Œæ¯ä¸€å¼ å›¾ç‰‡å¼ æˆçš„åˆ—å‘é‡éƒ½ä¸Žæ¯ä¸€ä¸ªæ¨¡æ¿ä½œæ¯”è¾ƒï¼Œæœ€åŽé€‰å‡ºæœ€åŒ¹é…çš„ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§æœ€è¿‘é‚»ç®—æ³•ã€‚ä»Žä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œè¿™é‡Œçš„æ¨¡æ¿æ˜¯å„ä¸ªå›¾åƒçš„ä¸€ç§æŠ˜ä¸­ã€‚å°†biasé¡¹æ”¾å…¥Wä¸­æ•°æ®é¢„å¤„ç†â€‹ æ•°æ®ä¸­å¿ƒåŒ–]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Qiniu cloud images batch upload and directory synchronization]]></title>
      <url>%2F2017%2F02%2F24%2FQiniu-cloud-images-batch-upload-and-directory-synchronization%2F</url>
      <content type="text"><![CDATA[æœ€è¿‘å†™åšå®¢çš„æ—¶å€™ä¼šç”¨åˆ°å›¾ç‰‡ï¼Œå› æ­¤ç”¨äº†ä¸ƒç‰›äº‘çš„å›¾ç‰‡å¤–é“¾åŠŸèƒ½ï¼Œä½†æ˜¯å…¶å†…å®¹ç®¡ç†ä¸èƒ½åˆ›å»ºç›®å½•ï¼Œæ‰€ä»¥å›¾ç‰‡çš„å‘½åä»¥åŠä¸Šä¼ éƒ½å¾ˆéº»çƒ¦ï¼Œç„¶åŽåŽ»ç½‘ä¸ŠæŸ¥äº†ä¸€ä¸‹ï¼Œä¹Ÿçœ‹äº†ä¸€ä¸‹å®˜æ–¹æ–‡æ¡£ï¼Œå‘çŽ°å®˜æ–¹æœ‰ä¸€ä¸ªæ‰¹é‡ä¸Šä¼ çš„å·¥å…·æŒºå¥½ç”¨ï¼Œæ‰€ä»¥è®°å½•ä¸€ä¸‹ï¼Œä¸‹é¢æŠŠå®˜æ–¹æ–‡æ¡£ä¸­çš„ä¸€äº›ä¸œè¥¿è´´å‡ºæ¥ï¼Œæ–¹ä¾¿æ—¥åŽæŸ¥é˜…ã€‚â€‹]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Correlation Analysis]]></title>
      <url>%2F2017%2F02%2F24%2FCorrelation-Analysis%2F</url>
      <content type="text"><![CDATA[é—®é¢˜æè¿°æœ€è¿‘æœ‰ä¸€ä¸ªå…³äºŽå…³è”åˆ†æžçš„å°ä½œä¸šï¼Œé—®é¢˜æè¿°å¦‚ä¸‹ï¼šå­©å­çš„æƒ…æ„ŸçŠ¶å†µã€å“è¡Œç—‡çŠ¶ã€äº²ç¤¾ä¼šè¡Œä¸ºç­‰ç­‰ä¸Žå¾ˆå¤šå› ç´ ç›¸å…³ï¼Œè¿™ä¸ªä½œä¸šä¸»è¦ç€çœ¼ç‚¹åœ¨äºŽå­©å­æ¯å¤©çœ‹è§†é¢‘çš„æ—¶é—´æ˜¯å¦å¯¹äºŽä»¥ä¸Šè¿™äº›æŒ‡æ ‡æœ‰ç€é‡è¦çš„å½±å“ï¼Œå› æ­¤éœ€è¦å¯¹è¾“å…¥ç‰¹å¾ä¸Žè¾“å‡ºæ ‡ç­¾ä¹‹é—´çš„ç›¸å…³å…³ç³»åšä¸€ä¸ªåˆ†æžã€‚å±žæ€§æè¿°å¦‚ä¸‹ï¼šç”±ä»¥ä¸Šå±žæ€§æè¿°è¡¨å¯ä»¥çœ‹å‡ºï¼Œå±žæ€§ç±»åž‹åŒæ—¶åŒ…æ‹¬ç±»åˆ«ä»¥åŠæ•°å€¼ï¼Œå› æ­¤è¿›è¡Œç›¸å…³åˆ†æžæ—¶é’ˆå¯¹è¿™ä¸¤ç§æƒ…å†µéœ€è¦é‡‡ç”¨ä¸åŒçš„æ–¹æ³•ï¼Œè¿™é‡Œé‡‡ç”¨çš„ç­–ç•¥å¦‚ä¸‹ï¼šå¯¹äºŽç¦»æ•£-ç¦»æ•£çš„æƒ…å†µï¼Œé‡‡ç”¨å¡æ–¹æ£€éªŒå¯¹äºŽè¿žç»­-ç¦»æ•£çš„æƒ…å†µï¼Œé‡‡ç”¨ANOVAæ–¹å·®æ£€éªŒä¸‹é¢å¯¹äºŽè¿™ä¸¤ç§æ£€éªŒæ–¹æ³•è¿›è¡Œä»‹ç»ã€‚å¡æ–¹æ£€éªŒä¸€ç»´æƒ…å†µå‡è®¾ä¸€æ¡æ²³é‡Œæœ‰ä¸‰ç§é±¼ï¼Œgumpies, sticklebarbs, ä»¥åŠspotheadsã€‚å¦‚æžœè¿™æ¡æ²³çš„ç”Ÿæ€çŽ¯å¢ƒæ²¡æœ‰é­åˆ°å¹²æ‰°ï¼Œé‚£ä¹ˆè¿™ä¸‰ç§é±¼çš„æ•°é‡æ˜¯ç›¸ç­‰çš„ï¼ˆä¸‹è¡¨ç¬¬ä¸‰è¡Œï¼‰ã€‚çŽ°åœ¨ä»Žæ²³é‡Œè¿›è¡Œ300æ¬¡æŠ½æ ·ï¼Œæœ€åŽæŠ½æ ·å¾—åˆ°çš„ç»“æžœå¦‚ä¸‹è¡¨ç¬¬äºŒè¡Œæ‰€ç¤ºï¼šgumpiessticklebarbsspothheadsTotalsè§‚å¯Ÿåˆ°çš„é¢‘æ•°8912091300æœŸæœ›çš„é¢‘æ•°100100100300çŽ°åœ¨éœ€è¦è§£å†³çš„é—®é¢˜æ˜¯ï¼Œæ˜¯å¦è¿™æ¡æ²³çš„ç”Ÿæ€çŽ¯å¢ƒæ”¶åˆ°äº†å¹²æ‰°ï¼Ÿæˆ‘ä»¬å‡è®¾ç”Ÿæ€çŽ¯å¢ƒæ­£å¸¸ï¼Œè¿™æ˜¯æˆ‘ä»¬çš„åŽŸå‡è®¾ã€‚ï¼ˆæ³¨æ„ï¼ŒæœŸæœ›çš„é¢‘æ•°æ˜¯æ ¹æ®å®žé™…æƒ…å†µè‡ªè¡Œå®šä¹‰çš„ï¼‰å¾ˆå®¹æ˜“æƒ³åˆ°çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€ç§åº¦é‡æ¥è¡¡é‡çŽ°å®žæƒ…å†µä¸ŽåŽŸå‡è®¾çš„åç¦»ç¨‹åº¦ï¼Œä¾‹å¦‚ï¼š$$\frac{è§‚å¯Ÿå€¼ - æœŸæœ›å€¼}{æœŸæœ›å€¼}$$æŠŠå®žé™…çš„æ•°æ®å¸¦å…¥ï¼Œå¯ä»¥å¾—åˆ°ä»¥ä¸‹çš„ç»“æžœï¼šgumpies: $\frac{89 - 100}{100} = -0.11$sticklebarbs: $\frac{120 - 100}{100} = +0.20$spothheads: $\frac{91 - 100}{100} = -0.09$å‘çŽ°ç»“æžœè¿˜ä¸é”™ï¼Œä½†æ˜¯è¿™åªèƒ½ç”¨æ¥è¡¡é‡å•ä¸ªç±»åˆ«çš„åå·®ç¨‹åº¦ï¼Œè€Œä¸èƒ½ç”¨æ¥è¡¡é‡æ•´ä½“çš„åå·®ç¨‹åº¦ï¼Œå› ä¸ºè¿™ä¸‰è€…çš„åŠ å’Œä¸ºé›¶ã€‚æ—¢ç„¶è¿™æ ·ï¼Œå¾ˆå®¹æ˜“æƒ³åˆ°å¯ä»¥å¯¹ä¹‹å‰çš„åº¦é‡è¿›è¡Œç®€å•çš„ä¿®æ”¹ï¼Œå˜æˆè¿™æ ·ï¼š$$\frac{(è§‚å¯Ÿå€¼ - æœŸæœ›å€¼)^2}{æœŸæœ›å€¼}$$å†æŠŠæ•°æ®å¸¦å…¥çœ‹çœ‹ï¼šgumpies: $\frac{(89 - 100)^2}{100} = 1.21$sticklebarbs: $\frac{(120 - 100)^2}{100} = 4.0$spothheads: $\frac{(91 - 100)}{100} = 0.81$sum: $1.21 + 4.0 + 0.81 = 6.02$è¿™æ ·ä¸€æ¥é—®é¢˜ä¾¿å¾—åˆ°äº†è§£å†³ï¼Œè€Œè¿™ä¹Ÿæ­£æ˜¯å¡æ–¹æ£€éªŒæ‰€é‡‡å–çš„æ–¹å¼ã€‚è€Œè¿™ä¸ªsumå€¼å°±æ˜¯å¡æ–¹ï¼ˆchi-squareï¼‰ï¼Œè®°ä½œ$\chi^2$ï¼Œ ä¸ºäº†æ›´åŠ å½¢å¼åŒ–åœ°è¡¨ç¤ºï¼Œæˆ‘ä»¬æŠŠè§‚å¯Ÿå€¼è®°ä¸º$O$ï¼Œ æœŸæœ›å€¼è®°ä¸º$E$ï¼Œ é‚£ä¹ˆæœ‰å¦‚ä¸‹ç­‰å¼æˆç«‹ï¼š$$\chi^2 = \sum \frac{(O - E) ^ 2}{E}$$ä¸‹é¢çš„é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬è¾¾åˆ°äº†å¡æ–¹å€¼ï¼Œä½†æ˜¯è¿™ä¸ªå€¼åˆ°åº•å¥½è¿˜æ˜¯ä¸å¥½å‘¢ï¼Ÿæ˜¯æ›´åŠ æ”¯æŒåŽŸå‡è®¾è¿˜æ˜¯æ‹’ç»åŽŸå‡è®¾å‘¢ï¼Ÿå¯ä»¥è®¾æƒ³è¿™æ ·ä¸€ç§æƒ…å†µï¼Œæˆ‘ä»¬å‡è®¾æ²³é‡Œçš„é±¼æœä»ŽåŽŸå‡è®¾çš„åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯ä¸‰ç§é±¼å‡ºçŽ°çš„æ¦‚çŽ‡ç›¸ç­‰ã€‚ç„¶åŽæˆ‘ä»¬æŠŠä¸‰ç™¾æ¬¡é‡‡æ ·çœ‹ä½œä¸€æ¬¡å®žéªŒï¼Œæ¯ä¸€æ¬¡å®žéªŒå®Œæ¯•ä¹‹åŽè®°å½•ä¸‹é‡‡æ ·å‡ºçš„300æ¡é±¼ä¸­æ¯ä¸€ç§é±¼çš„é¢‘æ•°ï¼Œç„¶åŽè®¡ç®—å¡æ–¹å€¼ã€‚åœ¨è¿›è¡Œå¾ˆå¤šæ¬¡è¿™æ ·çš„å®žéªŒä¹‹åŽï¼Œæˆ‘ä»¬å¯ä»¥ç”»ä¸€ä¸ªæŸ±çŠ¶å›¾ï¼Œè¿™ä¸ªå›¾è®°å½•ä¸‹äº†å¡æ–¹å€¼çš„åˆ†å¸ƒæƒ…å†µã€‚ç„¶åŽå†æŠŠå®žé™…çš„è§‚å¯Ÿå€¼ï¼ˆä¹Ÿå°±æ˜¯ä¸Šé¢è¡¨æ ¼çš„ç¬¬äºŒè¡Œï¼‰è®¡ç®—çš„å¡æ–¹å€¼ï¼ˆ6.02ï¼‰å¸¦å…¥è¿›åŽ»ï¼Œçœ‹çœ‹å¤§äºŽæˆ–è€…ç­‰äºŽè¿™ä¸ªå€¼åœ¨æŸ±çŠ¶å›¾ä¸­æ‰€æœ‰å¡æ–¹å€¼ä¸­å æœ‰å¤šå°‘æ¯”ä¾‹ã€‚å¦‚æžœå æœ‰çš„æ¯”ä¾‹å¾ˆå¤§ï¼Œè¯´æ˜Žè¿™ä¸ªå€¼æ˜¯ç”±è·ŸåŽŸå‡è®¾å¾ˆè¿‘ä¼¼çš„å‡è®¾å¾—å‡ºçš„ï¼Œè¿™å°±è¯æ˜Žäº†åŽŸå‡è®¾æ˜¯å¯¹çš„ï¼›åä¹‹ã€‚å¦‚æžœè¿™ä¸ªæ¯”ä¾‹å¾ˆå°ï¼Œè¯´æ˜Žå¦‚æžœåˆ†å¸ƒæœä»ŽåŽŸå‡è®¾ï¼Œé‚£ä¹ˆæ‰€è®¡ç®—å‡ºçš„å¡æ–¹å€¼åŸºæœ¬ä¸å¯èƒ½åŒ…å«è¿™ä¸ªè§‚æµ‹å‡ºçš„å¡æ–¹å€¼ï¼Œè¡¨æ˜ŽåŽŸå‡è®¾æ˜¯ä¸å¯¹çš„ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ‹’ç»åŽŸå‡è®¾ã€‚å…¶å®žç»Ÿè®¡æ£€éªŒçš„åŸºæœ¬æ€æƒ³å°±æ˜¯è¿™æ ·ã€‚ä½†æ˜¯å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸å¯èƒ½è¿›è¡ŒçœŸå®žçš„é‡‡æ ·ï¼ˆä»Žæ²³é‡ŒæŠ“é±¼ï¼‰ï¼Œæ‰€ä»¥ä¸€èˆ¬é‡‡ç”¨è®¡ç®—æœºæ¨¡æ‹Ÿçš„æ–¹å¼ï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹æ‰€ç¤ºï¼šç­‰æ¦‚çŽ‡çš„äº§ç”Ÿaï¼ˆä»£è¡¨gumpiesï¼‰, bï¼ˆä»£è¡¨sticklebarbsï¼‰, cï¼ˆä»£è¡¨spothheadsï¼‰çš„åºåˆ—è®¡ç®—ä¸€ä¸ªå¤§å°ä¸º300çš„åºåˆ—ä¸­a, b, cä¸‰è€…çš„é¢‘æ•°ï¼Œä½œä¸ºè§‚å¯Ÿå€¼ï¼Œç„¶åŽå°†a = b = c = 100ä½œä¸ºæœŸæœ›å€¼ï¼Œè®¡ç®—å¹¶è®°å½•ä¸‹ç®—å‡ºçš„å¡æ–¹å€¼é‡å¤1~2æ­¥10000æ¬¡ï¼Œ ç”»å‡ºæŸ±çŠ¶å›¾å¦‚ä¸‹ï¼šå¯ä»¥çœ‹å‡ºåªæœ‰5%å·¦å³çš„å€¼å¤§äºŽ6.02ï¼Œè¯´æ˜Žæˆ‘ä»¬å¯ä»¥ä»¥95%çš„ç½®ä¿¡åº¦æ‹’ç»åŽŸå‡è®¾ã€‚äºŒç»´æƒ…å†µå¯¹äºŽäºŒç»´çš„æƒ…å†µï¼Œå¡æ–¹æ£€éªŒåˆè¢«ç§°ä¸ºå¡æ–¹å…³è”åº¦æ£€éªŒï¼Œä¹Ÿå°±æ˜¯æ£€éªŒä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³ç¨‹åº¦ï¼ˆç‹¬ç«‹ç¨‹åº¦ï¼‰ï¼Œè€ƒè™‘ä¸‹é¢è¿™ä¸ªæ•°æ®è¡¨$O$ï¼šAlzhemerâ€™s onset -during 5-year periodnoyesrecieved-yes1479156estrogenno8101589689571671124è¿™æ˜¯è§‚å¯Ÿå€¼ï¼Œä¸ºäº†è®¡ç®—å¡æ–¹å€¼ï¼Œå¾ˆæ˜Žæ˜¾æˆ‘ä»¬éœ€è¦è®¡ç®—æœŸæœ›å€¼ã€‚ä¸ºäº†æ–¹ä¾¿è¡¨ç¤ºï¼ŒæŠŠä¸Šè¡¨å˜æˆå¦‚ä¸‹å½¢å¼ï¼šï¼ˆAï¼‰Alzhemerâ€™s onset -during 5-year periodnoyes(R) recieved-yes[cell a][cell b]156estrogenno[cell c][cell d]9689571671124æ‹¿aåšä¾‹å­ï¼š$$E_a = \frac{156}{1124} \times \frac{957}{1124} \times 1124$$è¿™ä¸ªå¼å­å¦‚ä½•è§£é‡Šå‘¢ï¼Ÿå¦‚æžœè¿™ä¸¤ä¸ªå˜é‡æ˜¯ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆ$A$å˜é‡å–$no$å€¼ä¸Ž$R$å˜é‡å–$yes$å€¼è¿™ä¸¤ä¸ªäº‹ä»¶ä¹‹é—´å°±æ˜¯ç‹¬ç«‹çš„ï¼Œé‚£ä¹ˆ$[cell_a]$äº‹ä»¶å‘ç”Ÿçš„æ¦‚çŽ‡å°±æ˜¯ä¸¤è€…ç›¸ä¹˜ï¼Œä¹Ÿå°±æ˜¯ä¸Šè¿°ç­‰å¼å³è¾¹å‰ä¸¤ä¸ªæ•°ç›¸ä¹˜ï¼Œæœ€åŽçš„æœŸæœ›å€¼è‡ªç„¶å°±æ˜¯æ¦‚çŽ‡ä¹˜ä»¥å®žéªŒæ€»æ•°ã€‚ä¸Šé¢çš„è§£é‡Šæ¯”è¾ƒä¸æ­£å¼ï¼Œæ¢ä¸€ç§è¾ƒä¸ºæ­£å¼çš„è¡¨è¾¾æ–¹å¼ã€‚å‡è®¾æˆ‘ä»¬è¦æ±‚$cell$çš„æœŸæœ›å€¼ï¼Œè®¾$R$ä¸º$cell$æ‰€åœ¨åˆ—çš„è¾¹ç¼˜äº‹ä»¶æ€»æ•°ï¼Œ$C$ä¸º$cell$æ‰€åœ¨è¡Œçš„è¾¹ç¼˜äº‹ä»¶æ€»æ•°ï¼Œ$N$ä¸ºå®žéªŒæ€»æ•°ç›®ï¼Œè¿™æ ·å°±æœ‰ï¼š$$E_{cell} =\frac{R}{N} \times \frac{C}{N} \times N$$æ‰€ä»¥å°±æœ‰æœŸæœ›å€¼$E$æ•°æ®è¡¨å¦‚ä¸‹ï¼šï¼ˆAï¼‰Alzhemerâ€™s onset -during 5-year periodnoyes(R) recieved-yes$E_a = \frac{156 \times 957}{1124} = 132.82$$E_b = \frac{156 \times 167}{1124} = 23.18$156estrogenno$E_c = \frac{968 \times 957}{1124} = 824.18$$E_d = \frac{968 \times 167}{1124} = 143.82$9689571671124è¿™æ ·å°±å¯ä»¥è°ƒç”¨å…¬å¼ï¼š$$\chi^2 = \sum \frac{(O - E) ^ 2}{E}$$ç‰¹åˆ«åœ°ï¼Œå½“è¡Œæ•°ä»¥åŠåˆ—æ•°éƒ½ä¸º2æ—¶ï¼Œä¸Šè¿°å…¬å¼éœ€è¦è¿›è¡Œä¸€ä¸‹ä¿®æ”¹ï¼š$$\chi^2 = \sum \frac{(|O - E| - 0.5) ^ 2}{E}$$è¿™æ ·ç®—å‡ºæ¥çš„å¡æ–¹å€¼ä¸º$11.01$æœ€åŽæ¶‰åŠåˆ°è‡ªç”±åº¦çš„é—®é¢˜ï¼Œå› ä¸ºæ¯”è¾ƒç®€å•ï¼Œæ‰€ä»¥åªå†™å‡ºç»“è®ºï¼š$$df = (r - 1)(c - 1)$$r = number of rowsc = number of columnsANOVAæ–¹å·®æ£€éªŒä¸ºäº†æ–¹ä¾¿æ¯”è¾ƒï¼ŒåŒæ ·é‡‡ç”¨ä¸Šè¿°é˜¿å°”å…¹æµ·é»˜ç—…çš„ä¾‹å­ã€‚ç ”ç©¶è¡¨æ˜Žï¼Œè€å¹´ç—´å‘†ç—‡æ‚£è€…æ‚£ç—…ä¹‹åŽä¼šç»å¸¸ç»åŽ†æƒ…ç»ªéžå¸¸ä¸ç¨³å®šåœ°é˜¶æ®µï¼ŒåŽŸå› æ˜¯å› ä¸ºæ‚£è€…æ‚£ç—…ä¹‹å‰çš„ç”Ÿæ´»ä¸­ç»å¸¸æœ‰ææƒ§æˆ–è€…ç„¦è™‘çš„ä½“éªŒï¼Œæ­£æ˜¯è¿™äº›ä¸€ç›´å­˜åœ¨äºŽè„‘æµ·ä¸­çš„è®°å¿†å‡ºå‘äº†æ‚£ç—…åŽçš„ä¸ç¨³å®šæƒ…ç»ªçš„äº§ç”Ÿã€‚çŽ°åœ¨æˆ‘ä»¬å‡è®¾æœ‰ä¸€ä¸ªå®žéªŒå›¢é˜Ÿå‘æ˜Žäº†ä¸€ç§è¯ç‰©ï¼Œå¯ä»¥ç¼“è§£è¿™ç§æƒ…ç»ªé—®é¢˜ï¼Œä»–ä»¬å¯¹å°ç™½é¼ è¿›è¡Œäº†å®žéªŒã€‚å®žéªŒè®¾è®¡å¦‚ä¸‹ï¼šå°†å°ç™½é¼ éšæœºåˆ†ä¸ºå››ç»„Aï¼Œ Bï¼Œ Cï¼Œ DAç»„ä½œä¸ºå‚ç…§ç»„ï¼Œä¸ç»™è¯ï¼›Bï¼Œ Cï¼Œ Dä¸‰ç»„åˆ†åˆ«æ³¨å°„ä¸€ä¸ªå•ä½ï¼Œä¸¤ä¸ªå•ä½ï¼Œä¸‰ä¸ªå•ä½çš„è¯å‰‚è®°å½•å®žéªŒç»“æžœï¼Œæ•°å€¼è¶Šä½Žè¡¨æ˜Žå®žéªŒæ•ˆæžœè¶Šå¥½å®žéªŒç»“æžœå¦‚ä¸‹ï¼šABCDTotal27.022.821.923.526.223.123.419.628.827.720.123.733.527.627.820.828.824.019.323.9$M_a = 28.86$$M_b = 25.04$$M_c = 22.50$$M_d = 22.30$$M_T = 24.68$ä¸‹é¢éœ€è¦è¿›è¡Œä¸€ä¸‹ç›¸å…³æ€§åˆ†æžï¼Œåˆ¤æ–­è¯ç‰©æ˜¯å¦å¯¹ç—‡çŠ¶çš„ç¼“è§£äº§ç”Ÿä½œç”¨ã€‚å’Œå¡æ–¹æ£€éªŒä¸€æ ·ï¼ŒANOVAæ£€éªŒæœ€åŽä¹Ÿæœ‰ä¸€ä¸ªè¡¡é‡æŒ‡æ ‡ï¼Œè®°ä¸º$F$ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š$$F = \frac{MS_{bg}}{MS_{wg}} = \frac{ç»„é—´ç›¸ä¼¼åº¦}{ç»„å†…ç›¸ä¼¼åº¦}$$å…·ä½“çš„è®¡ç®—æ­¥éª¤å¦‚ä¸‹æ‰€ç¤ºï¼ˆæŽ¨å¯¼è¿‡ç¨‹çœç•¥ï¼‰ï¼ˆ1ï¼‰ é¦–å…ˆè®¡ç®—å‡ºå¦‚ä¸‹å€¼ï¼šABCDTotal$N_A = 5$$N_B = 5$$N_C = 5$$N_D = 5$$N_T = 20$$\sum X_{Ai} = 144.30$$\sum X_{Bi} = 125.20$$\sum X_{Ci} = 112.50$$\sum X_{Di} = 111.50$$\sum X_{Ti} = 493.50$$\sum X^2_{Ai} = 4196.57$$\sum X^2_{Bi} = 3158.50$$\sum X^2_{Ci} = 2576.51$$\sum X^2_{Di} = 2501.95$$\sum X^2_{Ti} = 12433.53$$SS_A = 32.07$$SS_B = 23.49$$SS_C = 45.26$$SS_D = 15.50$$SS_T = 256.42$å…¶ä¸­ï¼š$$SS = \sum X^2_i - \frac{(\sum X_i)^2}{N}$$ï¼ˆ2ï¼‰è®¡ç®—$SS_{wg}$ä»¥åŠ$SS_{bg}$$$SS_{wg} = SS_A + SS_B + SS_C + SS_D$$$$SS_{bg} = SS_T - SS_{wg}$$ï¼ˆ4ï¼‰è®¡ç®—ç›¸å…³è‡ªç”±åº¦$$df_{bg} = k - 1 = 4 - 1 = 3$$$$df_{wg} = (N_A - 1) + (N_B - 1) + (N_C - 1) + (N_D - 1)$$ï¼ˆ5ï¼‰è®¡ç®— $MS_{bg}$ä»¥åŠ$MS_{wg}$$$MS_{bg} = \frac{SS_{bg}}{df_{bg}}$$$$MS_{wg} = \frac{SS_{wg}}{df_{wg}}$$ï¼ˆ6ï¼‰è®¡ç®—$F$æœ€åŽå¾—å‡ºF = 6.42 ï¼ˆdf = 3, 16ï¼‰ä»£ç å®žçŽ°123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269import osimport xlrdimport numpy as npimport scipy.stats as statsdef GetChildDetailLab(combine=True): """ Get the detail labs of each child data. Returns: child_detail_lab: the detail labs. """ CHILDS_FEATURE_LAB = ['tid', 'age_m', 'female', 'onlychild', 'divorce', 'medu_newcat', 'income_newcat', 'scr_ave', 'edu_ave', 'edu_of_scr', 'scr_h_cat', 'mediacoview', 'mediacontact'] CHILDS_CAT_LAB = ['emo_cat', 'con_cat', 'hyp_cat', 'pee_cat', 'difficulties_cat', 'pro_cat'] if combine: CHILDS_DETAIL_LAB = CHILDS_FEATURE_LAB CHILDS_DETAIL_LAB.extend(CHILDS_CAT_LAB) return CHILDS_DETAIL_LAB else: return CHILDS_FEATURE_LAB, CHILDS_CAT_LABdef ReadChildInfoFromExcel( file_name='å±å¹•æš´éœ²ä¸ŽSDQ.xlsx', sheet_name='data'): """ Read the screen-exposed vs.SDQ detail information of each child from the excel-type file. Arguments: file_name: the name of the excel-type file. sheet_name: the name of the sheet of the excel file. Returns: child_scr_exp_sdq: A list that contains the detail information of each child. labs: The lab corresponds to each colume of the data. """ CHILDS_FILE_NAME = 'child_scr_exp_sdq.npy' CHILDS_DETAIL_LAB = GetChildDetailLab() # print(CHILDS_DETAIL_LAB) NOT_INT_LAB_INDEIES = [CHILDS_DETAIL_LAB.index('age_m'), CHILDS_DETAIL_LAB.index('scr_ave'), CHILDS_DETAIL_LAB.index('edu_ave'), CHILDS_DETAIL_LAB.index('edu_of_scr')] child_scr_exp_sdq = [] if (os.path.isfile(CHILDS_FILE_NAME)): with open(CHILDS_FILE_NAME, 'rb') as f: child_scr_exp_sdq = np.load(f) else: workBook = xlrd.open_workbook(file_name) bookSheet = workBook.sheet_by_name(sheet_name) # read from second row because of the first row has tabs for row in range(1, bookSheet.nrows): child_row = [] for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = str(cel.value) # tolerant the value error if val == '': val = '-1.0' except Exception as e: print(e) # because of the type is different try: if col in NOT_INT_LAB_INDEIES: val = float(val) else: # in Excel, if cel.value is 1, then str(cel.value) is # '1.0' val = val.split('.')[0] val = int(val) except Exception as e: print(e) val = -1 child_row.append(val) child_scr_exp_sdq.append(child_row) child_scr_exp_sdq = np.array(child_scr_exp_sdq) with open(CHILDS_FILE_NAME, 'wb') as f: np.save(f, child_scr_exp_sdq) return child_scr_exp_sdq, CHILDS_DETAIL_LABdef SplitDataSet(data_set, feature, cat): """ Split the data set to two column, that is feature and cat Arguments: data_set: the source data set. feature: the input vector cat: the correspond category. Returns: splited_data_set: self-explation. """ CHILDS_DETAIL_LAB = GetChildDetailLab() feature_index = CHILDS_DETAIL_LAB.index(feature) cat_index = CHILDS_DETAIL_LAB.index(cat) # print(feature_index, cat_index) return data_set[:, (feature_index, cat_index)]def CalChi2(data_set): """ Calculate the chi-square value and p-value corresponds to the data set. Arguments: data_set: the object data set. Returns: chi2: the chi-square value. p: the p-value. """ rows_number = len(set(data_set[:, -1])) columns_number = len(set(data_set[:, 0])) # print(rows_number, columns_number) counts = np.zeros((rows_number, columns_number)) for row in data_set: if row[-1] != -1: if row[0] != -1: try: counts[int(row[-1])][int(row[0])] += 1 except: pass # drop the row that all item is 0 del_row_index = [] for index, count in enumerate(counts): if not count.any(): del_row_index.append(index) counts = np.delete(counts, tuple(del_row_index), axis=0) # drop the column that all item is 0 del_col_index = [] for index, count in enumerate(counts.T): if not count.any(): del_col_index.append(index) counts = np.delete(counts, tuple(del_col_index), axis=1) # print(counts) # calculate the chi-square value and correspond p-value chi2, p, dof, excepted = stats.chi2_contingency(counts) return chi2, pdef ANOVATest(data_set): """ Implement a ANOVA test on the data set. Arguments: data_set: the object data set. Return: f: The computed F-value of the test. p: The associated p-value from the F-distribution. """ # Initial the three categories normal = [] critical = [] abnormal = [] for data in data_set: if data[0] != -1: if data[-1] == 0: normal.append(data[0]) elif data[-1] == 1: critical.append(data[0]) elif data[-1] == 2: abnormal.append(data[0]) f, p = stats.f_oneway(normal, critical, abnormal) return f, pdef GenerateCoffMatrix(data_set): """ Calculate the chi-square and p-value of each feature-category pair. Arguments: data_set: the source data set. Returns: coff_matrix: the final cofficient matrix. """ coff_matrix = &#123;&#125; CHILDS_FEATURE_LAB, CHILDS_CAT_LAB = GetChildDetailLab(combine=False) NOT_KEEP_FEATURE_LAB = ['tid', 'age_m', 'scr_ave', 'edu_ave', 'edu_of_scr'] for feature in CHILDS_FEATURE_LAB: if feature in NOT_KEEP_FEATURE_LAB: if feature != NOT_KEEP_FEATURE_LAB[0]: for cat in CHILDS_CAT_LAB: splited_data_set = SplitDataSet(data_set, feature, cat) # print(feature, cat) f, p = ANOVATest(splited_data_set) key = feature + '-' + cat coff_matrix[key] = (f, p) else: for cat in CHILDS_CAT_LAB: splited_data_set = SplitDataSet(data_set, feature, cat) # print(feature, cat) chi2, p = CalChi2(splited_data_set) key = feature + '-' + cat coff_matrix[key] = (chi2, p) return coff_matrixdef SiftRelativeFeature(coff_matrix, conf=1e-5): """ Sift the feature that satisfy the caonfident condition. Arguemnts: coff_matrix: the calculated cofficient matrix for all features and categories. conf: the confident. Returns: relative_feature_matrix: the satisfied feature and correspond category and chi2 and p-value. """ relative_feature_matrix = &#123;&#125; for key in coff_matrix.keys(): if coff_matrix[key][-1] &lt;= conf: relative_feature_matrix[key] = coff_matrix[key] return relative_feature_matrixdef WriteResult(coff_matrix, file_name='result.txt'): """ Write the result to file. Arguments: relative_feature_matrix: the satisfied feature and correspond category and chi2 and p-value. file_name: the result file name. """ # Sort sorted_coff_matrix = sorted(coff_matrix.items(), key=lambda item: item[1][-1], reverse=False) # print(sorted_coff_matrix) with open(file_name, 'w') as f: for item in sorted_coff_matrix: f.write(str(item)) f.write('\n')if __name__ == '__main__': data_set, labels = ReadChildInfoFromExcel() # splited_data_set = SplitDataSet(data_set, 'female', 'difficulties_cat') # chi2, p = CalChi2(splited_data_set) coff_matrix = GenerateCoffMatrix(data_set) # relative_feature = SiftRelativeFeature(coff_matrix, 1) WriteResult(coff_matrix)å› å­åˆ†æžå…ˆè¯´è¯´å› å­åˆ†æžä¸Žä¸»æˆåˆ†åˆ†æžçš„åŒºåˆ«ï¼Œä¸‹é¢çš„è¯å¼•è‡ªçŸ¥ä¹Ž(https://www.zhihu.com/question/24524693)å…·ä½“å®žçŽ°ä½¿ç”¨SPSSè½¯ä»¶è¿›è¡Œå®žçŽ°ï¼šæ“ä½œæ­¥éª¤å¦‚ä¸‹(å¼•è‡ª SPSSæ•°æ®åˆ†æžä»Žå…¥é—¨åˆ°ç²¾é€š-é™ˆèƒœå¯)ï¼šå®žéªŒç»“æžœå¦‚ä¸‹ï¼š2017.2.27 Updateså†³ç­–æ ‘1234567import pandas as pdfrom sklearn import treeimport numpy as npfrom sklearn.model_selection import cross_val_scorefrom IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = "all"Read data1data = pd.read_excel('./scr_SDQ.xlsx', sheetname='data', index_col=0).dropna().sort_index()Convert the data format1234float_columns = ['scr_ave', 'edu_ave','scr_of_edu']for column in data.columns: if column not in float_columns: data[column] = data[column].astype(int)Split the data123data_feature_with_edu_ave = data.ix[:, :'mediacontact'].drop('scr_of_edu', axis=1)data_feature_with_scr_of_edu = data.ix[:, :'mediacontact'].drop('edu_ave', axis=1)data_classes = data.ix[:, 'emo_cat':]12# dtree = tree.DecisionTreeClassifier(min_samples_leaf=500)# cross_val_score(dtree,data_feature_with_edu_ave, data_classes['difficulties_cat'], cv=10)array([ 0.64356436, 0.64356436, 0.64391855, 0.64391855, 0.64407713, 0.64407713, 0.64407713, 0.64407713, 0.64407713, 0.64407713]) difficulties_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['difficulties_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['difficulties_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_diff.pngwith open('dtree_with_edu_ave_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_diff.pngwith open('dtree_with_scr_of_edu_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.553782medu_newcat0.217218age_m0.085820income_newcat0.048912edu_ave0.029740female0.026161onlychild0.021336mediacontact0.017030divorce0.000000scr_h_cat0.000000mediacoview0.000000Impscr_ave0.525587medu_newcat0.213067age_m0.084708scr_of_edu0.080575mediacontact0.029232female0.025661onlychild0.020929income_newcat0.020240divorce0.000000scr_h_cat0.000000mediacoview0.000000emo_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['emo_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['emo_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_emo.pngwith open('dtree_with_edu_ave_emo.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_emo.pngwith open('dtree_with_scr_of_edu_emo.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.521694medu_newcat0.159798income_newcat0.147978edu_ave0.083233age_m0.040750female0.023781mediacontact0.019127mediacoview0.003639onlychild0.000000divorce0.000000scr_h_cat0.000000Impscr_ave0.520711income_newcat0.155111medu_newcat0.146762scr_of_edu0.115745age_m0.023767female0.021924mediacontact0.011410mediacoview0.004571onlychild0.000000divorce0.000000scr_h_cat0.000000con_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['con_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['con_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_con.pngwith open('dtree_with_edu_ave_con.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_con.pngwith open('dtree_with_scr_of_edu_con.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.562325medu_newcat0.098819mediacontact0.095089edu_ave0.077572female0.064213income_newcat0.056334age_m0.023696onlychild0.013513mediacoview0.008440divorce0.000000scr_h_cat0.000000Impscr_ave0.511282scr_of_edu0.110155medu_newcat0.097473mediacontact0.093793female0.082843income_newcat0.055566age_m0.035552onlychild0.013335divorce0.000000scr_h_cat0.000000mediacoview0.000000hyp_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['hyp_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['hyp_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_hyp.pngwith open('dtree_with_edu_ave_hyp.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_hyp.pngwith open('dtree_with_scr_of_edu_hyp.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impmedu_newcat0.343448scr_ave0.265652onlychild0.165487edu_ave0.076194income_newcat0.054466age_m0.046774mediacontact0.020992mediacoview0.015532female0.011454divorce0.000000scr_h_cat0.000000Impmedu_newcat0.335192scr_ave0.257218onlychild0.161509scr_of_edu0.105471age_m0.065618income_newcat0.033129mediacontact0.022542mediacoview0.012901female0.006421divorce0.000000scr_h_cat0.000000pee_cat1234567891011121314151617dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['pee_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['pee_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_pee.pngwith open('dtree_with_edu_ave_pee.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_pee.pngwith open('dtree_with_scr_of_edu_pee.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impscr_ave0.441246income_newcat0.210646female0.152568age_m0.151888medu_newcat0.033530mediacoview0.009625edu_ave0.000498onlychild0.000000divorce0.000000scr_h_cat0.000000mediacontact0.000000Impscr_ave0.430946income_newcat0.200188age_m0.151286female0.139104medu_newcat0.033289scr_of_edu0.033259mediacoview0.011927onlychild0.000000divorce0.000000scr_h_cat0.000000mediacontact0.000000pro_cat12345678910111213141516dtree_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_edu_ave = dtree_with_edu_ave.fit(data_feature_with_edu_ave, data_classes['pro_cat'])pd.DataFrame(dtree_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_with_scr_of_edu = dtree_with_scr_of_edu.fit(data_feature_with_scr_of_edu, data_classes['pro_cat'])pd.DataFrame(dtree_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_with_edu_ave_pro.pngwith open('dtree_with_edu_ave_pro.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_edu_ave, out_file=dot_file, feature_names=data_feature_with_edu_ave.columns)# dtree_with_scr_of_edu_pro.pngwith open('dtree_with_scr_of_edu_pro.dot', 'w') as dot_file: tree.export_graphviz(dtree_with_scr_of_edu, out_file=dot_file, feature_names=data_feature_with_scr_of_edu.columns)Impfemale0.295389scr_ave0.252091mediacontact0.145010age_m0.137453income_newcat0.085421edu_ave0.062697medu_newcat0.021938onlychild0.000000divorce0.000000scr_h_cat0.000000mediacoview0.000000Impfemale0.299316scr_ave0.204496mediacontact0.131972age_m0.125169scr_of_edu0.111909income_newcat0.080072medu_newcat0.047066onlychild0.000000divorce0.000000scr_h_cat0.000000mediacoview0.000000Convert continuous variables to categorical variables1234data_category = data.copy()for column in data_category.columns: if column in float_columns: data_category[column] = pd.cut(data_category[column], 10, labels=np.arange(10))123data_cate_feature_with_edu_ave = data_category.ix[:, :'mediacontact'].drop('scr_of_edu', axis=1)data_cate_feature_with_scr_of_edu = data_category.ix[:, :'mediacontact'].drop('edu_ave', axis=1)data_cate_classes = data.ix[:, 'emo_cat':]12dtree_cate = tree.DecisionTreeClassifier(min_samples_leaf=50)cross_val_score(dtree_cate,data_cate_feature_with_edu_ave, data_cate_classes['difficulties_cat'], cv=10).sum() / 100.63948514409153423 12345678910111213141516dtree_cate_with_edu_ave = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_cate_with_edu_ave = dtree_with_edu_ave.fit(data_cate_feature_with_edu_ave, data_cate_classes['difficulties_cat'])pd.DataFrame(dtree_cate_with_edu_ave.feature_importances_, columns = ["Imp"], index = data_cate_feature_with_edu_ave.columns).sort_values(by='Imp', ascending = False)dtree_cate_with_scr_of_edu = tree.DecisionTreeClassifier(min_samples_leaf=500)dtree_cate_with_scr_of_edu = dtree_cate_with_scr_of_edu.fit(data_cate_feature_with_scr_of_edu, data_cate_classes['difficulties_cat'])pd.DataFrame(dtree_cate_with_scr_of_edu.feature_importances_, columns = ["Imp"], index = data_cate_feature_with_scr_of_edu.columns).sort_values(by='Imp', ascending = False)# dtree_cate_with_edu_ave_diff.pngwith open('dtree_cate_with_edu_ave_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_cate_with_edu_ave, out_file=dot_file, feature_names=data_cate_feature_with_edu_ave.columns)# dtree_cate_with_scr_of_edu_diff.pngwith open('dtree_cate_with_scr_of_edu_diff.dot', 'w') as dot_file: tree.export_graphviz(dtree_cate_with_scr_of_edu, out_file=dot_file, feature_names=data_cate_feature_with_scr_of_edu.columns)Impscr_h_cat0.528976medu_newcat0.221779age_m0.092168income_newcat0.057032onlychild0.034724mediacontact0.027193female0.021036scr_ave0.011653mediacoview0.005439divorce0.000000edu_ave0.000000Impscr_h_cat0.507678medu_newcat0.215210age_m0.089438scr_of_edu0.042832mediacontact0.041463income_newcat0.035742onlychild0.033695female0.020413scr_ave0.013527divorce0.000000mediacoview0.000000]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Get Pois uses God-map apis]]></title>
      <url>%2F2017%2F02%2F21%2FGet-Pois-uses-God-map-apis%2F</url>
      <content type="text"><![CDATA[ä¼ªä»£ç å¦‚ä¸‹ï¼š12345ä»ŽExcelæ–‡ä»¶ä¸­è¯»å‡ºæ•°æ®å¯¹äºŽæ¯ä¸€ä¸ªhouse: æå–å‡ºå…¶locationå­—æ®µï¼ˆç»çº¬åº¦ï¼‰ å°†locationå­—æ®µä½œä¸ºè¾“å…¥å‚æ•°ä¼ ç»™map api å°†è¿”å›žå€¼è¿›è¡Œé€‚å½“ç­›é€‰æœ€åŽå­˜å…¥åŽŸæ•°æ®é›†ä¸­ä»£ç å®žçŽ°å¦‚ä¸‹123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import osimport xlrdimport pickleimport requestsdef ReadHousesInfoFromExcel( file_name='houses_nadrop.xls', sheet_name='å°åŒºä¿¡æ¯'): """ Read the houses detail information from the excel-type file. Arguments: file_name: the name of the excel-type file. sheet_name: the name of the sheet of the excel file. Returns: houses: A dict that contains the detail information of each house. """ HOUSES_FILE_NAME = 'houses.pkl' HOUSES_DETAIL_TAB = ['name', 'address', 'property_category', 'area', 'avg_price', 'location', 'property_costs', 'volume_rate', 'green_rate'] houses = [] if (os.path.isfile(HOUSES_FILE_NAME)): with open(HOUSES_FILE_NAME, 'rb') as f: houses = pickle.load(f) else: workBook = xlrd.open_workbook(file_name) bookSheet = workBook.sheet_by_name(sheet_name) # read from second row because of the first row has tabs for row in range(1, bookSheet.nrows): house = &#123;&#125; for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = cel.value except: pass val = str(val) house[HOUSES_DETAIL_TAB[col]] = val houses.append(house) with open(HOUSES_FILE_NAME, 'wb') as f: pickle.dump(houses, f) return housesdef Geocode(location, poi_type): """ A tool that call the God-Map api. Arguments: location: The location of house. poi_type: The poi type. Returns: answer: The JSON-type data that contains pois infomation. """ location = str(location).strip() parameters = &#123;'location': location, 'key': 'e798a5bfb344a09977b79552ae415974', 'types': poi_type, 'offset': 10, 'page': 1, 'extensions': 'base'&#125; base = 'http://restapi.amap.com/v3/place/around' try: response = requests.get(base, parameters) answer = response.json() except Exception as e: print('error!', e) answer = 'null' finally: pass return answerdef GetPOI(houses): """ Get the pois information of the houses according to the location. Arguments: houses: The house detail information. Returns: houses_with_pois: The house detail information that contains the pois information. """ POI_TYPE_LAB = ['subway_station', 'bus_station', 'parking_lot', 'primary_school', 'secondary_school', 'university', 'mall', 'park'] POI_TYPE_CODE = ['150500', '150700', '150904', '141203', '141202', '141201', '060100', '110101'] KEEP_INFO_LAB = ['name', 'location', 'distance'] NO_INFO_NOW = '-' SIZE = len(houses) houses_with_pois = houses.copy() count = 0 for house in houses_with_pois: count = count + 1 if count % 100 == 0: print(count, '', SIZE) house['pois'] = &#123;&#125; for poi_type_index in range(len(POI_TYPE_LAB)): poi_info_json = Geocode(house['location'], POI_TYPE_CODE[poi_type_index]) if poi_info_json == 'null' or poi_info_json['pois'] is None: house['pois'][POI_TYPE_LAB[poi_type_index]] = NO_INFO_NOW else: house['pois'][POI_TYPE_LAB[poi_type_index]] = [] for poi in poi_info_json['pois']: pois_without_useless = &#123;&#125; for key in poi.keys(): if key in KEEP_INFO_LAB: pois_without_useless[key] = poi[key] house['pois'][POI_TYPE_LAB[poi_type_index]].append( pois_without_useless) # return houses_with_pois return houses_with_poisif __name__ == '__main__': houses = ReadHousesInfoFromExcel() # answer = Geocode(houses[0]['location'], '150905') houses_with_pois = GetPOI(houses)æ€»ç»“ä¸€ä¸‹æœ‰å‡ ä¸ªæ³¨æ„ç‚¹ï¼šä¼ ç»™parametersçš„locationå‚æ•°çš„æ ¼å¼ä¸€å®šè¦è§„èŒƒï¼Œå‰åŽéƒ½ä¸èƒ½æœ‰ç©ºæ ¼forå¾ªçŽ¯ä¸­ä¸èƒ½æ”¹å˜å­—å…¸çš„å¤§å°ï¼Œè¿™é‡Œçš„å¤§å°ä¸ä»…æŒ‡å…¶å…ƒç´ çš„æ•°ç›®ï¼Œä¹ŸåŒ…æ‹¬å…¶æ€»å ç”¨ç©ºé—´çš„å¤§å°æ³¨æ„pickleçš„ç”¨æ³•ä»ŽExcelä¸­è¯»å‡ºçš„å†…å®¹è¦è½¬æˆstræ ¼å¼æ•´ä¸ªè¿‡ç¨‹ååˆ†æ¸…æ™°æ˜Žäº†ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ç»†èŠ‚é—®é¢˜]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python data analysis-Learning note-Ch02]]></title>
      <url>%2F2017%2F02%2F19%2FPython%20data%20analysis-Learning%20notes-ch02%2F</url>
      <content type="text"><![CDATA[åˆ©ç”¨Pythonå†…ç½®çš„JSONæ¨¡å—å¯¹æ•°æ®è¿›è¡Œè§£æžå¹¶è½¬åŒ–ä¸ºå­—å…¸æ•°æ®å¦‚ä¸‹ï¼š123456'&#123; "a": "Mozilla\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\/535.11 (KHTML, like Gecko)Chrome\\/17.0.963.78 Safari\\/535.11", "c": "US", "nk": 1, "tz": "America\\/New_York", "gr":"MA", "g": "A6qOVH", "h": "wfLQtf", "l": "orofrog", "al": "en-US,en;q=0.8", "hh": "1.usa.gov","r": "http:\\/\\/www.facebook.com\\/l\\/7AQEFzjSi\\/1.usa.gov\\/wfLQtf", "u":"http:\\/\\/www.ncbi.nlm.nih.gov\\/pubmed\\/22415991", "t": 1331923247, "hc": 1331822918,"cy": "Danvers", "ll": [ 42.576698, -70.954903 ] &#125;\n'æ ¸å¿ƒä»£ç ï¼š123import jsonpath = 'ch02/usagov_bitly_data2012-03-16-1331923249.txt'records = [json.loads(line) for line in open(path)]123456789101112131415161718records[0]-----------------------------------&#123;'a': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.78 Safari/535.11', 'al': 'en-US,en;q=0.8', 'c': 'US', 'cy': 'Danvers', 'g': 'A6qOVH', 'gr': 'MA', 'h': 'wfLQtf', 'hc': 1331822918, 'hh': '1.usa.gov', 'l': 'orofrog', 'll': [42.576698, -70.954903], 'nk': 1, 'r': 'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf', 't': 1331923247, 'tz': 'America/New_York', 'u': 'http://www.ncbi.nlm.nih.gov/pubmed/22415991'&#125;å¯¹æ—¶åŒºå­—æ®µè¿›è¡Œè®¡æ•°ï¼ˆpure python vs. pandasï¼‰é¦–å…ˆä»Žè®°å½•ä¸­æå–æ—¶åŒºå­—æ®µå¹¶ä¸”æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨ä¸­1time_zones = [rec['tz'] for rec in records if 'tz' in rec]123456789101112time_zones[:10]-----------------------------------['America/New_York', 'America/Denver', 'America/New_York', 'America/Sao_Paulo', 'America/New_York', 'America/New_York', 'Europe/Warsaw', '', '', '']ä½¿ç”¨çº¯ç²¹çš„pythonè¿›è¡Œè®¡æ•°12345678def get_counts(sequence): counts = &#123;&#125; for x in sequence: if x in counts: counts[x] += 1 else: counts[x] = 1 return countsä½¿ç”¨ä¸‹åˆ—æ–¹æ³•æ›´åŠ ç®€æ´1234567from collections import defaultdictdef get_counts2(sequence): counts = defaultdict(int) # values will initialize to 0 for x in sequence: counts[x] += 1 return countså¦‚æžœéœ€è¦è¿”å›žå‰åä½çš„æ—¶åŒºåŠå…¶è®¡æ•°å€¼1234def top_counts(count_dict, n=10): value_key_pairs = [(count, tz) for tz, count in count_dict.items()] value_key_pairs.sort() return value_key_pairs[-n:]123456789101112top_counts(counts)--------------------------------------[(33, 'America/Sao_Paulo'), (35, 'Europe/Madrid'), (36, 'Pacific/Honolulu'), (37, 'Asia/Tokyo'), (74, 'Europe/London'), (191, 'America/Denver'), (382, 'America/Los_Angeles'), (400, 'America/Chicago'), (521, ''), (1251, 'America/New_York')]å¯ä»¥ä½¿ç”¨pythonè‡ªå¸¦çš„åº“1from collections import Counter1counts = Counter(time_zones)123456789101112counts.most_common(10)--------------------------------[('America/New_York', 1251), ('', 521), ('America/Chicago', 400), ('America/Los_Angeles', 382), ('America/Denver', 191), ('Europe/London', 74), ('Asia/Tokyo', 37), ('Pacific/Honolulu', 36), ('Europe/Madrid', 35), ('America/Sao_Paulo', 33)]ä½¿ç”¨pandasè¿›è¡Œç›¸åŒçš„ä»»åŠ¡pandasä¸­ä¸»è¦çš„æ•°æ®ç»“æž„æ˜¯DataFrameï¼Œ ä½œç”¨æ˜¯å°†æ•°æ®è¡¨ç¤ºæˆè¡¨æ ¼12345from pandas import DataFrame, Seriesimport pandas as pdframe = DataFrame(records)frame12345678910111213frame['tz'][:10]-------------------------------0 America/New_York1 America/Denver2 America/New_York3 America/Sao_Paulo4 America/New_York5 America/New_York6 Europe/Warsaw7 8 9 Name: tz, dtype: objectè®¡æ•°Â·1234567891011121314tz_counts = frame['tz'].value_counts()tz_counts[:10]--------------------------------------------------America/New_York 1251 521America/Chicago 400America/Los_Angeles 382America/Denver 191Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35America/Sao_Paulo 33Name: tz, dtype: int64å¡«è¡¥ç¼ºå¤±å€¼ä»¥åŠæœªçŸ¥å€¼12345678910111213141516clean_tz = frame['tz'].fillna('Missing')clean_tz[clean_tz == ''] = 'Unknown'tz_counts = clean_tz.value_counts()tz_counts[:10]----------------------------------------------America/New_York 1251Unknown 521America/Chicago 400America/Los_Angeles 382America/Denver 191Missing 120Europe/London 74Asia/Tokyo 37Pacific/Honolulu 36Europe/Madrid 35Name: tz, dtype: int64ç”»ä¸ªå›¾å±•ç¤ºä¸€ä¸‹12plt.figure(figsize=(10, 4))tz_counts[:10].plot(kind='barh', rot=0)ä¸‹é¢æˆ‘ä»¬å¯¹ç”¨æˆ·ä½¿ç”¨çš„æµè§ˆå™¨çš„ä¿¡æ¯åšä¸€äº›æ“ä½œSeriesåº”è¯¥ä»£è¡¨çš„æ˜¯DataFrameä¸­çš„ä¸€åˆ—123456789results = Series([x.split()[0] for x in frame.a.dropna()])results[:5]---------------------------------------------------0 Mozilla/5.01 GoogleMaps/RochesterNY2 Mozilla/4.03 Mozilla/5.04 Mozilla/5.0dtype: objectåŒæ ·å¯ä»¥è¿›è¡Œè®¡æ•°1234567891011results.value_counts()[:8]-----------------------------------------Mozilla/5.0 2594Mozilla/4.0 601GoogleMaps/RochesterNY 121Opera/9.80 34TEST_INTERNET_AGENT 24GoogleProducer 21Mozilla/6.0 5BlackBerry8520/5.0.0.681 4dtype: int64æ ¹æ®Windowså’ŒNon-Windowsç”¨æˆ·è¿›è¡Œæ—¶åŒºçš„åˆ†ç»„æ“ä½œ1cframe = frame[frame.a.notnull()]123456operating_system = np.where(cframe['a'].str.contains('Windows'), 'Windows', 'Not Windows')operating_system[:5]-----------------------------------------------------------------array(['Windows', 'Not Windows', 'Windows', 'Not Windows', 'Windows'], dtype='&lt;U11')1by_tz_os = cframe.groupby(['tz', operating_system])æ¥çœ‹çœ‹è¿™ä¸ªby_tz_osé•¿ä»€ä¹ˆæ ·1by_tz_os.size()å†æ¥çœ‹çœ‹unstack()çš„ç‚«é…·æ•ˆæžœæŽ’ä¸‹åºï¼Œ çœ‹çœ‹æŽ’åå¤šå°‘12345678910111213141516# Use to sort in ascending orderindexer = agg_counts.sum(1).argsort()indexer[:10]------------------------------------------------tz 24Africa/Cairo 20Africa/Casablanca 21Africa/Ceuta 92Africa/Johannesburg 87Africa/Lusaka 53America/Anchorage 54America/Argentina/Buenos_Aires 57America/Argentina/Cordoba 26America/Argentina/Mendoza 55dtype: int64å–å‡ºå‰åçš„æ¥çœ‹çœ‹12count_subset = agg_counts.take(indexer)[-10:]count_subsetåŒæ ·ç”»ä¸ªå›¾1count_subset.plot(kind='barh', stacked=True)çœ‹çœ‹ä¸¤ä¸ªç±»åˆ«æ‰€å çš„æ¯”ä¾‹æ˜¯å¤šå°‘12normed_subset = count_subset.div(count_subset.sum(1), axis=0)normed_subset.plot(kind='barh', stacked=True)ç”µå½±è¯„åˆ†æ•°æ®è¡¨è¿žæŽ¥æ“ä½œ123456789101112131415import pandas as pdimport osencoding = 'latin1'upath = os.path.expanduser('ch02/movielens/users.dat')rpath = os.path.expanduser('ch02/movielens/ratings.dat')mpath = os.path.expanduser('ch02/movielens/movies.dat')unames = ['user_id', 'gender', 'age', 'occupation', 'zip']rnames = ['user_id', 'movie_id', 'rating', 'timestamp']mnames = ['movie_id', 'title', 'genres']users = pd.read_csv(upath, sep='::', header=None, names=unames, encoding=encoding)ratings = pd.read_csv(rpath, sep='::', header=None, names=rnames, encoding=encoding)movies = pd.read_csv(mpath, sep='::', header=None, names=mnames, encoding=encoding)çœ‹çœ‹æ•°æ®é•¿ä»€ä¹ˆæ ·1users[:5]1ratings[:5]1movies[:5]å¤šè¡¨è¿žæŽ¥12data = pd.merge(pd.merge(ratings, users), movies)data12345678910111213data.ix[0]--------------------------------------------user_id 1movie_id 1193rating 5timestamp 978300760gender Fage 1occupation 10zip 48067title One Flew Over the Cuckoo's Nest (1975)genres DramaName: 0, dtype: objectæ ¹æ®æ€§åˆ«è®¡ç®—æ¯éƒ¨ç”µå½±çš„å¹³å‡è¯„åˆ†123mean_ratings = data.pivot_table('rating', index='title', columns='gender', aggfunc='mean')mean_ratings[:5]è¿‡æ»¤æŽ‰è¯„åˆ†æ•°å°äºŽ250çš„ç”µå½±1ratings_by_title = data.groupby('title').size()123456789ratings_by_title[:5]-----------------------------------------title$1,000,000 Duck (1971) 37'Night Mother (1986) 70'Til There Was You (1997) 52'burbs, The (1989) 303...And Justice for All (1979) 199dtype: int641active_titles = ratings_by_title.index[ratings_by_title &gt;= 250]12345678active_titles[:10]-----------------------------------------Index([''burbs, The (1989)', '10 Things I Hate About You (1999)', '101 Dalmatians (1961)', '101 Dalmatians (1996)', '12 Angry Men (1957)', '13th Warrior, The (1999)', '2 Days in the Valley (1996)', '20,000 Leagues Under the Sea (1954)', '2001: A Space Odyssey (1968)', '2010 (1984)'], dtype='object', name='title')ixåº”è¯¥æ˜¯ä¸€ä¸ªäº¤é›†æ“ä½œ12mean_ratings = mean_ratings.ix[active_titles]mean_ratingsæŒ‰ç…§å¥³æ€§æœ€å–œæ¬¢çš„ç”µå½±è¿›è¡Œé™åºæŽ’åº12top_female_ratings = mean_ratings.sort_values(by='F', ascending=False)top_female_ratings[:10]â€‹US Baby Names 1880-2010123import pandas as pdnames1880 = pd.read_csv('ch02/names/yob1880.txt', names=['name', 'sex', 'births'])names1880æŠŠæ‰€æœ‰å¹´ä»½çš„æ•°æ®åˆå¹¶ä¸€ä¸‹123456789101112131415# 2010 is the last available year right nowyears = range(1880, 2011)pieces = []columns = ['name', 'sex', 'births']for year in years: path = 'ch02/names/yob%d.txt' % year frame = pd.read_csv(path, names=columns) frame['year'] = year pieces.append(frame)# Concatenate everything into a single DataFramenames = pd.concat(pieces, ignore_index=True)è¿›è¡Œèšåˆæ“ä½œ12total_births = names.pivot_table('births', index='year', columns='sex', aggfunc=sum)1total_births.tail()è®¡ç®—ä¸€ä¸‹æ¯ä¸ªåå­—çš„å‡ºç”Ÿæ¯”ä¾‹1234567def add_prop(group): # Integer division floors births = group.births.astype(float) group['prop'] = births / births.sum() return groupnames = names.groupby(['year', 'sex']).apply(add_prop)1namesè¿›è¡Œä¸€ä¸‹æœ‰æ•ˆæ€§æ£€æŸ¥123np.allclose(names.groupby(['year', 'sex']).prop.sum(), 1)--------------------------------------------Trueç­›é€‰å‡ºæ¯ä¸€å¯¹year/sexä¸‹æ€»æ•°å‰1000çš„åå­—1234def get_top1000(group): return group.sort_values(by='births', ascending=False)[:1000]grouped = names.groupby(['year', 'sex'])top1000 = grouped.apply(get_top1000)åŠ ä¸ªç´¢å¼•ï¼Œç»“åˆäº†numpy1top1000.index = np.arange(len(top1000))Analyzing naming trendså°†æ•°æ®åˆ†ä¸ºç”·å¥³12boys = top1000[top1000.sex == 'M']girls = top1000[top1000.sex == 'F']è®¡ç®—æ¯ä¸€å¹´æ¯ä¸ªåå­—çš„å‡ºç”Ÿæ€»æ•°123total_births = top1000.pivot_table('births', index='year', columns='name', aggfunc=sum)total_birthsé€‰å‡ºå‡ ä¸ªåå­—çœ‹çœ‹æ€»æ•°éšå¹´ä»½çš„å˜åŒ–æƒ…å†µ123subset = total_births[['John', 'Harry', 'Mary', 'Marilyn']]subset.plot(subplots=True, figsize=(12, 10), grid=False, title="Number of births per year")Measuring the increase in naming diversityé€šè¿‡ç»Ÿè®¡å‰1000é¡¹åå­—æ‰€å çš„æ¯”ä¾‹æ¥åˆ¤æ–­å¤šæ ·æ€§çš„å˜åŒ–1234table = top1000.pivot_table('prop', index='year', columns='sex', aggfunc=sum)table.plot(title='Sum of table1000.prop by year and sex', yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10))å¦ä¸€ç§æ–¹æ³•ï¼Œè®¡ç®—å å‡ºç”Ÿäººæ•°50%çš„åå­—çš„æ•°é‡ä¹Ÿå³ä»Žå¼€å§‹ç´¯åŠ ï¼Œçœ‹åŠ åˆ°ç¬¬å‡ ä¸ªåå­—æ—¶æ‰€å æ¯”ä¾‹ä¸º50%å…ˆæ¥çœ‹çœ‹2010å¹´çš„ç”·å­©1df = boys[boys.year == 2010]1234567891011121314prop_cumsum = df.sort_index(by='prop', ascending=False).prop.cumsum()prop_cumsum[:10]--------------------------------------------------260877 0.011523260878 0.020934260879 0.029959260880 0.038930260881 0.047817260882 0.056579260883 0.065155260884 0.073414260885 0.081528260886 0.089621Name: prop, dtype: float64çœ‹æ¥æ˜¯ç¬¬116ä¸ªï¼Œä¸è¿‡åºå·ä»Ž0å¼€å§‹ï¼Œåº”è¯¥æ˜¯117123prop_cumsum.values.searchsorted(0.5)---------------------------------------------------116å†æ¥çœ‹çœ‹1900å¹´çš„ç”·å­©å„¿12345df = boys[boys.year == 1900]in1900 = df.sort_index(by='prop', ascending=False).prop.cumsum()in1900.values.searchsorted(0.5) + 1---------------------------------------------------25æ‰€ä»¥è¿™æ ·åšæ˜¯å¯è¡Œçš„æŠŠç›¸åŒçš„æ“ä½œèµ‹äºˆæ•´ä¸ªæ•°æ®é›†123456def get_quantile_count(group, q=0.5): group = group.sort_values(by='prop', ascending=False) return group.prop.cumsum().values.searchsorted(q) + 1diversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)diversity = diversity.unstack('sex')diversity.head()1diversity.plot(title="Number of popular names in top 50%")The â€œLast letterâ€ Revolutionå–å‡ºæ¯ä¸ªåå­—å¯¹åº”çš„æœ€åŽä¸€ä¸ªå­—æ¯ï¼ŒåŒæ—¶åºå·å¯¹åº”1234567# extract last letter from name columnget_last_letter = lambda x: x[-1]last_letters = names.name.map(get_last_letter)last_letters.name = 'last_letter'table = names.pivot_table('births', index=last_letters, columns=['sex', 'year'], aggfunc=sum)å•ç‹¬å–å‡ºä¸‰å¹´çš„æ¥çœ‹çœ‹12subtable = table.reindex(columns=[1910, 1960, 2010], level='year')subtable.head()è®¡ç®—ä¸€ä¸‹å­—æ¯æ¯”ä¾‹12345678910subtable.sum()-------------------------------------sex yearF 1910 396416.0 1960 2022062.0 2010 1759010.0M 1910 194198.0 1960 2132588.0 2010 1898382.0dtype: float641letter_prop = subtable / subtable.sum().astype(float)123456import matplotlib.pyplot as pltfig, axes = plt.subplots(2, 1, figsize=(10, 8))letter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')letter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female', legend=False)æœ€åŽçœ‹ä¸€ä¸‹æ‰€æœ‰çš„å¹´ä»½å¹¶ç”Ÿæˆä¸€ä¸ªè¶‹åŠ¿å›¾12letter_prop = table / table.sum().astype(float)dny_ts = letter_prop.ix[['d', 'n', 'y'], 'M'].T1dny_ts.plot()Boy names that became girl names (and vice versa)ä»¥leslå¼€å¤´çš„åå­—ä¸ºä¾‹123456all_names = top1000.name.unique()mask = np.array(['lesl' in x.lower() for x in all_names])lesley_like = all_names[mask]lesley_like----------------------------------------------array(['Leslie', 'Lesley', 'Leslee', 'Lesli', 'Lesly'], dtype=object)ä»ŽåŽŸæ•°æ®é›†ä¸­ç­›é€‰å‡ºæ¥12345678910filtered = top1000[top1000.name.isin(lesley_like)]filtered.groupby('name').births.sum()----------------------------------------------nameLeslee 1082Lesley 35022Lesli 929Leslie 370429Lesly 10067Name: births, dtype: int64åšä¸€ä¸‹èšåˆæ“ä½œå¹¶è®¡ç®—æ¯”ä¾‹1234table = filtered.pivot_table('births', index='year', columns='sex', aggfunc='sum')table = table.div(table.sum(1), axis=0)table.tail()çœ‹ä¸€ä¸‹è¶‹åŠ¿1table.plot(style=&#123;'M': 'k-', 'F': 'k--'&#125;)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Spider the house infomation and save to excel file]]></title>
      <url>%2F2017%2F02%2F18%2FSpider-the-house-infomation-and-save-to-excel-file%2F</url>
      <content type="text"><![CDATA[æ•°æ®æ¥æºhttp://sh.fang.com/é¡¹ç›®ç›®æ ‡çˆ¬å–äºŒæ‰‹æˆ¿ä¿¡æ¯ä¸­çš„å°åŒºä¿¡æ¯å®žçŽ°æ­¥éª¤ã€1ã€‘çˆ¬å–å°åŒºä¿¡æ¯ï¼ˆæ ¸å¿ƒä»£ç ï¼Œä¸‹åŒï¼‰123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150""" &lt;A spider to crawl the house information.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;. """class HouseSpider(scrapy.Spider): name = "house" head = "http://esf.sh.fang.com" allowed_domains = ["sh.fang.com"] start_urls = [ "http://esf.sh.fang.com/housing/" ] # å„åŒºå¯¹åº”çš„ç¼–å·ï¼ˆç”±äºŽä¸§å¿ƒç—…ç‹‚çš„urlï¼‰ area_map = &#123;25: 'pudong', 18: 'minhang', 19: 'xuhui', 30: 'baoshan', 28: 'putuo', 20: 'changning', 26: 'yangpu', 586: 'songjiang', 29: 'jiading', 23: 'hongkou', 27: 'zhabei', 21: 'jingan', 24: 'huangpu', 22: 'luwan', 31: 'qingpu', 32: 'fengxian', 35: 'jinshan', 996: 'chongming'&#125; estate_to_area_map = &#123;&#125; seperator = '=\n' def __init__(self): for key, value in self.area_map.items(): self.estate_to_area_map[key] = [] # print(self.estate_to_area_map) def parse(self, response): # è§£æžå‡ºä¸Šæµ·å¸‚å„åŒºçš„åœ°å€ area_lis = response.xpath('//*[@id="houselist_B03_02"]/div[1]') for a in area_lis.xpath('./a'): # areas = items.AreaItem() # areas['name'] = a.xpath('text()').extract()[0] yield Request(self.head + a.xpath('@href').extract()[0], callback=self.parse_area) # print(a.xpath('text()').extract()[0]) def parse_area(self, response): # ç¡®å®šresponseæ¥æºäºŽå“ªä¸€ä¸ªåŒº area_index = str(response).split('/')[-2].split('_')[0] if area_index == '': return else: # è§£æžå‡ºå„åŒºä¸­å°åŒºçš„è¯¦æƒ…é¡µé¢åœ°å€ detail_str = 'xiangqing' estate_list = response.xpath('/html/body/div[4]/div[5]/div[4]') for a in estate_list.xpath('.//a[@class="plotTit"]'): estate_url = a.xpath('@href').extract()[0] if estate_url.find('esf') != -1: estate_url = estate_url.replace('esf', detail_str) else: estate_url = estate_url + detail_str if estate_url.find('http') != -1: # print(estate_url) self.estate_to_area_map[int(area_index)].append(estate_url) # print(len(self.estate_to_area_map[int(area_index)])) next_page = response.xpath('//*[@id="PageControl1_hlk_next"]') if len(next_page) != 0: yield Request(self.head + next_page.xpath('@href').extract()[0], callback=self.parse_area) else: # print(len(self.estate_to_area_map[int(area_index)])) for url in self.estate_to_area_map[int(area_index)]: request = Request(url, callback=self.parse_house, dont_filter=True) request.meta['index'] = int(area_index) yield request def parse_house(self, response): flag = 0 area_index = response.meta['index'] area_name = self.area_map[area_index] filename = area_name + '.txt' # print(response.xpath('/html')) # è¯¦æƒ…é¡µé¢å­˜åœ¨ä¸¤ç§ï¼Œå› æ­¤åˆ†æƒ…å†µè®¨è®º house_name = response.xpath( '/html/body/div[4]/div[2]/div[2]/h1/a/text()') if len(house_name) == 0: # house_name = response.xpath( # '/html/body/div[1]/div[3]/div[2]/h1/a/text()') # flag = 1 return house_name = house_name.extract()[0] # æ¸…æ´å°åŒºå house_name = re.sub(r'å°åŒºç½‘', '', house_name) result_str = 'ã€å°åŒºåç§°ã€‘' + house_name + '\n' if flag == 0: avg_price_xpath = response.xpath( '/html/body/div[4]/div[4]/div[1]/div[1]/dl[1]/dd/span/text()') avg_price = avg_price_xpath.extract()[0] result_str = result_str + 'ã€å¹³å‡ä»·æ ¼ã€‘' + avg_price + '\n' detail_block_list = response.xpath( '/html/body/div[4]/div[4]/div[1]') for headline in detail_block_list.xpath('.//h3'): head_str = headline.xpath('./text()').extract()[0] if head_str == 'åŸºæœ¬ä¿¡æ¯': result_str = result_str + \ 'ã€' + \ head_str + 'ã€‘\n' for item in headline.xpath( '../../div[@class="inforwrap clearfix"]/dl/dd'): if len(item.xpath('./strong/text()')) != 0: if len(item.xpath('./text()')) != 0: result_str = result_str + \ item.xpath( './strong/text()').extract()[0] result_str = result_str + \ item.xpath('./text()').extract()[0] + '\n' # print(result_str) # elif head_str == 'äº¤é€šçŠ¶å†µ': # result_str = result_str + \ # 'ã€' + \ # head_str + 'ã€‘\n' # tempstr = headline.xpath( # '../../div[@class="inforwrap clearfix"]/dl/dt/text()').extract()[0] # result_str = result_str + tempstr + '\n' # # print(result_str) # elif head_str == 'å‘¨è¾¹ä¿¡æ¯': # result_str = result_str + \ # 'ã€' + \ # head_str + 'ã€‘\n' # for item in headline.xpath( # '../../div[@class="inforwrap clearfix"]/dl/dt'): # result_str = result_str + \ # item.xpath('./text()').extract()[0] + '\n' # # print(result_str) elif head_str == 'å°±è¿‘æ¥¼ç¾¤': result_str = result_str + \ 'ã€' + \ head_str + 'ã€‘\n' for item in headline.xpath( '../../div[@class="inforwrap clearfix"]/dl/dd'): result_str = result_str + \ item.xpath('./a/text()').extract()[0] + '\n' result_str = result_str + self.seperator # print(result_str) with open(filename, 'a', errors='ignore') as f: f.write(result_str)ã€2ã€‘æ ¼å¼åŒ–123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136""" &lt;A formatter&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;."""def GetDataFileList(path='.'): """ Get the houses file list. Arguments: path: Dir path. Returns: file_list: the list of data file that find houses data. """ file_list = [x for x in os.listdir(path) if os.path.isfile( x) and os.path.splitext(x)[1] == '.txt'] return file_listdef Parse(file_list): """ Parse the txt file that find houses data. Extract some import infomation such as house name, avarage price, address and so on. Arguments: file_list: the list of data file that find houses data. Returns: houses_dict_list: the list that each item find the detail dict of each house. """ HOUSE_NAME = 'å°åŒºåç§°' HOUSE_NAME_SPLITOR = 'ã€‘' HOUSE_ADDRESS = 'å°åŒºåœ°å€' HOUSE_ADDRESS_SPLITOR = 'ï¼š' HOUSE_AVG_PRICE = 'å¹³å‡ä»·æ ¼' HOUSE_AVG_PRICE_SPLITOR = 'ã€‘' AREA_OF_HOUSE_BELONGS_TO = 'æ‰€å±žåŒºåŸŸ' AREA_OF_HOUSE_BELONGS_TO_SPLITOR_1 = 'ï¼š' AREA_OF_HOUSE_BELONGS_TO_SPLITOR_2 = ' ' PROPERTY_CATEGORY = 'ç‰©ä¸šç±»åˆ«' PROPERTY_CATEGORY_SPLITOR = 'ï¼š' GREEN_RATE = 'ç»¿ åŒ– çŽ‡' GREEN_RATE_SPLITOR = 'ï¼š' VOLUME_RATE = 'å®¹ ç§¯ çŽ‡' VOLUME_RATE_SPLITOR = 'ï¼š' PROPERTY_COSTS = 'ç‰© ä¸š è´¹' PROPERTY_COSTS_SPLITOR = 'ï¼š' NO_INFO_NOW = 'æš‚æ— ä¿¡æ¯' DETAIL_LIST = [HOUSE_NAME, HOUSE_AVG_PRICE, HOUSE_ADDRESS, AREA_OF_HOUSE_BELONGS_TO, PROPERTY_CATEGORY, GREEN_RATE, VOLUME_RATE, PROPERTY_COSTS] houses_dict_list = [] for file_name in file_list: raw_houses_string = '' # read all lines as a string with open(file_name, 'r', errors='ignore') as f: for line in f.readlines(): raw_houses_string += line # split the string to the houses raw info list raw_houses_list = raw_houses_string.split('=\n') raw_houses_details_list = [] for raw_house in raw_houses_list: # format house raw info to lines raw_houses_details = raw_house.split('\n')[:-1] if len(raw_houses_details) == 0: continue # combine the all formated house raw info to a list raw_houses_details_list.append(raw_houses_details) for raw_house_details in raw_houses_details_list: house_details_dict = &#123;&#125; for raw_detail in raw_house_details: # search house name if re.search(HOUSE_NAME, raw_detail): house_details_dict[HOUSE_NAME] = raw_detail.split( HOUSE_NAME_SPLITOR)[-1] # search house avarage price elif re.search(HOUSE_AVG_PRICE, raw_detail): # print(raw_detail) house_details_dict[HOUSE_AVG_PRICE] = raw_detail.split( HOUSE_AVG_PRICE_SPLITOR)[-1] # search house address elif re.search(HOUSE_ADDRESS, raw_detail): house_details_dict[HOUSE_ADDRESS] = raw_detail.split( HOUSE_ADDRESS_SPLITOR)[-1] # search the area of house belongs to elif re.search(AREA_OF_HOUSE_BELONGS_TO, raw_detail): temp_detail_value = raw_detail.split( AREA_OF_HOUSE_BELONGS_TO_SPLITOR_1)[-1] detail_value = temp_detail_value.split( AREA_OF_HOUSE_BELONGS_TO_SPLITOR_2)[0] house_details_dict[AREA_OF_HOUSE_BELONGS_TO] = detail_value # search the property category of house elif re.search(PROPERTY_CATEGORY, raw_detail): house_details_dict[PROPERTY_CATEGORY] = raw_detail.split( PROPERTY_CATEGORY_SPLITOR)[-1] # search the green rate elif re.search(GREEN_RATE, raw_detail): house_details_dict[GREEN_RATE] = raw_detail.split( GREEN_RATE_SPLITOR)[-1] # search the volume rate elif re.search(VOLUME_RATE, raw_detail): house_details_dict[VOLUME_RATE] = raw_detail.split( VOLUME_RATE_SPLITOR)[-1] # search the property costs elif re.search(PROPERTY_COSTS, raw_detail): house_details_dict[PROPERTY_COSTS] = raw_detail.split( PROPERTY_COSTS_SPLITOR)[-1] # Judge if all details are contained. # If not, set to null. house_details_dict_keys = house_details_dict.keys() for detail_name in DETAIL_LIST: if detail_name not in house_details_dict_keys: house_details_dict[detail_name] = NO_INFO_NOW houses_dict_list.append(house_details_dict) return houses_dict_listã€3ã€‘é€šè¿‡é«˜å¾·åœ°å›¾apièŽ·å–ç»çº¬åº¦ä¿¡æ¯123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100""" &lt;A toolto transfer position.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;."""def Geocode(address): """ A tool that call the God-Map api. Arguments: address: the address to transfer. Returns: location: the transfered location. """ CITY_NAME = 'ä¸Šæµ·' parameters = &#123;'address': address, 'key': 'your key', 'city': CITY_NAME&#125; base = 'http://restapi.amap.com/v3/geocode/geo' try: response = requests.get(base, parameters) except Exception as e: print('error!', e) finally: pass answer = response.json() return answerdef GETGodMapLocation(houses): """ Get the location that corresponds to the house name. Use the God-Map api to get the corresponding location. Arguments: houses_dict_list: the houses info. Returns: houses_dict_list_contains_loc: the houses info that contains the location info. """ HOUSE_NAME = 'å°åŒºåç§°' HOUSE_LOCATION = 'ç»çº¬åº¦' NO_INFO_NOW = 'æš‚æ— ä¿¡æ¯' houses_dict_list = houses.copy() error_count = 0 count = 0 size = len(houses) for house_dict in houses_dict_list: # Count count = count + 1 # Loading needs if count % 1000 == 0: print(count, '/', size) address = house_dict[HOUSE_NAME] answer = Geocode(address) # print(answer) # If find if len(answer['geocodes']) != 0: # print(address + "çš„ç»çº¬åº¦ï¼š", answer['geocodes'][0]['location']) house_dict[HOUSE_LOCATION] = answer['geocodes'][0]['location'] else: # remaking the invalid address # print('address remaking...') if re.search(r'åˆ«å¢…', address): re.sub(r'åˆ«å¢…', '', address) else: address = address + 'å°åŒº' # print('retransfering...') # transfer again answer = Geocode(address) if len(answer['geocodes']) != 0: # print(address + "çš„ç»çº¬åº¦ï¼š", answer['geocodes'][0]['location']) house_dict[HOUSE_LOCATION] = answer['geocodes'][0]['location'] else: # print(address) error_count += 1 house_dict[HOUSE_LOCATION] = NO_INFO_NOW print('error counts: ', error_count) return houses_dict_listã€4ã€‘å­˜å‚¨æˆexcelæ–‡ä»¶1234567891011121314151617181920212223242526272829303132333435363738394041424344""" &lt;A tool to save the excel file.&gt; Copyright (C) &lt;2017&gt; Li W.H., Duan X This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see &lt;http://www.gnu.org/licenses/&gt;.""" def Save2ExcelFile(houses): """ Save the python based list file to excel file. Arguments: houses: the houses list. """ houses_dict_list = houses.copy() house_list = [] # format the source data to fit the xlwt package keys = houses[0].keys() for key in keys: house = [] house.append(key) for house_dict in houses_dict_list: house.append(house_dict[key]) house_list.append(house) # return house_list xls = ExcelWrite.Workbook() sheet = xls.add_sheet('å°åŒºä¿¡æ¯') for i in range(len(house_list)): for j in range(len(house_list[0])): sheet.write(j, i, house_list[i][j]) xls.save('houses.xls')ç»“æžœå±•ç¤º]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python_god_web_api]]></title>
      <url>%2F2017%2F02%2F17%2Fpython-god-web-api%2F</url>
      <content type="text"><![CDATA[http://lbs.amap.com/api/webservice/guide/api/search123456789101112131415161718#!/usr/bin/env python3#-*- coding:utf-8 -*-'''åˆ©ç”¨é«˜å¾·åœ°å›¾apiå®žçŽ°åœ°å€å’Œç»çº¬åº¦çš„è½¬æ¢'''import requestsdef geocode(address): parameters = &#123;'address': address, 'key': 'e798a5bfb344a09977b79552ae415974'&#125; base = 'http://restapi.amap.com/v3/geocode/geo' response = requests.get(base, parameters) answer = response.json() print(address + "çš„ç»çº¬åº¦ï¼š", answer['geocodes'][0]['location'])if __name__=='__main__': #address = input("è¯·è¾“å…¥åœ°å€:") address = 'åŒ—äº¬å¸‚æµ·æ·€åŒº' geocode(address)12345678910111213141516171819202122232425262728293031import xlrddef readXlsx(self, filename='CenterBottom2013.xlsx', sheetname='Sheet1'): rawData = [] if (os.path.isfile(self.fn_rawDat)): with open(self.fn_rawDat, 'rb') as f: self.rawDat = np.load(f) else: workBook = xlrd.open_workbook(filename) bookSheet = workBook.sheet_by_name(sheetname) # ä»Žç¬¬äºŒè¡Œå¼€å§‹è¯»å–ï¼Œå› ä¸ºç¬¬ä¸€è¡Œæœ‰æ ‡ç­¾ for row in range(1, bookSheet.nrows): rowData = [] for col in range(bookSheet.ncols): cel = bookSheet.cell(row, col) try: val = cel.value except: pass if type(val) == float: val = float(val) else: val = str(val) rowData.append(val) rawData.append(rowData) self.rawDat = np.array(rawData) with open(self.fn_rawDat, 'wb') as f: np.save(f, self.rawDat) return self.rawDatRead Excel filesTransfer the address to locaion infoPut back]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Decision tree]]></title>
      <url>%2F2017%2F02%2F15%2FDecision-tree%2F</url>
      <content type="text"><![CDATA[å†³ç­–æ ‘ï¼ˆID3ï¼‰å†³ç­–æ ‘çš„æž„å»ºæž„é€ å†³ç­–æ ‘æ—¶ï¼Œæ‰€éœ€è¦è§£å†³çš„ç¬¬ä¸€ä¸ªé—®é¢˜å°±æ˜¯ï¼Œæ¯åˆ’åˆ†ä¸€ä¸ªåˆ†æ”¯æ—¶ï¼Œåº”è¯¥æ ¹æ®å“ªä¸€ç»´ç‰¹å¾è¿›è¡Œåˆ’åˆ†ã€‚è¿™æ—¶å€™æˆ‘ä»¬éœ€è¦å®šä¹‰æŸç§æŒ‡æ ‡ï¼Œç„¶åŽå¯¹æ¯ä¸€ç»´ç‰¹å¾è¿›è¡Œè¯¥æŒ‡æ ‡çš„è¯„ä¼°ï¼Œæœ€åŽé€‰æ‹©æŒ‡æ ‡å€¼æœ€é«˜çš„ç‰¹å¾è¿›è¡Œåˆ’åˆ†ã€‚åˆ’åˆ†å®Œæ¯•ä¹‹åŽï¼ŒåŽŸå§‹æ•°æ®é›†å°±è¢«åˆ’åˆ†ä¸ºå‡ ä¸ªæ•°æ®å­é›†ã€‚å¦‚æžœæŸä¸€ä¸ªä¸‹çš„æ•°æ®å±žäºŽåŒä¸€ç±»åž‹ï¼Œåˆ™ç®—æ³•åœæ­¢ï¼›å¦åˆ™ï¼Œé‡å¤åˆ’åˆ†è¿‡ç¨‹ã€‚ä¼ªä»£ç ï¼ˆåˆ›å»ºåˆ†æ”¯ï¼‰1234567891011createbranch()æ£€æµ‹æ•°æ®é›†ä¸­çš„æ¯ä¸ªå­é¡¹æ˜¯å¦å±žäºŽåŒä¸€åˆ†ç±»: If so return ç±»æ ‡ç­¾; Else å¯»æ‰¾åˆ’åˆ†æ•°æ®é›†çš„æœ€å¥½ç‰¹å¾ åˆ’åˆ†æ•°æ®é›† åˆ›å»ºåˆ†æ”¯èŠ‚ç‚¹ for æ¯ä¸ªåˆ’åˆ†çš„å­é›† è°ƒç”¨å‡½æ•°createBranchå¹¶å¢žåŠ è¿”å›žç»“æžœåˆ°åˆ†æ”¯èŠ‚ç‚¹ä¸­ return åˆ†æ”¯èŠ‚ç‚¹é‚£ä¹ˆæŽ¥ä¸‹æ¥çš„é‡ç‚¹ä¾¿æ˜¯å¦‚ä½•å¯»æ‰¾åˆ’åˆ†æ•°æ®é›†çš„æœ€å¥½ç‰¹å¾ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ID3ç®—æ³•ä¸­ä½¿ç”¨çš„åˆ’åˆ†æ•°æ®é›†çš„æ–¹æ³•ï¼Œä¹Ÿå³æ ¹æ®ç†µæ¥åˆ’åˆ†ã€‚ä¿¡æ¯å¢žç›Šç†µåˆ’åˆ†æ•°æ®çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ— åºçš„æ•°æ®å˜å¾—æ›´åŠ æœ‰åºã€‚è€Œä¸€ä¸ªæ•°æ®æœ‰åºç¨‹åº¦å¯ä»¥è¿›è¡Œé‡åŒ–è¡¨ç¤ºï¼Œä¹Ÿå°±æ˜¯ä¿¡æ¯ï¼Œå…¶åº¦é‡æ–¹å¼å°±æ˜¯ç†µã€‚å…¶æ˜¾ç„¶ï¼Œæ•°æ®é›†åˆ’åˆ†å‰åŽå…¶æ‰€å«çš„ä¿¡æ¯ä¼šå‘ç”Ÿå˜åŒ–ï¼Œè¿™ä¸ªå˜åŒ–ä¾¿ç§°ä¸ºä¿¡æ¯å¢žç›Šã€‚ç†µå®šä¹‰ä¸ºä¿¡æ¯çš„æœŸæœ›ï¼Œå…¶ä¸­ä¿¡æ¯çš„å®šä¹‰å¦‚ä¸‹ï¼Œä¿¡æ¯ä¸€èˆ¬é’ˆå¯¹çš„å¯¹è±¡ä¸ºå¤šä¸ªç±»åˆ«ä¸­çš„æŸä¸€ä¸ªç±»åˆ«ï¼š$$l(x_i) = -log_{2}p(x_i)$$å…¶ä¸­$x_i$è¡¨ç¤ºæŸä¸€ç±»åˆ«ï¼Œ$p(x_i)$è¡¨ç¤ºä»Žå¤šä¸ªç±»åˆ«ä¸­é€‰æ‹©è¯¥ç±»åˆ«çš„æ¦‚çŽ‡ã€‚æŽ¥ä¸‹æ¥ï¼Œç†µçš„å®šä¹‰å¦‚ä¸‹ï¼š$$H = \sum_{i=1}^{n}p(x_i)l(x_i)=-\sum_{i=1}^{n}p(x_i)log_{2}p(x_i)$$ä¿¡æ¯å¢žç›Šå®šä¹‰å¦‚ä¸‹ï¼š$$IG(S|T) = H(S) - \sum_{value(T)} \frac{|S_v|}{|S|} H(S_v)$$å…¶ä¸­$S$ ä¸ºå…¨éƒ¨æ ·æœ¬é›†åˆï¼Œ$value(T) $æ˜¯å±žæ€§ $T$æ‰€æœ‰å–å€¼çš„é›†åˆï¼Œ$v$ æ˜¯ $T$ çš„å…¶ä¸­ä¸€ä¸ªå±žæ€§å€¼ï¼Œ$S_v$æ˜¯ $S$ ä¸­å±žæ€§ $T$ çš„å€¼ä¸º $v$ çš„æ ·ä¾‹é›†åˆï¼Œ$|S_v|$ ä¸º $S_v$ ä¸­æ‰€å«æ ·ä¾‹æ•°ï¼Œ$|S|$ ä¸º $S$ ä¸­æ‰€å«æ ·ä¾‹æ•°ã€‚ä»£ç å®žçŽ°ï¼š123456789101112131415161718192021222324252627from math import logdef CalcShannonEnt(data_set): """ Calculate the Shannon Entropy. Arguments: data_set: The object dataset. Returns: shannon_ent: The Shannon entropy of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Shannon entropy shannon_ent = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries shannon_ent -= prob * log(prob, 2) return shannon_entä¸ºäº†è¿›è¡Œæµ‹è¯•ï¼Œä»¥åŠä¹‹åŽçš„ç®—æ³•è¿è¡Œï¼Œæˆ‘ä»¬å†™ä¸€ä¸ªååˆ†naiveçš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼š123456789101112131415def CreateDataSet(): """ A naive data generation method. Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] return data_set, labelsæ³¨æ„ï¼Œè¿™é‡Œçš„labelså¹¶ä¸ä»£è¡¨åˆ†ç±»æ ‡ç­¾ï¼Œyesä»¥åŠnoæ‰æ˜¯ï¼Œlabelsä»£è¡¨ç‰¹å¾åã€‚ä¸‹é¢è¿›è¡Œä¸€ä¸ªç®€å•çš„demo:123456789In [22]: import treesIn [23]: my_dat, labels = trees.CreateDataSet()In [24]: my_datOut[24]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [25]: trees.CalcShannonEnt(my_dat)Out[25]: 0.9709505944546686ç†µè¶Šé«˜ï¼Œè¡¨æ˜Žæ•°æ®é›†ä¸­ç±»åˆ«æ•°è¶Šå¤šã€‚å¦ä¸€ä¸ªåº¦é‡æ— åºç¨‹åº¦çš„æ–¹æ³•æ˜¯åŸºå°¼ä¸çº¯åº¦ï¼ˆGini impurityï¼‰ã€‚åŸºå°¼ä¸çº¯åº¦åŸºå°¼ä¸çº¯åº¦çš„å®šä¹‰ä¸ºï¼Œå¯¹äºŽæ¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä»Žæ‰€æœ‰ç±»åˆ«æ ‡ç­¾ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªï¼Œé€‰æ‹©å‡ºæ¥çš„ç±»åˆ«æ ‡ç­¾ä¸Žå…¶æœ¬èº«çš„ç±»åˆ«æ ‡ç­¾ä¸ä¸€è‡´çš„æ¦‚çŽ‡ä¹‹å’Œã€‚å½¢å¼åŒ–åœ°å®šä¹‰å¦‚ä¸‹ï¼š$$G = \sum_{i \ne j}p(x_i)p(x_j) = \sum_{i}p(x_i)\sum_{j \ne i}p(x_j) = \sum_{i}p(x_i)(1-p(x_i)) = \sum_{i}p(x_i) - \sum_{i}(p(x_i))^2 = 1 - \sum_{i}(p(x_i))^2$$ä»£ç å®žçŽ°å¦‚ä¸‹ï¼š1234567891011121314151617181920212223242526def CalcGiniImpurity(data_set): """ Calculate the Gini impurity. Arguments: data_set: The object dataset. Returns: gini_impurity: The Gini impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Gini impurity gini_impurity = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries gini_impurity += pow(prob, 2) gini_impurity = 1 - gini_impurity return gini_impurityåŒæ ·è¿›è¡Œä¸€ä¸ªç®€å•çš„demoï¼š123456789In [4]: import treesIn [5]: my_dat, labels = trees.CreateDataSet()In [6]: my_datOut[6]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [7]: trees.CalcGiniImpurity(my_dat)Out[7]: 0.48æœ€åŽå†ä»‹ç»ä¸€ç§åº¦é‡æ— åºç¨‹åº¦çš„æ–¹å¼ï¼Œè¯¯åˆ†ç±»ä¸çº¯åº¦ã€‚è¯¯åˆ†ç±»ä¸çº¯åº¦å®šä¹‰å¦‚ä¸‹ï¼š$$M = 1 - \max_{i}(p(x_i))$$ä»£ç å®žçŽ°å¦‚ä¸‹ï¼š123456789101112131415161718192021222324def CalcMisClassifyImpurity(data_set): """ Calculate the misclassification impurity. Arguments: data_set: The object dataset. Returns: mis_classify_impurity: The misclassification impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the misclassification impurity mis_classify_impurity = 0.0 max_prob = max(label_counts.values()) / num_entries mis_classify_impurity = 1 - max_prob return mis_classify_impurityè¿›è¡Œä¸€ä¸ªç®€å•çš„demo:12345678910In [25]: reload(trees)Out[25]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [26]: my_dat, labels = trees.CreateDataSet()In [27]: my_datOut[27]: [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]In [28]: trees.CalcMisClassifyImpurity(my_dat)Out[28]: 0.4æœ€åŽç”¨ä¸€ä¸ªå›¾æ¥æ€»ç»“ä¸€ä¸‹è¿™ä¸‰ç§ä¸çº¯åº¦åº¦é‡çš„å‡½æ•°å›¾åƒï¼ˆä»¥äºŒç±»æƒ…å†µä¸ºä¾‹ï¼‰[Ref: http://www.cse.msu.edu/~cse802/DecisionTrees.pdf]ï¼šæ•°æ®åˆ’åˆ†æ ¹æ®ä»¥ä¸Šï¼Œæ•°æ®åˆ’åˆ†çš„æ€è·¯æ˜¯ï¼ŒåŸºäºŽæ¯ä¸€ç»´ç‰¹å¾çš„æ¯ä¸€ä¸ªå€¼è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶è®¡ç®—åˆ’åˆ†å‰åŽçš„ä¿¡æ¯å¢žç›Šï¼Œæœ€åŽé€‰å–å¢žç›Šæœ€å¤§çš„ç‰¹å¾åŠå…¶æ‰€å¯¹åº”çš„å€¼è¿›è¡Œåˆ’åˆ†ï¼Œç”±äºŽè¿™é‡Œè¿ç”¨çš„æ˜¯ID3ç®—æ³•ï¼Œå› æ­¤é€‰æ‹©çš„ä¿¡æ¯åº¦é‡æ–¹å¼æ˜¯ç†µã€‚ä»£ç å®žçŽ°å¦‚ä¸‹ï¼š1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def SplitDataSet(data_set, axis, value): """ Split the data set according to the given axis and correspond value. Arguments: data_set: Object data set. axis: The split-feature index. value: The value of the split-feature. Returns: ret_data_set: The splited data set. """ ret_data_set = [] for feat_vec in data_set: if feat_vec[axis] == value: reduced_feat_vec = feat_vec[:axis] reduced_feat_vec.extend(feat_vec[axis + 1:]) ret_data_set.append(reduced_feat_vec) return ret_data_setdef ChooseBestFeatureToSplit(data_set): """ Choose the best feature to split. Arguments: data_set: Object data set. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannonEnt(data_set) best_info_gain = 0.0 best_feature = -1 for i in range(num_features): # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) new_entropy = 0.0 # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannonEnt(sub_data_set) # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_featureç”±ä»¥ä¸Šä»£ç ï¼ˆID3ç®—æ³•ï¼‰å¯ä»¥çœ‹å‡ºï¼Œå…¶è®¡ç®—ç†µçš„ä¾æ®æ˜¯æ ¹æ®æœ€åŽä¸€ä¸ªç‰¹å¾ï¼Œæ˜¯å¦è¿™ç§naiveçš„é€‰å–æ–¹å¼èƒ½å¤Ÿè¾¾åˆ°å¹³å‡çš„æœ€å¥½ç»“æžœï¼Ÿå¦å¤–ï¼Œå…¶åˆ’åˆ†ä¾æ®ä»…ä»…æ ¹æ®åˆ’åˆ†ä¸€æ¬¡åŽçš„å­æ•°æ®é›†çš„ç†µä¹‹å’Œï¼Œå±žäºŽä¸€ç§è´ªå¿ƒç­–ç•¥ï¼Œè¿™æ ·æ˜¯å¦å¯ä»¥è¾¾åˆ°æœ€ä¼˜è§£ï¼Ÿé€’å½’æž„å»ºå†³ç­–æ ‘æ—¢ç„¶æ˜¯é€’å½’ç®—æ³•ï¼Œé‚£ä¹ˆå¿…é¡»è®¾å®šé€’å½’ç»“æŸæ¡ä»¶ï¼šéåŽ†å®Œæ‰€æœ‰å±žæ€§æ¯ä¸ªåˆ†æ”¯ä¸‹çš„æ•°æ®éƒ½å±žäºŽç›¸åŒçš„åˆ†ç±»è¿™é‡Œå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå¦‚æžœéåŽ†å®Œæ‰€æœ‰å±žæ€§åŽï¼ŒæŸäº›åˆ†æ”¯ä¸‹è¿˜æ˜¯å­˜åœ¨å¤šä¸ªåˆ†ç±»ï¼Œè¿™ç§æƒ…å†µä¸‹ä¸€èˆ¬é‡‡ç”¨å¤šæ•°è¡¨å†³çš„æ–¹å¼ï¼Œä»£ç å®žçŽ°æ–¹å¼å¦‚ä¸‹ï¼š12345678910111213141516171819202122def majority_cnt(class_list): """ Decided the final class. When the splited data is not belongs to the same class while all feature is handled, the final class is decided by the majority class. Arguments: class_list: The class list of the splited data set. Returns: sorted_class_count[0][0]: The majority class. """ class_count = &#123;&#125; for vote in class_list: if vote not in class_count.keys(): class_count[vote] = 0 class_count[vote] += 1 sorted_class_count = sorted( class_count.iteritems(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0]ä¸‹é¢è¿›è¡Œæ ‘çš„åˆ›å»ºï¼š1234567891011121314151617181920212223242526272829303132333435def CreateTree(data_set, labels): """ Create decision tree. Arguments: data_set: The object data set. labels: The feature labels in the data_set. Returns: my_tree: A dict that represents the decision tree. """ class_list = [example[-1] for example in data_set] # If the classes are fully same if class_list.count(class_list[0]) == len(class_list): return class_list[0] # If all feature is handled if len(data_set[0]) == 1: return majority_cnt(class_list) # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set) best_feat_label = labels[best_feat] # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Get the next step labels parameter del(labels[best_feat]) # Next step start feat_values = [example[best_feat] for example in data_set] unique_vals = set(feat_values) for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels) return my_treeä¸‹é¢è¿›è¡Œä¸€ä¸‹ç®€å•çš„æµ‹è¯•ï¼š123456789In [27]: reload(trees)Out[27]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [28]: my_dat, labels = trees.CreateDataSet()In [29]: my_tree = trees.CreateTree(my_dat, labels)In [30]: my_treeOut[30]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;å¯è§å†³ç­–æ ‘å·²ç»æž„é€ æˆåŠŸï¼ˆå›¾å½¢åŒ–å¦‚ä¸‹æ‰€ç¤ºï¼‰ï¼Œä½†æ˜¯è¿™æ˜¾ç„¶ä¸å¤Ÿï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯ç”¨å†³ç­–æ ‘è¿›è¡Œåˆ†ç±»ã€‚å†³ç­–æ ‘åˆ†ç±»demoå¦‚ä¸‹ï¼š123456789101112131415161718192021In [63]: reload(trees)Out[63]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [64]: my_dat, labels = trees.CreateDataSet()In [65]: labelsOut[65]: ['no surfacing', 'flippers']In [66]: my_tree = trees.CreateTree(my_dat, labels)In [67]: labelsOut[67]: ['no surfacing', 'flippers']In [68]: my_treeOut[68]: &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;In [69]: trees.Classify(my_tree, labels, [1, 0])Out[69]: 'no'In [70]: trees.Classify(my_tree, labels, [1, 1])Out[70]: 'yes'C4.5C4.5ç®—æ³•æ˜¯ç”±ID3ç®—æ³•å¼•ç”³è€Œæ¥ï¼Œä¸»è¦æ”¹è¿›æœ‰ä»¥ä¸‹ä¸¤ç‚¹ï¼šé€‰å–æœ€ä¼˜åˆ†è£‚å±žæ€§æ—¶æ ¹æ®ä¿¡æ¯å¢žç›ŠçŽ‡ (IGR)ä½¿ç®—æ³•å¯¹è¿žç»­å˜é‡å…¼å®¹ä¸‹é¢åˆ†åˆ«å¯¹åˆ†è£‚ä¿¡æ¯ä»¥åŠä¿¡æ¯å¢žç›ŠçŽ‡è¿›è¡Œå®šä¹‰ï¼š$$IGR = \frac{IG}{IV}$$å› æ­¤åªéœ€å¯¹ID3ç®—æ³•çš„ä»£ç åšä¸€äº›æ”¹åŠ¨å³å¯ï¼Œä¸ºäº†å…¼å®¹ID3ï¼Œ å…·ä½“å®žçŽ°å¦‚ä¸‹ï¼š1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465def ChooseBestFeatureToSplit(data_set, flag='ID3'): """ Choose the best feature to split. Arguments: data_set: Object data set. flag: Decide if use the infomation gain rate or not. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannon(data_set) method = 'ID3' best_feature = -1 best_info_gain = 0.0 best_info_gain_rate = 0.0 for i in range(num_features): new_entropy = 0.0 # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) if len(unique_vals) &gt; 3: method = 'C4.5' if method == 'ID3': # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannon(sub_data_set) else: data_set = np.array(data_set) sorted_feat = np.argsort(feat_list) for index in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [index + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; new_entropy: new_entropy = iv # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if flag == 'C4.5': info_gain_rate = info_gain / new_entropy # print('index', i, 'info_gain_rate', info_gain_rate) if info_gain_rate &gt; best_info_gain_rate: best_info_gain_rate = info_gain_rate best_feature = i if flag == 'ID3': if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_featureä¸‹é¢éœ€è¦è§£å†³çš„é—®é¢˜æ˜¯è¿žç»­å˜é‡çš„é—®é¢˜ï¼Œä¸ºäº†å®žéªŒçš„æ–¹ä¾¿ï¼Œæˆ‘ä»¬æ›´æ”¹ä¸€ä¸‹naiveçš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼ˆRef: http://blog.csdn.net/lemon_tree12138/article/details/51840361 ï¼‰ï¼š1234567891011121314151617181920212223242526272829303132333435363738def CreateDataSet(method='ID3'): """ A naive data generation method. Arguments: method: The algorithm class Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) if method == 'ID3': data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']] labels = ['no surfacing', 'flippers'] else: data_set = [[85, 85, 'no'], [80, 90, 'yes'], [83, 78, 'no'], [70, 96, 'no'], [68, 80, 'no'], [65, 70, 'yes'], [64, 65, 'yes'], [72, 95, 'no'], [69, 70, 'no'], [75, 80, 'no'], [75, 70, 'yes'], [72, 90, 'yes'], [81, 75, 'no'], [71, 80, 'yes']] labels = ['temperature', 'humidity'] return data_set, labelså‡è®¾æˆ‘ä»¬é€‰æ‹©äº†æ¸©åº¦å±žæ€§ï¼Œåˆ™è¢«æå–çš„å…³é”®æ•°æ®ä¸ºï¼š[[85, No], [80, No], [83, Yes], [70, Yes], [68, Yes], [65, No], [64, Yes], [72, No], [69, Yes], [75, Yes], [75, Yes], [72, Yes], [81, Yes], [71, No]]çŽ°åœ¨æˆ‘ä»¬å¯¹è¿™æ‰¹æ•°æ®è¿›è¡Œä»Žå°åˆ°å¤§è¿›è¡ŒæŽ’åºï¼ŒæŽ’åºåŽæ•°æ®é›†å°±å˜æˆï¼š[[64, Yes], [65, No], [68, Yes], [69, Yes], [70, Yes], [71, No], [72, No], [72, Yes], [75, Yes], [75, Yes], [80, No], [81, Yes], [83, Yes], [85, No]]ç»˜åˆ¶æˆå¦‚ä¸‹å›¾ä¾‹ï¼šå½“æˆ‘ä»¬æ‹¿åˆ°ä¸€ä¸ªå·²ç»æŽ’å¥½åºçš„ï¼ˆæ¸©åº¦ï¼Œç»“æžœï¼‰çš„åˆ—è¡¨ä¹‹åŽï¼Œåˆ†åˆ«è®¡ç®—è¢«æŸä¸ªå•å…ƒåˆ†éš”çš„å·¦è¾¹å’Œå³è¾¹çš„åˆ†è£‚ä¿¡æ¯ï¼Œæ¯”å¦‚çŽ°åœ¨è®¡ç®— index = 4 æ—¶çš„åˆ†è£‚ä¿¡æ¯ã€‚åˆ™ï¼š$$IV(v_4) = IV([4, 1], [5, 4]) = \frac{5}{14}IV([4, 1]) + \frac{9}{14}IV([5, 4])$$$$IV(v_4) = \frac{5}{14}(-\frac{4}{5} \log_{2} \frac{4}{5} - \frac{1}{5} \log_{2} \frac{1}{5}) + \frac{9}{14}(-\frac{5}{9} \log_{2} \frac{5}{9} - \frac{4}{9} \log_{2} \frac{4}{9})$$ä¸‹å›¾è¡¨ç¤ºäº†ä¸åŒåˆ†è£‚ä½ç½®æ‰€å¾—åˆ°çš„åˆ†è£‚ä¿¡æ¯ï¼šæœ€åŽç»™å‡ºå®Œæ•´çš„ä»£ç å®žçŽ° (æœ€åŽçš„Classifyæ–¹æ³•è¿˜éœ€ä¿®æ”¹)ï¼štrees.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383from math import logimport operatorimport numpy as npdef CalcShannon(data_set): """ Calculate the Shan0n Entropy. Arguments: data_set: The object dataset. Returns: shan0n_ent: The Shan0n entropy of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # print(label_counts) # Calculates the Shan0n entropy shan0n_ent = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries shan0n_ent -= prob * log(prob, 2) return shan0n_entdef CalcGiniImpurity(data_set): """ Calculate the Gini impurity. Arguments: data_set: The object dataset. Returns: gini_impurity: The Gini impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the Gini impurity gini_impurity = 0.0 for key in label_counts: prob = float(label_counts[key]) / num_entries gini_impurity += pow(prob, 2) gini_impurity = 1 - gini_impurity return gini_impuritydef CalcMisClassifyImpurity(data_set): """ Calculate the misclassification impurity. Arguments: data_set: The object dataset. Returns: mis_classify_impurity: The misclassification impurity of the object data set. """ # Initiation num_entries = len(data_set) label_counts = &#123;&#125; # Statistics the frequency of each class in the dataset for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 # Calculates the misclassification impurity mis_classify_impurity = 0.0 max_prob = max(label_counts.values()) / num_entries mis_classify_impurity = 1 - max_prob return mis_classify_impuritydef CreateDataSet(method='ID3'): """ A naive data generation method. Arguments: method: The algorithm class Returns: data_set: The data set excepts label info. labels: The data set only contains label info. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) if method == 'ID3': data_set = [[1, 1, 1], [1, 1, 1], [1, 0, 0], [0, 1, 0], [0, 1, 0]] labels = ['0 surfacing', 'flippers'] else: data_set = [[1, 85, 85, 0, 0], [1, 80, 90, 1, 0], [2, 83, 78, 0, 1], [3, 70, 96, 0, 1], [3, 68, 80, 0, 1], [3, 65, 70, 1, 0], [2, 64, 65, 1, 1], [1, 72, 95, 0, 0], [1, 69, 70, 0, 1], [3, 75, 80, 0, 1], [1, 75, 70, 1, 1], [2, 72, 90, 1, 1], [2, 81, 75, 0, 1], [3, 71, 80, 1, 0]] labels = ['outlook', 'temperature', 'humidity', 'windy'] return data_set, labelsdef SplitDataSet(data_set, axis, value): """ Split the data set according to the given axis and correspond value. Arguments: data_set: Object data set. axis: The split-feature index. value: The value of the split-feature. Returns: ret_data_set: The splited data set. """ ret_data_set = [] for feat_vec in data_set: if feat_vec[axis] == value: reduced_feat_vec = feat_vec[:axis] reduced_feat_vec.extend(feat_vec[axis + 1:]) ret_data_set.append(reduced_feat_vec) return ret_data_setdef ChooseBestFeatureToSplit(data_set, flag='ID3'): """ Choose the best feature to split. Arguments: data_set: Object data set. flag: Decide if use the infomation gain rate or not. Returns: best_feature: The index of the feature to split. """ # Initiation # Because the range() method excepts the lastest number num_features = len(data_set[0]) - 1 base_entropy = CalcShannon(data_set) method = 'ID3' best_feature = -1 best_info_gain = 0.0 best_info_gain_rate = 0.0 for i in range(num_features): new_entropy = 0.0 # Choose the i-th feature of all data feat_list = [example[i] for example in data_set] # Abandon the repeat feature value(s) unique_vals = set(feat_list) if len(unique_vals) &gt; 3: method = 'C4.5' if method == 'ID3': # Calculates the Shannon entropy of the splited data set for value in unique_vals: sub_data_set = SplitDataSet(data_set, i, value) prob = len(sub_data_set) / len(data_set) new_entropy += prob * CalcShannon(sub_data_set) else: data_set = np.array(data_set) sorted_feat = np.argsort(feat_list) for index in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [index + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; new_entropy: new_entropy = iv # base_entropy is equal or greatter than new_entropy info_gain = base_entropy - new_entropy if flag == 'C4.5': info_gain_rate = info_gain / new_entropy # print('index', i, 'info_gain_rate', info_gain_rate) if info_gain_rate &gt; best_info_gain_rate: best_info_gain_rate = info_gain_rate best_feature = i if flag == 'ID3': if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature = i return best_featuredef majority_cnt(class_list): """ Decided the final class. When the splited data is 0t belongs to the same class while all feature is handled, the final class is decided by the majority class. Arguments: class_list: The class list of the splited data set. Returns: sorted_class_count[0][0]: The majority class. """ class_count = &#123;&#125; for vote in class_list: if vote not in class_count.keys(): class_count[vote] = 0 class_count[vote] += 1 sorted_class_count = sorted( class_count. items(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0]def CreateTree(data_set, feat_labels, method='ID3'): """ Create decision tree. Arguments: data_set: The object data set. labels: The feature labels in the data_set. method: The algorithm class. Returns: my_tree: A dict that represents the decision tree. """ # Arguments check if method not in ('ID3', 'C4.5'): raise ValueError('invalid value: %s' % method) labels = feat_labels.copy() class_list = [example[-1] for example in data_set] # print(class_list) # If the classes are fully same print('class_list', class_list) if class_list.count(class_list[0]) == len(class_list): return class_list[0] # If all feature is handled if len(data_set[0]) == 1: return majority_cnt(class_list) if method == 'ID3': # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set) best_feat_label = labels[best_feat] # print(best_feat_label) # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Next step start feat_values = [example[best_feat] for example in data_set] # Get the next step labels parameter del(labels[best_feat]) unique_vals = set(feat_values) for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels) return my_tree else: flag = 'ID3' # Get the best split-feature and the correspond label best_feat = ChooseBestFeatureToSplit(data_set, 'C4.5') best_feat_label = labels[best_feat] print(best_feat_label) # Build a recurrence dict my_tree = &#123;best_feat_label: &#123;&#125;&#125; # Next step start feat_values = [example[best_feat] for example in data_set] del(labels[best_feat]) unique_vals = set(feat_values) if len(unique_vals) &gt; 3: flag = 'C4.5' if flag == 'ID3': for value in unique_vals: sub_labels = labels[:] # Recurrence calls my_tree[best_feat_label][value] = CreateTree( SplitDataSet(data_set, best_feat, value), sub_labels, 'C4.5') return my_tree else: data_set = np.array(data_set) best_iv = 0.0 best_split_value = -1 sorted_feat = np.argsort(feat_values) for i in range(len(sorted_feat) - 1): pre_sorted_feat, post_sorted_feat = np.split( sorted_feat, [i + 1, ]) pre_data_set = data_set[pre_sorted_feat] post_data_set = data_set[post_sorted_feat] pre_coff = len(pre_sorted_feat) / len(sorted_feat) post_coff = len(post_sorted_feat) / len(sorted_feat) # Calucate the split info iv = pre_coff * CalcShannon(pre_data_set) + \ post_coff * CalcShannon(post_data_set) if iv &gt; best_iv: best_iv = iv best_split_value = feat_values[sorted_feat[i]] print(best_feat, best_split_value) # print(best_split_value) left_data_set = data_set[ data_set[:, best_feat] &lt;= best_split_value] left_data_set = np.delete(left_data_set, best_feat, axis=1) # if len(left_data_set) == 1: # return left_data_set[0][-1] right_data_set = data_set[ data_set[:, best_feat] &gt; best_split_value] right_data_set = np.delete(right_data_set, best_feat, axis=1) # if len(right_data_set) == 1: # return right_data_set[0][-1] sub_labels = labels[:] my_tree[best_feat_label][ '&lt;=' + str(best_split_value)] = CreateTree( left_data_set.tolist(), sub_labels, 'C4.5') my_tree[best_feat_label][ '&gt;' + str(best_split_value)] = CreateTree( right_data_set.tolist(), sub_labels, 'C4.5') # print('continious tree', my_tree) return my_treedef Classify(input_tree, feat_labels, test_vec): """ Classify that uses the given decision tree. Arguments: input_tree: The Given decision tree. feat_labels: The labels of correspond feature. test_vec: The test data. Returns: class_label: The class label that corresponds to the test data. """ # Get the start feature label to split first_str = list(input_tree.keys())[0] # Get the sub-tree that corresponds to the start feature to split second_dict = input_tree[first_str] # Get the feature index that the label is the start feature label feat_index = feat_labels.index(first_str) # Start recurrence search for key in second_dict.keys(): if test_vec[feat_index] == key: if type(second_dict[key]).__name__ == 'dict': # Recurrence calls class_label = Classify(second_dict[key], feat_labels, test_vec) else: class_label = second_dict[key] return class_labelä¸€ä¸ªå°demoï¼š123456789101112131415161718192021222324252627282930313233In [108]: reload(trees)Out[108]: &lt;module 'trees' from 'C:\\Users\\Ewan\\Documents\\GitHub\\hexo\\public\\2017\\02\\15\\Decision-tree\\trees.py'&gt;In [109]: my_dat, labels = trees.CreateDataSet('C4.5')In [110]: my_tree_c = trees.CreateTree(my_dat, labels, 'C4.5')class_list [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]outlookclass_list [0, 0, 0, 1, 1]humidity1 90class_list [0, 0, 1, 1]temperature0 69class_list [1]class_list [0, 0, 1]windyclass_list [0]class_list [0, 1]class_list [0]class_list [1, 1, 1, 1]class_list [1, 1, 0, 1, 0]windyclass_list [1, 1, 1]class_list [0, 0]In [111]: my_tree_cOut[111]:&#123;'outlook': &#123;1: &#123;'humidity': &#123;'&lt;=90': &#123;'temperature': &#123;'&lt;=69': 1, '&gt;69': &#123;'windy': &#123;0: 0, 1: 0&#125;&#125;&#125;&#125;, '&gt;90': 0&#125;&#125;, 2: 1, 3: &#123;'windy': &#123;0: 1, 1: 0&#125;&#125;&#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kNN and kd-tree]]></title>
      <url>%2F2017%2F02%2F15%2FkNN-and-kd-tree%2F</url>
      <content type="text"><![CDATA[k-è¿‘é‚»ç®—æ³•å·¥ä½œåŽŸç†å­˜åœ¨ä¸€ç»„å¸¦æ ‡ç­¾çš„è®­ç»ƒé›†[1]ï¼Œæ¯å½“æœ‰æ–°çš„ä¸å¸¦æ ‡ç­¾çš„æ ·æœ¬[2]å‡ºçŽ°æ—¶ï¼Œå°†è®­ç»ƒé›†ä¸­æ•°æ®çš„ç‰¹å¾ä¸Žæµ‹è¯•é›†çš„ç‰¹å¾é€ä¸ªæ¯”è¾ƒï¼Œé€šè¿‡æŸç§æµ‹åº¦æ¥æå–å‡ºä¸Žæµ‹è¯•é›†æœ€ç›¸ä¼¼çš„kä¸ªè®­ç»ƒé›†æ ·æœ¬ï¼Œç„¶åŽå°†è¿™kä¸ªæ ·æœ¬ä¸­å å¤§å¤šæ•°[4]çš„æ ‡ç­¾èµ‹äºˆæµ‹è¯•é›†æ ·æœ¬ã€‚ä¼ªä»£ç å¯¹æµ‹è¯•é›†ä¸­çš„æ¯ä¸ªç‚¹ä¾æ¬¡æ‰§è¡Œå¦‚ä¸‹æ“ä½œï¼šè®¡ç®—è®­ç»ƒé›†ä¸­çš„æ¯ä¸ªç‚¹ä¸Žå½“å‰ç‚¹çš„è·ç¦»æŒ‰ç…§è·ç¦»é€’å¢žæ¬¡åºæŽ’åºåœ¨æŽ’åºå¥½çš„ç‚¹ä¸­é€‰å–å‰kä¸ªç‚¹ç»Ÿè®¡å‡ºkä¸ªç‚¹ä¸­ä¸åŒç±»åˆ«çš„å‡ºçŽ°é¢‘çŽ‡é€‰æ‹©é¢‘çŽ‡æœ€é«˜çš„ç±»åˆ«ä¸ºå½“å‰ç‚¹çš„é¢„æµ‹åˆ†ç±»â€‹ä»£ç å®žçŽ°é¦–å…ˆåˆ›å»ºæµ‹è¯•æ•°æ®é›†1from numpy import *1234def createDataset(): group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labelsè¿”å›žé¢„æµ‹åˆ†ç±»123456789101112131415def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] diffMat = tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 sortedDistIndices = distances.argsort() classCount = &#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndices[i]] classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1 # the return of sorted() is a list and its item is a tuple sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0] # returns the predict class labelè¿›ä¸€æ­¥æŽ¢ç´¢k-è¿‘é‚»ç®—æ³•çš„ç¼ºç‚¹åœ¨äºŽå½“æ•°æ®é‡å¾ˆå¤§æ—¶ï¼Œæ‹¥æœ‰ä¸å¯æŽ¥å—çš„ç©ºé—´å¤æ‚åº¦ä»¥åŠæ—¶é—´å¤æ‚åº¦å…¶æ¬¡è¯¥ç®—æ³•æœ€å…³é”®çš„åœ°æ–¹åœ¨ä¸Žè¶…å‚kçš„é€‰å–ã€‚å½“ké€‰å–çš„è¿‡å°æ—¶å®¹æ˜“é€ æˆè¿‡æ‹Ÿåˆï¼Œåä¹‹å®¹æ˜“é€ æˆæ¬ æ‹Ÿåˆã€‚è€ƒè™‘ä¸¤ä¸ªæžç«¯æƒ…å†µï¼Œå½“k=1æ—¶ï¼Œè¯¥ç®—æ³•åˆå«æœ€è¿‘é‚»ç®—æ³•ï¼›å½“k=N[3]æ—¶ï¼Œè¡¨ç¤ºç›´æŽ¥ä»ŽåŽŸå§‹æ•°æ®ä¸­é€‰å–å æ¯”æœ€å¤§çš„ç±»åˆ«ï¼Œæ˜¾ç„¶è¿™ä¸ªç®—æ³•å¤ªnaiveäº†ã€‚ä¸ºäº†è§£å†³kNNç®—æ³•æ—¶é—´å¤æ‚åº¦çš„é—®é¢˜ï¼Œæœ€å…³é”®çš„ä¾¿æ˜¯åœ¨äºŽå¦‚ä½•å¯¹æ•°æ®è¿›è¡Œå¿«é€Ÿçš„kè¿‘é‚»æœç´¢ï¼Œä¸€ç§è§£å†³æ–¹æ³•æ˜¯å¼•å…¥kdæ ‘æ¥è¿›è¡ŒåŠ é€Ÿã€‚kdæ ‘ç®€ä»‹ä»¥äºŒç»´ç©ºé—´ä¸ºä¾‹ï¼Œå‡è®¾æœ‰6ä¸ªäºŒç»´æ•°æ®ç‚¹{(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2)}ï¼Œå¯ä»¥ç”¨ä¸‹å›¾æ¥è¡¨æ˜Žkdæ ‘æ‰€èƒ½è¾¾åˆ°çš„æ•ˆæžœ:kdæ ‘ç®—æ³•ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼škdæ ‘æ•°æ®ç»“æž„çš„å»ºç«‹åœ¨kdæ ‘ä¸Šè¿›è¡ŒæŸ¥æ‰¾kdæ ‘æ˜¯ä¸€ç§å¯¹kç»´ç©ºé—´ä¸Šçš„æ•°æ®ç‚¹è¿›è¡Œå­˜å‚¨ä»¥ä¾¿è¿›è¡Œé«˜æ•ˆæŸ¥æ‰¾çš„æ ‘å½¢æ•°æ®ç»“æž„ï¼Œå±žäºŽäºŒå‰æ ‘ã€‚æž„é€ kdæ ‘ç›¸å½“äºŽä¸æ–­ç”¨åž‚ç›´äºŽåæ ‡è½´çš„è¶…å¹³é¢å¯¹kç»´ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œæž„æˆä¸€ç³»åˆ—kç»´è¶…çŸ©å½¢åŒºåŸŸã€‚kdæ ‘çš„æ¯ä¸€ä¸ªç»“ç‚¹å¯¹åº”ä¸€ä¸ªè¶…çŸ©å½¢åŒºåŸŸï¼Œè¡¨ç¤ºä¸€ä¸ªç©ºé—´èŒƒå›´ã€‚æ•°æ®ç»“æž„ä¸‹è¡¨ç»™å‡ºæ¯ä¸ªç»“ç‚¹ä¸»è¦åŒ…å«çš„æ•°æ®ç»“æž„:åŸŸåæ•°æ®ç±»åž‹æè¿°Node-dataæ•°æ®çŸ¢é‡æ•°æ®é›†ä¸­çš„æŸä¸ªæ•°æ®ç‚¹ï¼Œkç»´çŸ¢é‡Splitæ•´æ•°åž‚ç›´äºŽåˆ†å‰²è¶…å¹³é¢çš„æ–¹å‘è½´åºå·Leftkdæ ‘ç”±ä½äºŽè¯¥èŠ‚ç‚¹åˆ†å‰²è¶…å¹³é¢å·¦å­ç©ºé—´å†…æ‰€æœ‰æ•°æ®ç‚¹æž„æˆçš„kdæ ‘Rightkdæ ‘ç”±ä½äºŽè¯¥èŠ‚ç‚¹åˆ†å‰²è¶…å¹³é¢å³å­ç©ºé—´å†…æ‰€æœ‰æ•°æ®ç‚¹æž„æˆçš„kdæ ‘å»ºç«‹æ ‘ä¼ªä»£ç ä¸‹é¢ç»™å‡ºæž„å»ºkdæ ‘çš„ä¼ªä»£ç ï¼šç®—æ³•ï¼šæž„å»ºk-dæ ‘ï¼ˆcreateKDTreeï¼‰è¾“å…¥ï¼šæ•°æ®ç‚¹é›†Data-setè¾“å‡ºï¼šKdï¼Œç±»åž‹ä¸ºk-d tree1. If Data-setä¸ºç©ºï¼Œåˆ™è¿”å›žç©ºçš„k-d tree2. è°ƒç”¨èŠ‚ç‚¹ç”Ÿæˆç¨‹åºï¼š ï¼ˆ1ï¼‰ç¡®å®šsplitåŸŸï¼šå¯¹äºŽæ‰€æœ‰æè¿°å­æ•°æ®ï¼ˆç‰¹å¾çŸ¢é‡ï¼‰ï¼Œç»Ÿè®¡å®ƒä»¬åœ¨æ¯ä¸ªç»´ä¸Šçš„æ•°æ®æ–¹å·®ã€‚ä»¥SURFç‰¹å¾ä¸ºä¾‹ï¼Œæè¿°å­ä¸º64ç»´ï¼Œå¯è®¡ç®—64ä¸ªæ–¹å·®ã€‚æŒ‘é€‰å‡ºæœ€å¤§å€¼ï¼Œå¯¹åº”çš„ç»´å°±æ˜¯splitåŸŸçš„å€¼ã€‚æ•°æ®æ–¹å·®å¤§è¡¨æ˜Žæ²¿è¯¥åæ ‡è½´æ–¹å‘ä¸Šçš„æ•°æ®åˆ†æ•£å¾—æ¯”è¾ƒå¼€ï¼Œåœ¨è¿™ä¸ªæ–¹å‘ä¸Šè¿›è¡Œæ•°æ®åˆ†å‰²æœ‰è¾ƒå¥½çš„åˆ†è¾¨çŽ‡ï¼› ï¼ˆ2ï¼‰ç¡®å®šNode-dataåŸŸï¼šæ•°æ®ç‚¹é›†Data-setæŒ‰å…¶ç¬¬splitåŸŸçš„å€¼æŽ’åºã€‚ä½äºŽæ­£ä¸­é—´çš„é‚£ä¸ªæ•°æ®ç‚¹è¢«é€‰ä¸ºNode-dataã€‚æ­¤æ—¶æ–°çš„Data-setâ€™ = Data-set\Node-dataï¼ˆé™¤åŽ»å…¶ä¸­Node-dataè¿™ä¸€ç‚¹ï¼‰ã€‚3. dataleft = {då±žäºŽData-setâ€™ &amp;&amp; d[split] â‰¤ Node-data[split]} dataright = {då±žäºŽData-setâ€™ &amp;&amp; d[split] &gt; Node-data[split]}4. left = ç”±ï¼ˆdataleftï¼‰å»ºç«‹çš„k-d treeï¼Œå³é€’å½’è°ƒç”¨createKDTreeï¼ˆdataleftï¼‰å¹¶è®¾ç½®leftçš„parentåŸŸä¸ºKdï¼› right = ç”±ï¼ˆdatarightï¼‰å»ºç«‹çš„k-d treeï¼Œå³è°ƒç”¨createKDTreeï¼ˆdataleftï¼‰å¹¶è®¾ç½®rightçš„parentåŸŸä¸ºKdã€‚å®žä¾‹ç”¨æœ€å¼€å§‹çš„6ä¸ªäºŒç»´æ•°æ®ç‚¹çš„ä¾‹å­ï¼Œæ¥å…·ä½“åŒ–è¿™ä¸ªè¿‡ç¨‹ï¼šç¡®å®šsplitåŸŸçš„é¦–å…ˆè¯¥å–çš„å€¼ã€‚åˆ†åˆ«è®¡ç®—xï¼Œyæ–¹å‘ä¸Šæ•°æ®çš„æ–¹å·®å¾—çŸ¥xæ–¹å‘ä¸Šçš„æ–¹å·®æœ€å¤§ï¼Œæ‰€ä»¥splitåŸŸå€¼é¦–å…ˆå–0ï¼Œä¹Ÿå°±æ˜¯xè½´æ–¹å‘ï¼›ç¡®å®šNode-dataçš„åŸŸå€¼ã€‚æ ¹æ®xè½´æ–¹å‘çš„å€¼2,5,9,4,8,7æŽ’åºé€‰å‡ºä¸­å€¼ä¸º7ï¼Œæ‰€ä»¥Node-data = (7, 2)ã€‚è¿™æ ·ï¼Œè¯¥èŠ‚ç‚¹çš„åˆ†å‰²è¶…å¹³é¢å°±æ˜¯é€šè¿‡(7, 2)å¹¶åž‚ç›´äºŽsplit = 0ï¼ˆxè½´ï¼‰çš„ç›´çº¿x = 7ï¼›ç¡®å®šå·¦å­ç©ºé—´å’Œå³å­ç©ºé—´ã€‚åˆ†å‰²è¶…å¹³é¢x = 7å°†æ•´ä¸ªç©ºé—´åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚x &lt; = 7çš„éƒ¨åˆ†ä¸ºå·¦å­ç©ºé—´ï¼ŒåŒ…å«3ä¸ªèŠ‚ç‚¹{(2, 3), (5, 4), (4, 7)}ï¼›å¦ä¸€éƒ¨åˆ†ä¸ºå³å­ç©ºé—´ï¼ŒåŒ…å«2ä¸ªèŠ‚ç‚¹{(9, 6), (8, 1)}ã€‚å¦‚ç®—æ³•æ‰€è¿°ï¼Œk-dæ ‘çš„æž„å»ºæ˜¯ä¸€ä¸ªé€’å½’çš„è¿‡ç¨‹ã€‚ç„¶åŽå¯¹å·¦å­ç©ºé—´å’Œå³å­ç©ºé—´å†…çš„æ•°æ®é‡å¤æ ¹èŠ‚ç‚¹çš„è¿‡ç¨‹å°±å¯ä»¥å¾—åˆ°ä¸‹ä¸€çº§å­èŠ‚ç‚¹ï¼ˆ5,4ï¼‰å’Œï¼ˆ9,6ï¼‰ï¼ˆä¹Ÿå°±æ˜¯å·¦å³å­ç©ºé—´çš„â€™æ ¹â€™èŠ‚ç‚¹ï¼‰ï¼ŒåŒæ—¶å°†ç©ºé—´å’Œæ•°æ®é›†è¿›ä¸€æ­¥ç»†åˆ†ã€‚å¦‚æ­¤åå¤ç›´åˆ°ç©ºé—´ä¸­åªåŒ…å«ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚æœ€åŽç”Ÿæˆçš„k-dæ ‘å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æ³¨æ„ï¼šæ¯ä¸€çº§èŠ‚ç‚¹æ—è¾¹çš„â€™xâ€™å’Œâ€™yâ€™è¡¨ç¤ºä»¥è¯¥èŠ‚ç‚¹åˆ†å‰²å·¦å³å­ç©ºé—´æ—¶splitæ‰€å–çš„å€¼ã€‚è¿™é‡Œè¿›è¡Œä¸€ç‚¹è¡¥å……è¯´æ˜Žï¼Œkdæ ‘å…¶å®žå°±æ˜¯äºŒå‰æ ‘ï¼Œå…¶ä¸Žæ™®é€šçš„äºŒå‰æŸ¥æ‰¾æ ‘ä¸åŒä¹‹å¤„åœ¨äºŽï¼Œå…¶æ¯ä¸€å±‚æ ¹æ®splitçš„ç»´åº¦è¿›è¡ŒäºŒå‰æ‹†åˆ†ã€‚å…·ä½“æ¥è¯´ï¼Œæ ¹æ®ä¸Šå›¾ï¼Œç¬¬ä¸€å±‚çš„æ‹†åˆ†æ˜¯æ ¹æ®xï¼Œé‚£ä¹ˆå…¶å·¦å­©å­çš„xå€¼å°±å°äºŽæ ¹ç»“ç‚¹çš„xå€¼ï¼Œå³å­©å­åˆ™åä¹‹ã€‚yå€¼åˆ™æ²¡æœ‰è§„å®šï¼ˆè¿™é‡Œå‡ºçŽ°çš„å·¦å¤§å³å°åªæ˜¯çº¯ç²¹çš„å·§åˆï¼‰ã€‚ç¬¬äºŒå±‚æ˜¯æ ¹æ®yå€¼æ¥è¿›è¡Œsplitï¼Œå› æ­¤ç¬¬ä¸‰å±‚çš„è§„å¾‹æ˜¾è€Œæ˜“è§ã€‚ä»£ç å®žçŽ°è¿è¡ŒçŽ¯å¢ƒï¼šWindows 10 Pro 64-bit x64-based(Ver. 10.0.14393), Python 3.5.2, Anaconda 4.1.1(64-bit), IPython 5.0.0, Windows CMD,kdTreeCreate.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import numpy as npfrom kdTreeNode import *def createDataSet(): """ Create the test dataset. Returns: A numpy array that contains the test data. """ dataSet = np.array([[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]) return dataSetdef split(dataSet): """ Split the given dataset. Returns: LeftDataSet: A kdTreeNode object. RightDataSet: A kdTreeNode object. NodeData: A tuple. """ # Ensure the dimension to split dimenIndex = np.var(dataSet, axis=0).argmax() partitionDataSet = dataSet[:, dimenIndex] # print(partitionDataSet) # Ensure the position to split partitionDataSetArgSort = partitionDataSet.argsort() # print(partitionDataSetArgSort) lenOfPartitionDataSetArgSort = len(partitionDataSetArgSort) if lenOfPartitionDataSetArgSort % 2 == 0: posIndex = lenOfPartitionDataSetArgSort // 2 splitIndex = partitionDataSetArgSort[posIndex] else: posIndex = (lenOfPartitionDataSetArgSort - 1) // 2 splitIndex = partitionDataSetArgSort[posIndex] # print(splitIndex) # Split nodeData = dataSet[splitIndex] leftIndeies = partitionDataSetArgSort[:posIndex] rightIndeies = partitionDataSetArgSort[posIndex + 1:] leftDataSet = dataSet[leftIndeies] rightDataSet = dataSet[rightIndeies] return nodeData, dimenIndex, leftDataSet, rightDataSetdef createKDTree(dataSet): """ Create the KD tree. Returns: A kdTreeNode object. """ if len(dataSet) == 0: return nodeData, dimenIndex, leftDataSet, rightDataSet = split(dataSet) # print(nodeData, dimenIndex, leftDataSet, rightDataSet) node = kdTreeNode(nodeData, dimenIndex) node.setLeft(createKDTree(leftDataSet)) node.setRight(createKDTree(rightDataSet)) return nodedef midTravel(node): if node is None: return midTravel(node.getLeft()) print(node.getData()) midTravel(node.getRight())if __name__ == "__main__": dataSet = createDataSet() node = createKDTree(dataSet) midTravel(node)kdTreeNode.py123456789101112131415161718192021222324252627282930313233class kdTreeNode(object): """ Class of k-d tree nodes """ def __init__(self, data=None, split=None, left=None, right=None): self.__data = data self.__split = split self.__left = left self.__right = right def getData(self): return self.__data def setData(self, data): self.__data = data def getSplit(self): return self.__split def setSplit(self, split): self.__split = split def getLeft(self): return self.__left def setLeft(self, left): self.__left = left def getRight(self): return self.__right def setRight(self, right): self.__right = rightè¿è¡Œç»“æžœï¼š12345678In [1]: run kdTreeCreate.py-------------------------------------[2 3][5 4][4 7][7 2][8 1][9 6]æ—¶é—´å¤æ‚åº¦ï¼šNä¸ªKç»´æ•°æ®è¿›è¡ŒæŸ¥æ‰¾æ“ä½œæ—¶æ—¶é—´å¤æ‚åº¦ä¸º $t=O(KN^{2})$ä¸‹é¢å°±è¦åœ¨å·²ç»å»ºç«‹å¥½çš„kdæ ‘ä¸Šè¿›è¡ŒæŸ¥æ‰¾æ“ä½œã€‚æŸ¥æ‰¾kdæ ‘ä¸­è¿›è¡Œçš„æŸ¥æ‰¾ä¸Žæ™®é€šçš„æŸ¥æ‰¾æ“ä½œå­˜åœ¨è¾ƒå¤§çš„å·®å¼‚ï¼Œå…¶ç›®çš„æ˜¯ä¸ºäº†æ‰¾å‡ºä¸ŽæŸ¥è¯¢ç‚¹è·ç¦»æœ€è¿‘çš„ç‚¹ã€‚æ˜Ÿå·è¡¨ç¤ºè¦æŸ¥è¯¢çš„ç‚¹(2.1, 3.1)ã€‚é€šè¿‡äºŒå‰æœç´¢ï¼Œé¡ºç€æœç´¢è·¯å¾„å¾ˆå¿«å°±èƒ½æ‰¾åˆ°æœ€é‚»è¿‘çš„è¿‘ä¼¼ç‚¹ï¼Œä¹Ÿå°±æ˜¯å¶å­èŠ‚ç‚¹(2, 3)ã€‚è€Œæ‰¾åˆ°çš„å¶å­èŠ‚ç‚¹å¹¶ä¸ä¸€å®šå°±æ˜¯æœ€é‚»è¿‘çš„ï¼Œæœ€é‚»è¿‘è‚¯å®šè·ç¦»æŸ¥è¯¢ç‚¹æ›´è¿‘ï¼Œåº”è¯¥ä½äºŽä»¥æŸ¥è¯¢ç‚¹ä¸ºåœ†å¿ƒä¸”é€šè¿‡å¶å­èŠ‚ç‚¹çš„åœ†åŸŸå†…ã€‚ä¸ºäº†æ‰¾åˆ°çœŸæ­£çš„æœ€è¿‘é‚»ï¼Œè¿˜éœ€è¦è¿›è¡Œâ€™å›žæº¯â€™æ“ä½œï¼šç®—æ³•æ²¿æœç´¢è·¯å¾„åå‘æŸ¥æ‰¾æ˜¯å¦æœ‰è·ç¦»æŸ¥è¯¢ç‚¹æ›´è¿‘çš„æ•°æ®ç‚¹ã€‚æ­¤ä¾‹ä¸­å…ˆä»Ž(7, 2)ç‚¹å¼€å§‹è¿›è¡ŒäºŒå‰æŸ¥æ‰¾ï¼Œç„¶åŽåˆ°è¾¾(5, 4)ï¼Œæœ€åŽåˆ°è¾¾(2, 3)ï¼Œæ­¤æ—¶æœç´¢è·¯å¾„ä¸­çš„èŠ‚ç‚¹ä¸º&lt;(7, 2), (5, 4), (2, 3)&gt;ï¼Œé¦–å…ˆä»¥(2, 3)ä½œä¸ºå½“å‰æœ€è¿‘é‚»ç‚¹ï¼Œè®¡ç®—å…¶åˆ°æŸ¥è¯¢ç‚¹(2.1, 3.1)çš„è·ç¦»ä¸º0.1414ï¼Œç„¶åŽå›žæº¯åˆ°å…¶çˆ¶èŠ‚ç‚¹(5, 4)ï¼Œå¹¶åˆ¤æ–­åœ¨è¯¥çˆ¶èŠ‚ç‚¹çš„å…¶ä»–å­èŠ‚ç‚¹ç©ºé—´ä¸­æ˜¯å¦æœ‰è·ç¦»æŸ¥è¯¢ç‚¹æ›´è¿‘çš„æ•°æ®ç‚¹ã€‚ä»¥(2.1, 3.1)ä¸ºåœ†å¿ƒï¼Œä»¥0.1414ä¸ºåŠå¾„ç”»åœ†ï¼Œå¦‚å›¾4æ‰€ç¤ºã€‚å‘çŽ°è¯¥åœ†å¹¶ä¸å’Œè¶…å¹³é¢y = 4äº¤å‰²ï¼Œå› æ­¤ä¸ç”¨è¿›å…¥(5, 4)èŠ‚ç‚¹å³å­ç©ºé—´ä¸­åŽ»æœç´¢ã€‚å†å›žæº¯åˆ°(7, 2)ï¼Œä»¥(2.1, 3.1)ä¸ºåœ†å¿ƒï¼Œä»¥0.1414ä¸ºåŠå¾„çš„åœ†æ›´ä¸ä¼šä¸Žx = 7è¶…å¹³é¢äº¤å‰²ï¼Œå› æ­¤ä¸ç”¨è¿›å…¥(7, 2)å³å­ç©ºé—´è¿›è¡ŒæŸ¥æ‰¾ã€‚è‡³æ­¤ï¼Œæœç´¢è·¯å¾„ä¸­çš„èŠ‚ç‚¹å·²ç»å…¨éƒ¨å›žæº¯å®Œï¼Œç»“æŸæ•´ä¸ªæœç´¢ï¼Œè¿”å›žæœ€è¿‘é‚»ç‚¹(2, 3)ï¼Œæœ€è¿‘è·ç¦»ä¸º0.1414ã€‚ä¸€ä¸ªå¤æ‚ç‚¹äº†ä¾‹å­å¦‚æŸ¥æ‰¾ç‚¹ä¸º(2, 4.5)ã€‚åŒæ ·å…ˆè¿›è¡ŒäºŒå‰æŸ¥æ‰¾ï¼Œå…ˆä»Ž(7, 2)æŸ¥æ‰¾åˆ°(5, 4)èŠ‚ç‚¹ï¼Œåœ¨è¿›è¡ŒæŸ¥æ‰¾æ—¶æ˜¯ç”±y = 4ä¸ºåˆ†å‰²è¶…å¹³é¢çš„ï¼Œç”±äºŽæŸ¥æ‰¾ç‚¹ä¸ºyå€¼ä¸º4.5ï¼Œå› æ­¤è¿›å…¥å³å­ç©ºé—´æŸ¥æ‰¾åˆ°(4, 7)ï¼Œå½¢æˆæœç´¢è·¯å¾„&lt;(7, 2), (5, 4), (4, 7)&gt;ï¼Œå–(4, 7)ä¸ºå½“å‰æœ€è¿‘é‚»ç‚¹ï¼Œè®¡ç®—å…¶ä¸Žç›®æ ‡æŸ¥æ‰¾ç‚¹çš„è·ç¦»ä¸º3.202ã€‚ç„¶åŽå›žæº¯åˆ°(5, 4)ï¼Œè®¡ç®—å…¶ä¸ŽæŸ¥æ‰¾ç‚¹ä¹‹é—´çš„è·ç¦»ä¸º3.041ã€‚ä»¥(2, 4.5)ä¸ºåœ†å¿ƒï¼Œä»¥3.041ä¸ºåŠå¾„ä½œåœ†ã€‚å¯è§è¯¥åœ†å’Œy = 4è¶…å¹³é¢äº¤å‰²ï¼Œæ‰€ä»¥éœ€è¦è¿›å…¥(5, 4)å·¦å­ç©ºé—´è¿›è¡ŒæŸ¥æ‰¾ã€‚æ­¤æ—¶éœ€å°†(2, 3)èŠ‚ç‚¹åŠ å…¥æœç´¢è·¯å¾„ä¸­å¾—&lt;(7, 2), (2, 3)&gt;ã€‚å›žæº¯è‡³(2, 3)å¶å­èŠ‚ç‚¹ï¼Œ(2, 3)è·ç¦»(2, 4.5)æ¯”(5, 4)è¦è¿‘ï¼Œæ‰€ä»¥æœ€è¿‘é‚»ç‚¹æ›´æ–°ä¸º(2, 3)ï¼Œæœ€è¿‘è·ç¦»æ›´æ–°ä¸º1.5ã€‚å›žæº¯è‡³(7, 2)ï¼Œä»¥(2, 4.5)ä¸ºåœ†å¿ƒ1.5ä¸ºåŠå¾„ä½œåœ†ï¼Œå¹¶ä¸å’Œx = 7åˆ†å‰²è¶…å¹³é¢äº¤å‰²ã€‚è‡³æ­¤ï¼Œæœç´¢è·¯å¾„å›žæº¯å®Œã€‚è¿”å›žæœ€è¿‘é‚»ç‚¹(2, 3)ï¼Œæœ€è¿‘è·ç¦»1.5ã€‚k-dæ ‘æŸ¥è¯¢ç®—æ³•çš„ä¼ªä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚æŸ¥æ‰¾ä¼ªä»£ç ç®—æ³•ï¼š k-dæ ‘æœ€é‚»è¿‘æŸ¥æ‰¾è¾“å…¥ï¼šKdï¼Œ //k-d treeç±»åž‹target //æŸ¥è¯¢æ•°æ®ç‚¹è¾“å‡ºï¼šnearestï¼Œ //æœ€é‚»è¿‘æ•°æ®ç‚¹dist //æœ€é‚»è¿‘æ•°æ®ç‚¹å’ŒæŸ¥è¯¢ç‚¹é—´çš„è·ç¦»123456789101112131415161718192021222324252627282930311. If Kdä¸ºNULLï¼Œåˆ™è®¾distä¸ºinfiniteå¹¶è¿”å›ž2. //è¿›è¡ŒäºŒå‰æŸ¥æ‰¾ï¼Œç”Ÿæˆæœç´¢è·¯å¾„ Kd_point = &amp;Kdï¼› //Kd-pointä¸­ä¿å­˜k-d treeæ ¹èŠ‚ç‚¹åœ°å€ nearest = Kd_point -&gt; Node-dataï¼› //åˆå§‹åŒ–æœ€è¿‘é‚»ç‚¹ whileï¼ˆKd_pointï¼‰ pushï¼ˆKd_pointï¼‰åˆ°search_pathä¸­ï¼› //search_pathæ˜¯ä¸€ä¸ªå †æ ˆç»“æž„ï¼Œå­˜å‚¨ç€æœç´¢è·¯å¾„èŠ‚ç‚¹æŒ‡é’ˆ /*** If Distï¼ˆnearestï¼Œtargetï¼‰ &gt; Distï¼ˆKd_point -&gt; Node-dataï¼Œtargetï¼‰ nearest = Kd_point -&gt; Node-dataï¼› //æ›´æ–°æœ€è¿‘é‚»ç‚¹ Max_dist = Dist(Kd_pointï¼Œtargetï¼‰ï¼› //æ›´æ–°æœ€è¿‘é‚»ç‚¹ä¸ŽæŸ¥è¯¢ç‚¹é—´çš„è·ç¦» ***/ s = Kd_point -&gt; splitï¼› //ç¡®å®šå¾…åˆ†å‰²çš„æ–¹å‘ If target[s] &lt;= Kd_point -&gt; Node-data[s] //è¿›è¡ŒäºŒå‰æŸ¥æ‰¾ Kd_point = Kd_point -&gt; leftï¼› else Kd_point = Kd_point -&gt;rightï¼› nearest = search_pathä¸­æœ€åŽä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼› //æ³¨æ„ï¼šäºŒå‰æœç´¢æ—¶ä¸æ¯”è®¡ç®—é€‰æ‹©æœç´¢è·¯å¾„ä¸­çš„æœ€é‚»è¿‘ç‚¹ï¼Œè¿™éƒ¨åˆ†å·²è¢«æ³¨é‡Š Max_dist = Distï¼ˆnearestï¼Œtargetï¼‰ï¼› //ç›´æŽ¥å–æœ€åŽå¶å­èŠ‚ç‚¹ä½œä¸ºå›žæº¯å‰çš„åˆå§‹æœ€è¿‘é‚»ç‚¹ 3. //å›žæº¯æŸ¥æ‰¾ whileï¼ˆsearch_path != NULLï¼‰ back_point = ä»Žsearch_pathå–å‡ºä¸€ä¸ªèŠ‚ç‚¹æŒ‡é’ˆï¼› //ä»Žsearch_pathå †æ ˆå¼¹æ ˆ s = back_point -&gt; splitï¼› //ç¡®å®šåˆ†å‰²æ–¹å‘ If Distï¼ˆtarget[s]ï¼Œback_point -&gt; Node-data[s]ï¼‰ &lt; Max_dist //åˆ¤æ–­è¿˜éœ€è¿›å…¥çš„å­ç©ºé—´ If target[s] &lt;= back_point -&gt; Node-data[s] Kd_point = back_point -&gt; rightï¼› //å¦‚æžœtargetä½äºŽå·¦å­ç©ºé—´ï¼Œå°±åº”è¿›å…¥å³å­ç©ºé—´ else Kd_point = back_point -&gt; left; //å¦‚æžœtargetä½äºŽå³å­ç©ºé—´ï¼Œå°±åº”è¿›å…¥å·¦å­ç©ºé—´ å°†Kd_pointåŽ‹å…¥search_pathå †æ ˆï¼› If Distï¼ˆnearestï¼Œtargetï¼‰ &gt; Distï¼ˆKd_Point -&gt; Node-dataï¼Œtargetï¼‰ nearest = Kd_point -&gt; Node-dataï¼› //æ›´æ–°æœ€è¿‘é‚»ç‚¹ Min_dist = Distï¼ˆKd_point -&gt; Node-data,targetï¼‰ï¼› //æ›´æ–°æœ€è¿‘é‚»ç‚¹ä¸ŽæŸ¥è¯¢ç‚¹é—´çš„è·ç¦»ä»£ç å®žçŽ°kdTreeSearch.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import numpy as npdef cal_dist(node, target): """ Calculate the distance between the node and the target. Arguments: node: The kd-tree's one node. target: Search target. Returns: dist: The distance between the two nodes. """ node_data = np.array(node) target_data = np.array(target) square_dist_vector = (node_data - target_data) ** 2 square_dist = np.sum(square_dist_vector) dist = square_dist ** 0.5 return distdef search(root_node, target): """ Search the nearest node of the target node in the kd-tree that root node is the root_node Arguments: root_node: The kd-tree's root node. target: Search target. Returns: nearest: The nearest node of the target node in the kd-tree. min_dist: The nearest distance. """ if root_node is None: min_dist = float('inf') return min_dist # Two-fork search kd_point = root_node # Save the root node nearest = kd_point.getData() # Initial the nearest node search_path = [] # Initial the search stack while kd_point: search_path.append(kd_point) split_index = kd_point.getSplit() # Ensure the split path if target[split_index] &lt;= kd_point.getData()[split_index]: kd_point = kd_point.getLeft() else: kd_point = kd_point.getRight() nearest = search_path.pop().getData() min_dist = cal_dist(nearest, target) # Retrospect search while search_path: back_point = search_path.pop() # Ensure the back-split path back_split_index = back_point.getSplit() # Judge if needs to enter the subspace if cal_dist(target[back_split_index], back_point.getData()[back_split_index]) &lt; min_dist: # If the target is in the left subspace, then enter the right if target[back_split_index] &lt;= back_point.getData()[back_split_index]: kd_point = back_point.getRight() # Otherwise enter the left else: kd_point = back_point.getLeft() # Add the node to the search path if kd_point is not None: search_path.append(kd_point) if cal_dist(nearest, target) &gt; cal_dist(kd_point.getData(), target): # Update the nearest node nearest = kd_point.getData() # Update the maximum distance min_dist = cal_dist(kd_point.getData(), target) return nearest, min_distè¿è¡Œç»“æžœï¼š12345678910111213141516171819202122232425262728293031323334In [1]: run kdTreeCreate.py-------------------------------------[2 3][5 4][4 7][7 2][8 1][9 6]In [2]: node-------------------------------------Out [2]: &lt;kdTreeNode.kdTreeNode at 0x26bff22f160&gt;In [3]: import kdTreeSearchIn [4]: nearest, min_dist = kdTreeSearch.search(node, [2.1, 3.1])In [5]: nearest-------------------------------------Out [5]: array([2, 3])In [6]: min_dist-------------------------------------Out [6]: 0.14142135623730964In [7]: nearest, min_dist = kdTreeSearch.search(node, [2, 4.5])In [8]: nearest-------------------------------------Out [8]: array([2, 3])In [9]: min_dist-------------------------------------Out [9]: 1.5æ—¶é—´å¤æ‚åº¦ï¼šNä¸ªç»“ç‚¹çš„Kç»´kdæ ‘è¿›è¡ŒæŸ¥æ‰¾æ“ä½œæ—¶æœ€åæ—¶é—´å¤æ‚åº¦ä¸º $t_{worst}=O(KN^{1-1/k})$æ ¹æ®ç›¸å…³ç ”ç©¶ï¼Œå½“æ•°æ®ç»´åº¦ä¸ºKæ—¶ï¼Œåªæœ‰å½“æ•°æ®é‡Næ»¡è¶³ $N&gt;&gt;2^K$ æ—¶ï¼Œæ‰èƒ½è¾¾åˆ°é«˜æ•ˆçš„æœç´¢ï¼ˆK&lt;20ï¼Œè¶…è¿‡20ç»´æ—¶å¯é‡‡ç”¨ball-treeç®—æ³•ï¼‰ï¼Œæ‰€ä»¥å¼•å‡ºäº†ä¸€ç³»åˆ—çš„æ”¹è¿›ç®—æ³•ï¼ˆBBFç®—æ³•ï¼Œå’Œä¸€ç³»åˆ—Mæ ‘ã€VPæ ‘ã€MVPæ ‘ç­‰é«˜ç»´ç©ºé—´ç´¢å¼•æ ‘ï¼‰ï¼Œç•™å¾…åŽç»­è¡¥å……ã€‚é‡‡ç”¨kdæ ‘çš„k-è¿‘é‚»ç®—æ³•æŽ¥ä¸‹æ¥ä¾¿æ˜¯å°†ä¸¤è€…ç›¸ç»“åˆã€‚[1] è¯´æ˜¯è®­ç»ƒé›†å…¶å®žæ˜¯ä¸å‡†ç¡®çš„ï¼Œå› ä¸ºk-è¿‘é‚»ç®—æ³•æ˜¯ä¸€ä¸ªæ— å‚æ•°æ–¹æ³•ï¼Œåªå­˜åœ¨ä¸€ä¸ªè¶…å‚kï¼Œå› æ­¤å…¶ä¸å­˜åœ¨ä¸€ä¸ªè®­ç»ƒçš„è¿‡ç¨‹[2] æµ‹è¯•é›†[3] Nä»£è¡¨è®­ç»ƒé›†çš„æ•°ç›®[4] å¤šæ•°è¡¨å†³]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F02%2F15%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new "My New Post"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment]]></content>
    </entry>

    
  
  
</search>
